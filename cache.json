{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2104.07504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weize Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>",
          "description": "Privacy preservation remains a key challenge in data mining and Natural\nLanguage Understanding (NLU). Previous research shows that the input text or\neven text embeddings can leak private information. This concern motivates our\nresearch on effective privacy preservation approaches for pretrained Language\nModels (LMs). We investigate the privacy and utility implications of applying\ndx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU\napplications. More importantly, we further propose privacy-adaptive LM\npretraining methods and show that our approach can boost the utility of BERT\ndramatically while retaining the same level of privacy protection. We also\nquantify the level of privacy preservation and provide guidance on privacy\nconfiguration. Our experiments and findings lay the groundwork for future\nexplorations of privacy-preserving NLU with pretrained LMs.",
          "link": "http://arxiv.org/abs/2104.07504",
          "publishedOn": "2021-08-23T01:36:34.751Z",
          "wordCount": 593,
          "title": "Natural Language Understanding with Privacy-Preserving BERT. (arXiv:2104.07504v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1\">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasek_J/0/1/0/all/0/1\">Jan Pa&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>",
          "description": "This paper describes the training process of the first Czech monolingual\nlanguage representation models based on BERT and ALBERT architectures. We\npre-train our models on more than 340K of sentences, which is 50 times more\nthan multilingual models that include Czech data. We outperform the\nmultilingual models on 9 out of 11 datasets. In addition, we establish the new\nstate-of-the-art results on nine datasets. At the end, we discuss properties of\nmonolingual and multilingual models based upon our results. We publish all the\npre-trained and fine-tuned models freely for the research community.",
          "link": "http://arxiv.org/abs/2103.13031",
          "publishedOn": "2021-08-23T01:36:34.734Z",
          "wordCount": 563,
          "title": "Czert -- Czech BERT-like Model for Language Representation. (arXiv:2103.13031v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hanjie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>",
          "description": "The literature has witnessed the success of leveraging Pre-trained Language\nModels (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural\nLanguage Processing (NLP) applications, yet it is not easy to build an\neasy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the\nEasyTransfer platform is designed to develop deep TL algorithms for NLP\napplications. EasyTransfer is backended with a high-performance and scalable\nengine for efficient training and inference, and also integrates comprehensive\ndeep TL algorithms, to make the development of industrial-scale TL applications\neasier. In EasyTransfer, the built-in data and model parallelism strategies,\ncombined with AI compiler optimization, show to be 4.0x faster than the\ncommunity version of distributed training. EasyTransfer supports various NLP\nmodels in the ModelZoo, including mainstream PLMs and multi-modality models. It\nalso features various in-house developed TL algorithms, together with the\nAppZoo for NLP applications. The toolkit is convenient for users to quickly\nstart model training, evaluation, and online deployment. EasyTransfer is\ncurrently deployed at Alibaba to support a variety of business scenarios,\nincluding item recommendation, personalized search, conversational question\nanswering, etc. Extensive experiments on real-world datasets and online\napplications show that EasyTransfer is suitable for online production with\ncutting-edge performance for various applications. The source code of\nEasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).",
          "link": "http://arxiv.org/abs/2011.09463",
          "publishedOn": "2021-08-23T01:36:34.726Z",
          "wordCount": 718,
          "title": "EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications. (arXiv:2011.09463v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Magdalena Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "The rise of personal assistants has made conversational question answering\n(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art\nmethods for ConvQA over knowledge graphs (KGs) can only learn from crisp\nquestion-answer pairs found in popular benchmarks. In reality, however, such\ntraining data is hard to come by: users would rarely mark answers explicitly as\ncorrect or wrong. In this work, we take a step towards a more natural learning\nparadigm - from noisy and implicit feedback via question reformulations. A\nreformulation is likely to be triggered by an incorrect system response,\nwhereas a new follow-up question could be a positive signal on the previous\nturn's answer. We present a reinforcement learning model, termed CONQUER, that\ncan learn from a conversational stream of questions and reformulations. CONQUER\nmodels the answering process as multiple agents walking in parallel on the KG,\nwhere the walks are determined by actions sampled using a policy network. This\npolicy network takes the question along with the conversational context as\ninputs and is trained via noisy rewards obtained from the reformulation\nlikelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark\nwith about 11k natural conversations containing around 205k reformulations.\nExperiments show that CONQUER successfully learns to answer conversational\nquestions from noisy reward signals, significantly improving over a\nstate-of-the-art baseline.",
          "link": "http://arxiv.org/abs/2105.04850",
          "publishedOn": "2021-08-23T01:36:34.699Z",
          "wordCount": 692,
          "title": "Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs. (arXiv:2105.04850v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Changzhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>",
          "description": "Conversation generation as a challenging task in Natural Language Generation\n(NLG) has been increasingly attracting attention over the last years. A number\nof recent works adopted sequence-to-sequence structures along with external\nknowledge, which successfully enhanced the quality of generated conversations.\nNevertheless, few works utilized the knowledge extracted from similar\nconversations for utterance generation. Taking conversations in customer\nservice and court debate domains as examples, it is evident that essential\nentities/phrases, as well as their associated logic and inter-relationships can\nbe extracted and borrowed from similar conversation instances. Such information\ncould provide useful signals for improving conversation generation. In this\npaper, we propose a novel reading and memory framework called Deep Reading\nMemory Network (DRMN) which is capable of remembering useful information of\nsimilar conversations for improving utterance generation. We apply our model to\ntwo large-scale conversation datasets of justice and e-commerce fields.\nExperiments prove that the proposed model outperforms the state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2108.09164",
          "publishedOn": "2021-08-23T01:36:34.452Z",
          "wordCount": 595,
          "title": "A Neural Conversation Generation Model via Equivalent Shared Memory Investigation. (arXiv:2108.09164v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1\">Oscar Perez-Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1\">Vicki Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1\">Louisa Jorm</a>",
          "description": "Electronic Medical Records contain clinical narrative text that is of great\npotential value to medical researchers. However, this information is mixed with\nPersonally Identifiable Information that presents risks to patient and\nclinician confidentiality. This paper presents an end-to-end de-identification\nframework to automatically remove PII from hospital discharge summaries. Our\ncorpus included 600 hospital discharge summaries which were extracted from the\nEMRs of two principal referral hospitals in Sydney, Australia. Our end-to-end\nde-identification framework consists of three components: 1) Annotation:\nlabelling of PII in the hospital discharge summaries using five pre-defined\ncategories: person, address, date of birth, individual identification number,\nphone/fax number; 2) Modelling: training six named entity recognition deep\nlearning base-models on balanced and imbalanced datasets; and evaluating\nensembles that combine all six base-models, the three base-models with the best\nF1 scores and the three base-models with the best recall scores respectively,\nusing token-level majority voting and stacking methods; and 3)\nDe-identification: removing PII from the hospital discharge summaries. Our\nresults showed that the ensemble model combined using the stacking Support\nVector Machine method on the three base-models with the best F1 scores achieved\nexcellent results with a F1 score of 99.16% on the test set of our corpus. We\nalso evaluated the robustness of our modelling component on the 2014 i2b2\nde-identification dataset. Our ensemble model, which uses the token-level\nmajority voting method on all six base-models, achieved the highest F1 score of\n96.24% at strict entity matching and the highest F1 score of 98.64% at binary\ntoken-level matching compared to two state-of-the-art methods. The end-to-end\nframework provides a robust solution to de-identifying clinical narrative\ncorpuses safely.",
          "link": "http://arxiv.org/abs/2101.00146",
          "publishedOn": "2021-08-23T01:36:34.438Z",
          "wordCount": 743,
          "title": "De-identifying Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models. (arXiv:2101.00146v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.",
          "link": "http://arxiv.org/abs/2108.09084",
          "publishedOn": "2021-08-23T01:36:34.415Z",
          "wordCount": 562,
          "title": "Fastformer: Additive Attention is All You Need. (arXiv:2108.09084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces. The demo is available at\nhttps://prasemap.qizhy.com.",
          "link": "http://arxiv.org/abs/2106.08801",
          "publishedOn": "2021-08-23T01:36:34.405Z",
          "wordCount": 602,
          "title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Structured information is an important knowledge source for automatic\nverification of factual claims. Nevertheless, the majority of existing research\ninto this task has focused on textual data, and the few recent inquiries into\nstructured data have been for the closed-domain setting where appropriate\nevidence for each claim is assumed to have already been retrieved. In this\npaper, we investigate verification over structured data in the open-domain\nsetting, introducing a joint reranking-and-verification model which fuses\nevidence documents in the verification component. Our open-domain model\nachieves performance comparable to the closed-domain state-of-the-art on the\nTabFact dataset, and demonstrates performance gains from the inclusion of\nmultiple tables as well as a significant improvement over a heuristic retrieval\nbaseline.",
          "link": "http://arxiv.org/abs/2012.15115",
          "publishedOn": "2021-08-23T01:36:34.390Z",
          "wordCount": 599,
          "title": "Joint Verification and Reranking for Open Fact Checking Over Tables. (arXiv:2012.15115v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>",
          "description": "With the development of deep learning (DL), natural language processing (NLP)\nmakes it possible for us to analyze and understand a large amount of language\ntexts. Accordingly, we can achieve a semantic communication in terms of joint\nsemantic source and channel coding over a noisy channel with the help of NLP.\nHowever, the existing method to realize this goal is to use a fixed transformer\nof NLP while ignoring the difference of semantic information contained in each\nsentence. To solve this problem, we propose a new semantic communication system\nbased on Universal Transformer. Compared with the traditional transformer, an\nadaptive circulation mechanism is introduced in the Universal Transformer.\nThrough the introduction of the circulation mechanism, the new semantic\ncommunication system can be more flexible to transmit sentences with different\nsemantic information, and achieve better end-to-end performance under various\nchannel conditions.",
          "link": "http://arxiv.org/abs/2108.09119",
          "publishedOn": "2021-08-23T01:36:34.382Z",
          "wordCount": 574,
          "title": "Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1906.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>",
          "description": "Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.",
          "link": "http://arxiv.org/abs/1906.02358",
          "publishedOn": "2021-08-23T01:36:34.351Z",
          "wordCount": 708,
          "title": "Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v9 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>",
          "description": "Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.",
          "link": "http://arxiv.org/abs/2108.08988",
          "publishedOn": "2021-08-23T01:36:34.343Z",
          "wordCount": 605,
          "title": "Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1\">David Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensmann_F/0/1/0/all/0/1\">Felix Bensmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1\">Stefan Dietze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Frank Kr&#xfc;ger</a>",
          "description": "Knowledge about software used in scientific investigations is important for\nseveral reasons, for instance, to enable an understanding of provenance and\nmethods involved in data handling. However, software is usually not formally\ncited, but rather mentioned informally within the scholarly description of the\ninvestigation, raising the need for automatic information extraction and\ndisambiguation. Given the lack of reliable ground truth data, we present\nSoMeSci (Software Mentions in Science) a gold standard knowledge graph of\nsoftware mentions in scientific articles. It contains high quality annotations\n(IRR: $\\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central\narticles. Besides the plain mention of the software, we also provide relation\nlabels for additional information, such as the version, the developer, a URL or\ncitations. Moreover, we distinguish between different types, such as\napplication, plugin or programming environment, as well as different types of\nmentions, such as usage or creation. To the best of our knowledge, SoMeSci is\nthe most comprehensive corpus about software mentions in scientific articles,\nproviding training samples for Named Entity Recognition, Relation Extraction,\nEntity Disambiguation, and Entity Linking. Finally, we sketch potential use\ncases and provide baseline results.",
          "link": "http://arxiv.org/abs/2108.09070",
          "publishedOn": "2021-08-23T01:36:34.336Z",
          "wordCount": 646,
          "title": "SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles. (arXiv:2108.09070v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Miao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>",
          "description": "Providing timely accessibility reminders of a point-of-interest (POI) plays a\nvital role in improving user satisfaction of finding places and making visiting\ndecisions. However, it is difficult to keep the POI database in sync with the\nreal-world counterparts due to the dynamic nature of business changes. To\nalleviate this problem, we formulate and present a practical solution that\njointly extracts POI mentions and identifies their coupled accessibility labels\nfrom unstructured text. We approach this task as a sequence tagging problem,\nwhere the goal is to produce <POI name, accessibility label> pairs from\nunstructured text. This task is challenging because of two main issues: (1) POI\nnames are often newly-coined words so as to successfully register new entities\nor brands and (2) there may exist multiple pairs in the text, which\nnecessitates dealing with one-to-many or many-to-one mapping to make each POI\ncoupled with its accessibility label. To this end, we propose a\nGeographic-Enhanced and Dependency-guIded sequence Tagging (GEDIT) model to\nconcurrently address the two challenges. First, to alleviate challenge #1, we\ndevelop a geographic-enhanced pre-trained model to learn the text\nrepresentations. Second, to mitigate challenge #2, we apply a relational graph\nconvolutional network to learn the tree node representations from the parsed\ndependency tree. Finally, we construct a neural sequence tagging model by\nintegrating and feeding the previously pre-learned representations into a CRF\nlayer. Extensive experiments conducted on a real-world dataset demonstrate the\nsuperiority and effectiveness of GEDIT. In addition, it has already been\ndeployed in production at Baidu Maps. Statistics show that the proposed\nsolution can save significant human effort and labor costs to deal with the\nsame amount of documents, which confirms that it is a practical way for POI\naccessibility maintenance.",
          "link": "http://arxiv.org/abs/2108.09104",
          "publishedOn": "2021-08-23T01:36:34.328Z",
          "wordCount": 740,
          "title": "GEDIT: Geographic-Enhanced and Dependency-Guided Tagging for Joint POI and Accessibility Extraction at Baidu Maps. (arXiv:2108.09104v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>",
          "description": "Relations between entities can be represented by different instances, e.g., a\nsentence containing both entities or a fact in a Knowledge Graph (KG). However,\nthese instances may not well capture the general relations between entities,\nmay be difficult to understand by humans, even may not be found due to the\nincompleteness of the knowledge source.\n\nIn this paper, we introduce the Open Relation Modeling task - given two\nentities, generate a coherent sentence describing the relation between them. To\nsolve this task, we propose to teach machines to generate definition-like\nrelation descriptions by letting them learn from definitions of entities.\nSpecifically, we fine-tune Pre-trained Language Models (PLMs) to produce\ndefinitions conditioned on extracted entity pairs. To help PLMs reason between\nentities and provide additional relational knowledge to PLMs for open relation\nmodeling, we incorporate reasoning paths in KGs and include a reasoning path\nselection mechanism. We show that PLMs can select interpretable and informative\nreasoning paths by confidence estimation, and the selected path can guide PLMs\nto generate better relation descriptions. Experimental results show that our\nmodel can generate concise but informative relation descriptions that capture\nthe representative characteristics of entities and relations.",
          "link": "http://arxiv.org/abs/2108.09241",
          "publishedOn": "2021-08-23T01:36:34.317Z",
          "wordCount": 632,
          "title": "Open Relation Modeling: Learning to Define Relations between Entities. (arXiv:2108.09241v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Soumava Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_G/0/1/0/all/0/1\">Gurunath Reddy M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">K Sreenivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Pratim Das</a>",
          "description": "Singing Voice Detection (SVD) has been an active area of research in music\ninformation retrieval (MIR). Currently, two deep neural network-based methods,\none based on CNN and the other on RNN, exist in literature that learn optimized\nfeatures for the voice detection (VD) task and achieve state-of-the-art\nperformance on common datasets. Both these models have a huge number of\nparameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for\ndeployment on devices like smartphones or embedded sensors with limited\ncapacity in terms of memory and computation power. The most popular method to\naddress this issue is known as knowledge distillation in deep learning\nliterature (in addition to model compression) where a large pre-trained network\nknown as the teacher is used to train a smaller student network. Given the wide\napplications of SVD in music information retrieval, to the best of our\nknowledge, model compression for practical deployment has not yet been\nexplored. In this paper, efforts have been made to investigate this issue using\nboth conventional as well as ensemble knowledge distillation techniques.",
          "link": "http://arxiv.org/abs/2011.04297",
          "publishedOn": "2021-08-23T01:36:34.293Z",
          "wordCount": 656,
          "title": "Knowledge Distillation for Singing Voice Detection. (arXiv:2011.04297v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damani_A/0/1/0/all/0/1\">Aashka Damani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1\">Martin Gunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>",
          "description": "Medical imaging is critical to the diagnosis and treatment of numerous\nmedical problems, including many forms of cancer. Medical imaging reports\ndistill the findings and observations of radiologists, creating an unstructured\ntextual representation of unstructured medical images. Large-scale use of this\ntext-encoded information requires converting the unstructured text to a\nstructured, semantic representation. We explore the extraction and\nnormalization of anatomical information in radiology reports that is associated\nwith radiological findings. We investigate this extraction and normalization\ntask using a span-based relation extraction model that jointly extracts\nentities and relations using BERT. This work examines the factors that\ninfluence extraction and normalization performance, including the body\npart/organ system, frequency of occurrence, span length, and span diversity. It\ndiscusses approaches for improving performance and creating high-quality\nsemantic representations of radiological phenomena.",
          "link": "http://arxiv.org/abs/2108.09211",
          "publishedOn": "2021-08-23T01:36:34.284Z",
          "wordCount": 576,
          "title": "Extracting Radiological Findings With Normalized Anatomical Information Using a Span-Based BERT Relation Extraction Model. (arXiv:2108.09211v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.",
          "link": "http://arxiv.org/abs/2108.09193",
          "publishedOn": "2021-08-23T01:36:34.271Z",
          "wordCount": 606,
          "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>",
          "description": "Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.",
          "link": "http://arxiv.org/abs/2108.09151",
          "publishedOn": "2021-08-23T01:36:34.206Z",
          "wordCount": 711,
          "title": "Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7BA%7Dbrego_G/0/1/0/all/0/1\">Gustavo Hern&#xe1;ndez {&#xc1;}brego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith B. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>",
          "description": "We provide the first exploration of text-to-text transformers (T5) sentence\nembeddings. Sentence embeddings are broadly useful for language processing\ntasks. While T5 achieves impressive performance on language tasks cast as\nsequence-to-sequence mapping problems, it is unclear how to produce sentence\nembeddings from encoder-decoder models. We investigate three methods for\nextracting T5 sentence embeddings: two utilize only the T5 encoder and one uses\nthe full T5 encoder-decoder model. Our encoder-only models outperforms\nBERT-based sentence embeddings on both transfer tasks and semantic textual\nsimilarity (STS). Our encoder-decoder method achieves further improvement on\nSTS. Scaling up T5 from millions to billions of parameters is found to produce\nconsistent improvements on downstream tasks. Finally, we introduce a two-stage\ncontrastive learning approach that achieves a new state-of-art on STS using\nsentence embeddings, outperforming both Sentence BERT and SimCSE.",
          "link": "http://arxiv.org/abs/2108.08877",
          "publishedOn": "2021-08-23T01:36:34.055Z",
          "wordCount": 573,
          "title": "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. (arXiv:2108.08877v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zerui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bite Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>",
          "description": "Recently, the performance of Pre-trained Language Models (PLMs) has been\nsignificantly improved by injecting knowledge facts to enhance their abilities\nof language understanding. For medical domains, the background knowledge\nsources are especially useful, due to the massive medical terms and their\ncomplicated relations are difficult to understand in text. In this work, we\nintroduce SMedBERT, a medical PLM trained on large-scale medical corpora,\nincorporating deep structured semantic knowledge from neighbors of\nlinked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to\nlearn heterogeneous-entity information, which infuses the semantic\nrepresentations of entity types into the homogeneous neighboring entity\nstructure. Apart from knowledge integration as external features, we propose to\nemploy the neighbors of linked-entities in the knowledge graph as additional\nglobal contexts of text mentions, allowing them to communicate via shared\nneighbors, thus enrich their semantic representations. Experiments demonstrate\nthat SMedBERT significantly outperforms strong baselines in various\nknowledge-intensive Chinese medical tasks. It also improves the performance of\nother tasks such as question answering, question matching and natural language\ninference.",
          "link": "http://arxiv.org/abs/2108.08983",
          "publishedOn": "2021-08-23T01:36:34.045Z",
          "wordCount": 618,
          "title": "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining. (arXiv:2108.08983v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yansen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn P. Rose</a>",
          "description": "As an important task in multimodal context understanding, Text-VQA (Visual\nQuestion Answering) aims at question answering through reading text information\nin images. It differentiates from the original VQA task as Text-VQA requires\nlarge amounts of scene-text relationship understanding, in addition to the\ncross-modal grounding capability. In this paper, we propose Localize, Group,\nand Select (LOGOS), a novel model which attempts to tackle this problem from\nmultiple aspects. LOGOS leverages two grounding tasks to better localize the\nkey information of the image, utilizes scene text clustering to group\nindividual OCR tokens, and learns to select the best answer from different\nsources of OCR (Optical Character Recognition) texts. Experiments show that\nLOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks\nwithout using additional OCR annotation data. Ablation studies and analysis\ndemonstrate the capability of LOGOS to bridge different modalities and better\nunderstand scene text.",
          "link": "http://arxiv.org/abs/2108.08965",
          "publishedOn": "2021-08-23T01:36:34.035Z",
          "wordCount": 592,
          "title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling. (arXiv:2108.08965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1\">Shayan Fazeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>",
          "description": "Topic Modeling refers to the problem of discovering the main topics that have\noccurred in corpora of textual data, with solutions finding crucial\napplications in numerous fields. In this work, inspired by the recent\nadvancements in the Natural Language Processing domain, we introduce FAME, an\nopen-source framework enabling an efficient mechanism of extracting and\nincorporating textual features and utilizing them in discovering topics and\nclustering text documents that are semantically similar in a corpus. These\nfeatures range from traditional approaches (e.g., frequency-based) to the most\nrecent auto-encoding embeddings from transformer-based language models such as\nBERT model family. To demonstrate the effectiveness of this library, we\nconducted experiments on the well-known News-Group dataset. The library is\navailable online.",
          "link": "http://arxiv.org/abs/2108.08946",
          "publishedOn": "2021-08-23T01:36:34.015Z",
          "wordCount": 550,
          "title": "A Framework for Neural Topic Modeling of Text Corpora. (arXiv:2108.08946v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Jared Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Multi-modal generation has been widely explored in recent years. Current\nresearch directions involve generating text based on an image or vice versa. In\nthis paper, we propose a new task called CIGLI: Conditional Image Generation\nfrom Language and Image. Instead of generating an image based on text as in\ntext-image generation, this task requires the generation of an image from a\ntextual description and an image prompt. We designed a new dataset to ensure\nthat the text description describes information from both images, and that\nsolely analyzing the description is insufficient to generate an image. We then\npropose a novel language-image fusion model which improves the performance over\ntwo established baseline methods, as evaluated by quantitative (automatic) and\nqualitative (human) evaluations. The code and dataset is available at\nhttps://github.com/vincentlux/CIGLI.",
          "link": "http://arxiv.org/abs/2108.08955",
          "publishedOn": "2021-08-23T01:36:34.004Z",
          "wordCount": 574,
          "title": "CIGLI: Conditional Image Generation from Language & Image. (arXiv:2108.08955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.",
          "link": "http://arxiv.org/abs/2108.09105",
          "publishedOn": "2021-08-23T01:36:33.966Z",
          "wordCount": 661,
          "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bite Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>",
          "description": "Machine Reading Comprehension (MRC) aims to extract answers to questions\ngiven a passage. It has been widely studied recently, especially in open\ndomains. However, few efforts have been made on closed-domain MRC, mainly due\nto the lack of large-scale training data. In this paper, we introduce a\nmulti-target MRC task for the medical domain, whose goal is to predict answers\nto medical questions and the corresponding support sentences from medical\ninformation sources simultaneously, in order to ensure the high reliability of\nmedical knowledge serving. A high-quality dataset is manually constructed for\nthe purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with\ndetailed analysis conducted. We further propose the Chinese medical BERT model\nfor the task (CMedBERT), which fuses medical knowledge into pre-trained\nlanguage models by the dynamic fusion mechanism of heterogeneous features and\nthe multi-task learning strategy. Experiments show that CMedBERT consistently\noutperforms strong baselines by fusing context-aware and knowledge-aware token\nrepresentations.",
          "link": "http://arxiv.org/abs/2008.10327",
          "publishedOn": "2021-08-23T01:36:33.941Z",
          "wordCount": 637,
          "title": "Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources. (arXiv:2008.10327v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1\">Megha Sundriyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Parantak Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubhashis Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "The formulation of a claim rests at the core of argument mining. To demarcate\nbetween a claim and a non-claim is arduous for both humans and machines, owing\nto latent linguistic variance between the two and the inadequacy of extensive\ndefinition-based formalization. Furthermore, the increase in the usage of\nonline social media has resulted in an explosion of unsolicited information on\nthe web presented as informal text. To account for the aforementioned, in this\npaper, we proposed DESYR. It is a framework that intends on annulling the said\nissues for informal web-based text by leveraging a combination of hierarchical\nrepresentation learning (dependency-inspired Poincare embedding),\ndefinition-based alignment, and feature projection. We do away with fine-tuning\ncomputer-heavy language models in favor of fabricating a more domain-centric\nbut lighter approach. Experimental results indicate that DESYR builds upon the\nstate-of-the-art system across four benchmark claim datasets, most of which\nwere constructed with informal texts. We see an increase of 3 claim-F1 points\non the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1\npoints on the Online Comments(OC) dataset, an increase of 24 claim-F1 points\nand 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8\nclaim-F1 points and 5 macro-F1 points on the Micro Texts(MT) dataset. We also\nperform an extensive analysis of the results. We make a 100-D pre-trained\nversion of our Poincare-variant along with the source code.",
          "link": "http://arxiv.org/abs/2108.08759",
          "publishedOn": "2021-08-20T01:53:50.948Z",
          "wordCount": 678,
          "title": "DESYR: Definition and Syntactic Representation Based Claim Detection on the Web. (arXiv:2108.08759v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1\">Raphael Pisoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1\">Sri Lakshmi</a>",
          "description": "CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal\nmodel that jointly learns representations of images and texts. The model is\ntrained on a massive amount of English data and shows impressive performance on\nzero-shot classification tasks. Training the same model on a different language\nis not trivial, since data in other languages might be not enough and the model\nneeds high-quality translations of the texts to guarantee a good performance.\nIn this paper, we present the first CLIP model for the Italian Language\n(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show\nthat CLIP-Italian outperforms the multilingual CLIP model on the tasks of image\nretrieval and zero-shot classification.",
          "link": "http://arxiv.org/abs/2108.08688",
          "publishedOn": "2021-08-20T01:53:50.916Z",
          "wordCount": 552,
          "title": "Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.",
          "link": "http://arxiv.org/abs/2004.03744",
          "publishedOn": "2021-08-20T01:53:50.788Z",
          "wordCount": 621,
          "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Encheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yongping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meiling Hu</a>",
          "description": "Medical Dialogue Generation (MDG) is intended to build a medical dialogue\nsystem for intelligent consultation, which can communicate with patients in\nreal-time, thereby improving the efficiency of clinical diagnosis with broad\napplication prospects. This paper presents our proposed framework for the\nChinese MDG organized by the 2021 China conference on knowledge graph and\nsemantic computing (CCKS) competition, which requires generating\ncontext-consistent and medically meaningful responses conditioned on the\ndialogue history. In our framework, we propose a pipeline system composed of\nentity prediction and entity-aware dialogue generation, by adding predicted\nentities to the dialogue model with a fusion mechanism, thereby utilizing\ninformation from different sources. At the decoding stage, we propose a new\ndecoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve\nentity correctness and promote the length and quality of the final response.\nThe proposed method wins both the CCKS and the International Conference on\nLearning Representations (ICLR) 2021 Workshop Machine Learning for Preventing\nand Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which\ndemonstrate the practicality and effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.01266",
          "publishedOn": "2021-08-20T01:53:50.756Z",
          "wordCount": 643,
          "title": "More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.",
          "link": "http://arxiv.org/abs/2102.00225",
          "publishedOn": "2021-08-20T01:53:50.663Z",
          "wordCount": 581,
          "title": "Learning From How Human Correct. (arXiv:2102.00225v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1\">Rohit Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara Lee Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-benchmark.github.io/.",
          "link": "http://arxiv.org/abs/2106.04632",
          "publishedOn": "2021-08-20T01:53:50.648Z",
          "wordCount": 703,
          "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique is to apply named entity disambiguation (NED)\nsystems to the question, and retrieve KB facts for the disambiguated entities.\nThis work presents ECQA, an efficient method that prunes irrelevant parts of\nthe search space using KB-aware signals. ECQA is based on top-k query\nprocessing over score-ordered lists of KB items that combine signals about\nlexical matching, relevance to the question, coherence among candidate items,\nand connectivity in the KB graph. Experiments with two recent QA benchmarks\ndemonstrate the superiority of ECQA over state-of-the-art baselines with\nrespect to answer presence, size of the search space, and runtimes.",
          "link": "http://arxiv.org/abs/2108.08597",
          "publishedOn": "2021-08-20T01:53:50.622Z",
          "wordCount": 590,
          "title": "Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-08-20T01:53:50.615Z",
          "wordCount": 751,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.",
          "link": "http://arxiv.org/abs/2108.08614",
          "publishedOn": "2021-08-20T01:53:50.459Z",
          "wordCount": 654,
          "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.03599",
          "publishedOn": "2021-08-20T01:53:50.338Z",
          "wordCount": 619,
          "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>",
          "description": "This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).",
          "link": "http://arxiv.org/abs/2108.08708",
          "publishedOn": "2021-08-20T01:53:50.331Z",
          "wordCount": 594,
          "title": "Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingchao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Heng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liaosa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Existing system dealing with online complaint provides a final decision\nwithout explanations. We propose to analyse the complaint text of internet\nfraud in a fine-grained manner. Considering the complaint text includes\nmultiple clauses with various functions, we propose to identify the role of\neach clause and classify them into different types of fraud element. We\nconstruct a large labeled dataset originated from a real finance service\nplatform. We build an element identification model on top of BERT and propose\nadditional two modules to utilize the context of complaint text for better\nelement label classification, namely, global context encoder and label refiner.\nExperimental results show the effectiveness of our model.",
          "link": "http://arxiv.org/abs/2108.08676",
          "publishedOn": "2021-08-20T01:53:50.324Z",
          "wordCount": 569,
          "title": "Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>",
          "description": "Spoken Language Understanding (SLU) is one essential step in building a\ndialogue system. Due to the expensive cost of obtaining the labeled data, SLU\nsuffers from the data scarcity problem. Therefore, in this paper, we focus on\ndata augmentation for slot filling task in SLU. To achieve that, we aim at\ngenerating more diverse data based on existing data. Specifically, we try to\nexploit the latent language knowledge from pretrained language models by\nfinetuning them. We propose two strategies for finetuning process: value-based\nand context-based augmentation. Experimental results on two public SLU datasets\nhave shown that compared with existing data augmentation methods, our proposed\nmethod can generate more diverse sentences and significantly improve the\nperformance on SLU.",
          "link": "http://arxiv.org/abs/2108.08451",
          "publishedOn": "2021-08-20T01:53:50.243Z",
          "wordCount": 563,
          "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models. (arXiv:2108.08451v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Although automated metrics are commonly used to evaluate NLG systems, they\noften correlate poorly with human judgements. Newer metrics such as BERTScore\nhave addressed many weaknesses in prior metrics such as BLEU and ROUGE, which\nrely on n-gram matching. These newer methods, however, are still limited in\nthat they do not consider the generation context, so they cannot properly\nreward generated text that is correct but deviates from the given reference.\n\nIn this paper, we propose Language Model Augmented Relevance Score (MARS), a\nnew context-aware metric for NLG evaluation. MARS leverages off-the-shelf\nlanguage models, guided by reinforcement learning, to create augmented\nreferences that consider both the generation context and available human\nreferences, which are then used as additional references to score generated\ntext. Compared with seven existing metrics in three common NLG tasks, MARS not\nonly achieves higher correlation with human reference judgements, but also\ndifferentiates well-formed candidates from adversarial samples to a larger\ndegree.",
          "link": "http://arxiv.org/abs/2108.08485",
          "publishedOn": "2021-08-20T01:53:50.215Z",
          "wordCount": 589,
          "title": "Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangzhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jialiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_Z/0/1/0/all/0/1\">Ziquan Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Current storytelling systems focus more ongenerating stories with coherent\nplots regard-less of the narration style, which is impor-tant for controllable\ntext generation. There-fore, we propose a new task, stylized story gen-eration,\nnamely generating stories with speci-fied style given a leading context. To\ntacklethe problem, we propose a novel generationmodel that first plans the\nstylized keywordsand then generates the whole story with theguidance of the\nkeywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency\nbetween the generated story andthe specified style. Experiments\ndemonstratesthat our model can controllably generateemo-tion-driven\norevent-driven stories based onthe ROCStories dataset (Mostafazadeh et\nal.,2016). Our study presents insights for stylizedstory generation in further\nresearch.",
          "link": "http://arxiv.org/abs/2105.08625",
          "publishedOn": "2021-08-20T01:53:50.200Z",
          "wordCount": 588,
          "title": "Stylized Story Generation with Style-Guided Planning. (arXiv:2105.08625v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Megan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1\">Bryan Wilson</a>",
          "description": "Legal interpretation is a linguistic venture. In judicial opinions, for\nexample, courts are often asked to interpret the text of statutes and\nlegislation. As time has shown, this is not always as easy as it sounds.\nMatters can hinge on vague or inconsistent language and, under the surface,\nhuman biases can impact the decision-making of judges. This raises an important\nquestion: what if there was a method of extracting the meaning of statutes\nconsistently? That is, what if it were possible to use machines to encode\nlegislation in a mathematically precise form that would permit clearer\nresponses to legal questions? This article attempts to unpack the notion of\nmachine-readability, providing an overview of both its historical and recent\ndevelopments. The paper will reflect on logic syntax and symbolic language to\nassess the capacity and limits of representing legal knowledge. In doing so,\nthe paper seeks to move beyond existing literature to discuss the implications\nof various approaches to machine-readable legislation. Importantly, this study\nhopes to highlight the challenges encountered in this burgeoning ecosystem of\nmachine-readable legislation against existing human-readable counterparts.",
          "link": "http://arxiv.org/abs/2108.08678",
          "publishedOn": "2021-08-20T01:53:50.192Z",
          "wordCount": 604,
          "title": "The Legislative Recipe: Syntax for Machine-Readable Legislation. (arXiv:2108.08678v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual\nretrieval in eleven typologically diverse languages, designed to evaluate\nranking with learned dense representations. The goal of this resource is to\nspur research in dense retrieval techniques in non-English languages, motivated\nby recent observations that existing techniques for representation learning\nperform poorly when applied to out-of-distribution data. As a starting point,\nwe provide zero-shot baselines for this new dataset based on a multi-lingual\nadaptation of DPR that we call \"mDPR\". Experiments show that although the\neffectiveness of mDPR is much lower than BM25, dense representations\nnevertheless appear to provide valuable relevance signals, improving BM25\nresults in sparse-dense hybrids. In addition to analyses of our results, we\nalso discuss future challenges and present a research agenda in multi-lingual\ndense retrieval. Mr. TyDi can be downloaded at\nhttps://github.com/castorini/mr.tydi.",
          "link": "http://arxiv.org/abs/2108.08787",
          "publishedOn": "2021-08-20T01:53:50.169Z",
          "wordCount": 574,
          "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1\">Karthik Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durairaj_T/0/1/0/all/0/1\">Thenmozi Durairaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>",
          "description": "This paper reports the Machine Translation (MT) systems submitted by the\nIIITT team for the English->Marathi and English->Irish language pairs LoResMT\n2021 shared task. The task focuses on getting exceptional translations for\nrather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,\na pretrained multilingual NMT model for English->Marathi, using external\nparallel corpus as input for additional training. We have used a pretrained\nHelsinki-NLP Opus MT English->Irish model for the latter language pair. Our\napproaches yield relatively promising results on the BLEU metrics. Under the\nteam name IIITT, our systems ranked 1, 1, and 2 in English->Marathi,\nIrish->English, and English->Irish, respectively.",
          "link": "http://arxiv.org/abs/2108.08556",
          "publishedOn": "2021-08-20T01:53:50.131Z",
          "wordCount": 549,
          "title": "Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "This paper studies the relative importance of attention heads in\nTransformer-based models to aid their interpretability in cross-lingual and\nmulti-lingual tasks. Prior research has found that only a few attention heads\nare important in each mono-lingual Natural Language Processing (NLP) task and\npruning the remaining heads leads to comparable or improved performance of the\nmodel. However, the impact of pruning attention heads is not yet clear in\ncross-lingual and multi-lingual tasks. Through extensive experiments, we show\nthat (1) pruning a number of attention heads in a multi-lingual\nTransformer-based model has, in general, positive effects on its performance in\ncross-lingual and multi-lingual tasks and (2) the attention heads to be pruned\ncan be ranked using gradients and identified with a few trial experiments. Our\nexperiments focus on sequence labeling tasks, with potential applicability on\nother cross-lingual and multi-lingual tasks. For comprehensiveness, we examine\ntwo pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and\nXLM-R, on three tasks across 9 languages each. We also discuss the validity of\nour findings and their extensibility to truly resource-scarce languages and\nother task settings.",
          "link": "http://arxiv.org/abs/2108.08375",
          "publishedOn": "2021-08-20T01:53:50.110Z",
          "wordCount": 626,
          "title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1\">Pavel Dolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1\">Luc d&#x27;Hauthuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1\">Andrea Vattani</a>",
          "description": "Twitch chats pose a unique problem in natural language understanding due to a\nlarge presence of neologisms, specifically emotes. There are a total of 8.06\nmillion emotes, over 400k of which were used in the week studied. There is\nvirtually no information on the meaning or sentiment of emotes, and with a\nconstant influx of new emotes and drift in their frequencies, it becomes\nimpossible to maintain an updated manually-labeled dataset. Our paper makes a\ntwo fold contribution. First we establish a new baseline for sentiment analysis\non Twitch data, outperforming the previous supervised benchmark by 7.9% points.\nSecondly, we introduce a simple but powerful unsupervised framework based on\nword embeddings and k-NN to enrich existing models with out-of-vocabulary\nknowledge. This framework allows us to auto-generate a pseudo-dictionary of\nemotes and we show that we can nearly match the supervised benchmark above even\nwhen injecting such emote knowledge into sentiment classifiers trained on\nextraneous datasets such as movie reviews or Twitter.",
          "link": "http://arxiv.org/abs/2108.08411",
          "publishedOn": "2021-08-20T01:53:50.069Z",
          "wordCount": 591,
          "title": "FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zexian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohui Hu</a>",
          "description": "Conditional masked language models (CMLM) have shown impressive progress in\nnon-autoregressive machine translation (NAT). They learn the conditional\ntranslation model by predicting the random masked subset in the target\nsentence. Based on the CMLM framework, we introduce Multi-view Subset\nRegularization (MvSR), a novel regularization method to improve the performance\nof the NAT model. Specifically, MvSR consists of two parts: (1) \\textit{shared\nmask consistency}: we forward the same target with different mask strategies,\nand encourage the predictions of shared mask positions to be consistent with\neach other. (2) \\textit{model consistency}, we maintain an exponential moving\naverage of the model weights, and enforce the predictions to be consistent\nbetween the average model and the online model. Without changing the CMLM-based\narchitecture, our approach achieves remarkable performance on three public\nbenchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover,\ncompared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44\nBLEU scores on small datasets (WMT16 RO$\\leftrightarrow$EN and IWSLT\nDE$\\rightarrow$EN).",
          "link": "http://arxiv.org/abs/2108.08447",
          "publishedOn": "2021-08-20T01:53:50.056Z",
          "wordCount": 588,
          "title": "MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation. (arXiv:2108.08447v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1\">Jatin Ganhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zolt&#xe1;n T&#xfc;ske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>",
          "description": "End-to-end spoken language understanding (SLU) systems that process\nhuman-human or human-computer interactions are often context independent and\nprocess each turn of a conversation independently. Spoken conversations on the\nother hand, are very much context dependent, and dialog history contains useful\ninformation that can improve the processing of each conversational turn. In\nthis paper, we investigate the importance of dialog history and how it can be\neffectively integrated into end-to-end SLU systems. While processing a spoken\nutterance, our proposed RNN transducer (RNN-T) based SLU model has access to\nits dialog history in the form of decoded transcripts and SLU labels of\nprevious turns. We encode the dialog history as BERT embeddings, and use them\nas an additional input to the SLU model along with the speech features for the\ncurrent utterance. We evaluate our approach on a recently released spoken\ndialog data set, the HarperValleyBank corpus. We observe significant\nimprovements: 8% for dialog action and 30% for caller intent recognition tasks,\nin comparison to a competitive context independent end-to-end baseline system.",
          "link": "http://arxiv.org/abs/2108.08405",
          "publishedOn": "2021-08-20T01:53:50.003Z",
          "wordCount": 624,
          "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems. (arXiv:2108.08405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:49.979Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinden_K/0/1/0/all/0/1\">Kohei Shinden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Makoto P. Kato</a>",
          "description": "This paper addresses the problem of generating table captions for scholarly\ndocuments, which often require additional information outside the table. To\nthis end, we propose a method of retrieving relevant sentences from the paper\nbody, and feeding the table content as well as the retrieved sentences into\npre-trained language models (e.g. T5 and GPT-2) for generating table captions.\nThe contributions of this paper are: (1) discussion on the challenges in table\ncaptioning for scholarly documents; (2) development of a dataset DocBank-TB,\nwhich is publicly available; and (3) comparison of caption generation methods\nfor scholarly documents with different strategies to retrieve relevant\nsentences from the paper body. Our experimental results showed that T5 is the\nbetter generation model for this task, as it outperformed GPT-2 in BLEU and\nMETEOR implying that the generated text are clearer and more precise. Moreover,\ninputting relevant sentences matching the row header or whole table is\neffective.",
          "link": "http://arxiv.org/abs/2108.08111",
          "publishedOn": "2021-08-19T01:35:02.750Z",
          "wordCount": 599,
          "title": "Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models. (arXiv:2108.08111v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1\">Benjamin Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyu &quot;Sherwin&quot; Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Jeremy Straub</a>",
          "description": "Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.",
          "link": "http://arxiv.org/abs/2108.08264",
          "publishedOn": "2021-08-19T01:35:00.438Z",
          "wordCount": 604,
          "title": "Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration. The code and pretrained models are available at\n\\url{https://aka.ms/deltalm}.",
          "link": "http://arxiv.org/abs/2106.13736",
          "publishedOn": "2021-08-19T01:35:00.347Z",
          "wordCount": 637,
          "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>",
          "description": "Product summarization aims to automatically generate product descriptions,\nwhich is of great commercial potential. Considering the customer preferences on\ndifferent product aspects, it would benefit from generating aspect-oriented\ncustomized summaries. However, conventional systems typically focus on\nproviding general product summaries, which may miss the opportunity to match\nproducts with customer interests. To address the problem, we propose CUSTOM,\naspect-oriented product summarization for e-commerce, which generates diverse\nand controllable summaries towards different product aspects. To support the\nstudy of CUSTOM and further this line of research, we construct two Chinese\ndatasets, i.e., SMARTPHONE and COMPUTER, including 76,279 / 49,280 short\nsummaries for 12,118 / 11,497 real-world commercial products, respectively.\nFurthermore, we introduce EXT, an extraction-enhanced generation framework for\nCUSTOM, where two famous sequence-to-sequence models are implemented in this\npaper. We conduct extensive experiments on the two proposed datasets for CUSTOM\nand show results of two famous baseline models and EXT, which indicates that\nEXT can generate diverse, high-quality, and consistent summaries.",
          "link": "http://arxiv.org/abs/2108.08010",
          "publishedOn": "2021-08-19T01:35:00.336Z",
          "wordCount": 600,
          "title": "CUSTOM: Aspect-Oriented Product Summarization for E-Commerce. (arXiv:2108.08010v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>",
          "description": "We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.",
          "link": "http://arxiv.org/abs/2102.10772",
          "publishedOn": "2021-08-19T01:35:00.258Z",
          "wordCount": 617,
          "title": "UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1\">Bela Bohlender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1\">Vincent Hane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1\">Yanik Adamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1\">Jaber Khuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1\">Jonas Brossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.",
          "link": "http://arxiv.org/abs/2108.08103",
          "publishedOn": "2021-08-19T01:35:00.243Z",
          "wordCount": 627,
          "title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:00.217Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengkun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>",
          "description": "Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.",
          "link": "http://arxiv.org/abs/2108.08102",
          "publishedOn": "2021-08-19T01:35:00.209Z",
          "wordCount": 534,
          "title": "Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>",
          "description": "The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing BioNER models can effectively handle them. In\nthis work, we systematically analyze the three types of recognition abilities\nof BioNER models: memorization, synonym generalization, and concept\ngeneralization. We find that although BioNER models achieve state-of-the-art\nperformance on BioNER benchmarks based on overall performance, they have\nlimitations in identifying synonyms and new biomedical concepts such as\nCOVID-19. From this observation, we conclude that existing BioNER models are\noverestimated in terms of their generalization abilities. Also, we identify\nseveral difficulties in recognizing unseen mentions in BioNER and make the\nfollowing conclusions: (1) BioNER models tend to exploit dataset biases, which\nhinders the models' abilities to generalize, and (2) several biomedical names\nhave novel morphological patterns with little name regularity such as COVID-19,\nand models fail to recognize them. We apply a current statistics-based\ndebiasing method to our problem as a simple remedy and show the improvement in\ngeneralization to unseen mentions. We hope that our analyses and findings would\nbe able to facilitate further research into the generalization capabilities of\nNER models in a domain where their reliability is of utmost importance.",
          "link": "http://arxiv.org/abs/2101.00160",
          "publishedOn": "2021-08-19T01:35:00.202Z",
          "wordCount": 721,
          "title": "How Do Your Biomedical Named Entity Models Generalize to Novel Entities?. (arXiv:2101.00160v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-08-19T01:35:00.180Z",
          "wordCount": 531,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-19T01:35:00.171Z",
          "wordCount": 798,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>",
          "description": "Reasoning machine reading comprehension (R-MRC) aims to answer complex\nquestions that require discrete reasoning based on text. To support discrete\nreasoning, evidence, typically the concise textual fragments that describe\nquestion-related facts, including topic entities and attribute values, are\ncrucial clues from question to answer. However, previous end-to-end methods\nthat achieve state-of-the-art performance rarely solve the problem by paying\nenough emphasis on the modeling of evidence, missing the opportunity to further\nimprove the model's reasoning ability for R-MRC. To alleviate the above issue,\nin this paper, we propose an evidence-emphasized discrete reasoning approach\n(EviDR), in which sentence and clause level evidence is first detected based on\ndistant supervision, and then used to drive a reasoning module implemented with\na relational heterogeneous graph convolutional network to derive answers.\nExtensive experiments are conducted on DROP (discrete reasoning over\nparagraphs) dataset, and the results demonstrate the effectiveness of our\nproposed approach. In addition, qualitative analysis verifies the capability of\nthe proposed evidence-emphasized discrete reasoning for R-MRC.",
          "link": "http://arxiv.org/abs/2108.07994",
          "publishedOn": "2021-08-19T01:35:00.146Z",
          "wordCount": 612,
          "title": "EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension. (arXiv:2108.07994v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>",
          "description": "Existing data-driven methods can well handle short text generation. However,\nwhen applied to the long-text generation scenarios such as story generation or\nadvertising text generation in the commercial scenario, these methods may\ngenerate illogical and uncontrollable texts. To address these aforementioned\nissues, we propose a graph-based grouping planner(GGP) following the idea of\nfirst-plan-then-generate. Specifically, given a collection of key phrases, GGP\nfirstly encodes these phrases into an instance-level sequential representation\nand a corpus-level graph-based representation separately. With these two\nsynergic representations, we then regroup these phrases into a fine-grained\nplan, based on which we generate the final long text. We conduct our\nexperiments on three long text generation datasets and the experimental results\nreveal that GGP significantly outperforms baselines, which proves that GGP can\ncontrol the long text generation by knowing how to say and in what order.",
          "link": "http://arxiv.org/abs/2108.07998",
          "publishedOn": "2021-08-19T01:35:00.095Z",
          "wordCount": 582,
          "title": "GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation. (arXiv:2108.07998v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07909",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1\">D.-S. Wang</a>",
          "description": "Quantum computing has been a fascinating research field in quantum physics.\nRecent progresses motivate us to study in depth the universal quantum computing\nmodels (UQCM), which lie at the foundation of quantum computing and have tight\nconnections with fundamental physics. Although being developed decades ago, a\nphysically concise principle or picture to formalize and understand UQCM is\nstill lacking. This is challenging given the diversity of still-emerging\nmodels, but important to understand the difference between classical and\nquantum computing. In this work, we carried out a primary attempt to unify UQCM\nby classifying a few of them as two categories, hence making a table of models.\nWith such a table, some known models or schemes appear as hybridization or\ncombination of models, and more importantly, it leads to new schemes that have\nnot been explored yet. Our study of UQCM also leads to some insights into\nquantum algorithms. This work reveals the importance and feasibility of\nsystematic study of computing models.",
          "link": "http://arxiv.org/abs/2108.07909",
          "publishedOn": "2021-08-19T01:34:59.845Z",
          "wordCount": 602,
          "title": "A comparative study of universal quantum computing models: towards a physical unification. (arXiv:2108.07909v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>",
          "description": "Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.",
          "link": "http://arxiv.org/abs/2105.15176",
          "publishedOn": "2021-08-19T01:34:59.788Z",
          "wordCount": 668,
          "title": "Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.",
          "link": "http://arxiv.org/abs/2108.08226",
          "publishedOn": "2021-08-19T01:34:59.714Z",
          "wordCount": 700,
          "title": "TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weiwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazi_M/0/1/0/all/0/1\">Michaeel Kazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoutong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huiji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Many search systems work with large amounts of natural language data, e.g.,\nsearch queries, user profiles and documents, where deep learning based natural\nlanguage processing techniques (deep NLP) can be of great help. In this paper,\nwe introduce a comprehensive study of applying deep NLP techniques to five\nrepresentative tasks in search engines. Through the model design and\nexperiments of the five tasks, readers can find answers to three important\nquestions: (1) When is deep NLP helpful/not helpful in search systems? (2) How\nto address latency challenges? (3) How to ensure model robustness? This work\nbuilds on existing efforts of LinkedIn search, and is tested at scale on a\ncommercial search engine. We believe our experiences can provide useful\ninsights for the industry and research communities.",
          "link": "http://arxiv.org/abs/2108.08252",
          "publishedOn": "2021-08-19T01:34:59.696Z",
          "wordCount": 571,
          "title": "Deep Natural Language Processing for LinkedIn Search Systems. (arXiv:2108.08252v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bharwani_N/0/1/0/all/0/1\">Nashwin Bharwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushner_W/0/1/0/all/0/1\">Warren Kushner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandona_S/0/1/0/all/0/1\">Sangeet Dandona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_B/0/1/0/all/0/1\">Ben Schreiber</a>",
          "description": "Natural Language Processing research has recently been dominated by large\nscale transformer models. Although they achieve state of the art on many\nimportant language tasks, transformers often require expensive compute\nresources, and days spanning to weeks to train. This is feasible for\nresearchers at big tech companies and leading research universities, but not\nfor scrappy start-up founders, students, and independent researchers. Stephen\nMerity's SHA-RNN, a compact, hybrid attention-RNN model, is designed for\nconsumer-grade modeling as it requires significantly fewer parameters and less\ntraining time to reach near state of the art results. We analyze Merity's model\nhere through an exploratory model analysis over several units of the\narchitecture considering both training time and overall quality in our\nassessment. Ultimately, we combine these findings into a new architecture which\nwe call SHAQ: Single Headed Attention Quasi-recurrent Neural Network. With our\nnew architecture we achieved similar accuracy results as the SHA-RNN while\naccomplishing a 4x speed boost in training.",
          "link": "http://arxiv.org/abs/2108.08207",
          "publishedOn": "2021-08-19T01:34:59.675Z",
          "wordCount": 594,
          "title": "SHAQ: Single Headed Attention with Quasi-Recurrence. (arXiv:2108.08207v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Generating context-aware language that embodies diverse emotions is an\nimportant step towards building empathetic NLP systems. In this paper, we\npropose a formulation of modulated layer normalization -- a technique inspired\nby computer vision -- that allows us to use large-scale language models for\nemotional response generation. In automatic and human evaluation on the\nMojiTalk dataset, our proposed modulated layer normalization method outperforms\nprior baseline methods while maintaining diversity, fluency, and coherence. Our\nmethod also obtains competitive performance even when using only 10% of the\navailable training data.",
          "link": "http://arxiv.org/abs/2108.07886",
          "publishedOn": "2021-08-19T01:34:59.642Z",
          "wordCount": 525,
          "title": "Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suri_H/0/1/0/all/0/1\">Huqun Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_W/0/1/0/all/0/1\">Wenhua Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Chunsheng Guan</a>",
          "description": "In this paper, we introduce MeDiaQA, a novel question answering(QA) dataset,\nwhich constructed on real online Medical Dialogues. It contains 22k\nmultiple-choice questions annotated by human for over 11k dialogues with 120k\nutterances between patients and doctors, covering 150 specialties of diseases,\nwhich are collected from haodf.com and dxy.com. MeDiaQA is the first QA dataset\nwhere reasoning over medical dialogues, especially their quantitative contents.\nThe dataset has the potential to test the computing, reasoning and\nunderstanding ability of models across multi-turn dialogues, which is\nchallenging compared with the existing datasets. To address the challenges, we\ndesign MeDia-BERT, and it achieves 64.3% accuracy, while human performance of\n93% accuracy, which indicates that there still remains a large room for\nimprovement.",
          "link": "http://arxiv.org/abs/2108.08074",
          "publishedOn": "2021-08-19T01:34:59.635Z",
          "wordCount": 560,
          "title": "MeDiaQA: A Question Answering Dataset on Medical Dialogues. (arXiv:2108.08074v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Intent detection and slot filling are two main tasks in natural language\nunderstanding (NLU) for identifying users' needs from their utterances. These\ntwo tasks are highly related and often trained jointly. However, most previous\nworks assume that each utterance only corresponds to one intent, ignoring the\nfact that a user utterance in many cases could include multiple intents. In\nthis paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for\nmulti-intent NLU. First, we formulate multiple intent detection as a weakly\nsupervised problem and approach with multiple instance learning (MIL). Then, we\ndesign an auxiliary loop via self-distillation with three orderly arranged\ndecoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The\noutput of each decoder will serve as auxiliary information for the next\ndecoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we\nset Final Slot Decoder as the teacher model that imparts knowledge back to\nInitial Slot Decoder to complete the loop. The auxiliary loop enables intents\nand slots to guide mutually in-depth and further boost the overall NLU\nperformance. Experimental results on two public multi-intent datasets indicate\nthat our model achieves strong performance compared to others.",
          "link": "http://arxiv.org/abs/2108.08042",
          "publishedOn": "2021-08-19T01:34:59.627Z",
          "wordCount": 638,
          "title": "Joint Multiple Intent Detection and Slot Filling via Self-distillation. (arXiv:2108.08042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Animesh Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>",
          "description": "In this work, we present a Web-based annotation tool `Relation Triplets\nExtractor' \\footnote{https://abera87.github.io/annotate/} (RTE) for annotating\nrelation triplets from the text. Relation extraction is an important task for\nextracting structured information about real-world entities from the\nunstructured text available on the Web. In relation extraction, we focus on\nbinary relation that refers to relations between two entities. Recently, many\nsupervised models are proposed to solve this task, but they mostly use noisy\ntraining data obtained using the distant supervision method. In many cases,\nevaluation of the models is also done based on a noisy test dataset. The lack\nof annotated clean dataset is a key challenge in this area of research. In this\nwork, we built a web-based tool where researchers can annotate datasets for\nrelation extraction on their own very easily. We use a server-less architecture\nfor this tool, and the entire annotation operation is processed using\nclient-side code. Thus it does not suffer from any network latency, and the\nprivacy of the user's data is also maintained. We hope that this tool will be\nbeneficial for the researchers to advance the field of relation extraction.",
          "link": "http://arxiv.org/abs/2108.08184",
          "publishedOn": "2021-08-19T01:34:59.618Z",
          "wordCount": 620,
          "title": "RTE: A Tool for Annotating Relation Triplets from Text. (arXiv:2108.08184v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:34:59.554Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.",
          "link": "http://arxiv.org/abs/2108.07935",
          "publishedOn": "2021-08-19T01:34:59.499Z",
          "wordCount": 682,
          "title": "Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1\">Ali H&#xfc;rriyeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanev_H/0/1/0/all/0/1\">Hristo Tanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavarella_V/0/1/0/all/0/1\">Vanni Zavarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piskorski_J/0/1/0/all/0/1\">Jakub Piskorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoruk_E/0/1/0/all/0/1\">Erdem Y&#xf6;r&#xfc;k</a>",
          "description": "This workshop is the fourth issue of a series of workshops on automatic\nextraction of socio-political events from news, organized by the Emerging\nMarket Welfare Project, with the support of the Joint Research Centre of the\nEuropean Commission and with contributions from many other prominent scholars\nin this field. The purpose of this series of workshops is to foster research\nand development of reliable, valid, robust, and practical solutions for\nautomatically detecting descriptions of socio-political events, such as\nprotests, riots, wars and armed conflicts, in text streams. This year workshop\ncontributors make use of the state-of-the-art NLP technologies, such as Deep\nLearning, Word Embeddings and Transformers and cover a wide range of topics\nfrom text classification to news bias detection. Around 40 teams have\nregistered and 15 teams contributed to three tasks that are i) multilingual\nprotest news detection, ii) fine-grained classification of socio-political\nevents, and iii) discovering Black Lives Matter protest events. The workshop\nalso highlights two keynote and four invited talks about various aspects of\ncreating event data sets and multi- and cross-lingual machine learning in few-\nand zero-shot settings.",
          "link": "http://arxiv.org/abs/2108.07865",
          "publishedOn": "2021-08-19T01:34:59.161Z",
          "wordCount": 659,
          "title": "Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report. (arXiv:2108.07865v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1\">Stephanie Schoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>",
          "description": "Text style transfer involves rewriting the content of a source sentence in a\ntarget style. Despite there being a number of style tasks with available data,\nthere has been limited systematic discussion of how text style datasets relate\nto each other. This understanding, however, is likely to have implications for\nselecting multiple data sources for model training. While it is prudent to\nconsider inherent stylistic properties when determining these relationships, we\nalso must consider how a style is realized in a particular dataset. In this\npaper, we conduct several empirical analyses of existing text style datasets.\nBased on our results, we propose a categorization of stylistic and dataset\nproperties to consider when utilizing or comparing text style datasets.",
          "link": "http://arxiv.org/abs/2108.07871",
          "publishedOn": "2021-08-19T01:34:59.124Z",
          "wordCount": 551,
          "title": "Contextualizing Variation in Text Style Transfer Datasets. (arXiv:2108.07871v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhiroop Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krook_R/0/1/0/all/0/1\">Robert Krook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_B/0/1/0/all/0/1\">Bo Joel Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheeran_M/0/1/0/all/0/1\">Mary Sheeran</a>",
          "description": "Programming microcontrollers involves low-level interfacing with hardware and\nperipherals that are concurrent and reactive. Such programs are typically\nwritten in a mixture of C and assembly using concurrent language extensions\n(like $\\texttt{FreeRTOS tasks}$ and $\\texttt{semaphores}$), resulting in\nunsafe, callback-driven, error-prone and difficult-to-maintain code.\n\nWe address this challenge by introducing $\\texttt{SenseVM}$ - a\nbytecode-interpreted virtual machine that provides a message-passing based\n$\\textit{higher-order concurrency}$ model, originally introduced by Reppy, for\nmicrocontroller programming. This model treats synchronous operations as\nfirst-class values (called $\\texttt{Events}$) akin to the treatment of\nfirst-class functions in functional languages. This primarily allows the\nprogrammer to compose and tailor their own concurrency abstractions and,\nadditionally, abstracts away unsafe memory operations, common in shared-memory\nconcurrency models, thereby making microcontroller programs safer, composable\nand easier-to-maintain.\n\nOur VM is made portable via a low-level $\\textit{bridge}$ interface, built\natop the embedded OS - Zephyr. The bridge is implemented by all drivers and\ndesigned such that programming in response to a software message or a hardware\ninterrupt remains uniform and indistinguishable. In this paper we demonstrate\nthe features of our VM through an example, written in a Caml-like functional\nlanguage, running on the $\\texttt{nRF52840}$ and $\\texttt{STM32F4}$\nmicrocontrollers.",
          "link": "http://arxiv.org/abs/2108.07805",
          "publishedOn": "2021-08-19T01:34:59.072Z",
          "wordCount": 620,
          "title": "Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Neural text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models designed\nfor short texts cannot well address the long-form text matching problem,\nbecause there are many contexts in long-form texts can not be directly aligned\nwith each other, and it is difficult for existing models to capture the key\nmatching signals from such noisy data. Besides, these models are\ncomputationally expensive for simply use all textual data indiscriminately. To\ntackle the effectiveness and efficiency problem, we propose a novel\nhierarchical noise filtering model, namely Match-Ignition. The main idea is to\nplug the well-known PageRank algorithm into the Transformer, to identify and\nfilter both sentence and word level noisy information in the matching process.\nNoisy sentences are usually easy to detect because previous work has shown that\ntheir similarity can be explicitly evaluated by the word overlapping, so we\ndirectly use PageRank to filter such information based on a sentence similarity\ngraph. Unlike sentences, words rely on their contexts to express concrete\nmeanings, so we propose to jointly learn the filtering and matching process, to\nwell capture the critical word-level matching signals. Specifically, a word\ngraph is first built based on the attention scores in each self-attention block\nof Transformer, and key words are then selected by applying PageRank on this\ngraph. In this way, noisy words will be filtered out layer by layer in the\nmatching process. Experimental results show that Match-Ignition outperforms\nboth SOTA short text matching models and recent long-form text matching models.\nWe also conduct detailed analysis to show that Match-Ignition efficiently\ncaptures important sentences and words, to facilitate the long-form text\nmatching process.",
          "link": "http://arxiv.org/abs/2101.06423",
          "publishedOn": "2021-08-18T01:54:59.339Z",
          "wordCount": 749,
          "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>",
          "description": "This report presents the results of the EENLP project, done as a part of EEML\n2021 summer school.\n\nIt presents a broad index of NLP resources for Eastern European languages,\nwhich, we hope, could be helpful for the NLP community; several new\nhand-crafted cross-lingual datasets focused on Eastern European languages, and\na sketch evaluation of cross-lingual transfer learning abilities of several\nmodern multilingual Transformer-based models.",
          "link": "http://arxiv.org/abs/2108.02605",
          "publishedOn": "2021-08-18T01:54:59.299Z",
          "wordCount": 537,
          "title": "EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Social norms -- the unspoken commonsense rules about acceptable social\nbehavior -- are crucial in understanding the underlying causes and intents of\npeople's actions in narratives. For example, underlying an action such as\n\"wanting to call cops on my neighbors\" are social norms that inform our\nconduct, such as \"It is expected that you report crimes.\"\n\nWe present Social Chemistry, a new conceptual formalism to study people's\neveryday social norms and moral judgments over a rich spectrum of real life\nsituations described in natural language. We introduce Social-Chem-101, a\nlarge-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run\na blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further\nbroken down with 12 different dimensions of people's judgments, including\nsocial judgments of good and bad, moral foundations, expected cultural\npressure, and assumed legality, which together amount to over 4.5 million\nannotations of categorical labels and free-text descriptions.\n\nComprehensive empirical results based on state-of-the-art neural models\ndemonstrate that computational modeling of social norms is a promising research\ndirection. Our model framework, Neural Norm Transformer, learns and generalizes\nSocial-Chem-101 to successfully reason about previously unseen situations,\ngenerating relevant (and potentially novel) attribute-aware social\nrules-of-thumb.",
          "link": "http://arxiv.org/abs/2011.00620",
          "publishedOn": "2021-08-18T01:54:59.278Z",
          "wordCount": 688,
          "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms. (arXiv:2011.00620v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:54:59.270Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:54:59.244Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mundotiya_R/0/1/0/all/0/1\">Rajesh Kumar Mundotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_R/0/1/0/all/0/1\">Rahul Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swasti Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anil Kumar Singh</a>",
          "description": "Corpus preparation for low-resource languages and for development of human\nlanguage technology to analyze or computationally process them is a laborious\ntask, primarily due to the unavailability of expert linguists who are native\nspeakers of these languages and also due to the time and resources required.\nBhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in\nthe north-eastern parts), are low-resource languages belonging to the\nIndo-Aryan (or Indic) family. They are closely related to Hindi, which is a\nrelatively high-resource language, which is why we compare with Hindi. We\ncollected corpora for these three languages from various sources and cleaned\nthem to the extent possible, without changing the data in them. The text\nbelongs to different domains and genres. We calculated some basic statistical\nmeasures for these corpora at character, word, syllable, and morpheme levels.\nThese corpora were also annotated with parts-of-speech (POS) and chunk tags.\nThe basic statistical measures were both absolute and relative and were\nexptected to indicate of linguistic properties such as morphological, lexical,\nphonological, and syntactic complexities (or richness). The results were\ncompared with a standard Hindi corpus. For most of the measures, we tried to\nthe corpus size the same across the languages to avoid the effect of corpus\nsize, but in some cases it turned out that using the full corpus was better,\neven if sizes were very different. Although the results are not very clear, we\ntry to draw some conclusions about the languages and the corpora. For POS\ntagging and chunking, the BIS tagset was used to manually annotate the data.\nThe POS tagged data sizes are 16067, 14669 and 12310 sentences, respectively,\nfor Bhojpuri, Magahi and Maithili. The sizes for chunking are 9695 and 1954\nsentences for Bhojpuri and Maithili, respectively.",
          "link": "http://arxiv.org/abs/2004.13945",
          "publishedOn": "2021-08-18T01:54:59.234Z",
          "wordCount": 787,
          "title": "Linguistic Resources for Bhojpuri, Magahi and Maithili: Statistics about them, their Similarity Estimates, and Baselines for Three Applications. (arXiv:2004.13945v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>",
          "description": "With the increasing trend in the topic of migration in Europe, the public is\nnow more engaged in expressing their opinions through various platforms such as\nTwitter. Understanding the online discourses is therefore essential to capture\nthe public opinion. The goal of this study is the analysis of social media\nplatform to quantify public attitudes towards migrations and the identification\nof different factors causing these attitudes. The tweets spanning from 2013 to\nJul-2021 in the European countries which are hosts to immigrants are collected,\npre-processed, and filtered using advanced topic modeling technique. BERT-based\nentity linking and sentiment analysis, and attention-based hate speech\ndetection are performed to annotate the curated tweets. Moreover, the external\ndatabases are used to identify the potential social and economic factors\ncausing negative attitudes of the people about migration. To further promote\nresearch in the interdisciplinary fields of social science and computer\nscience, the outcomes are integrated into a Knowledge Base (KB), i.e.,\nMigrationsKB which significantly extends the existing models to take into\naccount the public attitudes towards migrations and the economic indicators.\nThis KB is made public using FAIR principles, which can be queried through\nSPARQL endpoint. Data dumps are made available on Zenodo.",
          "link": "http://arxiv.org/abs/2108.07593",
          "publishedOn": "2021-08-18T01:54:59.141Z",
          "wordCount": 650,
          "title": "MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors. (arXiv:2108.07593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiber_J/0/1/0/all/0/1\">Joachim Daiber</a>",
          "description": "Progress in cross-lingual modeling depends on challenging, realistic, and\ndiverse evaluation sets. We introduce Multilingual Knowledge Questions and\nAnswers (MKQA), an open-domain question answering evaluation set comprising 10k\nquestion-answer pairs aligned across 26 typologically diverse languages (260k\nquestion-answer pairs in total). Answers are based on a heavily curated,\nlanguage-independent data representation, making results comparable across\nlanguages and independent of language-specific passages. With 26 languages,\nthis dataset supplies the widest range of languages to-date for evaluating\nquestion answering. We benchmark a variety of state-of-the-art methods and\nbaselines for generative and extractive question answering, trained on Natural\nQuestions, in zero shot and translation settings. Results indicate this dataset\nis challenging even in English, but especially in low-resource languages",
          "link": "http://arxiv.org/abs/2007.15207",
          "publishedOn": "2021-08-18T01:54:59.131Z",
          "wordCount": 580,
          "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. (arXiv:2007.15207v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1\">Mathieu Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>",
          "description": "Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.",
          "link": "http://arxiv.org/abs/2108.07708",
          "publishedOn": "2021-08-18T01:54:59.118Z",
          "wordCount": 534,
          "title": "A Game Interface to Study Semantic Grounding in Text-Based Models. (arXiv:2108.07708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>",
          "description": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.",
          "link": "http://arxiv.org/abs/2106.06132",
          "publishedOn": "2021-08-18T01:54:58.990Z",
          "wordCount": 639,
          "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1\">Diogo Cortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jefferson O. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1\">Newton Calegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Ana Lu&#xed;sa Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Ana Ang&#xe9;lica Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1\">Carolina Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1\">Gabriel Gaudencio R&#xea;go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1\">Waldir Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1\">Paulo Sergio Boggio</a>",
          "description": "Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task in NLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weak supervised corpus for fine-grained emotion in Portuguese. We\nevaluate our dataset by fine-tuning a transformer-based language model (BERT)\nand validating it on a Golden Standard annotated validation set. Our results\n(F1-score= .64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resources environment.",
          "link": "http://arxiv.org/abs/2108.07638",
          "publishedOn": "2021-08-18T01:54:58.969Z",
          "wordCount": 555,
          "title": "A Weak Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nCurrent metrics suggest that contextualized word embedding models do not\nuniformly utilize all dimensions when embedding tokens in vector space. Here we\nargue that existing metrics are fragile and tend to obfuscate the true spatial\ndistribution of point clouds. To ameliorate this issue, we propose IsoScore: a\nnovel metric which quantifies the degree to which a point cloud uniformly\nutilizes the ambient vector space. We demonstrate that IsoScore has several\ndesirable properties such as mean invariance and direct correspondence to the\nnumber of dimensions used, which are properties that existing scores do not\npossess. Furthermore, IsoScore is conceptually intuitive and computationally\nefficient, making it well suited for analyzing the distribution of point clouds\nin arbitrary vector spaces, not necessarily limited to those of word embeddings\nalone. Additionally, we use IsoScore to demonstrate that a number of recent\nconclusions in the NLP literature that have been derived using brittle metrics\nof spatial distribution, such as average cosine similarity, may be incomplete\nor altogether inaccurate.",
          "link": "http://arxiv.org/abs/2108.07344",
          "publishedOn": "2021-08-18T01:54:58.946Z",
          "wordCount": 620,
          "title": "IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan Diego Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>",
          "description": "Developing documentation guidelines and easy-to-use templates for datasets\nand models is a challenging task, especially given the variety of backgrounds,\nskills, and incentives of the people involved in the building of natural\nlanguage processing (NLP) tools. Nevertheless, the adoption of standard\ndocumentation practices across the field of NLP promotes more accessible and\ndetailed descriptions of NLP datasets and models, while supporting researchers\nand developers in reflecting on their work. To help with the standardization of\ndocumentation, we present two case studies of efforts that aim to develop\nreusable documentation templates -- the HuggingFace data card, a general\npurpose card for datasets in NLP, and the GEM benchmark data and model cards\nwith a focus on natural language generation. We describe our process for\ndeveloping these templates, including the identification of relevant\nstakeholder groups, the definition of a set of guiding principles, the use of\nexisting templates as our foundation, and iterative revisions based on\nfeedback.",
          "link": "http://arxiv.org/abs/2108.07374",
          "publishedOn": "2021-08-18T01:54:58.938Z",
          "wordCount": 652,
          "title": "Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards. (arXiv:2108.07374v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>",
          "description": "Citation recommendation is intended to assist researchers in the process of\nsearching for relevant papers to cite by recommending appropriate citations for\na given input text. Existing test collections for this task are noisy and\nunreliable since they are built automatically from parsed PDF papers. In this\npaper, we present our ongoing effort at creating a publicly available, manually\nannotated test collection for citation recommendation. We also conduct a series\nof experiments to evaluate the effectiveness of content-based baseline models\non the test collection, providing results for future work to improve upon. Our\ntest collection and code to replicate experiments are available at\nhttps://github.com/boudinfl/acm-cr",
          "link": "http://arxiv.org/abs/2108.07571",
          "publishedOn": "2021-08-18T01:54:58.918Z",
          "wordCount": 545,
          "title": "ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xintong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>",
          "description": "Many generation tasks follow a one-to-many mapping relationship: each input\ncould be associated with multiple outputs. Existing methods like Conditional\nVariational AutoEncoder(CVAE) employ a latent variable to model this\none-to-many relationship. However, this high-dimensional and dense latent\nvariable lacks explainability and usually leads to poor and uncontrollable\ngenerations. In this paper, we innovatively introduce the linguistic concept of\npattern to decompose the one-to-many mapping into multiple one-to-one mappings\nand further propose a model named Sparse Pattern Mixture of Experts(SPMoE).\nEach one-to-one mapping is associated with a conditional generation pattern and\nis modeled with an expert in SPMoE. To ensure each language pattern can be\nexclusively handled with an expert model for better explainability and\ndiversity, a sparse mechanism is employed to coordinate all the expert models\nin SPMoE. We assess the performance of our SPMoE on the paraphrase generation\ntask and the experiment results prove that SPMoE can achieve a good balance in\nterms of quality, pattern-level diversity, and corpus-level diversity.",
          "link": "http://arxiv.org/abs/2108.07535",
          "publishedOn": "2021-08-18T01:54:58.884Z",
          "wordCount": 605,
          "title": "SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Expert. (arXiv:2108.07535v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>",
          "description": "It's challenging to customize transducer-based automatic speech recognition\n(ASR) system with context information which is dynamic and unavailable during\nmodel training. In this work, we introduce a light-weight contextual spelling\ncorrection model to correct context-related recognition errors in\ntransducer-based ASR systems. We incorporate the context information into the\nspelling correction model with a shared context encoder and use a filtering\nalgorithm to handle large-size context lists. Experiments show that the model\nimproves baseline ASR model performance with about 50% relative word error rate\nreduction, which also significantly outperforms the baseline method such as\ncontextual LM biasing. The model also shows excellent performance for\nout-of-vocabulary terms not seen during training.",
          "link": "http://arxiv.org/abs/2108.07493",
          "publishedOn": "2021-08-18T01:54:58.876Z",
          "wordCount": 566,
          "title": "A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>",
          "description": "Relation linking is essential to enable question answering over knowledge\nbases. Although there are various efforts to improve relation linking\nperformance, the current state-of-the-art methods do not achieve optimal\nresults, therefore, negatively impacting the overall end-to-end question\nanswering performance. In this work, we propose a novel approach for relation\nlinking framing it as a generative problem facilitating the use of pre-trained\nsequence-to-sequence models. We extend such sequence-to-sequence models with\nthe idea of infusing structured data from the target knowledge base, primarily\nto enable these models to handle the nuances of the knowledge base. Moreover,\nwe train the model with the aim to generate a structured output consisting of a\nlist of argument-relation pairs, enabling a knowledge validation step. We\ncompared our method against the existing relation linking systems on four\ndifferent datasets derived from DBpedia and Wikidata. Our method reports large\nimprovements over the state-of-the-art while using a much simpler model that\ncan be easily adapted to different knowledge bases.",
          "link": "http://arxiv.org/abs/2108.07337",
          "publishedOn": "2021-08-18T01:54:58.866Z",
          "wordCount": 615,
          "title": "Generative Relation Linking for Question Answering over Knowledge Bases. (arXiv:2108.07337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amitoj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1\">Ilana Golbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anand Rao</a>",
          "description": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.",
          "link": "http://arxiv.org/abs/2108.07627",
          "publishedOn": "2021-08-18T01:54:58.811Z",
          "wordCount": 614,
          "title": "Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shouyi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1\">Wang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dandan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>",
          "description": "Time Delay Neural Networks (TDNN)-based methods are widely used in dialect\nidentification. However, in previous work with TDNN application, subtle variant\nis being neglected in different feature scales. To address this issue, we\npropose a new architecture, named dynamic multi-scale convolution, which\nconsists of dynamic kernel convolution, local multi-scale learning, and global\nmulti-scale pooling. Dynamic kernel convolution captures features between\nshort-term and long-term context adaptively. Local multi-scale learning, which\nrepresents multi-scale features at a granular level, is able to increase the\nrange of receptive fields for convolution operation. Besides, global\nmulti-scale pooling is applied to aggregate features from different bottleneck\nlayers in order to collect information from multiple aspects. The proposed\narchitecture significantly outperforms state-of-the-art system on the\nAP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,\nwith the best average cost performance (Cavg) of 0.067 and the best equal error\nrate (EER) of 6.52%. Compared with the known best results, our method achieves\n9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the\nparameters of proposed model are 91% fewer than the best known model.",
          "link": "http://arxiv.org/abs/2108.07787",
          "publishedOn": "2021-08-18T01:54:58.802Z",
          "wordCount": 615,
          "title": "Dynamic Multi-scale Convolution for Dialect Identification. (arXiv:2108.07787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strzyz_M/0/1/0/all/0/1\">Michalina Strzyz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>",
          "description": "Different linearizations have been proposed to cast dependency parsing as\nsequence labeling and solve the task as: (i) a head selection problem, (ii)\nfinding a representation of the token arcs as bracket strings, or (iii)\nassociating partial transition sequences of a transition-based parser to words.\nYet, there is little understanding about how these linearizations behave in\nlow-resource setups. Here, we first study their data efficiency, simulating\ndata-restricted setups from a diverse set of rich-resource treebanks. Second,\nwe test whether such differences manifest in truly low-resource setups. The\nresults show that head selection encodings are more data-efficient and perform\nbetter in an ideal (gold) framework, but that such advantage greatly vanishes\nin favour of bracketing formats when the running setup resembles a real-world\nlow-resource configuration.",
          "link": "http://arxiv.org/abs/2108.07556",
          "publishedOn": "2021-08-18T01:54:58.795Z",
          "wordCount": 565,
          "title": "Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing. (arXiv:2108.07556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Latorre_J/0/1/0/all/0/1\">Javier Latorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailleul_C/0/1/0/all/0/1\">Charlotte Bailleul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1\">Tuuli Morrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conkie_A/0/1/0/all/0/1\">Alistair Conkie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_Y/0/1/0/all/0/1\">Yannis Stylianou</a>",
          "description": "In this work, we explore multiple architectures and training procedures for\ndeveloping a multi-speaker and multi-lingual neural TTS system with the goals\nof a) improving the quality when the available data in the target language is\nlimited and b) enabling cross-lingual synthesis. We report results from a large\nexperiment using 30 speakers in 8 different languages across 15 different\nlocales. The system is trained on the same amount of data per speaker. Compared\nto a single-speaker model, when the suggested system is fine tuned to a\nspeaker, it produces significantly better quality in most of the cases while it\nonly uses less than $40\\%$ of the speaker's data used to build the\nsingle-speaker model. In cross-lingual synthesis, on average, the generated\nquality is within $80\\%$ of native single-speaker models, in terms of Mean\nOpinion Score.",
          "link": "http://arxiv.org/abs/2108.07737",
          "publishedOn": "2021-08-18T01:54:58.787Z",
          "wordCount": 600,
          "title": "Combining speakers of multiple languages to improve quality of neural voices. (arXiv:2108.07737v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Li-Hsin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastas_I/0/1/0/all/0/1\">Iiro Rastas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantsi_V/0/1/0/all/0/1\">Valtteri Skantsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilpelainen_J/0/1/0/all/0/1\">Jemina Kilpel&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupari_H/0/1/0/all/0/1\">Hanna-Mari Kupari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piirto_A/0/1/0/all/0/1\">Aurora Piirto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saarni_J/0/1/0/all/0/1\">Jenna Saarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevon_M/0/1/0/all/0/1\">Maija Sev&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarkka_O/0/1/0/all/0/1\">Otto Tarkka</a>",
          "description": "This document describes the annotation guidelines used to construct the Turku\nParaphrase Corpus. These guidelines were developed together with the corpus\nannotation, revising and extending the guidelines regularly during the\nannotation work. Our paraphrase annotation scheme uses the base scale 1-4,\nwhere labels 1 and 2 are used for negative candidates (not paraphrases), while\nlabels 3 and 4 are paraphrases at least in the given context if not everywhere.\nIn addition to base labeling, the scheme is enriched with additional\nsubcategories (flags) for categorizing different types of paraphrases inside\nthe two positive labels, making the annotation scheme suitable for more\nfine-grained paraphrase categorization. The annotation scheme is used to\nannotate over 100,000 Finnish paraphrase pairs.",
          "link": "http://arxiv.org/abs/2108.07499",
          "publishedOn": "2021-08-18T01:54:58.776Z",
          "wordCount": 570,
          "title": "Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bock_A/0/1/0/all/0/1\">A. Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palladino_A/0/1/0/all/0/1\">A. Palladino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Heisters_S/0/1/0/all/0/1\">S. Smith-Heisters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boardman_I/0/1/0/all/0/1\">I. Boardman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_E/0/1/0/all/0/1\">E. Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bienenstock_E/0/1/0/all/0/1\">E.J. Bienenstock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1\">A. Valenti</a>",
          "description": "The proliferation of news media available online simultaneously presents a\nvaluable resource and significant challenge to analysts aiming to profile and\nunderstand social and cultural trends in a geographic location of interest.\nWhile an abundance of news reports documenting significant events, trends, and\nresponses provides a more democratized picture of the social characteristics of\na location, making sense of an entire corpus to extract significant trends is a\nsteep challenge for any one analyst or team. Here, we present an approach using\nnatural language processing techniques that seeks to quantify how a set of\npre-defined topics of interest change over time across a large corpus of text.\nWe found that, given a predefined topic, we can identify and rank sets of\nterms, or n-grams, that map to those topics and have usage patterns that\ndeviate from a normal baseline. Emergence, disappearance, or significant\nvariations in n-gram usage present a ground-up picture of a topic's dynamic\nsalience within a corpus of interest.",
          "link": "http://arxiv.org/abs/2108.07345",
          "publishedOn": "2021-08-18T01:54:58.736Z",
          "wordCount": 638,
          "title": "An NLP approach to quantify dynamic salience of predefined topics in a text corpus. (arXiv:2108.07345v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.",
          "link": "http://arxiv.org/abs/2108.07435",
          "publishedOn": "2021-08-18T01:54:58.726Z",
          "wordCount": 615,
          "title": "Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_A/0/1/0/all/0/1\">Arthur Azevedo de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>",
          "description": "Kleene algebra with tests (KAT) is a foundational equational framework for\nreasoning about programs, which has found applications in program\ntransformations, networking and compiler optimizations, among many other areas.\nIn his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,\nshowing that one can reason about the (partial) correctness of while programs\nby means of the equational theory of KAT.\n\nIn this work, we investigate the support that KAT provides for reasoning\nabout \\emph{incorrectness}, instead, as embodied by Ohearn's recently proposed\nincorrectness logic. We show that KAT cannot directly express incorrectness\nlogic. The main reason for this limitation can be traced to the fact that KAT\ncannot express explicitly the notion of codomain, which is essential to express\nincorrectness triples. To address this issue, we study Kleene algebra with Top\nand Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT\nis powerful enough to express a codomain operation, to express incorrectness\ntriples, and to prove all the rules of incorrectness logic sound. This shows\nthat one can reason about the incorrectness of while-like programs by means of\nthe equational theory of TopKAT.",
          "link": "http://arxiv.org/abs/2108.07707",
          "publishedOn": "2021-08-18T01:54:58.717Z",
          "wordCount": 633,
          "title": "On Incorrectness Logic and Kleene Algebra With Top and Tests. (arXiv:2108.07707v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-08-18T01:54:58.657Z",
          "wordCount": 597,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1\">Sijie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>",
          "description": "Humans express their opinions and emotions through multiple modalities which\nmainly consist of textual, acoustic and visual modalities. Prior works on\nmultimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to\nmodel aligned multimodal sequences. However, it is unpractical to align\nmultimodal sequences due to different sample rates for different modalities.\nMoreover, RNN is prone to the issues of gradient vanishing or exploding and it\nhas limited capacity of learning long-range dependency which is the major\nobstacle to model unaligned multimodal sequences. In this paper, we introduce\nGraph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences\nwith graph-based neural model and Capsule Network. By converting sequence data\ninto graph, the previously mentioned problems of RNN are avoided. In addition,\nthe aggregation capability of Capsule Network and the graph-based structure\nenable our model to be interpretable and better solve the problem of long-range\ndependency. Experimental results suggest that GraphCAGE achieves\nstate-of-the-art performance on two benchmark datasets with representations\nrefined by Capsule Network and interpretation provided.",
          "link": "http://arxiv.org/abs/2108.07543",
          "publishedOn": "2021-08-18T01:54:58.644Z",
          "wordCount": 593,
          "title": "Graph Capsule Aggregation for Unaligned Multimodal Sequences. (arXiv:2108.07543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, the proposed conversion for language prior probabilities\nenables BERT to receive an extra 3% relative WERR, and the combination of BERT,\nGPT and GPT-2 results in further improvements.",
          "link": "http://arxiv.org/abs/2108.07789",
          "publishedOn": "2021-08-18T01:54:58.598Z",
          "wordCount": 620,
          "title": "Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.04850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Magdalena Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "The rise of personal assistants has made conversational question answering\n(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art\nmethods for ConvQA over knowledge graphs (KGs) can only learn from crisp\nquestion-answer pairs found in popular benchmarks. In reality, however, such\ntraining data is hard to come by: users would rarely mark answers explicitly as\ncorrect or wrong. In this work, we take a step towards a more natural learning\nparadigm - from noisy and implicit feedback via question reformulations. A\nreformulation is likely to be triggered by an incorrect system response,\nwhereas a new follow-up question could be a positive signal on the previous\nturn's answer. We present a reinforcement learning model, termed CONQUER, that\ncan learn from a conversational stream of questions and reformulations. CONQUER\nmodels the answering process as multiple agents walking in parallel on the KG,\nwhere the walks are determined by actions sampled using a policy network. This\npolicy network takes the question along with the conversational context as\ninputs and is trained via noisy rewards obtained from the reformulation\nlikelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark\nwith about 11k natural conversations containing around 205k reformulations.\nExperiments show that CONQUER successfully learns to answer conversational\nquestions from noisy reward signals, significantly improving over a\nstate-of-the-art baseline.",
          "link": "http://arxiv.org/abs/2105.04850",
          "publishedOn": "2021-08-23T01:36:34.398Z",
          "wordCount": 692,
          "title": "Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs. (arXiv:2105.04850v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakesh_V/0/1/0/all/0/1\">Vineeth Rakesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekhar_J/0/1/0/all/0/1\">Jaideep Chandrashekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavedhi_A/0/1/0/all/0/1\">Adithya Samavedhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>",
          "description": "Recent advancements in deep learning techniques have transformed the area of\nsemantic text matching. However, most of the state-of-the-art models are\ndesigned to operate with short documents such as tweets, user reviews,\ncomments, etc., and have fundamental limitations when applied to long-form\ndocuments such as scientific papers, legal documents, and patents. When\nhandling such long documents, there are three primary challenges: (i) The\npresence of different contexts for the same word throughout the document, (ii)\nSmall sections of contextually similar text between two documents, but\ndissimilar text in the remaining parts -- this defies the basic understanding\nof \"similarity\", and (iii) The coarse nature of a single global similarity\nmeasure which fails to capture the heterogeneity of the document content. In\nthis paper, we describe CoLDE: Contrastive Long Document Encoder -- a\ntransformer-based framework that addresses these challenges and allows for\ninterpretable comparisons of long documents. CoLDE uses unique positional\nembeddings and a multi-headed chunkwise attention layer in conjunction with a\ncontrastive learning framework to capture similarity at three different levels:\n(i) high-level similarity scores between a pair of documents, (ii) similarity\nscores between different sections within and across documents, and (iii)\nsimilarity scores between different chunks in the same document and also other\ndocuments. These fine-grained similarity scores aid in better interpretability.\nWe evaluate CoLDE on three long document datasets namely, ACL Anthology\npublications, Wikipedia articles, and USPTO patents. Besides outperforming the\nstate-of-the-art methods on the document comparison task, CoLDE also proves\ninterpretable and robust to changes in document length and text perturbations.",
          "link": "http://arxiv.org/abs/2108.09190",
          "publishedOn": "2021-08-23T01:36:34.363Z",
          "wordCount": 686,
          "title": "Supervised Contrastive Learning for Interpretable Long Document Comparison. (arXiv:2108.09190v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weicong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hanlin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingshuo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guangxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qiang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xuezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dongying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qiao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>",
          "description": "Real-world recommendation systems often consist of two phases. In the first\nphase, multiple predictive models produce the probability of different\nimmediate user actions. In the second phase, these predictions are aggregated\naccording to a set of 'strategic parameters' to meet a diverse set of business\ngoals, such as longer user engagement, higher revenue potential, or more\ncommunity/network interactions. In addition to building accurate predictive\nmodels, it is also crucial to optimize this set of 'strategic parameters' so\nthat primary goals are optimized while secondary guardrails are not hurt. In\nthis setting with multiple and constrained goals, this paper discovers that a\nprobabilistic strategic parameter regime can achieve better value compared to\nthe standard regime of finding a single deterministic parameter. The new\nprobabilistic regime is to learn the best distribution over strategic parameter\nchoices and sample one strategic parameter from the distribution when each user\nvisits the platform. To pursue the optimal probabilistic solution, we formulate\nthe problem into a stochastic compositional optimization problem, in which the\nunbiased stochastic gradient is unavailable. Our approach is applied in a\npopular social network platform with hundreds of millions of daily users and\nachieves +0.22% lift of user engagement in a recommendation task and +1.7% lift\nin revenue in an advertising optimization scenario comparing to using the best\ndeterministic parameter strategy.",
          "link": "http://arxiv.org/abs/2108.09076",
          "publishedOn": "2021-08-23T01:36:34.111Z",
          "wordCount": 693,
          "title": "PASTO: Strategic Parameter Optimization in Recommendation Systems -- Probabilistic is Better than Deterministic. (arXiv:2108.09076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1\">Nahid Esmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1\">Anoosheh Heidarzadeh</a>",
          "description": "This paper considers the problem of multi-server Private Linear Computation,\nunder the joint and individual privacy guarantees. In this problem, identical\ncopies of a dataset comprised of $K$ messages are stored on $N$ non-colluding\nservers, and a user wishes to obtain one linear combination of a $D$-subset of\nmessages belonging to the dataset. The goal is to design a scheme for\nperforming the computation such that the total amount of information downloaded\nfrom the servers is minimized, while the privacy of the $D$ messages required\nfor the computation is protected. When joint privacy is required, the\nidentities of all of these $D$ messages must be kept private jointly, and when\nindividual privacy is required, the identity of every one of these $D$ messages\nmust be kept private individually. In this work, we characterize the capacity,\nwhich is defined as the maximum achievable download rate, under both joint and\nindividual privacy requirements. In particular, we show that the capacity is\ngiven respectively by ${(1+1/N+\\dots+1/N^{K-D})^{-1}}$ or\n${(1+1/N+\\dots+1/N^{\\lceil K/D\\rceil-1})^{-1}}$ when joint or individual\nprivacy is required. Our converse proofs are based on reduction from two\nvariants of the multi-server Private Information Retrieval problem in the\npresence of side information. Our achievability schemes build up on our\nrecently proposed schemes for single-server Private Linear Transformation and\nthe multi-server private computation scheme proposed by Sun and Jafar. Using\nsimilar proof techniques, we also establish upper and lower bounds on the\ncapacity for the cases in which the user wants to compute $L$ (potentially more\nthan one) linear combinations. Specifically, we show that the capacity is upper\nand lower bounded respectively by ${(1+1/N+\\dots+1/N^{(K-D)/L})^{-1}}$ and\n${(1+1/N+\\dots+1/N^{K-D+L-1})^{-1}}$, when joint privacy is required.",
          "link": "http://arxiv.org/abs/2108.09271",
          "publishedOn": "2021-08-23T01:36:34.087Z",
          "wordCount": 714,
          "title": "Multi-Server Private Linear Computation with Joint and Individual Privacy Guarantees. (arXiv:2108.09271v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1\">David Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensmann_F/0/1/0/all/0/1\">Felix Bensmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1\">Stefan Dietze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Frank Kr&#xfc;ger</a>",
          "description": "Knowledge about software used in scientific investigations is important for\nseveral reasons, for instance, to enable an understanding of provenance and\nmethods involved in data handling. However, software is usually not formally\ncited, but rather mentioned informally within the scholarly description of the\ninvestigation, raising the need for automatic information extraction and\ndisambiguation. Given the lack of reliable ground truth data, we present\nSoMeSci (Software Mentions in Science) a gold standard knowledge graph of\nsoftware mentions in scientific articles. It contains high quality annotations\n(IRR: $\\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central\narticles. Besides the plain mention of the software, we also provide relation\nlabels for additional information, such as the version, the developer, a URL or\ncitations. Moreover, we distinguish between different types, such as\napplication, plugin or programming environment, as well as different types of\nmentions, such as usage or creation. To the best of our knowledge, SoMeSci is\nthe most comprehensive corpus about software mentions in scientific articles,\nproviding training samples for Named Entity Recognition, Relation Extraction,\nEntity Disambiguation, and Entity Linking. Finally, we sketch potential use\ncases and provide baseline results.",
          "link": "http://arxiv.org/abs/2108.09070",
          "publishedOn": "2021-08-23T01:36:33.994Z",
          "wordCount": 646,
          "title": "SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles. (arXiv:2108.09070v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Ding Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1\">Becky West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiquan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinzhou Huang</a>",
          "description": "E-commerce sites strive to provide users the most timely relevant information\nin order to reduce shopping frictions and increase customer satisfaction. Multi\narmed bandit models (MAB) as a type of adaptive optimization algorithms provide\npossible approaches for such purposes. In this paper, we analyze using three\nclassic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper\nconfidence bound 1 (UCB1) for dynamic content recommendations, and walk through\nthe process of developing these algorithms internally to solve a real world\ne-commerce use case. First, we analyze the three MAB algorithms using simulated\npurchasing datasets with non-stationary reward distributions to simulate the\npossible time-varying customer preferences, where the traffic allocation\ndynamics and the accumulative rewards of different algorithms are studied.\nSecond, we compare the accumulative rewards of the three MAB algorithms with\nmore than 1,000 trials using actual historical A/B test datasets. We find that\nthe larger difference between the success rates of competing recommendations\nthe more accumulative rewards the MAB algorithms can achieve. In addition, we\nfind that TS shows the highest average accumulative rewards under different\ntesting scenarios. Third, we develop a batch-updated MAB algorithm to overcome\nthe delayed reward issue in e-commerce and enable an online content\noptimization on our App homepage. For a state-of-the-art comparison, a real A/B\ntest among our batch-updated MAB algorithm, a third-party MAB solution, and the\ndefault business logic are conducted. The result shows that our batch-updated\nMAB algorithm outperforms the counterparts and achieves 6.13% relative\nclick-through rate (CTR) increase and 16.1% relative conversion rate (CVR)\nincrease compared to the default experience, and 2.9% relative CTR increase and\n1.4% relative CVR increase compared to the external MAB service.",
          "link": "http://arxiv.org/abs/2108.01440",
          "publishedOn": "2021-08-23T01:36:33.980Z",
          "wordCount": 736,
          "title": "Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "News recommendation is often modeled as a sequential recommendation task,\nwhich assumes that there are rich short-term dependencies over historical\nclicked news. However, in news recommendation scenarios users usually have\nstrong preferences on the temporal diversity of news information and may not\ntend to click similar news successively, which is very different from many\nsequential recommendation scenarios such as e-commerce recommendation. In this\npaper, we study whether news recommendation can be regarded as a standard\nsequential recommendation problem. Through extensive experiments on two\nreal-world datasets, we find that modeling news recommendation as a sequential\nrecommendation problem is suboptimal. To handle this challenge, we further\npropose a temporal diversity-aware news recommendation method that can promote\ncandidate news that are diverse from recently clicked news, which can help\npredict future clicks more accurately. Experiments show that our approach can\nconsistently improve various news recommendation methods.",
          "link": "http://arxiv.org/abs/2108.08984",
          "publishedOn": "2021-08-23T01:36:33.929Z",
          "wordCount": 572,
          "title": "Can We Model News Recommendation as Sequential Recommendation?. (arXiv:2108.08984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_M/0/1/0/all/0/1\">Mandana Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1\">Gregory Kiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1\">Tristan Glatard</a>",
          "description": "Scientific datasets and analysis pipelines are increasingly being shared\npublicly in the interest of open science. However, mechanisms are lacking to\nreliably identify which pipelines and datasets can appropriately be used\ntogether. Given the increasing number of high-quality public datasets and\npipelines, this lack of clear compatibility threatens the findability and\nreusability of these resources. We investigate the feasibility of a\ncollaborative filtering system to recommend pipelines and datasets based on\nprovenance records from previous executions. We evaluate our system using\ndatasets and pipelines extracted from the Canadian Open Neuroscience Platform,\na national initiative for open neuroscience. The recommendations provided by\nour system (AUC$=0.83$) are significantly better than chance and outperform\nrecommendations made by domain experts using their previous knowledge as well\nas pipeline and dataset descriptions (AUC$=0.63$). In particular, domain\nexperts often neglect low-level technical aspects of a pipeline-dataset\ninteraction, such as the level of pre-processing, which are captured by a\nprovenance-based system. We conclude that provenance-based pipeline and dataset\nrecommenders are feasible and beneficial to the sharing and usage of\nopen-science resources. Future work will focus on the collection of more\ncomprehensive provenance traces, and on deploying the system in production.",
          "link": "http://arxiv.org/abs/2108.09275",
          "publishedOn": "2021-08-23T01:36:33.898Z",
          "wordCount": 626,
          "title": "A Recommender System for Scientific Datasets and Analysis Pipelines. (arXiv:2108.09275v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1\">Oscar Perez-Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1\">Vicki Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1\">Louisa Jorm</a>",
          "description": "Electronic Medical Records contain clinical narrative text that is of great\npotential value to medical researchers. However, this information is mixed with\nPersonally Identifiable Information that presents risks to patient and\nclinician confidentiality. This paper presents an end-to-end de-identification\nframework to automatically remove PII from hospital discharge summaries. Our\ncorpus included 600 hospital discharge summaries which were extracted from the\nEMRs of two principal referral hospitals in Sydney, Australia. Our end-to-end\nde-identification framework consists of three components: 1) Annotation:\nlabelling of PII in the hospital discharge summaries using five pre-defined\ncategories: person, address, date of birth, individual identification number,\nphone/fax number; 2) Modelling: training six named entity recognition deep\nlearning base-models on balanced and imbalanced datasets; and evaluating\nensembles that combine all six base-models, the three base-models with the best\nF1 scores and the three base-models with the best recall scores respectively,\nusing token-level majority voting and stacking methods; and 3)\nDe-identification: removing PII from the hospital discharge summaries. Our\nresults showed that the ensemble model combined using the stacking Support\nVector Machine method on the three base-models with the best F1 scores achieved\nexcellent results with a F1 score of 99.16% on the test set of our corpus. We\nalso evaluated the robustness of our modelling component on the 2014 i2b2\nde-identification dataset. Our ensemble model, which uses the token-level\nmajority voting method on all six base-models, achieved the highest F1 score of\n96.24% at strict entity matching and the highest F1 score of 98.64% at binary\ntoken-level matching compared to two state-of-the-art methods. The end-to-end\nframework provides a robust solution to de-identifying clinical narrative\ncorpuses safely.",
          "link": "http://arxiv.org/abs/2101.00146",
          "publishedOn": "2021-08-23T01:36:33.877Z",
          "wordCount": 743,
          "title": "De-identifying Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models. (arXiv:2101.00146v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Luo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingqing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Recommender system plays a crucial role in modern E-commerce platform. Due to\nthe lack of historical interactions between users and items, cold-start\nrecommendation is a challenging problem. In order to alleviate the cold-start\nissue, most existing methods introduce content and contextual information as\nthe auxiliary information. Nevertheless, these methods assume the recommended\nitems behave steadily over time, while in a typical E-commerce scenario, items\ngenerally have very different performances throughout their life period. In\nsuch a situation, it would be beneficial to consider the long-term return from\nthe item perspective, which is usually ignored in conventional methods.\nReinforcement learning (RL) naturally fits such a long-term optimization\nproblem, in which the recommender could identify high potential items,\nproactively allocate more user impressions to boost their growth, therefore\nimprove the multi-period cumulative gains. Inspired by this idea, we model the\nprocess as a Partially Observable and Controllable Markov Decision Process\n(POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the\nitem lifetime values (LTV) into the recommendation. In RL-LTV, the critic\nstudies historical trajectories of items and predict the future LTV of fresh\nitem, while the actor suggests a score-based policy which maximizes the future\nLTV expectation. Scores suggested by the actor are then combined with classical\nranking scores in a dual-rank framework, therefore the recommendation is\nbalanced with the LTV consideration. Our method outperforms the strong live\nbaseline with a relative improvement of 8.67% and 18.03% on IPV and GMV of\ncold-start items, on one of the largest E-commerce platform.",
          "link": "http://arxiv.org/abs/2108.09141",
          "publishedOn": "2021-08-23T01:36:33.843Z",
          "wordCount": 696,
          "title": "Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. (arXiv:2108.09141v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Recently, the retrieval models based on dense representations have been\ngradually applied in the first stage of the document retrieval tasks, showing\nbetter performance than traditional sparse vector space models. To obtain high\nefficiency, the basic structure of these models is Bi-encoder in most cases.\nHowever, this simple structure may cause serious information loss during the\nencoding of documents since the queries are agnostic. To address this problem,\nwe design a method to mimic the queries on each of the documents by an\niterative clustering process and represent the documents by multiple pseudo\nqueries (i.e., the cluster centroids). To boost the retrieval process using\napproximate nearest neighbor search library, we also optimize the matching\nfunction with a two-step score calculation procedure. Experimental results on\nseveral popular ranking and QA datasets show that our model can achieve\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2105.03599",
          "publishedOn": "2021-08-20T01:53:49.470Z",
          "wordCount": 619,
          "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>",
          "description": "We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual\nretrieval in eleven typologically diverse languages, designed to evaluate\nranking with learned dense representations. The goal of this resource is to\nspur research in dense retrieval techniques in non-English languages, motivated\nby recent observations that existing techniques for representation learning\nperform poorly when applied to out-of-distribution data. As a starting point,\nwe provide zero-shot baselines for this new dataset based on a multi-lingual\nadaptation of DPR that we call \"mDPR\". Experiments show that although the\neffectiveness of mDPR is much lower than BM25, dense representations\nnevertheless appear to provide valuable relevance signals, improving BM25\nresults in sparse-dense hybrids. In addition to analyses of our results, we\nalso discuss future challenges and present a research agenda in multi-lingual\ndense retrieval. Mr. TyDi can be downloaded at\nhttps://github.com/castorini/mr.tydi.",
          "link": "http://arxiv.org/abs/2108.08787",
          "publishedOn": "2021-08-20T01:53:49.443Z",
          "wordCount": 574,
          "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Changwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1\">Kyeong-Joong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>",
          "description": "In recent years, many recommender systems using network embedding (NE) such\nas graph neural networks (GNNs) have been extensively studied in the sense of\nimproving recommendation accuracy. However, such attempts have focused mostly\non utilizing only the information of positive user-item interactions with high\nratings. Thus, there is a challenge on how to make use of low rating scores for\nrepresenting users' preferences since low ratings can be still informative in\ndesigning NE-based recommender systems. In this study, we present SiReN, a new\nsign-aware recommender system based on GNN models. Specifically, SiReN has\nthree key components: 1) constructing a signed bipartite graph for more\nprecisely representing users' preferences, which is split into two\nedge-disjoint graphs with positive and negative edges each, 2) generating two\nembeddings for the partitioned graphs with positive and negative edges via a\nGNN model and a multi-layer perceptron (MLP), respectively, and then using an\nattention model to obtain the final embeddings, and 3) establishing a\nsign-aware Bayesian personalized ranking (BPR) loss function in the process of\noptimization. Through comprehensive experiments, we empirically demonstrate\nthat SiReN consistently outperforms state-of-the-art NE-aided recommendation\nmethods.",
          "link": "http://arxiv.org/abs/2108.08735",
          "publishedOn": "2021-08-20T01:53:49.411Z",
          "wordCount": 640,
          "title": "SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>",
          "description": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
          "link": "http://arxiv.org/abs/2010.06467",
          "publishedOn": "2021-08-20T01:53:49.398Z",
          "wordCount": 751,
          "title": "Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.",
          "link": "http://arxiv.org/abs/2108.08614",
          "publishedOn": "2021-08-20T01:53:49.380Z",
          "wordCount": 654,
          "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vardasbi_A/0/1/0/all/0/1\">Ali Vardasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Ilya Markov</a>",
          "description": "In counterfactual learning to rank (CLTR) user interactions are used as a\nsource of supervision. Since user interactions come with bias, an important\nfocus of research in this field lies in developing methods to correct for the\nbias of interactions. Inverse propensity scoring (IPS) is a popular method\nsuitable for correcting position bias. Affine correction (AC) is a\ngeneralization of IPS that corrects for position bias and trust bias. IPS and\nAC provably remove bias, conditioned on an accurate estimation of the bias\nparameters. Estimating the bias parameters, in turn, requires an accurate\nestimation of the relevance probabilities. This cyclic dependency introduces\npractical limitations in terms of sensitivity, convergence and efficiency.\n\nWe propose a new correction method for position and trust bias in CLTR in\nwhich, unlike the existing methods, the correction does not rely on relevance\nestimation. Our proposed method, mixture-based correction (MBC), is based on\nthe assumption that the distribution of the CTRs over the items being ranked is\na mixture of two distributions: the distribution of CTRs for relevant items and\nthe distribution of CTRs for non-relevant items. We prove that our method is\nunbiased. The validity of our proof is not conditioned on accurate bias\nparameter estimation. Our experiments show that MBC, when used in different\nbias settings and accompanied by different LTR algorithms, outperforms AC, the\nstate-of-the-art method for correcting position and trust bias, in some\nsettings, while performing on par in other settings. Furthermore, MBC is orders\nof magnitude more efficient than AC in terms of the training time.",
          "link": "http://arxiv.org/abs/2108.08538",
          "publishedOn": "2021-08-20T01:53:49.360Z",
          "wordCount": 697,
          "title": "Mixture-Based Correction for Position and Trust Bias in Counterfactual Learning to Rank. (arXiv:2108.08538v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:49.335Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique is to apply named entity disambiguation (NED)\nsystems to the question, and retrieve KB facts for the disambiguated entities.\nThis work presents ECQA, an efficient method that prunes irrelevant parts of\nthe search space using KB-aware signals. ECQA is based on top-k query\nprocessing over score-ordered lists of KB items that combine signals about\nlexical matching, relevance to the question, coherence among candidate items,\nand connectivity in the KB graph. Experiments with two recent QA benchmarks\ndemonstrate the superiority of ECQA over state-of-the-art baselines with\nrespect to answer presence, size of the search space, and runtimes.",
          "link": "http://arxiv.org/abs/2108.08597",
          "publishedOn": "2021-08-20T01:53:49.296Z",
          "wordCount": 590,
          "title": "Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>",
          "description": "BERT-based information retrieval models are expensive, in both time (query\nlatency) and computational resources (energy, hardware cost), making many of\nthese models impractical especially under resource constraints. The reliance on\na query encoder that only performs tokenization and on the pre-processing of\npassage representations at indexing, has allowed the recently proposed TILDE\nmethod to overcome the high query latency issue typical of BERT-based models.\nThis however is at the expense of a lower effectiveness compared to other\nBERT-based re-rankers and dense retrievers. In addition, the original TILDE\nmethod is characterised by indexes with a very high memory footprint, as it\nexpands each passage into the size of the BERT vocabulary. In this paper, we\npropose TILDEv2, a new model that stems from the original TILDE but that\naddresses its limitations. TILDEv2 relies on contextualized exact term matching\nwith expanded passages. This requires to only store in the index the score of\ntokens that appear in the expanded passages (rather than all the vocabulary),\nthus producing indexes that are 99% smaller than those of TILDE. This matching\nmechanism also improves ranking effectiveness by 24%, without adding to the\nquery latency. This makes TILDEv2 the state-of-the-art passage re-ranking\nmethod for CPU-only environments, capable of maintaining query latency below\n100ms on commodity hardware.",
          "link": "http://arxiv.org/abs/2108.08513",
          "publishedOn": "2021-08-20T01:53:49.239Z",
          "wordCount": 646,
          "title": "Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion. (arXiv:2108.08513v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-19T01:34:59.170Z",
          "wordCount": 668,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:34:59.148Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>",
          "description": "Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.",
          "link": "http://arxiv.org/abs/2108.08226",
          "publishedOn": "2021-08-19T01:34:59.096Z",
          "wordCount": 700,
          "title": "TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rusnak_L/0/1/0/all/0/1\">Lucas Rusnak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesic_J/0/1/0/all/0/1\">Jelena Te&#x161;i&#x107;</a>",
          "description": "Attitudinal Network Graphs are signed graphs where edges capture an expressed\nopinion; two vertices connected by an edge can be agreeable (positive) or\nantagonistic (negative). A signed graph is called balanced if each of its\ncycles includes an even number of negative edges. Balance is often\ncharacterized by the frustration index or by finding a single convergent\nbalanced state of network consensus. In this paper, we propose to expand the\nmeasures of consensus from a single balanced state associated with the\nfrustration index to the set of nearest balanced states. We introduce the\nfrustration cloud as a set of all nearest balanced states and use a\ngraph-balancing algorithm to find all nearest balanced states in a\ndeterministic way. Computational concerns are addressed by measuring consensus\nprobabilistically, and we introduce new vertex and edge metrics to quantify\nstatus, agreement, and influence. We also introduce a new global measure of\ncontroversy for a given signed graph and show that vertex status is a zero-sum\ngame in the signed network. We propose an efficient scalable algorithm for\ncalculating frustration cloud-based measures in social network and survey data\nof up to 80,000 vertices and half-a-million edges. We also demonstrate the\npower of the proposed approach to provide discriminant features for community\ndiscovery when compared to spectral clustering and to automatically identify\ndominant vertices and anomalous decisions in the network.",
          "link": "http://arxiv.org/abs/2009.07776",
          "publishedOn": "2021-08-19T01:34:59.058Z",
          "wordCount": 706,
          "title": "Characterizing Attitudinal Network Graphs through Frustration Cloud. (arXiv:2009.07776v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badita_A/0/1/0/all/0/1\">Ajay Badita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jinan_R/0/1/0/all/0/1\">Rooji Jinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamanan_B/0/1/0/all/0/1\">Balajee Vamanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parag_P/0/1/0/all/0/1\">Parimal Parag</a>",
          "description": "We consider energy minimization for data-intensive applications run on large\nnumber of servers, for given performance guarantees. We consider a system,\nwhere each incoming application is sent to a set of servers, and is considered\nto be completed if a subset of them finish serving it. We consider a simple\ncase when each server core has two speed levels, where the higher speed can be\nachieved by higher power for each core independently. The core selects one of\nthe two speeds probabilistically for each incoming application request. We\nmodel arrival of application requests by a Poisson process, and random service\ntime at the server with independent exponential random variables. Our model and\nanalysis generalizes to today's state-of-the-art in CPU energy management where\neach core can independently select a speed level from a set of supported speeds\nand corresponding voltages. The performance metrics under consideration are the\nmean number of applications in the system and the average energy expenditure.\nWe first provide a tight approximation to study this previously intractable\nproblem and derive closed form approximate expressions for the performance\nmetrics when service times are exponentially distributed. Next, we study the\ntrade-off between the approximate mean number of applications and energy\nexpenditure in terms of the switching probability.",
          "link": "http://arxiv.org/abs/2108.08199",
          "publishedOn": "2021-08-19T01:34:59.025Z",
          "wordCount": 648,
          "title": "Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. (arXiv:2108.08199v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>",
          "description": "In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.",
          "link": "http://arxiv.org/abs/2108.07935",
          "publishedOn": "2021-08-19T01:34:58.680Z",
          "wordCount": 682,
          "title": "Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanfeng Liu</a>",
          "description": "Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have\nbeen proposed to improve the recommendation accuracy in a target dataset\n(domain/system) with the help of a source one with relatively richer\ninformation. However, most existing CDR and CSR approaches are single-target,\nnamely, there is a single target dataset, which can only help the target\ndataset and thus cannot benefit the source dataset. In this paper, we focus on\nthree new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),\nand CDR+CSR, and aim to improve the recommendation accuracy in all datasets\nsimultaneously for all scenarios. To do this, we propose a unified framework,\ncalled GA (based on Graph embedding and Attention techniques), for all three\nscenarios. In GA, we first construct separate heterogeneous graphs to generate\nmore representative user and item embeddings. Then, we propose an element-wise\nattention mechanism to effectively combine the embeddings of common entities\n(users/items) learned from different datasets. Moreover, to avoid negative\ntransfer, we further propose a Personalized training strategy to minimize the\nembedding difference of common entities between a richer dataset and a sparser\ndataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and\nGA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments\nconducted on four real-world datasets demonstrate that our proposed GA models\nsignificantly outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.07976",
          "publishedOn": "2021-08-19T01:34:58.612Z",
          "wordCount": 676,
          "title": "A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianhui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>",
          "description": "Recent studies in recommender systems have managed to achieve significantly\nimproved performance by leveraging reviews for rating prediction. However,\ndespite being extensively studied, these methods still suffer from some\nlimitations. First, previous studies either encode the document or extract\nlatent sentiment via neural networks, which are difficult to interpret the\nsentiment of reviewers intuitively. Second, they neglect the personalized\ninteraction of reviews with user/item, i.e., each review has different\ncontributions when modeling the sentiment preference of user/item. To remedy\nthese issues, we propose a Sentiment-aware Interactive Fusion Network (SIFN)\nfor review-based item recommendation. Specifically, we first encode user/item\nreviews via BERT and propose a light-weighted sentiment learner to extract\nsemantic features of each review. Then, we propose a sentiment prediction task\nthat guides the sentiment learner to extract sentiment-aware features via\nexplicit sentiment labels. Finally, we design a rating prediction task that\ncontains a rating learner with an interactive and fusion module to fuse the\nidentity (i.e., user and item ID) and each review representation so that\nvarious interactive features can synergistically influence the final rating\nscore. Experimental results on five real-world datasets demonstrate that the\nproposed model is superior to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.08022",
          "publishedOn": "2021-08-19T01:34:58.596Z",
          "wordCount": 643,
          "title": "SIFN: A Sentiment-aware Interactive Fusion Network for Review-based Item Recommendation. (arXiv:2108.08022v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Neural text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models designed\nfor short texts cannot well address the long-form text matching problem,\nbecause there are many contexts in long-form texts can not be directly aligned\nwith each other, and it is difficult for existing models to capture the key\nmatching signals from such noisy data. Besides, these models are\ncomputationally expensive for simply use all textual data indiscriminately. To\ntackle the effectiveness and efficiency problem, we propose a novel\nhierarchical noise filtering model, namely Match-Ignition. The main idea is to\nplug the well-known PageRank algorithm into the Transformer, to identify and\nfilter both sentence and word level noisy information in the matching process.\nNoisy sentences are usually easy to detect because previous work has shown that\ntheir similarity can be explicitly evaluated by the word overlapping, so we\ndirectly use PageRank to filter such information based on a sentence similarity\ngraph. Unlike sentences, words rely on their contexts to express concrete\nmeanings, so we propose to jointly learn the filtering and matching process, to\nwell capture the critical word-level matching signals. Specifically, a word\ngraph is first built based on the attention scores in each self-attention block\nof Transformer, and key words are then selected by applying PageRank on this\ngraph. In this way, noisy words will be filtered out layer by layer in the\nmatching process. Experimental results show that Match-Ignition outperforms\nboth SOTA short text matching models and recent long-form text matching models.\nWe also conduct detailed analysis to show that Match-Ignition efficiently\ncaptures important sentences and words, to facilitate the long-form text\nmatching process.",
          "link": "http://arxiv.org/abs/2101.06423",
          "publishedOn": "2021-08-18T01:54:58.691Z",
          "wordCount": 749,
          "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Feng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>",
          "description": "Click-Through Rate (CTR) prediction, whose aim is to predict the probability\nof whether a user will click on an item, is an essential task for many online\napplications. Due to the nature of data sparsity and high dimensionality of CTR\nprediction, a key to making effective prediction is to model high-order feature\ninteraction. An efficient way to do this is to perform inner product of feature\nembeddings with self-attentive neural networks. To better model complex feature\ninteraction, in this paper we propose a novel DisentanglEd Self-atTentIve\nNEtwork (DESTINE) framework for CTR prediction that explicitly decouples the\ncomputation of unary feature importance from pairwise interaction.\nSpecifically, the unary term models the general importance of one feature on\nall other features, whereas the pairwise interaction term contributes to\nlearning the pure impact for each feature pair. We conduct extensive\nexperiments using two real-world benchmark datasets. The results show that\nDESTINE not only maintains computational efficiency but achieves consistent\nimprovements over state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2101.03654",
          "publishedOn": "2021-08-18T01:54:58.574Z",
          "wordCount": 640,
          "title": "Disentangled Self-Attentive Neural Networks for Click-Through Rate Prediction. (arXiv:2101.03654v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:54:58.386Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Saurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayant_R/0/1/0/all/0/1\">Rushil Jayant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charagulla_N/0/1/0/all/0/1\">Nihaar Charagulla</a>",
          "description": "The popularization of the internet created a revitalized digital media. With\nmonetization driven by clicks, journalists have reprioritized their content for\nthe highly competitive atmosphere of online news. The resulting negativity bias\nis harmful and can lead to anxiety and mood disturbance. We utilized a pipeline\nof 4 sentiment analysis models trained on various datasets - using Sequential,\nLSTM, BERT, and SVM models. When combined, the application, a mobile app,\nsolely displays uplifting and inspiring stories for users to read. Results have\nbeen successful - 1,300 users rate the app at 4.9 stars, and 85% report\nimproved mental health by using it.",
          "link": "http://arxiv.org/abs/2108.07706",
          "publishedOn": "2021-08-18T01:54:58.353Z",
          "wordCount": 546,
          "title": "Sentiment Analysis on the News to Improve Mental Health. (arXiv:2108.07706v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongyoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Successful sequential recommendation systems rely on accurately capturing the\nuser's short-term and long-term interest. Although Transformer-based models\nachieved state-of-the-art performance in the sequential recommendation task,\nthey generally require quadratic memory and time complexity to the sequence\nlength, making it difficult to extract the long-term interest of users. On the\nother hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their\nlinear memory and time complexity, have recently shown competitive results\ncompared to Transformer in various tasks. Given the availability of a massive\namount of the user's behavior history, the linear memory and time complexity of\nMLP-based models make them a promising alternative to explore in the sequential\nrecommendation task. To this end, we adopted MLP-based models in sequential\nrecommendation but consistently observed that MLP-based methods obtain lower\nperformance than those of Transformer despite their computational benefits.\nFrom experiments, we observed that introducing explicit high-order interactions\nto MLP layers mitigates such performance gap. In response, we propose the\nMulti-Order Interaction (MOI) layer, which is capable of expressing an\narbitrary order of interactions within the inputs while maintaining the memory\nand time complexity of the MLP layer. By replacing the MLP layer with the MOI\nlayer, our model was able to achieve comparable performance with\nTransformer-based models while retaining the MLP-based models' computational\nbenefits.",
          "link": "http://arxiv.org/abs/2108.07505",
          "publishedOn": "2021-08-18T01:54:58.337Z",
          "wordCount": 658,
          "title": "MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongji Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caihua Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1\">Khaled B. Letaief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Graph convolutional networks (GCNs) have recently enabled a popular class of\nalgorithms for collaborative filtering (CF). Nevertheless, the theoretical\nunderpinnings of their empirical successes remain elusive. In this paper, we\nendeavor to obtain a better understanding of GCN-based CF methods via the lens\nof graph signal processing. By identifying the critical role of smoothness, a\nkey concept in graph signal processing, we develop a unified graph\nconvolution-based framework for CF. We prove that many existing CF methods are\nspecial cases of this framework, including the neighborhood-based methods,\nlow-rank matrix factorization, linear auto-encoders, and LightGCN,\ncorresponding to different low-pass filters. Based on our framework, we then\npresent a simple and computationally efficient CF baseline, which we shall\nrefer to as Graph Filter based Collaborative Filtering (GF-CF). Given an\nimplicit feedback matrix, GF-CF can be obtained in a closed form instead of\nexpensive training with back-propagation. Experiments will show that GF-CF\nachieves competitive or better performance against deep learning-based methods\non three well-known datasets, notably with a $70\\%$ performance gain over\nLightGCN on the Amazon-book dataset.",
          "link": "http://arxiv.org/abs/2108.07567",
          "publishedOn": "2021-08-18T01:54:58.324Z",
          "wordCount": 619,
          "title": "How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangkun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "Personalization lies at the core of boosting the product search system\nperformance. Prior studies mainly resorted to the semantic matching between\ntextual queries and user/product related documents, leaving the user\ncollaborative behaviors untapped. In fact, the collaborative filtering signals\nbetween users intuitively offer a complementary information for the semantic\nmatching. To close the gap between collaborative filtering and product search,\nwe propose a Hierarchical Heterogeneous Graph Neural Network (HHGNN) approach\nin this paper. Specifically, we organize HHGNN with a hierarchical graph\nstructure according to the three edge types. The sequence edge accounts for the\nsyntax formulation from word nodes to sentence nodes; the composition edge\naggregates the semantic features to the user and product nodes; and the\ninteraction edge on the top performs graph convolutional operation between user\nand product nodes. At last, we integrate the higher-order neighboring\ncollaborative features and the semantic features for better representation\nlearning. We conduct extensive experiments on six Amazon review datasets. The\nresults show that our proposed method can outperform the state-of-the-art\nbaselines with a large margin. In addition, we empirically prove that\ncollaborative filtering and semantic matching are complementary to each other\nin product search performance enhancement.",
          "link": "http://arxiv.org/abs/2108.07574",
          "publishedOn": "2021-08-18T01:54:58.296Z",
          "wordCount": 634,
          "title": "When Product Search Meets Collaborative Filtering: A Hierarchical Heterogeneous Graph Neural Network Approach. (arXiv:2108.07574v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Eliana Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1\">Luca de Alfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1\">Elena Baralis</a>",
          "description": "When analyzing the behavior of machine learning algorithms, it is important\nto identify specific data subgroups for which the considered algorithm shows\ndifferent performance with respect to the entire dataset. The intervention of\ndomain experts is normally required to identify relevant attributes that define\nthese subgroups.\n\nWe introduce the notion of divergence to measure this performance difference\nand we exploit it in the context of (i) classification models and (ii) ranking\napplications to automatically detect data subgroups showing a significant\ndeviation in their behavior. Furthermore, we quantify the contribution of all\nattributes in the data subgroup to the divergent behavior by means of Shapley\nvalues, thus allowing the identification of the most impacting attributes.",
          "link": "http://arxiv.org/abs/2108.07450",
          "publishedOn": "2021-08-18T01:54:58.270Z",
          "wordCount": 564,
          "title": "Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>",
          "description": "Citation recommendation is intended to assist researchers in the process of\nsearching for relevant papers to cite by recommending appropriate citations for\na given input text. Existing test collections for this task are noisy and\nunreliable since they are built automatically from parsed PDF papers. In this\npaper, we present our ongoing effort at creating a publicly available, manually\nannotated test collection for citation recommendation. We also conduct a series\nof experiments to evaluate the effectiveness of content-based baseline models\non the test collection, providing results for future work to improve upon. Our\ntest collection and code to replicate experiments are available at\nhttps://github.com/boudinfl/acm-cr",
          "link": "http://arxiv.org/abs/2108.07571",
          "publishedOn": "2021-08-18T01:54:58.253Z",
          "wordCount": 545,
          "title": "ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amitoj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1\">Ilana Golbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anand Rao</a>",
          "description": "An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.",
          "link": "http://arxiv.org/abs/2108.07627",
          "publishedOn": "2021-08-18T01:54:58.209Z",
          "wordCount": 614,
          "title": "Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.00981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_L/0/1/0/all/0/1\">Lovish Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sarthak Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Abhijit Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Sandip Chakraborty</a>",
          "description": "With increasing advancements in technologies for capturing 360{\\deg} videos,\nadvances in streaming such videos have become a popular research topic.\nHowever, streaming 360{\\deg} videos require high bandwidth, thus escalating the\nneed for developing optimized streaming algorithms. Researchers have proposed\nvarious methods to tackle the problem, considering the network bandwidth or\nattempt to predict future viewports in advance. However, most of the existing\nworks either (1) do not consider video contents to predict user viewport, or\n(2) do not adapt to user preferences dynamically, or (3) require a lot of\ntraining data for new videos, thus making them potentially unfit for video\nstreaming purposes. We develop PARIMA, a fast and efficient online viewport\nprediction model that uses past viewports of users along with the trajectories\nof prime objects as a representative of video content to predict future\nviewports. We claim that the head movement of a user majorly depends upon the\ntrajectories of the prime objects in the video. We employ a pyramid-based\nbitrate allocation scheme and perform a comprehensive evaluation of the\nperformance of PARIMA. In our evaluation, we show that PARIMA outperforms\nstate-of-the-art approaches, improving the Quality of Experience by over 30\\%\nwhile maintaining a short response time.",
          "link": "http://arxiv.org/abs/2103.00981",
          "publishedOn": "2021-08-23T01:36:34.065Z",
          "wordCount": 674,
          "title": "PARIMA: Viewport Adaptive 360-Degree Video Streaming. (arXiv:2103.00981v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs. Code is available at\nhttps://github.com/yellowtownhz/STIGCN.",
          "link": "http://arxiv.org/abs/2011.13322",
          "publishedOn": "2021-08-23T01:36:33.538Z",
          "wordCount": 710,
          "title": "Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haihan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Sizheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhonghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wei Cai</a>",
          "description": "In recent years, the metaverse has attracted enormous attention from around\nthe world with the development of related technologies. The expected metaverse\nshould be a realistic society with more direct and physical interactions, while\nthe concepts of race, gender, and even physical disability would be weakened,\nwhich would be highly beneficial for society. However, the development of\nmetaverse is still in its infancy, with great potential for improvement.\nRegarding metaverse's huge potential, industry has already come forward with\nadvance preparation, accompanied by feverish investment, but there are few\ndiscussions about metaverse in academia to scientifically guide its\ndevelopment. In this paper, we highlight the representative applications for\nsocial good. Then we propose a three-layer metaverse architecture from a macro\nperspective, containing infrastructure, interaction, and ecosystem. Moreover,\nwe journey toward both a historical and novel metaverse with a detailed\ntimeline and table of specific attributes. Lastly, we illustrate our\nimplemented blockchain-driven metaverse prototype of a university campus and\ndiscuss the prototype design and insights.",
          "link": "http://arxiv.org/abs/2108.08985",
          "publishedOn": "2021-08-23T01:36:33.460Z",
          "wordCount": 620,
          "title": "Metaverse for Social Good: A University Campus Prototype. (arXiv:2108.08985v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1\">Stanislava Rangelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1\">Simon Flutura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "Virtual Reality (VR) games that feature physical activities have been shown\nto increase players' motivation to do physical exercise. However, for such\nexercises to have a positive healthcare effect, they have to be repeated\nseveral times a week. To maintain player motivation over longer periods of\ntime, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the\ngame's challenge according to the player's capabilities. For exercise games,\nthis is mostly done by tuning specific in-game parameters like the speed of\nobjects. In this work, we propose to use experience-driven Procedural Content\nGeneration for DDA in VR exercise games by procedurally generating levels that\nmatch the player's current capabilities. Not only finetuning specific\nparameters but creating completely new levels has the potential to decrease\nrepetition over longer time periods and allows for the simultaneous adaptation\nof the cognitive and physical challenge of the exergame. As a proof-of-concept,\nwe implement an initial prototype in which the player must traverse a maze that\nincludes several exercise rooms, whereby the generation of the maze is realized\nby a neural network. Passing those exercise rooms requires the player to\nperform physical activities. To match the player's capabilities, we use Deep\nReinforcement Learning to adjust the structure of the maze and to decide which\nexercise rooms to include in the maze. We evaluate our prototype in an\nexploratory user study utilizing both biodata and subjective questionnaires.",
          "link": "http://arxiv.org/abs/2108.08762",
          "publishedOn": "2021-08-20T01:53:50.120Z",
          "wordCount": 681,
          "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1\">Neel Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1\">Anirudh Thatipelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "The lack of fine-grained joints (facial joints, hand fingers) is a\nfundamental performance bottleneck for state of the art skeleton action\nrecognition models. Despite this bottleneck, community's efforts seem to be\ninvested only in coming up with novel architectures. To specifically address\nthis bottleneck, we introduce two new pose based human action datasets -\nNTU60-X and NTU120-X. Our datasets extend the largest existing action\nrecognition dataset, NTU-RGBD. In addition to the 25 body joints for each\nskeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and\nfacial joints, enabling a richer skeleton representation. We appropriately\nmodify the state of the art approaches to enable training using the introduced\ndatasets. Our results demonstrate the effectiveness of these NTU-X datasets in\novercoming the aforementioned bottleneck and improve state of the art\nperformance, overall and on previously worst performing action categories.",
          "link": "http://arxiv.org/abs/2101.11529",
          "publishedOn": "2021-08-20T01:53:50.100Z",
          "wordCount": 643,
          "title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1\">Sravya Vardhani Shivapuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1\">Mansi Pradeep Khamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1\">Divij Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "Datasets for training crowd counting deep networks are typically heavy-tailed\nin count distribution and exhibit discontinuities across the count range. As a\nresult, the de facto statistical measures (MSE, MAE) exhibit large variance and\ntend to be unreliable indicators of performance across the count range. To\naddress these concerns in a holistic manner, we revise processes at various\nstages of the standard crowd counting pipeline. To enable principled and\nbalanced minibatch sampling, we propose a novel smoothed Bayesian sample\nstratification approach. We propose a novel cost function which can be readily\nincorporated into existing crowd counting deep networks to encourage\nstrata-aware optimization. We analyze the performance of representative crowd\ncounting approaches across standard datasets at per strata level and in\naggregate. We analyze the performance of crowd counting approaches across\nstandard datasets and demonstrate that our proposed modifications noticeably\nreduce error standard deviation. Our contributions represent a nuanced,\nstatistically balanced and fine-grained characterization of performance for\ncrowd counting approaches. Code, pretrained models and interactive\nvisualizations can be viewed at our project page https://deepcount.iiit.ac.in/",
          "link": "http://arxiv.org/abs/2108.08784",
          "publishedOn": "2021-08-20T01:53:50.079Z",
          "wordCount": 652,
          "title": "Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data are available at https://github.com/skelemoa/tal-hmo.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-08-20T01:53:50.013Z",
          "wordCount": 631,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>",
          "description": "For a given video-based Human-Object Interaction scene, modeling the\nspatio-temporal relationship between humans and objects are the important cue\nto understand the contextual information presented in the video. With the\neffective spatio-temporal relationship modeling, it is possible not only to\nuncover contextual information in each frame but also to directly capture\ninter-time dependencies. It is more critical to capture the position changes of\nhuman and objects over the spatio-temporal dimension when their appearance\nfeatures may not show up significant changes over time. The full use of\nappearance features, the spatial location and the semantic information are also\nthe key to improve the video-based Human-Object Interaction recognition\nperformance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks\n(STIGPN) are constructed, which encode the videos with a graph composed of\nhuman and object nodes. These nodes are connected by two types of relations:\n(i) spatial relations modeling the interactions between human and the\ninteracted objects within each frame. (ii) inter-time relations capturing the\nlong range dependencies between human and the interacted objects across frame.\nWith the graph, STIGPN learn spatio-temporal features directly from the whole\nvideo-based Human-Object Interaction scenes. Multi-modal features and a\nmulti-stream fusion strategy are used to enhance the reasoning capability of\nSTIGPN. Two Human-Object Interaction video datasets, including CAD-120 and\nSomething-Else, are used to evaluate the proposed architectures, and the\nstate-of-the-art performance demonstrates the superiority of STIGPN.",
          "link": "http://arxiv.org/abs/2108.08633",
          "publishedOn": "2021-08-20T01:53:49.832Z",
          "wordCount": 677,
          "title": "Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1\">Jiro Katto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>",
          "description": "In this paper, we propose a learned video codec with a residual prediction\nnetwork (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we\nexploit the residual of previous multiple frames to further eliminate the\nredundancy of the current frame residual. For the LF-Net, the features from\nresidual decoding network and the motion compensation network are used to aid\nthe reconstruction quality. To reduce the complexity, a light ResNet structure\nis used as the backbone for both RP-Net and LF-Net. Experimental results\nillustrate that we can save about 10% BD-rate compared with previous learned\nvideo compression frameworks. Moreover, we can achieve faster coding speed due\nto the ResNet backbone. This project is available at\nhttps://github.com/chaoliu18/RPLVC.",
          "link": "http://arxiv.org/abs/2108.08551",
          "publishedOn": "2021-08-20T01:53:49.822Z",
          "wordCount": 570,
          "title": "Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:49.791Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>",
          "description": "Federated Learning (FL) is a privacy-protected machine learning paradigm that\nallows model to be trained directly at the edge without uploading data. One of\nthe biggest challenges faced by FL in practical applications is the\nheterogeneity of edge node data, which will slow down the convergence speed and\ndegrade the performance of the model. For the above problems, a representative\nsolution is to add additional constraints in the local training, such as\nFedProx, FedCurv and FedCL. However, the above algorithms still have room for\nimprovement. We propose to use the aggregation of all models obtained in the\npast as new constraint target to further improve the performance of such\nalgorithms. Experiments in various settings demonstrate that our method\nsignificantly improves the convergence speed and performance of the model.",
          "link": "http://arxiv.org/abs/2108.08577",
          "publishedOn": "2021-08-20T01:53:49.781Z",
          "wordCount": 563,
          "title": "Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gong_X/0/1/0/all/0/1\">Xia Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuxiang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Haidi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>",
          "description": "Musical instruments recognition is a widely used application for music\ninformation retrieval. As most of previous musical instruments recognition\ndataset focus on western musical instruments, it is difficult for researcher to\nstudy and evaluate the area of traditional Chinese musical instrument\nrecognition. This paper propose a traditional Chinese music dataset for\ntraining model and performance evaluation, named ChMusic. This dataset is free\nand publicly available, 11 traditional Chinese musical instruments and 55\ntraditional Chinese music excerpts are recorded in this dataset. Then an\nevaluation standard is proposed based on ChMusic dataset. With this standard,\nresearchers can compare their results following the same rule, and results from\ndifferent researchers will become comparable.",
          "link": "http://arxiv.org/abs/2108.08470",
          "publishedOn": "2021-08-20T01:53:49.431Z",
          "wordCount": 566,
          "title": "ChMusic: A Traditional Chinese Music Dataset for Evaluation of Instrument Recognition. (arXiv:2108.08470v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:34:58.952Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhou Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qihang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohno_S/0/1/0/all/0/1\">Satoru Ohno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1\">Pujana Paliyawan</a>",
          "description": "This paper presents a commentator for providing real-time game commentary in\na fighting game. The commentary takes into account highlight cues, obtained by\nanalyzing scenes during gameplay, as input to adjust the pitch and loudness of\ncommentary to be spoken by using a Text-to-Speech (TTS) technology. We\ninvestigate different designs for pitch and loudness adjustment. The proposed\nAI consists of two parts: a dynamic adjuster for controlling pitch and loudness\nof the TTS and a real-time game commentary generator. We conduct a pilot study\non a fighting game, and our result shows that by adjusting the loudness\nsignificantly according to the level of game highlight, the entertainment of\nthe gameplay can be enhanced.",
          "link": "http://arxiv.org/abs/2108.08112",
          "publishedOn": "2021-08-19T01:34:58.636Z",
          "wordCount": 565,
          "title": "Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing Highlight Cues. (arXiv:2108.08112v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yulin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhou Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1\">Pujana Paliyawan</a>",
          "description": "This paper proposes a method for generating bullet comments for\nlive-streaming games based on highlights (i.e., the exciting parts of video\nclips) extracted from the game content and evaluate the effect of mental health\npromotion. Game live streaming is becoming a popular theme for academic\nresearch. Compared to traditional online video sharing platforms, such as\nYoutube and Vimeo, video live streaming platform has the benefits of\ncommunicating with other viewers in real-time. In sports broadcasting, the\ncommentator plays an essential role as mood maker by making matches more\nexciting. The enjoyment emerged while watching game live streaming also\nbenefits the audience's mental health. However, many e-sports live streaming\nchannels do not have a commentator for entertaining viewers. Therefore, this\npaper presents a design of an AI commentator that can be embedded in live\nstreaming games. To generate bullet comments for real-time game live streaming,\nthe system employs highlight evaluation to detect the highlights, and generate\nthe bullet comments. An experiment is conducted and the effectiveness of\ngenerated bullet comments in a live-streaming fighting game channel is\nevaluated.",
          "link": "http://arxiv.org/abs/2108.08083",
          "publishedOn": "2021-08-19T01:34:58.562Z",
          "wordCount": 628,
          "title": "Promoting Mental Well-Being for Audiences in a Live-Streaming Game by Highlight-Based Bullet Comments. (arXiv:2108.08083v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-08-18T01:54:58.836Z",
          "wordCount": 684,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the test (validation)\nset of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence\nand 0.492 (0.649) for arousal, which significantly outperforms the baseline\nmethod with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for\nvalence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-08-18T01:54:58.621Z",
          "wordCount": 678,
          "title": "Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourmirzaei_M/0/1/0/all/0/1\">Mahdi Pourmirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montazer_G/0/1/0/all/0/1\">Gholam Ali Montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaili_F/0/1/0/all/0/1\">Farzaneh Esmaili</a>",
          "description": "Over the past few years, best SSL methods, gradually moved from the pre-text\ntask learning to the Contrastive learning. But contrastive methods have some\ndrawbacks which could not be solved completely, such as performing poor on\nfine-grained visual tasks compare to supervised learning methods. In this\nstudy, at first, the impact of ImageNet pre-training on fine-grained Facial\nExpression Recognition (FER) was tested. It could be seen from the results that\ntraining from scratch is better than ImageNet fine-tuning at stronger\naugmentation levels. After that, a framework was proposed for standard\nSupervised Learning (SL), called Hybrid Multi-Task Learning (HMTL) which merged\nSelf-Supervised as auxiliary task to the SL training setting. Leveraging\nSelf-Supervised Learning (SSL) can gain additional information from input data\nthan labels which can help the main fine-grained SL task. It is been\ninvestigated how this method could be used for FER by designing two customized\nversion of common pre-text techniques, Jigsaw puzzling and in-painting. The\nstate-of-the-art was reached on AffectNet via two types of HMTL, without\nutilizing pre-training on additional datasets. Moreover, we showed the\ndifference between SS pre-training and HMTL to demonstrate superiority of\nproposed method. Furthermore, the impact of proposed method was shown on two\nother fine-grained facial tasks, Head Poses estimation and Gender Recognition,\nwhich concluded to reduce in error rate by 11% and 1% respectively.",
          "link": "http://arxiv.org/abs/2105.06421",
          "publishedOn": "2021-08-23T01:36:36.437Z",
          "wordCount": 687,
          "title": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation. (arXiv:2105.06421v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1\">Rauf Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1\">Ali Shan Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khalid Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>",
          "description": "Facial expressions are the most common universal forms of body language. In\nthe past few years, automatic facial expression recognition (FER) has been an\nactive field of research. However, it is still a challenging task due to\ndifferent uncertainties and complications. Nevertheless, efficiency and\nperformance are yet essential aspects for building robust systems. We proposed\ntwo models, EmoXNet which is an ensemble learning technique for learning\nconvoluted facial representations, and EmoXNetLite which is a distillation\ntechnique that is useful for transferring the knowledge from our ensemble model\nto an efficient deep neural network using label-smoothen soft labels for able\nto effectively detect expressions in real-time. Both of the techniques\nperformed quite well, where the ensemble model (EmoXNet) helped to achieve\n85.07% test accuracy on FER2013 with FER+ annotations and 86.25% test accuracy\non RAF-DB. Moreover, the distilled model (EmoXNetLite) showed 82.07% test\naccuracy on FER2013 with FER+ annotations and 81.78% test accuracy on RAF-DB.\nResults show that our models seem to generalize well on new data and are\nlearned to focus on relevant facial representations for expressions\nrecognition.",
          "link": "http://arxiv.org/abs/2106.16126",
          "publishedOn": "2021-08-23T01:36:36.430Z",
          "wordCount": 666,
          "title": "Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vidit_V/0/1/0/all/0/1\">Vidit Vidit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "While domain adaptation has been used to improve the performance of object\ndetectors when the training and test data follow different distributions,\nprevious work has mostly focused on two-stage detectors. This is because their\nuse of region proposals makes it possible to perform local adaptation, which\nhas been shown to significantly improve the adaptation effectiveness. Here, by\ncontrast, we target single-stage architectures, which are better suited to\nresource-constrained detection than two-stage ones but do not provide region\nproposals. To nonetheless benefit from the strength of local adaptation, we\nintroduce an attention mechanism that lets us identify the important regions on\nwhich adaptation should focus. Our method gradually adapts the features from\nglobal, image-level to local, instance-level. Our approach is generic and can\nbe integrated into any single-stage detector. We demonstrate this on standard\nbenchmark datasets by applying it to both SSD and YOLOv5. Furthermore, for\nequivalent single-stage architectures, our method outperforms the\nstate-of-the-art domain adaptation techniques even though they were designed\nfor specific detectors.",
          "link": "http://arxiv.org/abs/2106.07283",
          "publishedOn": "2021-08-23T01:36:36.387Z",
          "wordCount": 619,
          "title": "Attention-based Domain Adaptation for Single Stage Detectors. (arXiv:2106.07283v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1\">Vijay Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Computer vision applications such as visual relationship detection and human\nobject interaction can be formulated as a composite (structured) set detection\nproblem in which both the parts (subject, object, and predicate) and the sum\n(triplet as a whole) are to be detected in a hierarchical fashion. In this\npaper, we present a new approach, denoted Part-and-Sum detection Transformer\n(PST), to perform end-to-end visual composite set detection. Different from\nexisting Transformers in which queries are at a single level, we simultaneously\nmodel the joint part and sum hypotheses/interactions with composite queries and\nattention modules. We explicitly incorporate sum queries to enable better\nmodeling of the part-and-sum relations that are absent in the standard\nTransformers. Our approach also uses novel tensor-based part queries and\nvector-based sum queries, and models their joint interaction. We report\nexperiments on two vision tasks, visual relationship detection and human object\ninteraction and demonstrate that PST achieves state of the art results among\nsingle-stage models, while nearly matching the results of custom designed\ntwo-stage models.",
          "link": "http://arxiv.org/abs/2105.02170",
          "publishedOn": "2021-08-23T01:36:36.380Z",
          "wordCount": 643,
          "title": "Visual Relationship Detection Using Part-and-Sum Transformers with Composite Queries. (arXiv:2105.02170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1\">Danila Rukhovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1\">Anna Vorontsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1\">Anton Konushin</a>",
          "description": "In this paper, we introduce the task of multi-view RGB-based 3D object\ndetection as an end-to-end optimization problem. To address this problem, we\npropose ImVoxelNet, a novel fully convolutional method of 3D object detection\nbased on monocular or multi-view RGB images. The number of monocular images in\neach multi-view input can variate during training and inference; actually, this\nnumber might be unique for each multi-view input. ImVoxelNet successfully\nhandles both indoor and outdoor scenes, which makes it general-purpose.\nSpecifically, it achieves state-of-the-art results in car detection on KITTI\n(monocular) and nuScenes (multi-view) benchmarks among all methods that accept\nRGB images. Moreover, it surpasses existing RGB-based 3D object detection\nmethods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\nfor multi-view 3D object detection. The source code and the trained models are\navailable at https://github.com/saic-vul/imvoxelnet.",
          "link": "http://arxiv.org/abs/2106.01178",
          "publishedOn": "2021-08-23T01:36:36.312Z",
          "wordCount": 610,
          "title": "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection. (arXiv:2106.01178v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time-sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous LSTM accelerators either exploited weight\nspatial sparsity or temporal activation sparsity. This paper proposes a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity is induced using our proposed\npruning method called Column-Balanced Targeted Dropout (CBTD), which structures\nsparse weight matrices for balanced workload. It achieved up to 96% weight\nsparsity with negligible accuracy difference for an LSTM network trained on a\nTIMIT phone recognition task. To induce temporal sparsity in LSTM, we create\nthe DeltaLSTM by extending the previous DeltaGRU method to the LSTM network.\nThis combined sparsity simultaneously saves on the weight memory access and\nassociated arithmetic operations. Spartus was implemented on a Xilinx Zynq-7100\nFPGA. The Spartus per-sample latency for a single DeltaLSTM layer of 1024\nneurons averages 1 us. Spartus achieved 9.4 TOp/s effective batch-1 throughput\nand 1.1 TOp/J energy efficiency, which, respectively, are 4X and 7X higher than\nthe previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-23T01:36:36.294Z",
          "wordCount": 677,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v3 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1\">Angela Constantinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Common fully glazed facades and transparent objects present architectural\nbarriers and impede the mobility of people with low vision or blindness, for\ninstance, a path detected behind a glass door is inaccessible unless it is\ncorrectly perceived and reacted. However, segmenting these safety-critical\nobjects is rarely covered by conventional assistive technologies. To tackle\nthis issue, we construct a wearable system with a novel dual-head Transformer\nfor Transparency (Trans4Trans) model, which is capable of segmenting general\nand transparent objects and performing real-time wayfinding to assist people\nwalking alone more safely. Especially, both decoders created by our proposed\nTransformer Parsing Module (TPM) enable effective joint learning from different\ndatasets. Besides, the efficient Trans4Trans model composed of symmetric\ntransformer-based encoder and decoder, requires little computational expenses\nand is readily deployed on portable GPUs. Our Trans4Trans model outperforms\nstate-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2\ndatasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various\npre-tests and a user study conducted in indoor and outdoor scenarios, the\nusability and reliability of our assistive system have been extensively\nverified.",
          "link": "http://arxiv.org/abs/2107.03172",
          "publishedOn": "2021-08-23T01:36:36.003Z",
          "wordCount": 689,
          "title": "Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World. (arXiv:2107.03172v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "We focus on contrastive methods for self-supervised video representation\nlearning. A common paradigm in contrastive learning is to construct positive\npairs by sampling different data views for the same instance, with different\ndata instances as negatives. These methods implicitly assume a set of\nrepresentational invariances to the view selection mechanism (eg, sampling\nframes with temporal shifts), which may lead to poor performance on downstream\ntasks which violate these invariances (fine-grained video action recognition\nthat would benefit from temporal information). To overcome this limitation, we\npropose an 'augmentation aware' contrastive learning framework, where we\nexplicitly provide a sequence of augmentation parameterisations (such as the\nvalues of the time shifts used to create data views) as composable augmentation\nencodings (CATE) to our model when projecting the video representations for\ncontrastive learning. We show that representations learned by our method encode\nvaluable information about specified spatial or temporal augmentation, and in\ndoing so also achieve state-of-the-art performance on a number of video\nbenchmarks.",
          "link": "http://arxiv.org/abs/2104.00616",
          "publishedOn": "2021-08-23T01:36:35.997Z",
          "wordCount": 628,
          "title": "Composable Augmentation Encoding for Video Representation Learning. (arXiv:2104.00616v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">Yuan Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Frame sampling is a fundamental problem in video action recognition due to\nthe essential redundancy in time and limited computation resources. The\nexisting sampling strategy often employs a fixed frame selection and lacks the\nflexibility to deal with complex variations in videos. In this paper, we\npresent a simple, sparse, and explainable frame sampler, termed as\nMotion-Guided Sampler (MGSampler). Our basic motivation is that motion is an\nimportant and universal signal that can drive us to adaptively select frames\nfrom videos. Accordingly, we propose two important properties in our MGSampler\ndesign: motion sensitive and motion uniform. First, we present two different\nmotion representations to enable us to efficiently distinguish the\nmotion-salient frames from the background. Then, we devise a motion-uniform\nsampling strategy based on the cumulative motion distribution to ensure the\nsampled frames evenly cover all the important segments with high motion\nsalience. Our MGSampler yields a new principled and holistic sampling scheme,\nthat could be incorporated into any existing video architecture. Experiments on\nfive benchmarks demonstrate the effectiveness of our MGSampler over the\nprevious fixed sampling strategies, and its generalization power across\ndifferent backbones, video models, and datasets.",
          "link": "http://arxiv.org/abs/2104.09952",
          "publishedOn": "2021-08-23T01:36:35.972Z",
          "wordCount": 659,
          "title": "MGSampler: An Explainable Sampling Strategy for Video Action Recognition. (arXiv:2104.09952v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1\">Ben Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Given an input image from a source domain and a guidance image from a target\ndomain, unsupervised many-to-many image-to-image (UMMI2I) translation methods\nseek to generate a plausible example from the target domain that preserves\ndomain-invariant information of the input source image and inherits the\ndomain-specific information from the guidance image. For example, when\ntranslating female faces to male faces, the generated male face should have the\nsame expression, pose and hair color as the input female image, and the same\nfacial hairstyle and other male-specific attributes as the guidance male image.\nCurrent state-of-the art UMMI2I methods generate visually pleasing images, but,\nsince for most pairs of real datasets we do not know which attributes are\ndomain-specific and which are domain-invariant, the semantic correctness of\nexisting approaches has not been quantitatively evaluated yet. In this paper,\nwe propose a set of benchmarks and metrics for the evaluation of semantic\ncorrectness of these methods. We provide an extensive study of existing\nstate-of-the-art UMMI2I translation methods, showing that all methods, to\ndifferent degrees, fail to infer which attributes are domain-specific and which\nare domain-invariant from data, and mostly rely on inductive biases hard-coded\ninto their architectures.",
          "link": "http://arxiv.org/abs/2103.15727",
          "publishedOn": "2021-08-23T01:36:35.959Z",
          "wordCount": 654,
          "title": "Evaluation of Correctness in Unsupervised Many-to-Many Image Translation. (arXiv:2103.15727v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ranjitha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan Plummer</a>",
          "description": "Measuring similarity between two images often requires performing complex\nreasoning along different axes (e.g., color, texture, or shape). Insights into\nwhat might be important for measuring similarity can can be provided by\nannotated attributes, but prior work tends to view these annotations as\ncomplete, resulting in them using a simplistic approach of predicting\nattributes on single images, which are, in turn, used to measure similarity.\nHowever, it is impractical for a dataset to fully annotate every attribute that\nmay be important. Thus, only representing images based on these incomplete\nannotations may miss out on key information. To address this issue, we propose\nthe Pairwise Attribute-informed similarity Network (PAN), which breaks\nsimilarity learning into capturing similarity conditions and relevance scores\nfrom a joint representation of two images. This enables our model to identify\nthat two images contain the same attribute, but can have it deemed irrelevant\n(e.g., due to fine-grained differences between them) and ignored for measuring\nsimilarity between the two images. Notably, while prior methods of using\nattribute annotations are often unable to outperform prior art, PAN obtains a\n4-9% improvement on compatibility prediction between clothing items on Polyvore\nOutfits, a 5% gain on few shot classification of images using Caltech-UCSD\nBirds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval.\nImplementation available at https://github.com/samarth4149/PAN",
          "link": "http://arxiv.org/abs/2105.01695",
          "publishedOn": "2021-08-23T01:36:35.953Z",
          "wordCount": 691,
          "title": "Effectively Leveraging Attributes for Visual Similarity. (arXiv:2105.01695v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs. Code is available at\nhttps://github.com/yellowtownhz/STIGCN.",
          "link": "http://arxiv.org/abs/2011.13322",
          "publishedOn": "2021-08-23T01:36:35.929Z",
          "wordCount": 710,
          "title": "Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yunye Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1\">Melinda Gervasio</a>",
          "description": "Existing calibration algorithms address the problem of covariate shift via\nunsupervised domain adaptation. However, these methods suffer from the\nfollowing limitations: 1) they require unlabeled data from the target domain,\nwhich may not be available at the stage of calibration in real-world\napplications and 2) their performance depends heavily on the disparity between\nthe distributions of the source and target domains. To address these two\nlimitations, we present novel calibration solutions via domain generalization.\nOur core idea is to leverage multiple calibration domains to reduce the\neffective distribution disparity between the target and calibration domains for\nimproved calibration transfer without needing any data from the target domain.\nWe provide theoretical justification and empirical experimental results to\ndemonstrate the effectiveness of our proposed algorithms. Compared against\nstate-of-the-art calibration methods designed for domain adaptation, we observe\na decrease of 8.86 percentage points in expected calibration error or,\nequivalently, an increase of 35 percentage points in improvement ratio for\nmulti-class classification on the Office-Home dataset.",
          "link": "http://arxiv.org/abs/2104.00742",
          "publishedOn": "2021-08-23T01:36:35.903Z",
          "wordCount": 635,
          "title": "Confidence Calibration for Domain Generalization under Covariate Shift. (arXiv:2104.00742v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, generating pseudo labels or low confidence\nlabels on the entire unlabeled test dataset, and then, (2) filter out quality\ngenerated labels and, (3) combining the generated labels with the previously\navailable high confidence labeled dataset. This assimilated dataset is used for\nthe next round of training ensemble models. This cyclical process is repeated\nuntil the performance improvement plateaus. Additionally, we post process our\nresults with Conditional Random Fields. Our approach sets the second highest\nscore on the public hold-out test leaderboard for the ETCI competition with\n0.7654 IoU. To the best of our knowledge we believe this is one of the first\nworks to try out semi-supervised learning to improve flood segmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-08-23T01:36:35.878Z",
          "wordCount": 776,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yanglan Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately, e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method significantly improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.",
          "link": "http://arxiv.org/abs/2103.14023",
          "publishedOn": "2021-08-23T01:36:35.871Z",
          "wordCount": 774,
          "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. (arXiv:2103.14023v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Picek_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Picek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan &#x160;ulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_Clausen_J/0/1/0/all/0/1\">Jacob Heilmann-Clausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeppesen_T/0/1/0/all/0/1\">Thomas S. Jeppesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laessoe_T/0/1/0/all/0/1\">Thomas L&#xe6;ss&#xf8;e</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Froslev_T/0/1/0/all/0/1\">Tobias Fr&#xf8;slev</a>",
          "description": "We introduce a novel fine-grained dataset and benchmark, the Danish Fungi\n2020 (DF20). The dataset, constructed from observations submitted to the Atlas\nof Danish Fungi, is unique in its taxonomy-accurate class labels, small number\nof errors, highly unbalanced long-tailed class distribution, rich observation\nmetadata, and well-defined class hierarchy. DF20 has zero overlap with\nImageNet, allowing unbiased comparison of models fine-tuned from publicly\navailable ImageNet checkpoints. The proposed evaluation protocol enables\ntesting the ability to improve classification using metadata -- e.g. precise\ngeographic location, habitat, and substrate, facilitates classifier calibration\ntesting, and finally allows to study the impact of the device settings on the\nclassification performance. Experiments using Convolutional Neural Networks\n(CNN) and the recent Vision Transformers (ViT) show that DF20 presents a\nchallenging task. Interestingly, ViT achieves results superior to CNN baselines\nwith 80.45% accuracy and 0.743 macro F1 score, reducing the CNN error by 9% and\n12% respectively. A simple procedure for including metadata into the decision\nprocess improves the classification accuracy by more than 2.95 percentage\npoints, reducing the error rate by 15%. The source code for all methods and\nexperiments is available at https://sites.google.com/view/danish-fungi-dataset.",
          "link": "http://arxiv.org/abs/2103.10107",
          "publishedOn": "2021-08-23T01:36:35.858Z",
          "wordCount": 687,
          "title": "Danish Fungi 2020 -- Not Just Another Image Recognition Dataset. (arXiv:2103.10107v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-23T01:36:35.835Z",
          "wordCount": 616,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohebbian_M/0/1/0/all/0/1\">Mohammad Reza Mohebbian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahid_K/0/1/0/all/0/1\">Khan A. Wahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babyn_P/0/1/0/all/0/1\">Paul Babyn</a>",
          "description": "Wireless Capsule Endoscopy (WCE) helps physicians examine the\ngastrointestinal (GI) tract noninvasively. There are few studies that address\npathological assessment of endoscopy images in multiclass classification and\nmost of them are based on binary anomaly detection or aim to detect a specific\ntype of anomaly. Multiclass anomaly detection is challenging, especially when\nthe dataset is poorly sampled or imbalanced. Many available datasets in\nendoscopy field, such as KID2, suffer from an imbalance issue, which makes it\ndifficult to train a high-performance model. Additionally, increasing the\nnumber of classes makes classification more difficult. We proposed a multiclass\nclassification algorithm that is extensible to any number of classes and can\nhandle an imbalance issue. The proposed method uses multiple autoencoders where\neach one is trained on one class to extract features with the most\ndiscrimination from other classes. The loss function of autoencoders is set\nbased on reconstruction, compactness, distance from other classes, and\nKullback-Leibler (KL) divergence. The extracted features are clustered and then\nclassified using an ensemble of support vector data descriptors. A total of\n1,778 normal, 227 inflammation, 303 vascular, and 44 polyp images from the KID2\ndataset are used for evaluation. The entire algorithm ran 5 times and achieved\nF1-score of 96.3 +- 0.2% and 85.0 +- 0.4% on the test set for binary and\nmulticlass anomaly detection, respectively. The impact of each step of the\nalgorithm was investigated by various ablation studies and the results were\ncompared with published works. The suggested approach is a competitive option\nfor detecting multiclass anomalies in the GI field.",
          "link": "http://arxiv.org/abs/2103.08508",
          "publishedOn": "2021-08-23T01:36:35.828Z",
          "wordCount": 726,
          "title": "Stack of discriminative autoencoders for multiclass anomaly detection in endoscopy images. (arXiv:2103.08508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1\">Armin Hadzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neil Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1\">Fady Alajaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Phil Burlina</a>",
          "description": "We propose a novel method for enforcing AI fairness with respect to protected\nor sensitive factors. This method uses a dual strategy performing training and\nrepresentation alteration (TARA) for the mitigation of prominent causes of AI\nbias by including: a) the use of representation learning alteration via\nadversarial independence to suppress the bias-inducing dependence of the data\nrepresentation from protected factors; and b) training set alteration via\nintelligent augmentation to address bias-causing data imbalance, by using\ngenerative models that allow the fine control of sensitive factors related to\nunderrepresented populations via domain adaptation and latent space\nmanipulation. When testing our methods on image analytics, experiments\ndemonstrate that TARA significantly or fully debiases baseline models while\noutperforming competing debiasing methods that have the same amount of\ninformation, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.\nthe baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.\n(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in\ncurrent metrics used for assessing debiasing performance, we propose novel\nconjunctive debiasing metrics. Our experiments also demonstrate the ability of\nthese novel metrics in assessing the Pareto efficiency of the proposed methods.",
          "link": "http://arxiv.org/abs/2012.06387",
          "publishedOn": "2021-08-23T01:36:35.821Z",
          "wordCount": 692,
          "title": "TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>",
          "description": "Scene graph generation aims to identify objects and their relations in\nimages, providing structured image representations that can facilitate numerous\napplications in computer vision. However, scene graph models usually require\nsupervised learning on large quantities of labeled data with intensive human\nannotation. In this work, we propose visual distant supervision, a novel\nparadigm of visual relation learning, which can train scene graph models\nwithout any human-labeled data. The intuition is that by aligning commonsense\nknowledge bases and images, we can automatically create large-scale labeled\ndata to provide distant supervision for visual relation learning. To alleviate\nthe noise in distantly labeled data, we further propose a framework that\niteratively estimates the probabilistic relation labels and eliminates the\nnoisy ones. Comprehensive experimental results show that our distantly\nsupervised model outperforms strong weakly supervised and semi-supervised\nbaselines. By further incorporating human-labeled data in a semi-supervised\nfashion, our model outperforms state-of-the-art fully supervised models by a\nlarge margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for\npredicate classification in Visual Genome evaluation). We make the data and\ncode for this paper publicly available at https://github.com/thunlp/VisualDS.",
          "link": "http://arxiv.org/abs/2103.15365",
          "publishedOn": "2021-08-23T01:36:35.812Z",
          "wordCount": 661,
          "title": "Visual Distant Supervision for Scene Graph Generation. (arXiv:2103.15365v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Recent work has shown that the performance of machine learning models can\nvary substantially when models are evaluated on data drawn from a distribution\nthat is close to but different from the training distribution. As a result,\npredicting model performance on unseen distributions is an important challenge.\nOur work connects techniques from domain adaptation and predictive uncertainty\nliterature, and allows us to predict model accuracy on challenging unseen\ndistributions without access to labeled data. In the context of distribution\nshift, distributional distances are often used to adapt models and improve\ntheir performance on new domains, however accuracy estimation, or other forms\nof predictive uncertainty, are often neglected in these investigations. Through\ninvestigating a wide range of established distributional distances, such as\nFrechet distance or Maximum Mean Discrepancy, we determine that they fail to\ninduce reliable estimates of performance under distribution shift. On the other\nhand, we find that the difference of confidences (DoC) of a classifier's\npredictions successfully estimates the classifier's performance change over a\nvariety of shifts. We specifically investigate the distinction between\nsynthetic and natural distribution shifts and observe that despite its\nsimplicity DoC consistently outperforms other quantifications of distributional\ndifference. $DoC$ reduces predictive error by almost half ($46\\%$) on several\nrealistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust\nand ImageNet-Rendition datasets.",
          "link": "http://arxiv.org/abs/2107.03315",
          "publishedOn": "2021-08-23T01:36:35.803Z",
          "wordCount": 697,
          "title": "Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Linyi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengyi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>",
          "description": "The paper studies planar surface reconstruction of indoor scenes from two\nviews with unknown camera poses. While prior approaches have successfully\ncreated object-centric reconstructions of many scenes, they fail to exploit\nother structures, such as planes, which are typically the dominant components\nof indoor scenes. In this paper, we reconstruct planar surfaces from multiple\nviews, while jointly estimating camera pose. Our experiments demonstrate that\nour method is able to advance the state of the art of reconstruction from\nsparse views, on challenging scenes from Matterport3D. Project site:\nhttps://jinlinyi.github.io/SparsePlanes/",
          "link": "http://arxiv.org/abs/2103.14644",
          "publishedOn": "2021-08-23T01:36:35.785Z",
          "wordCount": 559,
          "title": "Planar Surface Reconstruction from Sparse Views. (arXiv:2103.14644v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1\">Hamid Laga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padilla_M/0/1/0/all/0/1\">Marcel Padilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermyn_I/0/1/0/all/0/1\">Ian H. Jermyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtek_S/0/1/0/all/0/1\">Sebastian Kurtek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anuj Srivastava</a>",
          "description": "We propose a novel framework to learn the spatiotemporal variability in\nlongitudinal 3D shape data sets, which contain observations of objects that\nevolve and deform over time. This problem is challenging since surfaces come\nwith arbitrary parameterizations and thus, they need to be spatially\nregistered. Also, different deforming objects, also called 4D surfaces, evolve\nat different speeds and thus they need to be temporally aligned. We solve this\nspatiotemporal registration problem using a Riemannian approach. We treat a 3D\nsurface as a point in a shape space equipped with an elastic Riemannian metric\nthat measures the amount of bending and stretching that the surfaces undergo. A\n4D surface can then be seen as a trajectory in this space. With this\nformulation, the statistical analysis of 4D surfaces can be cast as the problem\nof analyzing trajectories embedded in a nonlinear Riemannian manifold. However,\nperforming the spatiotemporal registration, and subsequently computing\nstatistics, on such nonlinear spaces is not straightforward as they rely on\ncomplex nonlinear optimizations. Our core contribution is the mapping of the\nsurfaces to the space of Square-Root Normal Fields where the L2 metric is\nequivalent to the partial elastic metric in the space of surfaces. Thus, by\nsolving the spatial registration in the SRNF space, the problem of analyzing 4D\nsurfaces becomes the problem of analyzing trajectories embedded in the SRNF\nspace, which has a Euclidean structure. In this paper, we develop the building\nblocks that enable such analysis. These include: (1) the spatiotemporal\nregistration of arbitrarily parameterized 4D surfaces in the presence of large\nelastic deformations and large variations in their execution rates; (2) the\ncomputation of geodesics between 4D surfaces; (3) the computation of\nstatistical summaries; and (4) the synthesis of random 4D surfaces.",
          "link": "http://arxiv.org/abs/2101.09403",
          "publishedOn": "2021-08-23T01:36:35.741Z",
          "wordCount": 769,
          "title": "4D Atlas: Statistical Analysis of the Spatiotemporal Variability in Longitudinal 3D Shape Data. (arXiv:2101.09403v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01214",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Guzman_E/0/1/0/all/0/1\">Eric Guzman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Meyers_J/0/1/0/all/0/1\">Joel Meyers</a>",
          "description": "The precision anticipated from next-generation cosmic microwave background\n(CMB) surveys will create opportunities for characteristically new insights\ninto cosmology. Secondary anisotropies of the CMB will have an increased\nimportance in forthcoming surveys, due both to the cosmological information\nthey encode and the role they play in obscuring our view of the primary\nfluctuations. Quadratic estimators have become the standard tools for\nreconstructing the fields that distort the primary CMB and produce secondary\nanisotropies. While successful for lensing reconstruction with current data,\nquadratic estimators will be sub-optimal for the reconstruction of lensing and\nother effects at the expected sensitivity of the upcoming CMB surveys. In this\npaper we describe a convolutional neural network, ResUNet-CMB, that is capable\nof the simultaneous reconstruction of two sources of secondary CMB\nanisotropies, gravitational lensing and patchy reionization. We show that the\nResUNet-CMB network significantly outperforms the quadratic estimator at low\nnoise levels and is not subject to the lensing-induced bias on the patchy\nreionization reconstruction that would be present with a straightforward\napplication of the quadratic estimator.",
          "link": "http://arxiv.org/abs/2101.01214",
          "publishedOn": "2021-08-23T01:36:35.700Z",
          "wordCount": 647,
          "title": "Reconstructing Patchy Reionization with Deep Learning. (arXiv:2101.01214v2 [astro-ph.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Aybora Koksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazla_A/0/1/0/all/0/1\">Arda Fazla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>",
          "description": "We propose a semi-automatic bounding box annotation method for visual object\ntracking by utilizing temporal information with a tracking-by-detection\napproach. For detection, we use an off-the-shelf object detector which is\ntrained iteratively with the annotations generated by the proposed method, and\nwe perform object detection on each frame independently. We employ Multiple\nHypothesis Tracking (MHT) to exploit temporal information and to reduce the\nnumber of false-positives which makes it possible to use lower objectness\nthresholds for detection to increase recall. The tracklets formed by MHT are\nevaluated by human operators to enlarge the training set. This novel\nincremental learning approach helps to perform annotation iteratively. The\nexperiments performed on AUTH Multidrone Dataset reveal that the annotation\nworkload can be reduced up to 96% by the proposed approach.",
          "link": "http://arxiv.org/abs/2101.06977",
          "publishedOn": "2021-08-23T01:36:35.685Z",
          "wordCount": 632,
          "title": "Semi-Automatic Annotation For Visual Object Tracking. (arXiv:2101.06977v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.09245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1\">Diogo C Luvizon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1\">Hedi Tabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>",
          "description": "3D human pose estimation is frequently seen as the task of estimating 3D\nposes relative to the root body joint. Alternatively, we propose a 3D human\npose estimation method in camera coordinates, which allows effective\ncombination of 2D annotated data and 3D poses and a straightforward multi-view\ngeneralization. To that end, we cast the problem as a view frustum space pose\nestimation, where absolute depth prediction and joint relative depth\nestimations are disentangled. Final 3D predictions are obtained in camera\ncoordinates by the inverse camera projection. Based on this, we also present a\nconsensus-based optimization algorithm for multi-view predictions from\nuncalibrated images, which requires a single monocular training procedure.\nAlthough our method is indirectly tied to the training camera intrinsics, it\nstill converges for cameras with different intrinsic parameters, resulting in\ncoherent estimations up to a scale factor. Our method improves the state of the\nart on well known 3D human pose datasets, reducing the prediction error by 32%\nin the most common benchmark. We also reported our results in absolute pose\nposition error, achieving 80~mm for monocular estimations and 51~mm for\nmulti-view, on average.",
          "link": "http://arxiv.org/abs/1911.09245",
          "publishedOn": "2021-08-23T01:36:35.669Z",
          "wordCount": 671,
          "title": "Consensus-based Optimization for 3D Human Pose Estimation in Camera Coordinates. (arXiv:1911.09245v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohebbian_M/0/1/0/all/0/1\">Mohammad Reza Mohebbian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahid_K/0/1/0/all/0/1\">Khan A. Wahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_A/0/1/0/all/0/1\">Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babyn_P/0/1/0/all/0/1\">Paul Babyn</a>",
          "description": "Conventional Endoscopy (CE) and Wireless Capsule Endoscopy (WCE) are known\ntools for diagnosing gastrointestinal (GI) tract disorders. Detecting the\nanatomical location of GI tract can help clinicians to determine a more\nappropriate treatment plan, can reduce repetitive endoscopy and is important in\ndrug-delivery. There are few research that address detecting anatomical\nlocation of WCE and CE images using classification, mainly because of\ndifficulty in collecting data and anotating them. In this study, we present a\nfew-shot learning method based on distance metric learning which combines\ntransfer-learning and manifold mixup scheme for localizing endoscopy frames and\ncan be trained on few samples. The manifold mixup process improves few-shot\nlearning by increasing the number of training epochs while reducing\noverfitting, as well as providing more accurate decision boundaries. A dataset\nis collected from 10 different anatomical positions of human GI tract. Two\nmodels were trained using only 78 CE and 27 WCE annotated frames to predict the\nlocation of 25700 and 1825 video frames from CE and WCE, respectively. In\naddition, we performed subjective evaluation using nine gastroenterologists to\nshow the necessaity of having an AI system for localization. Various ablation\nstudies and interpretations are performed to show the importance of each step,\nsuch effect of transfer-learning approach, and impact of manifold mixup on\nperformance. The proposed method is also compared with various methods trained\non categorical cross-entropy loss and produced better results which show that\nproposed method has potential to be used for endoscopy image classification.",
          "link": "http://arxiv.org/abs/2103.08504",
          "publishedOn": "2021-08-23T01:36:35.661Z",
          "wordCount": 724,
          "title": "Distance Metric-Based Learning with Interpolated Latent Features for Location Classification in Endoscopy Image and Video. (arXiv:2103.08504v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yinzhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Phil Burlina</a>",
          "description": "Machine learning (ML) models used in medical imaging diagnostics can be\nvulnerable to a variety of privacy attacks, including membership inference\nattacks, that lead to violations of regulations governing the use of medical\ndata and threaten to compromise their effective deployment in the clinic. In\ncontrast to most recent work in privacy-aware ML that has been focused on model\nalteration and post-processing steps, we propose here a novel and complementary\nscheme that enhances the security of medical data by controlling the data\nsharing process. We develop and evaluate a privacy defense protocol based on\nusing a generative adversarial network (GAN) that allows a medical data sourcer\n(e.g. a hospital) to provide an external agent (a modeler) a proxy dataset\nsynthesized from the original images, so that the resulting diagnostic systems\nmade available to model consumers is rendered resilient to privacy attackers.\nWe validate the proposed method on retinal diagnostics AI used for diabetic\nretinopathy that bears the risk of possibly leaking private information. To\nincorporate concerns of both privacy advocates and modelers, we introduce a\nmetric to evaluate privacy and utility performance in combination, and\ndemonstrate, using these novel and classical metrics, that our approach, by\nitself or in conjunction with other defenses, provides state of the art (SOTA)\nperformance for defending against privacy attacks.",
          "link": "http://arxiv.org/abs/2103.03078",
          "publishedOn": "2021-08-23T01:36:35.630Z",
          "wordCount": 712,
          "title": "Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods. (arXiv:2103.03078v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_K/0/1/0/all/0/1\">Kyungjune Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaejun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>",
          "description": "Every recent image-to-image translation model inherently requires either\nimage-level (i.e. input-output pairs) or set-level (i.e. domain labels)\nsupervision. However, even set-level supervision can be a severe bottleneck for\ndata collection in practice. In this paper, we tackle image-to-image\ntranslation in a fully unsupervised setting, i.e., neither paired images nor\ndomain labels. To this end, we propose a truly unsupervised image-to-image\ntranslation model (TUNIT) that simultaneously learns to separate image domains\nand translates input images into the estimated domains. Experimental results\nshow that our model achieves comparable or even better performance than the\nset-level supervised model trained with full labels, generalizes well on\nvarious datasets, and is robust against the choice of hyperparameters (e.g. the\npreset number of pseudo domains). Furthermore, TUNIT can be easily extended to\nsemi-supervised learning with a few labeled data.",
          "link": "http://arxiv.org/abs/2006.06500",
          "publishedOn": "2021-08-23T01:36:35.617Z",
          "wordCount": 607,
          "title": "Rethinking the Truly Unsupervised Image-to-Image Translation. (arXiv:2006.06500v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Sanath Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Multi-label zero-shot learning (ZSL) is a more realistic counter-part of\nstandard single-label ZSL since several objects can co-exist in a natural\nimage. However, the occurrence of multiple objects complicates the reasoning\nand requires region-specific processing of visual features to preserve their\ncontextual cues. We note that the best existing multi-label ZSL method takes a\nshared approach towards attending to region features with a common set of\nattention maps for all the classes. Such shared maps lead to diffused\nattention, which does not discriminatively focus on relevant locations when the\nnumber of classes are large. Moreover, mapping spatially-pooled visual features\nto the class semantics leads to inter-class feature entanglement, thus\nhampering the classification. Here, we propose an alternate approach towards\nregion-based discriminability-preserving multi-label zero-shot classification.\nOur approach maintains the spatial resolution to preserve region-level\ncharacteristics and utilizes a bi-level attention module (BiAM) to enrich the\nfeatures by incorporating both region and scene context information. The\nenriched region-level features are then mapped to the class semantics and only\ntheir class predictions are spatially pooled to obtain image-level predictions,\nthereby keeping the multi-class features disentangled. Our approach sets a new\nstate of the art on two large-scale multi-label zero-shot benchmarks: NUS-WIDE\nand Open Images. On NUS-WIDE, our approach achieves an absolute gain of 6.9%\nmAP for ZSL, compared to the best published results.",
          "link": "http://arxiv.org/abs/2108.09301",
          "publishedOn": "2021-08-23T01:36:35.610Z",
          "wordCount": 667,
          "title": "Discriminative Region-based Multi-Label Zero-Shot Learning. (arXiv:2108.09301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>",
          "description": "2D/3D human pose estimation is needed to develop novel intelligent tools for\nthe operating room that can analyze and support the clinical activities. The\nlack of annotated data and the complexity of state-of-the-art pose estimation\napproaches limit, however, the deployment of such techniques inside the OR. In\nthis work, we propose to use knowledge distillation in a teacher/student\nframework to harness the knowledge present in a large-scale non-annotated\ndataset and in an accurate but complex multi-stage teacher network to train a\nlightweight network for joint 2D/3D pose estimation. The teacher network also\nexploits the unlabeled data to generate both hard and soft labels useful in\nimproving the student predictions. The easily deployable network trained using\nthis effective self-supervision strategy performs on par with the teacher\nnetwork on \\emph{MVOR+}, an extension of the public MVOR dataset where all\npersons have been fully annotated, thus providing a viable solution for\nreal-time 2D/3D human pose estimation in the OR.",
          "link": "http://arxiv.org/abs/2007.08354",
          "publishedOn": "2021-08-23T01:36:35.603Z",
          "wordCount": 645,
          "title": "Self-supervision on Unlabelled OR Data for Multi-person 2D/3D Human Pose Estimation. (arXiv:2007.08354v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>",
          "description": "Convolutional neural network has made remarkable achievements in\nclassification of idealized point cloud, however, non-idealized point cloud\nclassification is still a challenging task. In this paper, DNDFN, namely,\nDual-Neighborhood Deep Fusion Network, is proposed to deal with this problem.\nDNDFN has two key points. One is combination of local neighborhood and global\nneigh-borhood. nearest neighbor (kNN) or ball query can capture the local\nneighborhood but ignores long-distance dependencies. A trainable neighborhood\nlearning meth-od called TN-Learning is proposed, which can capture the global\nneighborhood. TN-Learning is combined with them to obtain richer neighborhood\ninformation. The other is information transfer convolution (IT-Conv) which can\nlearn the structural information between two points and transfer features\nthrough it. Extensive exper-iments on idealized and non-idealized benchmarks\nacross four tasks verify DNDFN achieves the state of the arts.",
          "link": "http://arxiv.org/abs/2108.09228",
          "publishedOn": "2021-08-23T01:36:35.596Z",
          "wordCount": 571,
          "title": "Dual-Neighborhood Deep Fusion Network for Point Cloud Analysis. (arXiv:2108.09228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.10739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "While the depth of modern Convolutional Neural Networks (CNNs) surpasses that\nof the pioneering networks with a significant margin, the traditional way of\nappending supervision only over the final classifier and progressively\npropagating gradient flow upstream remains the training mainstay. Seminal\nDeeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of\noptimization arising from gradient flow through a long chain. However, it is\nstill vulnerable to issues including interference to the hierarchical\nrepresentation generation process and inconsistent optimization objectives, as\nillustrated theoretically and empirically in this paper. Complementary to\nprevious training strategies, we propose Dynamic Hierarchical Mimicking, a\ngeneric feature learning mechanism, to advance CNN training with enhanced\ngeneralization ability. Partially inspired by DSN, we fork delicately designed\nside branches from the intermediate layers of a given neural network. Each\nbranch can emerge from certain locations of the main branch dynamically, which\nnot only retains representation rooted in the backbone network but also\ngenerates more diverse representations along its own pathway. We go one step\nfurther to promote multi-level interactions among different branches through an\noptimization formula with probabilistic prediction matching losses, thus\nguaranteeing a more robust optimization process and better representation\nability. Experiments on both category and instance recognition tasks\ndemonstrate the substantial improvements of our proposed method over its\ncorresponding counterparts using diverse state-of-the-art CNN architectures.\nCode and models are publicly available at https://github.com/d-li14/DHM",
          "link": "http://arxiv.org/abs/2003.10739",
          "publishedOn": "2021-08-23T01:36:35.589Z",
          "wordCount": 694,
          "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives. (arXiv:2003.10739v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15045",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Deep convolutional neural networks (CNNs) for video denoising are typically\ntrained with supervision, assuming the availability of clean videos. However,\nin many applications, such as microscopy, noiseless videos are not available.\nTo address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN\narchitecture designed to be trained exclusively with noisy data. The\nperformance of UDVD is comparable to the supervised state-of-the-art, even when\ntrained only on a single short noisy video. We demonstrate the promise of our\napproach in real-world imaging applications by denoising raw video,\nfluorescence-microscopy and electron-microscopy data. In contrast to many\ncurrent approaches to video denoising, UDVD does not require explicit motion\ncompensation. This is advantageous because motion compensation is\ncomputationally expensive, and can be unreliable when the input data are noisy.\nA gradient-based analysis reveals that UDVD automatically adapts to local\nmotion in the input noisy videos. Thus, the network learns to perform implicit\nmotion compensation, even though it is only trained for denoising.",
          "link": "http://arxiv.org/abs/2011.15045",
          "publishedOn": "2021-08-23T01:36:35.564Z",
          "wordCount": 672,
          "title": "Unsupervised Deep Video Denoising. (arXiv:2011.15045v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sparkman_J/0/1/0/all/0/1\">Jake Sparkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ayrot_A/0/1/0/all/0/1\">Abdalla Al-Ayrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_U/0/1/0/all/0/1\">Utkarsh Contractor</a>",
          "description": "Deep Neural Networks have been very successfully used for many computer\nvision and pattern recognition applications. While Convolutional Neural\nNetworks(CNNs) have shown the path to state of art image classifications,\nGenerative Adversarial Networks or GANs have provided state of art capabilities\nin image generation. In this paper we extend the applications of CNNs and GANs\nto experiment with up-sampling techniques in the domains of security and\nsurveillance. Through this work we evaluate, compare and contrast the state of\nart techniques in both CNN and GAN based image and video up-sampling in the\nsurveillance domain. As a result of this study we also provide experimental\nevidence to establish DISTS as a stronger Image Quality Assessment(IQA) metric\nfor comparing GAN Based Image Up-sampling in the surveillance domain.",
          "link": "http://arxiv.org/abs/2108.09285",
          "publishedOn": "2021-08-23T01:36:35.553Z",
          "wordCount": 562,
          "title": "Zoom, Enhance! Measuring Surveillance GAN Up-sampling. (arXiv:2108.09285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hutschenreiter_L/0/1/0/all/0/1\">Lisa Hutschenreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1\">Stefan Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feineis_L/0/1/0/all/0/1\">Lorenz Feineis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmuller_D/0/1/0/all/0/1\">Dagmar Kainm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1\">Bogdan Savchynskyy</a>",
          "description": "We contribute to approximate algorithms for the quadratic assignment problem\nalso known as graph matching. Inspired by the success of the fusion moves\ntechnique developed for multilabel discrete Markov random fields, we\ninvestigate its applicability to graph matching. In particular, we show how\nfusion moves can be efficiently combined with the dedicated state-of-the-art\ndual methods that have recently shown superior results in computer vision and\nbio-imaging applications. As our empirical evaluation on a wide variety of\ngraph matching datasets suggests, fusion moves significantly improve\nperformance of these methods in terms of speed and quality of the obtained\nsolutions. Our method sets a new state-of-the-art with a notable margin with\nrespect to its competitors.",
          "link": "http://arxiv.org/abs/2101.12085",
          "publishedOn": "2021-08-23T01:36:35.545Z",
          "wordCount": 617,
          "title": "Fusion Moves for Graph Matching. (arXiv:2101.12085v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_K/0/1/0/all/0/1\">Kamyar Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_Shirazi_K/0/1/0/all/0/1\">Kamaledin Ghiasi-Shirazi</a>",
          "description": "Convolutional neural networks (CNNs) have become the state-of-the-art tool\nfor dealing with unsolved problems in computer vision and image processing.\nSince the convolution operator is a linear operator, several generalizations\nhave been proposed to improve the performance of CNNs. One way to increase the\ncapability of the convolution operator is by applying activation functions on\nthe inner product operator. In this paper, we will introduce PowerLinear\nactivation functions, which are based on the polynomial kernel generalization\nof the convolution operator. EvenPowLin functions are the main branch of the\nPowerLinear activation functions. This class of activation functions is\nsaturated neither in the positive input region nor in the negative one. Also,\nthe negative inputs are activated with the same magnitude as the positive\ninputs. These features made the EvenPowLin activation functions able to be\nutilized in the first layer of CNN architectures and learn complex features of\ninput images. Additionally, EvenPowLin activation functions are used in CNN\nmodels to classify the inversion of grayscale images as accurately as the\noriginal grayscale images, which is significantly better than commonly used\nactivation functions.",
          "link": "http://arxiv.org/abs/2108.09256",
          "publishedOn": "2021-08-23T01:36:35.520Z",
          "wordCount": 624,
          "title": "PowerLinear Activation Functions with application to the first layer of CNNs. (arXiv:2108.09256v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09266",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Ramos_A/0/1/0/all/0/1\">A. Asensio Ramos</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Baso_C/0/1/0/all/0/1\">C. Diaz Baso</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kochukhov_O/0/1/0/all/0/1\">O. Kochukhov</a>",
          "description": "The non-uniform surface temperature distribution of rotating active stars is\nroutinely mapped with the Doppler Imaging technique. Inhomogeneities in the\nsurface produce features in high-resolution spectroscopic observations that\nshift in wavelength depending on their position on the visible hemisphere. The\ninversion problem has been systematically solved using maximum a-posteriori\nregularized methods assuming smoothness or maximum entropy. Our aim in this\nwork is to solve the full Bayesian inference problem, by providing access to\nthe posterior distribution of the surface temperature in the star. We use\namortized neural posterior estimation to produce a model that approximates the\nhigh-dimensional posterior distribution for spectroscopic observations of\nselected spectral ranges sampled at arbitrary rotation phases. The posterior\ndistribution is approximated with conditional normalizing flows, which are\nflexible, tractable and easy to sample approximations to arbitrary\ndistributions. When conditioned on the spectroscopic observations, they provide\na very efficient way of obtaining samples from the posterior distribution. The\nconditioning on observations is obtained through the use of Transformer\nencoders, which can deal with arbitrary wavelength sampling and rotation\nphases. Our model can produce thousands of posterior samples per second. Our\nvalidation of the model for very high signal-to-noise observations shows that\nit correctly approximates the posterior, although with some overestimation of\nthe broadening. We apply the model to the moderately fast rotator II Peg,\nproducing the first Bayesian map of its temperature inhomogenities. We conclude\nthat conditional normalizing flows are a very promising tool to carry out\napproximate Bayesian inference in more complex problems in stellar physics,\nlike constraining the magnetic properties.",
          "link": "http://arxiv.org/abs/2108.09266",
          "publishedOn": "2021-08-23T01:36:35.506Z",
          "wordCount": 707,
          "title": "Approximate Bayesian Neural Doppler Imaging. (arXiv:2108.09266v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_R/0/1/0/all/0/1\">Ryo Kawasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1\">Hajime Nagahara</a>",
          "description": "Explainable artificial intelligence has been gaining attention in the past\nfew years. However, most existing methods are based on gradients or\nintermediate features, which are not directly involved in the decision-making\nprocess of the classifier. In this paper, we propose a slot attention-based\nclassifier called SCOUTER for transparent yet accurate classification. Two\nmajor differences from other attention-based methods include: (a) SCOUTER's\nexplanation is involved in the final confidence for each category, offering\nmore intuitive interpretation, and (b) all the categories have their\ncorresponding positive or negative explanation, which tells \"why the image is\nof a certain category\" or \"why the image is not of a certain category.\" We\ndesign a new loss tailored for SCOUTER that controls the model's behavior to\nswitch between positive and negative explanations, as well as the size of\nexplanatory regions. Experimental results show that SCOUTER can give better\nvisual explanations in terms of various metrics while keeping good accuracy on\nsmall and medium-sized datasets.",
          "link": "http://arxiv.org/abs/2009.06138",
          "publishedOn": "2021-08-23T01:36:35.489Z",
          "wordCount": 645,
          "title": "SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition. (arXiv:2009.06138v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poels_Y/0/1/0/all/0/1\">Yoeri Poels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>",
          "description": "The goal of a classification model is to assign the correct labels to data.\nIn most cases, this data is not fully described by the given set of labels.\nOften a rich set of meaningful concepts exist in the domain that can much more\nprecisely describe each datapoint. Such concepts can also be highly useful for\ninterpreting the model's classifications. In this paper we propose a model,\ndenoted as Variational Autoencoder-based Contrastive Explanation (VAE-CE), that\nrepresents data with high-level concepts and uses this representation for both\nclassification and generating explanations. The explanations are produced in a\ncontrastive manner, conveying why a datapoint is assigned to one class rather\nthan an alternative class. An explanation is specified as a set of\ntransformations of the input datapoint, with each step depicting a concept\nchanging towards the contrastive class. We build the model using a disentangled\nVAE, extended with a new supervised method for disentangling individual\ndimensions. An analysis on synthetic data and MNIST shows that the approaches\nto both disentanglement and explanation provide benefits over other methods.",
          "link": "http://arxiv.org/abs/2108.09159",
          "publishedOn": "2021-08-23T01:36:35.476Z",
          "wordCount": 610,
          "title": "VAE-CE: Visual Contrastive Explanation using Disentangled VAEs. (arXiv:2108.09159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1808.08180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1\">Thibaut Issenhuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadkhodamohammadi_A/0/1/0/all/0/1\">Abdolrahim Kadkhodamohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathelin_M/0/1/0/all/0/1\">Michel de Mathelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>",
          "description": "Person detection and pose estimation is a key requirement to develop\nintelligent context-aware assistance systems. To foster the development of\nhuman pose estimation methods and their applications in the Operating Room\n(OR), we release the Multi-View Operating Room (MVOR) dataset, the first public\ndataset recorded during real clinical interventions. It consists of 732\nsynchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR.\nIt also includes the visual challenges present in such environments, such as\nocclusions and clutter. We provide camera calibration parameters, color and\ndepth frames, human bounding boxes, and 2D/3D pose annotations. In this paper,\nwe present the dataset, its annotations, as well as baseline results from\nseveral recent person detection and 2D/3D pose estimation methods. Since we\nneed to blur some parts of the images to hide identity and nudity in the\nreleased dataset, we also present a comparative study of how the baselines have\nbeen impacted by the blurring. Results show a large margin for improvement and\nsuggest that the MVOR dataset can be useful to compare the performance of the\ndifferent methods.",
          "link": "http://arxiv.org/abs/1808.08180",
          "publishedOn": "2021-08-23T01:36:35.469Z",
          "wordCount": 694,
          "title": "MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation. (arXiv:1808.08180v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>",
          "description": "Human pose estimation (HPE) is a key building block for developing AI-based\ncontext-aware systems inside the operating room (OR). The 24/7 use of images\ncoming from cameras mounted on the OR ceiling can however raise concerns for\nprivacy, even in the case of depth images captured by RGB-D sensors. Being able\nto solely use low-resolution privacy-preserving images would address these\nconcerns and help scale up the computer-assisted approaches that rely on such\ndata to a larger number of ORs. In this paper, we introduce the problem of HPE\non low-resolution depth images and propose an end-to-end solution that\nintegrates a multi-scale super-resolution network with a 2D human pose\nestimation network. By exploiting intermediate feature-maps generated at\ndifferent super-resolution, our approach achieves body pose results on\nlow-resolution images (of size 64x48) that are on par with those of an approach\ntrained and tested on full resolution images (of size 640x480).",
          "link": "http://arxiv.org/abs/2007.08340",
          "publishedOn": "2021-08-23T01:36:35.451Z",
          "wordCount": 629,
          "title": "Human Pose Estimation on Privacy-Preserving Low-Resolution Depth Images. (arXiv:2007.08340v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a novel approach to automatic image colorization by imitating the\nimagination process of human experts. Our imagination module is designed to\ngenerate color images that are context-correlated with black-and-white photos.\nGiven a black-and-white image, our imagination module firstly extracts the\ncontext information, which is then used to synthesize colorful and diverse\nimages using a conditional image synthesis network (e.g., semantic image\nsynthesis model). We then design a colorization module to colorize the\nblack-and-white images with the guidance of imagination for photorealistic\ncolorization. Experimental results show that our work produces more colorful\nand diverse results than state-of-the-art image colorization methods. Our\nsource codes will be publicly available.",
          "link": "http://arxiv.org/abs/2108.09195",
          "publishedOn": "2021-08-23T01:36:35.444Z",
          "wordCount": 540,
          "title": "Towards Photorealistic Colorization by Imagination. (arXiv:2108.09195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Longkun Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>",
          "description": "The point cloud representation of an object can have a large geometric\nvariation in view of inconsistent data acquisition procedure, which thus leads\nto domain discrepancy due to diverse and uncontrollable shape representation\ncross datasets. To improve discrimination on unseen distribution of point-based\ngeometries in a practical and feasible perspective, this paper proposes a new\nmethod of geometry-aware self-training (GAST) for unsupervised domain\nadaptation of object point cloud classification. Specifically, this paper aims\nto learn a domain-shared representation of semantic categories, via two novel\nself-supervised geometric learning tasks as feature regularization. On one\nhand, the representation learning is empowered by a linear mixup of point cloud\nsamples with their self-generated rotation labels, to capture a global\ntopological configuration of local geometries. On the other hand, a diverse\npoint distribution across datasets can be normalized with a novel\ncurvature-aware distortion localization. Experiments on the PointDA-10 dataset\nshow that our GAST method can significantly outperform the state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2108.09169",
          "publishedOn": "2021-08-23T01:36:35.437Z",
          "wordCount": 594,
          "title": "Geometry-Aware Self-Training for Unsupervised Domain Adaptationon Object Point Clouds. (arXiv:2108.09169v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Tomoyuki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1\">Antonio Tejero-de-Pablos</a>",
          "description": "Video ads segmentation and tagging is a challenging task due to two main\nreasons: (1) the video scene structure is complex and (2) it includes multiple\nmodalities (e.g., visual, audio, text.). While previous work focuses mostly on\nactivity videos (e.g. \"cooking\", \"sports\"), it is not clear how they can be\nleveraged to tackle the task of video ads content structuring. In this paper,\nwe propose a two-stage method that first provides the boundaries of the scenes,\nand then combines a confidence score for each segmented scene and the tag\nclasses predicted for that scene. We provide extensive experimental results on\nthe network architectures and modalities used for the proposed method. Our\ncombined method improves the previous baselines on the challenging \"Tencent\nAdvertisement Video\" dataset.",
          "link": "http://arxiv.org/abs/2108.09215",
          "publishedOn": "2021-08-23T01:36:35.430Z",
          "wordCount": 577,
          "title": "Video Ads Content Structuring by Combining Scene Confidence Prediction and Tagging. (arXiv:2108.09215v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laielli_M/0/1/0/all/0/1\">Michael Laielli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1\">Giscard Biamby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loeffler_A/0/1/0/all/0/1\">Adam Loeffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phat Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ross Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>",
          "description": "Active learning for object detection is conventionally achieved by applying\ntechniques developed for classification in a way that aggregates individual\ndetections into image-level selection criteria. This is typically coupled with\nthe costly assumption that every image selected for labelling must be\nexhaustively annotated. This yields incremental improvements on well-curated\nvision datasets and struggles in the presence of data imbalance and visual\nclutter that occurs in real-world imagery. Alternatives to the image-level\napproach are surprisingly under-explored in the literature. In this work, we\nintroduce a new strategy that subsumes previous Image-level and Object-level\napproaches into a generalized, Region-level approach that promotes\nspatial-diversity by avoiding nearby redundant queries from the same image and\nminimizes context-switching for the labeler. We show that this approach\nsignificantly decreases labeling effort and improves rare object search on\nrealistic data with inherent class-imbalance and cluttered scenes.",
          "link": "http://arxiv.org/abs/2108.09186",
          "publishedOn": "2021-08-23T01:36:35.423Z",
          "wordCount": 579,
          "title": "Region-level Active Learning for Cluttered Scenes. (arXiv:2108.09186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuejiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>",
          "description": "Learning socially-aware motion representations is at the core of recent\nadvances in multi-agent problems, such as human motion forecasting and robot\nnavigation in crowds. Despite promising progress, existing representations\nlearned with neural networks still struggle to generalize in closed-loop\npredictions (e.g., output colliding trajectories). This issue largely arises\nfrom the non-i.i.d. nature of sequential prediction in conjunction with\nill-distributed training data. Intuitively, if the training data only comes\nfrom human behaviors in safe spaces, i.e., from \"positive\" examples, it is\ndifficult for learning algorithms to capture the notion of \"negative\" examples\nlike collisions. In this work, we aim to address this issue by explicitly\nmodeling negative examples through self-supervision: (i) we introduce a social\ncontrastive loss that regularizes the extracted motion representation by\ndiscerning the ground-truth positive events from synthetic negative ones; (ii)\nwe construct informative negative samples based on our prior knowledge of rare\nbut dangerous circumstances. Our method substantially reduces the collision\nrates of recent trajectory forecasting, behavioral cloning and reinforcement\nlearning algorithms, outperforming state-of-the-art methods on several\nbenchmarks. Our code is available at https://github.com/vita-epfl/social-nce.",
          "link": "http://arxiv.org/abs/2012.11717",
          "publishedOn": "2021-08-23T01:36:35.416Z",
          "wordCount": 661,
          "title": "Social NCE: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2012.11717v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.12813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foehn_P/0/1/0/all/0/1\">Philipp Foehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brescianini_D/0/1/0/all/0/1\">Dario Brescianini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_E/0/1/0/all/0/1\">Elia Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cieslewski_T/0/1/0/all/0/1\">Titus Cieslewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>",
          "description": "This paper presents a novel system for autonomous, vision-based drone racing\ncombining learned data abstraction, nonlinear filtering, and time-optimal\ntrajectory planning. The system has successfully been deployed at the first\nautonomous drone racing world championship: the 2019 AlphaPilot Challenge.\nContrary to traditional drone racing systems, which only detect the next gate,\nour approach makes use of any visible gate and takes advantage of multiple,\nsimultaneous gate detections to compensate for drift in the state estimate and\nbuild a global map of the gates. The global map and drift-compensated state\nestimate allow the drone to navigate through the race course even when the\ngates are not immediately visible and further enable to plan a near\ntime-optimal path through the race course in real time based on approximate\ndrone dynamics. The proposed system has been demonstrated to successfully guide\nthe drone through tight race courses reaching speeds up to 8m/s and ranked\nsecond at the 2019 AlphaPilot Challenge.",
          "link": "http://arxiv.org/abs/2005.12813",
          "publishedOn": "2021-08-23T01:36:35.399Z",
          "wordCount": 672,
          "title": "AlphaPilot: Autonomous Drone Racing. (arXiv:2005.12813v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abualsaud_H/0/1/0/all/0/1\">Hala Abualsaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sean Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">David Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Situ_K/0/1/0/all/0/1\">Kenny Situ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "This study presents an approach to lane detection involving the prediction of\nbinary segmentation masks and per-pixel affinity fields. These affinity fields,\nalong with the binary masks, can then be used to cluster lane pixels\nhorizontally and vertically into corresponding lane instances in a\npost-processing step. This clustering is achieved through a simple row-by-row\ndecoding process with little overhead; such an approach allows LaneAF to detect\na variable number of lanes without assuming a fixed or maximum number of lanes.\nMoreover, this form of clustering is more interpretable in comparison to\nprevious visual clustering approaches, and can be analyzed to identify and\ncorrect sources of error. Qualitative and quantitative results obtained on\npopular lane detection datasets demonstrate the model's ability to detect and\ncluster lanes effectively and robustly. Our proposed approach sets a new\nstate-of-the-art on the challenging CULane dataset and the recently introduced\nUnsupervised LLAMAS dataset.",
          "link": "http://arxiv.org/abs/2103.12040",
          "publishedOn": "2021-08-23T01:36:35.391Z",
          "wordCount": 636,
          "title": "LaneAF: Robust Multi-Lane Detection with Affinity Fields. (arXiv:2103.12040v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dendorfer_P/0/1/0/all/0/1\">Patrick Dendorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1\">Sven Elflein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>",
          "description": "Pedestrian trajectory prediction is challenging due to its uncertain and\nmultimodal nature. While generative adversarial networks can learn a\ndistribution over future trajectories, they tend to predict out-of-distribution\nsamples when the distribution of future trajectories is a mixture of multiple,\npossibly disconnected modes. To address this issue, we propose a\nmulti-generator model for pedestrian trajectory prediction. Each generator\nspecializes in learning a distribution over trajectories routing towards one of\nthe primary modes in the scene, while a second network learns a categorical\ndistribution over these generators, conditioned on the dynamics and scene\ninput. This architecture allows us to effectively sample from specialized\ngenerators and to significantly reduce the out-of-distribution samples compared\nto single generator methods.",
          "link": "http://arxiv.org/abs/2108.09274",
          "publishedOn": "2021-08-23T01:36:35.383Z",
          "wordCount": 564,
          "title": "MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction. (arXiv:2108.09274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheka_A/0/1/0/all/0/1\">Andrey Sheka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samun_V/0/1/0/all/0/1\">Victor Samun</a>",
          "description": "This paper proposes a method for estimating the head pose from a single\nimage. This estimation uses a neural network (NN) obtained in two stages. In\nthe first stage, we trained the base NN, which has one regression head and four\nregression via classification (RvC) heads. We build the ensemble of offsets\nusing small offsets of face bounding boxes. In the second stage, we perform\nknowledge distillation (KD) from the ensemble of offsets of the base NN into\nthe final NN with one RvC head. On the main test protocol, the use of the\noffset ensemble improves the results of the base NN, and the KD improves the\nresults from the offset ensemble. The KD improves the results by an average of\n7.7\\% compared to the non-ensemble version. The proposed NN on the main test\nprotocol improves the state-of-the-art result on AFLW2000 and approaches, with\nonly a minimal gap, the state-of-the-art result on BIWI. Our NN uses only head\npose data, but the previous state-of-the-art model also uses facial landmarks\nduring training. We have made publicly available trained NNs and face bounding\nboxes for the 300W-LP, AFLW, AFLW2000, and BIWI datasets. KD-ResNet152 has the\nbest results, and KD-ResNet18 has a better result on the AFLW2000 dataset than\nany previous method.",
          "link": "http://arxiv.org/abs/2108.09183",
          "publishedOn": "2021-08-23T01:36:35.372Z",
          "wordCount": 645,
          "title": "Knowledge Distillation from Ensemble of Offsets for Head Pose Estimation. (arXiv:2108.09183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DInnocente_A/0/1/0/all/0/1\">Antono D&#x27;Innocente</a>",
          "description": "Computer vision has flourished in recent years thanks to Deep Learning\nadvancements, fast and scalable hardware solutions and large availability of\nstructured image data. Convolutional Neural Networks trained on supervised\ntasks with backpropagation learn to extract meaningful representations from raw\npixels automatically, and surpass shallow methods in image understanding.\nThough convenient, data-driven feature learning is prone to dataset bias: a\nnetwork learns its parameters from training signals alone, and will usually\nperform poorly if train and test distribution differ. To alleviate this\nproblem, research on Domain Generalization (DG), Domain Adaptation (DA) and\ntheir variations is increasing. This thesis contributes to these research\ntopics by presenting novel and effective ways to solve the dataset bias problem\nin its various settings. We propose new frameworks for Domain Generalization\nand Domain Adaptation which make use of feature aggregation strategies and\nvisual transformations via data-augmentation and multi-task integration of\nself-supervision. We also design an algorithm that adapts an object detection\nmodel to any out of distribution sample at test time. With through\nexperimentation, we show how our proposed solutions outperform competitive\nstate-of-the-art approaches in established DG and DA benchmarks.",
          "link": "http://arxiv.org/abs/2108.09208",
          "publishedOn": "2021-08-23T01:36:35.350Z",
          "wordCount": 621,
          "title": "Exploring Data Aggregation and Transformations to Generalize across Visual Domains. (arXiv:2108.09208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbet_C/0/1/0/all/0/1\">Christian Abbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Studer_L/0/1/0/all/0/1\">Linda Studer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Andreas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_H/0/1/0/all/0/1\">Heather Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlobec_I/0/1/0/all/0/1\">Inti Zlobec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>",
          "description": "Supervised learning is constrained by the availability of labeled data, which\nare especially expensive to acquire in the field of digital pathology. Making\nuse of open-source data for pre-training or using domain adaptation can be a\nway to overcome this issue. However, pre-trained networks often fail to\ngeneralize to new test domains that are not distributed identically due to\nvariations in tissue stainings, types, and textures. Additionally, current\ndomain adaptation methods mainly rely on fully-labeled source datasets. In this\nwork, we propose SRA, which takes advantage of self-supervised learning to\nperform domain adaptation and removes the necessity of a fully-labeled source\ndataset. SRA can effectively transfer the discriminative knowledge obtained\nfrom a few labeled source domain's data to a new target domain without\nrequiring additional tissue annotations. Our method harnesses both domains'\nstructures by capturing visual similarity with intra-domain and cross-domain\nself-supervision. Moreover, we present a generalized formulation of our\napproach that allows the architecture to learn from multi-source domains. We\nshow that our proposed method outperforms baselines for domain adaptation of\ncolorectal tissue type classification and further validate our approach on our\nin-house clinical cohort. The code and models are available open-source:\nhttps://github.com/christianabbet/SRA.",
          "link": "http://arxiv.org/abs/2108.09178",
          "publishedOn": "2021-08-23T01:36:35.331Z",
          "wordCount": 653,
          "title": "Self-Rule to Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection. (arXiv:2108.09178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantinescu_A/0/1/0/all/0/1\">Angela Constantinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karin M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "Transparent objects, such as glass walls and doors, constitute architectural\nobstacles hindering the mobility of people with low vision or blindness. For\ninstance, the open space behind glass doors is inaccessible, unless it is\ncorrectly perceived and interacted with. However, traditional assistive\ntechnologies rarely cover the segmentation of these safety-critical transparent\nobjects. In this paper, we build a wearable system with a novel dual-head\nTransformer for Transparency (Trans4Trans) perception model, which can segment\ngeneral- and transparent objects. The two dense segmentation results are\nfurther combined with depth information in the system to help users navigate\nsafely and assist them to negotiate transparent obstacles. We propose a\nlightweight Transformer Parsing Module (TPM) to perform multi-scale feature\ninterpretation in the transformer-based decoder. Benefiting from TPM, the\ndouble decoders can perform joint learning from corresponding datasets to\npursue robustness, meanwhile maintain efficiency on a portable GPU, with\nnegligible calculation increase. The entire Trans4Trans model is constructed in\na symmetrical encoder-decoder architecture, which outperforms state-of-the-art\nmethods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining\nmIoU of 45.13% and 75.14%, respectively. Through a user study and various\npre-tests conducted in indoor and outdoor scenes, the usability and reliability\nof our assistive system have been extensively verified. Meanwhile, the\nTran4Trans model has outstanding performances on driving scene datasets. On\nCityscapes, ACDC, and DADA-seg datasets corresponding to common environments,\nadverse weather, and traffic accident scenarios, mIoU scores of 81.5%, 76.3%,\nand 39.2% are obtained, demonstrating its high efficiency and robustness for\nreal-world transportation applications.",
          "link": "http://arxiv.org/abs/2108.09174",
          "publishedOn": "2021-08-23T01:36:35.322Z",
          "wordCount": 713,
          "title": "Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance. (arXiv:2108.09174v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nodet_P/0/1/0/all/0/1\">Pierre Nodet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1\">Vincent Lemaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1\">Alexis Bondu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornuejols_A/0/1/0/all/0/1\">Antoine Cornu&#xe9;jols</a>",
          "description": "In this paper we show that the combination of a Contrastive representation\nwith a label noise-robust classification head requires fine-tuning the\nrepresentation in order to achieve state-of-the-art performances. Since\nfine-tuned representations are shown to outperform frozen ones, one can\nconclude that noise-robust classification heads are indeed able to promote\nmeaningful representations if provided with a suitable starting point.\nExperiments are conducted to draw a comprehensive picture of performances by\nfeaturing six methods and nine noise instances of three different kinds (none,\nsymmetric, and asymmetric). In presence of noise the experiments show that fine\ntuning of Contrastive representation allows the six methods to achieve better\nresults than end-to-end learning and represent a new reference compare to the\nrecent state of art. Results are also remarkable stable versus the noise level.",
          "link": "http://arxiv.org/abs/2108.09154",
          "publishedOn": "2021-08-23T01:36:35.237Z",
          "wordCount": 563,
          "title": "Contrastive Representations for Label Noise Require Fine-Tuning. (arXiv:2108.09154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>",
          "description": "The adversarial patch attack against image classification models aims to\ninject adversarially crafted pixels within a localized restricted image region\n(i.e., a patch) for inducing model misclassification. This attack can be\nrealized in the physical world by printing and attaching the patch to the\nvictim object and thus imposes a real-world threat to computer vision systems.\nTo counter this threat, we propose PatchCleanser as a certifiably robust\ndefense against adversarial patches that is compatible with any image\nclassifier. In PatchCleanser, we perform two rounds of pixel masking on the\ninput image to neutralize the effect of the adversarial patch. In the first\nround of masking, we apply a set of carefully generated masks to the input\nimage and evaluate the model prediction on every masked image. If model\npredictions on all one-masked images reach a unanimous agreement, we output the\nagreed prediction label. Otherwise, we perform a second round of masking to\nsettle the disagreement, in which we evaluate model predictions on two-masked\nimages to robustly recover the correct prediction label. Notably, we can prove\nthat our defense will always make correct predictions on certain images against\nany adaptive white-box attacker within our threat model, achieving certified\nrobustness. We extensively evaluate our defense on the ImageNet, ImageNette,\nCIFAR-10, CIFAR-100, SVHN, and Flowers-102 datasets and demonstrate that our\ndefense achieves similar clean accuracy as state-of-the-art classification\nmodels and also significantly improves certified robustness from prior works.\nNotably, our defense can achieve 83.8% top-1 clean accuracy and 60.4% top-1\ncertified robust accuracy against a 2%-pixel square patch anywhere on the\n1000-class ImageNet dataset.",
          "link": "http://arxiv.org/abs/2108.09135",
          "publishedOn": "2021-08-23T01:36:35.203Z",
          "wordCount": 706,
          "title": "PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier. (arXiv:2108.09135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Abundant real-world data can be naturally represented by large-scale\nnetworks, which demands efficient and effective learning algorithms. At the\nsame time, labels may only be available for some networks, which demands these\nalgorithms to be able to adapt to unlabeled networks. Domain-adaptive hash\nlearning has enjoyed considerable success in the computer vision community in\nmany practical tasks due to its lower cost in both retrieval time and storage\nfootprint. However, it has not been applied to multiple-domain networks. In\nthis work, we bridge this gap by developing an unsupervised domain-adaptive\nhash learning method for networks, dubbed UDAH. Specifically, we develop four\n{task-specific yet correlated} components: (1) network structure preservation\nvia a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,\n(3) cross-domain intersected discriminators, and (4) semantic center alignment.\nWe conduct a wide range of experiments to evaluate the effectiveness and\nefficiency of our method on a range of tasks including link prediction, node\nclassification, and neighbor recommendation. Our evaluation results demonstrate\nthat our model achieves better performance than the state-of-the-art\nconventional discrete embedding methods over all the tasks.",
          "link": "http://arxiv.org/abs/2108.09136",
          "publishedOn": "2021-08-23T01:36:35.196Z",
          "wordCount": 614,
          "title": "Unsupervised Domain-adaptive Hash for Networks. (arXiv:2108.09136v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>",
          "description": "Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.",
          "link": "http://arxiv.org/abs/2108.09151",
          "publishedOn": "2021-08-23T01:36:35.175Z",
          "wordCount": 711,
          "title": "Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>",
          "description": "In Digital Holography (DH), it is crucial to extract the object distance from\na hologram in order to reconstruct its amplitude and phase. This step is called\nauto-focusing and it is conventionally solved by first reconstructing a stack\nof images and then by sharpening each reconstructed image using a focus metric\nsuch as entropy or variance. The distance corresponding to the sharpest image\nis considered the focal position. This approach, while effective, is\ncomputationally demanding and time-consuming. In this paper, the determination\nof the distance is performed by Deep Learning (DL). Two deep learning (DL)\narchitectures are compared: Convolutional Neural Network (CNN)and Visual\ntransformer (ViT). ViT and CNN are used to cope with the problem of\nauto-focusing as a classification problem. Compared to a first attempt [11] in\nwhich the distance between two consecutive classes was 100{\\mu}m, our proposal\nallows us to drastically reduce this distance to 1{\\mu}m. Moreover, ViT reaches\nsimilar accuracy and is more robust than CNN.",
          "link": "http://arxiv.org/abs/2108.09147",
          "publishedOn": "2021-08-23T01:36:35.167Z",
          "wordCount": 611,
          "title": "Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography. (arXiv:2108.09147v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atzmon_M/0/1/0/all/0/1\">Matan Atzmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Implicit neural representation is a recent approach to learn shape\ncollections as zero level-sets of neural networks, where each shape is\nrepresented by a latent code. So far, the focus has been shape reconstruction,\nwhile shape generalization was mostly left to generic encoder-decoder or\nauto-decoder regularization.\n\nIn this paper we advocate deformation-aware regularization for implicit\nneural representations, aiming at producing plausible deformations as latent\ncode changes. The challenge is that implicit representations do not capture\ncorrespondences between different shapes, which makes it difficult to represent\nand regularize their deformations. Thus, we propose to pair the implicit\nrepresentation of the shapes with an explicit, piecewise linear deformation\nfield, learned as an auxiliary function. We demonstrate that, by regularizing\nthese deformation fields, we can encourage the implicit neural representation\nto induce natural deformations in the learned shape space, such as\nas-rigid-as-possible deformations.",
          "link": "http://arxiv.org/abs/2108.08931",
          "publishedOn": "2021-08-23T01:36:35.161Z",
          "wordCount": 584,
          "title": "Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields. (arXiv:2108.08931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_Z/0/1/0/all/0/1\">Zakaria Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1\">Iaroslav Melekhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>",
          "description": "For several emerging technologies such as augmented reality, autonomous\ndriving and robotics, visual localization is a critical component. Directly\nregressing camera pose/3D scene coordinates from the input image using deep\nneural networks has shown great potential. However, such methods assume a\nstationary data distribution with all scenes simultaneously available during\ntraining. In this paper, we approach the problem of visual localization in a\ncontinual learning setup -- whereby the model is trained on scenes in an\nincremental manner. Our results show that similar to the classification domain,\nnon-stationary data induces catastrophic forgetting in deep networks for visual\nlocalization. To address this issue, a strong baseline based on storing and\nreplaying images from a fixed buffer is proposed. Furthermore, we propose a new\nsampling method based on coverage score (Buff-CS) that adapts the existing\nsampling strategies in the buffering process to the problem of visual\nlocalization. Results demonstrate consistent improvements over standard\nbuffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also\n19Scenes by combining the former scenes.",
          "link": "http://arxiv.org/abs/2108.09112",
          "publishedOn": "2021-08-23T01:36:35.154Z",
          "wordCount": 604,
          "title": "Continual Learning for Image-Based Camera Localization. (arXiv:2108.09112v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Limeng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>",
          "description": "Few-shot object detection, which aims at detecting novel objects rapidly from\nextremely few annotated examples of previously unseen classes, has attracted\nsignificant research interest in the community. Most existing approaches employ\nthe Faster R-CNN as basic detection framework, yet, due to the lack of tailored\nconsiderations for data-scarce scenario, their performance is often not\nsatisfactory. In this paper, we look closely into the conventional Faster R-CNN\nand analyze its contradictions from two orthogonal perspectives, namely\nmulti-stage (RPN vs. RCNN) and multi-task (classification vs. localization). To\nresolve these issues, we propose a simple yet effective architecture, named\nDecoupled Faster R-CNN (DeFRCN). To be concrete, we extend Faster R-CNN by\nintroducing Gradient Decoupled Layer for multi-stage decoupling and\nPrototypical Calibration Block for multi-task decoupling. The former is a novel\ndeep layer with redefining the feature-forward operation and gradient-backward\noperation for decoupling its subsequent layer and preceding layer, and the\nlatter is an offline prototype-based classification model with taking the\nproposals from detector as input and boosting the original classification\nscores with additional pairwise scores for calibration. Extensive experiments\non multiple benchmarks show our framework is remarkably superior to other\nexisting approaches and establishes a new state-of-the-art in few-shot\nliterature.",
          "link": "http://arxiv.org/abs/2108.09017",
          "publishedOn": "2021-08-23T01:36:35.146Z",
          "wordCount": 642,
          "title": "DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection. (arXiv:2108.09017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhipeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1\">Ozan Sener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "Continual learning is the problem of learning and retaining knowledge through\ntime over multiple tasks and environments. Research has primarily focused on\nthe incremental classification setting, where new tasks/classes are added at\ndiscrete time intervals. Such an \"offline\" setting does not evaluate the\nability of agents to learn effectively and efficiently, since an agent can\nperform multiple learning epochs without any time limitation when a task is\nadded. We argue that \"online\" continual learning, where data is a single\ncontinuous stream without task boundaries, enables evaluating both information\nretention and online learning efficacy. In online continual learning, each\nincoming small batch of data is first used for testing and then added to the\ntraining set, making the problem truly online. Trained models are later\nevaluated on historical data to assess information retention. We introduce a\nnew benchmark for online continual visual learning that exhibits large scale\nand natural distribution shifts. Through a large-scale analysis, we identify\ncritical and previously unobserved phenomena of gradient-based optimization in\ncontinual learning, and propose effective strategies for improving\ngradient-based online continual learning with real data. The source code and\ndataset are available in: https://github.com/IntelLabs/continuallearning.",
          "link": "http://arxiv.org/abs/2108.09020",
          "publishedOn": "2021-08-23T01:36:35.139Z",
          "wordCount": 640,
          "title": "Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meissner_R/0/1/0/all/0/1\">Roy Meissner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhland_C/0/1/0/all/0/1\">Claudia Ruhland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihsberner_K/0/1/0/all/0/1\">Katja Ihsberner</a>",
          "description": "In this article, we present a concept of how micro- and e-assessments can be\nused for the mathematical domain to automatically determine acquired and\nmissing individual skills and, based on these information, guide individuals to\nacquire missing or additional skills in a software-supported process. The\nmodels required for this concept are a digitally prepared and annotated\ne-assessment item pool, a digital modeling of the domain that includes topics,\nnecessary competencies, as well as introductory and continuative material, as\nwell as a digital individual model, which can reliably record competencies and\nintegrates aspects about the loss of such.",
          "link": "http://arxiv.org/abs/2108.09072",
          "publishedOn": "2021-08-23T01:36:35.116Z",
          "wordCount": 543,
          "title": "Kompetenzerwerbsf\\\"orderung durch E-Assessment: Individuelle Kompetenzerfassung am Beispiel des Fachs Mathematik. (arXiv:2108.09072v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sussmilch_M/0/1/0/all/0/1\">Marius S&#xfc;&#xdf;milch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Sushma Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "Face morphing attacks aim at creating face images that are verifiable to be\nthe face of multiple identities, which can lead to building faulty identity\nlinks in operations like border checks. While creating a morphed face detector\n(MFD), training on all possible attack types is essential to achieve good\ndetection performance. Therefore, investigating new methods of creating\nmorphing attacks drives the generalizability of MADs. Creating morphing attacks\nwas performed on the image level, by landmark interpolation, or on the\nlatent-space level, by manipulating latent vectors in a generative adversarial\nnetwork. The earlier results in varying blending artifacts and the latter\nresults in synthetic-like striping artifacts. This work presents the novel\nmorphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using\na GAN-based generation, as well as, eliminate the manipulation in the latent\nspace, resulting in visibly realistic morphed images compared to previous\nworks. The generated ReGenMorph appearance is compared to recent morphing\napproaches and evaluated for face recognition vulnerability and attack\ndetectability, whether as known or unknown attacks.",
          "link": "http://arxiv.org/abs/2108.09130",
          "publishedOn": "2021-08-23T01:36:35.110Z",
          "wordCount": 621,
          "title": "ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by Attack Re-generation. (arXiv:2108.09130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majhi_S/0/1/0/all/0/1\">Snehashis Majhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_R/0/1/0/all/0/1\">Ratnakar Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_P/0/1/0/all/0/1\">Pankaj Kumar Sa</a>",
          "description": "Anomaly activities such as robbery, explosion, accidents, etc. need immediate\nactions for preventing loss of human life and property in real world\nsurveillance systems. Although the recent automation in surveillance systems\nare capable of detecting the anomalies, but they still need human efforts for\ncategorizing the anomalies and taking necessary preventive actions. This is due\nto the lack of methodology performing both anomaly detection and classification\nfor real world scenarios. Thinking of a fully automatized surveillance system,\nwhich is capable of both detecting and classifying the anomalies that need\nimmediate actions, a joint anomaly detection and classification method is a\npressing need. The task of joint detection and classification of anomalies\nbecomes challenging due to the unavailability of dense annotated videos\npertaining to anomalous classes, which is a crucial factor for training modern\ndeep architecture. Furthermore, doing it through manual human effort seems\nimpossible. Thus, we propose a method that jointly handles the anomaly\ndetection and classification in a single framework by adopting a\nweakly-supervised learning paradigm. In weakly-supervised learning instead of\ndense temporal annotations, only video-level labels are sufficient for\nlearning. The proposed model is validated on a large-scale publicly available\nUCF-Crime dataset, achieving state-of-the-art results.",
          "link": "http://arxiv.org/abs/2108.08996",
          "publishedOn": "2021-08-23T01:36:35.102Z",
          "wordCount": 646,
          "title": "Weakly-supervised Joint Anomaly Detection and Classification. (arXiv:2108.08996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiacheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiacheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Recently, some works found an interesting phenomenon that adversarially\nrobust classifiers can generate good images comparable to generative models. We\ninvestigate this phenomenon from an energy perspective and provide a novel\nexplanation. We reformulate adversarial example generation, adversarial\ntraining, and image generation in terms of an energy function. We find that\nadversarial training contributes to obtaining an energy function that is flat\nand has low energy around the real data, which is the key for generative\ncapability. Based on our new understanding, we further propose a better\nadversarial training method, Joint Energy Adversarial Training (JEAT), which\ncan generate high-quality images and achieve new state-of-the-art robustness\nunder a wide range of attacks. The Inception Score of the images (CIFAR-10)\ngenerated by JEAT is 8.80, much better than original robust classifiers (7.50).\nIn particular, we achieve new state-of-the-art robustness on CIFAR-10 (from\n57.20% to 62.04%) and CIFAR-100 (from 30.03% to 30.18%) without extra training\ndata.",
          "link": "http://arxiv.org/abs/2108.09093",
          "publishedOn": "2021-08-23T01:36:35.096Z",
          "wordCount": 602,
          "title": "Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (arXiv:2108.09093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tri Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1\">Aiden Nibali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>",
          "description": "Medical image classification is often challenging for two reasons: a lack of\nlabelled examples due to expensive and time-consuming annotation protocols, and\nimbalanced class labels due to the relative scarcity of disease-positive\nindividuals in the wider population. Semi-supervised learning (SSL) methods\nexist for dealing with a lack of labels, but they generally do not address the\nproblem of class imbalance. In this study we propose Adaptive Blended\nConsistency Loss (ABCL), a drop-in replacement for consistency loss in\nperturbation-based SSL methods. ABCL counteracts data skew by adaptively mixing\nthe target class distribution of the consistency loss in accordance with class\nfrequency. Our experiments with ABCL reveal improvements to unweighted average\nrecall on two different imbalanced medical image classification datasets when\ncompared with existing consistency losses that are not designed to counteract\nclass imbalance.",
          "link": "http://arxiv.org/abs/2108.08956",
          "publishedOn": "2021-08-23T01:36:35.089Z",
          "wordCount": 584,
          "title": "Semi-supervised learning for medical image classification using imbalanced training data. (arXiv:2108.08956v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Attrish_A/0/1/0/all/0/1\">Aman Attrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharat_N/0/1/0/all/0/1\">Nagasai Bharat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_V/0/1/0/all/0/1\">Vijay Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhangad_V/0/1/0/all/0/1\">Vivek Kanhangad</a>",
          "description": "Fingerprints are one of the most widely explored biometric traits.\nSpecifically, contact-based fingerprint recognition systems reign supreme due\nto their robustness, portability and the extensive research work done in the\nfield. However, these systems suffer from issues such as hygiene, sensor\ndegradation due to constant physical contact, and latent fingerprint threats.\nIn this paper, we propose an approach for developing a contactless fingerprint\nrecognition system that captures finger photo from a distance using an image\nsensor in a suitable environment. The captured finger photos are then processed\nfurther to obtain global and local (minutiae-based) features. Specifically, a\nSiamese convolutional neural network (CNN) is designed to extract global\nfeatures from a given finger photo. The proposed system computes matching\nscores from CNN-based features and minutiae-based features. Finally, the two\nscores are fused to obtain the final matching score between the probe and\nreference fingerprint templates. Most importantly, the proposed system is\ndeveloped using the Nvidia Jetson Nano development kit, which allows us to\nperform contactless fingerprint recognition in real-time with minimum latency\nand acceptable matching accuracy. The performance of the proposed system is\nevaluated on an in-house IITI contactless fingerprint dataset (IITI-CFD)\ncontaining 105train and 100 test subjects. The proposed system achieves an\nequal-error-rate of 2.19% on IITI-CFD.",
          "link": "http://arxiv.org/abs/2108.09048",
          "publishedOn": "2021-08-23T01:36:35.070Z",
          "wordCount": 645,
          "title": "A Contactless Fingerprint Recognition System. (arXiv:2108.09048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1\">Kaustubh Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1\">N. Sai Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1\">Krishna Murthy Jatavallabhula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>",
          "description": "Given an image or a video captured from a monocular camera, amodal layout\nestimation is the task of predicting semantics and occupancy in bird's eye\nview. The term amodal implies we also reason about entities in the scene that\nare occluded or truncated in image space. While several recent efforts have\ntackled this problem, there is a lack of standardization in task specification,\ndatasets, and evaluation protocols. We address these gaps with AutoLay, a\ndataset and benchmark for amodal layout estimation from monocular images.\nAutoLay encompasses driving imagery from two popular datasets: KITTI and\nArgoverse. In addition to fine-grained attributes such as lanes, sidewalks, and\nvehicles, we also provide semantically annotated 3D point clouds. We implement\nseveral baselines and bleeding edge approaches, and release our data and code.",
          "link": "http://arxiv.org/abs/2108.09047",
          "publishedOn": "2021-08-23T01:36:35.063Z",
          "wordCount": 580,
          "title": "AutoLay: Benchmarking amodal layout estimation for autonomous driving. (arXiv:2108.09047v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.",
          "link": "http://arxiv.org/abs/2108.09105",
          "publishedOn": "2021-08-23T01:36:35.056Z",
          "wordCount": 661,
          "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>",
          "description": "This paper proposes a novel deep learning approach for single image defocus\ndeblurring based on inverse kernels. In a defocused image, the blur shapes are\nsimilar among pixels although the blur sizes can spatially vary. To utilize the\nproperty with inverse kernels, we exploit the observation that when only the\nsize of a defocus blur changes while keeping the shape, the shape of the\ncorresponding inverse kernel remains the same and only the scale changes. Based\non the observation, we propose a kernel-sharing parallel atrous convolutional\n(KPAC) block specifically designed by incorporating the property of inverse\nkernels for single image defocus deblurring. To effectively simulate the\ninvariant shapes of inverse kernels with different scales, KPAC shares the same\nconvolutional weights among multiple atrous convolution layers. To efficiently\nsimulate the varying scales of inverse kernels, KPAC consists of only a few\natrous convolution layers with different dilations and learns per-pixel scale\nattentions to aggregate the outputs of the layers. KPAC also utilizes the shape\nattention to combine the outputs of multiple convolution filters in each atrous\nconvolution layer, to deal with defocus blur with a slightly varying shape. We\ndemonstrate that our approach achieves state-of-the-art performance with a much\nsmaller number of parameters than previous methods.",
          "link": "http://arxiv.org/abs/2108.09108",
          "publishedOn": "2021-08-23T01:36:35.050Z",
          "wordCount": 648,
          "title": "Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions. (arXiv:2108.09108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Warping-based video stabilizers smooth camera trajectory by constraining each\npixel's displacement and warp stabilized frames from unstable ones accordingly.\nHowever, since the view outside the boundary is not available during warping,\nthe resulting holes around the boundary of the stabilized frame must be\ndiscarded (i.e., cropping) to maintain visual consistency, and thus does leads\nto a tradeoff between stability and cropping ratio. In this paper, we make a\nfirst attempt to address this issue by proposing a new Out-of-boundary View\nSynthesis (OVS) method. By the nature of spatial coherence between adjacent\nframes and within each frame, OVS extrapolates the out-of-boundary view by\naligning adjacent frames to each reference one. Technically, it first\ncalculates the optical flow and propagates it to the outer boundary region\naccording to the affinity, and then warps pixels accordingly. OVS can be\nintegrated into existing warping-based stabilizers as a plug-and-play module to\nsignificantly improve the cropping ratio of the stabilized results. In\naddition, stability is improved because the jitter amplification effect caused\nby cropping and resizing is reduced. Experimental results on the NUS benchmark\nshow that OVS can improve the performance of five representative\nstate-of-the-art methods in terms of objective metrics and subjective visual\nquality. The code is publicly available at\nhttps://github.com/Annbless/OVS_Stabilization.",
          "link": "http://arxiv.org/abs/2108.09041",
          "publishedOn": "2021-08-23T01:36:35.042Z",
          "wordCount": 648,
          "title": "Out-of-boundary View Synthesis Towards Full-Frame Video Stabilization. (arXiv:2108.09041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eom_C/0/1/0/all/0/1\">Chanho Eom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "Video-based person re-identification (reID) aims to retrieve person videos\nwith the same identity as a query person across multiple cameras. Spatial and\ntemporal distractors in person videos, such as background clutter and partial\nocclusions over frames, respectively, make this task much more challenging than\nimage-based person reID. We observe that spatial distractors appear\nconsistently in a particular location, and temporal distractors show several\npatterns, e.g., partial occlusions occur in the first few frames, where such\npatterns provide informative cues for predicting which frames to focus on\n(i.e., temporal attentions). Based on this, we introduce a novel Spatial and\nTemporal Memory Networks (STMN). The spatial memory stores features for spatial\ndistractors that frequently emerge across video frames, while the temporal\nmemory saves attentions which are optimized for typical temporal patterns in\nperson videos. We leverage the spatial and temporal memories to refine\nframe-level person representations and to aggregate the refined frame-level\nfeatures into a sequence-level person representation, respectively, effectively\nhandling spatial and temporal distractors in person videos. We also introduce a\nmemory spread loss preventing our model from addressing particular items only\nin the memories. Experimental results on standard benchmarks, including MARS,\nDukeMTMC-VideoReID, and LS-VID, demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.09039",
          "publishedOn": "2021-08-23T01:36:35.036Z",
          "wordCount": 646,
          "title": "Video-based Person Re-identification with Spatial and Temporal Memory Networks. (arXiv:2108.09039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1\">Ranjie Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1\">A. K. Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>",
          "description": "Human can easily recognize visual objects with lost information: even losing\nmost details with only contour reserved, e.g. cartoon. However, in terms of\nvisual perception of Deep Neural Networks (DNNs), the ability for recognizing\nabstract objects (visual objects with lost information) is still a challenge.\nIn this work, we investigate this issue from an adversarial viewpoint: will the\nperformance of DNNs decrease even for the images only losing a little\ninformation? Towards this end, we propose a novel adversarial attack, named\n\\textit{AdvDrop}, which crafts adversarial examples by dropping existing\ninformation of images. Previously, most adversarial attacks add extra\ndisturbing information on clean images explicitly. Opposite to previous works,\nour proposed work explores the adversarial robustness of DNN models in a novel\nperspective by dropping imperceptible details to craft adversarial examples. We\ndemonstrate the effectiveness of \\textit{AdvDrop} by extensive experiments, and\nshow that this new type of adversarial examples is more difficult to be\ndefended by current defense systems.",
          "link": "http://arxiv.org/abs/2108.09034",
          "publishedOn": "2021-08-23T01:36:35.016Z",
          "wordCount": 619,
          "title": "AdvDrop: Adversarial Attack to DNNs by Dropping Information. (arXiv:2108.09034v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanyi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bodi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhiqiang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>",
          "description": "We present a novel semi-supervised semantic segmentation method which jointly\nachieves two desiderata of segmentation model regularities: the label-space\nconsistency property between image augmentations and the feature-space\ncontrastive property among different pixels. We leverage the pixel-level L2\nloss and the pixel contrastive loss for the two purposes respectively. To\naddress the computational efficiency issue and the false negative noise issue\ninvolved in the pixel contrastive loss, we further introduce and investigate\nseveral negative sampling techniques. Extensive experiments demonstrate the\nstate-of-the-art performance of our method (PC2Seg) with the DeepLab-v3+\narchitecture, in several challenging semi-supervised settings derived from the\nVOC, Cityscapes, and COCO datasets.",
          "link": "http://arxiv.org/abs/2108.09025",
          "publishedOn": "2021-08-23T01:36:35.009Z",
          "wordCount": 542,
          "title": "Pixel Contrastive-Consistent Semi-Supervised Semantic Segmentation. (arXiv:2108.09025v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1\">Christos Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1\">Johan Fredin Haslum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1\">Magnus S&#xf6;derberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>",
          "description": "Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis. Recently, vision\ntransformers (ViTs) have appeared as a competitive alternative to CNNs,\nyielding similar levels of performance while possessing several interesting\nproperties that could prove beneficial for medical imaging tasks. In this work,\nwe explore whether it is time to move to transformer-based models or if we\nshould keep working with CNNs - can we trivially switch to transformers? If so,\nwhat are the advantages and drawbacks of switching to ViTs for medical image\ndiagnosis? We consider these questions in a series of experiments on three\nmainstream medical image datasets. Our findings show that, while CNNs perform\nbetter when trained from scratch, off-the-shelf vision transformers using\ndefault hyperparameters are on par with CNNs when pretrained on ImageNet, and\noutperform their CNN counterparts when pretrained using self-supervision.",
          "link": "http://arxiv.org/abs/2108.09038",
          "publishedOn": "2021-08-23T01:36:34.994Z",
          "wordCount": 608,
          "title": "Is it Time to Replace CNNs with Transformers for Medical Images?. (arXiv:2108.09038v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09075",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Montanaro_A/0/1/0/all/0/1\">Antonio Montanaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1\">Diego Valsesia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fracastoro_G/0/1/0/all/0/1\">Giulia Fracastoro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1\">Enrico Magli</a>",
          "description": "Self-supervised learning techniques are gaining popularity due to their\ncapability of building models that are effective, even when scarce amounts of\nlabeled data are available. In this paper, we present a framework and specific\ntasks for self-supervised training of multichannel models, such as the fusion\nof multispectral and synthetic aperture radar images. We show that the proposed\nself-supervised approach is highly effective at learning features that\ncorrelate with the labels for land cover classification. This is enabled by an\nexplicit design of pretraining tasks which promotes bridging the gaps between\nsensing modalities and exploiting the spectral characteristics of the input.\nWhen limited labels are available, using the proposed self-supervised\npretraining and supervised finetuning for land cover classification with SAR\nand multispectral data outperforms conventional approaches such as purely\nsupervised learning, initialization from training on Imagenet and recent\nself-supervised approaches for computer vision tasks.",
          "link": "http://arxiv.org/abs/2108.09075",
          "publishedOn": "2021-08-23T01:36:34.987Z",
          "wordCount": 591,
          "title": "Self-supervised learning for joint SAR and multispectral land cover classification. (arXiv:2108.09075v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Jia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu-Xiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>",
          "description": "We present a method for creating 3D indoor scenes with a generative model\nlearned from a collection of semantic-segmented depth images captured from\ndifferent unknown scenes. Given a room with a specified size, our method\nautomatically generates 3D objects in a room from a randomly sampled latent\ncode. Different from existing methods that represent an indoor scene with the\ntype, location, and other properties of objects in the room and learn the scene\nlayout from a collection of complete 3D indoor scenes, our method models each\nindoor scene as a 3D semantic scene volume and learns a volumetric generative\nadversarial network (GAN) from a collection of 2.5D partial observations of 3D\nscenes. To this end, we apply a differentiable projection layer to project the\ngenerated 3D semantic scene volumes into semantic-segmented depth images and\ndesign a new multiple-view discriminator for learning the complete 3D scene\nvolume from 2.5D semantic-segmented depth images. Compared to existing methods,\nour method not only efficiently reduces the workload of modeling and acquiring\n3D scenes for training, but also produces better object shapes and their\ndetailed layouts in the scene. We evaluate our method with different indoor\nscene datasets and demonstrate the advantages of our method. We also extend our\nmethod for generating 3D indoor scenes from semantic-segmented depth images\ninferred from RGB images of real scenes.",
          "link": "http://arxiv.org/abs/2108.09022",
          "publishedOn": "2021-08-23T01:36:34.959Z",
          "wordCount": 660,
          "title": "Indoor Scene Generation from a Collection of Semantic-Segmented Depth Images. (arXiv:2108.09022v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qiaosi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qinyan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Faming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>",
          "description": "Single image deraining is important for many high-level computer vision tasks\nsince the rain streaks can severely degrade the visibility of images, thereby\naffecting the recognition and analysis of the image. Recently, many CNN-based\nmethods have been proposed for rain removal. Although these methods can remove\npart of the rain streaks, it is difficult for them to adapt to real-world\nscenarios and restore high-quality rain-free images with clear and accurate\nstructures. To solve this problem, we propose a Structure-Preserving Deraining\nNetwork (SPDNet) with RCP guidance. SPDNet directly generates high-quality\nrain-free images with clear and accurate structures under the guidance of RCP\nbut does not rely on any rain-generating assumptions. Specifically, we found\nthat the RCP of images contains more accurate structural information than rainy\nimages. Therefore, we introduced it to our deraining network to protect\nstructure information of the rain-free image. Meanwhile, a Wavelet-based\nMulti-Level Module (WMLM) is proposed as the backbone for learning the\nbackground information of rainy images and an Interactive Fusion Module (IFM)\nis designed to make full use of RCP information. In addition, an iterative\nguidance strategy is proposed to gradually improve the accuracy of RCP,\nrefining the result in a progressive path. Extensive experimental results on\nboth synthetic and real-world datasets demonstrate that the proposed model\nachieves new state-of-the-art results. Code: https://github.com/Joyies/SPDNet",
          "link": "http://arxiv.org/abs/2108.09079",
          "publishedOn": "2021-08-23T01:36:34.942Z",
          "wordCount": 656,
          "title": "Structure-Preserving Deraining with Residue Channel Prior Guidance. (arXiv:2108.09079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1\">Duarte Rondao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>",
          "description": "Autonomous spacecraft relative navigation technology has been planned for and\napplied to many famous space missions. The development of on-board electronics\nsystems has enabled the use of vision-based and LiDAR-based methods to achieve\nbetter performances. Meanwhile, deep learning has reached great success in\ndifferent areas, especially in computer vision, which has also attracted the\nattention of space researchers. However, spacecraft navigation differs from\nground tasks due to high reliability requirements but lack of large datasets.\nThis survey aims to systematically investigate the current deep learning-based\nautonomous spacecraft relative navigation methods, focusing on concrete orbital\napplications such as spacecraft rendezvous and landing on small bodies or the\nMoon. The fundamental characteristics, primary motivations, and contributions\nof deep learning-based relative navigation algorithms are first summarised from\nthree perspectives of spacecraft rendezvous, asteroid exploration, and terrain\nnavigation. Furthermore, popular visual tracking benchmarks and their\nrespective properties are compared and summarised. Finally, potential\napplications are discussed, along with expected impediments.",
          "link": "http://arxiv.org/abs/2108.08876",
          "publishedOn": "2021-08-23T01:36:34.921Z",
          "wordCount": 611,
          "title": "Deep Learning-based Spacecraft Relative Navigation Methods: A Survey. (arXiv:2108.08876v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Liquan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yufei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiuyu Zhu</a>",
          "description": "Most deep models for underwater image enhancement resort to training on\nsynthetic datasets based on underwater image formation models. Although\npromising performances have been achieved, they are still limited by two\nproblems: (1) existing underwater image synthesis models have an intrinsic\nlimitation, in which the homogeneous ambient light is usually randomly\ngenerated and many important dependencies are ignored, and thus the synthesized\ntraining data cannot adequately express characteristics of real underwater\nenvironments; (2) most of deep models disregard lots of favorable underwater\npriors and heavily rely on training data, which extensively limits their\napplication ranges. To address these limitations, a new underwater synthetic\ndataset is first established, in which a revised ambient light synthesis\nequation is embedded. The revised equation explicitly defines the complex\nmathematical relationship among intensity values of the ambient light in RGB\nchannels and many dependencies such as surface-object depth, water types, etc,\nwhich helps to better simulate real underwater scene appearances. Secondly, a\nunified framework is proposed, named ANA-SYN, which can effectively enhance\nunderwater images under collaborations of priors (underwater domain knowledge)\nand data information (underwater distortion distribution). The proposed\nframework includes an analysis network and a synthesis network, one for priors\nexploration and another for priors integration. To exploit more accurate\npriors, the significance of each prior for the input image is explored in the\nanalysis network and an adaptive weighting module is designed to dynamically\nrecalibrate them. Meanwhile, a novel prior guidance module is introduced in the\nsynthesis network, which effectively aggregates the prior and data features and\nthus provides better hybrid information to perform the more reasonable image\nenhancement.",
          "link": "http://arxiv.org/abs/2108.09023",
          "publishedOn": "2021-08-23T01:36:34.890Z",
          "wordCount": 707,
          "title": "Single Underwater Image Enhancement Using an Analysis-Synthesis Network. (arXiv:2108.09023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Mahfujur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>",
          "description": "Domain generalization approaches aim to learn a domain invariant prediction\nmodel for unknown target domains from multiple training source domains with\ndifferent distributions. Significant efforts have recently been committed to\nbroad domain generalization, which is a challenging and topical problem in\nmachine learning and computer vision communities. Most previous domain\ngeneralization approaches assume that the conditional distribution across the\ndomains remain the same across the source domains and learn a domain invariant\nmodel by minimizing the marginal distributions. However, the assumption of a\nstable conditional distribution of the training source domains does not really\nhold in practice. The hyperplane learned from the source domains will easily\nmisclassify samples scattered at the boundary of clusters or far from their\ncorresponding class centres. To address the above two drawbacks, we propose a\ndiscriminative domain-invariant adversarial network (DDIAN) for domain\ngeneralization. The discriminativeness of the features are guaranteed through a\ndiscriminative feature module and domain-invariant features are guaranteed\nthrough the global domain and local sub-domain alignment modules. Extensive\nexperiments on several benchmarks show that DDIAN achieves better prediction on\nunseen target data during training compared to state-of-the-art domain\ngeneralization approaches.",
          "link": "http://arxiv.org/abs/2108.08995",
          "publishedOn": "2021-08-23T01:36:34.882Z",
          "wordCount": 643,
          "title": "Discriminative Domain-Invariant Adversarial Network for Deep Domain Generalization. (arXiv:2108.08995v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kyoungkook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seongtae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>",
          "description": "For successful semantic editing of real images, it is critical for a GAN\ninversion method to find an in-domain latent code that aligns with the domain\nof a pre-trained GAN model. Unfortunately, such in-domain latent codes can be\nfound only for in-range images that align with the training images of a GAN\nmodel. In this paper, we propose BDInvert, a novel GAN inversion approach to\nsemantic editing of out-of-range images that are geometrically unaligned with\nthe training images of a GAN model. To find a latent code that is semantically\neditable, BDInvert inverts an input out-of-range image into an alternative\nlatent space than the original latent space. We also propose a regularized\ninversion method to find a solution that supports semantic editing in the\nalternative space. Our experiments show that BDInvert effectively supports\nsemantic editing of out-of-range images with geometric transformations.",
          "link": "http://arxiv.org/abs/2108.08998",
          "publishedOn": "2021-08-23T01:36:34.874Z",
          "wordCount": 587,
          "title": "GAN Inversion for Out-of-Range Images with Geometric Transformations. (arXiv:2108.08998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>",
          "description": "Social media such as Instagram and Twitter have become important platforms\nfor marketing and selling illicit drugs. Detection of online illicit drug\ntrafficking has become critical to combat the online trade of illicit drugs.\nHowever, the legal status often varies spatially and temporally; even for the\nsame drug, federal and state legislation can have different regulations about\nits legality. Meanwhile, more drug trafficking events are disguised as a novel\nform of advertising commenting leading to information heterogeneity.\nAccordingly, accurate detection of illicit drug trafficking events (IDTEs) from\nsocial media has become even more challenging. In this work, we conduct the\nfirst systematic study on fine-grained detection of IDTEs on Instagram. We\npropose to take a deep multimodal multilabel learning (DMML) approach to detect\nIDTEs and demonstrate its effectiveness on a newly constructed dataset called\nmultimodal IDTE(MM-IDTE). Specifically, our model takes text and image data as\nthe input and combines multimodal information to predict multiple labels of\nillicit drugs. Inspired by the success of BERT, we have developed a\nself-supervised multimodal bidirectional transformer by jointly fine-tuning\npretrained text and image encoders. We have constructed a large-scale dataset\nMM-IDTE with manually annotated multiple drug labels to support fine-grained\ndetection of illicit drugs. Extensive experimental results on the MM-IDTE\ndataset show that the proposed DMML methodology can accurately detect IDTEs\neven in the presence of special characters and style changes attempting to\nevade detection.",
          "link": "http://arxiv.org/abs/2108.08920",
          "publishedOn": "2021-08-23T01:36:34.781Z",
          "wordCount": 693,
          "title": "Detection of Illicit Drug Trafficking Events on Instagram: A Deep Multimodal Multilabel Learning Approach. (arXiv:2108.08920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filchev_L/0/1/0/all/0/1\">Lachezar Filchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pashova_L/0/1/0/all/0/1\">Lyubka Pashova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frye_S/0/1/0/all/0/1\">Stuart Frye</a>",
          "description": "The ever-growing need of data preservation and their systematic analysis\ncontributing to sustainable development of the society spurred in the past\ndecade,numerous Big Data projects and initiatives are focusing on the Earth\nObservation (EO). The number of Big Data EO applications has grown extremely\nworldwide almost simultaneously with other scientific and technological areas\nof the human knowledge due to the revolutionary technological progress in the\nspace and information technology sciences. The substantial contribution to this\ndevelopment are the space programs of the renowned space agencies, such as\nNASA, ESA,Roskosmos, JAXA, DLR, INPE, ISRO, CNES etc. A snap-shot of the\ncurrent Big Data sets from available satellite missions covering the Bulgarian\nterritory is also presented. This short overview of the geoscience Big Data\ncollection with a focus on EO will emphasize to the multiple Vs of EO in order\nto provide a snapshot on the current state-of-the-art in EO data preservation\nand manipulation. Main modern approaches for compressing, clustering and\nmodelling EO in the geoinformation science for Big Data analysis,\ninterpretation and visualization for a variety of applications are outlined.\nSpecial attention is paid to the contemporary EO data modelling and\nvisualization systems.",
          "link": "http://arxiv.org/abs/2108.08886",
          "publishedOn": "2021-08-23T01:36:34.774Z",
          "wordCount": 678,
          "title": "Challenges and Solutions for Utilizing Earth Observations in the \"Big Data\" era. (arXiv:2108.08886v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yushu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jayaweera_M/0/1/0/all/0/1\">Malith Jayaweera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaeli_D/0/1/0/all/0/1\">David Kaeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Though recent years have witnessed remarkable progress in single image\nsuper-resolution (SISR) tasks with the prosperous development of deep neural\nnetworks (DNNs), the deep learning methods are confronted with the computation\nand memory consumption issues in practice, especially for resource-limited\nplatforms such as mobile devices. To overcome the challenge and facilitate the\nreal-time deployment of SISR tasks on mobile, we combine neural architecture\nsearch with pruning search and propose an automatic search framework that\nderives sparse super-resolution (SR) models with high image quality while\nsatisfying the real-time inference requirement. To decrease the search cost, we\nleverage the weight sharing strategy by introducing a supernet and decouple the\nsearch problem into three stages, including supernet construction,\ncompiler-aware architecture and pruning search, and compiler-aware pruning\nratio search. With the proposed framework, we are the first to achieve\nreal-time SR inference (with only tens of milliseconds per frame) for\nimplementing 720p resolution with competitive image quality (in terms of PSNR\nand SSIM) on mobile platforms (Samsung Galaxy S20).",
          "link": "http://arxiv.org/abs/2108.08910",
          "publishedOn": "2021-08-23T01:36:34.767Z",
          "wordCount": 639,
          "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yansen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn P. Rose</a>",
          "description": "As an important task in multimodal context understanding, Text-VQA (Visual\nQuestion Answering) aims at question answering through reading text information\nin images. It differentiates from the original VQA task as Text-VQA requires\nlarge amounts of scene-text relationship understanding, in addition to the\ncross-modal grounding capability. In this paper, we propose Localize, Group,\nand Select (LOGOS), a novel model which attempts to tackle this problem from\nmultiple aspects. LOGOS leverages two grounding tasks to better localize the\nkey information of the image, utilizes scene text clustering to group\nindividual OCR tokens, and learns to select the best answer from different\nsources of OCR (Optical Character Recognition) texts. Experiments show that\nLOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks\nwithout using additional OCR annotation data. Ablation studies and analysis\ndemonstrate the capability of LOGOS to bridge different modalities and better\nunderstand scene text.",
          "link": "http://arxiv.org/abs/2108.08965",
          "publishedOn": "2021-08-23T01:36:34.758Z",
          "wordCount": 592,
          "title": "Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling. (arXiv:2108.08965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zihang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sathya N. Ravi</a>",
          "description": "We study how stochastic differential equation (SDE) based ideas can inspire\nnew modifications to existing algorithms for a set of problems in computer\nvision. Loosely speaking, our formulation is related to both explicit and\nimplicit strategies for data augmentation and group equivariance, but is\nderived from new results in the SDE literature on estimating infinitesimal\ngenerators of a class of stochastic processes. If and when there is nominal\nagreement between the needs of an application/task and the inherent properties\nand behavior of the types of processes that we can efficiently handle, we\nobtain a very simple and efficient plug-in layer that can be incorporated\nwithin any existing network architecture, with minimal modification and only a\nfew additional parameters. We show promising experiments on a number of vision\ntasks including few shot learning, point cloud transformers and deep\nvariational segmentation obtaining efficiency or performance improvements.",
          "link": "http://arxiv.org/abs/2108.08891",
          "publishedOn": "2021-08-23T01:36:34.718Z",
          "wordCount": 587,
          "title": "Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators. (arXiv:2108.08891v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyomin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1\">Hyeonseo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>",
          "description": "This paper presents an effective method for generating a spatiotemporal\n(time-varying) texture map for a dynamic object using a single RGB-D camera.\nThe input of our framework is a 3D template model and an RGB-D image sequence.\nSince there are invisible areas of the object at a frame in a single-camera\nsetup, textures of such areas need to be borrowed from other frames. We\nformulate the problem as an MRF optimization and define cost functions to\nreconstruct a plausible spatiotemporal texture for a dynamic object.\nExperimental results demonstrate that our spatiotemporal textures can reproduce\nthe active appearances of captured objects better than approaches using a\nsingle texture map.",
          "link": "http://arxiv.org/abs/2108.09007",
          "publishedOn": "2021-08-23T01:36:34.712Z",
          "wordCount": 569,
          "title": "Spatiotemporal Texture Reconstruction for Dynamic Objects Using a Single RGB-D Camera. (arXiv:2108.09007v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1\">Vaibhav Vavilala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>",
          "description": "The state-of-the-art StyleGAN2 network supports powerful methods to create\nand edit art, including generating random images, finding images \"like\" some\nquery, and modifying content or style. Further, recent advancements enable\ntraining with small datasets. We apply these methods to synthesize card art, by\ntraining on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are\nessential for good synthesis, we find that, for small datasets, coarse-scale\nnoise interferes with latent variables because both control long-scale image\neffects. We observe over-aggressive variation in art with changes in noise and\nweak content control via latent variable edits. Here, we demonstrate that\ntraining a modified StyleGAN2, where coarse-scale noise is suppressed, removes\nthese unwanted effects. We obtain a superior FID; changes in noise result in\nlocal exploration of style; and identity control is markedly improved. These\nresults and analysis lead towards a GAN-assisted art synthesis tool for digital\nartists of all skill levels, which can be used in film, games, or any creative\nindustry for artistic ideation.",
          "link": "http://arxiv.org/abs/2108.08922",
          "publishedOn": "2021-08-23T01:36:34.706Z",
          "wordCount": 614,
          "title": "Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset -- Addressing the Noise-Latent Trade-Off. (arXiv:2108.08922v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Siddhansh Narang</a>",
          "description": "There has been a remarkable progress in learning a model which could\nrecognise novel classes with only a few labeled examples in the last few years.\nFew-shot learning (FSL) for action recognition is a challenging task of\nrecognising novel action categories which are represented by few instances in\nthe training data. We propose a novel variational inference based architectural\nframework (HF-AR) for few shot activity recognition. Our framework leverages\nvolume-preserving Householder Flow to learn a flexible posterior distribution\nof the novel classes. This results in better performance as compared to\nstate-of-the-art few shot approaches for human activity recognition. approach\nconsists of base model and an adapter model. Our architecture consists of a\nbase model and an adapter model. The base model is trained on seen classes and\nit computes an embedding that represent the spatial and temporal insights\nextracted from the input video, e.g. combination of Resnet-152 and LSTM based\nencoder-decoder model. The adapter model applies a series of Householder\ntransformations to compute a flexible posterior distribution that lends higher\naccuracy in the few shot approach. Extensive experiments on three well-known\ndatasets: UCF101, HMDB51 and Something-Something-V2, demonstrate similar or\nbetter performance on 1-shot and 5-shot classification as compared to\nstate-of-the-art few shot approaches that use only RGB frame sequence as input.\nTo the best of our knowledge, we are the first to explore variational inference\nalong with householder transformations to capture the full rank covariance\nmatrix of posterior distribution, for few shot learning in activity\nrecognition.",
          "link": "http://arxiv.org/abs/2108.08990",
          "publishedOn": "2021-08-23T01:36:34.681Z",
          "wordCount": 706,
          "title": "Few Shot Activity Recognition Using Variational Inference. (arXiv:2108.08990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyomin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kam_J/0/1/0/all/0/1\">Jaewon Kam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>",
          "description": "We propose deep virtual markers, a framework for estimating dense and\naccurate positional information for various types of 3D data. We design a\nconcept and construct a framework that maps 3D points of 3D articulated models,\nlike humans, into virtual marker labels. To realize the framework, we adopt a\nsparse convolutional neural network and classify 3D points of an articulated\nmodel into virtual marker labels. We propose to use soft labels for the\nclassifier to learn rich and dense interclass relationships based on geodesic\ndistance. To measure the localization accuracy of the virtual markers, we test\nFAUST challenge, and our result outperforms the state-of-the-art. We also\nobserve outstanding performance on the generalizability test, unseen data\nevaluation, and different 3D data types (meshes and depth maps). We show\nadditional applications using the estimated virtual markers, such as non-rigid\nregistration, texture transfer, and realtime dense marker prediction from depth\nmaps.",
          "link": "http://arxiv.org/abs/2108.09000",
          "publishedOn": "2021-08-23T01:36:34.674Z",
          "wordCount": 584,
          "title": "Deep Virtual Markers for Articulated 3D Shapes. (arXiv:2108.09000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1\">Martin Renqiang Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathopoulos_A/0/1/0/all/0/1\">Anastasis Stathopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Conditional Generative Adversarial Networks (cGANs) extend the standard\nunconditional GAN framework to learning joint data-label distributions from\nsamples, and have been established as powerful generative models capable of\ngenerating high-fidelity imagery. A challenge of training such a model lies in\nproperly infusing class information into its generator and discriminator. For\nthe discriminator, class conditioning can be achieved by either (1) directly\nincorporating labels as input or (2) involving labels in an auxiliary\nclassification loss. In this paper, we show that the former directly aligns the\nclass-conditioned fake-and-real data distributions\n$P(\\text{image}|\\text{class})$ ({\\em data matching}), while the latter aligns\ndata-conditioned class distributions $P(\\text{class}|\\text{image})$ ({\\em label\nmatching}). Although class separability does not directly translate to sample\nquality and becomes a burden if classification itself is intrinsically\ndifficult, the discriminator cannot provide useful guidance for the generator\nif features of distinct classes are mapped to the same point and thus become\ninseparable. Motivated by this intuition, we propose a Dual Projection GAN\n(P2GAN) model that learns to balance between {\\em data matching} and {\\em label\nmatching}. We then propose an improved cGAN model with Auxiliary Classification\nthat directly aligns the fake and real conditionals\n$P(\\text{class}|\\text{image})$ by minimizing their $f$-divergence. Experiments\non a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world\ndatasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of\nour proposed models.",
          "link": "http://arxiv.org/abs/2108.09016",
          "publishedOn": "2021-08-23T01:36:34.667Z",
          "wordCount": 670,
          "title": "Dual Projection Generative Adversarial Networks for Conditional Image Generation. (arXiv:2108.09016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Yong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeGol_J/0/1/0/all/0/1\">Joseph DeGol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1\">Chuhang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>",
          "description": "Recent learning-based multi-view stereo (MVS) methods show excellent\nperformance with dense cameras and small depth ranges. However, non-learning\nbased approaches still outperform for scenes with large depth ranges and\nsparser wide-baseline views, in part due to their PatchMatch optimization over\npixelwise estimates of depth, normals, and visibility. In this paper, we\npropose an end-to-end trainable PatchMatch-based MVS approach that combines\nadvantages of trainable costs and regularizations with pixelwise estimates. To\novercome the challenge of the non-differentiable PatchMatch optimization that\ninvolves iterative sampling and hard decisions, we use reinforcement learning\nto minimize expected photometric cost and maximize likelihood of ground truth\ndepth and normals. We incorporate normal estimation by using dilated patch\nkernels, and propose a recurrent cost regularization that applies beyond\nfrontal plane-sweep algorithms to our pixelwise depth/normal estimates. We\nevaluate our method on widely used MVS benchmarks, ETH3D and Tanks and Temples\n(TnT), and compare to other state of the art learning based MVS models. On\nETH3D, our method outperforms other recent learning-based approaches and\nperforms comparably on advanced TnT.",
          "link": "http://arxiv.org/abs/2108.08943",
          "publishedOn": "2021-08-23T01:36:34.658Z",
          "wordCount": 619,
          "title": "PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility. (arXiv:2108.08943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Jared Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Multi-modal generation has been widely explored in recent years. Current\nresearch directions involve generating text based on an image or vice versa. In\nthis paper, we propose a new task called CIGLI: Conditional Image Generation\nfrom Language and Image. Instead of generating an image based on text as in\ntext-image generation, this task requires the generation of an image from a\ntextual description and an image prompt. We designed a new dataset to ensure\nthat the text description describes information from both images, and that\nsolely analyzing the description is insufficient to generate an image. We then\npropose a novel language-image fusion model which improves the performance over\ntwo established baseline methods, as evaluated by quantitative (automatic) and\nqualitative (human) evaluations. The code and dataset is available at\nhttps://github.com/vincentlux/CIGLI.",
          "link": "http://arxiv.org/abs/2108.08955",
          "publishedOn": "2021-08-23T01:36:34.635Z",
          "wordCount": 574,
          "title": "CIGLI: Conditional Image Generation from Language & Image. (arXiv:2108.08955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08895",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yazdekhasty_P/0/1/0/all/0/1\">Parham Yazdekhasty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zindari_A/0/1/0/all/0/1\">Ali Zindari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1\">Zahra Nabizadeh-ShahreBabak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Coronavirus has caused hundreds of thousands of deaths. Fatalities could\ndecrease if every patient could get suitable treatment by the healthcare\nsystem. Machine learning, especially computer vision methods based on deep\nlearning, can help healthcare professionals diagnose and treat COVID-19\ninfected cases more efficiently. Hence, infected patients can get better\nservice from the healthcare system and decrease the number of deaths caused by\nthe coronavirus. This research proposes a method for segmenting infected lung\nregions in a CT image. For this purpose, a convolutional neural network with an\nattention mechanism is used to detect infected areas with complex patterns.\nAttention blocks improve the segmentation accuracy by focusing on informative\nparts of the image. Furthermore, a generative adversarial network generates\nsynthetic images for data augmentation and expansion of small available\ndatasets. Experimental results show the superiority of the proposed method\ncompared to some existing procedures.",
          "link": "http://arxiv.org/abs/2108.08895",
          "publishedOn": "2021-08-23T01:36:34.628Z",
          "wordCount": 653,
          "title": "Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data. (arXiv:2108.08895v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingren Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanzhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaguchi_Y/0/1/0/all/0/1\">Yudai Yaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haley_J/0/1/0/all/0/1\">Jack C. Haley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_K/0/1/0/all/0/1\">Kevin G. Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>",
          "description": "Videos captured using Transmission Electron Microscopy (TEM) can encode\ndetails regarding the morphological and temporal evolution of a material by\ntaking snapshots of the microstructure sequentially. However, manual analysis\nof such video is tedious, error-prone, unreliable, and prohibitively\ntime-consuming if one wishes to analyze a significant fraction of frames for\neven videos of modest length. In this work, we developed an automated TEM video\nanalysis system for microstructural features based on the advanced object\ndetection model called YOLO and tested the system on an in-situ ion irradiation\nTEM video of dislocation loops formed in a FeCrAl alloy. The system provides\nanalysis of features observed in TEM including both static and dynamic\nproperties using the YOLO-based defect detection module coupled to a geometry\nanalysis module and a dynamic tracking module. Results show that the system can\nachieve human comparable performance with an F1 score of 0.89 for fast,\nconsistent, and scalable frame-level defect analysis. This result is obtained\non a real but exceptionally clean and stable data set and more challenging data\nsets may not achieve this performance. The dynamic tracking also enabled\nevaluation of individual defect evolution like per defect growth rate at a\nfidelity never before achieved using common human analysis methods. Our work\nshows that automatically detecting and tracking interesting microstructures and\nproperties contained in TEM videos is viable and opens new doors for evaluating\nmaterials dynamics.",
          "link": "http://arxiv.org/abs/2108.08882",
          "publishedOn": "2021-08-23T01:36:34.621Z",
          "wordCount": 688,
          "title": "A Deep Learning Based Automatic Defect Analysis Framework for In-situ TEM Ion Irradiations. (arXiv:2108.08882v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kohler_S/0/1/0/all/0/1\">Sebastian K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1\">Richard Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinovic_I/0/1/0/all/0/1\">Ivan Martinovic</a>",
          "description": "Since cameras have become a crucial part in many safety-critical systems and\napplications, such as autonomous vehicles and surveillance, a large body of\nacademic and non-academic work has shown attacks against their main component -\nthe image sensor. However, these attacks are limited to coarse-grained and\noften suspicious injections because light is used as an attack vector.\nFurthermore, due to the nature of optical attacks, they require the\nline-of-sight between the adversary and the target camera.\n\nIn this paper, we present a novel post-transducer signal injection attack\nagainst CCD image sensors, as they are used in professional, scientific, and\neven military settings. We show how electromagnetic emanation can be used to\nmanipulate the image information captured by a CCD image sensor with the\ngranularity down to the brightness of individual pixels. We study the\nfeasibility of our attack and then demonstrate its effects in the scenario of\nautomatic barcode scanning. Our results indicate that the injected distortion\ncan disrupt automated vision-based intelligent systems.",
          "link": "http://arxiv.org/abs/2108.08881",
          "publishedOn": "2021-08-23T01:36:34.615Z",
          "wordCount": 598,
          "title": "Signal Injection Attacks against CCD Image Sensors. (arXiv:2108.08881v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perreault_H/0/1/0/all/0/1\">Hughes Perreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heritier_M/0/1/0/all/0/1\">Maguelonne H&#xe9;ritier</a>",
          "description": "We present a novel method, called CenterPoly, for real-time instance\nsegmentation using bounding polygons. We apply it to detect road users in dense\nurban environments, making it suitable for applications in intelligent\ntransportation systems like automated vehicles. CenterPoly detects objects by\ntheir center keypoint while predicting a fixed number of polygon vertices for\neach object, thus performing detection and segmentation in parallel. Most of\nthe network parameters are shared by the network heads, making it fast and\nlightweight enough to run at real-time speed. To properly convert mask\nground-truth to polygon ground-truth, we designed a vertex selection strategy\nto facilitate the learning of the polygons. Additionally, to better segment\noverlapping objects in dense urban scenes, we also train a relative depth\nbranch to determine which instances are closer and which are further, using\navailable weak annotations. We propose several models with different backbones\nto show the possible speed / accuracy trade-offs. The models were trained and\nevaluated on Cityscapes, KITTI and IDD and the results are reported on their\npublic benchmark, which are state-of-the-art at real-time speeds. Code is\navailable at https://github.com/hu64/CenterPoly",
          "link": "http://arxiv.org/abs/2108.08923",
          "publishedOn": "2021-08-23T01:36:34.607Z",
          "wordCount": 628,
          "title": "CenterPoly: real-time instance segmentation using bounding polygons. (arXiv:2108.08923v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1\">Andr&#xe9; Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bingyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askew_C/0/1/0/all/0/1\">Cam Askew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Mike Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilla_N/0/1/0/all/0/1\">N&#x27;Mah Fodiatu Yilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>",
          "description": "We introduce a new landmark recognition dataset, which is created with a\nfocus on fair worldwide representation. While previous work proposes to collect\nas many images as possible from web repositories, we instead argue that such\napproaches can lead to biased data. To create a more comprehensive and\nequitable dataset, we start by defining the fair relevance of a landmark to the\nworld population. These relevances are estimated by combining anonymized Google\nMaps user contribution statistics with the contributors' demographic\ninformation. We present a stratification approach and analysis which leads to a\nmuch fairer coverage of the world, compared to existing datasets. The resulting\ndatasets are used to evaluate computer vision models as part of the the Google\nLandmark Recognition and RetrievalChallenges 2021.",
          "link": "http://arxiv.org/abs/2108.08874",
          "publishedOn": "2021-08-23T01:36:34.601Z",
          "wordCount": 571,
          "title": "Towards A Fairer Landmark Recognition Dataset. (arXiv:2108.08874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_W/0/1/0/all/0/1\">Weikun Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaoyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>",
          "description": "In Simultaneous Localization And Mapping (SLAM) problems, high-level\nlandmarks have the potential to build compact and informative maps compared to\ntraditional point-based landmarks. This work is focused on the parameterization\nproblem of high-level geometric primitives that are most frequently used,\nincluding points, lines, planes, ellipsoids, cylinders, and cones. We first\npresent a unified representation of those geometric primitives using\n\\emph{quadrics} which yields a consistent and concise formulation. Then we\nfurther study a decomposed model of quadrics that discloses the symmetric and\ndegenerated nature of quadrics. Based on the decomposition, we develop\nphysically meaningful quadrics factors in the settings of the graph-SLAM\nproblem. Finally, in simulation experiments, it is shown that the decomposed\nformulation has better efficiency and robustness to observation noises than\nbaseline parameterizations. And in real-world experiments, the proposed\nback-end framework is demonstrated to be capable of building compact and\nregularized maps.",
          "link": "http://arxiv.org/abs/2108.08957",
          "publishedOn": "2021-08-23T01:36:34.594Z",
          "wordCount": 592,
          "title": "Unified Representation of Geometric Primitives for Graph-SLAM Optimization Using Decomposed Quadrics. (arXiv:2108.08957v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingren Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanzhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greaves_J/0/1/0/all/0/1\">Jacob Greaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Wei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krakauer_N/0/1/0/all/0/1\">Nathaniel J. Krakauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krudy_L/0/1/0/all/0/1\">Leah Krudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Jacob Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasan_V/0/1/0/all/0/1\">Varun Sreenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_B/0/1/0/all/0/1\">Bryan Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_O/0/1/0/all/0/1\">Oigimer Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_K/0/1/0/all/0/1\">Kevin Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>",
          "description": "Electron microscopy is widely used to explore defects in crystal structures,\nbut human detecting of defects is often time-consuming, error-prone, and\nunreliable, and is not scalable to large numbers of images or real-time\nanalysis. In this work, we discuss the application of machine learning\napproaches to find the location and geometry of different defect clusters in\nirradiated steels. We show that a deep learning based Faster R-CNN analysis\nsystem has a performance comparable to human analysis with relatively small\ntraining data sets. This study proves the promising ability to apply deep\nlearning to assist the development of automated microscopy data analysis even\nwhen multiple features are present and paves the way for fast, scalable, and\nreliable analysis systems for massive amounts of modern electron microscopy\ndata.",
          "link": "http://arxiv.org/abs/2108.08883",
          "publishedOn": "2021-08-23T01:36:34.493Z",
          "wordCount": 598,
          "title": "Multi defect detection and analysis of electron microscopy images with deep learning. (arXiv:2108.08883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Ke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "Multi-label image recognition is a challenging computer vision task of\npractical use. Progresses in this area, however, are often characterized by\ncomplicated methods, heavy computations, and lack of intuitive explanations. To\neffectively capture different spatial regions occupied by objects from\ndifferent categories, we propose an embarrassingly simple module, named\nclass-specific residual attention (CSRA). CSRA generates class-specific\nfeatures for every category by proposing a simple spatial attention score, and\nthen combines it with the class-agnostic average pooling feature. CSRA achieves\nstate-of-the-art results on multilabel recognition, and at the same time is\nmuch simpler than them. Furthermore, with only 4 lines of code, CSRA also leads\nto consistent improvement across many diverse pretrained models and datasets\nwithout any extra training. CSRA is both easy to implement and light in\ncomputations, which also enjoys intuitive explanations and visualizations.",
          "link": "http://arxiv.org/abs/2108.02456",
          "publishedOn": "2021-08-20T01:53:52.288Z",
          "wordCount": 595,
          "title": "Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1\">Rohit Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara Lee Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-benchmark.github.io/.",
          "link": "http://arxiv.org/abs/2106.04632",
          "publishedOn": "2021-08-20T01:53:52.257Z",
          "wordCount": 703,
          "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nalaie_K/0/1/0/all/0/1\">Keivan Nalaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>",
          "description": "In surveillance and search and rescue applications, it is important to\nperform multi-target tracking (MOT) in real-time on low-end devices. Today's\nMOT solutions employ deep neural networks, which tend to have high computation\ncomplexity. Recognizing the effects of frame sizes on tracking performance, we\npropose DeepScale, a model agnostic frame size selection approach that operates\non top of existing fully convolutional network-based trackers to accelerate\ntracking throughput. In the training stage, we incorporate detectability scores\ninto a one-shot tracker architecture so that DeepScale can learn representation\nestimations for different frame sizes in a self-supervised manner. {During\ninference, it can adapt frame sizes according to the complexity of visual\ncontents based on user-controlled parameters.} Extensive experiments and\nbenchmark tests on MOT datasets demonstrate the effectiveness and flexibility\nof DeepScale. Compared to a state-of-the-art tracker, DeepScale++, a variant of\nDeepScale achieves 1.57X accelerated with only moderate degradation (~ 2.3%) in\ntracking accuracy on the MOT15 dataset in one configuration.",
          "link": "http://arxiv.org/abs/2107.10404",
          "publishedOn": "2021-08-20T01:53:52.208Z",
          "wordCount": 623,
          "title": "DeepScale: An Online Frame Size Adaptation Approach to Accelerate Visual Multi-object Tracking. (arXiv:2107.10404v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1\">Daehoon Gwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin. Our code is publicly available at this\n$\\href{https://github.com/shjung13/Standardized-max-logits}{link}$.",
          "link": "http://arxiv.org/abs/2107.11264",
          "publishedOn": "2021-08-20T01:53:52.193Z",
          "wordCount": 723,
          "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation. (arXiv:2107.11264v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1\">Marcel Seelbach Benkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunderlich_C/0/1/0/all/0/1\">Christof Wunderlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Finding shape correspondences can be formulated as an NP-hard quadratic\nassignment problem (QAP) that becomes infeasible for shapes with high sampling\ndensity. A promising research direction is to tackle such quadratic\noptimization problems over binary variables with quantum annealing, which\nallows for some problems a more efficient search in the solution space.\nUnfortunately, enforcing the linear equality constraints in QAPs via a penalty\nsignificantly limits the success probability of such methods on currently\navailable quantum hardware. To address this limitation, this paper proposes\nQ-Match, i.e., a new iterative quantum method for QAPs inspired by the\nalpha-expansion algorithm, which allows solving problems of an order of\nmagnitude larger than current quantum methods. It implicitly enforces the QAP\nconstraints by updating the current estimates in a cyclic fashion. Further,\nQ-Match can be applied iteratively, on a subset of well-chosen correspondences,\nallowing us to scale to real-world problems. Using the latest quantum annealer,\nthe D-Wave Advantage, we evaluate the proposed method on a subset of QAPLIB as\nwell as on isometric shape matching problems from the FAUST dataset.",
          "link": "http://arxiv.org/abs/2105.02878",
          "publishedOn": "2021-08-20T01:53:52.153Z",
          "wordCount": 666,
          "title": "Q-Match: Iterative Shape Matching via Quantum Annealing. (arXiv:2105.02878v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Kalun Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1\">Franz-Josef Pfreundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>",
          "description": "Over the last decade, the development of deep image classification networks\nhas mostly been driven by the search for the best performance in terms of\nclassification accuracy on standardized benchmarks like ImageNet. More\nrecently, this focus has been expanded by the notion of model robustness, \\ie\nthe generalization abilities of models towards previously unseen changes in the\ndata distribution. While new benchmarks, like ImageNet-C, have been introduced\nto measure robustness properties, we argue that fixed testsets are only able to\ncapture a small portion of possible data variations and are thus limited and\nprone to generate new overfitted solutions. To overcome these drawbacks, we\nsuggest to estimate the robustness of a model directly from the structure of\nits learned feature-space. We introduce robustness indicators which are\nobtained via unsupervised clustering of latent representations from a trained\nclassifier and show very high correlations to the model performance on\ncorrupted test data.",
          "link": "http://arxiv.org/abs/2106.12303",
          "publishedOn": "2021-08-20T01:53:52.118Z",
          "wordCount": 623,
          "title": "Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by up to 10-times for tropical\nreforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-08-20T01:53:52.099Z",
          "wordCount": 693,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhengmi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyazaki_T/0/1/0/all/0/1\">Tomo Miyazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugaya_Y/0/1/0/all/0/1\">Yoshihiro Sugaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omachi_S/0/1/0/all/0/1\">Shinichiro Omachi</a>",
          "description": "Scene text erasing, which replaces text regions with reasonable content in\nnatural images, has drawn significant attention in the computer vision\ncommunity in recent years. There are two potential subtasks in scene text\nerasing: text detection and image inpainting. Either subtask requires\nconsiderable data to achieve better performance; however, the lack of a\nlarge-scale real-world scene-text removal dataset does not allow existing\nmethods to work according to their potential. To avoid the limitation of the\nlack of pairwise real-world data, we enhance and make considerable use of the\nsynthetic text and subsequently train our model only on the dataset generated\nby the improved synthetic text engine. Our proposed network contains a stroke\nmask prediction module and background inpainting module that can extract the\ntext stroke as a relatively small hole from the cropped text image to maintain\nmore background content for better inpainting results. This model can partially\nerase text instances in a scene image with a bounding box or work with an\nexisting scene-text detector for automatic scene text erasing. The experimental\nresults from the qualitative and quantitative evaluation of the SCUT-Syn,\nICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly\noutperforms existing state-of-the-art methods even when they were trained on\nreal-world data.",
          "link": "http://arxiv.org/abs/2104.11493",
          "publishedOn": "2021-08-20T01:53:52.053Z",
          "wordCount": 670,
          "title": "Stroke-Based Scene Text Erasing Using Synthetic Data for Training. (arXiv:2104.11493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>",
          "description": "We present a novel pyramidal output representation to ensure parsimony with\nour \"specialize and fuse\" process for semantic segmentation. A pyramidal\n\"output\" representation consists of coarse-to-fine levels, where each level is\n\"specialize\" in a different class distribution (e.g., more stuff than things\nclasses at coarser levels). Two types of pyramidal outputs (i.e., unity and\nsemantic pyramid) are \"fused\" into the final semantic output, where the unity\npyramid indicates unity-cells (i.e., all pixels in such cell share the same\nsemantic label). The process ensures parsimony by predicting a relatively small\nnumber of labels for unity-cells (e.g., a large cell of grass) to build the\nfinal semantic output. In addition to the \"output\" representation, we design a\ncoarse-to-fine contextual module to aggregate the \"features\" representation\nfrom different levels. We validate the effectiveness of each key module in our\nmethod through comprehensive ablation studies. Finally, our approach achieves\nstate-of-the-art performance on three widely-used semantic segmentation\ndatasets -- ADE20K, COCO-Stuff, and Pascal-Context.",
          "link": "http://arxiv.org/abs/2108.01866",
          "publishedOn": "2021-08-20T01:53:52.047Z",
          "wordCount": 616,
          "title": "Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1\">Otto Seiskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1\">Pekka Rantalankila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1\">Jerry Ylilammi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>",
          "description": "We present HybVIO, a novel hybrid approach for combining filtering-based\nvisual-inertial odometry (VIO) with optimization-based SLAM. The core of our\nmethod is highly robust, independent VIO with improved IMU bias modeling,\noutlier rejection, stationarity detection, and feature track selection, which\nis adjustable to run on embedded hardware. Long-term consistency is achieved\nwith a loosely-coupled SLAM module. In academic benchmarks, our solution yields\nexcellent performance in all categories, especially in the real-time use case,\nwhere we outperform the current state-of-the-art. We also demonstrate the\nfeasibility of VIO for vehicular tracking on consumer-grade hardware using a\ncustom dataset, and show good performance in comparison to current commercial\nVISLAM alternatives.",
          "link": "http://arxiv.org/abs/2106.11857",
          "publishedOn": "2021-08-20T01:53:52.029Z",
          "wordCount": 575,
          "title": "HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Appearance and motion are two important sources of information in video\nobject segmentation (VOS). Previous methods mainly focus on using simplex\nsolutions, lowering the upper bound of feature collaboration among and across\nthese two cues. In this paper, we study a novel framework, termed the FSNet\n(Full-duplex Strategy Network), which designs a relational cross-attention\nmodule (RCAM) to achieve the bidirectional message propagation across embedding\nsubspaces. Furthermore, the bidirectional purification module (BPM) is\nintroduced to update the inconsistent features between the spatial-temporal\nembeddings, effectively improving the model robustness. By considering the\nmutual restraint within the full-duplex strategy, our FSNet performs the\ncross-modal feature-passing (i.e., transmission and receiving) simultaneously\nbefore the fusion and decoding stage, making it robust to various challenging\nscenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five\npopular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and\nDAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both\nthe VOS and video salient object detection tasks.",
          "link": "http://arxiv.org/abs/2108.03151",
          "publishedOn": "2021-08-20T01:53:51.990Z",
          "wordCount": 617,
          "title": "Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>",
          "description": "Point cloud registration is a fundamental problem in 3D computer vision. In\nthis paper, we cast point cloud registration into a planning problem in\nreinforcement learning, which can seek the transformation between the source\nand target point clouds through trial and error. By modeling the point cloud\nregistration process as a Markov decision process (MDP), we develop a latent\ndynamic model of point clouds, consisting of a transformation network and\nevaluation network. The transformation network aims to predict the new\ntransformed feature of the point cloud after performing a rigid transformation\n(i.e., action) on it while the evaluation network aims to predict the alignment\nprecision between the transformed source point cloud and target point cloud as\nthe reward signal. Once the dynamic model of the point cloud is trained, we\nemploy the cross-entropy method (CEM) to iteratively update the planning policy\nby maximizing the rewards in the point cloud registration process. Thus, the\noptimal policy, i.e., the transformation between the source and target point\nclouds, can be obtained via gradually narrowing the search space of the\ntransformation. Experimental results on ModelNet40 and 7Scene benchmark\ndatasets demonstrate that our method can yield good registration performance in\nan unsupervised manner.",
          "link": "http://arxiv.org/abs/2108.02613",
          "publishedOn": "2021-08-20T01:53:51.962Z",
          "wordCount": 663,
          "title": "Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1\">Lior Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Collecting labeled data for the task of semantic segmentation is expensive\nand time-consuming, as it requires dense pixel-level annotations. While recent\nConvolutional Neural Network (CNN) based semantic segmentation approaches have\nachieved impressive results by using large amounts of labeled training data,\ntheir performance drops significantly as the amount of labeled data decreases.\nThis happens because deep CNNs trained with the de facto cross-entropy loss can\neasily overfit to small amounts of labeled data. To address this issue, we\npropose a simple and effective contrastive learning-based training strategy in\nwhich we first pretrain the network using a pixel-wise, label-based contrastive\nloss, and then fine-tune it using the cross-entropy loss. This approach\nincreases intra-class compactness and inter-class separability, thereby\nresulting in a better pixel classifier. We demonstrate the effectiveness of the\nproposed training strategy using the Cityscapes and PASCAL VOC 2012\nsegmentation datasets. Our results show that pretraining with the proposed\ncontrastive loss results in large performance gains (more than 20% absolute\nimprovement in some settings) when the amount of labeled data is limited. In\nmany settings, the proposed contrastive pretraining strategy, which does not\nuse any additional data, is able to match or outperform the widely-used\nImageNet pretraining strategy that uses more than a million additional labeled\nimages.",
          "link": "http://arxiv.org/abs/2012.06985",
          "publishedOn": "2021-08-20T01:53:51.794Z",
          "wordCount": 710,
          "title": "Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>",
          "description": "This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8\nof EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event\nchange, and respect human perception diversity. We view GEBD as an important\nstepping stone towards understanding the video as a whole, and believe it has\nbeen previously neglected due to a lack of proper task definition and\nannotations. Through experiment and human study we demonstrate the value of the\nannotations. Further, we benchmark supervised and un-supervised GEBD approaches\non the TAPOS dataset and our Kinetics-GEBD. We release our annotations and\nbaseline codes at CVPR'21 LOVEU Challenge:\nhttps://sites.google.com/view/loveucvpr21.",
          "link": "http://arxiv.org/abs/2101.10511",
          "publishedOn": "2021-08-20T01:53:51.786Z",
          "wordCount": 719,
          "title": "Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Open set recognition (OSR), aiming to simultaneously classify the seen\nclasses and identify the unseen classes as 'unknown', is essential for reliable\nmachine learning.The key challenge of OSR is how to reduce the empirical\nclassification risk on the labeled known data and the open space risk on the\npotential unknown data simultaneously. To handle the challenge, we formulate\nthe open space risk problem from the perspective of multi-class integration,\nand model the unexploited extra-class space with a novel concept Reciprocal\nPoint. Follow this, a novel learning framework, termed Adversarial Reciprocal\nPoint Learning (ARPL), is proposed to minimize the overlap of known\ndistribution and unknown distributions without loss of known classification\naccuracy. Specifically, each reciprocal point is learned by the extra-class\nspace with the corresponding known category, and the confrontation among\nmultiple known categories are employed to reduce the empirical classification\nrisk. Then, an adversarial margin constraint is proposed to reduce the open\nspace risk by limiting the latent open space constructed by reciprocal points.\nTo further estimate the unknown distribution from open space, an instantiated\nadversarial enhancement method is designed to generate diverse and confusing\ntraining samples, based on the adversarial mechanism between the reciprocal\npoints and known classes. This can effectively enhance the model\ndistinguishability to the unknown classes. Extensive experimental results on\nvarious benchmark datasets indicate that the proposed method is significantly\nsuperior to other existing approaches and achieves state-of-the-art\nperformance.",
          "link": "http://arxiv.org/abs/2103.00953",
          "publishedOn": "2021-08-20T01:53:51.768Z",
          "wordCount": 707,
          "title": "Adversarial Reciprocal Points Learning for Open Set Recognition. (arXiv:2103.00953v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "A myriad of recent breakthroughs in hand-crafted neural architectures for\nvisual recognition have highlighted the urgent need to explore hybrid\narchitectures consisting of diversified building blocks. Meanwhile, neural\narchitecture search methods are surging with an expectation to reduce human\nefforts. However, whether NAS methods can efficiently and effectively handle\ndiversified search spaces with disparate candidates (e.g. CNNs and\ntransformers) is still an open question. In this work, we present Block-wisely\nSelf-supervised Neural Architecture Search (BossNAS), an unsupervised NAS\nmethod that addresses the problem of inaccurate architecture rating caused by\nlarge weight-sharing space and biased supervision in previous methods. More\nspecifically, we factorize the search space into blocks and utilize a novel\nself-supervised training scheme, named ensemble bootstrapping, to train each\nblock separately before searching them as a whole towards the population\ncenter. Additionally, we present HyTra search space, a fabric-like hybrid\nCNN-transformer search space with searchable down-sampling positions. On this\nchallenging search space, our searched model, BossNet-T, achieves up to 82.5%\naccuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute\ntime. Moreover, our method achieves superior architecture rating accuracy with\n0.78 and 0.76 Spearman correlation on the canonical MBConv search space with\nImageNet and on NATS-Bench size search space with CIFAR-100, respectively,\nsurpassing state-of-the-art NAS methods. Code:\nhttps://github.com/changlin31/BossNAS",
          "link": "http://arxiv.org/abs/2103.12424",
          "publishedOn": "2021-08-20T01:53:51.761Z",
          "wordCount": 705,
          "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen-Hsuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.",
          "link": "http://arxiv.org/abs/2104.06405",
          "publishedOn": "2021-08-20T01:53:51.690Z",
          "wordCount": 657,
          "title": "BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1\">George Cazenavette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevara_M/0/1/0/all/0/1\">Manuel Ladron De Guevara</a>",
          "description": "While attention-based transformer networks achieve unparalleled success in\nnearly all language tasks, the large number of tokens (pixels) found in images\ncoupled with the quadratic activation memory usage makes them prohibitive for\nproblems in computer vision. As such, while language-to-language translation\nhas been revolutionized by the transformer model, convolutional networks remain\nthe de facto solution for image-to-image translation. The recently proposed\nMLP-Mixer architecture alleviates some of the computational issues associated\nwith attention-based networks while still retaining the long-range connections\nthat make transformer models desirable. Leveraging this memory-efficient\nalternative to self-attention, we propose a new exploratory model in unpaired\nimage-to-image translation called MixerGAN: a simpler MLP-based architecture\nthat considers long-distance relationships between pixels without the need for\nexpensive attention mechanisms. Quantitative and qualitative analysis shows\nthat MixerGAN achieves competitive results when compared to prior\nconvolutional-based methods.",
          "link": "http://arxiv.org/abs/2105.14110",
          "publishedOn": "2021-08-20T01:53:51.683Z",
          "wordCount": 600,
          "title": "MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation. (arXiv:2105.14110v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-08-20T01:53:51.664Z",
          "wordCount": 613,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-08-20T01:53:51.658Z",
          "wordCount": 666,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Massoli_F/0/1/0/all/0/1\">Fabio Valerio Massoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akti_S/0/1/0/all/0/1\">&#x15e;eymanur Akti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Hazim Kemal Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>",
          "description": "Anomalies are ubiquitous in all scientific fields and can express an\nunexpected event due to incomplete knowledge about the data distribution or an\nunknown process that suddenly comes into play and distorts observations. Due to\nsuch events' rarity, to train deep learning models on the Anomaly Detection\n(AD) task, scientists only rely on \"normal\" data, i.e., non-anomalous samples.\nThus, letting the neural network infer the distribution beneath the input data.\nIn such a context, we propose a novel framework, named Multi-layer One-Class\nClassificAtion (MOCCA),to train and test deep learning models on the AD task.\nSpecifically, we applied it to autoencoders. A key novelty in our work stems\nfrom the explicit optimization of intermediate representations for the AD task.\nIndeed, differently from commonly used approaches that consider a neural\nnetwork as a single computational block, i.e., using the output of the last\nlayer only, MOCCA explicitly leverages the multi-layer structure of deep\narchitectures. Each layer's feature space is optimized for AD during training,\nwhile in the test phase, the deep representations extracted from the trained\nlayers are combined to detect anomalies. With MOCCA, we split the training\nprocess into two steps. First, the autoencoder is trained on the reconstruction\ntask only. Then, we only retain the encoder tasked with minimizing the L_2\ndistance between the output representation and a reference point, the\nanomaly-free training data centroid, at each considered layer. Subsequently, we\ncombine the deep features extracted at the various trained layers of the\nencoder model to detect anomalies at inference time. To assess the performance\nof the models trained with MOCCA, we conduct extensive experiments on publicly\navailable datasets. We show that our proposed method reaches comparable or\nsuperior performance to state-of-the-art approaches available in the\nliterature.",
          "link": "http://arxiv.org/abs/2012.12111",
          "publishedOn": "2021-08-20T01:53:51.651Z",
          "wordCount": 775,
          "title": "MOCCA: Multi-Layer One-Class ClassificAtion for Anomaly Detection. (arXiv:2012.12111v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03078",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weihang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High quality imaging usually requires bulky and expensive lenses to\ncompensate geometric and chromatic aberrations. This poses high constraints on\nthe optical hash or low cost applications. Although one can utilize algorithmic\nreconstruction to remove the artifacts of low-end lenses, the degeneration from\noptical aberrations is spatially varying and the computation has to trade off\nefficiency for performance. For example, we need to conduct patch-wise\noptimization or train a large set of local deep neural networks to achieve high\nreconstruction performance across the whole image. In this paper, we propose a\nPSF aware plug-and-play deep network, which takes the aberrant image and PSF\nmap as input and produces the latent high quality version via incorporating\nlens-specific deep priors, thus leading to a universal and flexible optical\naberration correction method. Specifically, we pre-train a base model from a\nset of diverse lenses and then adapt it to a given lens by quickly refining the\nparameters, which largely alleviates the time and memory consumption of model\nlearning. The approach is of high efficiency in both training and testing\nstages. Extensive results verify the promising applications of our proposed\napproach for compact low-end cameras.",
          "link": "http://arxiv.org/abs/2104.03078",
          "publishedOn": "2021-08-20T01:53:51.644Z",
          "wordCount": 660,
          "title": "Universal and Flexible Optical Aberration Correction Using Deep-Prior Based Deconvolution. (arXiv:2104.03078v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>",
          "description": "Convolutional neural networks have allowed remarkable advances in single\nimage super-resolution (SISR) over the last decade. Among recent advances in\nSISR, attention mechanisms are crucial for high-performance SR models. However,\nthe attention mechanism remains unclear on why it works and how it works in\nSISR. In this work, we attempt to quantify and visualize attention mechanisms\nin SISR and show that not all attention modules are equally beneficial. We then\npropose attention in attention network (A$^2$N) for more efficient and accurate\nSISR. Specifically, A$^2$N consists of a non-attention branch and a coupling\nattention branch. A dynamic attention module is proposed to generate weights\nfor these two branches to suppress unwanted attention adjustments dynamically,\nwhere the weights change adaptively according to the input features. This\nallows attention modules to specialize to beneficial examples without otherwise\npenalties and thus greatly improve the capacity of the attention network with\nfew parameters overhead. Experimental results demonstrate that our final model\nA$^2$N could achieve superior trade-off performances comparing with\nstate-of-the-art networks of similar sizes. Codes are available at\nhttps://github.com/haoyuc/A2N.",
          "link": "http://arxiv.org/abs/2104.09497",
          "publishedOn": "2021-08-20T01:53:51.637Z",
          "wordCount": 650,
          "title": "Attention in Attention Network for Image Super-Resolution. (arXiv:2104.09497v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado J Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "We present Region Similarity Representation Learning (ReSim), a new approach\nto self-supervised representation learning for localization-based tasks such as\nobject detection and segmentation. While existing work has largely focused on\nsolely learning global representations for an entire image, ReSim learns both\nregional representations for localization as well as semantic image-level\nrepresentations. ReSim operates by sliding a fixed-sized window across the\noverlapping area between two views (e.g., image crops), aligning these areas\nwith their corresponding convolutional feature map regions, and then maximizing\nthe feature similarity across views. As a result, ReSim learns spatially and\nsemantically consistent feature representation throughout the convolutional\nfeature maps of a neural network. A shift or scale of an image region, e.g., a\nshift or scale of an object, has a corresponding change in the feature maps;\nthis allows downstream tasks to leverage these representations for\nlocalization. Through object detection, instance segmentation, and dense pose\nestimation experiments, we illustrate how ReSim learns representations which\nsignificantly improve the localization and classification performance compared\nto a competitive MoCo-v2 baseline: $+2.7$ AP$^{\\text{bb}}_{75}$ VOC, $+1.1$\nAP$^{\\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\\text{mk}}$ Cityscapes. Code and\npre-trained models are released at: \\url{https://github.com/Tete-Xiao/ReSim}",
          "link": "http://arxiv.org/abs/2103.12902",
          "publishedOn": "2021-08-20T01:53:51.617Z",
          "wordCount": 652,
          "title": "Region Similarity Representation Learning. (arXiv:2103.12902v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1\">Tom Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1\">Nasik Nafi</a>",
          "description": "Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n\nIn the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n\nOur approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium",
          "link": "http://arxiv.org/abs/2104.04162",
          "publishedOn": "2021-08-20T01:53:51.610Z",
          "wordCount": 675,
          "title": "eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daliparthi_V/0/1/0/all/0/1\">Venkata Satya Sai Ajay Daliparthi</a>",
          "description": "In recent years, deep neural networks (DNNs) achieved state-of-the-art\nperformance on several computer vision tasks. However, the one typical drawback\nof these DNNs is the requirement of massive labeled data. Even though few-shot\nlearning methods address this problem, they often use techniques such as\nmeta-learning and metric-learning on top of the existing methods. In this work,\nwe address this problem from a neuroscience perspective by proposing a\nhypothesis named Ikshana, which is supported by several findings in\nneuroscience. Our hypothesis approximates the refining process of conceptual\ngist in the human brain while understanding a natural scene/image. While our\nhypothesis holds no particular novelty in neuroscience, it provides a novel\nperspective for designing DNNs for vision tasks. By following the Ikshana\nhypothesis, we propose a novel neural-inspired CNN architecture named\nIkshanaNet. The empirical results demonstrate the effectiveness of our method\nby outperforming several baselines on the entire and subsets of the Cityscapes\nand the CamVid semantic segmentation benchmarks.",
          "link": "http://arxiv.org/abs/2101.10837",
          "publishedOn": "2021-08-20T01:53:51.603Z",
          "wordCount": 632,
          "title": "The Ikshana Hypothesis of Human Scene Understanding Mechanism. (arXiv:2101.10837v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-20T01:53:51.595Z",
          "wordCount": 664,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "The growing number of action classes has posed a new challenge for video\nunderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.\nThe ZSAR task aims to recognize target (unseen) actions without training\nexamples by leveraging semantic representations to bridge seen and unseen\nactions. However, due to the complexity and diversity of actions, it remains\nchallenging to semantically represent action classes and transfer knowledge\nfrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired by\nan effective human memory technique Elaborative Rehearsal (ER), which involves\nelaborating a new concept and relating it to known concepts. Specifically, we\nexpand each action class as an Elaborative Description (ED) sentence, which is\nmore discriminative than a class name and less costly than manual-defined\nattributes. Besides directly aligning class semantics with videos, we\nincorporate objects from the video as Elaborative Concepts (EC) to improve\nvideo semantics and generalization from seen actions to unseen actions. Our\nER-enhanced ZSAR model achieves state-of-the-art results on three existing\nbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics\ndataset to overcome limitations of current benchmarks and demonstrate the first\ncase where ZSAR performance is comparable to few-shot learning baselines on\nthis more realistic setting. We will release our codes and collected EDs at\nhttps://github.com/DeLightCMU/ElaborativeRehearsal.",
          "link": "http://arxiv.org/abs/2108.02833",
          "publishedOn": "2021-08-20T01:53:51.589Z",
          "wordCount": 662,
          "title": "Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_N/0/1/0/all/0/1\">Neel Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1\">Anirudh Thatipelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "The lack of fine-grained joints (facial joints, hand fingers) is a\nfundamental performance bottleneck for state of the art skeleton action\nrecognition models. Despite this bottleneck, community's efforts seem to be\ninvested only in coming up with novel architectures. To specifically address\nthis bottleneck, we introduce two new pose based human action datasets -\nNTU60-X and NTU120-X. Our datasets extend the largest existing action\nrecognition dataset, NTU-RGBD. In addition to the 25 body joints for each\nskeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and\nfacial joints, enabling a richer skeleton representation. We appropriately\nmodify the state of the art approaches to enable training using the introduced\ndatasets. Our results demonstrate the effectiveness of these NTU-X datasets in\novercoming the aforementioned bottleneck and improve state of the art\nperformance, overall and on previously worst performing action categories.",
          "link": "http://arxiv.org/abs/2101.11529",
          "publishedOn": "2021-08-20T01:53:51.571Z",
          "wordCount": 643,
          "title": "NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions. (arXiv:2101.11529v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1\">Megh Shukla</a>",
          "description": "Active learning algorithms select a subset of data for annotation to maximize\nthe model performance on a budget. One such algorithm is Expected Gradient\nLength, which as the name suggests uses the approximate gradient induced per\nexample in the sampling process. While Expected Gradient Length has been\nsuccessfully used for classification and regression, the formulation for\nregression remains intuitively driven. Hence, our theoretical contribution\ninvolves deriving this formulation, thereby supporting the experimental\nevidence. Subsequently, we show that expected gradient length in regression is\nequivalent to Bayesian uncertainty. If certain assumptions are infeasible, our\nalgorithmic contribution (EGL++) approximates the effect of ensembles with a\nsingle deterministic network. Instead of computing multiple possible inferences\nper input, we leverage previously annotated samples to quantify the probability\nof previous labels being the true label. Such an approach allows us to extend\nexpected gradient length to a new task: human pose estimation. We perform\nexperimental validation on two human pose datasets (MPII and LSP/LSPET),\nhighlighting the interpretability and competitiveness of EGL++ with different\nactive learning algorithms for human pose estimation.",
          "link": "http://arxiv.org/abs/2104.09493",
          "publishedOn": "2021-08-20T01:53:51.564Z",
          "wordCount": 649,
          "title": "Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arkabandhu Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingchao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>",
          "description": "Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.",
          "link": "http://arxiv.org/abs/2101.00562",
          "publishedOn": "2021-08-20T01:53:51.558Z",
          "wordCount": 596,
          "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Taewon Kang</a>",
          "description": "Existing state-of-the-art techniques in exemplar-based image-to-image\ntranslation hold several critical concerns. Existing methods related to\nexemplar-based image-to-image translation are impossible to translate on an\nimage tuple input (source, target) that is not aligned. Additionally, we can\nconfirm that the existing method exhibits limited generalization ability to\nunseen images. In order to overcome this limitation, we propose Multiple GAN\nInversion for Exemplar-based Image-to-Image Translation. Our novel Multiple GAN\nInversion avoids human intervention by using a self-deciding algorithm to\nchoose the number of layers using Fr\\'echet Inception Distance(FID), which\nselects more plausible image reconstruction results among multiple hypotheses\nwithout any training or supervision. Experimental results have in fact, shown\nthe advantage of the proposed method compared to existing state-of-the-art\nexemplar-based image-to-image translation methods.",
          "link": "http://arxiv.org/abs/2103.14471",
          "publishedOn": "2021-08-20T01:53:51.551Z",
          "wordCount": 616,
          "title": "Multiple GAN Inversion for Exemplar-based Image-to-Image Translation. (arXiv:2103.14471v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data are available at https://github.com/skelemoa/tal-hmo.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-08-20T01:53:51.544Z",
          "wordCount": 631,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yutong Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delorme_G/0/1/0/all/0/1\">Guillaume Delorme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>",
          "description": "Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation - bounding boxes with insufficient sparse queries - is not\noptimal to learning transformers for MOT. Inspired by recent research, we\npropose TransCenter, the first transformer-based MOT architecture for dense\nheatmap predictions. Methodologically, we propose the use of dense pixel-level\nmulti-scale queries in a transformer dual-decoder network, to be able to\nglobally and robustly infer the heatmap of targets' centers and associate them\nthrough time. TransCenter outperforms the current state-of-the-art in standard\nbenchmarks both in MOT17 and MOT20. Our ablation study demonstrates the\nadvantage in the proposed architecture compared to more naive alternatives. The\ncode will be made publicly available at\nhttps://github.com/yihongxu/transcenter.",
          "link": "http://arxiv.org/abs/2103.15145",
          "publishedOn": "2021-08-20T01:53:51.538Z",
          "wordCount": 640,
          "title": "TransCenter: Transformers with Dense Queries for Multiple-Object Tracking. (arXiv:2103.15145v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1\">Dominik Rivoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_M/0/1/0/all/0/1\">Micha Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docea_R/0/1/0/all/0/1\">Reuben Docea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolbinger_F/0/1/0/all/0/1\">Fiona Kolbinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riediger_C/0/1/0/all/0/1\">Carina Riediger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1\">J&#xfc;rgen Weitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1\">Stefanie Speidel</a>",
          "description": "Research in unpaired video translation has mainly focused on short-term\ntemporal consistency by conditioning on neighboring frames. However for\ntransfer from simulated to photorealistic sequences, available information on\nthe underlying geometry offers potential for achieving global consistency\nacross views. We propose a novel approach which combines unpaired image\ntranslation with neural rendering to transfer simulated to photorealistic\nsurgical abdominal scenes. By introducing global learnable textures and a\nlighting-invariant view-consistency loss, our method produces consistent\ntranslations of arbitrary views and thus enables long-term consistent video\nsynthesis. We design and test our model to generate video sequences from\nminimally-invasive surgical abdominal scenes. Because labeled data is often\nlimited in this domain, photorealistic data where ground truth information from\nthe simulated domain is preserved is especially relevant. By extending existing\nimage-based methods to view-consistent videos, we aim to impact the\napplicability of simulated training and evaluation environments for surgical\napplications. Code and data: this http URL",
          "link": "http://arxiv.org/abs/2103.17204",
          "publishedOn": "2021-08-20T01:53:51.518Z",
          "wordCount": 641,
          "title": "Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data. (arXiv:2103.17204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engin_D/0/1/0/all/0/1\">Deniz Engin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnitzler_F/0/1/0/all/0/1\">Fran&#xe7;ois Schnitzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_N/0/1/0/all/0/1\">Ngoc Q. K. Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "High-level understanding of stories in video such as movies and TV shows from\nraw data is extremely challenging. Modern video question answering (VideoQA)\nsystems often use additional human-made sources like plot synopses, scripts,\nvideo descriptions or knowledge bases. In this work, we present a new approach\nto understand the whole story without such external sources. The secret lies in\nthe dialog: unlike any prior work, we treat dialog as a noisy source to be\nconverted into text description via dialog summarization, much like recent\nmethods treat video. The input of each modality is encoded by transformers\nindependently, and a simple fusion method combines all modalities, using soft\ntemporal attention for localization over long inputs. Our model outperforms the\nstate of the art on the KnowIT VQA dataset by a large margin, without using\nquestion-specific human annotation or human-made plot summaries. It even\noutperforms human evaluators who have never watched any whole episode before.\nCode is available at https://engindeniz.github.io/dialogsummary-videoqa",
          "link": "http://arxiv.org/abs/2103.14517",
          "publishedOn": "2021-08-20T01:53:51.511Z",
          "wordCount": 635,
          "title": "On the hidden treasure of dialog in video question answering. (arXiv:2103.14517v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "In the past few years, we have witnessed remarkable breakthroughs in\nself-supervised representation learning. Despite the success and adoption of\nrepresentations learned through this paradigm, much is yet to be understood\nabout how different training methods and datasets influence performance on\ndownstream tasks. In this paper, we analyze contrastive approaches as one of\nthe most successful and popular variants of self-supervised representation\nlearning. We perform this analysis from the perspective of the training\nalgorithms, pre-training datasets and end tasks. We examine over 700 training\nexperiments including 30 encoders, 4 pre-training datasets and 20 diverse\ndownstream tasks. Our experiments address various questions regarding the\nperformance of self-supervised models compared to their supervised\ncounterparts, current benchmarks used for evaluation, and the effect of the\npre-training data on end task performance. Our Visual Representation Benchmark\n(ViRB) is available at: https://github.com/allenai/virb.",
          "link": "http://arxiv.org/abs/2103.14005",
          "publishedOn": "2021-08-20T01:53:51.504Z",
          "wordCount": 608,
          "title": "Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heeseung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Manjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>",
          "description": "Spatio-temporal convolution often fails to learn motion dynamics in videos\nand thus an effective motion representation is required for video understanding\nin the wild. In this paper, we propose a rich and robust motion representation\nbased on spatio-temporal self-similarity (STSS). Given a sequence of frames,\nSTSS represents each local region as similarities to its neighbors in space and\ntime. By converting appearance features into relational values, it enables the\nlearner to better recognize structural patterns in space and time. We leverage\nthe whole volume of STSS and let our model learn to extract an effective motion\nrepresentation from it. The proposed neural block, dubbed SELFY, can be easily\ninserted into neural architectures and trained end-to-end without additional\nsupervision. With a sufficient volume of the neighborhood in space and time, it\neffectively captures long-term interaction and fast motion in the video,\nleading to robust action recognition. Our experimental analysis demonstrates\nits superiority over previous methods for motion modeling as well as its\ncomplementarity to spatio-temporal features from direct convolution. On the\nstandard action recognition benchmarks, Something-Something-V1 & V2, Diving-48,\nand FineGym, the proposed method achieves the state-of-the-art results.",
          "link": "http://arxiv.org/abs/2102.07092",
          "publishedOn": "2021-08-20T01:53:51.497Z",
          "wordCount": 663,
          "title": "Learning Self-Similarity in Space and Time as Generalized Motion for Action Recognition. (arXiv:2102.07092v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_D/0/1/0/all/0/1\">Doug Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>",
          "description": "Proxy-based metric learning losses are superior to pair-based losses due to\ntheir fast convergence and low training complexity. However, existing\nproxy-based losses focus on learning class-discriminative features while\noverlooking the commonalities shared across classes which are potentially\nuseful in describing and matching samples. Moreover, they ignore the implicit\nhierarchy of categories in real-world datasets, where similar subordinate\nclasses can be grouped together. In this paper, we present a framework that\nleverages this implicit hierarchy by imposing a hierarchical structure on the\nproxies and can be used with any existing proxy-based loss. This allows our\nmodel to capture both class-discriminative features and class-shared\ncharacteristics without breaking the implicit data hierarchy. We evaluate our\nmethod on five established image retrieval datasets such as In-Shop and SOP.\nResults demonstrate that our hierarchical proxy-based loss framework improves\nthe performance of existing proxy-based losses, especially on large datasets\nwhich exhibit strong hierarchical structure.",
          "link": "http://arxiv.org/abs/2103.13538",
          "publishedOn": "2021-08-20T01:53:51.490Z",
          "wordCount": 612,
          "title": "Hierarchical Proxy-based Loss for Deep Metric Learning. (arXiv:2103.13538v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwase_S/0/1/0/all/0/1\">Shun Iwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokota_R/0/1/0/all/0/1\">Rio Yokota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>",
          "description": "We present RePOSE, a fast iterative refinement method for 6D object pose\nestimation. Prior methods perform refinement by feeding zoomed-in input and\nrendered RGB images into a CNN and directly regressing an update of a refined\npose. Their runtime is slow due to the computational cost of CNN, which is\nespecially prominent in multiple-object pose refinement. To overcome this\nproblem, RePOSE leverages image rendering for fast feature extraction using a\n3D model with a learnable texture. We call this deep texture rendering, which\nuses a shallow multi-layer perceptron to directly regress a view-invariant\nimage representation of an object. Furthermore, we utilize differentiable\nLevenberg-Marquardt (LM) optimization to refine a pose fast and accurately by\nminimizing the feature-metric error between the input and rendered image\nrepresentations without the need of zooming in. These image representations are\ntrained such that differentiable LM optimization converges within few\niterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art\naccuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute\nimprovement over the prior art, and comparable result on the YCB-Video dataset\nwith a much faster runtime. The code is available at\nhttps://github.com/sh8/repose.",
          "link": "http://arxiv.org/abs/2104.00633",
          "publishedOn": "2021-08-20T01:53:51.471Z",
          "wordCount": 664,
          "title": "RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering. (arXiv:2104.00633v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Temporal action proposal generation is an important and challenging task in\nvideo understanding, which aims at detecting all temporal segments containing\naction instances of interest. The existing proposal generation approaches are\ngenerally based on pre-defined anchor windows or heuristic bottom-up boundary\nmatching strategies. This paper presents a simple and efficient framework\n(RTD-Net) for direct action proposal generation, by re-purposing a\nTransformer-alike architecture. To tackle the essential visual difference\nbetween time and space, we make three important improvements over the original\ntransformer detection framework (DETR). First, to deal with slowness prior in\nvideos, we replace the original Transformer encoder with a boundary attentive\nmodule to better capture long-range temporal information. Second, due to the\nambiguous temporal boundary and relatively sparse annotations, we present a\nrelaxed matching scheme to relieve the strict criteria of single assignment to\neach groundtruth. Finally, we devise a three-branch head to further improve the\nproposal confidence estimation by explicitly predicting its completeness.\nExtensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate\nthe effectiveness of RTD-Net, on both tasks of temporal action proposal\ngeneration and temporal action detection. Moreover, due to its simplicity in\ndesign, our framework is more efficient than previous proposal generation\nmethods, without non-maximum suppression post-processing. The code and models\nare made available at https://github.com/MCG-NJU/RTD-Action.",
          "link": "http://arxiv.org/abs/2102.01894",
          "publishedOn": "2021-08-20T01:53:51.464Z",
          "wordCount": 689,
          "title": "Relaxed Transformer Decoders for Direct Action Proposal Generation. (arXiv:2102.01894v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1\">Ardhendu Shekhar Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "Learning in a low-data regime from only a few labeled examples is an\nimportant, but challenging problem. Recent advancements within meta-learning\nhave demonstrated encouraging performance, in particular, for the task of\nfew-shot classification. We propose a novel optimization-based meta-learning\napproach for few-shot classification. It consists of an embedding network,\nproviding a general representation of the image, and a base learner module. The\nlatter learns a linear classifier during the inference through an unrolled\noptimization procedure. We design an inner learning objective composed of (i) a\nrobust classification loss on the support set and (ii) an entropy loss,\nallowing transductive learning from unlabeled query samples. By employing an\nefficient initialization module and a Steepest Descent based optimization\nalgorithm, our base learner predicts a powerful classifier within only a few\niterations. Further, our strategy enables important aspects of the base learner\nobjective to be learned during meta-training. To the best of our knowledge,\nthis work is the first to integrate both induction and transduction into the\nbase learner in an optimization-based meta-learning framework. We perform a\ncomprehensive experimental analysis, demonstrating the effectiveness of our\napproach on four few-shot classification datasets.",
          "link": "http://arxiv.org/abs/2010.00511",
          "publishedOn": "2021-08-20T01:53:51.453Z",
          "wordCount": 656,
          "title": "Few-Shot Classification By Few-Iteration Meta-Learning. (arXiv:2010.00511v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "Convolutional neural networks (CNNs) have developed to become powerful models\nfor various computer vision tasks ranging from object detection to semantic\nsegmentation. However, most of the state-of-the-art CNNs cannot be deployed\ndirectly on edge devices such as smartphones and drones, which need low latency\nunder limited power and memory bandwidth. One popular, straightforward approach\nto compressing CNNs is network slimming, which imposes $\\ell_1$ regularization\non the channel-associated scaling factors via the batch normalization layers\nduring training. Network slimming thereby identifies insignificant channels\nthat can be pruned for inference. In this paper, we propose replacing the\n$\\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty in\norder to yield a more compressed and/or accurate CNN architecture. We\ninvestigate $\\ell_p (0 < p < 1)$, transformed $\\ell_1$ (T$\\ell_1$), minimax\nconcave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to\ntheir recent successes and popularity in solving sparse optimization problems,\nsuch as compressed sensing and variable selection. We demonstrate the\neffectiveness of network slimming with nonconvex penalties on three neural\nnetwork architectures -- VGG-19, DenseNet-40, and ResNet-164 -- on standard\nimage classification datasets. Based on the numerical experiments, T$\\ell_1$\npreserves model accuracy against channel pruning, $\\ell_{1/2, 3/4}$ yield\nbetter compressed models with similar accuracies after retraining as $\\ell_1$,\nand MCP and SCAD provide more accurate models after retraining with similar\ncompression as $\\ell_1$. Network slimming with T$\\ell_1$ regularization also\noutperforms the latest Bayesian modification of network slimming in compressing\na CNN architecture in terms of memory storage while preserving its model\naccuracy after channel pruning.",
          "link": "http://arxiv.org/abs/2010.01242",
          "publishedOn": "2021-08-20T01:53:51.446Z",
          "wordCount": 763,
          "title": "Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Adversarial attack algorithms are dominated by penalty methods, which are\nslow in practice, or more efficient distance-customized methods, which are\nheavily tailored to the properties of the distance considered. We propose a\nwhite-box attack algorithm to generate minimally perturbed adversarial examples\nbased on Augmented Lagrangian principles. We bring several algorithmic\nmodifications, which have a crucial effect on performance. Our attack enjoys\nthe generality of penalty methods and the computational efficiency of\ndistance-customized algorithms, and can be readily used for a wide set of\ndistances. We compare our attack to state-of-the-art methods on three datasets\nand several models, and consistently obtain competitive performances with\nsimilar or lower computational complexity.",
          "link": "http://arxiv.org/abs/2011.11857",
          "publishedOn": "2021-08-20T01:53:51.438Z",
          "wordCount": 578,
          "title": "Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1\">Udbhav Bamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_R/0/1/0/all/0/1\">Rishabh Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>",
          "description": "Deeper and wider CNNs are known to provide improved performance for deep\nlearning tasks. However, most such networks have poor performance gain per\nparameter increase. In this paper, we investigate whether the gain observed in\ndeeper models is purely due to the addition of more optimization parameters or\nwhether the physical size of the network as well plays a role. Further, we\npresent a novel rescaling strategy for CNNs based on learnable repetition of\nits parameters. Based on this strategy, we rescale CNNs without changing their\nparameter count, and show that learnable sharing of weights itself can provide\nsignificant boost in the performance of any given model without changing its\nparameter count. We show that small base networks when rescaled, can provide\nperformance comparable to deeper networks with as low as 6% of optimization\nparameters of the deeper one.\n\nThe relevance of weight sharing is further highlighted through the example of\ngroup-equivariant CNNs. We show that the significant improvements obtained with\ngroup-equivariant CNNs over the regular CNNs on classification problems are\nonly partly due to the added equivariance property, and part of it comes from\nthe learnable repetition of network weights. For rot-MNIST dataset, we show\nthat up to 40% of the relative gain reported by state-of-the-art methods for\nrotation equivariance could actually be due to just the learnt repetition of\nweights.",
          "link": "http://arxiv.org/abs/2101.05650",
          "publishedOn": "2021-08-20T01:53:51.420Z",
          "wordCount": 684,
          "title": "Rescaling CNN through Learnable Repetition of Network Parameters. (arXiv:2101.05650v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aayush Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debnath_S/0/1/0/all/0/1\">Shoubhik Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafleche_J/0/1/0/all/0/1\">Jean-Francois Lafleche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cameracci_E/0/1/0/all/0/1\">Eric Cameracci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+State_G/0/1/0/all/0/1\">Gavriel State</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1\">Stan Birchfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>",
          "description": "Synthetic data is emerging as a promising solution to the scalability issue\nof supervised deep learning, especially when real data are difficult to acquire\nor hard to annotate. Synthetic data generation, however, can itself be\nprohibitively expensive when domain experts have to manually and painstakingly\noversee the process. Moreover, neural networks trained on synthetic data often\ndo not perform well on real data because of the domain gap. To solve these\nchallenges, we propose Sim2SG, a self-supervised automatic scene generation\ntechnique for matching the distribution of real data. Importantly, Sim2SG does\nnot require supervision from the real-world dataset, thus making it applicable\nin situations for which such annotations are difficult to obtain. Sim2SG is\ndesigned to bridge both the content and appearance gaps, by matching the\ncontent of real data, and by matching the features in the source and target\ndomains. We select scene graph (SG) generation as the downstream task, due to\nthe limited availability of labeled datasets. Experiments demonstrate\nsignificant improvements over leading baselines in reducing the domain gap both\nqualitatively and quantitatively, on several synthetic datasets as well as the\nreal-world KITTI dataset.",
          "link": "http://arxiv.org/abs/2011.14488",
          "publishedOn": "2021-08-20T01:53:51.399Z",
          "wordCount": 662,
          "title": "Self-Supervised Real-to-Sim Scene Generation. (arXiv:2011.14488v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-08-20T01:53:51.392Z",
          "wordCount": 703,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>",
          "description": "Generating high-fidelity talking head video by fitting with the input audio\nsequence is a challenging problem that receives considerable attentions\nrecently. In this paper, we address this problem with the aid of neural scene\nrepresentation networks. Our method is completely different from existing\nmethods that rely on intermediate representations like 2D landmarks or 3D face\nmodels to bridge the gap between audio input and video output. Specifically,\nthe feature of input audio signal is directly fed into a conditional implicit\nfunction to generate a dynamic neural radiance field, from which a\nhigh-fidelity talking-head video corresponding to the audio signal is\nsynthesized using volume rendering. Another advantage of our framework is that\nnot only the head (with hair) region is synthesized as previous methods did,\nbut also the upper body is generated via two individual neural radiance fields.\nExperimental results demonstrate that our novel framework can (1) produce\nhigh-fidelity and natural results, and (2) support free adjustment of audio\nsignals, viewing directions, and background images. Code is available at\nhttps://github.com/YudongGuo/AD-NeRF.",
          "link": "http://arxiv.org/abs/2103.11078",
          "publishedOn": "2021-08-20T01:53:51.385Z",
          "wordCount": 664,
          "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1007.3881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "In this paper orthogonal multifilters for astronomical image processing are\npresented. We obtained new orthogonal multifilters based on the orthogonal\nwavelet of Haar and Daubechies. Recently, multiwavelets have been introduced as\na more powerful multiscale analysis tool. It adds several degrees of freedom in\nmultifilter design and makes it possible to have several useful properties such\nas symmetry, orthogonality, short support, and a higher number of vanishing\nmoments simultaneously. Multifilter decomposition of scanned photographic\nplates with astronomical images is made.",
          "link": "http://arxiv.org/abs/1007.3881",
          "publishedOn": "2021-08-20T01:53:51.378Z",
          "wordCount": 579,
          "title": "Orthogonal multifilters image processing of astronomical images from scanned photographic plates. (arXiv:1007.3881v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Human-Object Interaction (HOI) detection is a fundamental visual task aiming\nat localizing and recognizing interactions between humans and objects. Existing\nworks focus on the visual and linguistic features of humans and objects.\nHowever, they do not capitalise on the high-level and semantic relationships\npresent in the image, which provides crucial contextual and detailed relational\nknowledge for HOI inference. We propose a novel method to exploit this\ninformation, through the scene graph, for the Human-Object Interaction (SG2HOI)\ndetection task. Our method, SG2HOI, incorporates the SG information in two\nways: (1) we embed a scene graph into a global context clue, serving as the\nscene-specific environmental context; and (2) we build a relation-aware\nmessage-passing module to gather relationships from objects' neighborhood and\ntransfer them into interactions. Empirical evaluation shows that our SG2HOI\nmethod outperforms the state-of-the-art methods on two benchmark HOI datasets:\nV-COCO and HICO-DET. Code will be available at https://github.com/ht014/SG2HOI.",
          "link": "http://arxiv.org/abs/2108.08584",
          "publishedOn": "2021-08-20T01:53:51.358Z",
          "wordCount": 589,
          "title": "Exploiting Scene Graphs for Human-Object Interaction Detection. (arXiv:2108.08584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.07133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooklev_T/0/1/0/all/0/1\">Todor Cooklev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keinert_F/0/1/0/all/0/1\">Fritz Keinert</a>",
          "description": "We consider the design of an orthogonal symmetric/antisymmetric multiwavelet\nfrom its matrix product filter by matrix spectral factorization (MSF). As a\ntest problem, we construct a simple matrix product filter with desirable\nproperties, and factor it using Bauer's method, which in this case can be done\nin closed form. The corresponding orthogonal multiwavelet function is derived\nusing algebraic techniques which allow symmetry to be considered. This leads to\nthe known orthogonal multiwavelet SA1, which can also be derived directly. We\nalso give a lifting scheme for SA1, investigate the influence of the number of\nsignificant digits in the calculations, and show some numerical experiments.",
          "link": "http://arxiv.org/abs/1910.07133",
          "publishedOn": "2021-08-20T01:53:51.351Z",
          "wordCount": 647,
          "title": "Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral Factorization. (arXiv:1910.07133v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>",
          "description": "This paper proposes a novel location-aware deep-learning-based single image\nreflection removal method. Our network has a reflection detection module to\nregress a probabilistic reflection confidence map, taking multi-scale Laplacian\nfeatures as inputs. This probabilistic map tells if a region is\nreflection-dominated or transmission-dominated, and it is used as a cue for the\nnetwork to control the feature flow when predicting the reflection and\ntransmission layers. We design our network as a recurrent network to\nprogressively refine reflection removal results at each iteration. The novelty\nis that we leverage Laplacian kernel parameters to emphasize the boundaries of\nstrong reflections. It is beneficial to strong reflection detection and\nsubstantially improves the quality of reflection removal results. Extensive\nexperiments verify the superior performance of the proposed method over\nstate-of-the-art approaches. Our code and the pre-trained model can be found at\nhttps://github.com/zdlarr/Location-aware-SIRR.",
          "link": "http://arxiv.org/abs/2012.07131",
          "publishedOn": "2021-08-20T01:53:51.339Z",
          "wordCount": 613,
          "title": "Location-aware Single Image Reflection Removal. (arXiv:2012.07131v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katircioglu_I/0/1/0/all/0/1\">Isinsu Katircioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sporri_J/0/1/0/all/0/1\">J&#xf6;rg Sp&#xf6;rri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Self-supervised detection and segmentation of foreground objects aims for\naccuracy without annotated training data. However, existing approaches\npredominantly rely on restrictive assumptions on appearance and motion. For\nscenes with dynamic activities and camera motion, we propose a multi-camera\nframework in which geometric constraints are embedded in the form of multi-view\nconsistency during training via coarse 3D localization in a voxel grid and\nfine-grained offset regression. In this manner, we learn a joint distribution\nof proposals over multiple views. At inference time, our method operates on\nsingle RGB images. We outperform state-of-the-art techniques both on images\nthat visually depart from those of standard benchmarks and on those of the\nclassical Human3.6M dataset.",
          "link": "http://arxiv.org/abs/2012.05119",
          "publishedOn": "2021-08-20T01:53:51.333Z",
          "wordCount": 577,
          "title": "Human Detection and Segmentation via Multi-view Consensus. (arXiv:2012.05119v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sudipta Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>",
          "description": "Prior works on text-based video moment localization focus on temporally\ngrounding the textual query in an untrimmed video. These works assume that the\nrelevant video is already known and attempt to localize the moment on that\nrelevant video only. Different from such works, we relax this assumption and\naddress the task of localizing moments in a corpus of videos for a given\nsentence query. This task poses a unique challenge as the system is required to\nperform: (i) retrieval of the relevant video where only a segment of the video\ncorresponds with the queried sentence, and (ii) temporal localization of moment\nin the relevant video based on sentence query. Towards overcoming this\nchallenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns\nan effective joint embedding space for moments and sentences. In addition to\nlearning subtle differences between intra-video moments, HMAN focuses on\ndistinguishing inter-video global semantic concepts based on sentence queries.\nQualitative and quantitative results on three benchmark text-based video moment\nretrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions -\ndemonstrate that our method achieves promising performance on the proposed task\nof temporal localization of moments in a corpus of videos.",
          "link": "http://arxiv.org/abs/2008.08716",
          "publishedOn": "2021-08-20T01:53:51.326Z",
          "wordCount": 657,
          "title": "Text-based Localization of Moments in a Video Corpus. (arXiv:2008.08716v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1\">Rishabh Dabral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arjun Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>",
          "description": "This paper proposes GraviCap, i.e., a new approach for joint markerless 3D\nhuman motion capture and object trajectory estimation from monocular RGB\nvideos. We focus on scenes with objects partially observed during a free\nflight. In contrast to existing monocular methods, we can recover scale, object\ntrajectories as well as human bone lengths in meters and the ground plane's\norientation, thanks to the awareness of the gravity constraining object\nmotions. Our objective function is parametrised by the object's initial\nvelocity and position, gravity direction and focal length, and jointly\noptimised for one or several free flight episodes. The proposed human-object\ninteraction constraints ensure geometric consistency of the 3D reconstructions\nand improved physical plausibility of human poses compared to the unconstrained\ncase. We evaluate GraviCap on a new dataset with ground-truth annotations for\npersons and different objects undergoing free flights. In the experiments, our\napproach achieves state-of-the-art accuracy in 3D human motion capture on\nvarious metrics. We urge the reader to watch our supplementary video. Both the\nsource code and the dataset are released; see\nthis http URL",
          "link": "http://arxiv.org/abs/2108.08844",
          "publishedOn": "2021-08-20T01:53:51.309Z",
          "wordCount": 630,
          "title": "Gravity-Aware Monocular 3D Human-Object Reconstruction. (arXiv:2108.08844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr",
          "link": "http://arxiv.org/abs/2108.08839",
          "publishedOn": "2021-08-20T01:53:51.301Z",
          "wordCount": 663,
          "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_U/0/1/0/all/0/1\">Utako Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakao_M/0/1/0/all/0/1\">Megumi Nakao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohzeki_M/0/1/0/all/0/1\">Masayuki Ohzeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokuno_J/0/1/0/all/0/1\">Junko Tokuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Yoshikawa_T/0/1/0/all/0/1\">Toyofumi Fengshi Chen-Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1\">Tetsuya Matsuda</a>",
          "description": "In video-assisted thoracoscopic surgeries, successful procedures of nodule\nresection are highly dependent on the precise estimation of lung deformation\nbetween the inflated lung in the computed tomography (CT) images during\npreoperative planning and the deflated lung in the treatment views during\nsurgery. Lungs in the pneumothorax state during surgery have a large volume\nchange from normal lungs, making it difficult to build a mechanical model. The\npurpose of this study is to develop a deformation estimation method of the 3D\nsurface of a deflated lung from a few partial observations. To estimate\ndeformations for a largely deformed lung, a kernel regression-based solution\nwas introduced. The proposed method used a few landmarks to capture the partial\ndeformation between the 3D surface mesh obtained from preoperative CT and the\nintraoperative anatomical positions. The deformation for each vertex of the\nentire mesh model was estimated per-vertex as a relative position from the\nlandmarks. The landmarks were placed in the anatomical position of the lung's\nouter contour. The method was applied on nine datasets of the left lungs of\nlive Beagle dogs. Contrast-enhanced CT images of the lungs were acquired. The\nproposed method achieved a local positional error of vertices of 2.74 mm,\nHausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94.\nMoreover, the proposed method could estimate lung deformations from a small\nnumber of training cases and a small observation area. This study contributes\nto the data-driven modeling of pneumothorax deformation of the lung.",
          "link": "http://arxiv.org/abs/2102.12505",
          "publishedOn": "2021-08-20T01:53:51.294Z",
          "wordCount": 740,
          "title": "Kernel-based framework to estimate deformations of pneumothorax lung using relative position of anatomical landmarks. (arXiv:2102.12505v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McKee_D/0/1/0/all/0/1\">Daniel McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1\">Bing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berneshawi_A/0/1/0/all/0/1\">Andrew Berneshawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>",
          "description": "In this paper, we explore learning end-to-end deep neural trackers without\ntracking annotations. This is important as large-scale training data is\nessential for training deep neural trackers while tracking annotations are\nexpensive to acquire. In place of tracking annotations, we first hallucinate\nvideos from images with bounding box annotations using zoom-in/out motion\ntransformations to obtain free tracking labels. We add video simulation\naugmentations to create a diverse tracking dataset, albeit with simple motion.\nNext, to tackle harder tracking cases, we mine hard examples across an\nunlabeled pool of real videos with a tracker trained on our hallucinated video\ndata. For hard example mining, we propose an optimization-based connecting\nprocess to first identify and then rectify hard examples from the pool of\nunlabeled videos. Finally, we train our tracker jointly on hallucinated data\nand mined hard video examples. Our weakly supervised tracker achieves\nstate-of-the-art performance on the MOT17 and TAO-person datasets. On MOT17, we\nfurther demonstrate that the combination of our self-generated data and the\nexisting manually-annotated data leads to additional improvements.",
          "link": "http://arxiv.org/abs/2108.08836",
          "publishedOn": "2021-08-20T01:53:51.286Z",
          "wordCount": 611,
          "title": "Multi-Object Tracking with Hallucinated and Unlabeled Videos. (arXiv:2108.08836v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1\">David Belius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Convolutional neural networks (CNNs) have been tremendously successful in\nsolving imaging inverse problems. To understand their success, an effective\nstrategy is to construct simpler and mathematically more tractable\nconvolutional sparse coding (CSC) models that share essential ingredients with\nCNNs. Existing CSC methods, however, underperform leading CNNs in challenging\ninverse problems. We hypothesize that the performance gap may be attributed in\npart to how they process images at different spatial scales: While many CNNs\nuse multiscale feature representations, existing CSC models mostly rely on\nsingle-scale dictionaries. To close the performance gap, we thus propose a\nmultiscale convolutional dictionary structure. The proposed dictionary\nstructure is derived from the U-Net, arguably the most versatile and widely\nused CNN for image-to-image learning problems. We show that incorporating the\nproposed multiscale dictionary in an otherwise standard CSC framework yields\nperformance competitive with state-of-the-art CNNs across a range of\nchallenging inverse problems including CT and MRI reconstruction. Our work thus\ndemonstrates the effectiveness and scalability of the multiscale CSC approach\nin solving challenging inverse problems.",
          "link": "http://arxiv.org/abs/2011.12815",
          "publishedOn": "2021-08-20T01:53:51.274Z",
          "wordCount": 641,
          "title": "Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>",
          "description": "Video Visual Relation Detection (VidVRD), has received significant attention\nof our community over recent years. In this paper, we apply the\nstate-of-the-art video object tracklet detection pipeline MEGA and deepSORT to\ngenerate tracklet proposals. Then we perform VidVRD in a tracklet-based manner\nwithout any pre-cutting operations. Specifically, we design a tracklet-based\nvisual Transformer. It contains a temporal-aware decoder which performs feature\ninteractions between the tracklets and learnable predicate query embeddings,\nand finally predicts the relations. Experimental results strongly demonstrate\nthe superiority of our method, which outperforms other methods by a large\nmargin on the Video Relation Understanding (VRU) Grand Challenge in ACM\nMultimedia 2021. Codes are released at\nhttps://github.com/Dawn-LX/VidVRD-tracklets.",
          "link": "http://arxiv.org/abs/2108.08669",
          "publishedOn": "2021-08-20T01:53:51.267Z",
          "wordCount": 561,
          "title": "Video Relation Detection via Tracklet based Visual Transformer. (arXiv:2108.08669v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00081",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Barzekar_H/0/1/0/all/0/1\">Hosein Barzekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zeyun Yu</a>",
          "description": "Cancers are the leading cause of death in many countries. Early diagnosis\nplays a crucial role in having proper treatment for this debilitating disease.\nThe automated classification of the type of cancer is a challenging task since\npathologists must examine a huge number of histopathological images to detect\ninfinitesimal abnormalities. In this study, we propose a novel convolutional\nneural network (CNN) architecture composed of a Concatenation of multiple\nNetworks, called C-Net, to classify biomedical images. The model incorporates\nmultiple CNNs including Outer, Middle, and Inner. The first two parts of the\narchitecture contain six networks that serve as feature extractors to feed into\nthe Inner network to classify the images in terms of malignancy and benignancy.\nThe C-Net is applied for histopathological image classification on two public\ndatasets, including BreakHis and Osteosarcoma. To evaluate the performance, the\nmodel is tested using several evaluation metrics for its reliability. The C-Net\nmodel outperforms all other models on the individual metrics for both datasets\nand achieves zero misclassification. This approach has the potential to be\nextended to additional classification tasks, as experimental results\ndemonstrate utilizing extensive evaluation metrics.",
          "link": "http://arxiv.org/abs/2011.00081",
          "publishedOn": "2021-08-20T01:53:51.249Z",
          "wordCount": 647,
          "title": "C-Net: A Reliable Convolutional Neural Network for Biomedical Image Classification. (arXiv:2011.00081v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1310.1869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkova_K/0/1/0/all/0/1\">Katya Tsvetkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_M/0/1/0/all/0/1\">Milcho Tsvetkov</a>",
          "description": "We want to approximate the mxn image A from scanned astronomical photographic\nplates (from the Sofia Sky Archive Data Center) by using far fewer entries than\nin the original matrix. By using rank of a matrix, k we remove the redundant\ninformation or noise and use as Wiener filter, when rank k<m or k<n. With this\napproximation more than 98% compression ration of image of astronomical plate\nwithout that image details, is obtained. The SVD of images from scanned\nphotographic plates (SPP) is considered and its possible image compression.",
          "link": "http://arxiv.org/abs/1310.1869",
          "publishedOn": "2021-08-20T01:53:51.242Z",
          "wordCount": 616,
          "title": "Singular Value Decomposition of Images from Scanned Photographic Plates. (arXiv:1310.1869v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.04390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Meta-learning has been the most common framework for few-shot learning in\nrecent years. It learns the model from collections of few-shot classification\ntasks, which is believed to have a key advantage of making the training\nobjective consistent with the testing objective. However, some recent works\nreport that by training for whole-classification, i.e. classification on the\nwhole label-set, it can get comparable or even better embedding than many\nmeta-learning algorithms. The edge between these two lines of works has yet\nbeen underexplored, and the effectiveness of meta-learning in few-shot learning\nremains unclear. In this paper, we explore a simple process: meta-learning over\na whole-classification pre-trained model on its evaluation metric. We observe\nthis simple method achieves competitive performance to state-of-the-art methods\non standard benchmarks. Our further analysis shed some light on understanding\nthe trade-offs between the meta-learning objective and the whole-classification\nobjective in few-shot learning.",
          "link": "http://arxiv.org/abs/2003.04390",
          "publishedOn": "2021-08-20T01:53:51.235Z",
          "wordCount": 639,
          "title": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">Sunghoon Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minjun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "To reconstruct a 3D scene from a set of calibrated views, traditional\nmulti-view stereo techniques rely on two distinct stages: local depth maps\ncomputation and global depth maps fusion. Recent studies concentrate on deep\nneural architectures for depth estimation by using conventional depth fusion\nmethod or direct 3D reconstruction network by regressing Truncated Signed\nDistance Function (TSDF). In this paper, we advocate that replicating the\ntraditional two stages framework with deep neural networks improves both the\ninterpretability and the accuracy of the results. As mentioned, our network\noperates in two steps: 1) the local computation of the local depth maps with a\ndeep MVS technique, and, 2) the depth maps and images' features fusion to build\na single TSDF volume. In order to improve the matching performance between\nimages acquired from very different viewpoints (e.g., large-baseline and\nrotations), we introduce a rotation-invariant 3D convolution kernel called\nPosedConv. The effectiveness of the proposed architecture is underlined via a\nlarge series of experiments conducted on the ScanNet dataset where our approach\ncompares favorably against both traditional and deep learning techniques.",
          "link": "http://arxiv.org/abs/2108.08623",
          "publishedOn": "2021-08-20T01:53:51.229Z",
          "wordCount": 621,
          "title": "VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction. (arXiv:2108.08623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Category-level 6D pose estimation, aiming to predict the location and\norientation of unseen object instances, is fundamental to many scenarios such\nas robotic manipulation and augmented reality, yet still remains unsolved.\nPrecisely recovering instance 3D model in the canonical space and accurately\nmatching it with the observation is an essential point when estimating 6D pose\nfor unseen objects. In this paper, we achieve accurate category-level 6D pose\nestimation via cascaded relation and recurrent reconstruction networks.\nSpecifically, a novel cascaded relation network is dedicated for advanced\nrepresentation learning to explore the complex and informative relations among\ninstance RGB image, instance point cloud and category shape prior. Furthermore,\nwe design a recurrent reconstruction network for iterative residual refinement\nto progressively improve the reconstruction and correspondence estimations from\ncoarse to fine. Finally, the instance 6D pose is obtained leveraging the\nestimated dense correspondences between the instance point cloud and the\nreconstructed 3D model in the canonical space. We have conducted extensive\nexperiments on two well-acknowledged benchmarks of category-level 6D pose\nestimation, with significant performance improvement over existing approaches.\nOn the representatively strict evaluation metrics of $3D_{75}$ and $5^{\\circ}2\ncm$, our method exceeds the latest state-of-the-art SPD by $4.9\\%$ and $17.7\\%$\non the CAMERA25 dataset, and by $2.7\\%$ and $8.5\\%$ on the REAL275 dataset.\nCodes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.",
          "link": "http://arxiv.org/abs/2108.08755",
          "publishedOn": "2021-08-20T01:53:51.198Z",
          "wordCount": 663,
          "title": "Category-Level 6D Object Pose Estimation via Cascaded Relation and Recurrent Reconstruction Networks. (arXiv:2108.08755v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.",
          "link": "http://arxiv.org/abs/2004.03744",
          "publishedOn": "2021-08-20T01:53:51.191Z",
          "wordCount": 621,
          "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to adapt existing models of the\nsource domain to a new target domain with only unlabeled data. Most existing\nmethods suffer from noticeable negative transfer resulting from either the\nerror-prone discriminator network or the unreasonable teacher model. Besides,\nthe local regional consistency in UDA has been largely neglected, and only\nextracting the global-level pattern information is not powerful enough for\nfeature alignment due to the abuse use of contexts. To this end, we propose an\nuncertainty-aware consistency regularization method for cross-domain semantic\nsegmentation. Firstly, we introduce an uncertainty-guided consistency loss with\na dynamic weighting scheme by exploiting the latent uncertainty information of\nthe target samples. As such, more meaningful and reliable knowledge from the\nteacher model can be transferred to the student model. We further reveal the\nreason why the current consistency regularization is often unstable in\nminimizing the domain discrepancy. Besides, we design a ClassDrop mask\ngeneration algorithm to produce strong class-wise perturbations. Guided by this\nmask, we propose a ClassOut strategy to realize effective regional consistency\nin a fine-grained manner. Experiments demonstrate that our method outperforms\nthe state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV\n$\\rightarrow $ Cityscapes and SYNTHIA $\\rightarrow $ Cityscapes, Virtual KITTI\n$\\rightarrow$ KITTI and Cityscapes $\\rightarrow$ KITTI.",
          "link": "http://arxiv.org/abs/2004.08878",
          "publishedOn": "2021-08-20T01:53:51.184Z",
          "wordCount": 696,
          "title": "Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation. (arXiv:2004.08878v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1\">Ansheng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>",
          "description": "Adaptive and flexible image editing is a desirable function of modern\ngenerative models. In this work, we present a generative model with\nauto-encoder architecture for per-region style manipulation. We apply a code\nconsistency loss to enforce an explicit disentanglement between content and\nstyle latent representations, making the content and style of generated samples\nconsistent with their corresponding content and style references. The model is\nalso constrained by a content alignment loss to ensure the foreground editing\nwill not interfere background contents. As a result, given interested region\nmasks provided by users, our model supports foreground region-wise style\ntransfer. Specially, our model receives no extra annotations such as semantic\nlabels except for self-supervision. Extensive experiments show the\neffectiveness of the proposed method and exhibit the flexibility of the\nproposed model for various applications, including region-wise style editing,\nlatent space interpolation, cross-domain style transfer.",
          "link": "http://arxiv.org/abs/2108.08674",
          "publishedOn": "2021-08-20T01:53:51.177Z",
          "wordCount": 581,
          "title": "Towards Controllable and Photorealistic Region-wise Image Manipulation. (arXiv:2108.08674v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Garvita Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll1_G/0/1/0/all/0/1\">Gerard Pons-Moll1</a>",
          "description": "We present Neural Generalized Implicit Functions(Neural-GIF), to animate\npeople in clothing as a function of the body pose. Given a sequence of scans of\na subject in various poses, we learn to animate the character for new poses.\nExisting methods have relied on template-based representations of the human\nbody (or clothing). However such models usually have fixed and limited\nresolutions, require difficult data pre-processing steps and cannot be used\nwith complex clothing. We draw inspiration from template-based methods, which\nfactorize motion into articulation and non-rigid deformation, but generalize\nthis concept for implicit shape learning to obtain a more flexible model. We\nlearn to map every point in the space to a canonical space, where a learned\ndeformation field is applied to model non-rigid effects, before evaluating the\nsigned distance field. Our formulation allows the learning of complex and\nnon-rigid deformations of clothing and soft tissue, without computing a\ntemplate registration as it is common with current approaches. Neural-GIF can\nbe trained on raw 3D scans and reconstructs detailed complex surface geometry\nand deformations. Moreover, the model can generalize to new poses. We evaluate\nour method on a variety of characters from different public datasets in diverse\nclothing styles and show significant improvements over baseline methods,\nquantitatively and qualitatively. We also extend our model to multiple shape\nsetting. To stimulate further research, we will make the model, code and data\npublicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/",
          "link": "http://arxiv.org/abs/2108.08807",
          "publishedOn": "2021-08-20T01:53:51.170Z",
          "wordCount": 675,
          "title": "Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing. (arXiv:2108.08807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>",
          "description": "Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.",
          "link": "http://arxiv.org/abs/2108.08810",
          "publishedOn": "2021-08-20T01:53:51.164Z",
          "wordCount": 622,
          "title": "Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1\">Long Quan</a>",
          "description": "Matching local features across images is a fundamental problem in computer\nvision. Targeting towards high accuracy and efficiency, we propose Seeded Graph\nMatching Network, a graph neural network with sparse structure to reduce\nredundant connectivity and learn compact representation. The network consists\nof 1) Seeding Module, which initializes the matching by generating a small set\nof reliable matches as seeds. 2) Seeded Graph Neural Network, which utilizes\nseed matches to pass messages within/across images and predicts assignment\ncosts. Three novel operations are proposed as basic elements for message\npassing: 1) Attentional Pooling, which aggregates keypoint features within the\nimage to seed matches. 2) Seed Filtering, which enhances seed features and\nexchanges messages across images. 3) Attentional Unpooling, which propagates\nseed features back to original keypoints. Experiments show that our method\nreduces computational and memory complexity significantly compared with typical\nattention-based networks while competitive or higher performance is achieved.",
          "link": "http://arxiv.org/abs/2108.08771",
          "publishedOn": "2021-08-20T01:53:51.157Z",
          "wordCount": 606,
          "title": "Learning to Match Features with Seeded Graph Matching Network. (arXiv:2108.08771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>",
          "description": "Attention module does not always help deep models learn causal features that\nare robust in any confounding context, e.g., a foreground object feature is\ninvariant to different backgrounds. This is because the confounders trick the\nattention to capture spurious correlations that benefit the prediction when the\ntraining and testing data are IID (identical & independent distribution); while\nharm the prediction when the data are OOD (out-of-distribution). The sole\nfundamental solution to learn causal attention is by causal intervention, which\nrequires additional annotations of the confounders, e.g., a \"dog\" model is\nlearned within \"grass+dog\" and \"road+dog\" respectively, so the \"grass\" and\n\"road\" contexts will no longer confound the \"dog\" recognition. However, such\nannotation is not only prohibitively expensive, but also inherently\nproblematic, as the confounders are elusive in nature. In this paper, we\npropose a causal attention module (CaaM) that self-annotates the confounders in\nunsupervised fashion. In particular, multiple CaaMs can be stacked and\nintegrated in conventional attention CNN and self-attention Vision Transformer.\nIn OOD settings, deep models with CaaM outperform those without it\nsignificantly; even in IID settings, the attention localization is also\nimproved by CaaM, showing a great potential in applications that require robust\nvisual saliency. Codes are available at \\url{https://github.com/Wangt-CN/CaaM}.",
          "link": "http://arxiv.org/abs/2108.08782",
          "publishedOn": "2021-08-20T01:53:51.150Z",
          "wordCount": 640,
          "title": "Causal Attention for Unbiased Visual Recognition. (arXiv:2108.08782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>",
          "description": "For a given video-based Human-Object Interaction scene, modeling the\nspatio-temporal relationship between humans and objects are the important cue\nto understand the contextual information presented in the video. With the\neffective spatio-temporal relationship modeling, it is possible not only to\nuncover contextual information in each frame but also to directly capture\ninter-time dependencies. It is more critical to capture the position changes of\nhuman and objects over the spatio-temporal dimension when their appearance\nfeatures may not show up significant changes over time. The full use of\nappearance features, the spatial location and the semantic information are also\nthe key to improve the video-based Human-Object Interaction recognition\nperformance. In this paper, Spatio-Temporal Interaction Graph Parsing Networks\n(STIGPN) are constructed, which encode the videos with a graph composed of\nhuman and object nodes. These nodes are connected by two types of relations:\n(i) spatial relations modeling the interactions between human and the\ninteracted objects within each frame. (ii) inter-time relations capturing the\nlong range dependencies between human and the interacted objects across frame.\nWith the graph, STIGPN learn spatio-temporal features directly from the whole\nvideo-based Human-Object Interaction scenes. Multi-modal features and a\nmulti-stream fusion strategy are used to enhance the reasoning capability of\nSTIGPN. Two Human-Object Interaction video datasets, including CAD-120 and\nSomething-Else, are used to evaluate the proposed architectures, and the\nstate-of-the-art performance demonstrates the superiority of STIGPN.",
          "link": "http://arxiv.org/abs/2108.08633",
          "publishedOn": "2021-08-20T01:53:51.120Z",
          "wordCount": 677,
          "title": "Spatio-Temporal Interaction Graph Parsing Networks for Human-Object Interaction Recognition. (arXiv:2108.08633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyunyoung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunhyeok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>",
          "description": "Self-supervised monocular depth estimation has been widely studied, owing to\nits practical importance and recent promising improvements. However, most works\nsuffer from limited supervision of photometric consistency, especially in weak\ntexture regions and at object boundaries. To overcome this weakness, we propose\nnovel ideas to improve self-supervised monocular depth estimation by leveraging\ncross-domain information, especially scene semantics. We focus on incorporating\nimplicit semantic knowledge into geometric representation enhancement and\nsuggest two ideas: a metric learning approach that exploits the\nsemantics-guided local geometry to optimize intermediate depth representations\nand a novel feature fusion module that judiciously utilizes cross-modality\nbetween two heterogeneous feature representations. We comprehensively evaluate\nour methods on the KITTI dataset and demonstrate that our method outperforms\nstate-of-the-art methods. The source code is available at\nhttps://github.com/hyBlue/FSRE-Depth.",
          "link": "http://arxiv.org/abs/2108.08829",
          "publishedOn": "2021-08-20T01:53:51.103Z",
          "wordCount": 568,
          "title": "Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation. (arXiv:2108.08829v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1010.4059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "This article is about the architecture of a lossless wavelet filter bank with\nreprogrammable logic. It is based on second generation of wavelets with a\nreduced of number of operations. A new basic structure for parallel\narchitecture and modules to forward and backward integer discrete wavelet\ntransform is proposed.",
          "link": "http://arxiv.org/abs/1010.4059",
          "publishedOn": "2021-08-20T01:53:51.083Z",
          "wordCount": 553,
          "title": "Multiplierless Modules for Forward and Backward Integer Wavelet Transform. (arXiv:1010.4059v3 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>",
          "description": "We address the challenging task of few-shot segmentation in this work. It is\nessential for few-shot semantic segmentation to fully utilize the support\ninformation. Previous methods typically adapt masked average pooling over the\nsupport feature to extract the support clues as a global vector, usually\ndominated by the salient part and loses some important clues. In this work, we\nargue that every support pixel's information is desired to be transferred to\nall query pixels and propose a Correspondence Matching Network (CMNet) with an\nOptimal Transport Matching module to mine out the correspondence between the\nquery and support images. Besides, it is important to fully utilize both local\nand global information from the annotated support images. To this end, we\npropose a Message Flow module to propagate the message along the inner-flow\nwithin the same image and cross-flow between support and query images, which\ngreatly help enhance the local feature representations. We further address the\nfew-shot segmentation as a multi-task learning problem to alleviate the domain\ngap issue between different datasets. Experiments on PASCAL VOC 2012, MS COCO,\nand FSS-1000 datasets show that our network achieves new state-of-the-art\nfew-shot segmentation performance.",
          "link": "http://arxiv.org/abs/2108.08518",
          "publishedOn": "2021-08-20T01:53:51.064Z",
          "wordCount": 630,
          "title": "Few-shot Segmentation with Optimal Transport Matching and Message Flow. (arXiv:2108.08518v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.00618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie P. Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>",
          "description": "Pushing is a fundamental robotic skill. Existing work has shown how to\nexploit models of pushing to achieve a variety of tasks, including grasping\nunder uncertainty, in-hand manipulation and clearing clutter. Such models,\nhowever, are approximate, which limits their applicability. Learning-based\nmethods can reason directly from raw sensory data with accuracy, and have the\npotential to generalize to a wider diversity of scenarios. However, developing\nand testing such methods requires rich-enough datasets. In this paper we\nintroduce Omnipush, a dataset with high variety of planar pushing behavior. In\nparticular, we provide 250 pushes for each of 250 objects, all recorded with\nRGB-D and a high precision tracking system. The objects are constructed so as\nto systematically explore key factors that affect pushing -- the shape of the\nobject and its mass distribution -- which have not been broadly explored in\nprevious datasets, and allow to study generalization in model learning.\nOmnipush includes a benchmark for meta-learning dynamic models, which requires\nalgorithms that make good predictions and estimate their own uncertainty. We\nalso provide an RGB video prediction benchmark and propose other relevant tasks\nthat can be suited with this dataset.\n\nData and code are available at\n\\url{https://web.mit.edu/mcube/omnipush-dataset/}.",
          "link": "http://arxiv.org/abs/1910.00618",
          "publishedOn": "2021-08-20T01:53:51.058Z",
          "wordCount": 697,
          "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Juhi Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_L/0/1/0/all/0/1\">Lagan Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhiman_H/0/1/0/all/0/1\">Harsh S. Dhiman</a>",
          "description": "In this manuscript, an image analytics based deep learning framework for wind\nturbine blade surface damage detection is proposed. Turbine blade(s) which\ncarry approximately one-third of a turbine weight are susceptible to damage and\ncan cause sudden malfunction of a grid-connected wind energy conversion system.\nThe surface damage detection of wind turbine blade requires a large dataset so\nas to detect a type of damage at an early stage. Turbine blade images are\ncaptured via aerial imagery. Upon inspection, it is found that the image\ndataset was limited and hence image augmentation is applied to improve blade\nimage dataset. The approach is modeled as a multi-class supervised learning\nproblem and deep learning methods like Convolutional neural network (CNN),\nVGG16-RCNN and AlexNet are tested for determining the potential capability of\nturbine blade surface damage.",
          "link": "http://arxiv.org/abs/2108.08636",
          "publishedOn": "2021-08-20T01:53:51.051Z",
          "wordCount": 591,
          "title": "Wind Turbine Blade Surface Damage Detection based on Aerial Imagery and VGG16-RCNN Framework. (arXiv:2108.08636v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Huanhuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijunnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>",
          "description": "Accurate visual re-localization is very critical to many artificial\nintelligence applications, such as augmented reality, virtual reality, robotics\nand autonomous driving. To accomplish this task, we propose an integrated\nvisual re-localization method called RLOCS by combining image retrieval,\nsemantic consistency and geometry verification to achieve accurate estimations.\nThe localization pipeline is designed as a coarse-to-fine paradigm. In the\nretrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ\nDBSCAN followed by spatial verification to obtain a better initial coarse pose.\nWe design a module called observation constraints, which combines geometry\ninformation and semantic consistency for filtering outliers. Comprehensive\nexperiments are conducted on open datasets, including retrieval on R-Oxford5k\nand R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen\nDay-Night and InLoc. By creatively modifying separate modules in the total\npipeline, our method achieves many performance improvements on the challenging\nlocalization benchmarks.",
          "link": "http://arxiv.org/abs/2108.08516",
          "publishedOn": "2021-08-20T01:53:51.044Z",
          "wordCount": 592,
          "title": "Retrieval and Localization with Observation Constraints. (arXiv:2108.08516v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Scene graphs provide valuable information to many downstream tasks. Many\nscene graph generation (SGG) models solely use the limited annotated relation\ntriples for training, leading to their underperformance on low-shot (few and\nzero) scenarios, especially on the rare predicates. To address this problem, we\npropose a novel semantic compositional learning strategy that makes it possible\nto construct additional, realistic relation triples with objects from different\nimages. Specifically, our strategy decomposes a relation triple by identifying\nand removing the unessential component and composes a new relation triple by\nfusing with a semantically or visually similar object from a visual components\ndictionary, whilst ensuring the realisticity of the newly composed triple.\nNotably, our strategy is generic and can be combined with existing SGG models\nto significantly improve their performance. We performed a comprehensive\nevaluation on the benchmark dataset Visual Genome. For three recent SGG models,\nadding our strategy improves their performance by close to 50\\%, and all of\nthem substantially exceed the current state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.08600",
          "publishedOn": "2021-08-20T01:53:51.037Z",
          "wordCount": 599,
          "title": "Semantic Compositional Learning for Low-shot Scene Graph Generation. (arXiv:2108.08600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Autoregressive models and their sequential factorization of the data\nlikelihood have recently demonstrated great potential for image representation\nand synthesis. Nevertheless, they incorporate image context in a linear 1D\norder by attending only to previously synthesized image patches above or to the\nleft. Not only is this unidirectional, sequential bias of attention unnatural\nfor images as it disregards large parts of a scene until synthesis is almost\ncomplete. It also processes the entire image on a single scale, thus ignoring\nmore global contextual information up to the gist of the entire scene. As a\nremedy we incorporate a coarse-to-fine hierarchy of context by combining the\nautoregressive formulation with a multinomial diffusion process: Whereas a\nmultistage diffusion process successively removes information to coarsen an\nimage, we train a (short) Markov chain to invert this process. In each stage,\nthe resulting autoregressive ImageBART model progressively incorporates context\nfrom previous stages in a coarse-to-fine manner. Experiments show greatly\nimproved image modification capabilities over autoregressive models while also\nproviding high-fidelity image generation, both of which are enabled through\nefficient training in a compressed latent space. Specifically, our approach can\ntake unrestricted, user-provided masks into account to perform local image\nediting. Thus, in contrast to pure autoregressive models, it can solve\nfree-form image inpainting and, in the case of conditional models, local,\ntext-guided image modification without requiring mask-specific training.",
          "link": "http://arxiv.org/abs/2108.08827",
          "publishedOn": "2021-08-20T01:53:51.030Z",
          "wordCount": 664,
          "title": "ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis. (arXiv:2108.08827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B. Neill</a>",
          "description": "Machine learning is gaining popularity in a broad range of areas working with\ngeographic data, such as ecology or atmospheric sciences. Here, data often\nexhibit spatial effects, which can be difficult to learn for neural networks.\nIn this study, we propose SXL, a method for embedding information on the\nautoregressive nature of spatial data directly into the learning process using\nauxiliary tasks. We utilize the local Moran's I, a popular measure of local\nspatial autocorrelation, to \"nudge\" the model to learn the direction and\nmagnitude of local spatial effects, complementing the learning of the primary\ntask. We further introduce a novel expansion of Moran's I to multiple\nresolutions, thus capturing spatial interactions over longer and shorter\ndistances simultaneously. The novel multi-resolution Moran's I can be\nconstructed easily and as a multi-dimensional tensor offers seamless\nintegration into existing machine learning frameworks. Throughout a range of\nexperiments using real-world data, we highlight how our method consistently\nimproves the training of neural networks in unsupervised and supervised\nlearning tasks. In generative spatial modeling experiments, we propose a novel\nloss for auxiliary task GANs utilizing task uncertainty weights. Our proposed\nmethod outperforms domain-specific spatial interpolation benchmarks,\nhighlighting its potential for downstream applications. This study bridges\nexpertise from geographic information science and machine learning, showing how\nthis integration of disciplines can help to address domain-specific challenges.\nThe code for our experiments is available on Github:\nhttps://github.com/konstantinklemmer/sxl.",
          "link": "http://arxiv.org/abs/2006.10461",
          "publishedOn": "2021-08-20T01:53:51.003Z",
          "wordCount": 712,
          "title": "Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1\">Banglei Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Ji Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>",
          "description": "We propose three novel solvers for estimating the relative pose of a\nmulti-camera system from affine correspondences (ACs). A new constraint is\nderived interpreting the relationship of ACs and the generalized camera model.\nUsing the constraint, we demonstrate efficient solvers for two types of motions\nassumed. Considering that the cameras undergo planar motion, we propose a\nminimal solution using a single AC and a solver with two ACs to overcome the\ndegenerate case. Also, we propose a minimal solution using two ACs with known\nvertical direction, e.g., from an IMU. Since the proposed methods require\nsignificantly fewer correspondences than state-of-the-art algorithms, they can\nbe efficiently used within RANSAC for outlier removal and initial motion\nestimation. The solvers are tested both on synthetic data and on real-world\nscenes from the KITTI odometry benchmark. It is shown that the accuracy of the\nestimated poses is superior to the state-of-the-art techniques.",
          "link": "http://arxiv.org/abs/2007.10700",
          "publishedOn": "2021-08-20T01:53:50.994Z",
          "wordCount": 624,
          "title": "Minimal Cases for Computing the Generalized Relative Pose using Affine Correspondences. (arXiv:2007.10700v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state-of-the-art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks.",
          "link": "http://arxiv.org/abs/2006.12245",
          "publishedOn": "2021-08-20T01:53:50.988Z",
          "wordCount": 587,
          "title": "Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08505",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_M/0/1/0/all/0/1\">Meng Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xianpei Wang</a>",
          "description": "Perceptual quality assessment of the videos acquired in the wilds is of vital\nimportance for quality assurance of video services. The inaccessibility of\nreference videos with pristine quality and the complexity of authentic\ndistortions pose great challenges for this kind of blind video quality\nassessment (BVQA) task. Although model-based transfer learning is an effective\nand efficient paradigm for the BVQA task, it remains to be a challenge to\nexplore what and how to bridge the domain shifts for better video\nrepresentation. In this work, we propose to transfer knowledge from image\nquality assessment (IQA) databases with authentic distortions and large-scale\naction recognition with rich motion patterns. We rely on both groups of data to\nlearn the feature extractor. We train the proposed model on the target VQA\ndatabases using a mixed list-wise ranking loss function. Extensive experiments\non six databases demonstrate that our method performs very competitively under\nboth individual database and mixed database training settings. We also verify\nthe rationality of each component of the proposed method and explore a simple\nmanner for further improvement.",
          "link": "http://arxiv.org/abs/2108.08505",
          "publishedOn": "2021-08-20T01:53:50.981Z",
          "wordCount": 630,
          "title": "Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. (arXiv:2108.08505v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: \\url{https://ncd-uno.github.io}.",
          "link": "http://arxiv.org/abs/2108.08536",
          "publishedOn": "2021-08-20T01:53:50.975Z",
          "wordCount": 608,
          "title": "A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pochuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichung Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuh_C/0/1/0/all/0/1\">Chiou-Shann Fuh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kao-Lang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wei-Chih Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>",
          "description": "Federated learning (FL) for medical image segmentation becomes more\nchallenging in multi-task settings where clients might have different\ncategories of labels represented in their data. For example, one client might\nhave patient data with \"healthy'' pancreases only while datasets from other\nclients may contain cases with pancreatic tumors. The vanilla federated\naveraging algorithm makes it possible to obtain more generalizable deep\nlearning-based segmentation models representing the training data from multiple\ninstitutions without centralizing datasets. However, it might be sub-optimal\nfor the aforementioned multi-task scenarios. In this paper, we investigate\nheterogeneous optimization methods that show improvements for the automated\nsegmentation of pancreas and pancreatic tumors in abdominal CT images with FL\nsettings.",
          "link": "http://arxiv.org/abs/2108.08537",
          "publishedOn": "2021-08-20T01:53:50.957Z",
          "wordCount": 571,
          "title": "Multi-task Federated Learning for Heterogeneous Pancreas Segmentation. (arXiv:2108.08537v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shivapuja_S/0/1/0/all/0/1\">Sravya Vardhani Shivapuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamkar_M/0/1/0/all/0/1\">Mansi Pradeep Khamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_D/0/1/0/all/0/1\">Divij Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "Datasets for training crowd counting deep networks are typically heavy-tailed\nin count distribution and exhibit discontinuities across the count range. As a\nresult, the de facto statistical measures (MSE, MAE) exhibit large variance and\ntend to be unreliable indicators of performance across the count range. To\naddress these concerns in a holistic manner, we revise processes at various\nstages of the standard crowd counting pipeline. To enable principled and\nbalanced minibatch sampling, we propose a novel smoothed Bayesian sample\nstratification approach. We propose a novel cost function which can be readily\nincorporated into existing crowd counting deep networks to encourage\nstrata-aware optimization. We analyze the performance of representative crowd\ncounting approaches across standard datasets at per strata level and in\naggregate. We analyze the performance of crowd counting approaches across\nstandard datasets and demonstrate that our proposed modifications noticeably\nreduce error standard deviation. Our contributions represent a nuanced,\nstatistically balanced and fine-grained characterization of performance for\ncrowd counting approaches. Code, pretrained models and interactive\nvisualizations can be viewed at our project page https://deepcount.iiit.ac.in/",
          "link": "http://arxiv.org/abs/2108.08784",
          "publishedOn": "2021-08-20T01:53:50.941Z",
          "wordCount": 652,
          "title": "Wisdom of (Binned) Crowds: A Bayesian Stratification Paradigm for Crowd Counting. (arXiv:2108.08784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08305",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lichuan Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1\">Royson Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed S. Abdelfattah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1\">Hongkai Wen</a>",
          "description": "Deep learning-based blind super-resolution (SR) methods have recently\nachieved unprecedented performance in upscaling frames with unknown\ndegradation. These models are able to accurately estimate the unknown\ndownscaling kernel from a given low-resolution (LR) image in order to leverage\nthe kernel during restoration. Although these approaches have largely been\nsuccessful, they are predominantly image-based and therefore do not exploit the\ntemporal properties of the kernels across multiple video frames. In this paper,\nwe investigated the temporal properties of the kernels and highlighted its\nimportance in the task of blind video super-resolution. Specifically, we\nmeasured the kernel temporal consistency of real-world videos and illustrated\nhow the estimated kernels might change per frame in videos of varying\ndynamicity of the scene and its objects. With this new insight, we revisited\nprevious popular video SR approaches, and showed that previous assumptions of\nusing a fixed kernel throughout the restoration process can lead to visual\nartifacts when upscaling real-world videos. In order to counteract this, we\ntailored existing single-image and video SR techniques to leverage kernel\nconsistency during both kernel estimation and video upscaling processes.\nExtensive experiments on synthetic and real-world videos show substantial\nrestoration gains quantitatively and qualitatively, achieving the new\nstate-of-the-art in blind video SR and underlining the potential of exploiting\nkernel temporal consistency.",
          "link": "http://arxiv.org/abs/2108.08305",
          "publishedOn": "2021-08-20T01:53:50.934Z",
          "wordCount": 665,
          "title": "Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL",
          "link": "http://arxiv.org/abs/2108.08728",
          "publishedOn": "2021-08-20T01:53:50.925Z",
          "wordCount": 601,
          "title": "Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Aihua Zheng</a>",
          "description": "Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on\nperson image representation and re-identification task via feature erasing.\nHowever, BDB drops the features \\textbf{randomly} which may lead to sub-optimal\nresults. In this paper, we propose a novel Self-Thresholding attention guided\nAdaptive DropBlock network (STADB) for person re-ID which can\n\\textbf{adaptively} erase the most discriminative regions. Specifically, STADB\nfirst obtains an attention map by channel-wise pooling and returns a drop mask\nby thresholding the attention map. Then, the input features and\nself-thresholding attention guided drop mask are multiplied to generate the\ndropped feature maps. In addition, STADB utilizes the spatial and channel\nattention to learn a better feature map and iteratively trains the feature\ndropping module for person re-ID. Experiments on several benchmark datasets\ndemonstrate that the proposed STADB outperforms many other related methods for\nperson re-ID. The source code of this paper is released at:\n\\textcolor{red}{\\url{https://github.com/wangxiao5791509/STADB_ReID}}.",
          "link": "http://arxiv.org/abs/2007.03584",
          "publishedOn": "2021-08-20T01:53:50.898Z",
          "wordCount": 627,
          "title": "STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification. (arXiv:2007.03584v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiawu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Teng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Despite superior performance on many computer vision tasks, deep convolution\nneural networks are well known to be compressed on devices that have resource\nconstraints. Most existing network pruning methods require laborious human\nefforts and prohibitive computation resources, especially when the constraints\nare changed. This practically limits the application of model compression when\nthe model needs to be deployed on a wide range of devices. Besides, existing\nmethods are still challenged by the missing theoretical guidance. In this paper\nwe propose an information theory-inspired strategy for automatic model\ncompression. The principle behind our method is the information bottleneck\ntheory, i.e., the hidden representation should compress information with each\nother. We thus introduce the normalized Hilbert-Schmidt Independence Criterion\n(nHSIC) on network activations as a stable and generalized indicator of layer\nimportance. When a certain resource constraint is given, we integrate the HSIC\nindicator with the constraint to transform the architecture search problem into\na linear programming problem with quadratic constraints. Such a problem is\neasily solved by a convex optimization method with a few seconds. We also\nprovide a rigorous proof to reveal that optimizing the normalized HSIC\nsimultaneously minimizes the mutual information between different layers.\nWithout any search process, our method achieves better compression tradeoffs\ncomparing to the state-of-the-art compression algorithms. For instance, with\nResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on\nImageNet. Codes are avaliable at\nhttps://github.com/MAC-AutoML/ITPruner/tree/master.",
          "link": "http://arxiv.org/abs/2108.08532",
          "publishedOn": "2021-08-20T01:53:50.891Z",
          "wordCount": 679,
          "title": "An Information Theory-inspired Strategy for Automatic Network Pruning. (arXiv:2108.08532v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">JaeYoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>",
          "description": "3D Shape representation has substantial effects on 3D shape reconstruction.\nPrimitive-based representations approximate a 3D shape mainly by a set of\nsimple implicit primitives, but the low geometrical complexity of the\nprimitives limits the shape resolution. Moreover, setting a sufficient number\nof primitives for an arbitrary shape is challenging. To overcome these issues,\nwe propose a constrained implicit algebraic surface as the primitive with few\nlearnable coefficients and higher geometrical complexities and a deep neural\nnetwork to produce these primitives. Our experiments demonstrate the\nsuperiorities of our method in terms of representation power compared to the\nstate-of-the-art methods in single RGB image 3D shape reconstruction.\nFurthermore, we show that our method can semantically learn segments of 3D\nshapes in an unsupervised manner. The code is publicly available from\nhttps://myavartanoo.github.io/3dias/ .",
          "link": "http://arxiv.org/abs/2108.08653",
          "publishedOn": "2021-08-20T01:53:50.883Z",
          "wordCount": 573,
          "title": "3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces. (arXiv:2108.08653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ardino_P/0/1/0/all/0/1\">Pierfrancesco Ardino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>",
          "description": "This paper introduces Click to Move (C2M), a novel framework for video\ngeneration where the user can control the motion of the synthesized video\nthrough mouse clicks specifying simple object trajectories of the key objects\nin the scene. Our model receives as input an initial frame, its corresponding\nsegmentation map and the sparse motion vectors encoding the input provided by\nthe user. It outputs a plausible video sequence starting from the given frame\nand with a motion that is consistent with user input. Notably, our proposed\ndeep architecture incorporates a Graph Convolution Network (GCN) modelling the\nmovements of all the objects in the scene in a holistic manner and effectively\ncombining the sparse user motion information and image features. Experimental\nresults show that C2M outperforms existing methods on two publicly available\ndatasets, thus demonstrating the effectiveness of our GCN framework at\nmodelling object interactions. The source code is publicly available at\nhttps://github.com/PierfrancescoArdino/C2M.",
          "link": "http://arxiv.org/abs/2108.08815",
          "publishedOn": "2021-08-20T01:53:50.877Z",
          "wordCount": 610,
          "title": "Click to Move: Controlling Video Generation with Sparse Motion. (arXiv:2108.08815v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Danping Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxian Yu</a>",
          "description": "Self-supervised monocular depth estimation has achieved impressive\nperformance on outdoor datasets. Its performance however degrades notably in\nindoor environments because of the lack of textures. Without rich textures, the\nphotometric consistency is too weak to train a good depth network. Inspired by\nthe early works on indoor modeling, we leverage the structural regularities\nexhibited in indoor scenes, to train a better depth network. Specifically, we\nadopt two extra supervisory signals for self-supervised training: 1) the\nManhattan normal constraint and 2) the co-planar constraint. The Manhattan\nnormal constraint enforces the major surfaces (the floor, ceiling, and walls)\nto be aligned with dominant directions. The co-planar constraint states that\nthe 3D points be well fitted by a plane if they are located within the same\nplanar region. To generate the supervisory signals, we adopt two components to\nclassify the major surface normal into dominant directions and detect the\nplanar regions on the fly during training. As the predicted depth becomes more\naccurate after more training epochs, the supervisory signals also improve and\nin turn feedback to obtain a better depth model. Through extensive experiments\non indoor benchmark datasets, the results show that our network outperforms the\nstate-of-the-art methods. The source code is available at\nhttps://github.com/SJTU-ViSYS/StructDepth .",
          "link": "http://arxiv.org/abs/2108.08574",
          "publishedOn": "2021-08-20T01:53:50.869Z",
          "wordCount": 658,
          "title": "StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation. (arXiv:2108.08574v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>",
          "description": "Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation",
          "link": "http://arxiv.org/abs/2108.08432",
          "publishedOn": "2021-08-20T01:53:50.850Z",
          "wordCount": 625,
          "title": "Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision. (arXiv:2108.08432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Lane detection plays a key role in autonomous driving. While car cameras\nalways take streaming videos on the way, current lane detection works mainly\nfocus on individual images (frames) by ignoring dynamics along the video. In\nthis work, we collect a new video instance lane detection (VIL-100) dataset,\nwhich contains 100 videos with in total 10,000 frames, acquired from different\nreal traffic scenarios. All the frames in each video are manually annotated to\na high-quality instance-level lane annotation, and a set of frame-level and\nvideo-level metrics are included for quantitative performance evaluation.\nMoreover, we propose a new baseline model, named multi-level memory aggregation\nnetwork (MMA-Net), for video instance lane detection. In our approach, the\nrepresentation of current frame is enhanced by attentively aggregating both\nlocal and global memory features from other frames. Experiments on the new\ncollected dataset show that the proposed MMA-Net outperforms state-of-the-art\nlane detection methods and video object segmentation methods. We release our\ndataset and code at https://github.com/yujun0-0/MMA-Net.",
          "link": "http://arxiv.org/abs/2108.08482",
          "publishedOn": "2021-08-20T01:53:50.843Z",
          "wordCount": 616,
          "title": "VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection. (arXiv:2108.08482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Soumava Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_T/0/1/0/all/0/1\">Titir Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Soma Biswas</a>",
          "description": "In this work, for the first time, we address the problem of universal\ncross-domain retrieval, where the test data can belong to classes or domains\nwhich are unseen during training. Due to dynamically increasing number of\ncategories and practical constraint of training on every possible domain, which\nrequires large amounts of data, generalizing to both unseen classes and domains\nis important. Towards that goal, we propose SnMpNet (Semantic Neighbourhood and\nMixture Prediction Network), which incorporates two novel losses to account for\nthe unseen classes and domains encountered during testing. Specifically, we\nintroduce a novel Semantic Neighborhood loss to bridge the knowledge gap\nbetween seen and unseen classes and ensure that the latent space embedding of\nthe unseen classes is semantically meaningful with respect to its neighboring\nclasses. We also introduce a mix-up based supervision at image-level as well as\nsemantic-level of the data for training with the Mixture Prediction loss, which\nhelps in efficient retrieval when the query belongs to an unseen domain. These\nlosses are incorporated on the SE-ResNet50 backbone to obtain SnMpNet.\nExtensive experiments on two large-scale datasets, Sketchy Extended and\nDomainNet, and thorough comparisons with state-of-the-art justify the\neffectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2108.08356",
          "publishedOn": "2021-08-20T01:53:50.836Z",
          "wordCount": 638,
          "title": "Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains. (arXiv:2108.08356v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Recently, the generalization behavior of Convolutional Neural Networks (CNN)\nis gradually transparent through explanation techniques with the frequency\ncomponents decomposition. However, the importance of the phase spectrum of the\nimage for a robust vision system is still ignored. In this paper, we notice\nthat the CNN tends to converge at the local optimum which is closely related to\nthe high-frequency components of the training images, while the amplitude\nspectrum is easily disturbed such as noises or common corruptions. In contrast,\nmore empirical studies found that humans rely on more phase components to\nachieve robust recognition. This observation leads to more explanations of the\nCNN's generalization behaviors in both robustness to common perturbations and\nout-of-distribution detection, and motivates a new perspective on data\naugmentation designed by re-combing the phase spectrum of the current image and\nthe amplitude spectrum of the distracter image. That is, the generated samples\nforce the CNN to pay more attention to the structured information from phase\ncomponents and keep robust to the variation of the amplitude. Experiments on\nseveral image datasets indicate that the proposed method achieves\nstate-of-the-art performances on multiple generalizations and calibration\ntasks, including adaptability for common corruptions and surface variations,\nout-of-distribution detection, and adversarial attack.",
          "link": "http://arxiv.org/abs/2108.08487",
          "publishedOn": "2021-08-20T01:53:50.829Z",
          "wordCount": 654,
          "title": "Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1\">Rahul Sankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1\">Amithash M Prabhudev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1\">P A Mahesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1\">K Prakashini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudha Kiran Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The world is going through a challenging phase due to the disastrous effect\ncaused by the COVID-19 pandemic on the healthcare system and the economy. The\nrate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of\nCOVID-19 have put the healthcare systems in disruption across the globe. Due to\nthis, the task of accurately screening COVID-19 cases has become of utmost\npriority. Since the virus infects the respiratory system, Chest X-Ray is an\nimaging modality that is adopted extensively for the initial screening. We have\nperformed a comprehensive study that uses CXR images to identify COVID-19 cases\nand realized the necessity of having a more generalizable model. We utilize\nMobileNetV2 architecture as the feature extractor and integrate it into Capsule\nNetworks to construct a fully automated and lightweight model termed as\nMobileCaps. MobileCaps is trained and evaluated on the publicly available\ndataset with the model ensembling and Bayesian optimization strategies to\nefficiently classify CXR images of patients with COVID-19 from non-COVID-19\npneumonia and healthy cases. The proposed model is further evaluated on two\nadditional RT-PCR confirmed datasets to demonstrate the generalizability. We\nalso introduce MobileCaps-S and leverage it for performing severity assessment\nof CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema\n(RALE) scoring technique. Our classification model achieved an overall recall\nof 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,\nnon-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity\nassessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that\nthe proposed models have fewer trainable parameters than the state-of-the-art\nmodels reported in the literature, we believe our models will go a long way in\naiding healthcare systems in the battle against the pandemic.",
          "link": "http://arxiv.org/abs/2108.08775",
          "publishedOn": "2021-08-20T01:53:50.823Z",
          "wordCount": 814,
          "title": "MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingjun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zikui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>",
          "description": "Vision systems that deploy Deep Neural Networks (DNNs) are known to be\nvulnerable to adversarial examples. Recent research has shown that checking the\nintrinsic consistencies in the input data is a promising way to detect\nadversarial attacks (e.g., by checking the object co-occurrence relationships\nin complex scenes). However, existing approaches are tied to specific models\nand do not offer generalizability. Motivated by the observation that language\ndescriptions of natural scene images have already captured the object\nco-occurrence relationships that can be learned by a language model, we develop\na novel approach to perform context consistency checks using such language\nmodels. The distinguishing aspect of our approach is that it is independent of\nthe deployed object detector and yet offers very high accuracy in terms of\ndetecting adversarial examples in practical scenes with multiple objects.",
          "link": "http://arxiv.org/abs/2108.08421",
          "publishedOn": "2021-08-20T01:53:50.804Z",
          "wordCount": 590,
          "title": "Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>",
          "description": "Self-supervised learning has been successfully applied to pre-train video\nrepresentations, which aims at efficient adaptation from pre-training domain to\ndownstream tasks. Existing approaches merely leverage contrastive loss to learn\ninstance-level discrimination. However, lack of category information will lead\nto hard-positive problem that constrains the generalization ability of this\nkind of methods. We find that the multi-task process of meta learning can\nprovide a solution to this problem. In this paper, we propose a\nMeta-Contrastive Network (MCN), which combines the contrastive learning and\nmeta learning, to enhance the learning ability of existing self-supervised\napproaches. Our method contains two training stages based on model-agnostic\nmeta learning (MAML), each of which consists of a contrastive branch and a meta\nbranch. Extensive evaluations demonstrate the effectiveness of our method. For\ntwo downstream tasks, i.e., video action recognition and video retrieval, MCN\noutperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be\nmore specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%\nand 54.5% for video action recognition, as well as 52.5% and 23.7% for video\nretrieval.",
          "link": "http://arxiv.org/abs/2108.08426",
          "publishedOn": "2021-08-20T01:53:50.796Z",
          "wordCount": 618,
          "title": "Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1111.6276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1\">Vasil Kolev</a>",
          "description": "A simple approach for orthogonal wavelets in compressed sensing (CS)\napplications is presented. We compare efficient algorithm for different\northogonal wavelet measurement matrices in CS for image processing from scanned\nphotographic plates (SPP). Some important characteristics were obtained for\nastronomical image processing of SPP. The best orthogonal wavelet choice for\nmeasurement matrix construction in CS for image compression of images of SPP is\ngiven. The image quality measure for linear and nonlinear image compression\nmethod is defined.",
          "link": "http://arxiv.org/abs/1111.6276",
          "publishedOn": "2021-08-20T01:53:50.781Z",
          "wordCount": 587,
          "title": "Compressed sensing of astronomical images:orthogonal wavelets domains. (arXiv:1111.6276v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>",
          "description": "Recent progress in stochastic motion prediction, i.e., predicting multiple\npossible future human motions given a single past pose sequence, has led to\nproducing truly diverse future motions and even providing control over the\nmotion of some body parts. However, to achieve this, the state-of-the-art\nmethod requires learning several mappings for diversity and a dedicated model\nfor controllable motion prediction. In this paper, we introduce a unified deep\ngenerative network for both diverse and controllable motion prediction. To this\nend, we leverage the intuition that realistic human motions consist of smooth\nsequences of valid poses, and that, given limited data, learning a pose prior\nis much more tractable than a motion one. We therefore design a generator that\npredicts the motion of different body parts sequentially, and introduce a\nnormalizing flow based pose prior, together with a joint angle loss, to achieve\nmotion realism.Our experiments on two standard benchmark datasets, Human3.6M\nand HumanEva-I, demonstrate that our approach outperforms the state-of-the-art\nbaselines in terms of both sample diversity and accuracy. The code is available\nat https://github.com/wei-mao-2019/gsps",
          "link": "http://arxiv.org/abs/2108.08422",
          "publishedOn": "2021-08-20T01:53:50.773Z",
          "wordCount": 614,
          "title": "Generating Smooth Pose Sequences for Diverse Human Motion Prediction. (arXiv:2108.08422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Enxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghavi_E/0/1/0/all/0/1\">Ehsan Taghavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingbing_L/0/1/0/all/0/1\">Liu Bingbing</a>",
          "description": "Panoptic segmentation as an integrated task of both static environmental\nunderstanding and dynamic object identification, has recently begun to receive\nbroad research interest. In this paper, we propose a new computationally\nefficient LiDAR based panoptic segmentation framework, called GP-S3Net.\nGP-S3Net is a proposal-free approach in which no object proposals are needed to\nidentify the objects in contrast to conventional two-stage panoptic systems,\nwhere a detection network is incorporated for capturing instance information.\nOur new design consists of a novel instance-level network to process the\nsemantic results by constructing a graph convolutional network to identify\nobjects (foreground), which later on are fused with the background classes.\nThrough the fine-grained clusters of the foreground objects from the semantic\nsegmentation backbone, over-segmentation priors are generated and subsequently\nprocessed by 3D sparse convolution to embed each cluster. Each cluster is\ntreated as a node in the graph and its corresponding embedding is used as its\nnode feature. Then a GCNN predicts whether edges exist between each cluster\npair. We utilize the instance label to generate ground truth edge labels for\neach constructed graph in order to supervise the learning. Extensive\nexperiments demonstrate that GP-S3Net outperforms the current state-of-the-art\napproaches, by a significant margin across available datasets such as, nuScenes\nand SemanticPOSS, ranking first on the competitive public SemanticKITTI\nleaderboard upon publication.",
          "link": "http://arxiv.org/abs/2108.08401",
          "publishedOn": "2021-08-20T01:53:50.720Z",
          "wordCount": 654,
          "title": "GP-S3Net: Graph-based Panoptic Sparse Semantic Segmentation Network. (arXiv:2108.08401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1\">Kyle Lennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1\">Katharina Fransen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1\">Alexander O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yumeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Although LEGO sets have entertained generations of children and adults, the\nchallenge of designing customized builds matching the complexity of real-world\nor imagined scenes remains too great for the average enthusiast. In order to\nmake this feat possible, we implement a system that generates a LEGO brick\nmodel from 2D images. We design a novel solution to this problem that uses an\noctree-structured autoencoder trained on 3D voxelized models to obtain a\nfeasible latent representation for model reconstruction, and a separate network\ntrained to predict this latent representation from 2D images. LEGO models are\nobtained by algorithmic conversion of the 3D voxelized model to bricks. We\ndemonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An\noctree architecture enables the flexibility to produce multiple resolutions to\nbest fit a user's creative vision or design needs. In order to demonstrate the\nbroad applicability of our system, we generate step-by-step building\ninstructions and animations for LEGO models of objects and human faces.\nFinally, we test these automatically generated LEGO sets by constructing\nphysical builds using real LEGO bricks.",
          "link": "http://arxiv.org/abs/2108.08477",
          "publishedOn": "2021-08-20T01:53:50.713Z",
          "wordCount": 627,
          "title": "Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1\">Guohao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yufeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danwei Wang</a>",
          "description": "Large-scale visual place recognition (VPR) is inherently challenging because\nnot all visual cues in the image are beneficial to the task. In order to\nhighlight the task-relevant visual cues in the feature embedding, the existing\nattention mechanisms are either based on artificial rules or trained in a\nthorough data-driven manner. To fill the gap between the two types, we propose\na novel Semantic Reinforced Attention Learning Network (SRALNet), in which the\ninferred attention can benefit from both semantic priors and data-driven\nfine-tuning. The contribution lies in two-folds. (1) To suppress misleading\nlocal features, an interpretable local weighting scheme is proposed based on\nhierarchical feature distribution. (2) By exploiting the interpretability of\nthe local weighting scheme, a semantic constrained initialization is proposed\nso that the local attention can be reinforced by semantic priors. Experiments\ndemonstrate that our method outperforms state-of-the-art techniques on\ncity-scale VPR benchmark datasets.",
          "link": "http://arxiv.org/abs/2108.08443",
          "publishedOn": "2021-08-20T01:53:50.705Z",
          "wordCount": 591,
          "title": "Semantic Reinforced Attention Learning for Visual Place Recognition. (arXiv:2108.08443v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>",
          "description": "The advent of a panoply of resource limited devices opens up new challenges\nin the design of computer vision algorithms with a clear compromise between\naccuracy and computational requirements. In this paper we present new binary\nimage descriptors that emerge from the application of triplet ranking loss,\nhard negative mining and anchor swapping to traditional features based on pixel\ndifferences and image gradients. These descriptors, BAD (Box Average\nDifference) and HashSIFT, establish new operating points in the\nstate-of-the-art's accuracy vs.\\ resources trade-off curve. In our experiments\nwe evaluate the accuracy, execution time and energy consumption of the proposed\ndescriptors. We show that BAD bears the fastest descriptor implementation in\nthe literature while HashSIFT approaches in accuracy that of the top deep\nlearning-based descriptors, being computationally more efficient. We have made\nthe source code public.",
          "link": "http://arxiv.org/abs/2108.08380",
          "publishedOn": "2021-08-20T01:53:50.699Z",
          "wordCount": 570,
          "title": "Revisiting Binary Local Image Description for Resource Limited Devices. (arXiv:2108.08380v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_J/0/1/0/all/0/1\">Jenny Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stayt_J/0/1/0/all/0/1\">Jason Stayt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bienzle_D/0/1/0/all/0/1\">Dorothee Bienzle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_L/0/1/0/all/0/1\">Lutz Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1\">J&#xf6;rn Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>",
          "description": "Pulmonary hemorrhage (P-Hem) occurs among multiple species and can have\nvarious causes. Cytology of bronchoalveolarlavage fluid (BALF) using a 5-tier\nscoring system of alveolar macrophages based on their hemosiderin content is\nconsidered the most sensitive diagnostic method. We introduce a novel, fully\nannotated multi-species P-Hem dataset which consists of 74 cytology whole slide\nimages (WSIs) with equine, feline and human samples. To create this\nhigh-quality and high-quantity dataset, we developed an annotation pipeline\ncombining human expertise with deep learning and data visualisation techniques.\nWe applied a deep learning-based object detection approach trained on 17\nexpertly annotated equine WSIs, to the remaining 39 equine, 12 human and 7\nfeline WSIs. The resulting annotations were semi-automatically screened for\nerrors on multiple types of specialised annotation maps and finally reviewed by\na trained pathologists. Our dataset contains a total of 297,383\nhemosiderophages classified into five grades. It is one of the largest publicly\navailableWSIs datasets with respect to the number of annotations, the scanned\narea and the number of species covered.",
          "link": "http://arxiv.org/abs/2108.08529",
          "publishedOn": "2021-08-20T01:53:50.679Z",
          "wordCount": 644,
          "title": "Inter-Species Cell Detection: Datasets on pulmonary hemosiderophages in equine, human and feline specimens. (arXiv:2108.08529v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobeil_Y/0/1/0/all/0/1\">Yan Gobeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercier_S/0/1/0/all/0/1\">Samuel Mercier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "Semi-supervised learning (SSL) has proven to be effective at leveraging\nlarge-scale unlabeled data to mitigate the dependency on labeled data in order\nto learn better models for visual recognition and classification tasks.\nHowever, recent SSL methods rely on unlabeled image data at a scale of billions\nto work well. This becomes infeasible for tasks with relatively fewer unlabeled\ndata in terms of runtime, memory and data acquisition. To address this issue,\nwe propose noisy semi-supervised transfer learning, an efficient SSL approach\nthat integrates transfer learning and self-training with noisy student into a\nsingle framework, which is tailored for tasks that can leverage unlabeled image\ndata on a scale of thousands. We evaluate our method on both binary and\nmulti-class classification tasks, where the objective is to identify whether an\nimage displays people practicing sports or the type of sport, as well as to\nidentify the pose from a pool of popular yoga poses. Extensive experiments and\nablation studies demonstrate that by leveraging unlabeled data, our proposed\nframework significantly improves visual classification, especially in\nmulti-class classification settings compared to state-of-the-art methods.\nMoreover, incorporating transfer learning not only improves classification\nperformance, but also requires 6x less compute time and 5x less memory. We also\nshow that our method boosts robustness of visual classification models, even\nwithout specifically optimizing for adversarial robustness.",
          "link": "http://arxiv.org/abs/2108.08362",
          "publishedOn": "2021-08-20T01:53:50.672Z",
          "wordCount": 656,
          "title": "STAR: Noisy Semi-Supervised Transfer Learning for Visual Classification. (arXiv:2108.08362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafee_A/0/1/0/all/0/1\">Alif Ashrafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Mohammed Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>",
          "description": "Automatic License Plate Recognition systems aim to provide an end-to-end\nsolution towards detecting, localizing, and recognizing license plate\ncharacters from vehicles appearing in video frames. However, deploying such\nsystems in the real world requires real-time performance in low-resource\nenvironments. In our paper, we propose a novel two-stage detection pipeline\npaired with Vision API that aims to provide real-time inference speed along\nwith consistently accurate detection and recognition performance. We used a\nhaar-cascade classifier as a filter on top of our backbone MobileNet SSDv2\ndetection model. This reduces inference time by only focusing on high\nconfidence detections and using them for recognition. We also impose a temporal\nframe separation strategy to identify multiple vehicle license plates in the\nsame clip. Furthermore, there are no publicly available Bangla license plate\ndatasets, for which we created an image dataset and a video dataset containing\nlicense plates in the wild. We trained our models on the image dataset and\nachieved an AP(0.5) score of 86% and tested our pipeline on the video dataset\nand observed reasonable detection and recognition performance (82.7% detection\nrate, and 60.8% OCR F1 score) with real-time processing speed (27.2 frames per\nsecond).",
          "link": "http://arxiv.org/abs/2108.08339",
          "publishedOn": "2021-08-20T01:53:50.655Z",
          "wordCount": 647,
          "title": "End-to-End License Plate Recognition Pipeline for Real-time Low Resource Video Based Applications. (arXiv:2108.08339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_A/0/1/0/all/0/1\">Armin Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_C/0/1/0/all/0/1\">Cive Fraser</a>",
          "description": "Recently developed automatic dense image matching algorithms are now being\nimplemented for DSM/DTM production, with their pixel-level surface generation\ncapability offering the prospect of partially alleviating the need for manual\nand semi-automatic stereoscopic measurements. In this paper, five\ncommercial/public software packages for 3D surface generation are evaluated,\nusing 5cm GSD imagery recorded from a UAV. Generated surface models are\nassessed against point clouds generated from mobile LiDAR and manual\nstereoscopic measurements. The software packages considered are APS, MICMAC,\nSURE, Pix4UAV and an SGM implementation from DLR.",
          "link": "http://arxiv.org/abs/2108.08369",
          "publishedOn": "2021-08-20T01:53:50.628Z",
          "wordCount": 542,
          "title": "Quality assessment of image matchers for DSM generation -- a comparative study based on UAV images. (arXiv:2108.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kin-man Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>",
          "description": "This paper considers the problem of generating an HDR image of a scene from\nits LDR images. Recent studies employ deep learning and solve the problem in an\nend-to-end fashion, leading to significant performance improvements. However,\nit is still hard to generate a good quality image from LDR images of a dynamic\nscene captured by a hand-held camera, e.g., occlusion due to the large motion\nof foreground objects, causing ghosting artifacts. The key to success relies on\nhow well we can fuse the input images in their feature space, where we wish to\nremove the factors leading to low-quality image generation while performing the\nfundamental computations for HDR image generation, e.g., selecting the\nbest-exposed image/region. We propose a novel method that can better fuse the\nfeatures based on two ideas. One is multi-step feature fusion; our network\ngradually fuses the features in a stack of blocks having the same structure.\nThe other is the design of the component block that effectively performs two\noperations essential to the problem, i.e., comparing and selecting appropriate\nimages/regions. Experimental results show that the proposed method outperforms\nthe previous state-of-the-art methods on the standard benchmark tests.",
          "link": "http://arxiv.org/abs/2108.08585",
          "publishedOn": "2021-08-20T01:53:50.607Z",
          "wordCount": 630,
          "title": "Progressive and Selective Fusion Network for High Dynamic Range Imaging. (arXiv:2108.08585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jonathan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1\">Bowen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1\">Rahul Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David Liang</a>",
          "description": "In this work, deep learning algorithms are used to classify fundus images in\nterms of diabetic retinopathy severity. Six different combinations of two model\narchitectures, the Dense Convolutional Network-121 and the Residual Neural\nNetwork-50 and three image types, RGB, Green, and High Contrast, were tested to\nfind the highest performing combination. We achieved an average validation loss\nof 0.17 and a max validation accuracy of 85 percent. By testing out multiple\ncombinations, certain combinations of parameters performed better than others,\nthough minimal variance was found overall. Green filtration was shown to\nperform the poorest, while amplified contrast appeared to have a negligible\neffect in comparison to RGB analysis. ResNet50 proved to be less of a robust\nmodel as opposed to DenseNet121.",
          "link": "http://arxiv.org/abs/2108.08473",
          "publishedOn": "2021-08-20T01:53:50.599Z",
          "wordCount": 603,
          "title": "Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_Q/0/1/0/all/0/1\">Quazi Marufur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1\">Peter Corke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>",
          "description": "Semantic segmentation is an important task that helps autonomous vehicles\nunderstand their surroundings and navigate safely. During deployment, even the\nmost mature segmentation models are vulnerable to various external factors that\ncan degrade the segmentation performance with potentially catastrophic\nconsequences for the vehicle and its surroundings. To address this issue, we\npropose a failure detection framework to identify pixel-level\nmisclassification. We do so by exploiting internal features of the segmentation\nmodel and training it simultaneously with a failure detection network. During\ndeployment, the failure detector can flag areas in the image where the\nsegmentation model have failed to segment correctly. We evaluate the proposed\napproach against state-of-the-art methods and achieve 12.30%, 9.46%, and 9.65%\nperformance improvement in the AUPR-Error metric for Cityscapes, BDD100K, and\nMapillary semantic segmentation datasets.",
          "link": "http://arxiv.org/abs/2108.08748",
          "publishedOn": "2021-08-20T01:53:50.580Z",
          "wordCount": 569,
          "title": "FSNet: A Failure Detection Framework for Semantic Segmentation. (arXiv:2108.08748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>",
          "description": "Deep networks often make confident, yet incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models are a candidate metric for\noutlier detection with unlabeled data. Yet, previous studies have shown that\nsuch likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest class of deep generative\nmodels. First, we show that a theoretically-grounded correction readily\nameliorates a key bias with VAE likelihood estimates. The bias correction is\nmodel-free, sample-specific, and accurately computed with the Bernoulli and\ncontinuous Bernoulli visible distributions. Second, we show that a well-known\npreprocessing technique, contrast normalization, extends the effectiveness of\nbias correction to natural image datasets. Third, we show that the variance of\nthe likelihoods computed over an ensemble of VAEs also enables robust outlier\ndetection. We perform a comprehensive evaluation of our remedies with nine\n(grayscale and natural) image datasets, and demonstrate significant advantages,\nin terms of both speed and accuracy, over four other state-of-the-art methods.\nOur lightweight remedies are biologically inspired and may serve to achieve\nefficient outlier detection with many types of deep generative models.",
          "link": "http://arxiv.org/abs/2108.08760",
          "publishedOn": "2021-08-20T01:53:50.573Z",
          "wordCount": 645,
          "title": "Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Existing self-supervised learning methods learn representation by means of\npretext tasks which are either (1) discriminating that explicitly specify which\nfeatures should be separated or (2) aligning that precisely indicate which\nfeatures should be closed together, but ignore the fact how to jointly and\nprincipally define which features to be repelled and which ones to be\nattracted. In this work, we combine the positive aspects of the discriminating\nand aligning methods, and design a hybrid method that addresses the above\nissue. Our method explicitly specifies the repulsion and attraction mechanism\nrespectively by discriminative predictive task and concurrently maximizing\nmutual information between paired views sharing redundant information. We\nqualitatively and quantitatively show that our proposed model learns better\nfeatures that are more effective for the diverse downstream tasks ranging from\nclassification to semantic segmentation. Our experiments on nine established\nbenchmarks show that the proposed model consistently outperforms the existing\nstate-of-the-art results of self-supervised and transfer learning protocol.",
          "link": "http://arxiv.org/abs/2108.08562",
          "publishedOn": "2021-08-20T01:53:50.566Z",
          "wordCount": 602,
          "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "We present a novel framework for mesh reconstruction from unstructured point\nclouds by taking advantage of the learned visibility of the 3D points in the\nvirtual views and traditional graph-cut based mesh generation. Specifically, we\nfirst propose a three-step network that explicitly employs depth completion for\nvisibility prediction. Then the visibility information of multiple views is\naggregated to generate a 3D mesh model by solving an optimization problem\nconsidering visibility in which a novel adaptive visibility weighting in\nsurface determination is also introduced to suppress line of sight with a large\nincident angle. Compared to other learning-based approaches, our pipeline only\nexercises the learning on a 2D binary classification task, \\ie, points visible\nor not in a view, which is much more generalizable and practically more\nefficient and capable to deal with a large number of points. Experiments\ndemonstrate that our method with favorable transferability and robustness, and\nachieve competing performances \\wrt state-of-the-art learning-based approaches\non small complex objects and outperforms on large indoor and outdoor scenes.\nCode is available at https://github.com/GDAOSU/vis2mesh.",
          "link": "http://arxiv.org/abs/2108.08378",
          "publishedOn": "2021-08-20T01:53:50.557Z",
          "wordCount": 626,
          "title": "Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility. (arXiv:2108.08378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08618",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Starmans_M/0/1/0/all/0/1\">Martijn P. A. Starmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voort_S/0/1/0/all/0/1\">Sebastian R. van der Voort</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phil_T/0/1/0/all/0/1\">Thomas Phil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timbergen_M/0/1/0/all/0/1\">Milea J. M. Timbergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1\">Melissa Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmos_G/0/1/0/all/0/1\">Guillaume A. Padmos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kessels_W/0/1/0/all/0/1\">Wouter Kessels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanff_D/0/1/0/all/0/1\">David Hanff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grunhagen_D/0/1/0/all/0/1\">Dirk J. Grunhagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verhoef_C/0/1/0/all/0/1\">Cornelis Verhoef</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sleijfer_S/0/1/0/all/0/1\">Stefan Sleijfer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bent_M/0/1/0/all/0/1\">Martin J. van den Bent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smits_M/0/1/0/all/0/1\">Marion Smits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dwarkasing_R/0/1/0/all/0/1\">Roy S. Dwarkasing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Els_C/0/1/0/all/0/1\">Christopher J. Els</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fiduzi_F/0/1/0/all/0/1\">Federico Fiduzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leenders_G/0/1/0/all/0/1\">Geert J. L. H. van Leenders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blazevic_A/0/1/0/all/0/1\">Anela Blazevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hofland_J/0/1/0/all/0/1\">Johannes Hofland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brabander_T/0/1/0/all/0/1\">Tessa Brabander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gils_R/0/1/0/all/0/1\">Renza A. H. van Gils</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franssen_G/0/1/0/all/0/1\">Gaston J. H. Franssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feelders_R/0/1/0/all/0/1\">Richard A. Feelders</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herder_W/0/1/0/all/0/1\">Wouter W. de Herder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buisman_F/0/1/0/all/0/1\">Florian E. Buisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willemssen_F/0/1/0/all/0/1\">Francois E. J. A. Willemssen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koerkamp_B/0/1/0/all/0/1\">Bas Groot Koerkamp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angus_L/0/1/0/all/0/1\">Lindsay Angus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veldt_A/0/1/0/all/0/1\">Astrid A. M. van der Veldt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajicic_A/0/1/0/all/0/1\">Ana Rajicic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Odink_A/0/1/0/all/0/1\">Arlette E. Odink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deen_M/0/1/0/all/0/1\">Mitchell Deen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+T%2E_J/0/1/0/all/0/1\">Jose M. Castillo T.</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veenland_J/0/1/0/all/0/1\">Jifke Veenland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoots_I/0/1/0/all/0/1\">Ivo Schoots</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Renckens_M/0/1/0/all/0/1\">Michel Renckens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doukas_M/0/1/0/all/0/1\">Michail Doukas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Man_R/0/1/0/all/0/1\">Rob A. de Man</a>, <a href=\"http://arxiv.org/find/eess/1/au:+IJzermans_J/0/1/0/all/0/1\">Jan N. M. IJzermans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miclea_R/0/1/0/all/0/1\">Razvan L. Miclea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeulen_P/0/1/0/all/0/1\">Peter B. Vermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bron_E/0/1/0/all/0/1\">Esther E. Bron</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thomeer_M/0/1/0/all/0/1\">Maarten G. Thomeer</a>, et al. (3 additional authors not shown)",
          "description": "Radiomics uses quantitative medical imaging features to predict clinical\noutcomes. While many radiomics methods have been described in the literature,\nthese are generally designed for a single application. The aim of this study is\nto generalize radiomics across applications by proposing a framework to\nautomatically construct and optimize the radiomics workflow per application. To\nthis end, we formulate radiomics as a modular workflow, consisting of several\ncomponents: image and segmentation preprocessing, feature extraction, feature\nand sample preprocessing, and machine learning. For each component, a\ncollection of common algorithms is included. To optimize the workflow per\napplication, we employ automated machine learning using a random search and\nensembling. We evaluate our method in twelve different clinical applications,\nresulting in the following area under the curves: 1) liposarcoma (0.83); 2)\ndesmoid-type fibromatosis (0.82); 3) primary liver tumors (0.81); 4)\ngastrointestinal stromal tumors (0.77); 5) colorectal liver metastases (0.68);\n6) melanoma metastases (0.51); 7) hepatocellular carcinoma (0.75); 8)\nmesenteric fibrosis (0.81); 9) prostate cancer (0.72); 10) glioma (0.70); 11)\nAlzheimer's disease (0.87); and 12) head and neck cancer (0.84). Concluding,\nour method fully automatically constructs and optimizes the radiomics workflow,\nthereby streamlining the search for radiomics biomarkers in new applications.\nTo facilitate reproducibility and future research, we publicly release six\ndatasets, the software implementation of our framework (open-source), and the\ncode to reproduce this study.",
          "link": "http://arxiv.org/abs/2108.08618",
          "publishedOn": "2021-08-20T01:53:50.548Z",
          "wordCount": 830,
          "title": "Reproducible radiomics through automated machine learning validated on twelve clinical applications. (arXiv:2108.08618v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1\">Helisa Dhamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Controllable scene synthesis consists of generating 3D information that\nsatisfy underlying specifications. Thereby, these specifications should be\nabstract, i.e. allowing easy user interaction, whilst providing enough\ninterface for detailed control. Scene graphs are representations of a scene,\ncomposed of objects (nodes) and inter-object relationships (edges), proven to\nbe particularly suited for this task, as they allow for semantic control on the\ngenerated content. Previous works tackling this task often rely on synthetic\ndata, and retrieve object meshes, which naturally limits the generation\ncapabilities. To circumvent this issue, we instead propose the first work that\ndirectly generates shapes from a scene graph in an end-to-end manner. In\naddition, we show that the same model supports scene modification, using the\nrespective scene graph as interface. Leveraging Graph Convolutional Networks\n(GCN) we train a variational Auto-Encoder on top of the object and edge\ncategories, as well as 3D shapes and scene layouts, allowing latter sampling of\nnew scenes and shapes.",
          "link": "http://arxiv.org/abs/2108.08841",
          "publishedOn": "2021-08-20T01:53:50.522Z",
          "wordCount": 604,
          "title": "Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs. (arXiv:2108.08841v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Colorization has attracted increasing interest in recent years. Classic\nreference-based methods usually rely on external color images for plausible\nresults. A large image database or online search engine is inevitably required\nfor retrieving such exemplars. Recent deep-learning-based methods could\nautomatically colorize images at a low cost. However, unsatisfactory artifacts\nand incoherent colors are always accompanied. In this work, we aim at\nrecovering vivid colors by leveraging the rich and diverse color priors\nencapsulated in a pretrained Generative Adversarial Networks (GAN).\nSpecifically, we first \"retrieve\" matched features (similar to exemplars) via a\nGAN encoder and then incorporate these features into the colorization process\nwith feature modulations. Thanks to the powerful generative color prior and\ndelicate designs, our method could produce vivid colors with a single forward\npass. Moreover, it is highly convenient to obtain diverse results by modifying\nGAN latent codes. Our method also inherits the merit of interpretable controls\nof GANs and could attain controllable and smooth transitions by walking through\nGAN latent space. Extensive experiments and user studies demonstrate that our\nmethod achieves superior performance than previous works.",
          "link": "http://arxiv.org/abs/2108.08826",
          "publishedOn": "2021-08-20T01:53:50.515Z",
          "wordCount": 626,
          "title": "Towards Vivid and Diverse Image Colorization with Generative Color Prior. (arXiv:2108.08826v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jingyang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yipeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fenglong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>",
          "description": "Recently, deep learning-based image enhancement algorithms achieved\nstate-of-the-art (SOTA) performance on several publicly available datasets.\nHowever, most existing methods fail to meet practical requirements either for\nvisual perception or for computation efficiency, especially for high-resolution\nimages. In this paper, we propose a novel real-time image enhancer via\nlearnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well\nconsiders global scenario and local spatial information. Specifically, we\nintroduce a light weight two-head weight predictor that has two outputs. One is\na 1D weight vector used for image-level scenario adaptation, the other is a 3D\nweight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D\nLUTs and fuse them according to the aforementioned weights in an end-to-end\nmanner. The fused LUT is then used to transform the source image into the\ntarget tone in an efficient way. Extensive results show that our model\noutperforms SOTA image enhancement methods on public datasets both subjectively\nand objectively, and that our model only takes about 4ms to process a 4K\nresolution image on one NVIDIA V100 GPU.",
          "link": "http://arxiv.org/abs/2108.08697",
          "publishedOn": "2021-08-20T01:53:50.507Z",
          "wordCount": 627,
          "title": "Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables. (arXiv:2108.08697v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a general learning-based solution for restoring images suffering\nfrom spatially-varying degradations. Prior approaches are typically\ndegradation-specific and employ the same processing across different images and\ndifferent pixels within. However, we hypothesize that such spatially rigid\nprocessing is suboptimal for simultaneously restoring the degraded pixels as\nwell as reconstructing the clean regions of the image. To overcome this\nlimitation, we propose SPAIR, a network design that harnesses\ndistortion-localization information and dynamically adjusts computation to\ndifficult regions in the image. SPAIR comprises of two components, (1) a\nlocalization network that identifies degraded pixels, and (2) a restoration\nnetwork that exploits knowledge from the localization network in filter and\nfeature domain to selectively and adaptively restore degraded pixels. Our key\nidea is to exploit the non-uniformity of heavy degradations in spatial-domain\nand suitably embed this knowledge within distortion-guided modules performing\nsparse normalization, feature extraction and attention. Our architecture is\nagnostic to physical formation model and generalizes across several types of\nspatially-varying degradations. We demonstrate the efficacy of SPAIR\nindividually on four restoration tasks-removal of rain-streaks, raindrops,\nshadows and motion blur. Extensive qualitative and quantitative comparisons\nwith prior art on 11 benchmark datasets demonstrate that our\ndegradation-agnostic network design offers significant performance gains over\nstate-of-the-art degradation-specific architectures. Code available at\nhttps://github.com/human-analysis/spatially-adaptive-image-restoration.",
          "link": "http://arxiv.org/abs/2108.08617",
          "publishedOn": "2021-08-20T01:53:50.499Z",
          "wordCount": 653,
          "title": "Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Superpixel segmentation has recently seen important progress benefiting from\nthe advances in differentiable deep learning. However, the very high-resolution\nsuperpixel segmentation still remains challenging due to the expensive memory\nand computation cost, making the current advanced superpixel networks fail to\nprocess. In this paper, we devise Patch Calibration Networks (PCNet), aiming to\nefficiently and accurately implement high-resolution superpixel segmentation.\nPCNet follows the principle of producing high-resolution output from\nlow-resolution input for saving GPU memory and relieving computation cost. To\nrecall the fine details destroyed by the down-sampling operation, we propose a\nnovel Decoupled Patch Calibration (DPC) branch for collaboratively augment the\nmain superpixel generation branch. In particular, DPC takes a local patch from\nthe high-resolution images and dynamically generates a binary mask to impose\nthe network to focus on region boundaries. By sharing the parameters of DPC and\nmain branches, the fine-detailed knowledge learned from high-resolution patches\nwill be transferred to help calibrate the destroyed information. To the best of\nour knowledge, we make the first attempt to consider the deep-learning-based\nsuperpixel generation for high-resolution cases. To facilitate this research,\nwe build evaluation benchmarks from two public datasets and one new constructed\none, covering a wide range of diversities from fine-grained human parts to\ncityscapes. Extensive experiments demonstrate that our PCNet can not only\nperform favorably against the state-of-the-arts in the quantitative results but\nalso improve the resolution upper bound from 3K to 5K on 1080Ti GPUs.",
          "link": "http://arxiv.org/abs/2108.08607",
          "publishedOn": "2021-08-20T01:53:50.483Z",
          "wordCount": 684,
          "title": "Generating Superpixels for High-resolution Images with Decoupled Patch Calibration. (arXiv:2108.08607v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eilertsen_G/0/1/0/all/0/1\">Gabriel Eilertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajisharif_S/0/1/0/all/0/1\">Saghi Hajisharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanji_P/0/1/0/all/0/1\">Param Hanji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsirikoglou_A/0/1/0/all/0/1\">Apostolia Tsirikoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rafal K. Mantiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unger_J/0/1/0/all/0/1\">Jonas Unger</a>",
          "description": "Single-image high dynamic range (SI-HDR) reconstruction has recently emerged\nas a problem well-suited for deep learning methods. Each successive technique\ndemonstrates an improvement over existing methods by reporting higher image\nquality scores. This paper, however, highlights that such improvements in\nobjective metrics do not necessarily translate to visually superior images. The\nfirst problem is the use of disparate evaluation conditions in terms of data\nand metric parameters, calling for a standardized protocol to make it possible\nto compare between papers. The second problem, which forms the main focus of\nthis paper, is the inherent difficulty in evaluating SI-HDR reconstructions\nsince certain aspects of the reconstruction problem dominate objective\ndifferences, thereby introducing a bias. Here, we reproduce a typical\nevaluation using existing as well as simulated SI-HDR methods to demonstrate\nhow different aspects of the problem affect objective quality metrics.\nSurprisingly, we found that methods that do not even reconstruct HDR\ninformation can compete with state-of-the-art deep learning methods. We show\nhow such results are not representative of the perceived quality and that\nSI-HDR reconstruction needs better evaluation protocols.",
          "link": "http://arxiv.org/abs/2108.08713",
          "publishedOn": "2021-08-20T01:53:50.475Z",
          "wordCount": 640,
          "title": "How to cheat with metrics in single-image HDR reconstruction. (arXiv:2108.08713v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Harsh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Amey Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1\">Shivam Sahni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_U/0/1/0/all/0/1\">Udit Vyas</a>",
          "description": "Image Inpainting is one of the very popular tasks in the field of image\nprocessing with broad applications in computer vision. In various practical\napplications, images are often deteriorated by noise due to the presence of\ncorrupted, lost, or undesirable information. There have been various\nrestoration techniques used in the past with both classical and deep learning\napproaches for handling such issues. Some traditional methods include image\nrestoration by filling gap pixels using the nearby known pixels or using the\nmoving average over the same. The aim of this paper is to perform image\ninpainting using robust deep learning methods that use partial convolution\nlayers.",
          "link": "http://arxiv.org/abs/2108.08791",
          "publishedOn": "2021-08-20T01:53:50.465Z",
          "wordCount": 534,
          "title": "Image Inpainting using Partial Convolution. (arXiv:2108.08791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jewook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Domain generalization aims to enhance the model robustness against domain\nshift without accessing the target domain. Since the available source domains\nfor training are limited, recent approaches focus on generating samples of\nnovel domains. Nevertheless, they either struggle with the optimization problem\nwhen synthesizing abundant domains or cause the distortion of class semantics.\nTo these ends, we propose a novel domain generalization framework where feature\nstatistics are utilized for stylizing original features to ones with novel\ndomain properties. To preserve class information during stylization, we first\ndecompose features into high and low frequency components. Afterward, we\nstylize the low frequency components with the novel domain styles sampled from\nthe manipulated statistics, while preserving the shape cues in high frequency\nones. As the final step, we re-merge both components to synthesize novel domain\nfeatures. To enhance domain robustness, we utilize the stylized features to\nmaintain the model consistency in terms of features as well as outputs. We\nachieve the feature consistency with the proposed domain-aware supervised\ncontrastive loss, which ensures domain invariance while increasing class\ndiscriminability. Experimental results demonstrate the effectiveness of the\nproposed feature stylization and the domain-aware contrastive loss. Through\nquantitative comparisons, we verify the lead of our method upon existing\nstate-of-the-art methods on two benchmarks, PACS and Office-Home.",
          "link": "http://arxiv.org/abs/2108.08596",
          "publishedOn": "2021-08-20T01:53:50.452Z",
          "wordCount": 657,
          "title": "Feature Stylization and Domain-aware Contrastive Learning for Domain Generalization. (arXiv:2108.08596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1\">Nicola Garau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1\">Niccol&#xf2; Bisagno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1\">Piotr Br&#xf3;dka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1\">Nicola Conci</a>",
          "description": "Human Pose Estimation (HPE) aims at retrieving the 3D position of human\njoints from images or videos. We show that current 3D HPE methods suffer a lack\nof viewpoint equivariance, namely they tend to fail or perform poorly when\ndealing with viewpoints unseen at training time. Deep learning methods often\nrely on either scale-invariant, translation-invariant, or rotation-invariant\noperations, such as max-pooling. However, the adoption of such procedures does\nnot necessarily improve viewpoint generalization, rather leading to more\ndata-dependent methods. To tackle this issue, we propose a novel capsule\nautoencoder network with fast Variational Bayes capsule routing, named DECA. By\nmodeling each joint as a capsule entity, combined with the routing algorithm,\nour approach can preserve the joints' hierarchical and geometrical structure in\nthe feature space, independently from the viewpoint. By achieving viewpoint\nequivariance, we drastically reduce the network data dependency at training\ntime, resulting in an improved ability to generalize for unseen viewpoints. In\nthe experimental validation, we outperform other methods on depth images from\nboth seen and unseen viewpoints, both top-view, and front-view. In the RGB\ndomain, the same network gives state-of-the-art results on the challenging\nviewpoint transfer task, also establishing a new framework for top-view HPE.\nThe code can be found at https://github.com/mmlab-cv/DECA.",
          "link": "http://arxiv.org/abs/2108.08557",
          "publishedOn": "2021-08-20T01:53:50.445Z",
          "wordCount": 668,
          "title": "DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08508",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Araki_K/0/1/0/all/0/1\">Kengo Araki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rokutan_Kurata_M/0/1/0/all/0/1\">Mariyo Rokutan-Kurata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terada_K/0/1/0/all/0/1\">Kazuhiro Terada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshizawa_A/0/1/0/all/0/1\">Akihiko Yoshizawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "Pathological diagnosis is used for examining cancer in detail, and its\nautomation is in demand. To automatically segment each cancer area, a\npatch-based approach is usually used since a Whole Slide Image (WSI) is huge.\nHowever, this approach loses the global information needed to distinguish\nbetween classes. In this paper, we utilized the Distance from the Boundary of\ntissue (DfB), which is global information that can be extracted from the\noriginal image. We experimentally applied our method to the three-class\nclassification of cervical cancer, and found that it improved the total\nperformance compared with the conventional method.",
          "link": "http://arxiv.org/abs/2108.08508",
          "publishedOn": "2021-08-20T01:53:50.438Z",
          "wordCount": 554,
          "title": "Patch-Based Cervical Cancer Segmentation using Distance from Boundary of Tissue. (arXiv:2108.08508v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisoni_R/0/1/0/all/0/1\">Raphael Pisoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmi_S/0/1/0/all/0/1\">Sri Lakshmi</a>",
          "description": "CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal\nmodel that jointly learns representations of images and texts. The model is\ntrained on a massive amount of English data and shows impressive performance on\nzero-shot classification tasks. Training the same model on a different language\nis not trivial, since data in other languages might be not enough and the model\nneeds high-quality translations of the texts to guarantee a good performance.\nIn this paper, we present the first CLIP model for the Italian Language\n(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show\nthat CLIP-Italian outperforms the multilingual CLIP model on the tasks of image\nretrieval and zero-shot classification.",
          "link": "http://arxiv.org/abs/2108.08688",
          "publishedOn": "2021-08-20T01:53:50.419Z",
          "wordCount": 552,
          "title": "Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:50.413Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yongmei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozerov_M/0/1/0/all/0/1\">Mikhail G. Mozerov</a>",
          "description": "A signed distance function (SDF) as the 3D shape description is one of the\nmost effective approaches to represent 3D geometry for rendering and\nreconstruction. Our work is inspired by the state-of-the-art method DeepSDF\nthat learns and analyzes the 3D shape as the iso-surface of its shell and this\nmethod has shown promising results especially in the 3D shape reconstruction\nand compression domain. In this paper, we consider the degeneration problem of\nreconstruction coming from the capacity decrease of the DeepSDF model, which\napproximates the SDF with a neural network and a single latent code. We propose\nLocal Geometry Code Learning (LGCL), a model that improves the original DeepSDF\nresults by learning from a local shape geometry of the full 3D shape. We add an\nextra graph neural network to split the single transmittable latent code into a\nset of local latent codes distributed on the 3D shape. Mentioned latent codes\nare used to approximate the SDF in their local regions, which will alleviate\nthe complexity of the approximation compared to the original DeepSDF.\nFurthermore, we introduce a new geometric loss function to facilitate the\ntraining of these local latent codes. Note that other local shape adjusting\nmethods use the 3D voxel representation, which in turn is a problem highly\ndifficult to solve or even is insolvable. In contrast, our architecture is\nbased on graph processing implicitly and performs the learning regression\nprocess directly in the latent code space, thus make the proposed architecture\nmore flexible and also simple for realization. Our experiments on 3D shape\nreconstruction demonstrate that our LGCL method can keep more details with a\nsignificantly smaller size of the SDF decoder and outperforms considerably the\noriginal DeepSDF method under the most important quantitative metrics.",
          "link": "http://arxiv.org/abs/2108.08593",
          "publishedOn": "2021-08-20T01:53:50.406Z",
          "wordCount": 731,
          "title": "3D Shapes Local Geometry Codes Learning with SDF. (arXiv:2108.08593v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katto_J/0/1/0/all/0/1\">Jiro Katto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>",
          "description": "In this paper, we propose a learned video codec with a residual prediction\nnetwork (RP-Net) and a feature-aided loop filter (LF-Net). For the RP-Net, we\nexploit the residual of previous multiple frames to further eliminate the\nredundancy of the current frame residual. For the LF-Net, the features from\nresidual decoding network and the motion compensation network are used to aid\nthe reconstruction quality. To reduce the complexity, a light ResNet structure\nis used as the backbone for both RP-Net and LF-Net. Experimental results\nillustrate that we can save about 10% BD-rate compared with previous learned\nvideo compression frameworks. Moreover, we can achieve faster coding speed due\nto the ResNet backbone. This project is available at\nhttps://github.com/chaoliu18/RPLVC.",
          "link": "http://arxiv.org/abs/2108.08551",
          "publishedOn": "2021-08-20T01:53:50.389Z",
          "wordCount": 570,
          "title": "Learned Video Compression with Residual Prediction and Loop Filter. (arXiv:2108.08551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1\">Yan Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1\">Fabian Manhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (e.g.\nthe 3D rotation and translation) in a cluttered environment from a single RGB\nimage is a challenging problem. While end-to-end methods have recently\ndemonstrated promising results at high efficiency, they are still inferior when\ncompared with elaborate P$n$P/RANSAC-based approaches in terms of pose\naccuracy. In this work, we address this shortcoming by means of a novel\nreasoning about self-occlusion, in order to establish a two-layer\nrepresentation for 3D objects which considerably enhances the accuracy of\nend-to-end 6D pose estimation. Our framework, named SO-Pose, takes a single RGB\nimage as input and respectively generates 2D-3D correspondences as well as\nself-occlusion information harnessing a shared encoder and two separate\ndecoders. Both outputs are then fused to directly regress the 6DoF pose\nparameters. Incorporating cross-layer consistencies that align correspondences,\nself-occlusion and 6D pose, we can further improve accuracy and robustness,\nsurpassing or rivaling all other state-of-the-art approaches on various\nchallenging datasets.",
          "link": "http://arxiv.org/abs/2108.08367",
          "publishedOn": "2021-08-20T01:53:50.382Z",
          "wordCount": 600,
          "title": "SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation. (arXiv:2108.08367v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>",
          "description": "We introduce D3D-HOI: a dataset of monocular videos with ground truth\nannotations of 3D object pose, shape and part motion during human-object\ninteractions. Our dataset consists of several common articulated objects\ncaptured from diverse real-world scenes and camera viewpoints. Each manipulated\nobject (e.g., microwave oven) is represented with a matching 3D parametric\nmodel. This data allows us to evaluate the reconstruction quality of\narticulated objects and establish a benchmark for this challenging task. In\nparticular, we leverage the estimated 3D human pose for more accurate inference\nof the object spatial layout and dynamics. We evaluate this approach on our\ndataset, demonstrating that human-object relations can significantly reduce the\nambiguity of articulated object reconstructions from challenging real-world\nvideos. Code and dataset are available at\nhttps://github.com/facebookresearch/d3d-hoi.",
          "link": "http://arxiv.org/abs/2108.08420",
          "publishedOn": "2021-08-20T01:53:50.375Z",
          "wordCount": 559,
          "title": "D3D-HOI: Dynamic 3D Human-Object Interactions from Videos. (arXiv:2108.08420v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "While single-view 3D reconstruction has made significant progress benefiting\nfrom deep shape representations in recent years, garment reconstruction is\nstill not solved well due to open surfaces, diverse topologies and complex\ngeometric details. In this paper, we propose a novel learnable Anchored\nUnsigned Distance Function (AnchorUDF) representation for 3D garment\nreconstruction from a single image. AnchorUDF represents 3D shapes by\npredicting unsigned distance fields (UDFs) to enable open garment surface\nmodeling at arbitrary resolution. To capture diverse garment topologies,\nAnchorUDF not only computes pixel-aligned local image features of query points,\nbut also leverages a set of anchor points located around the surface to enrich\n3D position features for query points, which provides stronger 3D space context\nfor the distance function. Furthermore, in order to obtain more accurate point\nprojection direction at inference, we explicitly align the spatial gradient\ndirection of AnchorUDF with the ground-truth direction to the surface during\ntraining. Extensive experiments on two public 3D garment datasets, i.e., MGN\nand Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art\nperformance on single-view garment reconstruction.",
          "link": "http://arxiv.org/abs/2108.08478",
          "publishedOn": "2021-08-20T01:53:50.368Z",
          "wordCount": 623,
          "title": "Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction. (arXiv:2108.08478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S. Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.",
          "link": "http://arxiv.org/abs/2108.08467",
          "publishedOn": "2021-08-20T01:53:50.316Z",
          "wordCount": 574,
          "title": "Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haoran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhicheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>",
          "description": "In this paper, we introduce the Multi-Modal Video Reasoning and Analyzing\nCompetition (MMVRAC) workshop in conjunction with ICCV 2021. This competition\nis composed of four different tracks, namely, video question answering,\nskeleton-based action recognition, fisheye video-based action recognition, and\nperson re-identification, which are based on two datasets: SUTD-TrafficQA and\nUAV-Human. We summarize the top-performing methods submitted by the\nparticipants in this competition and show their results achieved in the\ncompetition.",
          "link": "http://arxiv.org/abs/2108.08344",
          "publishedOn": "2021-08-20T01:53:50.258Z",
          "wordCount": 546,
          "title": "The Multi-Modal Video Reasoning and Analyzing Competition. (arXiv:2108.08344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This paper strives to classify and detect the relationship between object\ntubelets appearing within a video as a <subject-predicate-object> triplet.\nWhere existing works treat object proposals or tubelets as single entities and\nmodel their relations a posteriori, we propose to classify and detect\npredicates for pairs of object tubelets a priori. We also propose Social\nFabric: an encoding that represents a pair of object tubelets as a composition\nof interaction primitives. These primitives are learned over all relations,\nresulting in a compact representation able to localize and classify relations\nfrom the pool of co-occurring object tubelets across all timespans in a video.\nThe encoding enables our two-stage network. In the first stage, we train Social\nFabric to suggest proposals that are likely interacting. We use the Social\nFabric in the second stage to simultaneously fine-tune and predict predicate\nlabels for the tubelets. Experiments demonstrate the benefit of early video\nrelation modeling, our encoding and the two-stage architecture, leading to a\nnew state-of-the-art on two benchmarks. We also show how the encoding enables\nquery-by-primitive-example to search for spatio-temporal video relations. Code:\nhttps://github.com/shanshuo/Social-Fabric.",
          "link": "http://arxiv.org/abs/2108.08363",
          "publishedOn": "2021-08-20T01:53:50.207Z",
          "wordCount": 624,
          "title": "Social Fabric: Tubelet Compositions for Video Relation Detection. (arXiv:2108.08363v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiazhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>",
          "description": "Generative Adversarial Networks (GANs) have achieved huge success in\ngenerating high-fidelity images, however, they suffer from low efficiency due\nto tremendous computational cost and bulky memory usage. Recent efforts on\ncompression GANs show noticeable progress in obtaining smaller generators by\nsacrificing image quality or involving a time-consuming searching process. In\nthis work, we aim to address these issues by introducing a teacher network that\nprovides a search space in which efficient network architectures can be found,\nin addition to performing knowledge distillation. First, we revisit the search\nspace of generative models, introducing an inception-based residual block into\ngenerators. Second, to achieve target computation cost, we propose a one-step\npruning algorithm that searches a student architecture from the teacher model\nand substantially reduces searching cost. It requires no l1 sparsity\nregularization and its associated hyper-parameters, simplifying the training\nprocedure. Finally, we propose to distill knowledge through maximizing feature\nsimilarity between teacher and student via an index named Global Kernel\nAlignment (GKA). Our compressed networks achieve similar or even better image\nfidelity (FID, mIoU) than the original models with much-reduced computational\ncost, e.g., MACs. Code will be released at\nhttps://github.com/snap-research/CAT.",
          "link": "http://arxiv.org/abs/2103.03467",
          "publishedOn": "2021-08-19T01:35:03.930Z",
          "wordCount": 673,
          "title": "Teachers Do More Than Teach: Compressing Image-to-Image Models. (arXiv:2103.03467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:03.923Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "3D ultrasound (US) is widely used for its rich diagnostic information.\nHowever, it is criticized for its limited field of view. 3D freehand US\nreconstruction is promising in addressing the problem by providing broad range\nand freeform scan. The existing deep learning based methods only focus on the\nbasic cases of skill sequences, and the model relies on the training data\nheavily. The sequences in real clinical practice are a mix of diverse skills\nand have complex scanning paths. Besides, deep models should adapt themselves\nto the testing cases with prior knowledge for better robustness, rather than\nonly fit to the training cases. In this paper, we propose a novel approach to\nsensorless freehand 3D US reconstruction considering the complex skill\nsequences. Our contribution is three-fold. First, we advance a novel online\nlearning framework by designing a differentiable reconstruction algorithm. It\nrealizes an end-to-end optimization from section sequences to the reconstructed\nvolume. Second, a self-supervised learning method is developed to explore the\ncontext information that reconstructed by the testing data itself, promoting\nthe perception of the model. Third, inspired by the effectiveness of shape\nprior, we also introduce adversarial training to strengthen the learning of\nanatomical shape prior in the reconstructed volume. By mining the context and\nstructural cues of the testing data, our online learning methods can drive the\nmodel to handle complex skill sequences. Experimental results on developmental\ndysplasia of the hip US and fetal US datasets show that, our proposed method\ncan outperform the start-of-the-art methods regarding the shift errors and path\nsimilarities.",
          "link": "http://arxiv.org/abs/2108.00274",
          "publishedOn": "2021-08-19T01:35:03.916Z",
          "wordCount": 737,
          "title": "Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>",
          "description": "We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.",
          "link": "http://arxiv.org/abs/2105.04668",
          "publishedOn": "2021-08-19T01:35:03.909Z",
          "wordCount": 623,
          "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Conditional image synthesis aims to create an image according to some\nmulti-modal guidance in the forms of textual descriptions, reference images,\nand image blocks to preserve, as well as their combinations. In this paper,\ninstead of investigating these control signals separately, we propose a new\ntwo-stage architecture, UFC-BERT, to unify any number of multi-modal controls.\nIn UFC-BERT, both the diverse control signals and the synthesized image are\nuniformly represented as a sequence of discrete tokens to be processed by\nTransformer. Different from existing two-stage autoregressive approaches such\nas DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the\nsecond stage to enhance the holistic consistency of the synthesized image, to\nsupport preserving specified image blocks, and to improve the synthesis speed.\nFurther, we design a progressive algorithm that iteratively improves the\nnon-autoregressively generated image, with the help of two estimators developed\nfor evaluating the compliance with the controls and evaluating the fidelity of\nthe synthesized image, respectively. Extensive experiments on a newly collected\nlarge-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal\nCelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply\nwith flexible multi-modal controls.",
          "link": "http://arxiv.org/abs/2105.14211",
          "publishedOn": "2021-08-19T01:35:03.900Z",
          "wordCount": 666,
          "title": "M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis. (arXiv:2105.14211v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.",
          "link": "http://arxiv.org/abs/2108.05067",
          "publishedOn": "2021-08-19T01:35:03.893Z",
          "wordCount": 798,
          "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagadec_B/0/1/0/all/0/1\">Benoit Lagadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances has not been fully explored in previous contrastive\nmethods, especially for instance-level contrastive loss. To address this issue,\nwe propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\nCode is made available at https://github.com/chenhao2345/ICE.",
          "link": "http://arxiv.org/abs/2103.16364",
          "publishedOn": "2021-08-19T01:35:03.871Z",
          "wordCount": 630,
          "title": "ICE: Inter-instance Contrastive Encoding for Unsupervised Person Re-identification. (arXiv:2103.16364v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>",
          "description": "COVID-19 has led to hundreds of millions of cases and millions of deaths\nworldwide since its onset. The fight against this pandemic is on-going on\nmultiple fronts. While vaccinations are picking up speed, there are still\nbillions of unvaccinated people. In this fight against the virus, diagnosis of\nthe disease and isolation of the patients to prevent any spread play a huge\nrole. Machine Learning approaches have assisted in the diagnosis of COVID-19\ncases by analyzing chest X-rays and CT-scan images of patients. To push\nalgorithm development and research in this direction of radiological diagnosis,\na challenge to classify CT-scan series was organized in conjunction with ICCV,\n2021. In this research we present a simple and shallow Convolutional Neural\nNetwork based approach, TeliNet, to classify these CT-scan images of COVID-19\npatients presented as part of this competition. Our results outperform the F1\n`macro' score of the competition benchmark and VGGNet approaches. Our proposed\nsolution is also more lightweight in comparison to the other methods.",
          "link": "http://arxiv.org/abs/2107.04930",
          "publishedOn": "2021-08-19T01:35:03.843Z",
          "wordCount": 676,
          "title": "TeliNet: Classifying CT scan images for COVID-19 diagnosis. (arXiv:2107.04930v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1\">Sekitoshi Kanai</a>",
          "description": "Generative adversarial networks built from deep convolutional neural networks\n(GANs) lack the ability to exactly replicate the high-frequency components of\nnatural images. To alleviate this issue, we introduce two novel training\ntechniques called frequency dropping (F-Drop) and frequency matching (F-Match).\nThe key idea of F-Drop is to filter out unnecessary high-frequency components\nfrom the input images of the discriminators. This simple modification prevents\nthe discriminators from being confused by perturbations of the high-frequency\ncomponents. In addition, F-Drop makes the GANs focus on fitting in the\nlow-frequency domain, in which there are the dominant components of natural\nimages. F-Match minimizes the difference between real and fake images in the\nfrequency domain for generating more realistic images. F-Match is implemented\nas a regularization term in the objective functions of the generators; it\npenalizes the batch mean error in the frequency domain. F-Match helps the\ngenerators to fit in the high-frequency domain filtered out by F-Drop to the\nreal image. We experimentally demonstrate that the combination of F-Drop and\nF-Match improves the generative performance of GANs in both the frequency and\nspatial domain on multiple image benchmarks.",
          "link": "http://arxiv.org/abs/2106.02343",
          "publishedOn": "2021-08-19T01:35:03.835Z",
          "wordCount": 666,
          "title": "F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07977",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mengu_D/0/1/0/all/0/1\">Deniz Mengu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Veli_M/0/1/0/all/0/1\">Muhammed Veli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rivenson_Y/0/1/0/all/0/1\">Yair Rivenson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>",
          "description": "Diffractive optical networks unify wave optics and deep learning to\nall-optically compute a given machine learning or computational imaging task as\nthe light propagates from the input to the output plane. Here, we report the\ndesign of diffractive optical networks for the classification and\nreconstruction of spatially overlapping, phase-encoded objects. When two\ndifferent phase-only objects spatially overlap, the individual object functions\nare perturbed since their phase patterns are summed up. The retrieval of the\nunderlying phase images from solely the overlapping phase distribution presents\na challenging problem, the solution of which is generally not unique. We show\nthat through a task-specific training process, passive diffractive networks\ncomposed of successive transmissive layers can all-optically and simultaneously\nclassify two different randomly-selected, spatially overlapping phase images at\nthe input. After trained with ~550 million unique combinations of phase-encoded\nhandwritten digits from the MNIST dataset, our blind testing results reveal\nthat the diffractive network achieves an accuracy of >85.8% for all-optical\nclassification of two overlapping phase images of new handwritten digits. In\naddition to all-optical classification of overlapping phase objects, we also\ndemonstrate the reconstruction of these phase images based on a shallow\nelectronic neural network that uses the highly compressed output of the\ndiffractive network as its input (with e.g., ~20-65 times less number of\npixels) to rapidly reconstruct both of the phase images, despite their spatial\noverlap and related phase ambiguity. The presented phase image classification\nand reconstruction framework might find applications in e.g., computational\nimaging, microscopy and quantitative phase imaging fields.",
          "link": "http://arxiv.org/abs/2108.07977",
          "publishedOn": "2021-08-19T01:35:03.816Z",
          "wordCount": 709,
          "title": "Classification and reconstruction of spatially overlapping phase images using diffractive optical networks. (arXiv:2108.07977v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "End-to-end approaches to autonomous driving commonly rely on expert\ndemonstrations. Although humans are good drivers, they are not good coaches for\nend-to-end algorithms that demand dense on-policy supervision. On the contrary,\nautomated experts that leverage privileged information can efficiently generate\nlarge scale on-policy and off-policy demonstrations. However, existing\nautomated experts for urban driving make heavy use of hand-crafted rules and\nperform suboptimally even on driving simulators, where ground-truth information\nis available. To address these issues, we train a reinforcement learning expert\nthat maps bird's-eye view images to continuous low-level actions. While setting\na new performance upper-bound on CARLA, our expert is also a better coach that\nprovides informative supervision signals for imitation learning agents to learn\nfrom. Supervised by our reinforcement learning coach, a baseline end-to-end\nagent with monocular camera-input achieves expert-level performance. Our\nend-to-end agent achieves a 78% success rate while generalizing to a new town\nand new weather on the NoCrash-dense benchmark and state-of-the-art performance\non the more challenging CARLA LeaderBoard.",
          "link": "http://arxiv.org/abs/2108.08265",
          "publishedOn": "2021-08-19T01:35:03.800Z",
          "wordCount": 610,
          "title": "End-to-End Urban Driving by Imitating a Reinforcement Learning Coach. (arXiv:2108.08265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12310",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Miao_Y/0/1/0/all/0/1\">Yu-Chun Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xi-Le Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jian-Li Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu-Bang Zheng</a>",
          "description": "Image denoising is often empowered by accurate prior information. In recent\nyears, data-driven neural network priors have shown promising performance for\nRGB natural image denoising. Compared to classic handcrafted priors (e.g.,\nsparsity and total variation), the \"deep priors\" are learned using a large\nnumber of training samples -- which can accurately model the complex image\ngenerating process. However, data-driven priors are hard to acquire for\nhyperspectral images (HSIs) due to the lack of training data. A remedy is to\nuse the so-called unsupervised deep image prior (DIP). Under the unsupervised\nDIP framework, it is hypothesized and empirically demonstrated that proper\nneural network structures are reasonable priors of certain types of images, and\nthe network weights can be learned without training data. Nonetheless, the most\neffective unsupervised DIP structures were proposed for natural images instead\nof HSIs. The performance of unsupervised DIP-based HSI denoising is limited by\na couple of serious challenges, namely, network structure design and network\ncomplexity. This work puts forth an unsupervised DIP framework that is based on\nthe classic spatio-spectral decomposition of HSIs. Utilizing the so-called\nlinear mixture model of HSIs, two types of unsupervised DIPs, i.e., U-Net-like\nnetwork and fully-connected networks, are employed to model the abundance maps\nand endmembers contained in the HSIs, respectively. This way, empirically\nvalidated unsupervised DIP structures for natural images can be easily\nincorporated for HSI denoising. Besides, the decomposition also substantially\nreduces network complexity. An efficient alternating optimization algorithm is\nproposed to handle the formulated denoising problem. Semi-real and real data\nexperiments are employed to showcase the effectiveness of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2102.12310",
          "publishedOn": "2021-08-19T01:35:03.793Z",
          "wordCount": 740,
          "title": "Hyperspectral Denoising Using Unsupervised Disentangled Spatio-Spectral Deep Priors. (arXiv:2102.12310v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Runyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>",
          "description": "Spatio-temporal action detection is an important and challenging problem in\nvideo understanding. The existing action detection benchmarks are limited in\naspects of small numbers of instances in a trimmed video or low-level atomic\nactions. This paper aims to present a new multi-person dataset of\nspatio-temporal localized sports actions, coined as MultiSports. We first\nanalyze the important ingredients of constructing a realistic and challenging\ndataset for spatio-temporal action detection by proposing three criteria: (1)\nmulti-person scenes and motion dependent identification, (2) with well-defined\nboundaries, (3) relatively fine-grained classes of high complexity. Based on\nthese guide-lines, we build the dataset of MultiSports v1.0 by selecting 4\nsports classes, collecting 3200 video clips, and annotating 37701 action\ninstances with 902k bounding boxes. Our datasets are characterized with\nimportant properties of high diversity, dense annotation, and high quality. Our\nMulti-Sports, with its realistic setting and detailed annotations, exposes the\nintrinsic challenges of spatio-temporal action detection. To benchmark this, we\nadapt several baseline methods to our dataset and give an in-depth analysis on\nthe action detection results in our dataset. We hope our MultiSports can serve\nas a standard benchmark for spatio-temporal action detection in the future. Our\ndataset website is at https://deeperaction.github.io/multisports/.",
          "link": "http://arxiv.org/abs/2105.07404",
          "publishedOn": "2021-08-19T01:35:03.774Z",
          "wordCount": 676,
          "title": "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions. (arXiv:2105.07404v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>",
          "description": "Video activity localisation has recently attained increasing attention due to\nits practical values in automatically localising the most salient visual\nsegments corresponding to their language descriptions (sentences) from\nuntrimmed and unstructured videos. For supervised model training, a temporal\nannotation of both the start and end time index of each video segment for a\nsentence (a video moment) must be given. This is not only very expensive but\nalso sensitive to ambiguity and subjective annotation bias, a much harder task\nthan image labelling. In this work, we develop a more accurate\nweakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\nin video moment proposal generation and matching when only a paragraph\ndescription of activities without per-sentence temporal annotation is\navailable. Specifically, we explore two cross-sentence relational constraints:\n(1) Temporal ordering and (2) semantic consistency among sentences in a\nparagraph description of video activities. Existing weakly-supervised\ntechniques only consider within-sentence video segment correlations in training\nwithout considering cross-sentence paragraph context. This can mislead due to\nambiguous expressions of individual sentences with visually indiscriminate\nvideo moment proposals in isolation. Experiments on two publicly available\nactivity localisation datasets show the advantages of our approach over the\nstate-of-the-art weakly supervised methods, especially so when the video\nactivity descriptions become more complex.",
          "link": "http://arxiv.org/abs/2107.11443",
          "publishedOn": "2021-08-19T01:35:03.761Z",
          "wordCount": 679,
          "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation. (arXiv:2107.11443v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Schyler C. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhuangkun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1\">Antonios Tsourdos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weisi Guo</a>",
          "description": "Increased drone proliferation in civilian and professional settings has\ncreated new threat vectors for airports and national infrastructures. The\neconomic damage for a single major airport from drone incursions is estimated\nto be millions per day. Due to the lack of diverse drone training data,\naccurate training of deep learning detection algorithms under scarce data is an\nopen challenge. Existing methods largely rely on collecting diverse and\ncomprehensive experimental drone footage data, artificially induced data\naugmentation, transfer and meta-learning, as well as physics-informed learning.\nHowever, these methods cannot guarantee capturing diverse drone designs and\nfully understanding the deep feature space of drones. Here, we show how\nunderstanding the general distribution of the drone data via a Generative\nAdversarial Network (GAN) and explaining the missing features using Topological\nData Analysis (TDA) - can allow us to acquire missing data to achieve rapid and\nmore accurate learning. We demonstrate our results on a drone image dataset,\nwhich contains both real drone images as well as simulated images from\ncomputer-aided design. When compared to random data collection (usual practice\n- discriminator accuracy of 94.67\\% after 200 epochs), our proposed GAN-TDA\ninformed data collection method offers a significant 4\\% improvement (99.42\\%\nafter 200 epochs). We believe that this approach of exploiting general data\ndistribution knowledge form neural networks can be applied to a wide range of\nscarce data open challenges.",
          "link": "http://arxiv.org/abs/2108.08244",
          "publishedOn": "2021-08-19T01:35:03.692Z",
          "wordCount": 679,
          "title": "Scarce Data Driven Deep Learning of Drones via Generalized Data Distribution Space. (arXiv:2108.08244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Splietker_M/0/1/0/all/0/1\">Malte Splietker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "Dense real-time tracking and mapping from RGB-D images is an important tool\nfor many robotic applications, such as navigation or grasping. The recently\npresented Directional Truncated Signed Distance Function (DTSDF) is an\naugmentation of the regular TSDF and shows potential for more coherent maps and\nimproved tracking performance. In this work, we present methods for rendering\ndepth- and color maps from the DTSDF, making it a true drop-in replacement for\nthe regular TSDF in established trackers. We evaluate and show, that our method\nincreases re-usability of mapped scenes. Furthermore, we add color integration\nwhich notably improves color-correctness at adjacent surfaces.",
          "link": "http://arxiv.org/abs/2108.08115",
          "publishedOn": "2021-08-19T01:35:03.685Z",
          "wordCount": 556,
          "title": "Rendering and Tracking the Directional TSDF: Modeling Surface Orientation for Coherent Maps. (arXiv:2108.08115v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lindenberger_P/0/1/0/all/0/1\">Philipp Lindenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarlin_P/0/1/0/all/0/1\">Paul-Edouard Sarlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "Finding local features that are repeatable across multiple views is a\ncornerstone of sparse 3D reconstruction. The classical image matching paradigm\ndetects keypoints per-image once and for all, which can yield poorly-localized\nfeatures and propagate large errors to the final geometry. In this paper, we\nrefine two key steps of structure-from-motion by a direct alignment of\nlow-level image information from multiple views: we first adjust the initial\nkeypoint locations prior to any geometric estimation, and subsequently refine\npoints and camera poses as a post-processing. This refinement is robust to\nlarge detection noise and appearance changes, as it optimizes a featuremetric\nerror based on dense features predicted by a neural network. This significantly\nimproves the accuracy of camera poses and scene geometry for a wide range of\nkeypoint detectors, challenging viewing conditions, and off-the-shelf deep\nfeatures. Our system easily scales to large image collections, enabling\npixel-perfect crowd-sourced localization at scale. Our code is publicly\navailable at https://github.com/cvg/pixel-perfect-sfm as an add-on to the\npopular SfM software COLMAP.",
          "link": "http://arxiv.org/abs/2108.08291",
          "publishedOn": "2021-08-19T01:35:03.613Z",
          "wordCount": 605,
          "title": "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement. (arXiv:2108.08291v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.06592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.",
          "link": "http://arxiv.org/abs/1907.06592",
          "publishedOn": "2021-08-19T01:35:03.597Z",
          "wordCount": 700,
          "title": "Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the evaluation of sensing technologies to study plants\nunder field conditions. The TERRA-REF program deployed a suite of\nhigh-resolution, cutting edge technology sensors on a gantry system with the\naim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution\nmultiple times per week. The system contains co-located sensors including a\nstereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D\nstructure, and two hyperspectral cameras covering wavelengths of 300-2500nm.\nThis sensor data is provided alongside over sixty types of traditional plant\nphenotype measurements that can be used to train new machine learning models.\nAssociated weather and environmental measurements, information about agronomic\nmanagement and experimental design, and the genomic sequences of hundreds of\nplant varieties have been collected and are available alongside the sensor and\nplant phenotype data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThe focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-08-19T01:35:03.579Z",
          "wordCount": 733,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the challenges of real-world scenarios in which camera feeds capture\nmultiple people, who in most cases pass in front of the camera only once. We\naddress privacy concerns by using only motion information of walking\nindividuals, with no identifiable appearance-based information. As such, we\npropose a novel weakly supervised learning framework, WildGait, which consists\nof training a Spatio-Temporal Graph Convolutional Network on a large number of\nautomatically annotated skeleton sequences obtained from raw, real-world,\nsurveillance streams to learn useful gait signatures. We collected the training\ndata and compiled the largest dataset of walking skeletons called Uncooperative\nWild Gait, containing over 38k tracklets of anonymized walking 2D skeletons. We\nrelease the dataset for public use. Our results show that, with fine-tuning, we\nsurpass the current state-of-the-art pose-based gait recognition solutions. Our\nproposed method is reliable in training gait recognition methods in\nunconstrained environments, especially in settings with scarce amounts of\nannotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-08-19T01:35:03.572Z",
          "wordCount": 712,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Rui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihan Zhou</a>",
          "description": "Human trajectory prediction has received increased attention lately due to\nits importance in applications such as autonomous vehicles and indoor robots.\nHowever, most existing methods make predictions based on human-labeled\ntrajectories and ignore the errors and noises in detection and tracking. In\nthis paper, we study the problem of human trajectory forecasting in raw videos,\nand show that the prediction accuracy can be severely affected by various types\nof tracking errors. Accordingly, we propose a simple yet effective strategy to\ncorrect the tracking failures by enforcing prediction consistency over time.\nThe proposed \"re-tracking\" algorithm can be applied to any existing tracking\nand prediction pipelines. Experiments on public benchmark datasets demonstrate\nthat the proposed method can improve both tracking and prediction performance\nin challenging real-world scenarios. The code and data are available at\nhttps://git.io/retracking-prediction.",
          "link": "http://arxiv.org/abs/2108.08259",
          "publishedOn": "2021-08-19T01:35:03.526Z",
          "wordCount": 588,
          "title": "Towards Robust Human Trajectory Prediction in Raw Videos. (arXiv:2108.08259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Te-Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1\">Daniel Prusinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1\">Luis Stevens</a>",
          "description": "Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.",
          "link": "http://arxiv.org/abs/2008.01380",
          "publishedOn": "2021-08-19T01:35:03.498Z",
          "wordCount": 613,
          "title": "Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>",
          "description": "Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.",
          "link": "http://arxiv.org/abs/2101.02672",
          "publishedOn": "2021-08-19T01:35:03.472Z",
          "wordCount": 694,
          "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhongxi Zheng</a>",
          "description": "Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make some corrections corresponding\nto potentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present\none-step abductive multi-target learning (OSAMTL) that imposes a one-step\nlogical reasoning upon machine learning via a multi-target learning procedure\nto constrain the predictions of the learning model to be subject to our prior\nknowledge about the true target. For the problem 2), we propose a logical\nassessment formula (LAF) that evaluates the logical rationality of the outputs\nof an approach by estimating the consistencies between the predictions of the\nlearning model and the logical facts narrated from the results of the one-step\nlogical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori\n(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable\nthe machine learning model achieving logically more rational predictions, which\nis beyond various state-of-the-art approaches in handling complex noisy labels.",
          "link": "http://arxiv.org/abs/2011.14956",
          "publishedOn": "2021-08-19T01:35:03.464Z",
          "wordCount": 767,
          "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:35:03.456Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Song Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengsheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenkui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>",
          "description": "Video-Text Retrieval has been a hot research topic with the growth of\nmultimedia data on the internet. Transformer for video-text learning has\nattracted increasing attention due to its promising performance. However,\nexisting cross-modal transformer approaches typically suffer from two major\nlimitations: 1) Exploitation of the transformer architecture where different\nlayers have different feature characteristics is limited; 2) End-to-end\ntraining mechanism limits negative sample interactions in a mini-batch. In this\npaper, we propose a novel approach named Hierarchical Transformer (HiT) for\nvideo-text retrieval. HiT performs Hierarchical Cross-modal Contrastive\nMatching in both feature-level and semantic-level, achieving multi-view and\ncomprehensive retrieval results. Moreover, inspired by MoCo, we propose\nMomentum Cross-modal Contrast for cross-modal learning to enable large-scale\nnegative sample interactions on-the-fly, which contributes to the generation of\nmore precise and discriminative representations. Experimental results on the\nthree major Video-Text Retrieval benchmark datasets demonstrate the advantages\nof our method.",
          "link": "http://arxiv.org/abs/2103.15049",
          "publishedOn": "2021-08-19T01:35:03.450Z",
          "wordCount": 620,
          "title": "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval. (arXiv:2103.15049v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>",
          "description": "This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.",
          "link": "http://arxiv.org/abs/2108.03788",
          "publishedOn": "2021-08-19T01:35:03.433Z",
          "wordCount": 668,
          "title": "Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08202",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kaixin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>",
          "description": "Internet video delivery has undergone a tremendous explosion of growth over\nthe past few years. However, the quality of video delivery system greatly\ndepends on the Internet bandwidth. Deep Neural Networks (DNNs) are utilized to\nimprove the quality of video delivery recently. These methods divide a video\ninto chunks, and stream LR video chunks and corresponding content-aware models\nto the client. The client runs the inference of models to super-resolve the LR\nchunks. Consequently, a large number of models are streamed in order to deliver\na video. In this paper, we first carefully study the relation between models of\ndifferent chunks, then we tactfully design a joint training framework along\nwith the Content-aware Feature Modulation (CaFM) layer to compress these models\nfor neural video delivery. {\\bf With our method, each video chunk only requires\nless than $1\\% $ of original parameters to be streamed, achieving even better\nSR performance.} We conduct extensive experiments across various SR backbones,\nvideo time length, and scaling factors to demonstrate the advantages of our\nmethod. Besides, our method can be also viewed as a new approach of video\ncoding. Our primary experiments achieve better video quality compared with the\ncommercial H.264 and H.265 standard under the same storage cost, showing the\ngreat potential of the proposed method. Code is available\nat:\\url{https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021}",
          "link": "http://arxiv.org/abs/2108.08202",
          "publishedOn": "2021-08-19T01:35:03.426Z",
          "wordCount": 686,
          "title": "Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation. (arXiv:2108.08202v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08286",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhat_G/0/1/0/all/0/1\">Goutam Bhat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "We propose a deep reparametrization of the maximum a posteriori formulation\ncommonly employed in multi-frame image restoration tasks. Our approach is\nderived by introducing a learned error metric and a latent representation of\nthe target image, which transforms the MAP objective to a deep feature space.\nThe deep reparametrization allows us to directly model the image formation\nprocess in the latent space, and to integrate learned image priors into the\nprediction. Our approach thereby leverages the advantages of deep learning,\nwhile also benefiting from the principled multi-frame fusion provided by the\nclassical MAP formulation. We validate our approach through comprehensive\nexperiments on burst denoising and burst super-resolution datasets. Our\napproach sets a new state-of-the-art for both tasks, demonstrating the\ngenerality and effectiveness of the proposed formulation.",
          "link": "http://arxiv.org/abs/2108.08286",
          "publishedOn": "2021-08-19T01:35:03.419Z",
          "wordCount": 576,
          "title": "Deep Reparametrization of Multi-Frame Super-Resolution and Denoising. (arXiv:2108.08286v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.12323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peck_D/0/1/0/all/0/1\">Diondra Peck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_S/0/1/0/all/0/1\">Scott Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dialani_V/0/1/0/all/0/1\">Vandana Dialani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patterson_G/0/1/0/all/0/1\">Genevieve Patterson</a>",
          "description": "We propose DeepMiner, a framework to discover interpretable representations\nin deep neural networks and to build explanations for medical predictions. By\nprobing convolutional neural networks (CNNs) trained to classify cancer in\nmammograms, we show that many individual units in the final convolutional layer\nof a CNN respond strongly to diseased tissue concepts specified by the BI-RADS\nlexicon. After expert annotation of the interpretable units, our proposed\nmethod is able to generate explanations for CNN mammogram classification that\nare consistent with ground truth radiology reports on the Digital Database for\nScreening Mammography. We show that DeepMiner not only enables better\nunderstanding of the nuances of CNN classification decisions but also possibly\ndiscovers new visual knowledge relevant to medical diagnosis.",
          "link": "http://arxiv.org/abs/1805.12323",
          "publishedOn": "2021-08-19T01:35:03.413Z",
          "wordCount": 606,
          "title": "DeepMiner: Discovering Interpretable Representations for Mammogram Classification and Explanation. (arXiv:1805.12323v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-08-19T01:35:03.404Z",
          "wordCount": 602,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.",
          "link": "http://arxiv.org/abs/2108.08236",
          "publishedOn": "2021-08-19T01:35:03.397Z",
          "wordCount": 640,
          "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baozhou_Z/0/1/0/all/0/1\">Zhu Baozhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofstee_P/0/1/0/all/0/1\">Peter Hofstee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ars_Z/0/1/0/all/0/1\">Zaid Al-Ars</a>",
          "description": "Attention mechanism has been regarded as an advanced technique to capture\nlong-range feature interactions and to boost the representation capability for\nconvolutional neural networks. However, we found two ignored problems in\ncurrent attentional activations-based models: the approximation problem and the\ninsufficient capacity problem of the attention maps. To solve the two problems\ntogether, we initially propose an attention module for convolutional neural\nnetworks by developing an AW-convolution, where the shape of attention maps\nmatches that of the weights rather than the activations. Our proposed attention\nmodule is a complementary method to previous attention-based schemes, such as\nthose that apply the attention mechanism to explore the relationship between\nchannel-wise and spatial features. Experiments on several datasets for image\nclassification and object detection tasks show the effectiveness of our\nproposed attention module. In particular, our proposed attention module\nachieves 1.00% Top-1 accuracy improvement on ImageNet classification over a\nResNet101 baseline and 0.63 COCO-style Average Precision improvement on the\nCOCO object detection on top of a Faster R-CNN baseline with the backbone of\nResNet101-FPN. When integrating with the previous attentional activations-based\nmodels, our proposed attention module can further increase their Top-1 accuracy\non ImageNet classification by up to 0.57% and COCO-style Average Precision on\nthe COCO object detection by up to 0.45. Code and pre-trained models will be\npublicly available.",
          "link": "http://arxiv.org/abs/2108.08205",
          "publishedOn": "2021-08-19T01:35:03.378Z",
          "wordCount": 650,
          "title": "An Attention Module for Convolutional Neural Networks. (arXiv:2108.08205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1812.02302",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Damelin_S/0/1/0/all/0/1\">Steven B. Damelin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ragozin_D/0/1/0/all/0/1\">David L. Ragozin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>",
          "description": "We study Min-Max affine approximants of a continuous convex or concave\nfunction $f:\\Delta\\subset \\mathbb R^k\\xrightarrow{} \\mathbb R$ where $\\Delta$\nis a convex compact subset of $\\mathbb R^k$. In the case when $\\Delta$ is a\nsimplex we prove that there is a vertical translate of the supporting\nhyperplane in $\\mathbb R^{k+1}$ of the graph of $f$ at the vertices which is\nthe unique best affine approximant to $f$ on $\\Delta$. For $k=1$, this result\nprovides an extension of the Chebyshev equioscillation theorem for linear\napproximants. Our result has interesting connections to the computer graphics\nproblem of rapid rendering of projective transformations.",
          "link": "http://arxiv.org/abs/1812.02302",
          "publishedOn": "2021-08-19T01:35:03.372Z",
          "wordCount": 692,
          "title": "On Min-Max affine approximants of convex or concave real valued functions from $\\mathbb R^k$, Chebyshev equioscillation and graphics. (arXiv:1812.02302v10 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>",
          "description": "Object detection is a challenging task in remote sensing because objects only\noccupy a few pixels in the images, and the models are required to\nsimultaneously learn object locations and detection. Even though the\nestablished approaches well perform for the objects of regular sizes, they\nachieve weak performance when analyzing small ones or getting stuck in the\nlocal minima (e.g. false object parts). Two possible issues stand in their way.\nFirst, the existing methods struggle to perform stably on the detection of\nsmall objects because of the complicated background. Second, most of the\nstandard methods used hand-crafted features, and do not work well on the\ndetection of objects parts of which are missing. We here address the above\nissues and propose a new architecture with a multiple patch feature pyramid\nnetwork (MPFP-Net). Different from the current models that during training only\npursue the most discriminative patches, in MPFPNet the patches are divided into\nclass-affiliated subsets, in which the patches are related and based on the\nprimary loss function, a sequence of smooth loss functions are determined for\nthe subsets to improve the model for collecting small object parts. To enhance\nthe feature representation for patch selection, we introduce an effective\nmethod to regularize the residual values and make the fusion transition layers\nstrictly norm-preserving. The network contains bottom-up and crosswise\nconnections to fuse the features of different scales to achieve better\naccuracy, compared to several state-of-the-art object detection models. Also,\nthe developed architecture is more efficient than the baselines.",
          "link": "http://arxiv.org/abs/2108.08063",
          "publishedOn": "2021-08-19T01:35:03.365Z",
          "wordCount": 698,
          "title": "Multi-patch Feature Pyramid Network for Weakly Supervised Object Detection in Optical Remote Sensing Images. (arXiv:2108.08063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huawei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiashu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Huaiguang Lei</a>",
          "description": "Fingerprint is an important biological feature of human body, which contains\nabundant gender information. At present, the academic research of fingerprint\ngender characteristics is generally at the level of understanding, while the\nstandardization research is quite limited. In this work, we propose a more\nrobust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid\ngender information from fingerprints. By replacing the normal convolution\noperations with the atrous convolution in the backbone, prior knowledge is\nprovided to keep the edge details and the global reception field can be\nextended. We explored the results in 3 ways: 1) The efficiency of the\nDDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9\nmainstream classifiers are evaluated in our dataset with fair implementation\ndetails. Experimental results demonstrate that the combination of our approach\noutperforms other combinations in terms of average accuracy and separate-gender\naccuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for\nseparate-gender accuracy. 2) The effect of fingers. It is found that the best\nperformance of classifying gender with separate fingers is achieved by the\nright ring finger. 3) The effect of specific features. Based on the\nobservations of the concentrations of fingerprints visualized by our approach,\nit can be inferred that loops and whorls (level 1), bifurcations (level 2), as\nwell as line shapes (level 3) are connected with gender. Finally, we will open\nsource the dataset that contains 6000 fingerprint images",
          "link": "http://arxiv.org/abs/2108.08233",
          "publishedOn": "2021-08-19T01:35:03.358Z",
          "wordCount": 672,
          "title": "Research on Gender-related Fingerprint Features. (arXiv:2108.08233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Quan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "We introduce GNeRF, a framework to marry Generative Adversarial Networks\n(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex\nscenarios with unknown and even randomly initialized camera poses. Recent\nNeRF-based advances have gained popularity for remarkable realistic novel view\nsynthesis. However, most of them heavily rely on accurate camera poses\nestimation, while few recent methods can only optimize the unknown camera poses\nin roughly forward-facing scenes with relatively short camera trajectories and\nrequire rough camera poses initialization. Differently, our GNeRF only utilizes\nrandomly initialized poses for complex outside-in scenarios. We propose a novel\ntwo-phases end-to-end framework. The first phase takes the use of GANs into the\nnew realm for optimizing coarse camera poses and radiance fields jointly, while\nthe second phase refines them with additional photometric loss. We overcome\nlocal minima using a hybrid and iterative optimization scheme. Extensive\nexperiments on a variety of synthetic and natural scenes demonstrate the\neffectiveness of GNeRF. More impressively, our approach outperforms the\nbaselines favorably in those scenes with repeated patterns or even low textures\nthat are regarded as extremely challenging before.",
          "link": "http://arxiv.org/abs/2103.15606",
          "publishedOn": "2021-08-19T01:35:03.342Z",
          "wordCount": 667,
          "title": "GNeRF: GAN-based Neural Radiance Field without Posed Camera. (arXiv:2103.15606v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Recently, FCNs have attracted widespread attention in the CD field. In\npursuit of better CD performance, it has become a tendency to design deeper and\nmore complicated FCNs, which inevitably brings about huge numbers of parameters\nand an unbearable computational burden. With the goal of designing a quite deep\narchitecture to obtain more precise CD results while simultaneously decreasing\nparameter numbers to improve efficiency, in this work, we present a very deep\nand efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the\nnumerous parameters associated with deep architecture, an efficient convolution\nconsisting of depth-wise convolution and group convolution with a channel\nshuffle mechanism is introduced to replace standard convolutional layers. In\nterms of the specific network architecture, EffCDNet does not use mainstream\nUNet-like architecture, but rather adopts the architecture with a very deep\nencoder and a lightweight decoder. In the very deep encoder, two very deep\nsiamese streams stacked by efficient convolution first extract two highly\nrepresentative and informative feature maps from input image-pairs.\nSubsequently, an efficient ASPP module is designed to capture multi-scale\nchange information. In the lightweight decoder, a recurrent criss-cross\nself-attention (RCCA) module is applied to efficiently utilize non-local\nsimilar feature representations to enhance discriminability for each pixel,\nthus effectively separating the changed and unchanged regions. Moreover, to\ntackle the optimization problem in confused pixels, two novel loss functions\nbased on information entropy are presented. On two challenging CD datasets, our\napproach outperforms other SOTA FCN-based methods, with only benchmark-level\nparameter numbers and quite low computational overhead.",
          "link": "http://arxiv.org/abs/2108.08157",
          "publishedOn": "2021-08-19T01:35:03.335Z",
          "wordCount": 716,
          "title": "Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a standard\n20-second long microvascular video takes on average 20 minutes and requires\nextensive training. Several studies have reported that manual analysis hinders\nthe application of microvascular microscopy in a clinical setting. In this\npaper, we present a fully automated state-of-the-art system, called\nCapillaryNet, that can quantify skin nutritive capillary density and red blood\ncell velocity from handheld microscopy videos. Moreover, CapillaryNet measures\nseveral novel microvascular parameters that researchers were previously unable\nto quantify, i.e. capillary hematocrit and Intra-capillary flow velocity\nheterogeneity. Our system has been used to analyze skin microcirculation videos\nfrom various patient groups (COVID-19, pancreatitis, and acute heart diseases).\nOur proposed system excels from existing capillary detection systems as it\ncombines the speed of traditional computer vision algorithms and the accuracy\nof convolutional neural networks.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-08-19T01:35:03.328Z",
          "wordCount": 740,
          "title": "CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Stereo-based 3D detection aims at detecting 3D object bounding boxes from\nstereo images using intermediate depth maps or implicit 3D geometry\nrepresentations, which provides a low-cost solution for 3D perception. However,\nits performance is still inferior compared with LiDAR-based detection\nalgorithms. To detect and localize accurate 3D bounding boxes, LiDAR-based\nmodels can encode accurate object boundaries and surface normal directions from\nLiDAR point clouds. However, the detection results of stereo-based detectors\nare easily affected by the erroneous depth features due to the limitation of\nstereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry\nAware Stereo Detector) to learn stereo-based 3D detectors under the guidance of\nhigh-level geometry-aware representations of LiDAR-based detection models. In\naddition, we found existing voxel-based stereo detectors failed to learn\nsemantic features effectively from indirect 3D supervisions. We attach an\nauxiliary 2D detection head to provide direct 2D semantic supervisions.\nExperiment results show that the above two strategies improved the geometric\nand semantic representation capabilities. Compared with the state-of-the-art\nstereo detector, our method has improved the 3D detection performance of cars,\npedestrians, cyclists by 10.44%, 5.69%, 5.97% mAP respectively on the official\nKITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is\nfurther narrowed.",
          "link": "http://arxiv.org/abs/2108.08258",
          "publishedOn": "2021-08-19T01:35:03.318Z",
          "wordCount": 641,
          "title": "LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector. (arXiv:2108.08258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengsu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "The past year has witnessed the rapid development of applying the Transformer\nmodule to vision problems. While some researchers have demonstrated that\nTransformer-based models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models suffer over-fitting\nespecially when the training data is limited. This paper offers an empirical\nstudy by performing step-by-step operations to gradually transit a\nTransformer-based model to a convolution-based model. The results we obtain\nduring the transition process deliver useful messages for improving visual\nrecognition. Based on these observations, we propose a new architecture named\nVisformer, which is abbreviated from the `Vision-friendly Transformer'. With\nthe same computational complexity, Visformer outperforms both the\nTransformer-based and convolution-based models in terms of ImageNet\nclassification accuracy, and the advantage becomes more significant when the\nmodel complexity is lower or the training set is smaller. The code is available\nat https://github.com/danczs/Visformer.",
          "link": "http://arxiv.org/abs/2104.12533",
          "publishedOn": "2021-08-19T01:35:03.285Z",
          "wordCount": 607,
          "title": "Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chia-Hsiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yu-Shin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yuan-Yao Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai-Chiang Wu</a>",
          "description": "Neural architecture search can discover neural networks with good\nperformance, and One-Shot approaches are prevalent. One-Shot approaches\ntypically require a supernet with weight sharing and predictors that predict\nthe performance of architecture. However, the previous methods take much time\nto generate performance predictors thus are inefficient. To this end, we\npropose FOX-NAS that consists of fast and explainable predictors based on\nsimulated annealing and multivariate regression. Our method is\nquantization-friendly and can be efficiently deployed to the edge. The\nexperiments on different hardware show that FOX-NAS models outperform some\nother popular neural network architectures. For example, FOX-NAS matches\nMobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on\nthe edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer\nVision Challenge (LPCVC), DSP classification track. See all evaluation results\nat https://lpcv.ai/competitions/2020. Search code and pre-trained models are\nreleased at https://github.com/great8nctu/FOX-NAS.",
          "link": "http://arxiv.org/abs/2108.08189",
          "publishedOn": "2021-08-19T01:35:03.279Z",
          "wordCount": 598,
          "title": "FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhaoyang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Based on the powerful feature extraction ability of deep learning\narchitecture, recently, deep-learning based watermarking algorithms have been\nwidely studied. The basic framework of such algorithm is the auto-encoder like\nend-to-end architecture with an encoder, a noise layer and a decoder. The key\nto guarantee robustness is the adversarial training with the differential noise\nlayer. However, we found that none of the existing framework can well ensure\nthe robustness against JPEG compression, which is non-differential but is an\nessential and important image processing operation. To address such\nlimitations, we proposed a novel end-to-end training architecture, which\nutilizes Mini-Batch of Real and Simulated JPEG compression (MBRS) to enhance\nthe JPEG robustness. Precisely, for different mini-batches, we randomly choose\none of real JPEG, simulated JPEG and noise-free layer as the noise layer.\nBesides, we suggest to utilize the Squeeze-and-Excitation blocks which can\nlearn better feature in embedding and extracting stage, and propose a \"message\nprocessor\" to expand the message in a more appreciate way. Meanwhile, to\nimprove the robustness against crop attack, we propose an additive diffusion\nblock into the network. The extensive experimental results have demonstrated\nthe superior performance of the proposed scheme compared with the\nstate-of-the-art algorithms. Under the JPEG compression with quality factor\nQ=50, our models achieve a bit error rate less than 0.01% for extracted\nmessages, with PSNR larger than 36 for the encoded images, which shows the\nwell-enhanced robustness against JPEG attack. Besides, under many other\ndistortions such as Gaussian filter, crop, cropout and dropout, the proposed\nframework also obtains strong robustness. The code implemented by PyTorch\n\\cite{2011torch7} is avaiable in https://github.com/jzyustc/MBRS.",
          "link": "http://arxiv.org/abs/2108.08211",
          "publishedOn": "2021-08-19T01:35:03.270Z",
          "wordCount": 731,
          "title": "MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression. (arXiv:2108.08211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geneva_P/0/1/0/all/0/1\">Patrick Geneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoquan Huang</a>",
          "description": "In this paper we present a consistent and distributed state estimator for\nmulti-robot cooperative localization (CL) which efficiently fuses environmental\nfeatures and loop-closure constraints across time and robots. In particular, we\nleverage covariance intersection (CI) to allow each robot to only estimate its\nown state and autocovariance and compensate for the unknown correlations\nbetween robots. Two novel multi-robot methods for utilizing common\nenvironmental SLAM features are introduced and evaluated in terms of accuracy\nand efficiency. Moreover, we adapt CI to enable drift-free estimation through\nthe use of loop-closure measurement constraints to other robots' historical\nposes without a significant increase in computational cost. The proposed\ndistributed CL estimator is validated against its non-realtime centralized\ncounterpart extensively in both simulations and real-world experiments.",
          "link": "http://arxiv.org/abs/2103.12770",
          "publishedOn": "2021-08-19T01:35:03.254Z",
          "wordCount": 576,
          "title": "Distributed Visual-Inertial Cooperative Localization. (arXiv:2103.12770v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1\">Mike Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_A/0/1/0/all/0/1\">Anurag Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Atulit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1\">Miguel Angel Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paczan_N/0/1/0/all/0/1\">Nathan Paczan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1\">Russ Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>",
          "description": "For many fundamental scene understanding tasks, it is difficult or impossible\nto obtain per-pixel ground truth labels from real images. We address this\nchallenge by introducing Hypersim, a photorealistic synthetic dataset for\nholistic indoor scene understanding. To create our dataset, we leverage a large\nrepository of synthetic scenes created by professional artists, and we generate\n77,400 images of 461 indoor scenes with detailed per-pixel labels and\ncorresponding ground truth geometry. Our dataset: (1) relies exclusively on\npublicly available 3D assets; (2) includes complete scene geometry, material\ninformation, and lighting information for every scene; (3) includes dense\nper-pixel semantic instance segmentations and complete camera information for\nevery image; and (4) factors every image into diffuse reflectance, diffuse\nillumination, and a non-diffuse residual term that captures view-dependent\nlighting effects.\n\nWe analyze our dataset at the level of scenes, objects, and pixels, and we\nanalyze costs in terms of money, computation time, and annotation effort.\nRemarkably, we find that it is possible to generate our entire dataset from\nscratch, for roughly half the cost of training a popular open-source natural\nlanguage processing model. We also evaluate sim-to-real transfer performance on\ntwo real-world scene understanding tasks - semantic segmentation and 3D shape\nprediction - where we find that pre-training on our dataset significantly\nimproves performance on both tasks, and achieves state-of-the-art performance\non the most challenging Pix3D test set. All of our rendered image data, as well\nas all the code we used to generate our dataset and perform our experiments, is\navailable online.",
          "link": "http://arxiv.org/abs/2011.02523",
          "publishedOn": "2021-08-19T01:35:03.246Z",
          "wordCount": 775,
          "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. (arXiv:2011.02523v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1\">Byeongho Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>",
          "description": "Vision Transformer (ViT) extends the application range of transformers from\nlanguage processing to computer vision tasks as being an alternative\narchitecture against the existing convolutional neural networks (CNN). Since\nthe transformer-based architecture has been innovative for computer vision\nmodeling, the design convention towards an effective architecture has been less\nstudied yet. From the successful design principles of CNN, we investigate the\nrole of spatial dimension conversion and its effectiveness on transformer-based\narchitecture. We particularly attend to the dimension reduction principle of\nCNNs; as the depth increases, a conventional CNN increases channel dimension\nand decreases spatial dimensions. We empirically show that such a spatial\ndimension reduction is beneficial to a transformer architecture as well, and\npropose a novel Pooling-based Vision Transformer (PiT) upon the original ViT\nmodel. We show that PiT achieves the improved model capability and\ngeneralization performance against ViT. Throughout the extensive experiments,\nwe further show PiT outperforms the baseline on several tasks such as image\nclassification, object detection, and robustness evaluation. Source codes and\nImageNet models are available at https://github.com/naver-ai/pit",
          "link": "http://arxiv.org/abs/2103.16302",
          "publishedOn": "2021-08-19T01:35:03.239Z",
          "wordCount": 646,
          "title": "Rethinking Spatial Dimensions of Vision Transformers. (arXiv:2103.16302v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhaohui Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>",
          "description": "Bicubic downscaling is a prevalent technique used to reduce the video storage\nburden or to accelerate the downstream processing speed. However, the inverse\nupscaling step is non-trivial, and the downscaled video may also deteriorate\nthe performance of downstream tasks. In this paper, we propose a\nself-conditioned probabilistic framework for video rescaling to learn the\npaired downscaling and upscaling procedures simultaneously. During the\ntraining, we decrease the entropy of the information lost in the downscaling by\nmaximizing its probability conditioned on the strong spatial-temporal prior\ninformation within the downscaled video. After optimization, the downscaled\nvideo by our framework preserves more meaningful information, which is\nbeneficial for both the upscaling step and the downstream tasks, e.g., video\naction recognition task. We further extend the framework to a lossy video\ncompression system, in which a gradient estimator for non-differential\nindustrial lossy codecs is proposed for the end-to-end training of the whole\nsystem. Extensive experimental results demonstrate the superiority of our\napproach on video rescaling, video compression, and efficient action\nrecognition tasks.",
          "link": "http://arxiv.org/abs/2107.11639",
          "publishedOn": "2021-08-19T01:35:03.222Z",
          "wordCount": 627,
          "title": "Self-Conditioned Probabilistic Learning of Video Rescaling. (arXiv:2107.11639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Man M. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjia Zhou</a>",
          "description": "Physical photographs now can be conveniently scanned by smartphones and\nstored forever as a digital version, yet the scanned photos are not restored\nwell. One solution is to train a supervised deep neural network on many digital\nphotos and the corresponding scanned photos. However, it requires a high labor\ncost, leading to limited training data. Previous works create training pairs by\nsimulating degradation using image processing techniques. Their synthetic\nimages are formed with perfectly scanned photos in latent space. Even so, the\nreal-world degradation in smartphone photo scanning remains unsolved since it\nis more complicated due to lens defocus, lighting conditions, losing details\nvia printing. Besides, locally structural misalignment still occurs in data due\nto distorted shapes captured in a 3-D world, reducing restoration performance\nand the reliability of the quantitative evaluation. To solve these problems, we\npropose a semi-supervised Deep Photo Scan (DPScan). First, we present a way of\nproducing real-world degradation and provide the DIV2K-SCAN dataset for\nsmartphone-scanned photo restoration. Also, Local Alignment is proposed to\nreduce the minor misalignment remaining in data. Second, we simulate many\ndifferent variants of the real-world degradation using low-level image\ntransformation to gain a generalization in smartphone-scanned image properties,\nthen train a degradation network to generalize all styles of degradation and\nprovide pseudo-scanned photos for unscanned images as if they were scanned by a\nsmartphone. Finally, we propose a Semi-Supervised Learning that allows our\nrestoration network to be trained on both scanned and unscanned images,\ndiversifying training image content. As a result, the proposed DPScan\nquantitatively and qualitatively outperforms its baseline architecture,\nstate-of-the-art academic research, and industrial products in smartphone photo\nscanning.",
          "link": "http://arxiv.org/abs/2102.06120",
          "publishedOn": "2021-08-19T01:35:03.215Z",
          "wordCount": 751,
          "title": "Deep Photo Scan: Semi-Supervised Learning for dealing with the real-world degradation in Smartphone Photo Scanning. (arXiv:2102.06120v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>",
          "description": "In a real-world scenario, human actions are typically out of the distribution\nfrom training data, which requires a model to both recognize the known actions\nand reject the unknown. Different from image data, video actions are more\nchallenging to be recognized in an open-set setting due to the uncertain\ntemporal dynamics and static bias of human actions. In this paper, we propose a\nDeep Evidential Action Recognition (DEAR) method to recognize actions in an\nopen testing set. Specifically, we formulate the action recognition problem\nfrom the evidential deep learning (EDL) perspective and propose a novel model\ncalibration method to regularize the EDL training. Besides, to mitigate the\nstatic bias of video representation, we propose a plug-and-play module to\ndebias the learned representation through contrastive learning. Experimental\nresults show that our DEAR method achieves consistent performance gain on\nmultiple mainstream action recognition models and benchmarks. Code and\npre-trained models are available at\n{\\small{\\url{https://www.rit.edu/actionlab/dear}}}.",
          "link": "http://arxiv.org/abs/2107.10161",
          "publishedOn": "2021-08-19T01:35:03.208Z",
          "wordCount": 619,
          "title": "Evidential Deep Learning for Open Set Action Recognition. (arXiv:2107.10161v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhuangwei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "3D LiDAR (light detection and ranging) semantic segmentation is important in\nscene understanding for many applications, such as auto-driving and robotics.\nFor example, for autonomous cars equipped with RGB cameras and LiDAR, it is\ncrucial to fuse complementary information from different sensors for robust and\naccurate segmentation. Existing fusion-based methods, however, may not achieve\npromising performance due to the vast difference between the two modalities. In\nthis work, we investigate a collaborative fusion scheme called perception-aware\nmulti-sensor fusion (PMF) to exploit perceptual information from two\nmodalities, namely, appearance information from RGB images and spatio-depth\ninformation from point clouds. To this end, we first project point clouds to\nthe camera coordinates to provide spatio-depth information for RGB images.\nThen, we propose a two-stream network to extract features from the two\nmodalities, separately, and fuse the features by effective residual-based\nfusion modules. Moreover, we propose additional perception-aware losses to\nmeasure the perceptual difference between the two modalities. Extensive\nexperiments on two benchmark data sets show the superiority of our method. For\nexample, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8 in\nmIoU.",
          "link": "http://arxiv.org/abs/2106.15277",
          "publishedOn": "2021-08-19T01:35:03.200Z",
          "wordCount": 656,
          "title": "Perception-Aware Multi-Sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Recently, there has been discussions on the ill-posed nature of\nsuper-resolution that multiple possible reconstructions exist for a given\nlow-resolution image. Using normalizing flows, SRflow[23] achieves\nstate-of-the-art perceptual quality by learning the distribution of the output\ninstead of a deterministic output to one estimate. In this paper, we adapt the\nconcepts of SRFlow to improve GAN-based super-resolution by properly\nimplementing the one-to-many property. We modify the generator to estimate a\ndistribution as a mapping from random noise. We improve the content loss that\nhampers the perceptual training objectives. We also propose additional training\ntechniques to further enhance the perceptual quality of generated images. Using\nour proposed methods, we were able to improve the performance of ESRGAN[1] in\nx4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual\nextreme SR by applying our methods to RFB-ESRGAN[21].",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-08-19T01:35:03.194Z",
          "wordCount": 601,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models can synthesize photorealistic images of human faces\nwith novel identities. However, a key challenge to the wide applicability of\nsuch techniques is to provide independent control over semantically meaningful\nparameters: appearance, head pose, face shape, and facial expressions. In this\npaper, we propose VariTex - to the best of our knowledge the first method that\nlearns a variational latent feature space of neural face textures, which allows\nsampling of novel identities. We combine this generative model with a\nparametric face model and gain explicit control over head pose and facial\nexpressions. To generate complete images of human heads, we propose an additive\ndecoder that adds plausible details such as hair. A novel training scheme\nenforces a pose-independent latent space and in consequence, allows learning a\none-to-many mapping between latent codes and pose-conditioned exterior regions.\nThe resulting method can generate geometrically consistent images of novel\nidentities under fine-grained control over head pose, face shape, and facial\nexpressions. This facilitates a broad range of downstream tasks, like sampling\nnovel identities, changing the head pose, expression transfer, and more. Code\nand models are available for research on https://mcbuehler.github.io/VariTex.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-08-19T01:35:03.155Z",
          "wordCount": 694,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathod_V/0/1/0/all/0/1\">Vivek Rathod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jonathan Huang</a>",
          "description": "Instance segmentation models today are very accurate when trained on large\nannotated datasets, but collecting mask annotations at scale is prohibitively\nexpensive. We address the partially supervised instance segmentation problem in\nwhich one can train on (significantly cheaper) bounding boxes for all\ncategories but use masks only for a subset of categories. In this work, we\nfocus on a popular family of models which apply differentiable cropping to a\nfeature map and predict a mask based on the resulting crop. Under this family,\nwe study Mask R-CNN and discover that instead of its default strategy of\ntraining the mask-head with a combination of proposals and groundtruth boxes,\ntraining the mask-head with only groundtruth boxes dramatically improves its\nperformance on novel classes. This training strategy also allows us to take\nadvantage of alternative mask-head architectures, which we exploit by replacing\nthe typical mask-head of 2-4 layers with significantly deeper off-the-shelf\narchitectures (e.g. ResNet, Hourglass models). While many of these\narchitectures perform similarly when trained in fully supervised mode, our main\nfinding is that they can generalize to novel classes in dramatically different\nways. We call this ability of mask-heads to generalize to unseen classes the\nstrong mask generalization effect and show that without any specialty modules\nor losses, we can achieve state-of-the-art results in the partially supervised\nCOCO instance segmentation benchmark. Finally, we demonstrate that our effect\nis general, holding across underlying detection methodologies (including\nanchor-based, anchor-free or no detector at all) and across different backbone\nnetworks. Code and pre-trained models are available at https://git.io/deepmac.",
          "link": "http://arxiv.org/abs/2104.00613",
          "publishedOn": "2021-08-19T01:35:03.148Z",
          "wordCount": 728,
          "title": "The surprising impact of mask-head architecture on novel class segmentation. (arXiv:2104.00613v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>",
          "description": "We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.",
          "link": "http://arxiv.org/abs/2102.10772",
          "publishedOn": "2021-08-19T01:35:03.140Z",
          "wordCount": 617,
          "title": "UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>",
          "description": "The recently proposed Visual image Transformers (ViT) with pure attention\nhave achieved promising performance on image recognition tasks, such as image\nclassification. However, the routine of the current ViT model is to maintain a\nfull-length patch sequence during inference, which is redundant and lacks\nhierarchical representation. To this end, we propose a Hierarchical Visual\nTransformer (HVT) which progressively pools visual tokens to shrink the\nsequence length and hence reduces the computational cost, analogous to the\nfeature maps downsampling in Convolutional Neural Networks (CNNs). It brings a\ngreat benefit that we can increase the model capacity by scaling dimensions of\ndepth/width/resolution/patch size without introducing extra computational\ncomplexity due to the reduced sequence length. Moreover, we empirically find\nthat the average pooled visual tokens contain more discriminative information\nthan the single class token. To demonstrate the improved scalability of our\nHVT, we conduct extensive experiments on the image classification task. With\ncomparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and\nCIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT",
          "link": "http://arxiv.org/abs/2103.10619",
          "publishedOn": "2021-08-19T01:35:03.123Z",
          "wordCount": 640,
          "title": "Scalable Vision Transformers with Hierarchical Pooling. (arXiv:2103.10619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1\">Atsuhiro Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "We present Neural Articulated Radiance Field (NARF), a novel deformable 3D\nrepresentation for articulated objects learned from images. While recent\nadvances in 3D implicit representation have made it possible to learn models of\ncomplex objects, learning pose-controllable representations of articulated\nobjects remains a challenge, as current methods require 3D shape supervision\nand are unable to render appearance. In formulating an implicit representation\nof 3D articulated objects, our method considers only the rigid transformation\nof the most relevant object part in solving for the radiance field at each 3D\nlocation. In this way, the proposed method represents pose-dependent changes\nwithout significantly increasing the computational complexity. NARF is fully\ndifferentiable and can be trained from images with pose annotations. Moreover,\nthrough the use of an autoencoder, it can learn appearance variations over\nmultiple instances of an object class. Experiments show that the proposed\nmethod is efficient and can generalize well to novel poses. The code is\navailable for research purposes at https://github.com/nogu-atsu/NARF",
          "link": "http://arxiv.org/abs/2104.03110",
          "publishedOn": "2021-08-19T01:35:03.108Z",
          "wordCount": 623,
          "title": "Neural Articulated Radiance Field. (arXiv:2104.03110v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanran He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Deep neural networks are known to be extremely vulnerable to adversarial\nexamples under white-box setting. Moreover, the malicious adversaries crafted\non the surrogate (source) model often exhibit black-box transferability on\nother models with the same learning task but having different architectures.\nRecently, various methods are proposed to boost the adversarial\ntransferability, among which the input transformation is one of the most\neffective approaches. We investigate in this direction and observe that\nexisting transformations are all applied on a single image, which might limit\nthe adversarial transferability. To this end, we propose a new input\ntransformation based attack method called Admix that considers the input image\nand a set of images randomly sampled from other categories. Instead of directly\ncalculating the gradient on the original input, Admix calculates the gradient\non the input image admixed with a small portion of each add-in image while\nusing the original label of the input to craft more transferable adversaries.\nEmpirical evaluations on standard ImageNet dataset demonstrate that Admix could\nachieve significantly better transferability than existing input transformation\nmethods under both single model setting and ensemble-model setting. By\nincorporating with existing input transformations, our method could further\nimprove the transferability and outperforms the state-of-the-art combination of\ninput transformations by a clear margin when attacking nine advanced defense\nmodels under ensemble-model setting. Code is available at\nhttps://github.com/JHL-HUST/Admix.",
          "link": "http://arxiv.org/abs/2102.00436",
          "publishedOn": "2021-08-19T01:35:03.100Z",
          "wordCount": 693,
          "title": "Admix: Enhancing the Transferability of Adversarial Attacks. (arXiv:2102.00436v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yingjie Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multi-level feature fusion is a fundamental topic in computer vision. It has\nbeen exploited to detect, segment and classify objects at various scales. When\nmulti-level features meet multi-modal cues, the optimal feature aggregation and\nmulti-modal learning strategy become a hot potato. In this paper, we leverage\nthe inherent multi-modal and multi-level nature of RGB-D salient object\ndetection to devise a novel cascaded refinement network. In particular, first,\nwe propose to regroup the multi-level features into teacher and student\nfeatures using a bifurcated backbone strategy (BBS). Second, we introduce a\ndepth-enhanced module (DEM) to excavate informative depth cues from the channel\nand spatial views. Then, RGB and depth modalities are fused in a complementary\nway. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is\nsimple, efficient, and backbone-independent. Extensive experiments show that\nBBS-Net significantly outperforms eighteen SOTA models on eight challenging\ndatasets under five evaluation measures, demonstrating the superiority of our\napproach ($\\sim 4 \\%$ improvement in S-measure $vs.$ the top-ranked model:\nDMRA-iccv2019). In addition, we provide a comprehensive analysis on the\ngeneralization ability of different RGB-D datasets and provide a powerful\ntraining set for future research.",
          "link": "http://arxiv.org/abs/2007.02713",
          "publishedOn": "2021-08-19T01:35:03.094Z",
          "wordCount": 666,
          "title": "Bifurcated backbone strategy for RGB-D salient object detection. (arXiv:2007.02713v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1\">Melanie Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1\">Vanessa Emanuela Guarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1\">Ashkan Mokarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1\">Lisa Mais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1\">Jan Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>",
          "description": "Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.",
          "link": "http://arxiv.org/abs/2101.05846",
          "publishedOn": "2021-08-19T01:35:03.087Z",
          "wordCount": 737,
          "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code are available at https://worldsheet.github.io.",
          "link": "http://arxiv.org/abs/2012.09854",
          "publishedOn": "2021-08-19T01:35:03.069Z",
          "wordCount": 658,
          "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "For all the ways convolutional neural nets have revolutionized computer\nvision in recent years, one important aspect has received surprisingly little\nattention: the effect of image size on the accuracy of tasks being trained for.\nTypically, to be efficient, the input images are resized to a relatively small\nspatial resolution (e.g. 224x224), and both training and inference are carried\nout at this resolution. The actual mechanism for this re-scaling has been an\nafterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic\nare commonly used in most machine learning software frameworks. But do these\nresizers limit the on task performance of the trained networks? The answer is\nyes. Indeed, we show that the typical linear resizer can be replaced with\nlearned resizers that can substantially improve performance. Importantly, while\nthe classical resizers typically result in better perceptual quality of the\ndownscaled images, our proposed learned resizers do not necessarily give better\nvisual quality, but instead improve task performance. Our learned image resizer\nis jointly trained with a baseline vision model. This learned CNN-based resizer\ncreates machine friendly visual manipulations that lead to a consistent\nimprovement of the end task metric over the baseline model. Specifically, here\nwe focus on the classification task with the ImageNet dataset, and experiment\nwith four different models to learn resizers adapted to each model. Moreover,\nwe show that the proposed resizer can also be useful for fine-tuning the\nclassification baselines for other vision tasks. To this end, we experiment\nwith three different baselines to develop image quality assessment (IQA) models\non the AVA dataset.",
          "link": "http://arxiv.org/abs/2103.09950",
          "publishedOn": "2021-08-19T01:35:03.058Z",
          "wordCount": 727,
          "title": "Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "The great success of Convolutional Neural Networks (CNN) for facial attribute\nprediction relies on a large amount of labeled images. Facial image datasets\nare usually annotated by some commonly used attributes (e.g., gender), while\nlabels for the other attributes (e.g., big nose) are limited which causes their\nprediction challenging. To address this problem, we use a new Multi-Task\nLearning (MTL) paradigm in which a facial attribute predictor uses the\nknowledge of other related attributes to obtain a better generalization\nperformance. Here, we leverage MLT paradigm in two problem settings. First, it\nis assumed that the structure of the tasks (e.g., grouping pattern of facial\nattributes) is known as a prior knowledge, and parameters of the tasks (i.e.,\npredictors) within the same group are represented by a linear combination of a\nlimited number of underlying basis tasks. Here, a sparsity constraint on the\ncoefficients of this linear combination is also considered such that each task\nis represented in a more structured and simpler manner. Second, it is assumed\nthat the structure of the tasks is unknown, and then structure and parameters\nof the tasks are learned jointly by using a Laplacian regularization framework.\nOur MTL methods are compared with competing methods for facial attribute\nprediction to show its effectiveness.",
          "link": "http://arxiv.org/abs/2108.04353",
          "publishedOn": "2021-08-19T01:35:03.048Z",
          "wordCount": 669,
          "title": "Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ci_Y/0/1/0/all/0/1\">Yuanzheng Ci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "The automation of neural architecture design has been a coveted alternative\nto human experts. Recent works have small search space, which is easier to\noptimize but has a limited upper bound of the optimal solution. Extra human\ndesign is needed for those methods to propose a more suitable space with\nrespect to the specific task and algorithm capacity. To further enhance the\ndegree of automation for neural architecture search, we present a Neural\nSearch-space Evolution (NSE) scheme that iteratively amplifies the results from\nthe previous effort by maintaining an optimized search space subset. This\ndesign minimizes the necessity of a well-designed search space. We further\nextend the flexibility of obtainable architectures by introducing a learnable\nmulti-branch setting. By employing the proposed method, a consistent\nperformance gain is achieved during a progressive search over upcoming search\nspaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs,\nwhich yielded a state-of-the-art performance among previous auto-generated\narchitectures that do not involve knowledge distillation or weight pruning.\nWhen the latency constraint is adopted, our result also performs better than\nthe previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.",
          "link": "http://arxiv.org/abs/2011.10904",
          "publishedOn": "2021-08-19T01:35:03.027Z",
          "wordCount": 671,
          "title": "Evolving Search Space for Neural Architecture Search. (arXiv:2011.10904v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1\">Christoph Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "The presence of objects that are confusingly similar to the tracked target,\nposes a fundamental challenge in appearance-based visual tracking. Such\ndistractor objects are easily misclassified as the target itself, leading to\neventual tracking failure. While most methods strive to suppress distractors\nthrough more powerful appearance models, we take an alternative approach.\n\nWe propose to keep track of distractor objects in order to continue tracking\nthe target. To this end, we introduce a learned association network, allowing\nus to propagate the identities of all target candidates from frame-to-frame. To\ntackle the problem of lacking ground-truth correspondences between distractor\nobjects in visual tracking, we propose a training strategy that combines\npartial annotations with self-supervision. We conduct comprehensive\nexperimental validation and analysis of our approach on several challenging\ndatasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving\nan AUC score of 67.1% on LaSOT and a +5.8% absolute gain on the OxUvA long-term\ndataset.",
          "link": "http://arxiv.org/abs/2103.16556",
          "publishedOn": "2021-08-19T01:35:03.019Z",
          "wordCount": 647,
          "title": "Learning Target Candidate Association to Keep Track of What Not to Track. (arXiv:2103.16556v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongmei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "Due to the over-parameterization nature, neural networks are a powerful tool\nfor nonlinear function approximation. In order to achieve good generalization\non unseen data, a suitable inductive bias is of great importance for neural\nnetworks. One of the most straightforward ways is to regularize the neural\nnetwork with some additional objectives. L2 regularization serves as a standard\nregularization for neural networks. Despite its popularity, it essentially\nregularizes one dimension of the individual neuron, which is not strong enough\nto control the capacity of highly over-parameterized neural networks. Motivated\nby this, hyperspherical uniformity is proposed as a novel family of relational\nregularizations that impact the interaction among neurons. We consider several\ngeometrically distinct ways to achieve hyperspherical uniformity. The\neffectiveness of hyperspherical uniformity is justified by theoretical insights\nand empirical evaluations.",
          "link": "http://arxiv.org/abs/2103.01649",
          "publishedOn": "2021-08-19T01:35:02.970Z",
          "wordCount": 598,
          "title": "Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaoua_R/0/1/0/all/0/1\">Ryad Kaoua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durr_A/0/1/0/all/0/1\">Alexandra Durr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaris_S/0/1/0/all/0/1\">Stavros Lazaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>",
          "description": "Illustrations are an essential transmission instrument. For an historian, the\nfirst step in studying their evolution in a corpus of similar manuscripts is to\nidentify which ones correspond to each other. This image collation task is\ndaunting for manuscripts separated by many lost copies, spreading over\ncenturies, which might have been completely re-organized and greatly modified\nto adapt to novel knowledge or belief and include hundreds of illustrations.\nOur contributions in this paper are threefold. First, we introduce the task of\nillustration collation and a large annotated public dataset to evaluate\nsolutions, including 6 manuscripts of 2 different texts with more than 2 000\nillustrations and 1 200 annotated correspondences. Second, we analyze state of\nthe art similarity measures for this task and show that they succeed in simple\ncases but struggle for large manuscripts when the illustrations have undergone\nvery significant changes and are discriminated only by fine details. Finally,\nwe show clear evidence that significant performance boosts can be expected by\nexploiting cycle-consistent correspondences. Our code and data are available on\nthis http URL",
          "link": "http://arxiv.org/abs/2108.08109",
          "publishedOn": "2021-08-19T01:35:02.959Z",
          "wordCount": 616,
          "title": "Image Collation: Matching illustrations in manuscripts. (arXiv:2108.08109v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengfei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>",
          "description": "Differentiable architecture search (DARTS) marks a milestone in Neural\nArchitecture Search (NAS), boasting simplicity and small search costs. However,\nDARTS still suffers from frequent performance collapse, which happens when some\noperations, such as skip connections, zeroes and poolings, dominate the\narchitecture. In this paper, we are the first to point out that the phenomenon\nis attributed to bi-level optimization. We propose Single-DARTS which merely\nuses single-level optimization, updating network weights and architecture\nparameters simultaneously with the same data batch. Even single-level\noptimization has been previously attempted, no literature provides a systematic\nexplanation on this essential point. Replacing the bi-level optimization,\nSingle-DARTS obviously alleviates performance collapse as well as enhances the\nstability of architecture search. Experiment results show that Single-DARTS\nachieves state-of-the-art performance on mainstream search spaces. For\ninstance, on NAS-Benchmark-201, the searched architectures are nearly optimal\nones. We also validate that the single-level optimization framework is much\nmore stable than the bi-level one. We hope that this simple yet effective\nmethod will give some insights on differential architecture search. The code is\navailable at https://github.com/PencilAndBike/Single-DARTS.git.",
          "link": "http://arxiv.org/abs/2108.08128",
          "publishedOn": "2021-08-19T01:35:02.920Z",
          "wordCount": 616,
          "title": "Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "RGB-D saliency detection has attracted increasing attention, due to its\neffectiveness and the fact that depth cues can now be conveniently captured.\nExisting works often focus on learning a shared representation through various\nfusion strategies, with few methods explicitly considering how to preserve\nmodality-specific characteristics. In this paper, taking a new perspective, we\npropose a specificity-preserving network (SP-Net) for RGB-D saliency detection,\nwhich benefits saliency detection performance by exploring both the shared\ninformation and modality-specific properties (e.g., specificity). Specifically,\ntwo modality-specific networks and a shared learning network are adopted to\ngenerate individual and shared saliency maps. A cross-enhanced integration\nmodule (CIM) is proposed to fuse cross-modal features in the shared learning\nnetwork, which are then propagated to the next layer for integrating\ncross-level information. Besides, we propose a multi-modal feature aggregation\n(MFA) module to integrate the modality-specific features from each individual\ndecoder into the shared decoder, which can provide rich complementary\nmulti-modal information to boost the saliency detection performance. Further, a\nskip connection is used to combine hierarchical features between the encoder\nand decoder layers. Experiments on six benchmark datasets demonstrate that our\nSP-Net outperforms other state-of-the-art methods. Code is available at:\nhttps://github.com/taozh2017/SPNet.",
          "link": "http://arxiv.org/abs/2108.08162",
          "publishedOn": "2021-08-19T01:35:02.912Z",
          "wordCount": 632,
          "title": "Specificity-preserving RGB-D Saliency Detection. (arXiv:2108.08162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1\">Anna Kukleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Both generalized and incremental few-shot learning have to deal with three\nmajor challenges: learning novel classes from only few samples per class,\npreventing catastrophic forgetting of base classes, and classifier calibration\nacross novel and base classes. In this work we propose a three-stage framework\nthat allows to explicitly and effectively address these challenges. While the\nfirst phase learns base classes with many samples, the second phase learns a\ncalibrated classifier for novel classes from few samples while also preventing\ncatastrophic forgetting. In the final phase, calibration is achieved across all\nclasses. We evaluate the proposed framework on four challenging benchmark\ndatasets for image and video few-shot classification and obtain\nstate-of-the-art results for both generalized and incremental few shot\nlearning.",
          "link": "http://arxiv.org/abs/2108.08165",
          "publishedOn": "2021-08-19T01:35:02.878Z",
          "wordCount": 564,
          "title": "Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting. (arXiv:2108.08165v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08158",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Okamoto_H/0/1/0/all/0/1\">Hideaki Okamoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nomura_T/0/1/0/all/0/1\">Takakiyo Nomura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabeshima_K/0/1/0/all/0/1\">Kazuhito Nabeshima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_J/0/1/0/all/0/1\">Jun Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "X-ray examination is suitable for screening of gastric cancer. Compared to\nendoscopy, which can only be performed by doctors, X-ray imaging can also be\nperformed by radiographers, and thus, can treat more patients. However, the\ndiagnostic accuracy of gastric radiographs is as low as 85%. To address this\nproblem, highly accurate and quantitative automated diagnosis using machine\nlearning needs to be performed. This paper proposes a diagnostic support method\nfor detecting gastric cancer sites from X-ray images with high accuracy. The\ntwo new technical proposal of the method are (1) stochastic functional gastric\nimage augmentation (sfGAIA), and (2) hard boundary box training (HBBT). The\nformer is a probabilistic enhancement of gastric folds in X-ray images based on\nmedical knowledge, whereas the latter is a recursive retraining technique to\nreduce false positives. We use 4,724 gastric radiographs of 145 patients in\nclinical practice and evaluate the cancer detection performance of the method\nin a patient-based five-group cross-validation. The proposed sfGAIA and HBBT\nsignificantly enhance the performance of the EfficientDet-D7 network by 5.9% in\nterms of the F1-score, and our screening method reaches a practical screening\ncapability for gastric cancer (F1: 57.8%, recall: 90.2%, precision: 42.5%).",
          "link": "http://arxiv.org/abs/2108.08158",
          "publishedOn": "2021-08-19T01:35:02.870Z",
          "wordCount": 659,
          "title": "Gastric Cancer Detection from X-ray Images Using Effective Data Augmentation and Hard Boundary Box Training. (arXiv:2108.08158v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruohao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Learning RAW-to-sRGB mapping has drawn increasing attention in recent years,\nwherein an input raw image is trained to imitate the target sRGB image captured\nby another camera. However, the severe color inconsistency makes it very\nchallenging to generate well-aligned training pairs of input raw and target\nsRGB images. While learning with inaccurately aligned supervision is prone to\ncausing pixel shift and producing blurry results. In this paper, we circumvent\nsuch issue by presenting a joint learning model for image alignment and\nRAW-to-sRGB mapping. To diminish the effect of color inconsistency in image\nalignment, we introduce to use a global color mapping (GCM) module to generate\nan initial sRGB image given the input raw image, which can keep the spatial\nlocation of the pixels unchanged, and the target sRGB image is utilized to\nguide GCM for converting the color towards it. Then a pre-trained optical flow\nestimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to\nalign with the GCM output. To alleviate the effect of inaccurately aligned\nsupervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB\nmapping. When training is done, the GCM module and optical flow network can be\ndetached, thereby bringing no extra computation cost for inference. Experiments\nshow that our method performs favorably against state-of-the-arts on ZRR and\nSR-RAW datasets. With our joint learning model, a light-weight backbone can\nachieve better quantitative and qualitative performance on ZRR dataset. Codes\nare available at https://github.com/cszhilu1998/RAW-to-sRGB.",
          "link": "http://arxiv.org/abs/2108.08119",
          "publishedOn": "2021-08-19T01:35:02.843Z",
          "wordCount": 686,
          "title": "Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision. (arXiv:2108.08119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbach_M/0/1/0/all/0/1\">Markus D. Solbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>",
          "description": "The STAR architecture was designed to test the value of the full Selective\nTuning model of visual attention for complex real-world visuospatial tasks and\nbehaviors. However, knowledge of how humans solve such tasks in 3D as active\nobservers is lean. We thus devised a novel experimental setup and examined such\nbehavior. We discovered that humans exhibit a variety of problem-solving\nstrategies whose breadth and complexity are surprising and not easily handled\nby current methodologies. It is apparent that solution methods are dynamically\ncomposed by hypothesizing sequences of actions, testing them, and if they fail,\ntrying different ones. The importance of active observation is striking as is\nthe lack of any learning effect. These results inform our Cognitive Program\nrepresentation of STAR extending its relevance to real-world tasks.",
          "link": "http://arxiv.org/abs/2108.08145",
          "publishedOn": "2021-08-19T01:35:02.800Z",
          "wordCount": 580,
          "title": "Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested. (arXiv:2108.08145v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1\">Munan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Unsupervised domain adaption has proven to be an effective approach for\nalleviating the intensive workload of manual annotation by aligning the\nsynthetic source-domain data and the real-world target-domain samples.\nUnfortunately, mapping the target-domain distribution to the source-domain\nunconditionally may distort the essential structural information of the\ntarget-domain data. To this end, we firstly propose to introduce a novel\nmulti-anchor based active learning strategy to assist domain adaptation\nregarding the semantic segmentation task. By innovatively adopting multiple\nanchors instead of a single centroid, the source domain can be better\ncharacterized as a multimodal distribution, thus more representative and\ncomplimentary samples are selected from the target domain. With little workload\nto manually annotate these active samples, the distortion of the target-domain\ndistribution can be effectively alleviated, resulting in a large performance\ngain. The multi-anchor strategy is additionally employed to model the\ntarget-distribution. By regularizing the latent representation of the target\nsamples compact around multiple anchors through a novel soft alignment loss,\nmore precise segmentation can be achieved. Extensive experiments are conducted\non public datasets to demonstrate that the proposed approach outperforms\nstate-of-the-art methods significantly, along with thorough ablation study to\nverify the effectiveness of each component.",
          "link": "http://arxiv.org/abs/2108.08012",
          "publishedOn": "2021-08-19T01:35:02.794Z",
          "wordCount": 640,
          "title": "Multi-Anchor Active Domain Adaptation for Semantic Segmentation. (arXiv:2108.08012v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08095",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1\">Thiab Taha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>",
          "description": "Medical Imaging is one of the growing fields in the world of computer vision.\nIn this study, we aim to address the Diabetic Retinopathy (DR) problem as one\nof the open challenges in medical imaging. In this research, we propose a new\nlesion detection architecture, comprising of two sub-modules, which is an\noptimal solution to detect and find not only the type of lesions caused by DR,\ntheir corresponding bounding boxes, and their masks; but also the severity\nlevel of the overall case. Aside from traditional accuracy, we also use two\npopular evaluation criteria to evaluate the outputs of our models, which are\nintersection over union (IOU) and mean average precision (mAP). We hypothesize\nthat this new solution enables specialists to detect lesions with high\nconfidence and estimate the severity of the damage with high accuracy.",
          "link": "http://arxiv.org/abs/2108.08095",
          "publishedOn": "2021-08-19T01:35:02.758Z",
          "wordCount": 620,
          "title": "DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>",
          "description": "In this work, an extensive review of literature in the field of gesture\nrecognition carried out along with the implementation of a simple\nclassification system for hand hygiene stages based on deep learning solutions.\nA subset of robust dataset that consist of handwashing gestures with two hands\nas well as one-hand gestures such as linear hand movement utilized. A\npretrained neural network model, RES Net 50, with image net weights used for\nthe classification of 3 categories: Linear hand movement, rub hands palm to\npalm and rub hands with fingers interlaced movement. Correct predictions made\nfor the first two classes with > 60% accuracy. A complete dataset along with\nincreased number of classes and training steps will be explored as a future\nwork.",
          "link": "http://arxiv.org/abs/2108.08127",
          "publishedOn": "2021-08-19T01:35:02.742Z",
          "wordCount": 551,
          "title": "Hand Hygiene Video Classification Based on Deep Learning. (arXiv:2108.08127v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yao Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "This paper deals with a challenging task of video scene graph generation\n(VidSGG), which could serve as a structured video representation for high-level\nunderstanding tasks. We present a new {\\em detect-to-track} paradigm for this\ntask by decoupling the context modeling for relation prediction from the\ncomplicated low-level entity tracking. Specifically, we design an efficient\nmethod for frame-level VidSGG, termed as {\\em Target Adaptive Context\nAggregation Network} (TRACE), with a focus on capturing spatio-temporal context\ninformation for relation recognition. Our TRACE framework streamlines the\nVidSGG pipeline with a modular design, and presents two unique blocks of\nHierarchical Relation Tree (HRTree) construction and Target-adaptive Context\nAggregation. More specific, our HRTree first provides an adpative structure for\norganizing possible relation candidates efficiently, and guides context\naggregation module to effectively capture spatio-temporal structure\ninformation. Then, we obtain a contextualized feature representation for each\nrelation candidate and build a classification head to recognize its relation\ncategory. Finally, we provide a simple temporal association strategy to track\nTRACE detected results to yield the video-level VidSGG. We perform experiments\non two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results\ndemonstrate that our TRACE achieves the state-of-the-art performance. The code\nand models are made available at \\url{https://github.com/MCG-NJU/TRACE}.",
          "link": "http://arxiv.org/abs/2108.08121",
          "publishedOn": "2021-08-19T01:35:02.736Z",
          "wordCount": 644,
          "title": "Target Adaptive Context Aggregation for Video Scene Graph Generation. (arXiv:2108.08121v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_A/0/1/0/all/0/1\">Anuj Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Kshitij Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majee_A/0/1/0/all/0/1\">Anay Majee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Anbumani Subramanian</a>",
          "description": "Incremental few-shot learning has emerged as a new and challenging area in\ndeep learning, whose objective is to train deep learning models using very few\nsamples of new class data, and none of the old class data. In this work we\ntackle the problem of batch incremental few-shot road object detection using\ndata from the India Driving Dataset (IDD). Our approach, DualFusion, combines\nobject detectors in a manner that allows us to learn to detect rare objects\nwith very limited data, all without severely degrading the performance of the\ndetector on the abundant classes. In the IDD OpenSet incremental few-shot\ndetection task, we achieve a mAP50 score of 40.0 on the base classes and an\noverall mAP50 score of 38.8, both of which are the highest to date. In the COCO\nbatch incremental few-shot detection task, we achieve a novel AP score of 9.9,\nsurpassing the state-of-the-art novel class performance on the same by over 6.6\ntimes.",
          "link": "http://arxiv.org/abs/2108.08048",
          "publishedOn": "2021-08-19T01:35:02.729Z",
          "wordCount": 607,
          "title": "Few-Shot Batch Incremental Road Object Detection via Detector Fusion. (arXiv:2108.08048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingkui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>",
          "description": "We present a deep learning pipeline that leverages network self-prior to\nrecover a full 3D model consisting of both a triangular mesh and a texture map\nfrom the colored 3D point cloud. Different from previous methods either\nexploiting 2D self-prior for image editing or 3D self-prior for pure surface\nreconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep\nneural networks to significantly improve the geometry quality and produce a\nhigh-resolution texture map, which is typically missing from the output of\ncommodity-level 3D scanners. In particular, we first generate an initial mesh\nusing a 3D convolutional neural network with 3D self-prior, and then encode\nboth 3D information and color information in the 2D UV atlas, which is further\nrefined by 2D convolutional neural networks with the self-prior. In this way,\nboth 2D and 3D self-priors are utilized for the mesh and texture recovery.\nExperiments show that, without the need of any additional training data, our\nmethod recovers the 3D textured mesh model of high quality from sparse input,\nand outperforms the state-of-the-art methods in terms of both the geometry and\ntexture quality.",
          "link": "http://arxiv.org/abs/2108.08017",
          "publishedOn": "2021-08-19T01:35:02.704Z",
          "wordCount": 626,
          "title": "Deep Hybrid Self-Prior for Full 3D Mesh Generation. (arXiv:2108.08017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1\">Nelson Nauata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1\">Yasutaka Furukawa</a>",
          "description": "This paper presents an explore-and-classify framework for structured\narchitectural reconstruction from an aerial image. Starting from a potentially\nimperfect building reconstruction by an existing algorithm, our approach 1)\nexplores the space of building models by modifying the reconstruction via\nheuristic actions; 2) learns to classify the correctness of building models\nwhile generating classification labels based on the ground-truth, and 3)\nrepeat. At test time, we iterate exploration and classification, seeking for a\nresult with the best classification score. We evaluate the approach using\ninitial reconstructions by two baselines and two state-of-the-art\nreconstruction algorithms. Qualitative and quantitative evaluations demonstrate\nthat our approach consistently improves the reconstruction quality from every\ninitial reconstruction.",
          "link": "http://arxiv.org/abs/2108.07990",
          "publishedOn": "2021-08-19T01:35:02.694Z",
          "wordCount": 555,
          "title": "Structured Outdoor Architecture Reconstruction by Exploration and Classification. (arXiv:2108.07990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yike Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bailan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenggang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_F/0/1/0/all/0/1\">Feng Dai</a>",
          "description": "As one of the most fundamental and challenging problems in computer vision,\nobject detection tries to locate object instances and find their categories in\nnatural images. The most important step in the evaluation of object detection\nalgorithm is calculating the intersection-over-union (IoU) between the\npredicted bounding box and the ground truth one. Although this procedure is\nwell-defined and solved for planar images, it is not easy for spherical image\nobject detection. Existing methods either compute the IoUs based on biased\nbounding box representations or make excessive approximations, thus would give\nincorrect results. In this paper, we first identify that spherical rectangles\nare unbiased bounding boxes for objects in spherical images, and then propose\nan analytical method for IoU calculation without any approximations. Based on\nthe unbiased representation and calculation, we also present an anchor free\nobject detection algorithm for spherical images. The experiments on two\nspherical object detection datasets show that the proposed method can achieve\nbetter performance than existing methods.",
          "link": "http://arxiv.org/abs/2108.08029",
          "publishedOn": "2021-08-19T01:35:02.684Z",
          "wordCount": 603,
          "title": "Unbiased IoU for Spherical Image Object Detection. (arXiv:2108.08029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "In crowd counting, due to the problem of laborious labelling, it is perceived\nintractability of collecting a new large-scale dataset which has plentiful\nimages with large diversity in density, scene, etc. Thus, for learning a\ngeneral model, training with data from multiple different datasets might be a\nremedy and be of great value. In this paper, we resort to the multi-domain\njoint learning and propose a simple but effective Domain-specific Knowledge\nPropagating Network (DKPNet)1 for unbiasedly learning the knowledge from\nmultiple diverse data domains at the same time. It is mainly achieved by\nproposing the novel Variational Attention(VA) technique for explicitly modeling\nthe attention distributions for different domains. And as an extension to VA,\nIntrinsic Variational Attention(InVA) is proposed to handle the problems of\nover-lapped domains and sub-domains. Extensive experiments have been conducted\nto validate the superiority of our DKPNet over several popular datasets,\nincluding ShanghaiTech A/B, UCF-QNRF and NWPU.",
          "link": "http://arxiv.org/abs/2108.08023",
          "publishedOn": "2021-08-19T01:35:02.677Z",
          "wordCount": 601,
          "title": "Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting. (arXiv:2108.08023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Hui Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1\">Tianjia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>",
          "description": "Image generation has been heavily investigated in computer vision, where one\ncore research challenge is to generate images from arbitrarily complex\ndistributions with little supervision. Generative Adversarial Networks (GANs)\nas an implicit approach have achieved great successes in this direction and\ntherefore been employed widely. However, GANs are known to suffer from issues\nsuch as mode collapse, non-structured latent space, being unable to compute\nlikelihoods, etc. In this paper, we propose a new unsupervised non-parametric\nmethod named mixture of infinite conditional GANs or MIC-GANs, to tackle\nseveral GAN issues together, aiming for image generation with parsimonious\nprior knowledge. Through comprehensive evaluations across different datasets,\nwe show that MIC-GANs are effective in structuring the latent space and\navoiding mode collapse, and outperform state-of-the-art methods. MICGANs are\nadaptive, versatile, and robust. They offer a promising solution to several\nwell-known GAN issues. Code available: github.com/yinghdb/MICGANs.",
          "link": "http://arxiv.org/abs/2108.07975",
          "publishedOn": "2021-08-19T01:35:02.652Z",
          "wordCount": 587,
          "title": "Unsupervised Image Generation with Infinite Generative Adversarial Networks. (arXiv:2108.07975v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Keyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>",
          "description": "Depth estimation, as a necessary clue to convert 2D images into the 3D space,\nhas been applied in many machine vision areas. However, to achieve an entire\nsurrounding 360-degree geometric sensing, traditional stereo matching\nalgorithms for depth estimation are limited due to large noise, low accuracy,\nand strict requirements for multi-camera calibration. In this work, for a\nunified surrounding perception, we introduce panoramic images to obtain larger\nfield of view. We extend PADENet first appeared in our previous conference work\nfor outdoor scene understanding, to perform panoramic monocular depth\nestimation with a focus for indoor scenes. At the same time, we improve the\ntraining process of the neural network adapted to the characteristics of\npanoramic images. In addition, we fuse traditional stereo matching algorithm\nwith deep learning methods and further improve the accuracy of depth\npredictions. With a comprehensive variety of experiments, this research\ndemonstrates the effectiveness of our schemes aiming for indoor scene\nperception.",
          "link": "http://arxiv.org/abs/2108.08076",
          "publishedOn": "2021-08-19T01:35:02.631Z",
          "wordCount": 613,
          "title": "Panoramic Depth Estimation via Supervised and Unsupervised Learning in Indoor Scenes. (arXiv:2108.08076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Unsupervised Domain Adaptation (UDA) can transfer knowledge from labeled\nsource data to unlabeled target data of the same categories. However, UDA for\nfirst-person action recognition is an under-explored problem, with lack of\ndatasets and limited consideration of first-person video characteristics. This\npaper focuses on addressing this problem. Firstly, we propose two small-scale\nfirst-person video domain adaptation datasets: ADL$_{small}$ and GTEA-KITCHEN.\nSecondly, we introduce channel-temporal attention blocks to capture the\nchannel-wise and temporal-wise relationships and model their inter-dependencies\nimportant to first-person vision. Finally, we propose a Channel-Temporal\nAttention Network (CTAN) to integrate these blocks into existing architectures.\nCTAN outperforms baselines on the two proposed datasets and one existing\ndataset EPIC$_{cvpr20}$.",
          "link": "http://arxiv.org/abs/2108.07846",
          "publishedOn": "2021-08-19T01:35:02.625Z",
          "wordCount": 547,
          "title": "Channel-Temporal Attention for First-Person Video Domain Adaptation. (arXiv:2108.07846v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haibo Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "With the recent success of deep neural networks, remarkable progress has been\nachieved on face recognition. However, collecting large-scale real-world\ntraining data for face recognition has turned out to be challenging, especially\ndue to the label noise and privacy issues. Meanwhile, existing face recognition\ndatasets are usually collected from web images, lacking detailed annotations on\nattributes (e.g., pose and expression), so the influences of different\nattributes on face recognition have been poorly investigated. In this paper, we\naddress the above-mentioned issues in face recognition using synthetic face\nimages, i.e., SynFace. Specifically, we first explore the performance gap\nbetween recent state-of-the-art face recognition models trained with synthetic\nand real face images. We then analyze the underlying causes behind the\nperformance gap, e.g., the poor intra-class variations and the domain gap\nbetween synthetic and real face images. Inspired by this, we devise the SynFace\nwith identity mixup (IM) and domain mixup (DM) to mitigate the above\nperformance gap, demonstrating the great potentials of synthetic data for face\nrecognition. Furthermore, with the controllable face synthesis model, we can\neasily manage different factors of synthetic face generation, including pose,\nexpression, illumination, the number of identities, and samples per identity.\nTherefore, we also perform a systematically empirical analysis on synthetic\nface images to provide some insights on how to effectively utilize synthetic\ndata for face recognition.",
          "link": "http://arxiv.org/abs/2108.07960",
          "publishedOn": "2021-08-19T01:35:02.601Z",
          "wordCount": 662,
          "title": "SynFace: Face Recognition with Synthetic Data. (arXiv:2108.07960v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>",
          "description": "Majority models of remote sensing image changing detection can only get great\neffect in a specific resolution data set. With the purpose of improving change\ndetection effectiveness of the model in the multi-resolution data set, a\nweighted rich-scale inception coder network (WRICNet) is proposed in this\narticle, which can make a great fusion of shallow multi-scale features, and\ndeep multi-scale features. The weighted rich-scale inception module of the\nproposed can obtain shallow multi-scale features, the weighted rich-scale coder\nmodule can obtain deep multi-scale features. The weighted scale block assigns\nappropriate weights to features of different scales, which can strengthen\nexpressive ability of the edge of the changing area. The performance\nexperiments on the multi-resolution data set demonstrate that, compared to the\ncomparative methods, the proposed can further reduce the false alarm outside\nthe change area, and the missed alarm in the change area, besides, the edge of\nthe change area is more accurate. The ablation study of the proposed shows that\nthe training strategy, and improvements of this article can improve the\neffectiveness of change detection.",
          "link": "http://arxiv.org/abs/2108.07955",
          "publishedOn": "2021-08-19T01:35:02.589Z",
          "wordCount": 620,
          "title": "WRICNet:A Weighted Rich-scale Inception Coder Network for Multi-Resolution Remote Sensing Image Change Detection. (arXiv:2108.07955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zichen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiang Qiu</a>",
          "description": "Applying feature dependent network weights have been proved to be effective\nin many fields. However, in practice, restricted by the enormous size of model\nparameters and memory footprints, scalable and versatile dynamic convolutions\nwith per-pixel adapted filters are yet to be fully explored. In this paper, we\naddress this challenge by decomposing filters, adapted to each spatial\nposition, over dynamic filter atoms generated by a light-weight network from\nlocal features. Adaptive receptive fields can be supported by further\nrepresenting each filter atom over sets of pre-fixed multi-scale bases. As\nplug-and-play replacements to convolutional layers, the introduced adaptive\nconvolutions with per-pixel dynamic atoms enable explicit modeling of\nintra-image variance, while avoiding heavy computation, parameters, and memory\ncost. Our method preserves the appealing properties of conventional\nconvolutions as being translation-equivariant and parametrically efficient. We\npresent experiments to show that, the proposed method delivers comparable or\neven better performance across tasks, and are particularly effective on\nhandling tasks with significant intra-image variance.",
          "link": "http://arxiv.org/abs/2108.07895",
          "publishedOn": "2021-08-19T01:35:02.579Z",
          "wordCount": 593,
          "title": "Adaptive Convolutions with Per-pixel Dynamic Filter Atom. (arXiv:2108.07895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Advanced self-supervised visual representation learning methods rely on the\ninstance discrimination (ID) pretext task. We point out that the ID task has an\nimplicit semantic consistency (SC) assumption, which may not hold in\nunconstrained datasets. In this paper, we propose a novel contrastive mask\nprediction (CMP) task for visual representation learning and design a mask\ncontrast (MaskCo) framework to implement the idea. MaskCo contrasts\nregion-level features instead of view-level features, which makes it possible\nto identify the positive sample without any assumptions. To solve the domain\ngap between masked and unmasked features, we design a dedicated mask prediction\nhead in MaskCo. This module is shown to be the key to the success of the CMP.\nWe evaluated MaskCo on training datasets beyond ImageNet and compare its\nperformance with MoCo V2. Results show that MaskCo achieves comparable\nperformance with MoCo V2 using ImageNet training dataset, but demonstrates a\nstronger performance across a range of downstream tasks when COCO or Conceptual\nCaptions are used for training. MaskCo provides a promising alternative to the\nID-based methods for self-supervised learning in the wild.",
          "link": "http://arxiv.org/abs/2108.07954",
          "publishedOn": "2021-08-19T01:35:02.549Z",
          "wordCount": 621,
          "title": "Self-Supervised Visual Representations Learning by Contrastive Mask Prediction. (arXiv:2108.07954v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07978",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lynhoo Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>",
          "description": "Nowadays modern displays are capable to render video content with high\ndynamic range (HDR) and wide color gamut (WCG). However, most available\nresources are still in standard dynamic range (SDR). Therefore, there is an\nurgent demand to transform existing SDR-TV contents into their HDR-TV versions.\nIn this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the\nformation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step\nsolution pipeline including adaptive global color mapping, local enhancement\nand highlight generation. Moreover, the above analysis inspires us to present a\nlightweight network that utilizes global statistics as guidance to conduct\nimage-adaptive color mapping. In addition, we construct a dataset using HDR\nvideos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate\nthe results of SDRTV-to-HDRTV algorithms. Furthermore, our final results\nachieve state-of-the-art performance in quantitative comparisons and visual\nquality. The code and dataset are available at\nhttps://github.com/chxy95/HDRTVNet.",
          "link": "http://arxiv.org/abs/2108.07978",
          "publishedOn": "2021-08-19T01:35:02.488Z",
          "wordCount": 604,
          "title": "A New Journey from SDRTV to HDRTV. (arXiv:2108.07978v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_M/0/1/0/all/0/1\">Mehrnaz Fani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1\">David A. Clausi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>",
          "description": "Identifying players in sports videos by recognizing their jersey numbers is a\nchallenging task in computer vision. We have designed and implemented a\nmulti-task learning network for jersey number recognition. In order to train a\nnetwork to recognize jersey numbers, two output label representations are used\n(1) Holistic - considers the entire jersey number as one class, and (2)\nDigit-wise - considers the two digits in a jersey number as two separate\nclasses. The proposed network learns both holistic and digit-wise\nrepresentations through a multi-task loss function. We determine the optimal\nweights to be assigned to holistic and digit-wise losses through an ablation\nstudy. Experimental results demonstrate that the proposed multi-task learning\nnetwork performs better than the constituent holistic and digit-wise\nsingle-task learning networks.",
          "link": "http://arxiv.org/abs/2108.07848",
          "publishedOn": "2021-08-19T01:35:02.466Z",
          "wordCount": 576,
          "title": "Multi-task learning for jersey number recognition in Ice Hockey. (arXiv:2108.07848v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1\">Michael Fitzke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1\">Derick Whitley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1\">Wilson Yau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Fernando Rodrigues Jr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1\">Vladimir Fadeev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1\">Cindy Bacmeister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1\">Chris Carter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1\">Jeffrey Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1\">Mark Parkinson</a>",
          "description": "Background: Histopathology is an important modality for the diagnosis and\nmanagement of many diseases in modern healthcare, and plays a critical role in\ncancer care. Pathology samples can be large and require multi-site sampling,\nleading to upwards of 20 slides for a single tumor, and the human-expert tasks\nof site selection and and quantitative assessment of mitotic figures are time\nconsuming and subjective. Automating these tasks in the setting of a digital\npathology service presents significant opportunities to improve workflow\nefficiency and augment human experts in practice. Approach: Multiple\nstate-of-the-art deep learning techniques for histopathology image\nclassification and mitotic figure detection were used in the development of\nOncoPetNet. Additionally, model-free approaches were used to increase speed and\naccuracy. The robust and scalable inference engine leverages Pytorch's\nperformance optimizations as well as specifically developed speed up techniques\nin inference. Results: The proposed system, demonstrated significantly improved\nmitotic counting performance for 41 cancer cases across 14 cancer types\ncompared to human expert baselines. In 21.9% of cases use of OncoPetNet led to\nchange in tumor grading compared to human expert evaluation. In deployment, an\neffective 0.27 min/slide inference was achieved in a high throughput veterinary\ndiagnostic pathology service across 2 centers processing 3,323 digital whole\nslide images daily. Conclusion: This work represents the first successful\nautomated deployment of deep learning systems for real-time expert-level\nperformance on important histopathology tasks at scale in a high volume\nclinical practice. The resulting impact outlines important considerations for\nmodel development, deployment, clinical decision making, and informs best\npractices for implementation of deep learning systems in digital histopathology\npractices.",
          "link": "http://arxiv.org/abs/2108.07856",
          "publishedOn": "2021-08-19T01:35:02.445Z",
          "wordCount": 758,
          "title": "OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "The key challenge in learning dense correspondences lies in the lack of\nground-truth matches for real image pairs. While photometric consistency losses\nprovide unsupervised alternatives, they struggle with large appearance changes,\nwhich are ubiquitous in geometric and semantic matching tasks. Moreover,\nmethods relying on synthetic training pairs often suffer from poor\ngeneralisation to real data.\n\nWe propose Warp Consistency, an unsupervised learning objective for dense\ncorrespondence regression. Our objective is effective even in settings with\nlarge appearance and view-point changes. Given a pair of real images, we first\nconstruct an image triplet by applying a randomly sampled warp to one of the\noriginal images. We derive and analyze all flow-consistency constraints arising\nbetween the triplet. From our observations and empirical results, we design a\ngeneral unsupervised objective employing two of the derived constraints. We\nvalidate our warp consistency loss by training three recent dense\ncorrespondence networks for the geometric and semantic matching tasks. Our\napproach sets a new state-of-the-art on several challenging benchmarks,\nincluding MegaDepth, RobotCar and TSS. Code and models are at\ngithub.com/PruneTruong/DenseMatching.",
          "link": "http://arxiv.org/abs/2104.03308",
          "publishedOn": "2021-08-19T01:35:02.399Z",
          "wordCount": 669,
          "title": "Warp Consistency for Unsupervised Learning of Dense Correspondences. (arXiv:2104.03308v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "The unsupervised domain adaptation (UDA) has been widely adopted to alleviate\nthe data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is adapted for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vector that violates the poset constraints by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-08-19T01:35:02.392Z",
          "wordCount": 651,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code is available\nat https://github.com/blackfeather-wang/AdaFocus.",
          "link": "http://arxiv.org/abs/2105.03245",
          "publishedOn": "2021-08-19T01:35:02.383Z",
          "wordCount": 675,
          "title": "Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Predictor-based algorithms have achieved remarkable performance in the Neural\nArchitecture Search (NAS) tasks. However, these methods suffer from high\ncomputation costs, as training the performance predictor usually requires\ntraining and evaluating hundreds of architectures from scratch. Previous works\nalong this line mainly focus on reducing the number of architectures required\nto fit the predictor. In this work, we tackle this challenge from a different\nperspective - improve search efficiency by cutting down the computation budget\nof architecture training. We propose NOn-uniform Successive Halving (NOSH), a\nhierarchical scheduling algorithm that terminates the training of\nunderperforming architectures early to avoid wasting budget. To effectively\nleverage the non-uniform supervision signals produced by NOSH, we formulate\npredictor-based architecture search as learning to rank with pairwise\ncomparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x\nwhile achieving competitive or even better performance than previous\nstate-of-the-art predictor-based methods on various spaces and datasets.",
          "link": "http://arxiv.org/abs/2108.08019",
          "publishedOn": "2021-08-19T01:35:02.376Z",
          "wordCount": 609,
          "title": "RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stacker_L/0/1/0/all/0/1\">Lukas St&#xe4;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonarens_F/0/1/0/all/0/1\">Frank Bonarens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>",
          "description": "Deep neural networks have proven increasingly important for automotive scene\nunderstanding with new algorithms offering constant improvements of the\ndetection performance. However, there is little emphasis on experiences and\nneeds for deployment in embedded environments. We therefore perform a case\nstudy of the deployment of two representative object detection networks on an\nedge AI platform. In particular, we consider RetinaNet for image-based 2D\nobject detection and PointPillars for LiDAR-based 3D object detection. We\ndescribe the modifications necessary to convert the algorithms from a PyTorch\ntraining environment to the deployment environment taking into account the\navailable tools. We evaluate the runtime of the deployed DNN using two\ndifferent libraries, TensorRT and TorchScript. In our experiments, we observe\nslight advantages of TensorRT for convolutional layers and TorchScript for\nfully connected layers. We also study the trade-off between runtime and\nperformance, when selecting an optimized setup for deployment, and observe that\nquantization significantly reduces the runtime while having only little impact\non the detection performance.",
          "link": "http://arxiv.org/abs/2108.08166",
          "publishedOn": "2021-08-19T01:35:02.369Z",
          "wordCount": 628,
          "title": "Deployment of Deep Neural Networks for Object Detection on Edge AI Devices with Runtime Optimization. (arXiv:2108.08166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1\">Ufuk Efe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>",
          "description": "Deep learning-based image matching methods are improved significantly during\nthe recent years. Although these methods are reported to outperform the\nclassical techniques, the performance of the classical methods is not examined\nin detail. In this study, we compare classical and learning-based methods by\nemploying mutual nearest neighbor search with ratio test and optimizing the\nratio test threshold to achieve the best performance on two different\nperformance metrics. After a fair comparison, the experimental results on\nHPatches dataset reveal that the performance gap between classical and\nlearning-based methods is not that significant. Throughout the experiments, we\ndemonstrated that SuperGlue is the state-of-the-art technique for the image\nmatching problem on HPatches dataset. However, if a single parameter, namely\nratio test threshold, is carefully optimized, a well-known traditional method\nSIFT performs quite close to SuperGlue and even outperforms in terms of mean\nmatching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent\napproach, DFM, which only uses pre-trained VGG features as descriptors and\nratio test, is shown to outperform most of the well-trained learning-based\nmethods. Therefore, we conclude that the parameters of any classical method\nshould be analyzed carefully before comparing against a learning-based\ntechnique.",
          "link": "http://arxiv.org/abs/2108.08179",
          "publishedOn": "2021-08-19T01:35:02.364Z",
          "wordCount": 648,
          "title": "Effect of Parameter Optimization on Classical and Learning-based Image Matching Methods. (arXiv:2108.08179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>",
          "description": "A formal autism diagnosis is an inefficient and lengthy process. Families\noften have to wait years before receiving a diagnosis for their child; some may\nnot receive one at all due to this delay. One approach to this problem is to\nuse digital technologies to detect the presence of behaviors related to autism,\nwhich in aggregate may lead to remote and automated diagnostics. One of the\nstrongest indicators of autism is stimming, which is a set of repetitive,\nself-stimulatory behaviors such as hand flapping, headbanging, and spinning.\nUsing computer vision to detect hand flapping is especially difficult due to\nthe sparsity of public training data in this space and excessive shakiness and\nmotion in such data. Our work demonstrates a novel method that overcomes these\nissues: we use hand landmark detection over time as a feature representation\nwhich is then fed into a Long Short-Term Memory (LSTM) model. We achieve a\nvalidation accuracy and F1 Score of about 72% on detecting whether videos from\nthe Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our\nbest model also predicts accurately on external videos we recorded of ourselves\noutside of the dataset it was trained on. This model uses less than 26,000\nparameters, providing promise for fast deployment into ubiquitous and wearable\ndigital settings for a remote autism diagnosis.",
          "link": "http://arxiv.org/abs/2108.07917",
          "publishedOn": "2021-08-19T01:35:02.339Z",
          "wordCount": 645,
          "title": "Activity Recognition for Autism Diagnosis. (arXiv:2108.07917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1\">Sai Mattapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1\">Rishi Athavale</a>",
          "description": "Due to morphological similarity at the microscopic level, making an accurate\nand time-sensitive distinction between blood cells affected by Acute\nLymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage\nof machine learning architectures. However, three of the most common models,\nVGG, ResNet, and Inception, each come with their own set of flaws with room for\nimprovement which demands the need for a superior model. ALLNet, the proposed\nhybrid convolutional neural network architecture, consists of a combination of\nthe VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019\n(available here) contains 10,691 images of white blood cells which were used to\ntrain and test the models. 7,272 of the images in the dataset are of cells with\nALL and 3,419 of them are of healthy cells. Of the images, 60% were used to\ntrain the model, 20% were used for the cross-validation set, and 20% were used\nfor the test set. ALLNet outperformed the VGG, ResNet, and the Inception models\nacross the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,\na specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803\nin the cross-validation set. In the test set, ALLNet achieved an accuracy of\n92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of\n0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the\nclinical workspace can better treat the thousands of people suffering from ALL\nacross the world, many of whom are children.",
          "link": "http://arxiv.org/abs/2108.08195",
          "publishedOn": "2021-08-19T01:35:02.322Z",
          "wordCount": 711,
          "title": "ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07973",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dave_A/0/1/0/all/0/1\">Akshat Dave</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>",
          "description": "We introduce DeepIR, a new thermal image processing framework that combines\nphysically accurate sensor modeling with deep network-based image\nrepresentation. Our key enabling observations are that the images captured by\nthermal sensors can be factored into slowly changing, scene-independent sensor\nnon-uniformities (that can be accurately modeled using physics) and a\nscene-specific radiance flux (that is well-represented using a deep\nnetwork-based regularizer). DeepIR requires neither training data nor periodic\nground-truth calibration with a known black body target--making it well suited\nfor practical computer vision tasks. We demonstrate the power of going DeepIR\nby developing new denoising and super-resolution algorithms that exploit\nmultiple images of the scene captured with camera jitter. Simulated and real\ndata experiments demonstrate that DeepIR can perform high-quality\nnon-uniformity correction with as few as three images, achieving a 10dB PSNR\nimprovement over competing approaches.",
          "link": "http://arxiv.org/abs/2108.07973",
          "publishedOn": "2021-08-19T01:35:02.313Z",
          "wordCount": 592,
          "title": "Thermal Image Processing via Physics-Inspired Deep Networks. (arXiv:2108.07973v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Bingchen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Point completion refers to completing the missing geometries of an object\nfrom incomplete observations. Main-stream methods predict the missing shapes by\ndecoding a global feature learned from the input point cloud, which often leads\nto deficient results in preserving topology consistency and surface details. In\nthis work, we present ME-PCN, a point completion network that leverages\n`emptiness' in 3D shape space. Given a single depth scan, previous methods\noften encode the occupied partial shapes while ignoring the empty regions (e.g.\nholes) in depth maps. In contrast, we argue that these `emptiness' clues\nindicate shape boundaries that can be used to improve topology representation\nand detail granularity on surfaces. Specifically, our ME-PCN encodes both the\noccupied point cloud and the neighboring `empty points'. It estimates\ncoarse-grained but complete and reasonable surface points in the first stage,\nfollowed by a refinement stage to produce fine-grained surface details.\nComprehensive experiments verify that our ME-PCN presents better qualitative\nand quantitative performance against the state-of-the-art. Besides, we further\nprove that our `emptiness' design is lightweight and easy to embed in existing\nmethods, which shows consistent effectiveness in improving the CD and EMD\nscores.",
          "link": "http://arxiv.org/abs/2108.08187",
          "publishedOn": "2021-08-19T01:35:02.303Z",
          "wordCount": 630,
          "title": "ME-PCN: Point Completion Conditioned on Mask Emptiness. (arXiv:2108.08187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Saifeng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budagavi_M/0/1/0/all/0/1\">Madhukar Budagavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaohu Guo</a>",
          "description": "In this paper, we propose a talking face generation method that takes an\naudio signal as input and a short target video clip as reference, and\nsynthesizes a photo-realistic video of the target face with natural lip\nmotions, head poses, and eye blinks that are in-sync with the input audio\nsignal. We note that the synthetic face attributes include not only explicit\nones such as lip motions that have high correlations with speech, but also\nimplicit ones such as head poses and eye blinks that have only weak correlation\nwith the input audio. To model such complicated relationships among different\nface attributes with input audio, we propose a FACe Implicit Attribute Learning\nGenerative Adversarial Network (FACIAL-GAN), which integrates the\nphonetics-aware, context-aware, and identity-aware information to synthesize\nthe 3D face animation with realistic motions of lips, head poses, and eye\nblinks. Then, our Rendering-to-Video network takes the rendered face images and\nthe attention map of eye blinks as input to generate the photo-realistic output\nvideo frames. Experimental results and user studies show our method can\ngenerate realistic talking face videos with not only synchronized lip motions,\nbut also natural head movements and eye blinks, with better qualities than the\nresults of state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.07938",
          "publishedOn": "2021-08-19T01:35:02.276Z",
          "wordCount": 655,
          "title": "FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning. (arXiv:2108.07938v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1\">Brian K. S. Isaac-Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1\">Matt Poyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1\">Daniel Organisciak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due\nto both negligent and malicious use. For this reason, the automated detection\nand tracking of UAV is a fundamental task in aerial security systems. Common\ntechnologies for UAV detection include visible-band and thermal infrared\nimaging, radio frequency and radar. Recent advances in deep neural networks\n(DNNs) for image-based object detection open the possibility to use visual\ninformation for this detection and tracking task. Furthermore, these detection\narchitectures can be implemented as backbones for visual tracking systems,\nthereby enabling persistent tracking of UAV incursions. To date, no\ncomprehensive performance benchmark exists that applies DNNs to visible-band\nimagery for UAV detection and tracking. To this end, three datasets with varied\nenvironmental conditions for UAV detection and tracking, comprising a total of\n241 videos (331,486 images), are assessed using four detection architectures\nand three tracking frameworks. The best performing detector architecture\nobtains an mAP of 98.6% and the best performing tracking framework obtains a\nMOTA of 96.3%. Cross-modality evaluation is carried out between visible and\ninfrared spectrums, achieving a maximal 82.8% mAP on visible images when\ntraining in the infrared modality. These results provide the first public\nmulti-approach benchmark for state-of-the-art deep learning-based methods and\ngive insight into which detection and tracking architectures are effective in\nthe UAV domain.",
          "link": "http://arxiv.org/abs/2103.13933",
          "publishedOn": "2021-08-19T01:35:02.262Z",
          "wordCount": 721,
          "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.13216",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1\">George I Lambrou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n\nIn this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.",
          "link": "http://arxiv.org/abs/1904.13216",
          "publishedOn": "2021-08-19T01:35:02.254Z",
          "wordCount": 722,
          "title": "Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v5 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gongjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Deep learning-based 3D object detection has achieved unprecedented success\nwith the advent of large-scale autonomous driving datasets. However, drastic\nperformance degradation remains a critical challenge for cross-domain\ndeployment. In addition, existing 3D domain adaptive detection methods often\nassume prior access to the target domain annotations, which is rarely feasible\nin the real world. To address this challenge, we study a more realistic\nsetting, unsupervised 3D domain adaptive detection, which only utilizes source\ndomain annotations. 1) We first comprehensively investigate the major\nunderlying factors of the domain gap in 3D detection. Our key insight is that\ngeometric mismatch is the key factor of domain shift. 2) Then, we propose a\nnovel and unified framework, Multi-Level Consistency Network (MLC-Net), which\nemploys a teacher-student paradigm to generate adaptive and reliable\npseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level\nconsistency to facilitate cross-domain transfer. Extensive experiments\ndemonstrate that MLC-Net outperforms existing state-of-the-art methods\n(including those using additional target domain information) on standard\nbenchmarks. Notably, our approach is detector-agnostic, which achieves\nconsistent gains on both single- and two-stage 3D detectors.",
          "link": "http://arxiv.org/abs/2107.11355",
          "publishedOn": "2021-08-19T01:35:02.167Z",
          "wordCount": 654,
          "title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency. (arXiv:2107.11355v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:35:02.159Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.06803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>",
          "description": "Video data is with complex temporal dynamics due to various factors such as\ncamera motion, speed variation, and different activities. To effectively\ncapture this diverse motion pattern, this paper presents a new temporal\nadaptive module ({\\bf TAM}) to generate video-specific temporal kernels based\non its own feature map. TAM proposes a unique two-level adaptive modeling\nscheme by decoupling the dynamic kernel into a location sensitive importance\nmap and a location invariant aggregation weight. The importance map is learned\nin a local temporal window to capture short-term information, while the\naggregation weight is generated from a global view with a focus on long-term\nstructure. TAM is a modular block and could be integrated into 2D CNNs to yield\na powerful video architecture (TANet) with a very small extra computational\ncost. The extensive experiments on Kinetics-400 and Something-Something\ndatasets demonstrate that our TAM outperforms other temporal modeling methods\nconsistently, and achieves the state-of-the-art performance under the similar\ncomplexity. The code is available at \\url{\nhttps://github.com/liu-zhy/temporal-adaptive-module}.",
          "link": "http://arxiv.org/abs/2005.06803",
          "publishedOn": "2021-08-19T01:35:02.140Z",
          "wordCount": 651,
          "title": "TAM: Temporal Adaptive Module for Video Recognition. (arXiv:2005.06803v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>",
          "description": "Simulators can efficiently generate large amounts of labeled synthetic data\nwith perfect supervision for hard-to-label tasks like semantic segmentation.\nHowever, they introduce a domain gap that severely hurts real-world\nperformance. We propose to use self-supervised monocular depth estimation as a\nproxy task to bridge this gap and improve sim-to-real unsupervised domain\nadaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)\nlearns a domain-invariant representation via a multi-task objective combining\nsynthetic semantic supervision with real-world geometric constraints on videos.\nGUDA establishes a new state of the art in UDA for semantic segmentation on\nthree benchmarks, outperforming methods that use domain adversarial learning,\nself-training, or other self-supervised proxy tasks. Furthermore, we show that\nour method scales well with the quality and quantity of synthetic data while\nalso improving depth prediction.",
          "link": "http://arxiv.org/abs/2103.16694",
          "publishedOn": "2021-08-19T01:35:02.122Z",
          "wordCount": 596,
          "title": "Geometric Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2103.16694v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.02265",
          "publishedOn": "2021-08-19T01:35:02.115Z",
          "wordCount": 664,
          "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Kunming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>",
          "description": "Existing optical flow methods are erroneous in challenging scenes, such as\nfog, rain, and night because the basic optical flow assumptions such as\nbrightness and gradient constancy are broken. To address this problem, we\npresent an unsupervised learning approach that fuses gyroscope into optical\nflow learning. Specifically, we first convert gyroscope readings into motion\nfields named gyro field. Second, we design a self-guided fusion module to fuse\nthe background motion extracted from the gyro field with the optical flow and\nguide the network to focus on motion details. To the best of our knowledge,\nthis is the first deep learning-based framework that fuses gyroscope data and\nimage content for optical flow learning. To validate our method, we propose a\nnew dataset that covers regular and challenging scenes. Experiments show that\nour method outperforms the state-of-art methods in both regular and challenging\nscenes. Code and dataset are available at\nhttps://github.com/megvii-research/GyroFlow.",
          "link": "http://arxiv.org/abs/2103.13725",
          "publishedOn": "2021-08-19T01:35:02.108Z",
          "wordCount": 607,
          "title": "GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning. (arXiv:2103.13725v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yuecong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Aiming Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_X/0/1/0/all/0/1\">Xiujuan Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize\nunsegmented signs from image streams. Overfitting is one of the most critical\nproblems in CSLR training, and previous works show that the iterative training\nscheme can partially solve this problem while also costing more training time.\nIn this study, we revisit the iterative training scheme in recent CSLR works\nand realize that sufficient training of the feature extractor is critical to\nsolving the overfitting problem. Therefore, we propose a Visual Alignment\nConstraint (VAC) to enhance the feature extractor with alignment supervision.\nSpecifically, the proposed VAC comprises two auxiliary losses: one focuses on\nvisual features only, and the other enforces prediction alignment between the\nfeature extractor and the alignment module. Moreover, we propose two metrics to\nreflect overfitting by measuring the prediction inconsistency between the\nfeature extractor and the alignment module. Experimental results on two\nchallenging CSLR datasets show that the proposed VAC makes CSLR networks\nend-to-end trainable and achieves competitive performance.",
          "link": "http://arxiv.org/abs/2104.02330",
          "publishedOn": "2021-08-19T01:35:02.101Z",
          "wordCount": 638,
          "title": "Visual Alignment Constraint for Continuous Sign Language Recognition. (arXiv:2104.02330v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1812.02134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1\">Jose Oramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>",
          "description": "We address the problem of unpaired geometric image-to-image translation.\nRather than transferring the style of an image as a whole, our goal is to\ntranslate the geometry of an object as depicted in different domains while\npreserving its appearance characteristics. Our model is trained in an unpaired\nfashion, i.e. without the need of paired images during training. It performs\nall steps of the shape transfer within a single model and without additional\npost-processing stages. Extensive experiments on the VITON, CMU-Multi-PIE and\nour own FashionStyle datasets show the effectiveness of the method. In\naddition, we show that despite their low-dimensionality, the features learned\nby our model are useful to the item retrieval task.",
          "link": "http://arxiv.org/abs/1812.02134",
          "publishedOn": "2021-08-19T01:35:02.094Z",
          "wordCount": 617,
          "title": "An Unpaired Shape Transforming Method for Image Translation and Cross-Domain Retrieval. (arXiv:1812.02134v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Herman_J/0/1/0/all/0/1\">James Herman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skabelkin_A/0/1/0/all/0/1\">Alexey Skabelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1\">Ivan Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1\">Max Kumskoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>",
          "description": "Existing research on autonomous driving primarily focuses on urban driving,\nwhich is insufficient for characterising the complex driving behaviour\nunderlying high-speed racing. At the same time, existing racing simulation\nframeworks struggle in capturing realism, with respect to visual rendering,\nvehicular dynamics, and task objectives, inhibiting the transfer of learning\nagents to real-world contexts. We introduce a new environment, where agents\nLearn-to-Race (L2R) in simulated competition-style racing, using multimodal\ninformation--from virtual cameras to a comprehensive array of inertial\nmeasurement sensors. Our environment, which includes a simulator and an\ninterfacing training framework, accurately models vehicle dynamics and racing\nconditions. In this paper, we release the Arrival simulator for autonomous\nracing. Next, we propose the L2R task with challenging metrics, inspired by\nlearning-to-drive challenges, Formula-style racing, and multimodal trajectory\nprediction for autonomous driving. Additionally, we provide the L2R framework\nsuite, facilitating simulated racing on high-precision models of real-world\ntracks. Finally, we provide an official L2R task dataset of expert\ndemonstrations, as well as a series of baseline experiments and reference\nimplementations. We make all code available:\nhttps://github.com/learn-to-race/l2r.",
          "link": "http://arxiv.org/abs/2103.11575",
          "publishedOn": "2021-08-19T01:35:02.073Z",
          "wordCount": 670,
          "title": "Learn-to-Race: A Multimodal Control Environment for Autonomous Racing. (arXiv:2103.11575v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1\">Xiang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "During the COVID-19 coronavirus epidemic, almost everyone wears a facial\nmask, which poses a huge challenge to deep face recognition. In this workshop,\nwe organize Masked Face Recognition (MFR) challenge and focus on bench-marking\ndeep face recognition methods under the existence of facial masks. In the MFR\nchallenge, there are two main tracks: the InsightFace track and the WebFace260M\ntrack. For the InsightFace track, we manually collect a large-scale masked face\ntest set with 7K identities. In addition, we also collect a children test set\nincluding 14K identities and a multi-racial test set containing 242K\nidentities. By using these three test sets, we build up an online model testing\nsystem, which can give a comprehensive evaluation of face recognition models.\nTo avoid data privacy problems, no test image is released to the public. As the\nchallenge is still under-going, we will keep on updating the top-ranked\nsolutions as well as this report on the arxiv.",
          "link": "http://arxiv.org/abs/2108.08191",
          "publishedOn": "2021-08-19T01:35:02.055Z",
          "wordCount": 652,
          "title": "Masked Face Recognition Challenge: The InsightFace Track Report. (arXiv:2108.08191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Mohamed Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael Black</a>",
          "description": "A long-standing goal in computer vision is to capture, model, and\nrealistically synthesize human behavior. Specifically, by learning from data,\nour goal is to enable virtual humans to navigate within cluttered indoor scenes\nand naturally interact with objects. Such embodied behavior has applications in\nvirtual reality, computer games, and robotics, while synthesized behavior can\nbe used as a source of training data. This is challenging because real human\nmotion is diverse and adapts to the scene. For example, a person can sit or lie\non a sofa in many places and with varying styles. It is necessary to model this\ndiversity when synthesizing virtual humans that realistically perform\nhuman-scene interactions. We present a novel data-driven, stochastic motion\nsynthesis method that models different styles of performing a given action with\na target object. Our method, called SAMP, for Scene-Aware Motion Prediction,\ngeneralizes to target objects of various geometries while enabling the\ncharacter to navigate in cluttered scenes. To train our method, we collected\nMoCap data covering various sitting, lying down, walking, and running styles.\nWe demonstrate our method on complex indoor scenes and achieve superior\nperformance compared to existing solutions. Our code and data are available for\nresearch at https://samp.is.tue.mpg.de.",
          "link": "http://arxiv.org/abs/2108.08284",
          "publishedOn": "2021-08-19T01:35:02.048Z",
          "wordCount": 637,
          "title": "Stochastic Scene-Aware Motion Prediction. (arXiv:2108.08284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>",
          "description": "In this paper, we introduce a new framework for unsupervised deep homography\nestimation. Our contributions are 3 folds. First, unlike previous methods that\nregress 4 offsets for a homography, we propose a homography flow\nrepresentation, which can be estimated by a weighted sum of 8 pre-defined\nhomography flow bases. Second, considering a homography contains 8\nDegree-of-Freedoms (DOFs) that is much less than the rank of the network\nfeatures, we propose a Low Rank Representation (LRR) block that reduces the\nfeature rank, so that features corresponding to the dominant motions are\nretained while others are rejected. Last, we propose a Feature Identity Loss\n(FIL) to enforce the learned image feature warp-equivariant, meaning that the\nresult should be identical if the order of warp operation and feature\nextraction is swapped. With this constraint, the unsupervised optimization is\nachieved more effectively and more stable features are learned. Extensive\nexperiments are conducted to demonstrate the effectiveness of all the newly\nproposed components, and results show that our approach outperforms the\nstate-of-the-art on the homography benchmark datasets both qualitatively and\nquantitatively. Code is available at\nhttps://github.com/megvii-research/BasesHomo.",
          "link": "http://arxiv.org/abs/2103.15346",
          "publishedOn": "2021-08-19T01:35:02.011Z",
          "wordCount": 653,
          "title": "Motion Basis Learning for Unsupervised Deep Homography Estimation with Subspace Projection. (arXiv:2103.15346v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ynjiun Paul Wang</a>",
          "description": "Most of stereo vision works are focusing on computing the dense pixel\ndisparity of a given pair of left and right images. A camera pair usually\nrequired lens undistortion and stereo calibration to provide an undistorted\nepipolar line calibrated image pair for accurate dense pixel disparity\ncomputation. Due to noise, object occlusion, repetitive or lack of texture and\nlimitation of matching algorithms, the pixel disparity accuracy usually suffers\nthe most at those object boundary areas. Although statistically the total\nnumber of pixel disparity errors might be low (under 2% according to the Kitti\nVision Benchmark of current top ranking algorithms), the percentage of these\ndisparity errors at object boundaries are very high. This renders the\nsubsequence 3D object distance detection with much lower accuracy than desired.\nThis paper proposed a different approach for solving a 3D object distance\ndetection by detecting object disparity directly without going through a dense\npixel disparity computation. An example squeezenet Object Disparity-SSD\n(OD-SSD) was constructed to demonstrate an efficient object disparity detection\nwith comparable accuracy compared with Kitti dataset pixel disparity ground\ntruth. Further training and testing results with mixed image dataset captured\nby several different stereo systems may suggest that an OD-SSD might be\nagnostic to stereo system parameters such as a baseline, FOV, lens distortion,\neven left/right camera epipolar line misalignment.",
          "link": "http://arxiv.org/abs/2108.07939",
          "publishedOn": "2021-08-19T01:35:02.004Z",
          "wordCount": 649,
          "title": "Object Disparity. (arXiv:2108.07939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1\">Munan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenglang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Domain shift happens in cross-domain scenarios commonly because of the wide\ngaps between different domains: when applying a deep learning model\nwell-trained in one domain to another target domain, the model usually performs\npoorly. To tackle this problem, unsupervised domain adaptation (UDA) techniques\nare proposed to bridge the gap between different domains, for the purpose of\nimproving model performance without annotation in the target domain.\nParticularly, UDA has a great value for multimodal medical image analysis,\nwhere annotation difficulty is a practical concern. However, most existing UDA\nmethods can only achieve satisfactory improvements in one adaptation direction\n(e.g., MRI to CT), but often perform poorly in the other (CT to MRI), limiting\ntheir practical usage. In this paper, we propose a bidirectional UDA (BiUDA)\nframework based on disentangled representation learning for equally competent\ntwo-way UDA performances. This framework employs a unified domain-aware pattern\nencoder which not only can adaptively encode images in different domains\nthrough a domain controller, but also improve model efficiency by eliminating\nredundant parameters. Furthermore, to avoid distortion of contents and patterns\nof input images during the adaptation process, a content-pattern consistency\nloss is introduced. Additionally, for better UDA segmentation performance, a\nlabel consistency strategy is proposed to provide extra supervision by\nrecomposing target-domain-styled images and corresponding source-domain\nannotations. Comparison experiments and ablation studies conducted on two\npublic datasets demonstrate the superiority of our BiUDA framework to current\nstate-of-the-art UDA methods and the effectiveness of its novel designs. By\nsuccessfully addressing two-way adaptations, our BiUDA framework offers a\nflexible solution of UDA techniques to the real-world scenario.",
          "link": "http://arxiv.org/abs/2108.07979",
          "publishedOn": "2021-08-19T01:35:01.997Z",
          "wordCount": 714,
          "title": "A New Bidirectional Unsupervised Domain Adaptation Segmentation Framework. (arXiv:2108.07979v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shenhan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhi Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1\">YiHao Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "Co-speech gesture generation is to synthesize a gesture sequence that not\nonly looks real but also matches with the input speech audio. Our method\ngenerates the movements of a complete upper body, including arms, hands, and\nthe head. Although recent data-driven methods achieve great success, challenges\nstill exist, such as limited variety, poor fidelity, and lack of objective\nmetrics. Motivated by the fact that the speech cannot fully determine the\ngesture, we design a method that learns a set of gesture template vectors to\nmodel the latent conditions, which relieve the ambiguity. For our method, the\ntemplate vector determines the general appearance of a generated gesture\nsequence, while the speech audio drives subtle movements of the body, both\nindispensable for synthesizing a realistic gesture sequence. Due to the\nintractability of an objective metric for gesture-speech synchronization, we\nadopt the lip-sync error as a proxy metric to tune and evaluate the\nsynchronization ability of our model. Extensive experiments show the\nsuperiority of our method in both objective and subjective evaluations on\nfidelity and synchronization.",
          "link": "http://arxiv.org/abs/2108.08020",
          "publishedOn": "2021-08-19T01:35:01.981Z",
          "wordCount": 617,
          "title": "Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates. (arXiv:2108.08020v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yidan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Mingsheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>",
          "description": "Convolution on 3D point clouds that generalized from 2D grid-like domains is\nwidely researched yet far from perfect. The standard convolution characterises\nfeature correspondences indistinguishably among 3D points, presenting an\nintrinsic limitation of poor distinctive feature learning. In this paper, we\npropose Adaptive Graph Convolution (AdaptConv) which generates adaptive kernels\nfor points according to their dynamically learned features. Compared with using\na fixed/isotropic kernel, AdaptConv improves the flexibility of point cloud\nconvolutions, effectively and precisely capturing the diverse relations between\npoints from different semantic parts. Unlike popular attentional weight\nschemes, the proposed AdaptConv implements the adaptiveness inside the\nconvolution operation instead of simply assigning different weights to the\nneighboring points. Extensive qualitative and quantitative evaluations show\nthat our method outperforms state-of-the-art point cloud classification and\nsegmentation approaches on several benchmark datasets. Our code is available at\nhttps://github.com/hrzhou2/AdaptConv-master.",
          "link": "http://arxiv.org/abs/2108.08035",
          "publishedOn": "2021-08-19T01:35:01.962Z",
          "wordCount": 585,
          "title": "Adaptive Graph Convolution for Point Cloud Analysis. (arXiv:2108.08035v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Amirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1\">Matthew Kowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1\">Neil D. B. Bruce</a>",
          "description": "In this paper, we challenge the common assumption that collapsing the spatial\ndimensions of a 3D (spatial-channel) tensor in a convolutional neural network\n(CNN) into a vector via global pooling removes all spatial information.\nSpecifically, we demonstrate that positional information is encoded based on\nthe ordering of the channel dimensions, while semantic information is largely\nnot. Following this demonstration, we show the real world impact of these\nfindings by applying them to two applications. First, we propose a simple yet\neffective data augmentation strategy and loss function which improves the\ntranslation invariance of a CNN's output. Second, we propose a method to\nefficiently determine which channels in the latent representation are\nresponsible for (i) encoding overall position information or (ii)\nregion-specific positions. We first show that semantic segmentation has a\nsignificant reliance on the overall position channels to make predictions. We\nthen show for the first time that it is possible to perform a `region-specific'\nattack, and degrade a network's performance in a particular part of the input.\nWe believe our findings and demonstrated applications will benefit research\nareas concerned with understanding the characteristics of CNNs.",
          "link": "http://arxiv.org/abs/2108.07884",
          "publishedOn": "2021-08-19T01:35:01.953Z",
          "wordCount": 644,
          "title": "Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs. (arXiv:2108.07884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kawamata_R/0/1/0/all/0/1\">Ryota Kawamata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Betsui_K/0/1/0/all/0/1\">Keiichi Betsui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kazuyoshi Yamazaki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakakibara_R/0/1/0/all/0/1\">Rei Sakakibara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shimano_T/0/1/0/all/0/1\">Takeshi Shimano</a>",
          "description": "Compact and low-cost devices are needed for autonomous driving to image and\nmeasure distances to objects 360-degree around. We have been developing an\nomnidirectional stereo camera exploiting two hyperbolic mirrors and a single\nset of a lens and sensor, which makes this camera compact and cost efficient.\nWe establish a new calibration method for this camera considering higher-order\nradial distortion, detailed tangential distortion, an image sensor tilt, and a\nlens-mirror offset. Our method reduces the calibration error by 6.0 and 4.3\ntimes for the upper- and lower-view images, respectively. The random error of\nthe distance measurement is 4.9% and the systematic error is 5.7% up to objects\n14 meters apart, which is improved almost nine times compared to the\nconventional method. The remaining distance errors is due to a degraded optical\nresolution of the prototype, which we plan to make further improvements as\nfuture work.",
          "link": "http://arxiv.org/abs/2108.07936",
          "publishedOn": "2021-08-19T01:35:01.893Z",
          "wordCount": 607,
          "title": "Calibration Method of the Monocular Omnidirectional Stereo Camera. (arXiv:2108.07936v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_e_Silva_A/0/1/0/all/0/1\">Andr&#xe9; Luiz Buarque Vieira-e-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1\">Heitor de Castro Felix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_T/0/1/0/all/0/1\">Thiago de Menezes Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1\">Francisco Paulo Magalh&#xe3;es Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1\">Veronica Teichrieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Michel Mozinho dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1\">Hemir da Cunha Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1\">Virginia Ad&#xe9;lia Cordeiro Sgotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1\">Henrique Baptista Duffles Teixeira Lott Neto</a>",
          "description": "Many power line companies are using UAVs to perform their inspection\nprocesses instead of putting their workers at risk by making them climb high\nvoltage power line towers, for instance. A crucial task for the inspection is\nto detect and classify assets in the power transmission lines. However, public\ndata related to power line assets are scarce, preventing a faster evolution of\nthis area. This work proposes the Power Line Assets Dataset, containing\nhigh-resolution and real-world images of multiple high-voltage power line\ncomponents. It has 2,409 annotated objects divided into five classes:\ntransmission tower, insulator, spacer, tower plate, and Stockbridge damper,\nwhich vary in size (resolution), orientation, illumination, angulation, and\nbackground. This work also presents an evaluation with popular deep object\ndetection methods, showing considerable room for improvement. The PLAD dataset\nis publicly available at https://github.com/andreluizbvs/PLAD.",
          "link": "http://arxiv.org/abs/2108.07944",
          "publishedOn": "2021-08-19T01:35:01.885Z",
          "wordCount": 621,
          "title": "PLAD: A Dataset for Multi-Size Power Line Assets Detection in High-Resolution UAV Images. (arXiv:2108.07944v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1\">Ivan Welaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1\">Adil Dahlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1\">Ronan T Hearne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1\">Misgina Tsighe Hagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "This work employs a pre-trained, multi-view Convolutional Neural Network\n(CNN) with a spatial attention block to optimise knee injury detection. An\nopen-source Magnetic Resonance Imaging (MRI) data set with image-level labels\nwas leveraged for this analysis. As MRI data is acquired from three planes, we\ncompare our technique using data from a single-plane and multiple planes\n(multi-plane). For multi-plane, we investigate various methods of fusing the\nplanes in the network. This analysis resulted in the novel 'MPFuseNet' network\nand state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior\nCruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977\nand 0.957 respectively. We then developed an objective metric, Penalised\nLocalisation Accuracy (PLA), to validate the model's localisation ability. This\nmetric compares binary masks generated from Grad-Cam output and the\nradiologist's annotations on a sample of MRIs. We also extracted explainability\nfeatures in a model-agnostic approach that were then verified as clinically\nrelevant by the radiologist.",
          "link": "http://arxiv.org/abs/2108.08136",
          "publishedOn": "2021-08-19T01:35:01.861Z",
          "wordCount": 623,
          "title": "Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>",
          "description": "We present ARCH++, an image-based method to reconstruct 3D avatars with\narbitrary clothing styles. Our reconstructed avatars are animation-ready and\nhighly realistic, in both the visible regions from input views and the unseen\nregions. While prior work shows great promise of reconstructing animatable\nclothed humans with various topologies, we observe that there exist fundamental\nlimitations resulting in sub-optimal reconstruction quality. In this paper, we\nrevisit the major steps of image-based avatar reconstruction and address the\nlimitations with ARCH++. First, we introduce an end-to-end point based geometry\nencoder to better describe the semantics of the underlying 3D human body, in\nreplacement of previous hand-crafted features. Second, in order to address the\noccupancy ambiguity caused by topological changes of clothed humans in the\ncanonical pose, we propose a co-supervising framework with cross-space\nconsistency to jointly estimate the occupancy in both the posed and canonical\nspaces. Last, we use image-to-image translation networks to further refine\ndetailed geometry and texture on the reconstructed surface, which improves the\nfidelity and consistency across arbitrary viewpoints. In the experiments, we\ndemonstrate improvements over the state of the art on both public benchmarks\nand user studies in reconstruction quality and realism.",
          "link": "http://arxiv.org/abs/2108.07845",
          "publishedOn": "2021-08-19T01:35:01.772Z",
          "wordCount": 635,
          "title": "ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiajun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "The DeepFakes, which are the facial manipulation techniques, is the emerging\nthreat to digital society. Various DeepFake detection methods and datasets are\nproposed for detecting such data, especially for face-swapping. However, recent\nresearches less consider facial animation, which is also important in the\nDeepFake attack side. It tries to animate a face image with actions provided by\na driving video, which also leads to a concern about the security of recent\npayment systems that reply on liveness detection to authenticate real users via\nrecognising a sequence of user facial actions. However, our experiments show\nthat the existed datasets are not sufficient to develop reliable detection\nmethods. While the current liveness detector cannot defend such videos as the\nattack. As a response, we propose a new human face animation dataset, called\nDeepFake MNIST+, generated by a SOTA image animation generator. It includes\n10,000 facial animation videos in ten different actions, which can spoof the\nrecent liveness detectors. A baseline detection method and a comprehensive\nanalysis of the method is also included in this paper. In addition, we analyze\nthe proposed dataset's properties and reveal the difficulty and importance of\ndetecting animation datasets under different types of motion and compression\nquality.",
          "link": "http://arxiv.org/abs/2108.07949",
          "publishedOn": "2021-08-19T01:35:01.734Z",
          "wordCount": 641,
          "title": "DeepFake MNIST+: A DeepFake Facial Animation Dataset. (arXiv:2108.07949v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marques_B/0/1/0/all/0/1\">Bruno Augusto Dorta Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clua_E/0/1/0/all/0/1\">Esteban Walter Gonzalez Clua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_A/0/1/0/all/0/1\">Anselmo Antunes Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Nader Vasconcelos</a>",
          "description": "The representation of consistent mixed reality (XR) environments requires\nadequate real and virtual illumination composition in real-time. Estimating the\nlighting of a real scenario is still a challenge. Due to the ill-posed nature\nof the problem, classical inverse-rendering techniques tackle the problem for\nsimple lighting setups. However, those assumptions do not satisfy the current\nstate-of-art in computer graphics and XR applications. While many recent works\nsolve the problem using machine learning techniques to estimate the environment\nlight and scene's materials, most of them are limited to geometry or previous\nknowledge. This paper presents a CNN-based model to estimate complex lighting\nfor mixed reality environments with no previous information about the scene. We\nmodel the environment illumination using a set of spherical harmonics (SH)\nenvironment lighting, capable of efficiently represent area lighting. We\npropose a new CNN architecture that inputs an RGB image and recognizes, in\nreal-time, the environment lighting. Unlike previous CNN-based lighting\nestimation methods, we propose using a highly optimized deep neural network\narchitecture, with a reduced number of parameters, that can learn high complex\nlighting scenarios from real-world high-dynamic-range (HDR) environment images.\nWe show in the experiments that the CNN architecture can predict the\nenvironment lighting with an average mean squared error (MSE) of \\num{7.85e-04}\nwhen comparing SH lighting coefficients. We validate our model in a variety of\nmixed reality scenarios. Furthermore, we present qualitative results comparing\nrelights of real-world scenes.",
          "link": "http://arxiv.org/abs/2108.07903",
          "publishedOn": "2021-08-19T01:35:01.702Z",
          "wordCount": 689,
          "title": "Spatially and color consistent environment lighting estimation using deep neural networks for mixed reality. (arXiv:2108.07903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07966",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yucheng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_Y/0/1/0/all/0/1\">Yi Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin C. Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>",
          "description": "Lensless cameras provide a framework to build thin imaging systems by\nreplacing the lens in a conventional camera with an amplitude or phase mask\nnear the sensor. Existing methods for lensless imaging can recover the depth\nand intensity of the scene, but they require solving computationally-expensive\ninverse problems. Furthermore, existing methods struggle to recover dense\nscenes with large depth variations. In this paper, we propose a lensless\nimaging system that captures a small number of measurements using different\npatterns on a programmable mask. In this context, we make three contributions.\nFirst, we present a fast recovery algorithm to recover textures on a fixed\nnumber of depth planes in the scene. Second, we consider the mask design\nproblem, for programmable lensless cameras, and provide a design template for\noptimizing the mask patterns with the goal of improving depth estimation.\nThird, we use a refinement network as a post-processing step to identify and\nremove artifacts in the reconstruction. These modifications are evaluated\nextensively with experimental results on a lensless camera prototype to\nshowcase the performance benefits of the optimized masks and recovery\nalgorithms over the state of the art.",
          "link": "http://arxiv.org/abs/2108.07966",
          "publishedOn": "2021-08-19T01:35:01.695Z",
          "wordCount": 656,
          "title": "A Simple Framework for 3D Lensless Imaging with Programmable Masks. (arXiv:2108.07966v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07948",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Heliang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "An important scenario for image quality assessment (IQA) is to evaluate image\nrestoration (IR) algorithms. The state-of-the-art approaches adopt a\nfull-reference paradigm that compares restored images with their corresponding\npristine-quality images. However, pristine-quality images are usually\nunavailable in blind image restoration tasks and real-world scenarios. In this\npaper, we propose a practical solution named degraded-reference IQA (DR-IQA),\nwhich exploits the inputs of IR models, degraded images, as references.\nSpecifically, we extract reference information from degraded images by\ndistilling knowledge from pristine-quality images. The distillation is achieved\nthrough learning a reference space, where various degraded images are\nencouraged to share the same feature statistics with pristine-quality images.\nAnd the reference space is optimized to capture deep image priors that are\nuseful for quality assessment. Note that pristine-quality images are only used\nduring training. Our work provides a powerful and differentiable metric for\nblind IRs, especially for GAN-based methods. Extensive experiments show that\nour results can even be close to the performance of full-reference settings.",
          "link": "http://arxiv.org/abs/2108.07948",
          "publishedOn": "2021-08-19T01:35:01.662Z",
          "wordCount": 611,
          "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment. (arXiv:2108.07948v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>",
          "description": "Deep face recognition (FR) has achieved significantly high accuracy on\nseveral challenging datasets and fosters successful real-world applications,\neven showing high robustness to the illumination variation that is usually\nregarded as a main threat to the FR system. However, in the real world,\nillumination variation caused by diverse lighting conditions cannot be fully\ncovered by the limited face dataset. In this paper, we study the threat of\nlighting against FR from a new angle, i.e., adversarial attack, and identify a\nnew task, i.e., adversarial relighting. Given a face image, adversarial\nrelighting aims to produce a naturally relighted counterpart while fooling the\nstate-of-the-art deep FR methods. To this end, we first propose the physical\nmodel-based adversarial relighting attack (ARA) denoted as\nalbedo-quotient-based adversarial relighting attack (AQ-ARA). It generates\nnatural adversarial light under the physical lighting model and guidance of FR\nsystems and synthesizes adversarially relighted face images. Moreover, we\npropose the auto-predictive adversarial relighting attack (AP-ARA) by training\nan adversarial relighting network (ARNet) to automatically predict the\nadversarial light in a one-step manner according to different input faces,\nallowing efficiency-sensitive applications. More importantly, we propose to\ntransfer the above digital attacks to physical ARA (Phy-ARA) through a precise\nrelighting device, making the estimated adversarial lighting condition\nreproducible in the real world. We validate our methods on three\nstate-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two\npublic datasets. The extensive and insightful results demonstrate our work can\ngenerate realistic adversarial relighted face images fooling FR easily,\nrevealing the threat of specific light directions and strengths.",
          "link": "http://arxiv.org/abs/2108.07920",
          "publishedOn": "2021-08-19T01:35:01.601Z",
          "wordCount": 690,
          "title": "Adversarial Relighting against Face Recognition. (arXiv:2108.07920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Automated systems that detect the social behavior of deception can enhance\nhuman well-being across medical, social work, and legal domains. Labeled\ndatasets to train supervised deception detection models can rarely be collected\nfor real-world, high-stakes contexts. To address this challenge, we propose the\nfirst unsupervised approach for detecting real-world, high-stakes deception in\nvideos without requiring labels. This paper presents our novel approach for\naffect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative\nrepresentations of deceptive and truthful behavior. Drawing on psychology\ntheories that link affect and deception, we experimented with unimodal and\nmultimodal DBN-based approaches trained on facial valence, facial arousal,\naudio, and visual features. In addition to using facial affect as a feature on\nwhich DBN models are trained, we also introduce a DBN training procedure that\nuses facial affect as an aligner of audio-visual representations. We conducted\nclassification experiments with unsupervised Gaussian Mixture Model clustering\nto evaluate our approaches. Our best unsupervised approach (trained on facial\nvalence and visual features) achieved an AUC of 80%, outperforming human\nability and performing comparably to fully-supervised models. Our results\nmotivate future work on unsupervised, affect-aware computational approaches for\ndetecting deception and other social behaviors in the wild.",
          "link": "http://arxiv.org/abs/2108.07897",
          "publishedOn": "2021-08-19T01:35:01.554Z",
          "wordCount": 637,
          "title": "Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>",
          "description": "Existing salient object detection (SOD) models mainly rely on CNN-based\nU-shaped structures with skip connections to combine the global contexts and\nlocal spatial details that are crucial for locating salient objects and\nrefining object details, respectively. Despite great successes, the ability of\nCNN in learning global contexts is limited. Recently, the vision transformer\nhas achieved revolutionary progress in computer vision owing to its powerful\nmodeling of global dependencies. However, directly applying the transformer to\nSOD is obviously suboptimal because the transformer lacks the ability to learn\nlocal spatial representations. To this end, this paper explores the combination\nof transformer and CNN to learn both global and local representations for SOD.\nWe propose a transformer-based Asymmetric Bilateral U-Net (AbiU-Net). The\nasymmetric bilateral encoder has a transformer path and a lightweight CNN path,\nwhere the two paths communicate at each encoder stage to learn complementary\nglobal contexts and local spatial details, respectively. The asymmetric\nbilateral decoder also consists of two paths to process features from the\ntransformer and CNN encoder paths, with communication at each decoder stage for\ndecoding coarse salient object locations and find-grained object details,\nrespectively. Such communication between the two encoder/decoder paths enables\nAbiU-Net to learn complementary global and local representations, taking\nadvantage of the natural properties of transformer and CNN, respectively.\nHence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive\nexperiments demonstrate that ABiU-Net performs favorably against previous\nstate-of-the-art SOD methods. The code will be released.",
          "link": "http://arxiv.org/abs/2108.07851",
          "publishedOn": "2021-08-19T01:35:01.476Z",
          "wordCount": 676,
          "title": "Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11001",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_B/0/1/0/all/0/1\">Bhavya Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>",
          "description": "Scene understanding under low-light conditions is a challenging problem. This\nis due to the small number of photons captured by the camera and the resulting\nlow signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging\nsensing modality that are capable of capturing images with high sensitivity.\nDespite having minimal read-noise, images captured by SPCs in photon-starved\nconditions still suffer from strong shot noise, preventing reliable scene\ninference. We propose photon scale-space a collection of high-SNR images\nspanning a wide range of photons-per-pixel (PPP) levels (but same scene\ncontent) as guides to train inference model on low photon flux images. We\ndevelop training techniques that push images with different illumination levels\ncloser to each other in feature representation space. The key idea is that\nhaving a spectrum of different brightness levels during training enables\neffective guidance, and increases robustness to shot noise even in extreme\nnoise cases. Based on the proposed approach, we demonstrate, via simulations\nand real experiments with a SPAD camera, high-performance on various inference\ntasks such as image classification and monocular depth estimation under ultra\nlow-light, down to < 1 PPP.",
          "link": "http://arxiv.org/abs/2107.11001",
          "publishedOn": "2021-08-18T01:55:02.599Z",
          "wordCount": 658,
          "title": "Photon-Starved Scene Inference using Single Photon Cameras. (arXiv:2107.11001v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>",
          "description": "Label distributions in real-world are oftentimes long-tailed and imbalanced,\nresulting in biased models towards dominant labels. While long-tailed\nrecognition has been extensively studied for image classification tasks,\nlimited effort has been made for video domain. In this paper, we introduce\nVideoLT, a large-scale long-tailed video recognition dataset, as a step toward\nreal-world video recognition. Our VideoLT contains 256,218 untrimmed videos,\nannotated into 1,004 classes with a long-tailed distribution. Through extensive\nstudies, we demonstrate that state-of-the-art methods used for long-tailed\nimage recognition do not perform well in the video domain due to the additional\ntemporal dimension in video data. This motivates us to propose FrameStack, a\nsimple yet effective method for long-tailed video recognition task. In\nparticular, FrameStack performs sampling at the frame-level in order to balance\nclass distributions, and the sampling ratio is dynamically determined using\nknowledge derived from the network during training. Experimental results\ndemonstrate that FrameStack can improve classification performance without\nsacrificing overall accuracy. Code and dataset are available at:\nhttps://github.com/17Skye17/VideoLT.",
          "link": "http://arxiv.org/abs/2105.02668",
          "publishedOn": "2021-08-18T01:55:02.592Z",
          "wordCount": 638,
          "title": "VideoLT: Large-scale Long-tailed Video Recognition. (arXiv:2105.02668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifeng Shi</a>",
          "description": "Long-range and short-range temporal modeling are two complementary and\ncrucial aspects of video recognition. Most of the state-of-the-arts focus on\nshort-range spatio-temporal modeling and then average multiple snippet-level\npredictions to yield the final video-level prediction. Thus, their video-level\nprediction does not consider spatio-temporal features of how video evolves\nalong the temporal dimension. In this paper, we introduce a novel Dynamic\nSegment Aggregation (DSA) module to capture relationship among snippets. To be\nmore specific, we attempt to generate a dynamic kernel for a convolutional\noperation to aggregate long-range temporal information among adjacent snippets\nadaptively. The DSA module is an efficient plug-and-play module and can be\ncombined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform\npowerful long-range modeling with minimal overhead. The final video\narchitecture, coined as DSANet. We conduct extensive experiments on several\nvideo recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400,\nSomething-Something V1 and ActivityNet) to show its superiority. Our proposed\nDSA module is shown to benefit various video recognition models significantly.\nFor example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is\nimproved from 74.9% to 78.2% on Kinetics-400. Codes are available at\nhttps://github.com/whwu95/DSANet.",
          "link": "http://arxiv.org/abs/2105.12085",
          "publishedOn": "2021-08-18T01:55:02.586Z",
          "wordCount": 684,
          "title": "DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning. (arXiv:2105.12085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>",
          "description": "The convolutional neural network (CNN) is vulnerable to degraded images with\neven very small variations (e.g. corrupted and adversarial samples). One of the\npossible reasons is that CNN pays more attention to the most discriminative\nregions, but ignores the auxiliary features when learning, leading to the lack\nof feature diversity for final judgment. In our method, we propose to\ndynamically suppress significant activation values of CNN by group-wise\ninhibition, but not fixedly or randomly handle them when training. The feature\nmaps with different activation distribution are then processed separately to\ntake the feature independence into account. CNN is finally guided to learn\nricher discriminative features hierarchically for robust classification\naccording to the proposed regularization. Our method is comprehensively\nevaluated under multiple settings, including classification against\ncorruptions, adversarial attacks and low data regime. Extensive experimental\nresults show that the proposed method can achieve significant improvements in\nterms of both robustness and generalization performances, when compared with\nthe state-of-the-art methods. Code is available at\nhttps://github.com/LinusWu/TENET_Training.",
          "link": "http://arxiv.org/abs/2103.02152",
          "publishedOn": "2021-08-18T01:55:02.577Z",
          "wordCount": 649,
          "title": "Group-wise Inhibition based Feature Regularization for Robust Classification. (arXiv:2103.02152v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_I/0/1/0/all/0/1\">Ishwarya Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenakshisundaram_N/0/1/0/all/0/1\">Nishaali Meenakshisundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_I/0/1/0/all/0/1\">Ishwarya Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+D_S/0/1/0/all/0/1\">Shiloah Elizabeth D</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1\">Sunil Retmin Raj C</a>",
          "description": "Vision plays a crucial role to comprehend the world around us as more than\n85% of the external information is obtained through the vision system. It\nlargely influences our mobility, cognition, information access, and interaction\nwith the environment as well as with other people. Blindness prevents a person\nfrom gaining knowledge of the surrounding environment and makes unassisted\nnavigation, object recognition, obstacle avoidance, and reading tasks major\nchallenges. Many existing systems are often limited by cost and complexity. To\nhelp the visually challenged overcome these difficulties faced in everyday\nlife, we propose the idea of VisBuddy, a smart assistant which will help the\nvisually challenged with their day-to-day activities. VisBuddy is a voice-based\nassistant, where the user can give voice commands to perform specific tasks.\nVisBuddy uses the techniques of image captioning for describing the user's\nsurroundings, optical character recognition (OCR) for reading the text in the\nuser's view, object detection to search and find the objects in a room and web\nscraping to give the user the latest news. VisBuddy has been built by combining\nthe concepts from Deep Learning and the Internet of Things. Thus, VisBuddy\nserves as a cost-efficient, powerful and all-in-one assistant for the visually\nchallenged by helping them with their day-to-day activities.",
          "link": "http://arxiv.org/abs/2108.07761",
          "publishedOn": "2021-08-18T01:55:02.555Z",
          "wordCount": 655,
          "title": "VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frederic Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We address the problem of detecting human-object interactions in images using\ngraphical neural networks. Unlike conventional methods, where nodes send scaled\nbut otherwise identical messages to each of their neighbours, we propose to\ncondition messages between pairs of nodes on their spatial relationships,\nresulting in different messages going to neighbours of the same node. To this\nend, we explore various ways of applying spatial conditioning under a\nmulti-branch structure. Through extensive experimentation we demonstrate the\nadvantages of spatial conditioning for the computation of the adjacency\nstructure, messages and the refined graph features. In particular, we\nempirically show that as the quality of the bounding boxes increases, their\ncoarse appearance features contribute relatively less to the disambiguation of\ninteractions compared to the spatial information. Our method achieves an mAP of\n31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming\nstate-of-the-art on fine-tuned detections.",
          "link": "http://arxiv.org/abs/2012.06060",
          "publishedOn": "2021-08-18T01:55:02.548Z",
          "wordCount": 624,
          "title": "Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10833",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>",
          "description": "Though many attempts have been made in blind super-resolution to restore\nlow-resolution images with unknown and complex degradations, they are still far\nfrom addressing general real-world degraded images. In this work, we extend the\npowerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),\nwhich is trained with pure synthetic data. Specifically, a high-order\ndegradation modeling process is introduced to better simulate complex\nreal-world degradations. We also consider the common ringing and overshoot\nartifacts in the synthesis process. In addition, we employ a U-Net\ndiscriminator with spectral normalization to increase discriminator capability\nand stabilize the training dynamics. Extensive comparisons have shown its\nsuperior visual performance than prior works on various real datasets. We also\nprovide efficient implementations to synthesize training pairs on the fly.",
          "link": "http://arxiv.org/abs/2107.10833",
          "publishedOn": "2021-08-18T01:55:02.542Z",
          "wordCount": 600,
          "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data. (arXiv:2107.10833v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Conventional face super-resolution methods usually assume testing\nlow-resolution (LR) images lie in the same domain as the training ones. Due to\ndifferent lighting conditions and imaging hardware, domain gaps between\ntraining and testing images inevitably occur in many real-world scenarios.\nNeglecting those domain gaps would lead to inferior face super-resolution (FSR)\nperformance. However, how to transfer a trained FSR model to a target domain\nefficiently and effectively has not been investigated. To tackle this problem,\nwe develop a Domain-Aware Pyramid-based Face Super-Resolution network, named\nDAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces\nfrom a target domain by exploiting only a pair of high-resolution (HR) and LR\nexemplar in the target domain. To be specific, our DAP-FSR firstly employs its\nencoder to extract the multi-scale latent representations of the input LR face.\nConsidering only one target domain example is available, we propose to augment\nthe target domain data by mixing the latent representations of the target\ndomain face and source domain ones, and then feed the mixed representations to\nthe decoder of our DAP-FSR. The decoder will generate new face images\nresembling the target domain image style. The generated HR faces in turn are\nused to optimize our decoder to reduce the domain gap. By iteratively updating\nthe latent representations and our decoder, our DAP-FSR will be adapted to the\ntarget domain, thus achieving authentic and high-quality upsampled HR faces.\nExtensive experiments on three newly constructed benchmarks validate the\neffectiveness and superior performance of our DAP-FSR compared to the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2103.08863",
          "publishedOn": "2021-08-18T01:55:02.534Z",
          "wordCount": 719,
          "title": "Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar. (arXiv:2103.08863v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-08-18T01:55:02.516Z",
          "wordCount": 702,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on local surrogate model-based method to show which components contribute\nto the classification. Moreover, we propose quantitative fidelity validations\nfor generated explanations that enhance the persuasive power of explainability\nand compare the plausibility of different existing point cloud-applicable\nexplainability methods. Our new explainability approach provides a fairly\naccurate, more semantically coherent and widely applicable explanation for\npoint cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-08-18T01:55:02.510Z",
          "wordCount": 597,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1\">Aurelien Lucchi</a>",
          "description": "Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning. We release our code at\nhttps://github.com/dariopavllo/textured-3d-gan",
          "link": "http://arxiv.org/abs/2103.15627",
          "publishedOn": "2021-08-18T01:55:02.503Z",
          "wordCount": 642,
          "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jaehui Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "The video-based action recognition task has been extensively studied in\nrecent years. In this paper, we study the structural vulnerability of deep\nlearning-based action recognition models against the adversarial attack using\nthe one frame attack that adds an inconspicuous perturbation to only a single\nframe of a given video clip. Our analysis shows that the models are highly\nvulnerable against the one frame attack due to their structural properties.\nExperiments demonstrate high fooling rates and inconspicuous characteristics of\nthe attack. Furthermore, we show that strong universal one frame perturbations\ncan be obtained under various scenarios. Our work raises the serious issue of\nadversarial vulnerability of the state-of-the-art action recognition models in\nvarious perspectives.",
          "link": "http://arxiv.org/abs/2011.14585",
          "publishedOn": "2021-08-18T01:55:02.497Z",
          "wordCount": 596,
          "title": "Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.12028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalanced learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones, e.g., our ResNet-200\nachieves 81.8% top-1 accuracy. Our code is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.",
          "link": "http://arxiv.org/abs/2107.12028",
          "publishedOn": "2021-08-18T01:55:02.491Z",
          "wordCount": 600,
          "title": "Parametric Contrastive Learning. (arXiv:2107.12028v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the test (validation)\nset of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence\nand 0.492 (0.649) for arousal, which significantly outperforms the baseline\nmethod with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for\nvalence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-08-18T01:55:02.485Z",
          "wordCount": 678,
          "title": "Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion. (arXiv:2107.01175v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03035",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xinghua Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kuanquan Wang</a>",
          "description": "Coronary artery disease (CAD) has posed a leading threat to the lives of\ncardiovascular disease patients worldwide for a long time. Therefore, automated\ndiagnosis of CAD has indispensable significance in clinical medicine. However,\nthe complexity of coronary artery plaques that cause CAD makes the automatic\ndetection of coronary artery stenosis in Coronary CT angiography (CCTA) a\ndifficult task. In this paper, we propose a Transformer network (TR-Net) for\nthe automatic detection of significant stenosis (i.e. luminal narrowing > 50%)\nwhile practically completing the computer-assisted diagnosis of CAD. The\nproposed TR-Net introduces a novel Transformer, and tightly combines\nconvolutional layers and Transformer encoders, allowing their advantages to be\ndemonstrated in the task. By analyzing semantic information sequences, TR-Net\ncan fully understand the relationship between image information in each\nposition of a multiplanar reformatted (MPR) image, and accurately detect\nsignificant stenosis based on both local and global information. We evaluate\nour TR-Net on a dataset of 76 patients from different patients annotated by\nexperienced radiologists. Experimental results illustrate that our TR-Net has\nachieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and\nMCC (0.74) indicators compared with the state-of-the-art methods. The source\ncode is publicly available from the link (https://github.com/XinghuaMa/TR-Net).",
          "link": "http://arxiv.org/abs/2107.03035",
          "publishedOn": "2021-08-18T01:55:02.478Z",
          "wordCount": 676,
          "title": "Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries. (arXiv:2107.03035v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-08-18T01:55:02.472Z",
          "wordCount": 671,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1\">Arij Bouazizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiederer_J/0/1/0/all/0/1\">Julian Wiederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1\">Ulrich Kressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "We present a self-supervised learning algorithm for 3D human pose estimation\nof a single person based on a multiple-view camera system and 2D body pose\nestimates for each view. To train our model, represented by a deep neural\nnetwork, we propose a four-loss function learning algorithm, which does not\nrequire any 2D or 3D body pose ground-truth. The proposed loss functions make\nuse of the multiple-view geometry to reconstruct 3D body pose estimates and\nimpose body pose constraints across the camera views. Our approach utilizes all\navailable camera views during training, while the inference is single-view. In\nour evaluations, we show promising performance on Human3.6M and HumanEva\nbenchmarks, while we also present a generalization study on MPI-INF-3DHP\ndataset, as well as several ablation results. Overall, we outperform all\nself-supervised learning methods and reach comparable results to supervised and\nweakly-supervised learning approaches. Our code and models are publicly\navailable",
          "link": "http://arxiv.org/abs/2108.07777",
          "publishedOn": "2021-08-18T01:55:02.452Z",
          "wordCount": 591,
          "title": "Self-Supervised 3D Human Pose Estimation with Multiple-View Geometry. (arXiv:2108.07777v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Current state-of-the-art image captioning models adopt autoregressive\ndecoders, \\ie they generate each word by conditioning on previously generated\nwords, which leads to heavy latency during inference. To tackle this issue,\nnon-autoregressive image captioning models have recently been proposed to\nsignificantly accelerate the speed of inference by generating all words in\nparallel. However, these non-autoregressive models inevitably suffer from large\ngeneration quality degradation since they remove words dependence excessively.\nTo make a better trade-off between speed and quality, we introduce a\nsemi-autoregressive model for image captioning~(dubbed as SATIC), which keeps\nthe autoregressive property in global but generates words parallelly in local .\nBased on Transformer, there are only a few modifications needed to implement\nSATIC. Experimental results on the MSCOCO image captioning benchmark show that\nSATIC can achieve a good trade-off without bells and whistles. Code is\navailable at {\\color{magenta}\\url{https://github.com/YuanEZhou/satic}}.",
          "link": "http://arxiv.org/abs/2106.09436",
          "publishedOn": "2021-08-18T01:55:02.427Z",
          "wordCount": 601,
          "title": "Semi-Autoregressive Transformer for Image Captioning. (arXiv:2106.09436v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1\">Junjie yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.",
          "link": "http://arxiv.org/abs/2107.02960",
          "publishedOn": "2021-08-18T01:55:02.407Z",
          "wordCount": 677,
          "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>",
          "description": "Style transfer aims to reproduce content images with the styles from\nreference images. Existing universal style transfer methods successfully\ndeliver arbitrary styles to original images either in an artistic or a\nphoto-realistic way. However, the range of 'arbitrary style' defined by\nexisting works is bounded in the particular domain due to their structural\nlimitation. Specifically, the degrees of content preservation and stylization\nare established according to a predefined target domain. As a result, both\nphoto-realistic and artistic models have difficulty in performing the desired\nstyle transfer for the other domain. To overcome this limitation, we propose a\nunified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer\nnot only the style but also the property of domain (i.e., domainness) from a\ngiven reference image. To this end, we design a novel domainness indicator that\ncaptures the domainness value from the texture and structural features of\nreference images. Moreover, we introduce a unified framework with domain-aware\nskip connection to adaptively transfer the stroke and palette to the input\ncontents guided by the domainness indicator. Our extensive experiments validate\nthat our model produces better qualitative results and outperforms previous\nmethods in terms of proxy metrics on both artistic and photo-realistic\nstylizations.",
          "link": "http://arxiv.org/abs/2108.04441",
          "publishedOn": "2021-08-18T01:55:02.390Z",
          "wordCount": 666,
          "title": "Domain-Aware Universal Style Transfer. (arXiv:2108.04441v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alwassel_H/0/1/0/all/0/1\">Humam Alwassel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Due to the large memory footprint of untrimmed videos, current\nstate-of-the-art video localization methods operate atop precomputed video clip\nfeatures. These features are extracted from video encoders typically trained\nfor trimmed action classification tasks, making such features not necessarily\nsuitable for temporal localization. In this work, we propose a novel supervised\npretraining paradigm for clip features that not only trains to classify\nactivities but also considers background clips and global video information to\nimprove temporal sensitivity. Extensive experiments show that using features\ntrained with our novel pretraining strategy significantly improves the\nperformance of recent state-of-the-art methods on three tasks: Temporal Action\nLocalization, Action Proposal Generation, and Dense Video Captioning. We also\nshow that our pretraining approach is effective across three encoder\narchitectures and two pretraining datasets. We believe video feature encoding\nis an important building block for localization algorithms, and extracting\ntemporally-sensitive features should be of paramount importance in building\nmore accurate models. The code and pretrained models are available on our\nproject website.",
          "link": "http://arxiv.org/abs/2011.11479",
          "publishedOn": "2021-08-18T01:55:02.384Z",
          "wordCount": 644,
          "title": "TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks. (arXiv:2011.11479v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Lingwei Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yongwei Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guiqing Li</a>",
          "description": "Human motion prediction is a challenging task due to the stochasticity and\naperiodicity of future poses. Recently, graph convolutional network has been\nproven to be very effective to learn dynamic relations among pose joints, which\nis helpful for pose prediction. On the other hand, one can abstract a human\npose recursively to obtain a set of poses at multiple scales. With the increase\nof the abstraction level, the motion of the pose becomes more stable, which\nbenefits pose prediction too. In this paper, we propose a novel Multi-Scale\nResidual Graph Convolution Network (MSR-GCN) for human pose prediction task in\nthe manner of end-to-end. The GCNs are used to extract features from fine to\ncoarse scale and then from coarse to fine scale. The extracted features at each\nscale are then combined and decoded to obtain the residuals between the input\nand target poses. Intermediate supervisions are imposed on all the predicted\nposes, which enforces the network to learn more representative features. Our\nproposed approach is evaluated on two standard benchmark datasets, i.e., the\nHuman3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate\nthat our method outperforms the state-of-the-art approaches. Code and\npre-trained models are available at https://github.com/Droliven/MSRGCN.",
          "link": "http://arxiv.org/abs/2108.07152",
          "publishedOn": "2021-08-18T01:55:02.377Z",
          "wordCount": 672,
          "title": "MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction. (arXiv:2108.07152v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:55:02.356Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model at various scales\nand has linear computational complexity with respect to image size. These\nqualities of Swin Transformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and\ndense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP\non COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its\nperformance surpasses the previous state-of-the-art by a large margin of +2.7\nbox AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the\npotential of Transformer-based models as vision backbones. The hierarchical\ndesign and the shifted window approach also prove beneficial for all-MLP\narchitectures. The code and models are publicly available\nat~\\url{https://github.com/microsoft/Swin-Transformer}.",
          "link": "http://arxiv.org/abs/2103.14030",
          "publishedOn": "2021-08-18T01:55:02.345Z",
          "wordCount": 701,
          "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chun-Han Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "Federated learning methods enable us to train machine learning models on\ndistributed user data while preserving its privacy. However, it is not always\nfeasible to obtain high-quality supervisory signals from users, especially for\nvision tasks. Unlike typical federated settings with labeled client data, we\nconsider a more practical scenario where the distributed client data is\nunlabeled, and a centralized labeled dataset is available on the server. We\nfurther take the server-client and inter-client domain shifts into account and\npose a domain adaptation problem with one source (centralized server data) and\nmultiple targets (distributed client data). Within this new Federated\nMulti-Target Domain Adaptation (FMTDA) task, we analyze the model performance\nof exiting domain adaptation methods and propose an effective DualAdapt method\nto address the new challenges. Extensive experimental results on image\nclassification and semantic segmentation tasks demonstrate that our method\nachieves high accuracy, incurs minimal communication cost, and requires low\ncomputational resources on client devices.",
          "link": "http://arxiv.org/abs/2108.07792",
          "publishedOn": "2021-08-18T01:55:02.330Z",
          "wordCount": 585,
          "title": "Federated Multi-Target Domain Adaptation. (arXiv:2108.07792v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ping_G/0/1/0/all/0/1\">Guiju Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esfahani_M/0/1/0/all/0/1\">Mahdi Abolfazli Esfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>",
          "description": "Solving the challenging problem of 3D object reconstruction from a single\nimage appropriately gives existing technologies the ability to perform with a\nsingle monocular camera rather than requiring depth sensors. In recent years,\nthanks to the development of deep learning, 3D reconstruction of a single image\nhas demonstrated impressive progress. Existing researches use Chamfer distance\nas a loss function to guide the training of the neural network. However, the\nChamfer loss will give equal weights to all points inside the 3D point clouds.\nIt tends to sacrifice fine-grained and thin structures to avoid incurring a\nhigh loss, which will lead to visually unsatisfactory results. This paper\nproposes a framework that can recover a detailed three-dimensional point cloud\nfrom a single image by focusing more on boundaries (edge and corner points).\nExperimental results demonstrate that the proposed method outperforms existing\ntechniques significantly, both qualitatively and quantitatively, and has fewer\ntraining parameters.",
          "link": "http://arxiv.org/abs/2108.07685",
          "publishedOn": "2021-08-18T01:55:02.322Z",
          "wordCount": 596,
          "title": "Visual Enhanced 3D Point Cloud Reconstruction from A Single Image. (arXiv:2108.07685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrasiyabi_A/0/1/0/all/0/1\">Arman Afrasiyabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagn&#xe9;</a>",
          "description": "We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a\nrich and robust feature representation in the context of few-shot image\nclassification. Previous works have proposed to model each base class either\nwith a single point or with a mixture model by relying on offline clustering\nalgorithms. In contrast, we propose to model base classes with mixture models\nby simultaneously training the feature extractor and learning the mixture model\nparameters in an online manner. This results in a richer and more\ndiscriminative feature space which can be employed to classify novel examples\nfrom very few samples. Two main stages are proposed to train the MixtFSL model.\nFirst, the multimodal mixtures for each base class and the feature extractor\nparameters are learned using a combination of two loss functions. Second, the\nresulting network and mixture models are progressively refined through a\nleader-follower learning procedure, which uses the current estimate as a\n\"target\" network. This target network is used to make a consistent assignment\nof instances to mixture components, which increases performance and stabilizes\ntraining. The effectiveness of our end-to-end feature space learning approach\nis demonstrated with extensive experiments on four standard datasets and four\nbackbones. Notably, we demonstrate that when we combine our robust\nrepresentation with recent alignment-based approaches, we achieve new\nstate-of-the-art results in the inductive setting, with an absolute accuracy\nfor 5-shot classification of 82.45 on miniImageNet, 88.20 with tieredImageNet,\nand 60.70 in FC100 using the ResNet-12 backbone.",
          "link": "http://arxiv.org/abs/2011.11872",
          "publishedOn": "2021-08-18T01:55:02.315Z",
          "wordCount": 701,
          "title": "Mixture-based Feature Space Learning for Few-shot Image Classification. (arXiv:2011.11872v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "Dense video captioning aims to generate multiple associated captions with\ntheir temporal locations from the video. Previous methods follow a\nsophisticated \"localize-then-describe\" scheme, which heavily relies on numerous\nhand-crafted components. In this paper, we proposed a simple yet effective\nframework for end-to-end dense video captioning with parallel decoding (PDVC),\nby formulating the dense caption generation as a set prediction task. In\npractice, through stacking a newly proposed event counter on the top of a\ntransformer decoder, the PDVC precisely segments the video into a number of\nevent pieces under the holistic understanding of the video content, which\neffectively increases the coherence and readability of predicted captions.\nCompared with prior arts, the PDVC has several appealing advantages: (1)\nWithout relying on heuristic non-maximum suppression or a recurrent event\nsequence selection network to remove redundancy, PDVC directly produces an\nevent set with an appropriate size; (2) In contrast to adopting the two-stage\nscheme, we feed the enhanced representations of event queries into the\nlocalization head and caption head in parallel, making these two sub-tasks\ndeeply interrelated and mutually promoted through the optimization; (3) Without\nbells and whistles, extensive experiments on ActivityNet Captions and YouCook2\nshow that PDVC is capable of producing high-quality captioning results,\nsurpassing the state-of-the-art two-stage methods when its localization\naccuracy is on par with them. Code is available at\nhttps://github.com/ttengwang/PDVC.",
          "link": "http://arxiv.org/abs/2108.07781",
          "publishedOn": "2021-08-18T01:55:02.308Z",
          "wordCount": 673,
          "title": "End-to-End Dense Video Captioning with Parallel Decoding. (arXiv:2108.07781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03255",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delbracio_M/0/1/0/all/0/1\">Mauricio Delbracio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "Recent work has shown impressive results on data-driven defocus deblurring\nusing the two-image views available on modern dual-pixel (DP) sensors. One\nsignificant challenge in this line of research is access to DP data. Despite\nmany cameras having DP sensors, only a limited number provide access to the\nlow-level DP sensor images. In addition, capturing training data for defocus\ndeblurring involves a time-consuming and tedious setup requiring the camera's\naperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do\nnot have adjustable apertures, further limiting the ability to produce the\nnecessary training data. We address the data capture bottleneck by proposing a\nprocedure to generate realistic DP data synthetically. Our synthesis approach\nmimics the optical image formation found on DP sensors and can be applied to\nvirtual scenes rendered with standard computer software. Leveraging these\nrealistic synthetic DP images, we introduce a recurrent convolutional network\n(RCN) architecture that improves deblurring results and is suitable for use\nwith single-frame and multi-frame data (e.g., video) captured by DP sensors.\nFinally, we show that our synthetic DP data is useful for training DNN models\ntargeting video deblurring applications where access to DP data remains\nchallenging.",
          "link": "http://arxiv.org/abs/2012.03255",
          "publishedOn": "2021-08-18T01:55:02.289Z",
          "wordCount": 662,
          "title": "Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data. (arXiv:2012.03255v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We propose a novel framework for video inpainting by adopting an internal\nlearning strategy. Unlike previous methods that use optical flow for\ncross-frame context propagation to inpaint unknown regions, we show that this\ncan be achieved implicitly by fitting a convolutional neural network to known\nregions. Moreover, to handle challenging sequences with ambiguous backgrounds\nor long-term occlusion, we design two regularization terms to preserve\nhigh-frequency details and long-term temporal consistency. Extensive\nexperiments on the DAVIS dataset demonstrate that the proposed method achieves\nstate-of-the-art inpainting quality quantitatively and qualitatively. We\nfurther extend the proposed method to another challenging task: learning to\nremove an object from a video giving a single object mask in only one frame in\na 4K video.",
          "link": "http://arxiv.org/abs/2108.01912",
          "publishedOn": "2021-08-18T01:55:02.283Z",
          "wordCount": 580,
          "title": "Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wittich_D/0/1/0/all/0/1\">Dennis Wittich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottensteiner_F/0/1/0/all/0/1\">Franz Rottensteiner</a>",
          "description": "This paper addresses domain adaptation for the pixel-wise classification of\nremotely sensed data using deep neural networks (DNN) as a strategy to reduce\nthe requirements of DNN with respect to the availability of training data. We\nfocus on the setting in which labelled data are only available in a source\ndomain DS, but not in a target domain DT. Our method is based on adversarial\ntraining of an appearance adaptation network (AAN) that transforms images from\nDS such that they look like images from DT. Together with the original label\nmaps from DS, the transformed images are used to adapt a DNN to DT. We propose\na joint training strategy of the AAN and the classifier, which constrains the\nAAN to transform the images such that they are correctly classified. In this\nway, objects of a certain class are changed such that they resemble objects of\nthe same class in DT. To further improve the adaptation performance, we propose\na new regularization loss for the discriminator network used in domain\nadversarial training. We also address the problem of finding the optimal values\nof the trained network parameters, proposing an unsupervised entropy based\nparameter selection criterion which compensates for the fact that there is no\nvalidation set in DT that could be monitored. As a minor contribution, we\npresent a new weighting strategy for the cross-entropy loss, addressing the\nproblem of imbalanced class distributions. Our method is evaluated in 42\nadaptation scenarios using datasets from 7 cities, all consisting of\nhigh-resolution digital orthophotos and height data. It achieves a positive\ntransfer in all cases, and on average it improves the performance in the target\ndomain by 4.3% in overall accuracy. In adaptation scenarios between datasets\nfrom the ISPRS semantic labelling benchmark our method outperforms those from\nrecent publications by 10-20% with respect to the mean intersection over union.",
          "link": "http://arxiv.org/abs/2108.07779",
          "publishedOn": "2021-08-18T01:55:02.276Z",
          "wordCount": 744,
          "title": "Appearance Based Deep Domain Adaptation for the Classification of Aerial Images. (arXiv:2108.07779v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xiaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1\">Junwei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Depth completion aims to recover a dense depth map from a sparse depth map\nwith the corresponding color image as input. Recent approaches mainly formulate\ndepth completion as a one-stage end-to-end learning task, which outputs dense\ndepth maps directly. However, the feature extraction and supervision in\none-stage frameworks are insufficient, limiting the performance of these\napproaches. To address this problem, we propose a novel end-to-end residual\nlearning framework, which formulates the depth completion as a two-stage\nlearning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage.\nFirst, a coarse dense depth map is obtained by a simple CNN framework. Then, a\nrefined depth map is further obtained using a residual learning strategy in the\ncoarse-to-fine stage with a coarse depth map and color image as input.\nSpecially, in the coarse-to-fine stage, a channel shuffle extraction operation\nis utilized to extract more representative features from the color image and\ncoarse depth map, and an energy based fusion operation is exploited to\neffectively fuse these features obtained by channel shuffle operation, thus\nleading to more accurate and refined depth maps. We achieve SoTA performance in\nRMSE on KITTI benchmark. Extensive experiments on other datasets future\ndemonstrate the superiority of our approach over current state-of-the-art depth\ncompletion approaches.",
          "link": "http://arxiv.org/abs/2012.08270",
          "publishedOn": "2021-08-18T01:55:02.269Z",
          "wordCount": 694,
          "title": "FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Depth Completion. (arXiv:2012.08270v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1\">Frederik Hagelskjaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1\">Anders Glent Buch</a>",
          "description": "Since the introduction of modern deep learning methods for object pose\nestimation, test accuracy and efficiency has increased significantly. For\ntraining, however, large amounts of annotated training data are required for\ngood performance. While the use of synthetic training data prevents the need\nfor manual annotation, there is currently a large performance gap between\nmethods trained on real and synthetic data. This paper introduces a new method,\nwhich bridges this gap.\n\nMost methods trained on synthetic data use 2D images, as domain randomization\nin 2D is more developed. To obtain precise poses, many of these methods perform\na final refinement using 3D data. Our method integrates the 3D data into the\nnetwork to increase the accuracy of the pose estimation. To allow for domain\nrandomization in 3D, a sensor-based data augmentation has been developed.\nAdditionally, we introduce the SparseEdge feature, which uses a wider search\nspace during point cloud propagation to avoid relying on specific features\nwithout increasing run-time.\n\nExperiments on three large pose estimation benchmarks show that the presented\nmethod outperforms previous methods trained on synthetic data and achieves\ncomparable results to existing methods trained on real data.",
          "link": "http://arxiv.org/abs/2011.08517",
          "publishedOn": "2021-08-18T01:55:02.262Z",
          "wordCount": 673,
          "title": "Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization. (arXiv:2011.08517v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timothee Masquelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have attracted enormous research interest due\nto temporal information processing capability, low power consumption, and high\nbiological plausibility. However, the formulation of efficient and\nhigh-performance learning algorithms for SNNs is still challenging. Most\nexisting learning methods learn weights only, and require manual tuning of the\nmembrane-related parameters that determine the dynamics of a single spiking\nneuron. These parameters are typically chosen to be the same for all neurons,\nwhich limits the diversity of neurons and thus the expressiveness of the\nresulting SNNs. In this paper, we take inspiration from the observation that\nmembrane-related parameters are different across brain regions, and propose a\ntraining algorithm that is capable of learning not only the synaptic weights\nbut also the membrane time constants of SNNs. We show that incorporating\nlearnable membrane time constants can make the network less sensitive to\ninitial values and can speed up learning. In addition, we reevaluate the\npooling methods in SNNs and find that max-pooling will not lead to significant\ninformation loss and have the advantage of low computation cost and binary\ncompatibility. We evaluate the proposed method for image classification tasks\non both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and\nneuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment\nresults show that the proposed method outperforms the state-of-the-art accuracy\non nearly all datasets, using fewer time-steps. Our codes are available at\nhttps://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
          "link": "http://arxiv.org/abs/2007.05785",
          "publishedOn": "2021-08-18T01:55:02.243Z",
          "wordCount": 750,
          "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we present a conceptually simple, strong, and efficient\nframework for fully- and weakly-supervised panoptic segmentation, called\nPanoptic FCN. Our approach aims to represent and predict foreground things and\nbackground stuff in a unified fully convolutional pipeline, which can be\noptimized with point-based fully or weak supervision. In particular, Panoptic\nFCN encodes each object instance or stuff category with the proposed kernel\ngenerator and produces the prediction by convolving the high-resolution feature\ndirectly. With this approach, instance-aware and semantically consistent\nproperties for things and stuff can be respectively satisfied in a simple\ngenerate-kernel-then-segment workflow. Without extra boxes for localization or\ninstance separation, the proposed approach outperforms the previous box-based\nand -free models with high efficiency. Furthermore, we propose a new form of\npoint-based annotation for weakly-supervised panoptic segmentation. It only\nneeds several random points for both things and stuff, which dramatically\nreduces the annotation cost of human. The proposed Panoptic FCN is also proved\nto have much superior performance in this weakly-supervised setting, which\nachieves 82% of the fully-supervised performance with only 20 randomly\nannotated points per instance. Extensive experiments demonstrate the\neffectiveness and efficiency of Panoptic FCN on COCO, VOC 2012, Cityscapes, and\nMapillary Vistas datasets. And it sets up a new leading benchmark for both\nfully- and weakly-supervised panoptic segmentation. Our code and models are\nmade publicly available at https://github.com/dvlab-research/PanopticFCN",
          "link": "http://arxiv.org/abs/2108.07682",
          "publishedOn": "2021-08-18T01:55:02.236Z",
          "wordCount": 683,
          "title": "Fully Convolutional Networks for Panoptic Segmentation with Point-based Supervision. (arXiv:2108.07682v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunlong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianqi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yufan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fuhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1\">Jie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>",
          "description": "Chromosome classification is an important but difficult and tedious task in\nkaryotyping. Previous methods only classify manually segmented single\nchromosome, which is far from clinical practice. In this work, we propose a\ndetection based method, DeepACC, to locate and fine classify chromosomes\nsimultaneously based on the whole metaphase image. We firstly introduce the\nAdditive Angular Margin Loss to enhance the discriminative power of model. To\nalleviate batch effects, we transform decision boundary of each class\ncase-by-case through a siamese network which make full use of prior knowledges\nthat chromosomes usually appear in pairs. Furthermore, we take the clinically\nseven group criterion as a prior knowledge and design an additional Group\nInner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase\nimages from clinical laboratory are collected and labelled to evaluate the\nperformance. Results show that the new design brings encouraging performance\ngains comparing to the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2006.15528",
          "publishedOn": "2021-08-18T01:55:02.227Z",
          "wordCount": 692,
          "title": "DeepACC:Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge. (arXiv:2006.15528v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jiwan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>",
          "description": "The natural association between visual observations and their corresponding\nsound provides powerful self-supervisory signals for learning video\nrepresentations, which makes the ever-growing amount of online videos an\nattractive source of training data. However, large portions of online videos\ncontain irrelevant audio-visual signals because of edited/overdubbed audio, and\nmodels trained on such uncurated videos have shown to learn suboptimal\nrepresentations. Therefore, existing approaches rely almost exclusively on\ndatasets with predetermined taxonomies of semantic concepts, where there is a\nhigh chance of audio-visual correspondence. Unfortunately, constructing such\ndatasets require labor intensive manual annotation and/or verification, which\nseverely limits the utility of online videos for large-scale learning. In this\nwork, we present an automatic dataset curation approach based on subset\noptimization where the objective is to maximize the mutual information between\naudio and visual channels in videos. We demonstrate that our approach finds\nvideos with high audio-visual correspondence and show that self-supervised\nmodels trained on our data achieve competitive performances compared to models\ntrained on existing manually curated datasets. The most significant benefit of\nour approach is scalability: We release ACAV100M that contains 100 million\nvideos with high audio-visual correspondence, ideal for self-supervised video\nrepresentation learning.",
          "link": "http://arxiv.org/abs/2101.10803",
          "publishedOn": "2021-08-18T01:55:02.220Z",
          "wordCount": 685,
          "title": "ACAV100M: Automatic Curation of Large-Scale Datasets for Audio-Visual Video Representation Learning. (arXiv:2101.10803v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1\">Matthew R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>",
          "description": "One-stage object detection is commonly implemented by optimizing two\nsub-tasks: object classification and localization, using heads with two\nparallel branches, which might lead to a certain level of spatial misalignment\nin predictions between the two tasks. In this work, we propose a Task-aligned\nOne-stage Object Detection (TOOD) that explicitly aligns the two tasks in a\nlearning-based manner. First, we design a novel Task-aligned Head (T-Head)\nwhich offers a better balance between learning task-interactive and\ntask-specific features, as well as a greater flexibility to learn the alignment\nvia a task-aligned predictor. Second, we propose Task Alignment Learning (TAL)\nto explicitly pull closer (or even unify) the optimal anchors for the two tasks\nduring training via a designed sample assignment scheme and a task-aligned\nloss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a\n51.1 AP at single-model single-scale testing. This surpasses the recent\none-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP),\nand PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also\ndemonstrate the effectiveness of TOOD for better aligning the tasks of object\nclassification and localization. Code is available at\nhttps://github.com/fcjian/TOOD.",
          "link": "http://arxiv.org/abs/2108.07755",
          "publishedOn": "2021-08-18T01:55:02.212Z",
          "wordCount": 624,
          "title": "TOOD: Task-aligned One-stage Object Detection. (arXiv:2108.07755v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Video-based person re-identification (re-ID) aims at matching the same person\nacross video clips. Efficiently exploiting multi-scale fine-grained features\nwhile building the structural interaction among them is pivotal for its\nsuccess. In this paper, we propose a hybrid framework, Dense Interaction\nLearning (DenseIL), that takes the principal advantages of both CNN-based and\nAttention-based architectures to tackle video-based person re-ID difficulties.\nDenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN\nencoder is responsible for efficiently extracting discriminative spatial\nfeatures while the DI decoder is designed to densely model spatial-temporal\ninherent interaction across frames. Different from previous works, we\nadditionally let the DI decoder densely attends to intermediate fine-grained\nCNN features and that naturally yields multi-grained spatial-temporal\nrepresentation for each video clip. Moreover, we introduce Spatio-TEmporal\nPositional Embedding (STEP-Emb) into the DI decoder to investigate the\npositional relation among the spatial-temporal inputs. Our experiments\nconsistently and significantly outperform all the state-of-the-art methods on\nmultiple standard video-based person re-ID datasets.",
          "link": "http://arxiv.org/abs/2103.09013",
          "publishedOn": "2021-08-18T01:55:02.193Z",
          "wordCount": 640,
          "title": "Dense Interaction Learning for Video-based Person Re-identification. (arXiv:2103.09013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Abdullah Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Multi-view projection methods have demonstrated their ability to reach\nstate-of-the-art performance on 3D shape recognition. Those methods learn\ndifferent ways to aggregate information from multiple views. However, the\ncamera view-points for those views tend to be heuristically set and fixed for\nall shapes. To circumvent the lack of dynamism of current multi-view methods,\nwe propose to learn those view-points. In particular, we introduce the\nMulti-View Transformation Network (MVTN) that regresses optimal view-points for\n3D shape recognition, building upon advances in differentiable rendering. As a\nresult, MVTN can be trained end-to-end along with any multi-view network for 3D\nshape classification. We integrate MVTN in a novel adaptive multi-view pipeline\nthat can render either 3D meshes or point clouds. MVTN exhibits clear\nperformance gains in the tasks of 3D shape classification and 3D shape\nretrieval without the need for extra training supervision. In these tasks, MVTN\nachieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the\nmost recent and realistic ScanObjectNN dataset (up to 6% improvement).\nInterestingly, we also show that MVTN can provide network robustness against\nrotation and occlusion in the 3D domain. The code is available at\nhttps://github.com/ajhamdi/MVTN .",
          "link": "http://arxiv.org/abs/2011.13244",
          "publishedOn": "2021-08-18T01:55:02.186Z",
          "wordCount": 675,
          "title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Assessing action quality is challenging due to the subtle differences between\nvideos and large variations in scores. Most existing approaches tackle this\nproblem by regressing a quality score from a single video, suffering a lot from\nthe large inter-video score variations. In this paper, we show that the\nrelations among videos can provide important clues for more accurate action\nquality assessment during both training and inference. Specifically, we\nreformulate the problem of action quality assessment as regressing the relative\nscores with reference to another video that has shared attributes (e.g.,\ncategory and difficulty), instead of learning unreferenced scores. Following\nthis formulation, we propose a new Contrastive Regression (CoRe) framework to\nlearn the relative scores by pair-wise comparison, which highlights the\ndifferences between videos and guides the models to learn the key hints for\nassessment. In order to further exploit the relative information between two\nvideos, we devise a group-aware regression tree to convert the conventional\nscore regression into two easier sub-problems: coarse-to-fine classification\nand regression in small intervals. To demonstrate the effectiveness of CoRe, we\nconduct extensive experiments on three mainstream AQA datasets including AQA-7,\nMTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large\nmargin and establishes new state-of-the-art on all three benchmarks.",
          "link": "http://arxiv.org/abs/2108.07797",
          "publishedOn": "2021-08-18T01:55:02.178Z",
          "wordCount": 654,
          "title": "Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schutte_A/0/1/0/all/0/1\">August DuMont Sch&#xfc;tte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hetzel_J/0/1/0/all/0/1\">J&#xfc;rgen Hetzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dietz_B/0/1/0/all/0/1\">Benedikt Dietz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwab_P/0/1/0/all/0/1\">Patrick Schwab</a>",
          "description": "Privacy concerns around sharing personally identifiable information are a\nmajor practical barrier to data sharing in medical research. However, in many\ncases, researchers have no interest in a particular individual's information\nbut rather aim to derive insights at the level of cohorts. Here, we utilize\nGenerative Adversarial Networks (GANs) to create derived medical imaging\ndatasets consisting entirely of synthetic patient data. The synthetic images\nideally have, in aggregate, similar statistical properties to those of a source\ndataset but do not contain sensitive personal information. We assess the\nquality of synthetic data generated by two GAN models for chest radiographs\nwith 14 different radiology findings and brain computed tomography (CT) scans\nwith six types of intracranial hemorrhages. We measure the synthetic image\nquality by the performance difference of predictive models trained on either\nthe synthetic or the real dataset. We find that synthetic data performance\ndisproportionately benefits from a reduced number of unique label combinations.\nOur open-source benchmark also indicates that at low number of samples per\nclass, label overfitting effects start to dominate GAN training. We\nadditionally conducted a reader study in which trained radiologists do not\nperform better than random on discriminating between synthetic and real medical\nimages for intermediate levels of resolutions. In accordance with our benchmark\nresults, the classification accuracy of radiologists increases at higher\nspatial resolution levels. Our study offers valuable guidelines and outlines\npractical conditions under which insights derived from synthetic medical images\nare similar to those that would have been derived from real imaging data. Our\nresults indicate that synthetic data sharing may be an attractive and\nprivacy-preserving alternative to sharing real patient-level data in the right\nsettings.",
          "link": "http://arxiv.org/abs/2012.03769",
          "publishedOn": "2021-08-18T01:55:02.171Z",
          "wordCount": 761,
          "title": "Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation. (arXiv:2012.03769v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "3D point cloud understanding has made great progress in recent years.\nHowever, one major bottleneck is the scarcity of annotated real datasets,\nespecially compared to 2D object detection tasks, since a large amount of labor\nis involved in annotating the real scans of a scene. A promising solution to\nthis problem is to make better use of the synthetic dataset, which consists of\nCAD object models, to boost the learning on real datasets. This can be achieved\nby the pre-training and fine-tuning procedure. However, recent work on 3D\npre-training exhibits failure when transfer features learned on synthetic\nobjects to other real-world applications. In this work, we put forward a new\nmethod called RandomRooms to accomplish this objective. In particular, we\npropose to generate random layouts of a scene by making use of the objects in\nthe synthetic CAD dataset and learn the 3D scene representation by applying\nobject-level contrastive learning on two random scenes generated from the same\nset of synthetic objects. The model pre-trained in this way can serve as a\nbetter initialization when later fine-tuning on the 3D object detection task.\nEmpirically, we show consistent improvement in downstream 3D detection tasks on\nseveral base models, especially when less training data are used, which\nstrongly demonstrates the effectiveness and generalization of our method.\nBenefiting from the rich semantic knowledge and diverse objects from synthetic\ndata, our method establishes the new state-of-the-art on widely-used 3D\ndetection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide\na new perspective for bridging object and scene-level 3D understanding.",
          "link": "http://arxiv.org/abs/2108.07794",
          "publishedOn": "2021-08-18T01:55:02.163Z",
          "wordCount": 720,
          "title": "RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_L/0/1/0/all/0/1\">Louis Gallagher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John B. McDonald</a>",
          "description": "In this paper, we present a system for incrementally reconstructing a dense\n3D model of the geometry of an outdoor environment using a single monocular\ncamera attached to a moving vehicle. Dense models provide a rich representation\nof the environment facilitating higher-level scene understanding, perception,\nand planning. Our system employs dense depth prediction with a hybrid mapping\narchitecture combining state-of-the-art sparse features and dense fusion-based\nvisual SLAM algorithms within an integrated framework. Our novel contributions\ninclude design of hybrid sparse-dense camera tracking and loop closure, and\nscale estimation improvements in dense depth prediction. We use the motion\nestimates from the sparse method to overcome the large and variable inter-frame\ndisplacement typical of outdoor vehicle scenarios. Our system then registers\nthe live image with the dense model using whole-image alignment. This enables\nthe fusion of the live frame and dense depth prediction into the model. Global\nconsistency and alignment between the sparse and dense models are achieved by\napplying pose constraints from the sparse method directly within the\ndeformation of the dense model. We provide qualitative and quantitative results\nfor both trajectory estimation and surface reconstruction accuracy,\ndemonstrating competitive performance on the KITTI dataset. Qualitative results\nof the proposed approach are illustrated in https://youtu.be/Pn2uaVqjskY.\nSource code for the project is publicly available at the following repository\nhttps://github.com/robotvisionmu/DenseMonoSLAM.",
          "link": "http://arxiv.org/abs/2108.07736",
          "publishedOn": "2021-08-18T01:55:02.142Z",
          "wordCount": 683,
          "title": "A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving. (arXiv:2108.07736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Remarkable results have been achieved by DCNN based self-supervised depth\nestimation approaches. However, most of these approaches can only handle either\nday-time or night-time images, while their performance degrades for all-day\nimages due to large domain shift and the variation of illumination between day\nand night images. To relieve these limitations, we propose a domain-separated\nnetwork for self-supervised depth estimation of all-day images. Specifically,\nto relieve the negative influence of disturbing terms (illumination, etc.), we\npartition the information of day and night image pairs into two complementary\nsub-spaces: private and invariant domains, where the former contains the unique\ninformation (illumination, etc.) of day and night images and the latter\ncontains essential shared information (texture, etc.). Meanwhile, to guarantee\nthat the day and night images contain the same information, the\ndomain-separated network takes the day-time images and corresponding night-time\nimages (generated by GAN) as input, and the private and invariant feature\nextractors are learned by orthogonality and similarity loss, where the domain\ngap can be alleviated, thus better depth maps can be expected. Meanwhile, the\nreconstruction and photometric losses are utilized to estimate complementary\ninformation and depth maps effectively. Experimental results demonstrate that\nour approach achieves state-of-the-art depth estimation results for all-day\nimages on the challenging Oxford RobotCar dataset, proving the superiority of\nour proposed approach.",
          "link": "http://arxiv.org/abs/2108.07628",
          "publishedOn": "2021-08-18T01:55:02.021Z",
          "wordCount": 664,
          "title": "Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation. (arXiv:2108.07628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuxiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yupeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Unsupervised disentanglement learning is a crucial issue for understanding\nand exploiting deep generative models. Recently, SeFa tries to find latent\ndisentangled directions by performing SVD on the first projection of a\npre-trained GAN. However, it is only applied to the first layer and works in a\npost-processing way. Hessian Penalty minimizes the off-diagonal entries of the\noutput's Hessian matrix to facilitate disentanglement, and can be applied to\nmulti-layers.However, it constrains each entry of output independently, making\nit not sufficient in disentangling the latent directions (e.g., shape, size,\nrotation, etc.) of spatially correlated variations. In this paper, we propose a\nsimple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative\nmodel to learn disentangled representations. It simply encourages the variation\nof output caused by perturbations on different latent dimensions to be\northogonal, and the Jacobian with respect to the input is calculated to\nrepresent this variation. We show that our OroJaR also encourages the output's\nHessian matrix to be diagonal in an indirect manner. In contrast to the Hessian\nPenalty, our OroJaR constrains the output in a holistic way, making it very\neffective in disentangling latent dimensions corresponding to spatially\ncorrelated variations. Quantitative and qualitative experimental results show\nthat our method is effective in disentangled and controllable image generation,\nand performs favorably against the state-of-the-art methods. Our code is\navailable at https://github.com/csyxwei/OroJaR",
          "link": "http://arxiv.org/abs/2108.07668",
          "publishedOn": "2021-08-18T01:55:02.014Z",
          "wordCount": 676,
          "title": "Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation. (arXiv:2108.07668v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07739",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>",
          "description": "The study of 3D hyperspectral image (HSI) reconstruction refers to the\ninverse process of snapshot compressive imaging, during which the optical\nsystem, e.g., the coded aperture snapshot spectral imaging (CASSI) system,\ncaptures the 3D spatial-spectral signal and encodes it to a 2D measurement.\nWhile numerous sophisticated neural networks have been elaborated for\nend-to-end reconstruction, trade-offs still need to be made among performance,\nefficiency (training and inference time), and feasibility (the ability of\nrestoring high resolution HSI on limited GPU memory). This raises a challenge\nto design a new baseline to conjointly meet the above requirements. In this\npaper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual\nU-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)\nscale/spectral-invariant learning, 2) nested residual learning, and 3)\ncomputational efficiency. Benefiting from these three modules, the proposed\nSSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3\ndB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To\nthe greatest extent, SSI-ResU-Net achieves competing performance with over\n77.3% reduction in terms of floating-point operations (FLOPs), which for the\nfirst time, makes high-resolution HSI reconstruction feasible under practical\napplication scenarios. Code and pre-trained models are made available at\nhttps://github.com/Jiamian-Wang/HSI_baseline.",
          "link": "http://arxiv.org/abs/2108.07739",
          "publishedOn": "2021-08-18T01:55:01.920Z",
          "wordCount": 647,
          "title": "A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1\">Soyeon Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soo-Whan Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yoohwan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>",
          "description": "In this work, we present a novel audio-visual dataset for active speaker\ndetection in the wild. A speaker is considered active when his or her face is\nvisible and the voice is audible simultaneously. Although active speaker\ndetection is a crucial pre-processing step for many audio-visual tasks, there\nis no existing dataset of natural human speech to evaluate the performance of\nactive speaker detection. We therefore curate the Active Speakers in the Wild\n(ASW) dataset which contains videos and co-occurring speech segments with dense\nspeech activity labels. Videos and timestamps of audible segments are parsed\nand adopted from VoxConverse, an existing speaker diarisation dataset that\nconsists of videos in the wild. Face tracks are extracted from the videos and\nactive segments are annotated based on the timestamps of VoxConverse in a\nsemi-automatic way. Two reference systems, a self-supervised system and a fully\nsupervised one, are evaluated on the dataset to provide the baseline\nperformances of ASW. Cross-domain evaluation is conducted in order to show the\nnegative effect of dubbed videos in the training data.",
          "link": "http://arxiv.org/abs/2108.07640",
          "publishedOn": "2021-08-18T01:55:01.829Z",
          "wordCount": 653,
          "title": "Look Who's Talking: Active Speaker Detection in the Wild. (arXiv:2108.07640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Deng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangmiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>",
          "description": "We study self-supervised video representation learning, which is a\nchallenging task due to 1) lack of labels for explicit supervision; 2)\nunstructured and noisy visual information. Existing methods mainly use\ncontrastive loss with video clips as the instances and learn visual\nrepresentation by discriminating instances from each other, but they need a\ncareful treatment of negative pairs by either relying on large batch sizes,\nmemory banks, extra modalities or customized mining strategies, which\ninevitably includes noisy data. In this paper, we observe that the consistency\nbetween positive samples is the key to learn robust video representation.\nSpecifically, we propose two tasks to learn the appearance and speed\nconsistency, respectively. The appearance consistency task aims to maximize the\nsimilarity between two clips of the same video with different playback speeds.\nThe speed consistency task aims to maximize the similarity between two clips\nwith the same playback speed but different appearance information. We show that\noptimizing the two tasks jointly consistently improves the performance on\ndownstream tasks, e.g., action recognition and video retrieval. Remarkably, for\naction recognition on the UCF-101 dataset, we achieve 90.8\\% accuracy without\nusing any extra modalities or negative pairs for unsupervised pretraining,\nwhich outperforms the ImageNet supervised pretrained model. Codes and models\nwill be available.",
          "link": "http://arxiv.org/abs/2106.02342",
          "publishedOn": "2021-08-18T01:55:01.729Z",
          "wordCount": 686,
          "title": "ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1\">Michail Chatzianastasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1\">George Dasoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1\">Georgios Siolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between the\nstructural properties of the architecture and its performance.",
          "link": "http://arxiv.org/abs/2105.04885",
          "publishedOn": "2021-08-18T01:55:01.722Z",
          "wordCount": 645,
          "title": "Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr S. Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1\">Ali Abdelkader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anany_M/0/1/0/all/0/1\">Mohamed Anany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Behady_O/0/1/0/all/0/1\">Omar El-Behady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_M/0/1/0/all/0/1\">Muhammad Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangal_A/0/1/0/all/0/1\">Asser Hangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1\">Mohamed N. Moustafa</a>",
          "description": "LiDARs and cameras are the two main sensors that are planned to be included\nin many announced autonomous vehicles prototypes. Each of the two provides a\nunique form of data from a different perspective to the surrounding\nenvironment. In this paper, we explore and attempt to answer the question: is\nthere an added benefit by fusing those two forms of data for the purpose of\nsemantic segmentation within the context of autonomous driving? We also attempt\nto show at which level does said fusion prove to be the most useful. We\nevaluated our algorithms on the publicly available SemanticKITTI dataset. All\nfusion models show improvements over the base model, with the mid-level fusion\nshowing the highest improvement of 2.7% in terms of mean Intersection over\nUnion (mIoU) metric.",
          "link": "http://arxiv.org/abs/2108.07661",
          "publishedOn": "2021-08-18T01:55:01.716Z",
          "wordCount": 579,
          "title": "An Evaluation of RGB and LiDAR Fusion for Semantic Segmentation. (arXiv:2108.07661v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07595",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Horgan_C/0/1/0/all/0/1\">Conor C. Horgan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergholt_M/0/1/0/all/0/1\">Mads S. Bergholt</a>",
          "description": "Deep learning computer vision techniques have achieved many successes in\nrecent years across numerous imaging domains. However, the application of deep\nlearning to spectral data remains a complex task due to the need for\naugmentation routines, specific architectures for spectral data, and\nsignificant memory requirements. Here we present spectrai, an open-source deep\nlearning framework designed to facilitate the training of neural networks on\nspectral data and enable comparison between different methods. Spectrai\nprovides numerous built-in spectral data pre-processing and augmentation\nmethods, neural networks for spectral data including spectral (image)\ndenoising, spectral (image) classification, spectral image segmentation, and\nspectral image super-resolution. Spectrai includes both command line and\ngraphical user interfaces (GUI) designed to guide users through model and\nhyperparameter decisions for a wide range of applications.",
          "link": "http://arxiv.org/abs/2108.07595",
          "publishedOn": "2021-08-18T01:55:01.710Z",
          "wordCount": 566,
          "title": "spectrai: A deep learning framework for spectral data. (arXiv:2108.07595v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity.",
          "link": "http://arxiv.org/abs/2106.04263",
          "publishedOn": "2021-08-18T01:55:01.704Z",
          "wordCount": 691,
          "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight. (arXiv:2106.04263v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>",
          "description": "The conventional detectors tend to make imbalanced classification and suffer\nperformance drop, when the distribution of the training data is severely\nskewed. In this paper, we propose to use the mean classification score to\nindicate the classification accuracy for each category during training. Based\non this indicator, we balance the classification via an Equilibrium Loss (EBL)\nand a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL\nincreases the intensity of the adjustment of the decision boundary for the weak\nclasses by a designed score-guided loss margin between any two classes. On the\nother hand, MFS improves the frequency and accuracy of the adjustment of the\ndecision boundary for the weak classes through over-sampling the instance\nfeatures of those classes. Therefore, EBL and MFS work collaboratively for\nfinding the classification equilibrium in long-tailed detection, and\ndramatically improve the performance of tail classes while maintaining or even\nimproving the performance of head classes. We conduct experiments on LVIS using\nMask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to\nshow the superiority of the proposed method. It improves the detection\nperformance of tail classes by 15.6 AP, and outperforms the most recent\nlong-tailed object detectors by more than 1 AP. Code is available at\nhttps://github.com/fcjian/LOCE.",
          "link": "http://arxiv.org/abs/2108.07507",
          "publishedOn": "2021-08-18T01:55:01.685Z",
          "wordCount": 636,
          "title": "Exploring Classification Equilibrium in Long-Tailed Object Detection. (arXiv:2108.07507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>",
          "description": "High quality facial image editing is a challenging problem in the movie\npost-production industry, requiring a high degree of control and identity\npreservation. Previous works that attempt to tackle this problem may suffer\nfrom the entanglement of facial attributes and the loss of the person's\nidentity. Furthermore, many algorithms are limited to a certain task. To tackle\nthese limitations, we propose to edit facial attributes via the latent space of\na StyleGAN generator, by training a dedicated latent transformation network and\nincorporating explicit disentanglement and identity preservation terms in the\nloss function. We further introduce a pipeline to generalize our face editing\nto videos. Our model achieves a disentangled, controllable, and\nidentity-preserving facial attribute editing, even in the challenging case of\nreal (i.e., non-synthetic) images and videos. We conduct extensive experiments\non image and video datasets and show that our model outperforms other\nstate-of-the-art methods in visual quality and quantitative evaluation. Source\ncodes are available at https://github.com/InterDigitalInc/latent-transformer.",
          "link": "http://arxiv.org/abs/2106.11895",
          "publishedOn": "2021-08-18T01:55:01.678Z",
          "wordCount": 641,
          "title": "A Latent Transformer for Disentangled Face Editing in Images and Videos. (arXiv:2106.11895v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>",
          "description": "Various deep learning techniques have been proposed to solve the single-view\n2D-to-3D pose estimation problem. While the average prediction accuracy has\nbeen improved significantly over the years, the performance on hard poses with\ndepth ambiguity, self-occlusion, and complex or rare poses is still far from\nsatisfactory. In this work, we target these hard poses and present a novel\nskeletal GNN learning solution. To be specific, we propose a hop-aware\nhierarchical channel-squeezing fusion layer to effectively extract relevant\ninformation from neighboring nodes while suppressing undesired noises in GNN\nlearning. In addition, we propose a temporal-aware dynamic graph construction\nprocedure that is robust and effective for 3D pose estimation. Experimental\nresults on the Human3.6M dataset show that our solution achieves 10.3\\% average\nprediction accuracy improvement and greatly improves on hard poses over\nstate-of-the-art techniques. We further apply the proposed technique on the\nskeleton-based action recognition task and also achieve state-of-the-art\nperformance. Our code is available at\nhttps://github.com/ailingzengzzz/Skeletal-GNN.",
          "link": "http://arxiv.org/abs/2108.07181",
          "publishedOn": "2021-08-18T01:55:01.672Z",
          "wordCount": 621,
          "title": "Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation. (arXiv:2108.07181v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Alex Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tancik_M/0/1/0/all/0/1\">Matthew Tancik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Ren Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>",
          "description": "We introduce a method to render Neural Radiance Fields (NeRFs) in real time\nusing PlenOctrees, an octree-based 3D representation which supports\nview-dependent effects. Our method can render 800x800 images at more than 150\nFPS, which is over 3000 times faster than conventional NeRFs. We do so without\nsacrificing quality while preserving the ability of NeRFs to perform\nfree-viewpoint rendering of scenes with arbitrary geometry and view-dependent\neffects. Real-time performance is achieved by pre-tabulating the NeRF into a\nPlenOctree. In order to preserve view-dependent effects such as specularities,\nwe factorize the appearance via closed-form spherical basis functions.\nSpecifically, we show that it is possible to train NeRFs to predict a spherical\nharmonic representation of radiance, removing the viewing direction as an input\nto the neural network. Furthermore, we show that PlenOctrees can be directly\noptimized to further minimize the reconstruction loss, which leads to equal or\nbetter quality compared to competing methods. Moreover, this octree\noptimization step can be used to reduce the training time, as we no longer need\nto wait for the NeRF training to converge fully. Our real-time neural rendering\napproach may potentially enable new applications such as 6-DOF industrial and\nproduct visualizations, as well as next generation AR/VR systems. PlenOctrees\nare amenable to in-browser rendering as well; please visit the project page for\nthe interactive online demo, as well as video and code:\nhttps://alexyu.net/plenoctrees",
          "link": "http://arxiv.org/abs/2103.14024",
          "publishedOn": "2021-08-18T01:55:01.666Z",
          "wordCount": 705,
          "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields. (arXiv:2103.14024v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neimark_D/0/1/0/all/0/1\">Daniel Neimark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_O/0/1/0/all/0/1\">Omri Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zohar_M/0/1/0/all/0/1\">Maya Zohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asselmann_D/0/1/0/all/0/1\">Dotan Asselmann</a>",
          "description": "This paper presents VTN, a transformer-based framework for video recognition.\nInspired by recent developments in vision transformers, we ditch the standard\napproach in video action recognition that relies on 3D ConvNets and introduce a\nmethod that classifies actions by attending to the entire video sequence\ninformation. Our approach is generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains $16.1\\times$ faster and runs\n$5.1\\times$ faster during inference while maintaining competitive accuracy\ncompared to other state-of-the-art methods. It enables whole video analysis,\nvia a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We\nreport competitive results on Kinetics-400 and present an ablation study of VTN\nproperties and the trade-off between accuracy and inference speed. We hope our\napproach will serve as a new baseline and start a fresh line of research in the\nvideo recognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md",
          "link": "http://arxiv.org/abs/2102.00719",
          "publishedOn": "2021-08-18T01:55:01.658Z",
          "wordCount": 613,
          "title": "Video Transformer Network. (arXiv:2102.00719v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Fruit flies are established model systems for studying olfactory learning as\nthey will readily learn to associate odors with both electric shock or sugar\nrewards. The mechanisms of the insect brain apparently responsible for odor\nlearning form a relatively shallow neuronal architecture. Olfactory inputs are\nreceived by the antennal lobe (AL) of the brain, which produces an encoding of\neach odor mixture across ~50 sub-units known as glomeruli. Each of these\nglomeruli then project its component of this feature vector to several of ~2000\nso-called Kenyon Cells (KCs) in a region of the brain known as the mushroom\nbody (MB). Fly responses to odors are generated by small downstream neuropils\nthat decode the higher-order representation from the MB. Research has shown\nthat there is no recognizable pattern in the glomeruli--KC connections (and\nthus the particular higher-order representations); they are akin to\nfingerprints~-- even isogenic flies have different projections. Leveraging\ninsights from this architecture, we propose KCNet, a single-hidden-layer neural\nnetwork that contains sparse, randomized, binary weights between the input\nlayer and the hidden layer and analytically learned weights between the hidden\nlayer and the output layer. Furthermore, we also propose a dynamic optimization\nalgorithm that enables the KCNet to increase performance beyond its structural\nlimits by searching a more efficient set of inputs. For odorant-perception\ntasks that predict perceptual properties of an odorant, we show that KCNet\noutperforms existing data-driven approaches, such as XGBoost. For\nimage-classification tasks, KCNet achieves reasonable performance on benchmark\ndatasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation\nmethods or convolutional layers and shows particularly fast running time. Thus,\nneural networks inspired by the insect brain can be both economical and perform\nwell.",
          "link": "http://arxiv.org/abs/2108.07554",
          "publishedOn": "2021-08-18T01:55:01.632Z",
          "wordCount": 730,
          "title": "KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Meng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Recently, image captioning has aroused great interest in both academic and\nindustrial worlds. Most existing systems are built upon large-scale datasets\nconsisting of image-sentence pairs, which, however, are time-consuming to\nconstruct. In addition, even for the most advanced image captioning systems, it\nis still difficult to realize deep image understanding. In this work, we\nachieve unpaired image captioning by bridging the vision and the language\ndomains with high-level semantic information. The motivation stems from the\nfact that the semantic concepts with the same modality can be extracted from\nboth images and descriptions. To further improve the quality of captions\ngenerated by the model, we propose the Semantic Relationship Explorer, which\nexplores the relationships between semantic concepts for better understanding\nof the image. Extensive experiments on MSCOCO dataset show that we can generate\ndesirable captions without paired datasets. Furthermore, the proposed approach\nboosts five strong baselines under the paired setting, where the most\nsignificant improvement in CIDEr score reaches 8%, demonstrating that it is\neffective and generalizes well to a wide range of models.",
          "link": "http://arxiv.org/abs/2106.10658",
          "publishedOn": "2021-08-18T01:55:01.625Z",
          "wordCount": 634,
          "title": "Exploring Semantic Relationships for Unpaired Image Captioning. (arXiv:2106.10658v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>",
          "description": "In open set recognition, a classifier has to detect unknown classes that are\nnot known at training time. In order to recognize new categories, the\nclassifier has to project the input samples of known classes in very compact\nand separated regions of the features space for discriminating samples of\nunknown classes. Recently proposed Capsule Networks have shown to outperform\nalternatives in many fields, particularly in image recognition, however they\nhave not been fully applied yet to open-set recognition. In capsule networks,\nscalar neurons are replaced by capsule vectors or matrices, whose entries\nrepresent different properties of objects. In our proposal, during training,\ncapsules features of the same known class are encouraged to match a pre-defined\ngaussian, one for each class. To this end, we use the variational autoencoder\nframework, with a set of gaussian priors as the approximation for the posterior\ndistribution. In this way, we are able to control the compactness of the\nfeatures of the same class around the center of the gaussians, thus controlling\nthe ability of the classifier in detecting samples from unknown classes. We\nconducted several experiments and ablation of our model, obtaining state of the\nart results on different datasets in the open set recognition and unknown\ndetection tasks.",
          "link": "http://arxiv.org/abs/2104.09159",
          "publishedOn": "2021-08-18T01:55:01.619Z",
          "wordCount": 675,
          "title": "Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>",
          "description": "People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.",
          "link": "http://arxiv.org/abs/2105.08756",
          "publishedOn": "2021-08-18T01:55:01.612Z",
          "wordCount": 657,
          "title": "Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Penghua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1\">Huaiwei Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>",
          "description": "With the renaissance of deep learning, automatic diagnostic systems for\ncomputed tomography (CT) have achieved many successful applications. However,\nthey are mostly attributed to careful expert annotations, which are often\nscarce in practice. This drives our interest to the unsupervised representation\nlearning. Recent studies have shown that self-supervised learning is an\neffective approach for learning representations, but most of them rely on the\nempirical design of transformations and pretext tasks. To avoid the\nsubjectivity associated with these methods, we propose the MVCNet, a novel\nunsupervised three dimensional (3D) representation learning method working in a\ntransformation-free manner. We view each 3D lesion from different orientations\nto collect multiple two dimensional (2D) views. Then, an embedding function is\nlearned by minimizing a contrastive loss so that the 2D views of the same 3D\nlesion are aggregated, and the 2D views of different lesions are separated. We\nevaluate the representations by training a simple classification head upon the\nembedding layer. Experimental results show that MVCNet achieves\nstate-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and\nTianChi (79.96%) datasets for unsupervised representation learning. When\nfine-tuned on 10% of the labeled data, the accuracies are comparable to the\nsupervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.\n83.34% on the three datasets, respectively), indicating the superiority of\nMVCNet in learning representations with limited annotations. Code is released\nat: https://github.com/penghuazhai/MVCNet.",
          "link": "http://arxiv.org/abs/2108.07662",
          "publishedOn": "2021-08-18T01:55:01.606Z",
          "wordCount": 696,
          "title": "MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Despite the great progress of person re-identification (ReID) with the\nadoption of Convolutional Neural Networks, current ReID models are opaque and\nonly outputs a scalar distance between two persons. There are few methods\nproviding users semantically understandable explanations for why two persons\nare the same one or not. In this paper, we propose a post-hoc method, named\nAttribute-guided Metric Distillation (AMD), to explain existing ReID models.\nThis is the first method to explore attributes to answer: 1) what and where the\nattributes make two persons different, and 2) how much each attribute\ncontributes to the difference. In AMD, we design a pluggable interpreter\nnetwork for target models to generate quantitative contributions of attributes\nand visualize accurate attention maps of the most discriminative attributes. To\nachieve this goal, we propose a metric distillation loss by which the\ninterpreter learns to decompose the distance of two persons into components of\nattributes with knowledge distilled from the target model. Moreover, we propose\nan attribute prior loss to make the interpreter generate attribute-guided\nattention maps and to eliminate biases caused by the imbalanced distribution of\nattributes. This loss can guide the interpreter to focus on the exclusive and\ndiscriminative attributes rather than the large-area but common attributes of\ntwo persons. Comprehensive experiments show that the interpreter can generate\neffective and intuitive explanations for varied models and generalize well\nunder cross-domain settings. As a by-product, the accuracy of target models can\nbe further improved with our interpreter.",
          "link": "http://arxiv.org/abs/2103.01451",
          "publishedOn": "2021-08-18T01:55:01.588Z",
          "wordCount": 722,
          "title": "Explainable Person Re-Identification with Attribute-guided Metric Distillation. (arXiv:2103.01451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "In this paper, we investigate the knowledge distillation (KD) strategy for\nobject detection and propose an effective framework applicable to both\nhomogeneous and heterogeneous student-teacher pairs. The conventional feature\nimitation paradigm introduces imitation masks to focus on informative\nforeground areas while excluding the background noises. However, we find that\nthose methods fail to fully utilize the semantic information in all feature\npyramid levels, which leads to inefficiency for knowledge distillation between\nFPN-based detectors. To this end, we propose a novel semantic-guided feature\nimitation technique, which automatically performs soft matching between feature\npairs across all pyramid levels to provide the optimal guidance to the student.\nTo push the envelop even further, we introduce contrastive distillation to\neffectively capture the information encoded in the relationship between\ndifferent feature regions. Finally, we propose a generalized detection KD\npipeline, which is capable of distilling both homogeneous and heterogeneous\ndetector pairs. Our method consistently outperforms the existing detection KD\ntechniques, and works when (1) components in the framework are used separately\nand in conjunction; (2) for both homogeneous and heterogenous student-teacher\npairs and (3) on multiple detection benchmarks. With a powerful\nX101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches\n44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO\ndataset.",
          "link": "http://arxiv.org/abs/2108.07482",
          "publishedOn": "2021-08-18T01:55:01.581Z",
          "wordCount": 663,
          "title": "G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation. (arXiv:2108.07482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-08-18T01:55:01.575Z",
          "wordCount": 707,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiabi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaopeng Gong</a>",
          "description": "Image co-segmentation has attracted a lot of attentions in computer vision\ncommunity. In this paper, we propose a new approach to image co-segmentation\nthrough introducing the dense connections into the decoder path of Siamese\nU-net and presenting a new edge enhanced 3D IOU loss measured over distance\nmaps. Considering the rigorous mapping between the signed normalized distance\nmap (SNDM) and the binary segmentation mask, we estimate the SNDMs directly\nfrom original images and use them to determine the segmentation results. We\napply the Siamese U-net for solving this problem and improve its effectiveness\nby densely connecting each layer with subsequent layers in the decoder path.\nFurthermore, a new learning loss is designed to measure the 3D intersection\nover union (IOU) between the generated SNDMs and the labeled SNDMs. The\nexperimental results on commonly used datasets for image co-segmentation\ndemonstrate the effectiveness of our presented dense structure and edge\nenhanced 3D IOU loss of SNDM. To our best knowledge, they lead to the\nstate-of-the-art performance on the Internet and iCoseg datasets.",
          "link": "http://arxiv.org/abs/2108.07491",
          "publishedOn": "2021-08-18T01:55:01.567Z",
          "wordCount": 618,
          "title": "A Dense Siamese U-Net trained with Edge Enhanced 3D IOU Loss for Image Co-segmentation. (arXiv:2108.07491v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1\">Vitjan Zavrtanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1\">Danijel Sko&#x10d;aj</a>",
          "description": "Visual surface anomaly detection aims to detect local image regions that\nsignificantly deviate from normal appearance. Recent surface anomaly detection\nmethods rely on generative models to accurately reconstruct the normal areas\nand to fail on anomalies. These methods are trained only on anomaly-free\nimages, and often require hand-crafted post-processing steps to localize the\nanomalies, which prohibits optimizing the feature extraction for maximal\ndetection capability. In addition to reconstructive approach, we cast surface\nanomaly detection primarily as a discriminative problem and propose a\ndiscriminatively trained reconstruction anomaly embedding model (DRAEM). The\nproposed method learns a joint representation of an anomalous image and its\nanomaly-free reconstruction, while simultaneously learning a decision boundary\nbetween normal and anomalous examples. The method enables direct anomaly\nlocalization without the need for additional complicated post-processing of the\nnetwork output and can be trained using simple and general anomaly simulations.\nOn the challenging MVTec anomaly detection dataset, DRAEM outperforms the\ncurrent state-of-the-art unsupervised methods by a large margin and even\ndelivers detection performance close to the fully-supervised methods on the\nwidely used DAGM surface-defect detection dataset, while substantially\noutperforming them in localization accuracy.",
          "link": "http://arxiv.org/abs/2108.07610",
          "publishedOn": "2021-08-18T01:55:01.561Z",
          "wordCount": 628,
          "title": "DR{\\AE}M -- A discriminatively trained reconstruction embedding for surface anomaly detection. (arXiv:2108.07610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>",
          "description": "Light field (LF) image super-resolution (SR) aims at reconstructing\nhigh-resolution LF images from their low-resolution counterparts. Although\nCNN-based methods have achieved remarkable performance in LF image SR, these\nmethods cannot fully model the non-local properties of the 4D LF data. In this\npaper, we propose a simple but effective Transformer-based method for LF image\nSR. In our method, an angular Transformer is designed to incorporate\ncomplementary information among different views, and a spatial Transformer is\ndeveloped to capture both local and long-range dependencies within each\nsub-aperture image. With the proposed angular and spatial Transformers, the\nbeneficial information in an LF can be fully exploited and the SR performance\nis boosted. We validate the effectiveness of our angular and spatial\nTransformers through extensive ablation studies, and compare our method to\nrecent state-of-the-art methods on five public LF datasets. Our method achieves\nsuperior SR performance with a small model size and low computational cost.",
          "link": "http://arxiv.org/abs/2108.07597",
          "publishedOn": "2021-08-18T01:55:01.554Z",
          "wordCount": 602,
          "title": "Light Field Image Super-Resolution with Transformers. (arXiv:2108.07597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Adversarial robustness has attracted extensive studies recently by revealing\nthe vulnerability and intrinsic characteristics of deep networks. However,\nexisting works on adversarial robustness mainly focus on balanced datasets,\nwhile real-world data usually exhibits a long-tailed distribution. To push\nadversarial robustness towards more realistic scenarios, in this work we\ninvestigate the adversarial vulnerability as well as defense under long-tailed\ndistributions. In particular, we first reveal the negative impacts induced by\nimbalanced data on both recognition performance and adversarial robustness,\nuncovering the intrinsic challenges of this problem. We then perform a\nsystematic study on existing long-tailed recognition methods in conjunction\nwith the adversarial training framework. Several valuable observations are\nobtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of\nrobust accuracy exists under unreliable evaluation, and 3) boundary error\nlimits the promotion of robustness. Inspired by these observations, we propose\na clean yet effective framework, RoBal, which consists of two dedicated\nmodules, a scale-invariant classifier and data re-balancing via both margin\nengineering at training stage and boundary adjustment during inference.\nExtensive experiments demonstrate the superiority of our approach over other\nstate-of-the-art defense methods. To our best knowledge, we are the first to\ntackle adversarial robustness under long-tailed distributions, which we believe\nwould be a significant step towards real-world robustness. Our code is\navailable at: https://github.com/wutong16/Adversarial_Long-Tail .",
          "link": "http://arxiv.org/abs/2104.02703",
          "publishedOn": "2021-08-18T01:55:01.536Z",
          "wordCount": 696,
          "title": "Adversarial Robustness under Long-Tailed Distribution. (arXiv:2104.02703v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tilmon_B/0/1/0/all/0/1\">Brevin Tilmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppal_S/0/1/0/all/0/1\">Sanjeev J. Koppal</a>",
          "description": "Most monocular depth sensing methods use conventionally captured images that\nare created without considering scene content. In contrast, animal eyes have\nfast mechanical motions, called saccades, that control how the scene is imaged\nby the fovea, where resolution is highest. In this paper, we present the\nSaccadeCam framework for adaptively distributing resolution onto regions of\ninterest in the scene. Our algorithm for adaptive resolution is a\nself-supervised network and we demonstrate results for end-to-end learning for\nmonocular depth estimation. We also show preliminary results with a real\nSaccadeCam hardware prototype.",
          "link": "http://arxiv.org/abs/2103.12981",
          "publishedOn": "2021-08-18T01:55:01.529Z",
          "wordCount": 568,
          "title": "SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing. (arXiv:2103.12981v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Genser_A/0/1/0/all/0/1\">Alexander Genser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautle_N/0/1/0/all/0/1\">Noel Hautle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makridis_M/0/1/0/all/0/1\">Michail Makridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouvelas_A/0/1/0/all/0/1\">Anastasios Kouvelas</a>",
          "description": "Accurate estimation of the traffic state over a network is essential since it\nis the starting point for designing and implementing any traffic management\nstrategy. Hence, traffic operators and users of a transportation network can\nmake reliable decisions such as influence/change route or mode choice. However,\nthe problem of traffic state estimation from various sensors within an urban\nenvironment is very complex for several different reasons, such as availability\nof sensors, different noise levels, different output quantities, sensor\naccuracy, heterogeneous data fusion, and many more. To provide a better\nunderstanding of this problem, we organized an experimental campaign with video\nmeasurement in an area within the urban network of Zurich, Switzerland. We\nfocus on capturing the traffic state in terms of traffic flow and travel times\nby ensuring measurements from established thermal cameras by the city's\nauthorities, processed video data, and the Google Distance Matrix. We assess\nthe different data sources, and we propose a simple yet efficient Multiple\nLinear Regression (MLR) model to estimate travel times with fusion of various\ndata sources. Comparative results with ground-truth data (derived from video\nmeasurements) show the efficiency and robustness of the proposed methodology.",
          "link": "http://arxiv.org/abs/2108.07698",
          "publishedOn": "2021-08-18T01:55:01.523Z",
          "wordCount": 644,
          "title": "An Experimental Urban Case Study with Various Data Sources and a Model for Traffic Estimation. (arXiv:2108.07698v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trulls_E/0/1/0/all/0/1\">Eduard Trulls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosang_J/0/1/0/all/0/1\">Jan Hosang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>",
          "description": "We propose a novel framework for finding correspondences in images based on a\ndeep neural network that, given two images and a query point in one of them,\nfinds its correspondence in the other. By doing so, one has the option to query\nonly the points of interest and retrieve sparse correspondences, or to query\nall points in an image and obtain dense mappings. Importantly, in order to\ncapture both local and global priors, and to let our model relate between image\nregions using the most relevant among said priors, we realize our network using\na transformer. At inference time, we apply our correspondence network by\nrecursively zooming in around the estimates, yielding a multiscale pipeline\nable to provide highly-accurate correspondences. Our method significantly\noutperforms the state of the art on both sparse and dense correspondence\nproblems on multiple datasets and tasks, ranging from wide-baseline stereo to\noptical flow, without any retraining for a specific dataset. We commit to\nreleasing data, code, and all the tools necessary to train from scratch and\nensure reproducibility.",
          "link": "http://arxiv.org/abs/2103.14167",
          "publishedOn": "2021-08-18T01:55:01.516Z",
          "wordCount": 642,
          "title": "COTR: Correspondence Transformer for Matching Across Images. (arXiv:2103.14167v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopinathan_M/0/1/0/all/0/1\">Muraleekrishna Gopinathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_G/0/1/0/all/0/1\">Giang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Khalaf_J/0/1/0/all/0/1\">Jumana Abu-Khalaf</a>",
          "description": "Seamless Human-Robot Interaction is the ultimate goal of developing service\nrobotic systems. For this, the robotic agents have to understand their\nsurroundings to better complete a given task. Semantic scene understanding\nallows a robotic agent to extract semantic knowledge about the objects in the\nenvironment. In this work, we present a semantic scene understanding pipeline\nthat fuses 2D and 3D detection branches to generate a semantic map of the\nenvironment. The 2D mask proposals from state-of-the-art 2D detectors are\ninverse-projected to the 3D space and combined with 3D detections from point\nsegmentation networks. Unlike previous works that were evaluated on collected\ndatasets, we test our pipeline on an active photo-realistic robotic environment\n- BenchBot. Our novelty includes rectification of 3D proposals using projected\n2D detections and modality fusion based on object size. This work is done as\npart of the Robotic Vision Scene Understanding Challenge (RVSU). The\nperformance evaluation demonstrates that our pipeline has improved on baseline\nmethods without significant computational bottleneck.",
          "link": "http://arxiv.org/abs/2108.07616",
          "publishedOn": "2021-08-18T01:55:01.493Z",
          "wordCount": 617,
          "title": "Indoor Semantic Scene Understanding using Multi-modality Fusion. (arXiv:2108.07616v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1\">Tariq Alkhalifah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1\">Oleg Ovcharenko</a>",
          "description": "We propose a direct domain adaptation (DDA) approach to enrich the training\nof supervised neural networks on synthetic data by features from real-world\ndata. The process involves a series of linear operations on the input features\nto the NN model, whether they are from the source or target domains, as\nfollows: 1) A cross-correlation of the input data (i.e. images) with a randomly\npicked sample pixel (or pixels) of all images from that domain or the mean of\nall randomly picked sample pixel (or pixels) of all images. 2) The convolution\nof the resulting data with the mean of the autocorrelated input images from the\nother domain. In the training stage, as expected, the input images are from the\nsource domain, and the mean of auto-correlated images are evaluated from the\ntarget domain. In the inference/application stage, the input images are from\nthe target domain, and the mean of auto-correlated images are evaluated from\nthe source domain. The proposed method only manipulates the data from the\nsource and target domains and does not explicitly interfere with the training\nworkflow and network architecture. An application that includes training a\nconvolutional neural network on the MNIST dataset and testing the network on\nthe MNIST-M dataset achieves a 70% accuracy on the test data. A principal\ncomponent analysis (PCA), as well as t-SNE, show that the input features from\nthe source and target domains, after the proposed direct transformations, share\nsimilar properties along with the principal components as compared to the\noriginal MNIST and MNIST-M input features.",
          "link": "http://arxiv.org/abs/2108.07600",
          "publishedOn": "2021-08-18T01:55:01.485Z",
          "wordCount": 691,
          "title": "Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lianyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Feng Wang</a>",
          "description": "A large gap exists between fully-supervised object detection and\nweakly-supervised object detection. To narrow this gap, some methods consider\nknowledge transfer from additional fully-supervised dataset. But these methods\ndo not fully exploit discriminative category information in the\nfully-supervised dataset, thus causing low mAP. To solve this issue, we propose\na novel category transfer framework for weakly supervised object detection. The\nintuition is to fully leverage both visually-discriminative and\nsemantically-correlated category information in the fully-supervised dataset to\nenhance the object-classification ability of a weakly-supervised detector. To\nhandle overlapping category transfer, we propose a double-supervision mean\nteacher to gather common category information and bridge the domain gap between\ntwo datasets. To handle non-overlapping category transfer, we propose a\nsemantic graph convolutional network to promote the aggregation of semantic\nfeatures between correlated categories. Experiments are conducted with Pascal\nVOC 2007 as the target weakly-supervised dataset and COCO as the source\nfully-supervised dataset. Our category transfer framework achieves 63.5% mAP\nand 80.3% CorLoc with 5 overlapping categories between two datasets, which\noutperforms the state-of-the-art methods. Codes are avaliable at\nhttps://github.com/MediaBrain-SJTU/CaT.",
          "link": "http://arxiv.org/abs/2108.07487",
          "publishedOn": "2021-08-18T01:55:01.479Z",
          "wordCount": 617,
          "title": "CaT: Weakly Supervised Object Detection with Category Transfer. (arXiv:2108.07487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfieri_A/0/1/0/all/0/1\">Andrea Alfieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yancong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "Transformers can generate predictions in two approaches: 1. auto-regressively\nby conditioning each sequence element on the previous ones, or 2. directly\nproduce an output sequences in parallel. While research has mostly explored\nupon this difference on sequential tasks in NLP, we study the difference\nbetween auto-regressive and parallel prediction on visual set prediction tasks,\nand in particular on polygonal shapes in images because polygons are\nrepresentative of numerous types of objects, such as buildings or obstacles for\naerial vehicles. This is challenging for deep learning architectures as a\npolygon can consist of a varying carnality of points. We provide evidence on\nthe importance of natural orders for Transformers, and show the benefit of\ndecomposing complex polygons into collections of points in an auto-regressive\nmanner.",
          "link": "http://arxiv.org/abs/2108.07533",
          "publishedOn": "2021-08-18T01:55:01.472Z",
          "wordCount": 568,
          "title": "Investigating transformers in the decomposition of polygonal shapes as point collections. (arXiv:2108.07533v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07619",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1\">George Yiasemis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "In spite of its extensive adaptation in almost every medical diagnostic and\nexaminatorial application, Magnetic Resonance Imaging (MRI) is still a slow\nimaging modality which limits its use for dynamic imaging. In recent years,\nParallel Imaging (PI) and Compressed Sensing (CS) have been utilised to\naccelerate the MRI acquisition. In clinical settings, subsampling the k-space\nmeasurements during scanning time using Cartesian trajectories, such as\nrectilinear sampling, is currently the most conventional CS approach applied\nwhich, however, is prone to producing aliased reconstructions. With the advent\nof the involvement of Deep Learning (DL) in accelerating the MRI,\nreconstructing faithful images from subsampled data became increasingly\npromising. Retrospectively applying a subsampling mask onto the k-space data is\na way of simulating the accelerated acquisition of k-space data in real\nclinical setting. In this paper we compare and provide a review for the effect\nof applying either rectilinear or radial retrospective subsampling on the\nquality of the reconstructions outputted by trained deep neural networks. With\nthe same choice of hyper-parameters, we train and evaluate two distinct\nRecurrent Inference Machines (RIMs), one for each type of subsampling. The\nqualitative and quantitative results of our experiments indicate that the model\ntrained on data with radial subsampling attains higher performance and learns\nto estimate reconstructions with higher fidelity paving the way for other DL\napproaches to involve radial subsampling.",
          "link": "http://arxiv.org/abs/2108.07619",
          "publishedOn": "2021-08-18T01:55:01.465Z",
          "wordCount": 679,
          "title": "Deep MRI Reconstruction with Radial Subsampling. (arXiv:2108.07619v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chi-Tung Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinzheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_W/0/1/0/all/0/1\">Wei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youjing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">YuTing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chien-Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Youbao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Chen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_T/0/1/0/all/0/1\">Ta-Sen Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chien-Hung Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P. Harrison</a>",
          "description": "Hepatocellular carcinoma (HCC) can be potentially discovered from abdominal\ncomputed tomography (CT) studies under varied clinical scenarios, e.g., fully\ndynamic contrast enhanced (DCE) studies, non-contrast (NC) plus venous phase\n(VP) abdominal studies, or NC-only studies. We develop a flexible\nthree-dimensional deep algorithm, called hetero-phase volumetric detection\n(HPVD), that can accept any combination of contrast-phase inputs and with\nadjustable sensitivity depending on the clinical purpose. We trained HPVD on\n771 DCE CT scans to detect HCCs and tested on external 164 positives and 206\ncontrols, respectively. We compare performance against six clinical readers,\nincluding two radiologists, two hepato-pancreatico-biliary (HPB) surgeons, and\ntwo hepatologists. The area under curve (AUC) of the localization receiver\noperating characteristic (LROC) for NC-only, NC plus VP, and full DCE CT\nyielded 0.71, 0.81, 0.89 respectively. At a high sensitivity operating point of\n80% on DCE CT, HPVD achieved 97% specificity, which is comparable to measured\nphysician performance. We also demonstrate performance improvements over more\ntypical and less flexible non hetero-phase detectors. Thus, we demonstrate that\na single deep learning algorithm can be effectively applied to diverse HCC\ndetection clinical scenarios.",
          "link": "http://arxiv.org/abs/2108.07492",
          "publishedOn": "2021-08-18T01:55:01.459Z",
          "wordCount": 657,
          "title": "A Flexible Three-Dimensional Hetero-phase Computed Tomography Hepatocellular Carcinoma (HCC) Detection Algorithm for Generalizable and Practical HCC Screening. (arXiv:2108.07492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Ze-Hua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bo-Wen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaqi Zhang</a>",
          "description": "Compared to color images captured by conventional RGB cameras, monochrome\nimages usually have better signal-to-noise ratio (SNR) and richer textures due\nto its higher quantum efficiency. It is thus natural to apply a mono-color\ndual-camera system to restore color images with higher visual quality. In this\npaper, we propose a mono-color image enhancement algorithm that colorizes the\nmonochrome image with the color one. Based on the assumption that adjacent\nstructures with similar luminance values are likely to have similar colors, we\nfirst perform dense scribbling to assign colors to the monochrome pixels\nthrough block matching. Two types of outliers, including occlusion and color\nambiguity, are detected and removed from the initial scribbles. We also\nintroduce a sampling strategy to accelerate the scribbling process. Then, the\ndense scribbles are propagated to the entire image. To alleviate incorrect\ncolor propagation in the regions that have no color hints at all, we generate\nextra color seeds based on the existed scribbles to guide the propagation\nprocess. Experimental results show that, our algorithm can efficiently restore\ncolor images with higher SNR and richer details from the mono-color image\npairs, and achieves good performance in solving the color bleeding problem.",
          "link": "http://arxiv.org/abs/2108.07471",
          "publishedOn": "2021-08-18T01:55:01.452Z",
          "wordCount": 627,
          "title": "Guided Colorization Using Mono-Color Image Pairs. (arXiv:2108.07471v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songcen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>",
          "description": "Instance segmentation in 3D scenes is fundamental in many applications of\nscene understanding. It is yet challenging due to the compound factors of data\nirregularity and uncertainty in the numbers of instances. State-of-the-art\nmethods largely rely on a general pipeline that first learns point-wise\nfeatures discriminative at semantic and instance levels, followed by a separate\nstep of point grouping for proposing object instances. While promising, they\nhave the shortcomings that (1) the second step is not supervised by the main\nobjective of instance segmentation, and (2) their point-wise feature learning\nand grouping are less effective to deal with data irregularities, possibly\nresulting in fragmented segmentations. To address these issues, we propose in\nthis work an end-to-end solution of Semantic Superpoint Tree Network (SSTNet)\nfor proposing object instances from scene points. Key in SSTNet is an\nintermediate, semantic superpoint tree (SST), which is constructed based on the\nlearned semantic features of superpoints, and which will be traversed and split\nat intermediate tree nodes for proposals of object instances. We also design in\nSSTNet a refinement module, termed CliqueNet, to prune superpoints that may be\nwrongly grouped into instance proposals. Experiments on the benchmarks of\nScanNet and S3DIS show the efficacy of our proposed method. At the time of\nsubmission, SSTNet ranks top on the ScanNet (V2) leaderboard, with 2% higher of\nmAP than the second best method. The source code in PyTorch is available at\nhttps://github.com/Gorilla-Lab-SCUT/SSTNet.",
          "link": "http://arxiv.org/abs/2108.07478",
          "publishedOn": "2021-08-18T01:55:01.446Z",
          "wordCount": 683,
          "title": "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks. (arXiv:2108.07478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaojing Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Taizhe Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>",
          "description": "With the development of Generative Adversarial Network, image-based virtual\ntry-on methods have made great progress. However, limited work has explored the\ntask of video-based virtual try-on while it is important in real-world\napplications. Most existing video-based virtual try-on methods usually require\nclothing templates and they can only generate blurred and low-resolution\nresults. To address these challenges, we propose a Memory-based Video virtual\nTry-On Network (MV-TON), which seamlessly transfers desired clothes to a target\nperson without using any clothing templates and generates high-resolution\nrealistic videos. Specifically, MV-TON consists of two modules: 1) a try-on\nmodule that transfers the desired clothes from model images to frame images by\npose alignment and region-wise replacing of pixels; 2) a memory refinement\nmodule that learns to embed the existing generated frames into the latent space\nas external memory for the following frame generation. Experimental results\nshow the effectiveness of our method in the video virtual try-on task and its\nsuperiority over other existing methods.",
          "link": "http://arxiv.org/abs/2108.07502",
          "publishedOn": "2021-08-18T01:55:01.431Z",
          "wordCount": 595,
          "title": "MV-TON: Memory-based Video Virtual Try-on network. (arXiv:2108.07502v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaochen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajnsek_I/0/1/0/all/0/1\">Irena Hajnsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>",
          "description": "Automated animal censuses with aerial imagery are a vital ingredient towards\nwildlife conservation. Recent models are generally based on deep learning and\nthus require vast amounts of training data. Due to their scarcity and minuscule\nsize, annotating animals in aerial imagery is a highly tedious process. In this\nproject, we present a methodology to reduce the amount of required training\ndata by resorting to self-supervised pretraining. In detail, we examine a\ncombination of recent contrastive learning methodologies like Momentum Contrast\n(MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our\nmodel on the aerial images without the requirement for labels. We show that a\ncombination of MoCo, CLD, and geometric augmentations outperforms conventional\nmodels pre-trained on ImageNet by a large margin. Crucially, our method still\nyields favorable results even if we reduce the number of training animals to\njust 10%, at which point our best model scores double the recall of the\nbaseline at similar precision. This effectively allows reducing the number of\nrequired annotations to a fraction while still being able to train\nhigh-accuracy models in such highly challenging settings.",
          "link": "http://arxiv.org/abs/2108.07582",
          "publishedOn": "2021-08-18T01:55:01.418Z",
          "wordCount": 637,
          "title": "Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images. (arXiv:2108.07582v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haitian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We propose PR-RRN, a novel neural-network based method for Non-rigid\nStructure-from-Motion (NRSfM). PR-RRN consists of Residual-Recursive Networks\n(RRN) and two extra regularization losses. RRN is designed to effectively\nrecover 3D shape and camera from 2D keypoints with novel residual-recursive\nstructure. As NRSfM is a highly under-constrained problem, we propose two new\npairwise regularization to further regularize the reconstruction. The\nRigidity-based Pairwise Contrastive Loss regularizes the shape representation\nby encouraging higher similarity between the representations of high-rigidity\npairs of frames than low-rigidity pairs. We propose minimum singular-value\nratio to measure the pairwise rigidity. The Pairwise Consistency Loss enforces\nthe reconstruction to be consistent when the estimated shapes and cameras are\nexchanged between pairs. Our approach achieves state-of-the-art performance on\nCMU MOCAP and PASCAL3D+ dataset.",
          "link": "http://arxiv.org/abs/2108.07506",
          "publishedOn": "2021-08-18T01:55:01.399Z",
          "wordCount": 563,
          "title": "PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion. (arXiv:2108.07506v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Henglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "With the strength of deep generative models, 3D pose transfer regains\nintensive research interests in recent years. Existing methods mainly rely on a\nvariety of constraints to achieve the pose transfer over 3D meshes, e.g., the\nneed for the manually encoding for shape and pose disentanglement. In this\npaper, we present an unsupervised approach to conduct the pose transfer between\nany arbitrate given 3D meshes. Specifically, a novel Intrinsic-Extrinsic\nPreserved Generative Adversarial Network (IEP-GAN) is presented for both\nintrinsic (i.e., shape) and extrinsic (i.e., pose) information preservation.\nExtrinsically, we propose a co-occurrence discriminator to capture the\nstructural/pose invariance from distinct Laplacians of the mesh. Meanwhile,\nintrinsically, a local intrinsic-preserved loss is introduced to preserve the\ngeodesic priors while avoiding the heavy computations. At last, we show the\npossibility of using IEP-GAN to manipulate 3D human meshes in various ways,\nincluding pose transfer, identity swapping and pose interpolation with latent\ncode vector arithmetic. The extensive experiments on various 3D datasets of\nhumans, animals and hands qualitatively and quantitatively demonstrate the\ngenerality of our approach. Our proposed model produces better results and is\nsubstantially more efficient compared to recent state-of-the-art methods. Code\nis available: https://github.com/mikecheninoulu/Unsupervised_IEPGAN.",
          "link": "http://arxiv.org/abs/2108.07520",
          "publishedOn": "2021-08-18T01:55:01.384Z",
          "wordCount": 642,
          "title": "Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer. (arXiv:2108.07520v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strohm_F/0/1/0/all/0/1\">Florian Strohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_E/0/1/0/all/0/1\">Ekta Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_S/0/1/0/all/0/1\">Sven Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Philipp M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bace_M/0/1/0/all/0/1\">Mihai B&#xe2;ce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1\">Andreas Bulling</a>",
          "description": "We propose a novel method that leverages human fixations to visually decode\nthe image a person has in mind into a photofit (facial composite). Our method\ncombines three neural networks: An encoder, a scoring network, and a decoder.\nThe encoder extracts image features and predicts a neural activation map for\neach face looked at by a human observer. A neural scoring network compares the\nhuman and neural attention and predicts a relevance score for each extracted\nimage feature. Finally, image features are aggregated into a single feature\nvector as a linear combination of all features weighted by relevance which a\ndecoder decodes into the final photofit. We train the neural scoring network on\na novel dataset containing gaze data of 19 participants looking at collages of\nsynthetic faces. We show that our method significantly outperforms a mean\nbaseline predictor and report on a human study that shows that we can decode\nphotofits that are visually plausible and close to the observer's mental image.",
          "link": "http://arxiv.org/abs/2108.07524",
          "publishedOn": "2021-08-18T01:55:01.365Z",
          "wordCount": 606,
          "title": "Neural Photofit: Gaze-based Mental Image Reconstruction. (arXiv:2108.07524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenbing Tao</a>",
          "description": "Camera and 3D LiDAR sensors have become indispensable devices in modern\nautonomous driving vehicles, where the camera provides the fine-grained\ntexture, color information in 2D space and LiDAR captures more precise and\nfarther-away distance measurements of the surrounding environments. The\ncomplementary information from these two sensors makes the two-modality fusion\nbe a desired option. However, two major issues of the fusion between camera and\nLiDAR hinder its performance, \\ie, how to effectively fuse these two modalities\nand how to precisely align them (suffering from the weak spatiotemporal\nsynchronization problem). In this paper, we propose a coarse-to-fine LiDAR and\ncamera fusion-based network (termed as LIF-Seg) for LiDAR segmentation. For the\nfirst issue, unlike these previous works fusing the point cloud and image\ninformation in a one-to-one manner, the proposed method fully utilizes the\ncontextual information of images and introduces a simple but effective\nearly-fusion strategy. Second, due to the weak spatiotemporal synchronization\nproblem, an offset rectification approach is designed to align these\ntwo-modality features. The cooperation of these two components leads to the\nsuccess of the effective camera-LiDAR fusion. Experimental results on the\nnuScenes dataset show the superiority of the proposed LIF-Seg over existing\nmethods with a large margin. Ablation studies and analyses demonstrate that our\nproposed LIF-Seg can effectively tackle the weak spatiotemporal synchronization\nproblem.",
          "link": "http://arxiv.org/abs/2108.07511",
          "publishedOn": "2021-08-18T01:55:01.328Z",
          "wordCount": 661,
          "title": "LIF-Seg: LiDAR and Camera Image Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2108.07511v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanini_T/0/1/0/all/0/1\">Tomaso Fontanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_L/0/1/0/all/0/1\">Luca Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1\">Andrea Prati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>",
          "description": "Gradient-based attention modeling has been used widely as a way to visualize\nand understand convolutional neural networks. However, exploiting these visual\nexplanations during the training of generative adversarial networks (GANs) is\nan unexplored area in computer vision research. Indeed, we argue that this kind\nof information can be used to influence GANs training in a positive way. For\nthis reason, in this paper, it is shown how gradient based attentions can be\nused as knowledge to be conveyed in a teacher-student paradigm for multi-domain\nimage-to-image translation tasks in order to improve the results of the student\narchitecture. Further, it is demonstrated how \"pseudo\"-attentions can also be\nemployed during training when teacher and student networks are trained on\ndifferent domains which share some similarities. The approach is validated on\nmulti-domain facial attributes transfer and human expression synthesis showing\nboth qualitative and quantitative results.",
          "link": "http://arxiv.org/abs/2108.07466",
          "publishedOn": "2021-08-18T01:55:01.272Z",
          "wordCount": 590,
          "title": "Transferring Knowledge with Attention Distillation for Multi-Domain Image-to-Image Translation. (arXiv:2108.07466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangfei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>",
          "description": "Weakly supervised image segmentation trained with image-level labels usually\nsuffers from inaccurate coverage of object areas during the generation of the\npseudo groundtruth. This is because the object activation maps are trained with\nthe classification objective and lack the ability to generalize. To improve the\ngenerality of the objective activation maps, we propose a region prototypical\nnetwork RPNet to explore the cross-image object diversity of the training set.\nSimilar object parts across images are identified via region feature\ncomparison. Object confidence is propagated between regions to discover new\nobject areas while background regions are suppressed. Experiments show that the\nproposed method generates more complete and accurate pseudo object masks, while\nachieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In\naddition, we investigate the robustness of the proposed method on reduced\ntraining sets.",
          "link": "http://arxiv.org/abs/2108.07413",
          "publishedOn": "2021-08-18T01:55:01.259Z",
          "wordCount": 576,
          "title": "Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation. (arXiv:2108.07413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shuang Mei</a>",
          "description": "The existing particle image velocimetry (PIV) do not consider the curvature\neffect of the non-straight particle trajectory, because it seems to be\nimpossible to obtain the curvature information from a pair of particle images.\nAs a result, the computed vector underestimates the real velocity due to the\nstraight-line approximation, that further causes a systematic error for the PIV\ninstrument. In this work, the particle curved trajectory between two recordings\nis firstly explained with the streamline segment of a steady flow\n(diffeomorphic transformation) instead of a single vector, and this idea is\ntermed as diffeomorphic PIV. Specifically, a deformation field is introduced to\ndescribe the particle displacement, i.e., we try to find the optimal velocity\nfield, of which the corresponding deformation vector field agrees with the\nparticle displacement. Because the variation of the deformation function can be\napproximated with the variation of the velocity function, the diffeomorphic PIV\ncan be implemented as iterative PIV. That says, the diffeomorphic PIV warps the\nimages with deformation vector field instead of the velocity, and keeps the\nrest as same as iterative PIVs. Two diffeomorphic deformation schemes --\nforward diffeomorphic deformation interrogation (FDDI) and central\ndiffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on\nsynthetic images, the FDDI achieves significant accuracy improvement across\ndifferent one-pass displacement estimators (cross-correlation, optical flow,\ndeep learning flow). Besides, the results on three real PIV image pairs\ndemonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI\nprovides larger velocity estimation (more accurate) in the fast curvy\nstreamline areas. The accuracy improvement of the combination of FDDI and\naccurate dense estimator means that our diffeomorphic PIV paves a new way for\ncomplex flow measurement.",
          "link": "http://arxiv.org/abs/2108.07438",
          "publishedOn": "2021-08-18T01:55:01.241Z",
          "wordCount": 710,
          "title": "Diffeomorphic Particle Image Velocimetry. (arXiv:2108.07438v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyunjong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>",
          "description": "We address the problem of visible-infrared person re-identification\n(VI-reID), that is, retrieving a set of person images, captured by visible or\ninfrared cameras, in a cross-modal setting. Two main challenges in VI-reID are\nintra-class variations across person images, and cross-modal discrepancies\nbetween visible and infrared images. Assuming that the person images are\nroughly aligned, previous approaches attempt to learn coarse image- or rigid\npart-level person representations that are discriminative and generalizable\nacross different modalities. However, the person images, typically cropped by\noff-the-shelf object detectors, are not necessarily well-aligned, which\ndistract discriminative person representation learning. In this paper, we\nintroduce a novel feature learning framework that addresses these problems in a\nunified way. To this end, we propose to exploit dense correspondences between\ncross-modal person images. This allows to address the cross-modal discrepancies\nin a pixel-level, suppressing modality-related features from person\nrepresentations more effectively. This also encourages pixel-wise associations\nbetween cross-modal local features, further facilitating discriminative feature\nlearning for VI-reID. Extensive experiments and analyses on standard VI-reID\nbenchmarks demonstrate the effectiveness of our approach, which significantly\noutperforms the state of the art.",
          "link": "http://arxiv.org/abs/2108.07422",
          "publishedOn": "2021-08-18T01:55:01.234Z",
          "wordCount": 622,
          "title": "Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences. (arXiv:2108.07422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1\">Hilal AlQuabeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1\">Ameera Bawazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1\">Abdulateef Alhashmi</a>",
          "description": "Data labeling in supervised learning is considered an expensive and\ninfeasible tool in some conditions. The self-supervised learning method is\nproposed to tackle the learning effectiveness with fewer labeled data, however,\nthere is a lack of confidence in the size of labeled data needed to achieve\nadequate results. This study aims to draw a baseline on the proportion of the\nlabeled data that models can appreciate to yield competent accuracy when\ncompared to training with additional labels. The study implements the\nkaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the\nself-supervised learning task by implementing random rotations augmentation on\nthe original datasets. To reveal the true effectiveness of the pretext process\nin self-supervised learning, the original dataset is divided into smaller\nbatches, and learning is repeated on each batch with and without the pretext\npre-training. Results show that the pretext process in the self-supervised\nlearning improves the accuracy around 15% in the downstream classification task\nwhen compared to the plain supervised learning.",
          "link": "http://arxiv.org/abs/2108.07464",
          "publishedOn": "2021-08-18T01:55:01.207Z",
          "wordCount": 613,
          "title": "Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "The crux of self-supervised video representation learning is to build general\nfeatures from unlabeled videos. However, most recent works have mainly focused\non high-level semantics and neglected lower-level representations and their\ntemporal relationship which are crucial for general video understanding. To\naddress these challenges, this paper proposes a multi-level feature\noptimization framework to improve the generalization and temporal modeling\nability of learned video representations. Concretely, high-level features\nobtained from naive and prototypical contrastive learning are utilized to build\ndistribution graphs, guiding the process of low-level and mid-level feature\nlearning. We also devise a simple temporal modeling module from multi-level\nfeatures to enhance motion pattern learning. Experiments demonstrate that\nmulti-level feature optimization with the graph constraint and temporal\nmodeling can greatly improve the representation ability in video understanding.\nCode is available at\nhttps://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization.",
          "link": "http://arxiv.org/abs/2108.02183",
          "publishedOn": "2021-08-18T01:55:01.057Z",
          "wordCount": 600,
          "title": "Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Samuel G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Automatic augmentation methods have recently become a crucial pillar for\nstrong model performance in vision tasks. While existing automatic augmentation\nmethods need to trade off simplicity, cost and performance, we present a most\nsimple baseline, TrivialAugment, that outperforms previous methods for almost\nfree. TrivialAugment is parameter-free and only applies a single augmentation\nto each image. Thus, TrivialAugment's effectiveness is very unexpected to us\nand we performed very thorough experiments to study its performance. First, we\ncompare TrivialAugment to previous state-of-the-art methods in a variety of\nimage classification scenarios. Then, we perform multiple ablation studies with\ndifferent augmentation spaces, augmentation methods and setups to understand\nthe crucial requirements for its performance. Additionally, we provide a simple\ninterface to facilitate the widespread adoption of automatic augmentation\nmethods, as well as our full code base for reproducibility. Since our work\nreveals a stagnation in many parts of automatic augmentation research, we end\nwith a short proposal of best practices for sustained future progress in\nautomatic augmentation methods.",
          "link": "http://arxiv.org/abs/2103.10158",
          "publishedOn": "2021-08-18T01:55:01.026Z",
          "wordCount": 630,
          "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>",
          "description": "Time-lapse fluorescence microscopy (TLFM) is an important and powerful tool\nin synthetic biological research. Modeling TLFM experiments based on real data\nmay enable researchers to repeat certain experiments with minor effort. This\nthesis is a study towards deep learning-based modeling of TLFM experiments on\nthe image level. The modeling of TLFM experiments, by way of the example of\ntrapped yeast cells, is split into two tasks. The first task is to generate\nsynthetic image data based on real image data. To approach this problem, a\nnovel generative adversarial network, for conditionalized and unconditionalized\nimage generation, is proposed. The second task is the simulation of brightfield\nmicroscopy images over multiple discrete time-steps. To tackle this simulation\ntask an advanced future frame prediction model is introduced. The proposed\nmodels are trained and tested on a novel dataset that is presented in this\nthesis. The obtained results showed that the modeling of TLFM experiments, with\ndeep learning, is a proper approach, but requires future research to\neffectively model real-world experiments.",
          "link": "http://arxiv.org/abs/2103.11834",
          "publishedOn": "2021-08-18T01:55:00.994Z",
          "wordCount": 643,
          "title": "Generation and Simulation of Yeast Microscopy Imagery with Deep Learning. (arXiv:2103.11834v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1\">Denys Rozumnyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sroubek_F/0/1/0/all/0/1\">Filip Sroubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>",
          "description": "We propose the first learning-based approach for fast moving objects\ndetection. Such objects are highly blurred and move over large distances within\none video frame. Fast moving objects are associated with a deblurring and\nmatting problem, also called deblatting. We show that the separation of\ndeblatting into consecutive matting and deblurring allows achieving real-time\nperformance, i.e. an order of magnitude speed-up, and thus enabling new classes\nof application. The proposed method detects fast moving objects as a truncated\ndistance function to the trajectory by learning from synthetic data. For the\nsharp appearance estimation and accurate trajectory estimation, we propose a\nmatting and fitting network that estimates the blurred appearance without\nbackground, followed by an energy minimization based deblurring. The\nstate-of-the-art methods are outperformed in terms of recall, precision,\ntrajectory estimation, and sharp appearance reconstruction. Compared to other\nmethods, such as deblatting, the inference is of several orders of magnitude\nfaster and allows applications such as real-time fast moving object detection\nand retrieval in large video collections.",
          "link": "http://arxiv.org/abs/2012.08216",
          "publishedOn": "2021-08-18T01:55:00.962Z",
          "wordCount": 643,
          "title": "FMODetect: Robust Detection of Fast Moving Objects. (arXiv:2012.08216v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>",
          "description": "Graph matching (GM) has been a building block in many areas including\ncomputer vision and pattern recognition. Despite the recent impressive\nprogress, existing deep GM methods often have difficulty in handling outliers\nin both graphs, which are ubiquitous in practice. We propose a deep\nreinforcement learning (RL) based approach RGM for weighted graph matching,\nwhose sequential node matching scheme naturally fits with the strategy for\nselective inlier matching against outliers, and supports seed graph matching. A\nrevocable action scheme is devised to improve the agent's flexibility against\nthe complex constrained matching task. Moreover, we propose a quadratic\napproximation technique to regularize the affinity matrix, in the presence of\noutliers. As such, the RL agent can finish inlier matching timely when the\nobjective score stop growing, for which otherwise an additional hyperparameter\ni.e. the number of common inliers is needed to avoid matching outliers. In this\npaper, we are focused on learning the back-end solver for the most general form\nof GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can\nalso boost other solvers using the affinity input. Experimental results on both\nsynthetic and real-world datasets showcase its superior performance regarding\nboth matching accuracy and robustness.",
          "link": "http://arxiv.org/abs/2012.08950",
          "publishedOn": "2021-08-18T01:55:00.956Z",
          "wordCount": 686,
          "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1\">Simon Klenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1\">Jason Chui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>",
          "description": "Event cameras are bio-inspired vision sensors which measure per pixel\nbrightness changes. They offer numerous benefits over traditional, frame-based\ncameras, including low latency, high dynamic range, high temporal resolution\nand low power consumption. Thus, these sensors are suited for robotics and\nvirtual reality applications. To foster the development of 3D perception and\nnavigation algorithms with event cameras, we present the TUM-VIE dataset. It\nconsists of a large variety of handheld and head-mounted sequences in indoor\nand outdoor environments, including rapid motion during sports and high dynamic\nrange scenarios. The dataset contains stereo event data, stereo grayscale\nframes at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are\nsynchronized in hardware. The event cameras contain a large sensor of 1280x720\npixels, which is significantly larger than the sensors used in existing stereo\nevent datasets (at least by a factor of ten). We provide ground truth poses\nfrom a motion capture system at 120Hz during the beginning and end of each\nsequence, which can be used for trajectory evaluation. TUM-VIE includes\nchallenging sequences where state-of-the art visual SLAM algorithms either fail\nor result in large drift. Hence, our dataset can help to push the boundary of\nfuture research on event-based visual-inertial perception algorithms.",
          "link": "http://arxiv.org/abs/2108.07329",
          "publishedOn": "2021-08-18T01:55:00.917Z",
          "wordCount": 648,
          "title": "TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset. (arXiv:2108.07329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "Segmenting medical images accurately and reliably is important for disease\ndiagnosis and treatment. It is a challenging task because of the wide variety\nof objects' sizes, shapes, and scanning modalities. Recently, many\nconvolutional neural networks (CNN) have been designed for segmentation tasks\nand achieved great success. Few studies, however, have fully considered the\nsizes of objects and thus most demonstrate poor performance on segmentation of\nsmall objects segmentation. This can have significant impact on early detection\nof disease. This paper proposes a Context Axial Reserve Attention Network\n(CaraNet) to improve the segmentation performance on small objects compared\nwith recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS\n2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300 and\nETIS-LaribPolypDB) segmentation. Our CaraNet not only achieves the top-rank\nmean Dice segmentation accuracy, but also shows a distinct advantage in\nsegmentation of small medical objects.",
          "link": "http://arxiv.org/abs/2108.07368",
          "publishedOn": "2021-08-18T01:55:00.909Z",
          "wordCount": 598,
          "title": "CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects. (arXiv:2108.07368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>",
          "description": "We present BN-NAS, neural architecture search with Batch Normalization\n(BN-NAS), to accelerate neural architecture search (NAS). BN-NAS can\nsignificantly reduce the time required by model training and evaluation in NAS.\nSpecifically, for fast evaluation, we propose a BN-based indicator for\npredicting subnet performance at a very early training stage. The BN-based\nindicator further facilitates us to improve the training efficiency by only\ntraining the BN parameters during the supernet training. This is based on our\nobservation that training the whole supernet is not necessary while training\nonly BN parameters accelerates network convergence for network architecture\nsearch. Extensive experiments show that our method can significantly shorten\nthe time of training supernet by more than 10 times and shorten the time of\nevaluating subnets by more than 600,000 times without losing accuracy.",
          "link": "http://arxiv.org/abs/2108.07375",
          "publishedOn": "2021-08-18T01:55:00.886Z",
          "wordCount": 574,
          "title": "BN-NAS: Neural Architecture Search with Batch Normalization. (arXiv:2108.07375v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenju Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruisheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "The paper proposes a Dynamic ResBlock Generative Adversarial Network\n(DRB-GAN) for artistic style transfer. The style code is modeled as the shared\nparameters for Dynamic ResBlocks connecting both the style encoding network and\nthe style transfer network. In the style encoding network, a style class-aware\nattention mechanism is used to attend the style feature representation for\ngenerating the style codes. In the style transfer network, multiple Dynamic\nResBlocks are designed to integrate the style code and the extracted CNN\nsemantic feature and then feed into the spatial window Layer-Instance\nNormalization (SW-LIN) decoder, which enables high-quality synthetic images\nwith artistic style transfer. Moreover, the style collection conditional\ndiscriminator is designed to equip our DRB-GAN model with abilities for both\narbitrary style transfer and collection style transfer during the training\nstage. No matter for arbitrary style transfer or collection style transfer,\nextensive experiments strongly demonstrate that our proposed DRB-GAN\noutperforms state-of-the-art methods and exhibits its superior performance in\nterms of visual quality and efficiency. Our source code is available at\n\\color{magenta}{\\url{https://github.com/xuwenju123/DRB-GAN}}.",
          "link": "http://arxiv.org/abs/2108.07379",
          "publishedOn": "2021-08-18T01:55:00.880Z",
          "wordCount": 619,
          "title": "DRB-GAN: A Dynamic ResBlock Generative Adversarial Network for Artistic Style Transfer. (arXiv:2108.07379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1\">Ionut Cosmin Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose contextual convolution (CoConv) for visual recognition. CoConv is\na direct replacement of the standard convolution, which is the core component\nof convolutional neural networks. CoConv is implicitly equipped with the\ncapability of incorporating contextual information while maintaining a similar\nnumber of parameters and computational cost compared to the standard\nconvolution. CoConv is inspired by neuroscience studies indicating that (i)\nneurons, even from the primary visual cortex (V1 area), are involved in\ndetection of contextual cues and that (ii) the activity of a visual neuron can\nbe influenced by the stimuli placed entirely outside of its theoretical\nreceptive field. On the one hand, we integrate CoConv in the widely-used\nresidual networks and show improved recognition performance over baselines on\nthe core tasks and benchmarks for visual recognition, namely image\nclassification on the ImageNet data set and object detection on the MS COCO\ndata set. On the other hand, we introduce CoConv in the generator of a\nstate-of-the-art Generative Adversarial Network, showing improved generative\nresults on CIFAR-10 and CelebA. Our code is available at\nhttps://github.com/iduta/coconv.",
          "link": "http://arxiv.org/abs/2108.07387",
          "publishedOn": "2021-08-18T01:55:00.846Z",
          "wordCount": 621,
          "title": "Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Collecting large annotated datasets in Remote Sensing is often expensive and\nthus can become a major obstacle for training advanced machine learning models.\nCommon techniques of addressing this issue, based on the underlying idea of\npre-training the Deep Neural Networks (DNN) on freely available large datasets,\ncannot be used for Remote Sensing due to the unavailability of such large-scale\nlabeled datasets and the heterogeneity of data sources caused by the varying\nspatial and spectral resolution of different sensors. Self-supervised learning\nis an alternative approach that learns feature representation from unlabeled\nimages without using any human annotations. In this paper, we introduce a new\nmethod for land cover mapping by using a clustering based pretext task for\nself-supervised learning. We demonstrate the effectiveness of the method on two\nsocietally relevant applications from the aspect of segmentation performance,\ndiscriminative feature representation learning and the underlying cluster\nstructure. We also show the effectiveness of the active sampling using the\nclusters obtained from our method in improving the mapping accuracy given a\nlimited budget of annotating.",
          "link": "http://arxiv.org/abs/2108.07323",
          "publishedOn": "2021-08-18T01:55:00.840Z",
          "wordCount": 623,
          "title": "Clustering augmented Self-Supervised Learning: Anapplication to Land Cover Mapping. (arXiv:2108.07323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Ponti</a>",
          "description": "Scene Designer is a novel method for searching and generating images using\nfree-hand sketches of scene compositions; i.e. drawings that describe both the\nappearance and relative positions of objects. Our core contribution is a single\nunified model to learn both a cross-modal search embedding for matching\nsketched compositions to images, and an object embedding for layout synthesis.\nWe show that a graph neural network (GNN) followed by Transformer under our\nnovel contrastive learning setting is required to allow learning correlations\nbetween object type, appearance and arrangement, driving a mask generation\nmodule that synthesises coherent scene layouts, whilst also delivering state of\nthe art sketch based visual search of scenes.",
          "link": "http://arxiv.org/abs/2108.07353",
          "publishedOn": "2021-08-18T01:55:00.832Z",
          "wordCount": 571,
          "title": "Scene Designer: a Unified Model for Scene Search and Synthesis from Sketch. (arXiv:2108.07353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "With the help of the deep learning paradigm, many point cloud networks have\nbeen invented for visual analysis. However, there is great potential for\ndevelopment of these networks since the given information of point cloud data\nhas not been fully exploited. To improve the effectiveness of existing networks\nin analyzing point cloud data, we propose a plug-and-play module, PnP-3D,\naiming to refine the fundamental point cloud feature representations by\ninvolving more local context and global bilinear response from explicit 3D\nspace and implicit feature space. To thoroughly evaluate our approach, we\nconduct experiments on three standard point cloud analysis tasks, including\nclassification, semantic segmentation, and object detection, where we select\nthree state-of-the-art networks from each task for evaluation. Serving as a\nplug-and-play module, PnP-3D can significantly boost the performances of\nestablished networks. In addition to achieving state-of-the-art results on four\nwidely used point cloud benchmarks, we present comprehensive ablation studies\nand visualizations to demonstrate our approach's advantages. The code will be\navailable at https://github.com/ShiQiu0419/pnp-3d.",
          "link": "http://arxiv.org/abs/2108.07378",
          "publishedOn": "2021-08-18T01:55:00.812Z",
          "wordCount": 598,
          "title": "PnP-3D: A Plug-and-Play for 3D Point Clouds. (arXiv:2108.07378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OBrien_M/0/1/0/all/0/1\">Molly O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medoff_M/0/1/0/all/0/1\">Mike Medoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukowski_J/0/1/0/all/0/1\">Julia Bukowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Greg Hager</a>",
          "description": "It is well known that Neural Network (network) performance often degrades\nwhen a network is used in novel operating domains that differ from its training\nand testing domains. This is a major limitation, as networks are being\nintegrated into safety critical, cyber-physical systems that must work in\nunconstrained environments, e.g., perception for autonomous vehicles. Training\nnetworks that generalize to novel operating domains and that extract robust\nfeatures is an active area of research, but previous work fails to predict what\nthe network performance will be in novel operating domains. We propose the task\nNetwork Generalization Prediction: predicting the expected network performance\nin novel operating domains. We describe the network performance in terms of an\ninterpretable Context Subspace, and we propose a methodology for selecting the\nfeatures of the Context Subspace that provide the most information about the\nnetwork performance. We identify the Context Subspace for a pretrained Faster\nRCNN network performing pedestrian detection on the Berkeley Deep Drive (BDD)\nDataset, and demonstrate Network Generalization Prediction accuracy within 5%\nor less of observed performance. We also demonstrate that the Context Subspace\nfrom the BDD Dataset is informative for completely unseen datasets, JAAD and\nCityscapes, where predictions have a bias of 10% or less.",
          "link": "http://arxiv.org/abs/2108.07399",
          "publishedOn": "2021-08-18T01:55:00.806Z",
          "wordCount": 643,
          "title": "Network Generalization Prediction for Safety Critical Tasks in Novel Operating Domains. (arXiv:2108.07399v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mantang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>",
          "description": "In this paper, we tackle the problem of dense light field (LF) reconstruction\nfrom sparsely-sampled ones with wide baselines and propose a learnable model,\nnamely dynamic interpolation, to replace the commonly-used geometry warping\noperation. Specifically, with the estimated geometric relation between input\nviews, we first construct a lightweight neural network to dynamically learn\nweights for interpolating neighbouring pixels from input views to synthesize\neach pixel of novel views independently. In contrast to the fixed and\ncontent-independent weights employed in the geometry warping operation, the\nlearned interpolation weights implicitly incorporate the correspondences\nbetween the source and novel views and adapt to different image content\ninformation. Then, we recover the spatial correlation between the independently\nsynthesized pixels of each novel view by referring to that of input views using\na geometry-based spatial refinement module. We also constrain the angular\ncorrelation between the novel views through a disparity-oriented LF structure\nloss. Experimental results on LF datasets with wide baselines show that the\nreconstructed LFs achieve much higher PSNR/SSIM and preserve the LF parallax\nstructure better than state-of-the-art methods. The source code is publicly\navailable at https://github.com/MantangGuo/DI4SLF.",
          "link": "http://arxiv.org/abs/2108.07408",
          "publishedOn": "2021-08-18T01:55:00.740Z",
          "wordCount": 626,
          "title": "Learning Dynamic Interpolation for Extremely Sparse Light Fields with Wide Baselines. (arXiv:2108.07408v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.01440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Ding Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1\">Becky West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiquan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinzhou Huang</a>",
          "description": "E-commerce sites strive to provide users the most timely relevant information\nin order to reduce shopping frictions and increase customer satisfaction. Multi\narmed bandit models (MAB) as a type of adaptive optimization algorithms provide\npossible approaches for such purposes. In this paper, we analyze using three\nclassic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper\nconfidence bound 1 (UCB1) for dynamic content recommendations, and walk through\nthe process of developing these algorithms internally to solve a real world\ne-commerce use case. First, we analyze the three MAB algorithms using simulated\npurchasing datasets with non-stationary reward distributions to simulate the\npossible time-varying customer preferences, where the traffic allocation\ndynamics and the accumulative rewards of different algorithms are studied.\nSecond, we compare the accumulative rewards of the three MAB algorithms with\nmore than 1,000 trials using actual historical A/B test datasets. We find that\nthe larger difference between the success rates of competing recommendations\nthe more accumulative rewards the MAB algorithms can achieve. In addition, we\nfind that TS shows the highest average accumulative rewards under different\ntesting scenarios. Third, we develop a batch-updated MAB algorithm to overcome\nthe delayed reward issue in e-commerce and enable an online content\noptimization on our App homepage. For a state-of-the-art comparison, a real A/B\ntest among our batch-updated MAB algorithm, a third-party MAB solution, and the\ndefault business logic are conducted. The result shows that our batch-updated\nMAB algorithm outperforms the counterparts and achieves 6.13% relative\nclick-through rate (CTR) increase and 16.1% relative conversion rate (CVR)\nincrease compared to the default experience, and 2.9% relative CTR increase and\n1.4% relative CVR increase compared to the external MAB service.",
          "link": "http://arxiv.org/abs/2108.01440",
          "publishedOn": "2021-08-23T01:36:37.527Z",
          "wordCount": 736,
          "title": "Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yanglan Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>",
          "description": "Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately, e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method significantly improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.",
          "link": "http://arxiv.org/abs/2103.14023",
          "publishedOn": "2021-08-23T01:36:37.514Z",
          "wordCount": 774,
          "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting. (arXiv:2103.14023v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yinzhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Phil Burlina</a>",
          "description": "Machine learning (ML) models used in medical imaging diagnostics can be\nvulnerable to a variety of privacy attacks, including membership inference\nattacks, that lead to violations of regulations governing the use of medical\ndata and threaten to compromise their effective deployment in the clinic. In\ncontrast to most recent work in privacy-aware ML that has been focused on model\nalteration and post-processing steps, we propose here a novel and complementary\nscheme that enhances the security of medical data by controlling the data\nsharing process. We develop and evaluate a privacy defense protocol based on\nusing a generative adversarial network (GAN) that allows a medical data sourcer\n(e.g. a hospital) to provide an external agent (a modeler) a proxy dataset\nsynthesized from the original images, so that the resulting diagnostic systems\nmade available to model consumers is rendered resilient to privacy attackers.\nWe validate the proposed method on retinal diagnostics AI used for diabetic\nretinopathy that bears the risk of possibly leaking private information. To\nincorporate concerns of both privacy advocates and modelers, we introduce a\nmetric to evaluate privacy and utility performance in combination, and\ndemonstrate, using these novel and classical metrics, that our approach, by\nitself or in conjunction with other defenses, provides state of the art (SOTA)\nperformance for defending against privacy attacks.",
          "link": "http://arxiv.org/abs/2103.03078",
          "publishedOn": "2021-08-23T01:36:37.495Z",
          "wordCount": 712,
          "title": "Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods. (arXiv:2103.03078v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces. The demo is available at\nhttps://prasemap.qizhy.com.",
          "link": "http://arxiv.org/abs/2106.08801",
          "publishedOn": "2021-08-23T01:36:37.489Z",
          "wordCount": 602,
          "title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1\">Mahdi Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1\">Ramy E. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1\">Hessam Mahdavifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_A/0/1/0/all/0/1\">A. Salman Avestimehr</a>",
          "description": "We consider the problem of coded computing, where a computational task is\nperformed in a distributed fashion in the presence of adversarial workers. We\npropose techniques to break the adversarial toleration threshold barrier\npreviously known in coded computing. More specifically, we leverage\nlist-decoding techniques for folded Reed-Solomon codes and propose novel\nalgorithms to recover the correct codeword using side information. In the coded\ncomputing setting, we show how the master node can perform certain carefully\ndesigned extra computations to obtain the side information. The workload of\ncomputing this side information is negligible compared to the computations done\nby each worker. This side information is then utilized to prune the output of\nthe list decoder and uniquely recover the true outcome. We further propose\nfolded Lagrange coded computing (FLCC) to incorporate the developed techniques\ninto a specific coded computing setting. Our results show that FLCC outperforms\nLCC by breaking the barrier on the number of adversaries that can be tolerated.\nIn particular, the corresponding threshold in FLCC is improved by a factor of\ntwo asymptotically compared to that of LCC.",
          "link": "http://arxiv.org/abs/2101.11653",
          "publishedOn": "2021-08-23T01:36:37.473Z",
          "wordCount": 655,
          "title": "List-Decodable Coded Computing: Breaking the Adversarial Toleration Barrier. (arXiv:2101.11653v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15045",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sheth_D/0/1/0/all/0/1\">Dev Yashpal Sheth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vincent_J/0/1/0/all/0/1\">Joshua L. Vincent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manzorro_R/0/1/0/all/0/1\">Ramon Manzorro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_P/0/1/0/all/0/1\">Peter A. Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simoncelli_E/0/1/0/all/0/1\">Eero P. Simoncelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>",
          "description": "Deep convolutional neural networks (CNNs) for video denoising are typically\ntrained with supervision, assuming the availability of clean videos. However,\nin many applications, such as microscopy, noiseless videos are not available.\nTo address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN\narchitecture designed to be trained exclusively with noisy data. The\nperformance of UDVD is comparable to the supervised state-of-the-art, even when\ntrained only on a single short noisy video. We demonstrate the promise of our\napproach in real-world imaging applications by denoising raw video,\nfluorescence-microscopy and electron-microscopy data. In contrast to many\ncurrent approaches to video denoising, UDVD does not require explicit motion\ncompensation. This is advantageous because motion compensation is\ncomputationally expensive, and can be unreliable when the input data are noisy.\nA gradient-based analysis reveals that UDVD automatically adapts to local\nmotion in the input noisy videos. Thus, the network learns to perform implicit\nmotion compensation, even though it is only trained for denoising.",
          "link": "http://arxiv.org/abs/2011.15045",
          "publishedOn": "2021-08-23T01:36:37.466Z",
          "wordCount": 672,
          "title": "Unsupervised Deep Video Denoising. (arXiv:2011.15045v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aguiar_D/0/1/0/all/0/1\">Davi Pedrosa de Aguiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>",
          "description": "Inertial Measurement Unit (IMU) sensors are present in everyday devices such\nas smartphones and fitness watches. As a result, the array of health-related\nresearch and applications that tap onto this data has been growing, but little\nattention has been devoted to the prediction of an individual's heart rate (HR)\nfrom IMU data, when undergoing a physical activity. Would that be even\npossible? If so, this could be used to design personalized sets of aerobic\nexercises, for instance. In this work, we show that it is viable to obtain\naccurate HR predictions from IMU data using Recurrent Neural Networks, provided\nonly access to HR and IMU data from a short-lived, previously executed\nactivity. We propose a novel method for initializing an RNN's hidden state\nvectors, using a specialized network that attempts to extract an embedding of\nthe physical conditioning (PCE) of a subject. We show that using a\ndiscriminator in the training phase to help the model learn whether two PCEs\nbelong to the same individual further reduces the prediction error. We evaluate\nthe proposed model when predicting the HR of 23 subjects performing a variety\nof physical activities from IMU data available in public datasets (PAMAP2,\nPPG-DaLiA). For comparison, we use as baselines the only model specifically\nproposed for this task and an adapted state-of-the-art model for Human Activity\nRecognition (HAR), a closely related task. Our method, PCE-LSTM, yields over\n10% lower mean absolute error. We demonstrate empirically that this error\nreduction is in part due to the use of the PCE. Last, we use the two datasets\n(PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when\nphotoplethysmography (PPG) sensors are available, outperforming the\nstate-of-the-art deep learning baselines by more than 30%.",
          "link": "http://arxiv.org/abs/2103.12095",
          "publishedOn": "2021-08-23T01:36:37.459Z",
          "wordCount": 777,
          "title": "Am I fit for this physical activity? Neural embedding of physical conditioning from inertial sensors. (arXiv:2103.12095v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pritz_P/0/1/0/all/0/1\">Paul J. Pritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1\">Kin K. Leung</a>",
          "description": "While reinforcement learning has achieved considerable successes in recent\nyears, state-of-the-art models are often still limited by the size of state and\naction spaces. Model-free reinforcement learning approaches use some form of\nstate representations and the latest work has explored embedding techniques for\nactions, both with the aim of achieving better generalization and\napplicability. However, these approaches consider only states or actions,\nignoring the interaction between them when generating embedded representations.\nIn this work, we establish the theoretical foundations for the validity of\ntraining a reinforcement learning agent using embedded states and actions. We\nthen propose a new approach for jointly learning embeddings for states and\nactions that combines aspects of model-free and model-based reinforcement\nlearning, which can be applied in both discrete and continuous domains.\nSpecifically, we use a model of the environment to obtain embeddings for states\nand actions and present a generic architecture that leverages these to learn a\npolicy. In this way, the embedded representations obtained via our approach\nenable better generalization over both states and actions by capturing\nsimilarities in the embedding spaces. Evaluations of our approach on several\ngaming, robotic control, and recommender systems show it significantly\noutperforms state-of-the-art models in both discrete/continuous domains with\nlarge state/action spaces, thus confirming its efficacy.",
          "link": "http://arxiv.org/abs/2010.04444",
          "publishedOn": "2021-08-23T01:36:37.383Z",
          "wordCount": 689,
          "title": "Jointly-Learned State-Action Embedding for Efficient Reinforcement Learning. (arXiv:2010.04444v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.05438",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1\">Atsushi Nitanda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "We consider stochastic gradient descent and its averaging variant for binary\nclassification problems in a reproducing kernel Hilbert space. In the\ntraditional analysis using a consistency property of loss functions, it is\nknown that the expected classification error converges more slowly than the\nexpected risk even when assuming a low-noise condition on the conditional label\nprobabilities. Consequently, the resulting rate is sublinear. Therefore, it is\nimportant to consider whether much faster convergence of the expected\nclassification error can be achieved. In recent research, an exponential\nconvergence rate for stochastic gradient descent was shown under a strong\nlow-noise condition but provided theoretical analysis was limited to the\nsquared loss function, which is somewhat inadequate for binary classification\ntasks. In this paper, we show an exponential convergence of the expected\nclassification error in the final phase of the stochastic gradient descent for\na wide class of differentiable convex loss functions under similar assumptions.\nAs for the averaged stochastic gradient descent, we show that the same\nconvergence rate holds from the early phase of training. In experiments, we\nverify our analyses on the $L_2$-regularized logistic regression.",
          "link": "http://arxiv.org/abs/1806.05438",
          "publishedOn": "2021-08-23T01:36:37.376Z",
          "wordCount": 656,
          "title": "Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors. (arXiv:1806.05438v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1\">William Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1\">Armin Hadzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neil Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1\">Fady Alajaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1\">Phil Burlina</a>",
          "description": "We propose a novel method for enforcing AI fairness with respect to protected\nor sensitive factors. This method uses a dual strategy performing training and\nrepresentation alteration (TARA) for the mitigation of prominent causes of AI\nbias by including: a) the use of representation learning alteration via\nadversarial independence to suppress the bias-inducing dependence of the data\nrepresentation from protected factors; and b) training set alteration via\nintelligent augmentation to address bias-causing data imbalance, by using\ngenerative models that allow the fine control of sensitive factors related to\nunderrepresented populations via domain adaptation and latent space\nmanipulation. When testing our methods on image analytics, experiments\ndemonstrate that TARA significantly or fully debiases baseline models while\noutperforming competing debiasing methods that have the same amount of\ninformation, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.\nthe baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.\n(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in\ncurrent metrics used for assessing debiasing performance, we propose novel\nconjunctive debiasing metrics. Our experiments also demonstrate the ability of\nthese novel metrics in assessing the Pareto efficiency of the proposed methods.",
          "link": "http://arxiv.org/abs/2012.06387",
          "publishedOn": "2021-08-23T01:36:37.343Z",
          "wordCount": 692,
          "title": "TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosson_R/0/1/0/all/0/1\">Romain Cosson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>",
          "description": "Variational approximation, such as mean-field (MF) and tree-reweighted (TRW),\nprovide a computationally efficient approximation of the log-partition function\nfor a generic graphical model. TRW provably provides an upper bound, but the\napproximation ratio is generally not quantified.\n\nAs the primary contribution of this work, we provide an approach to quantify\nthe approximation ratio through the property of the underlying graph structure.\nSpecifically, we argue that (a variant of) TRW produces an estimate that is\nwithin factor $\\frac{1}{\\sqrt{\\kappa(G)}}$ of the true log-partition function\nfor any discrete pairwise graphical model over graph $G$, where $\\kappa(G) \\in\n(0,1]$ captures how far $G$ is from tree structure with $\\kappa(G) = 1$ for\ntrees and $2/N$ for the complete graph over $N$ vertices. As a consequence, the\napproximation ratio is $1$ for trees, $\\sqrt{(d+1)/2}$ for any graph with\nmaximum average degree $d$, and $\\stackrel{\\beta\\to\\infty}{\\approx}\n1+1/(2\\beta)$ for graphs with girth (shortest cycle) at least $\\beta \\log N$.\nIn general, $\\kappa(G)$ is the solution of a max-min problem associated with\n$G$ that can be evaluated in polynomial time for any graph.\n\nUsing samples from the uniform distribution over the spanning trees of G, we\nprovide a near linear-time variant that achieves an approximation ratio equal\nto the inverse of square-root of minimal (across edges) effective resistance of\nthe graph. We connect our results to the graph partition-based approximation\nmethod and thus provide a unified perspective.\n\nKeywords: variational inference, log-partition function, spanning tree\npolytope, minimum effective resistance, min-max spanning tree, local inference",
          "link": "http://arxiv.org/abs/2102.10196",
          "publishedOn": "2021-08-23T01:36:37.311Z",
          "wordCount": 709,
          "title": "Quantifying Variational Approximation for the Log-Partition Function. (arXiv:2102.10196v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Recent work has shown that the performance of machine learning models can\nvary substantially when models are evaluated on data drawn from a distribution\nthat is close to but different from the training distribution. As a result,\npredicting model performance on unseen distributions is an important challenge.\nOur work connects techniques from domain adaptation and predictive uncertainty\nliterature, and allows us to predict model accuracy on challenging unseen\ndistributions without access to labeled data. In the context of distribution\nshift, distributional distances are often used to adapt models and improve\ntheir performance on new domains, however accuracy estimation, or other forms\nof predictive uncertainty, are often neglected in these investigations. Through\ninvestigating a wide range of established distributional distances, such as\nFrechet distance or Maximum Mean Discrepancy, we determine that they fail to\ninduce reliable estimates of performance under distribution shift. On the other\nhand, we find that the difference of confidences (DoC) of a classifier's\npredictions successfully estimates the classifier's performance change over a\nvariety of shifts. We specifically investigate the distinction between\nsynthetic and natural distribution shifts and observe that despite its\nsimplicity DoC consistently outperforms other quantifications of distributional\ndifference. $DoC$ reduces predictive error by almost half ($46\\%$) on several\nrealistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust\nand ImageNet-Rendition datasets.",
          "link": "http://arxiv.org/abs/2107.03315",
          "publishedOn": "2021-08-23T01:36:37.303Z",
          "wordCount": 697,
          "title": "Predicting with Confidence on Unseen Distributions. (arXiv:2107.03315v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Structured information is an important knowledge source for automatic\nverification of factual claims. Nevertheless, the majority of existing research\ninto this task has focused on textual data, and the few recent inquiries into\nstructured data have been for the closed-domain setting where appropriate\nevidence for each claim is assumed to have already been retrieved. In this\npaper, we investigate verification over structured data in the open-domain\nsetting, introducing a joint reranking-and-verification model which fuses\nevidence documents in the verification component. Our open-domain model\nachieves performance comparable to the closed-domain state-of-the-art on the\nTabFact dataset, and demonstrates performance gains from the inclusion of\nmultiple tables as well as a significant improvement over a heuristic retrieval\nbaseline.",
          "link": "http://arxiv.org/abs/2012.15115",
          "publishedOn": "2021-08-23T01:36:37.296Z",
          "wordCount": 599,
          "title": "Joint Verification and Reranking for Open Fact Checking Over Tables. (arXiv:2012.15115v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuejiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>",
          "description": "Learning socially-aware motion representations is at the core of recent\nadvances in multi-agent problems, such as human motion forecasting and robot\nnavigation in crowds. Despite promising progress, existing representations\nlearned with neural networks still struggle to generalize in closed-loop\npredictions (e.g., output colliding trajectories). This issue largely arises\nfrom the non-i.i.d. nature of sequential prediction in conjunction with\nill-distributed training data. Intuitively, if the training data only comes\nfrom human behaviors in safe spaces, i.e., from \"positive\" examples, it is\ndifficult for learning algorithms to capture the notion of \"negative\" examples\nlike collisions. In this work, we aim to address this issue by explicitly\nmodeling negative examples through self-supervision: (i) we introduce a social\ncontrastive loss that regularizes the extracted motion representation by\ndiscerning the ground-truth positive events from synthetic negative ones; (ii)\nwe construct informative negative samples based on our prior knowledge of rare\nbut dangerous circumstances. Our method substantially reduces the collision\nrates of recent trajectory forecasting, behavioral cloning and reinforcement\nlearning algorithms, outperforming state-of-the-art methods on several\nbenchmarks. Our code is available at https://github.com/vita-epfl/social-nce.",
          "link": "http://arxiv.org/abs/2012.11717",
          "publishedOn": "2021-08-23T01:36:37.274Z",
          "wordCount": 661,
          "title": "Social NCE: Contrastive Learning of Socially-aware Motion Representations. (arXiv:2012.11717v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.",
          "link": "http://arxiv.org/abs/2108.01077",
          "publishedOn": "2021-08-23T01:36:37.267Z",
          "wordCount": 616,
          "title": "Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1\">Jacob M. Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinstadler_B/0/1/0/all/0/1\">Bryn Marie Reinstadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1\">Una-May O&#x27;Reilly</a>",
          "description": "Neural networks are well-known to be vulnerable to imperceptible\nperturbations in the input, called adversarial examples, that result in\nmisclassification. Generating adversarial examples for source code poses an\nadditional challenge compared to the domains of images and natural language,\nbecause source code perturbations must retain the functional meaning of the\ncode. We identify a striking relationship between token frequency statistics\nand learned token embeddings: the L2 norm of learned token embeddings increases\nwith the frequency of the token except for the highest-frequnecy tokens. We\nleverage this relationship to construct a simple and efficient gradient-free\nmethod for generating state-of-the-art adversarial examples on models of code.\nOur method empirically outperforms competing gradient-based methods with less\ninformation and less computational effort.",
          "link": "http://arxiv.org/abs/2009.13562",
          "publishedOn": "2021-08-23T01:36:37.259Z",
          "wordCount": 590,
          "title": "STRATA: Simple, Gradient-Free Attacks for Models of Code. (arXiv:2009.13562v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishad_S/0/1/0/all/0/1\">Sunil Nishad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shubhangi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1\">Sayan Ranu</a>",
          "description": "Majority of the existing graph neural networks (GNN) learn node embeddings\nthat encode their local neighborhoods but not their positions. Consequently,\ntwo nodes that are vastly distant but located in similar local neighborhoods\nmap to similar embeddings in those networks. This limitation prevents accurate\nperformance in predictive tasks that rely on position information. In this\npaper, we develop GraphReach, a position-aware inductive GNN that captures the\nglobal positions of nodes through reachability estimations with respect to a\nset of anchor nodes. The anchors are strategically selected so that\nreachability estimations across all the nodes are maximized. We show that this\ncombinatorial anchor selection problem is NP-hard and, consequently, develop a\ngreedy (1-1/e) approximation heuristic. Empirical evaluation against\nstate-of-the-art GNN architectures reveal that GraphReach provides up to 40%\nrelative improvement in accuracy. In addition, it is more robust to adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2008.09657",
          "publishedOn": "2021-08-23T01:36:37.252Z",
          "wordCount": 632,
          "title": "GraphReach: Position-Aware Graph Neural Network using Reachability Estimations. (arXiv:2008.09657v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oettershagen_L/0/1/0/all/0/1\">Lutz Oettershagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriege_N/0/1/0/all/0/1\">Nils M. Kriege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1\">Christopher Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutzel_P/0/1/0/all/0/1\">Petra Mutzel</a>",
          "description": "Many real-world graphs or networks are temporal, e.g., in a social network\npersons only interact at specific points in time. This information directs\ndissemination processes on the network, such as the spread of rumors, fake\nnews, or diseases. However, the current state-of-the-art methods for supervised\ngraph classification are designed mainly for static graphs and may not be able\nto capture temporal information. Hence, they are not powerful enough to\ndistinguish between graphs modeling different dissemination processes. To\naddress this, we introduce a framework to lift standard graph kernels to the\ntemporal domain. Specifically, we explore three different approaches and\ninvestigate the trade-offs between loss of temporal information and efficiency.\nMoreover, to handle large-scale graphs, we propose stochastic variants of our\nkernels with provable approximation guarantees. We evaluate our methods on a\nwide range of real-world social networks. Our methods beat static kernels by a\nlarge margin in terms of accuracy while still being scalable to large graphs\nand data sets. Hence, we confirm that taking temporal information into account\nis crucial for the successful classification of dissemination processes.",
          "link": "http://arxiv.org/abs/1911.05496",
          "publishedOn": "2021-08-23T01:36:37.245Z",
          "wordCount": 650,
          "title": "Temporal Graph Kernels for Classifying Dissemination Processes. (arXiv:1911.05496v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yunye Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1\">Melinda Gervasio</a>",
          "description": "Existing calibration algorithms address the problem of covariate shift via\nunsupervised domain adaptation. However, these methods suffer from the\nfollowing limitations: 1) they require unlabeled data from the target domain,\nwhich may not be available at the stage of calibration in real-world\napplications and 2) their performance depends heavily on the disparity between\nthe distributions of the source and target domains. To address these two\nlimitations, we present novel calibration solutions via domain generalization.\nOur core idea is to leverage multiple calibration domains to reduce the\neffective distribution disparity between the target and calibration domains for\nimproved calibration transfer without needing any data from the target domain.\nWe provide theoretical justification and empirical experimental results to\ndemonstrate the effectiveness of our proposed algorithms. Compared against\nstate-of-the-art calibration methods designed for domain adaptation, we observe\na decrease of 8.86 percentage points in expected calibration error or,\nequivalently, an increase of 35 percentage points in improvement ratio for\nmulti-class classification on the Office-Home dataset.",
          "link": "http://arxiv.org/abs/2104.00742",
          "publishedOn": "2021-08-23T01:36:37.227Z",
          "wordCount": 635,
          "title": "Confidence Calibration for Domain Generalization under Covariate Shift. (arXiv:2104.00742v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bite Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>",
          "description": "Machine Reading Comprehension (MRC) aims to extract answers to questions\ngiven a passage. It has been widely studied recently, especially in open\ndomains. However, few efforts have been made on closed-domain MRC, mainly due\nto the lack of large-scale training data. In this paper, we introduce a\nmulti-target MRC task for the medical domain, whose goal is to predict answers\nto medical questions and the corresponding support sentences from medical\ninformation sources simultaneously, in order to ensure the high reliability of\nmedical knowledge serving. A high-quality dataset is manually constructed for\nthe purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with\ndetailed analysis conducted. We further propose the Chinese medical BERT model\nfor the task (CMedBERT), which fuses medical knowledge into pre-trained\nlanguage models by the dynamic fusion mechanism of heterogeneous features and\nthe multi-task learning strategy. Experiments show that CMedBERT consistently\noutperforms strong baselines by fusing context-aware and knowledge-aware token\nrepresentations.",
          "link": "http://arxiv.org/abs/2008.10327",
          "publishedOn": "2021-08-23T01:36:37.220Z",
          "wordCount": 637,
          "title": "Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources. (arXiv:2008.10327v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_K/0/1/0/all/0/1\">Kyungjune Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaejun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>",
          "description": "Every recent image-to-image translation model inherently requires either\nimage-level (i.e. input-output pairs) or set-level (i.e. domain labels)\nsupervision. However, even set-level supervision can be a severe bottleneck for\ndata collection in practice. In this paper, we tackle image-to-image\ntranslation in a fully unsupervised setting, i.e., neither paired images nor\ndomain labels. To this end, we propose a truly unsupervised image-to-image\ntranslation model (TUNIT) that simultaneously learns to separate image domains\nand translates input images into the estimated domains. Experimental results\nshow that our model achieves comparable or even better performance than the\nset-level supervised model trained with full labels, generalizes well on\nvarious datasets, and is robust against the choice of hyperparameters (e.g. the\npreset number of pseudo domains). Furthermore, TUNIT can be easily extended to\nsemi-supervised learning with a few labeled data.",
          "link": "http://arxiv.org/abs/2006.06500",
          "publishedOn": "2021-08-23T01:36:37.214Z",
          "wordCount": 607,
          "title": "Rethinking the Truly Unsupervised Image-to-Image Translation. (arXiv:2006.06500v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs. Code is available at\nhttps://github.com/yellowtownhz/STIGCN.",
          "link": "http://arxiv.org/abs/2011.13322",
          "publishedOn": "2021-08-23T01:36:37.207Z",
          "wordCount": 710,
          "title": "Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2011.13322v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.12725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "A fundamental problem in computational chemistry is to find a set of\nreactants to synthesize a target molecule, a.k.a. retrosynthesis prediction.\nExisting state-of-the-art methods rely on matching the target molecule with a\nlarge set of reaction templates, which are very computationally expensive and\nalso suffer from the problem of coverage. In this paper, we propose a novel\ntemplate-free approach called G2Gs by transforming a target molecular graph\ninto a set of reactant molecular graphs. G2Gs first splits the target molecular\ngraph into a set of synthons by identifying the reaction centers, and then\ntranslates the synthons to the final reactant graphs via a variational graph\ntranslation framework. Experimental results show that G2Gs significantly\noutperforms existing template-free approaches by up to 63% in terms of the\ntop-1 accuracy and achieves a performance close to that of state-of-the-art\ntemplate based approaches, but does not require domain knowledge and is much\nmore scalable.",
          "link": "http://arxiv.org/abs/2003.12725",
          "publishedOn": "2021-08-23T01:36:37.200Z",
          "wordCount": 628,
          "title": "A Graph to Graphs Framework for Retrosynthesis Prediction. (arXiv:2003.12725v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.10167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaohua Hu</a>",
          "description": "We propose a general formulation of nonconvex and nonsmooth sparse\noptimization problems with convex set constraint, which can take into account\nmost existing types of nonconvex sparsity-inducing terms, bringing strong\napplicability to a wide range of applications. We design a general algorithmic\nframework of iteratively reweighted algorithms for solving the proposed\nnonconvex and nonsmooth sparse optimization problems, which solves a sequence\nof weighted convex regularization problems with adaptively updated weights.\nFirst-order optimality condition is derived and global convergence results are\nprovided under loose assumptions, making our theoretical results a practical\ntool for analyzing a family of various reweighted algorithms. The effectiveness\nand efficiency of our proposed formulation and the algorithms are demonstrated\nin numerical experiments on various sparse optimization problems.",
          "link": "http://arxiv.org/abs/1810.10167",
          "publishedOn": "2021-08-23T01:36:37.183Z",
          "wordCount": 596,
          "title": "Nonconvex and Nonsmooth Sparse Optimization via Adaptively Iterative Reweighted Methods. (arXiv:1810.10167v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.07348",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Trosset_M/0/1/0/all/0/1\">Michael W. Trosset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1\">Mingyue Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1\">Minh Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "A random dot product graph (RDPG) is a generative model for networks in which\nvertices correspond to positions in a latent Euclidean space and edge\nprobabilities are determined by the dot products of the latent positions. We\nconsider RDPGs for which the latent positions are randomly sampled from an\nunknown $1$-dimensional submanifold of the latent space. In principle,\nrestricted inference, i.e., procedures that exploit the structure of the\nsubmanifold, should be more effective than unrestricted inference; however, it\nis not clear how to conduct restricted inference when the submanifold is\nunknown. We submit that techniques for manifold learning can be used to learn\nthe unknown submanifold well enough to realize benefit from restricted\ninference. To illustrate, we test $1$- and $2$-sample hypotheses about the\nFr\\'{e}chet means of small communities of vertices, using the complete set of\nvertices to infer latent structure. We propose test statistics that deploy the\nIsomap procedure for manifold learning, using shortest path distances on\nneighborhood graphs constructed from estimated latent positions to estimate arc\nlengths on the unknown $1$-dimensional submanifold. Unlike conventional\napplications of Isomap, the estimated latent positions do not lie on the\nsubmanifold of interest. We extend existing convergence results for Isomap to\nthis setting and use them to demonstrate that, as the number of auxiliary\nvertices increases, the power of our test converges to the power of the\ncorresponding test when the submanifold is known. Finally, we apply our methods\nto an inference problem that arises in studying the connectome of the\nDrosophila larval mushroom body. The univariate learnt manifold test rejects\n($p<0.05$), while the multivariate ambient space test does not ($p\\gg0.05$),\nillustrating the value of identifying and exploiting low-dimensional structure\nfor subsequent inference.",
          "link": "http://arxiv.org/abs/2004.07348",
          "publishedOn": "2021-08-23T01:36:37.175Z",
          "wordCount": 773,
          "title": "Learning 1-Dimensional Submanifolds for Subsequent Inference on Random Dot Product Graphs. (arXiv:2004.07348v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_M/0/1/0/all/0/1\">Mandana Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1\">Gregory Kiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1\">Tristan Glatard</a>",
          "description": "Scientific datasets and analysis pipelines are increasingly being shared\npublicly in the interest of open science. However, mechanisms are lacking to\nreliably identify which pipelines and datasets can appropriately be used\ntogether. Given the increasing number of high-quality public datasets and\npipelines, this lack of clear compatibility threatens the findability and\nreusability of these resources. We investigate the feasibility of a\ncollaborative filtering system to recommend pipelines and datasets based on\nprovenance records from previous executions. We evaluate our system using\ndatasets and pipelines extracted from the Canadian Open Neuroscience Platform,\na national initiative for open neuroscience. The recommendations provided by\nour system (AUC$=0.83$) are significantly better than chance and outperform\nrecommendations made by domain experts using their previous knowledge as well\nas pipeline and dataset descriptions (AUC$=0.63$). In particular, domain\nexperts often neglect low-level technical aspects of a pipeline-dataset\ninteraction, such as the level of pre-processing, which are captured by a\nprovenance-based system. We conclude that provenance-based pipeline and dataset\nrecommenders are feasible and beneficial to the sharing and usage of\nopen-science resources. Future work will focus on the collection of more\ncomprehensive provenance traces, and on deploying the system in production.",
          "link": "http://arxiv.org/abs/2108.09275",
          "publishedOn": "2021-08-23T01:36:37.168Z",
          "wordCount": 626,
          "title": "A Recommender System for Scientific Datasets and Analysis Pipelines. (arXiv:2108.09275v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Soumava Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_G/0/1/0/all/0/1\">Gurunath Reddy M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">K Sreenivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Pratim Das</a>",
          "description": "Singing Voice Detection (SVD) has been an active area of research in music\ninformation retrieval (MIR). Currently, two deep neural network-based methods,\none based on CNN and the other on RNN, exist in literature that learn optimized\nfeatures for the voice detection (VD) task and achieve state-of-the-art\nperformance on common datasets. Both these models have a huge number of\nparameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for\ndeployment on devices like smartphones or embedded sensors with limited\ncapacity in terms of memory and computation power. The most popular method to\naddress this issue is known as knowledge distillation in deep learning\nliterature (in addition to model compression) where a large pre-trained network\nknown as the teacher is used to train a smaller student network. Given the wide\napplications of SVD in music information retrieval, to the best of our\nknowledge, model compression for practical deployment has not yet been\nexplored. In this paper, efforts have been made to investigate this issue using\nboth conventional as well as ensemble knowledge distillation techniques.",
          "link": "http://arxiv.org/abs/2011.04297",
          "publishedOn": "2021-08-23T01:36:37.162Z",
          "wordCount": 656,
          "title": "Knowledge Distillation for Singing Voice Detection. (arXiv:2011.04297v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shantanu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childers_D/0/1/0/all/0/1\">David Childers</a>",
          "description": "Researchers often face data fusion problems, where multiple data sources are\navailable, each capturing a distinct subset of variables. While problem\nformulations typically take the data as given, in practice, data acquisition\ncan be an ongoing process. In this paper, we aim to estimate any functional of\na probabilistic model (e.g., a causal effect) as efficiently as possible, by\ndeciding, at each time, which data source to query. We propose online moment\nselection (OMS), a framework in which structural assumptions are encoded as\nmoment conditions. The optimal action at each step depends, in part, on the\nvery moments that identify the functional of interest. Our algorithms balance\nexploration with choosing the best action as suggested by current estimates of\nthe moments. We propose two selection strategies: (1) explore-then-commit\n(OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero\nasymptotic regret as assessed by MSE. We instantiate our setup for average\ntreatment effect estimation, where structural assumptions are given by a causal\ngraph and data sources may include subsets of mediators, confounders, and\ninstrumental variables.",
          "link": "http://arxiv.org/abs/2108.09265",
          "publishedOn": "2021-08-23T01:36:37.153Z",
          "wordCount": 609,
          "title": "Efficient Online Estimation of Causal Effects by Deciding What to Observe. (arXiv:2108.09265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>",
          "description": "In this paper, we introduce Apollo, a quasi-Newton method for nonconvex\nstochastic optimization, which dynamically incorporates the curvature of the\nloss function by approximating the Hessian via a diagonal matrix. Importantly,\nthe update and storage of the diagonal approximation of Hessian is as efficient\nas adaptive first-order optimization methods with linear complexity for both\ntime and memory. To handle nonconvexity, we replace the Hessian with its\nrectified absolute value, which is guaranteed to be positive-definite.\nExperiments on three tasks of vision and language show that Apollo achieves\nsignificant improvements over other stochastic optimization methods, including\nSGD and variants of Adam, in term of both convergence speed and generalization\nperformance. The implementation of the algorithm is available at\nhttps://github.com/XuezheMax/apollo.",
          "link": "http://arxiv.org/abs/2009.13586",
          "publishedOn": "2021-08-23T01:36:37.135Z",
          "wordCount": 631,
          "title": "Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization. (arXiv:2009.13586v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hutter_A/0/1/0/all/0/1\">Adrian Hutter</a>",
          "description": "We consider a scenario in which two reinforcement learning agents repeatedly\nplay a matrix game against each other and update their parameters after each\nround. The agents' decision-making is transparent to each other, which allows\neach agent to predict how their opponent will play against them. To prevent an\ninfinite regress of both agents recursively predicting each other indefinitely,\neach agent is required to give an opponent-independent response with some\nprobability at least epsilon. Transparency also allows each agent to anticipate\nand shape the other agent's gradient step, i.e. to move to regions of parameter\nspace in which the opponent's gradient points in a direction favourable to\nthem. We study the resulting dynamics experimentally, using two algorithms from\nprevious literature (LOLA and SOS) for opponent-aware learning. We find that\nthe combination of mutually transparent decision-making and opponent-aware\nlearning robustly leads to mutual cooperation in a single-shot prisoner's\ndilemma. In a game of chicken, in which both agents try to manoeuvre their\nopponent towards their preferred equilibrium, converging to a mutually\nbeneficial outcome turns out to be much harder, and opponent-aware learning can\neven lead to worst-case outcomes for both agents. This highlights the need to\ndevelop opponent-aware learning algorithms that achieve acceptable outcomes in\nsocial dilemmas involving an equilibrium selection problem.",
          "link": "http://arxiv.org/abs/2012.02671",
          "publishedOn": "2021-08-23T01:36:37.127Z",
          "wordCount": 679,
          "title": "Learning in two-player games between transparent opponents. (arXiv:2012.02671v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Sanchez_P/0/1/0/all/0/1\">Pedro A. Moreno-Sanchez</a>",
          "description": "Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing\nincidence and high cost to health systems. A delayed recognition implies\npremature mortality due to progressive loss of kidney function. The employment\nof data mining to discover subtle patterns in CKD indicators would contribute\nachieving early diagnosis. This work presents the development and evaluation of\nan explainable prediction model that would support clinicians in the early\ndiagnosis of CKD patients. The model development is based on a data management\npipeline that detects the best combination of ensemble trees algorithms and\nfeatures selected concerning classification performance. The results obtained\nthrough the pipeline equals the performance of the best CKD prediction models\nidentified in the literature. Furthermore, the main contribution of the paper\ninvolves an explainability-driven approach that allows selecting the best\nprediction model maintaining a balance between accuracy and explainability.\nTherefore, the most balanced explainable prediction model of CKD implements an\nXGBoost classifier over a group of 4 features (packed cell value, specific\ngravity, albumin, and hypertension), achieving an accuracy of 98.9% and 97.5%\nwith cross-validation technique and with new unseen data respectively. In\naddition, by analysing the model's explainability by means of different\npost-hoc techniques, the packed cell value and the specific gravity are\ndetermined as the most relevant features that influence the prediction results\nof the model. This small number of feature selected results in a reduced cost\nof the early diagnosis of CKD implying a promising solution for developing\ncountries.",
          "link": "http://arxiv.org/abs/2105.10368",
          "publishedOn": "2021-08-23T01:36:37.121Z",
          "wordCount": 713,
          "title": "Development and evaluation of an Explainable Prediction Model for Chronic Kidney Disease Patients based on Ensemble Trees. (arXiv:2105.10368v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guangyin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ziyou Gao</a>",
          "description": "Urban rail transit (URT) system plays a dominating role in many megacities\nlike Beijing and Hong Kong. Due to its important role and complex nature, it is\nalways in great need for public agencies to better understand the performance\nof the URT system. This paper focuses on an essential and hard problem to\nestimate the network-wide link travel time and station waiting time using the\nautomatic fare collection (AFC) data in the URT system, which is beneficial to\nbetter understand the system-wide real-time operation state. The emerging\ndata-driven techniques, such as computational graph (CG) models in the machine\nlearning field, provide a new solution for solving this problem. In this study,\nwe first formulate a data-driven estimation optimization framework to estimate\nthe link travel time and station waiting time. Then, we cast the estimation\noptimization model into a CG framework to solve the optimization problem and\nobtain the estimation results. The methodology is verified on a synthetic URT\nnetwork and applied to a real-world URT network using the synthetic and\nreal-world AFC data, respectively. Results show the robustness and\neffectiveness of the CG-based framework. To the best of our knowledge, this is\nthe first time that the CG is applied to the URT. This study can provide\ncritical insights to better understand the operational state in URT.",
          "link": "http://arxiv.org/abs/2108.09292",
          "publishedOn": "2021-08-23T01:36:37.114Z",
          "wordCount": 681,
          "title": "Network-wide link travel time and station waiting time estimation using automatic fare collection data: A computational graph approach. (arXiv:2108.09292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Srivatsan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zishen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_K/0/1/0/all/0/1\">Kshitij Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuman_S/0/1/0/all/0/1\">Sabrina Neuman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>",
          "description": "Building domain-specific accelerators for autonomous unmanned aerial vehicles\n(UAVs) is challenging due to a lack of systematic methodology for designing\nonboard compute. Balancing a computing system for a UAV requires considering\nboth the cyber (e.g., sensor rate, compute performance) and physical (e.g.,\npayload weight) characteristics that affect overall performance. Iterating over\nthe many component choices results in a combinatorial explosion of the number\nof possible combinations: from 10s of thousands to billions, depending on\nimplementation details. Manually selecting combinations of these components is\ntedious and expensive. To navigate the {cyber-physical design space}\nefficiently, we introduce \\emph{AutoPilot}, a framework that automates\nfull-system UAV co-design. AutoPilot uses Bayesian optimization to navigate a\nlarge design space and automatically select a combination of autonomy algorithm\nand hardware accelerator while considering the cross-product effect of other\ncyber and physical UAV components. We show that the AutoPilot methodology\nconsistently outperforms general-purpose hardware selections like Xavier NX and\nJetson TX2, as well as dedicated hardware accelerators built for autonomous\nUAVs, across a range of representative scenarios (three different UAV types and\nthree deployment environments). Designs generated by AutoPilot increase the\nnumber of missions on average by up to 2.25x, 1.62x, and 1.43x for nano, micro,\nand mini-UAVs respectively over baselines. Our work demonstrates the need for\nholistic full-UAV co-design to achieve maximum overall UAV performance and the\nneed for automated flows to simplify the design process for autonomous\ncyber-physical systems.",
          "link": "http://arxiv.org/abs/2102.02988",
          "publishedOn": "2021-08-23T01:36:37.108Z",
          "wordCount": 721,
          "title": "AutoPilot: Automating SoC Design Space Exploration for SWaP Constrained Autonomous UAVs. (arXiv:2102.02988v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>",
          "description": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time-sequential data such as speech recognition. However, it is\ndifficult to deploy these networks on hardware to achieve high throughput and\nlow latency because the fully connected structure makes LSTM networks a\nmemory-bounded algorithm. Previous LSTM accelerators either exploited weight\nspatial sparsity or temporal activation sparsity. This paper proposes a new\naccelerator called \"Spartus\" that exploits spatio-temporal sparsity to achieve\nultra-low latency inference. The spatial sparsity is induced using our proposed\npruning method called Column-Balanced Targeted Dropout (CBTD), which structures\nsparse weight matrices for balanced workload. It achieved up to 96% weight\nsparsity with negligible accuracy difference for an LSTM network trained on a\nTIMIT phone recognition task. To induce temporal sparsity in LSTM, we create\nthe DeltaLSTM by extending the previous DeltaGRU method to the LSTM network.\nThis combined sparsity simultaneously saves on the weight memory access and\nassociated arithmetic operations. Spartus was implemented on a Xilinx Zynq-7100\nFPGA. The Spartus per-sample latency for a single DeltaLSTM layer of 1024\nneurons averages 1 us. Spartus achieved 9.4 TOp/s effective batch-1 throughput\nand 1.1 TOp/J energy efficiency, which, respectively, are 4X and 7X higher than\nthe previous state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.02297",
          "publishedOn": "2021-08-23T01:36:37.101Z",
          "wordCount": 677,
          "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v3 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09432",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1\">Sanjaya Lohani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1\">Thomas A. Searles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1\">Brian T. Kirby</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1\">Ryan T. Glasser</a>",
          "description": "We determine the resource scaling of machine learning-based quantum state\nreconstruction methods, in terms of inference and training, for systems of up\nto four qubits when constrained to pure states. Further, we examine system\nperformance in the low-count regime, likely to be encountered in the tomography\nof high-dimensional systems. Finally, we implement our quantum state\nreconstruction method on an IBM Q quantum computer, and compare against both\nunconstrained and constrained MLE state reconstruction.",
          "link": "http://arxiv.org/abs/2012.09432",
          "publishedOn": "2021-08-23T01:36:37.083Z",
          "wordCount": 547,
          "title": "On the experimental feasibility of quantum state reconstruction via machine learning. (arXiv:2012.09432v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tuanfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>",
          "description": "Many important real-world applications involve time-series data with skewed\ndistribution. Compared to conventional imbalance learning problems, the\nclassification of imbalanced time-series data is more challenging due to high\ndimensionality and high inter-variable correlation. This paper proposes a\nstructure preserving Oversampling method to combat the High-dimensional\nImbalanced Time-series classification (OHIT). OHIT first leverages a\ndensity-ratio based shared nearest neighbor clustering algorithm to capture the\nmodes of minority class in high-dimensional space. It then for each mode\napplies the shrinkage technique of large-dimensional covariance matrix to\nobtain accurate and reliable covariance structure. Finally, OHIT generates the\nstructure-preserving synthetic samples based on multivariate Gaussian\ndistribution by using the estimated covariance matrices. Experimental results\non several publicly available time-series datasets (including unimodal and\nmultimodal) demonstrate the superiority of OHIT against the state-of-the-art\noversampling algorithms in terms of F1, G-mean, and AUC.",
          "link": "http://arxiv.org/abs/2004.06373",
          "publishedOn": "2021-08-23T01:36:37.077Z",
          "wordCount": 622,
          "title": "Minority Oversampling for Imbalanced Time Series Classification. (arXiv:2004.06373v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10053",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Esler_S/0/1/0/all/0/1\">Samuel Esler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Thiruvady_D/0/1/0/all/0/1\">Dhananjay Thiruvady</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ernst_A/0/1/0/all/0/1\">Andreas T. Ernst</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Morgan_K/0/1/0/all/0/1\">Kerri Morgan</a>",
          "description": "We investigate an important research question for solving the car sequencing\nproblem, that is, which characteristics make an instance hard to solve? To do\nso, we carry out an instance space analysis for the car sequencing problem, by\nextracting a vector of problem features to characterize an instance. In order\nto visualize the instance space, the feature vectors are projected onto a\ntwo-dimensional space using dimensionality reduction techniques. The resulting\ntwo-dimensional visualizations provide new insights into the characteristics of\nthe instances used for testing and how these characteristics influence the\nbehaviours of an optimization algorithm. This analysis guides us in\nconstructing a new set of benchmark instances with a range of instance\nproperties. We demonstrate that these new instances are more diverse than the\nprevious benchmarks, including some instances that are significantly more\ndifficult to solve. We introduce two new algorithms for solving the car\nsequencing problem and compare them with four existing methods from the\nliterature. Our new algorithms are shown to perform competitively for this\nproblem but no single algorithm can outperform all others over all instances.\nThis observation motivates us to build an algorithm selection model based on\nmachine learning, to identify the niche in the instance space that an algorithm\nis expected to perform well on. Our analysis helps to understand problem\nhardness and select an appropriate algorithm for solving a given car sequencing\nproblem instance.",
          "link": "http://arxiv.org/abs/2012.10053",
          "publishedOn": "2021-08-23T01:36:37.070Z",
          "wordCount": 692,
          "title": "Instance Space Analysis for the Car Sequencing Problem. (arXiv:2012.10053v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourmirzaei_M/0/1/0/all/0/1\">Mahdi Pourmirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montazer_G/0/1/0/all/0/1\">Gholam Ali Montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaili_F/0/1/0/all/0/1\">Farzaneh Esmaili</a>",
          "description": "Over the past few years, best SSL methods, gradually moved from the pre-text\ntask learning to the Contrastive learning. But contrastive methods have some\ndrawbacks which could not be solved completely, such as performing poor on\nfine-grained visual tasks compare to supervised learning methods. In this\nstudy, at first, the impact of ImageNet pre-training on fine-grained Facial\nExpression Recognition (FER) was tested. It could be seen from the results that\ntraining from scratch is better than ImageNet fine-tuning at stronger\naugmentation levels. After that, a framework was proposed for standard\nSupervised Learning (SL), called Hybrid Multi-Task Learning (HMTL) which merged\nSelf-Supervised as auxiliary task to the SL training setting. Leveraging\nSelf-Supervised Learning (SSL) can gain additional information from input data\nthan labels which can help the main fine-grained SL task. It is been\ninvestigated how this method could be used for FER by designing two customized\nversion of common pre-text techniques, Jigsaw puzzling and in-painting. The\nstate-of-the-art was reached on AffectNet via two types of HMTL, without\nutilizing pre-training on additional datasets. Moreover, we showed the\ndifference between SS pre-training and HMTL to demonstrate superiority of\nproposed method. Furthermore, the impact of proposed method was shown on two\nother fine-grained facial tasks, Head Poses estimation and Gender Recognition,\nwhich concluded to reduce in error rate by 11% and 1% respectively.",
          "link": "http://arxiv.org/abs/2105.06421",
          "publishedOn": "2021-08-23T01:36:37.064Z",
          "wordCount": 687,
          "title": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation. (arXiv:2105.06421v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Learning accurate low-dimensional embeddings for a network is a crucial task\nas it facilitates many downstream network analytics tasks. For large networks,\nthe trained embeddings often require a significant amount of space to store,\nmaking storage and processing a challenge. Building on our previous work on\nsemi-supervised network embedding, we develop d-SNEQ, a differentiable\nDNN-based quantisation method for network embedding. d-SNEQ incorporates a rank\nloss to equip the learned quantisation codes with rich high-order information\nand is able to substantially compress the size of trained embeddings, thus\nreducing storage footprint and accelerating retrieval speed. We also propose a\nnew evaluation metric, path prediction, to fairly and more directly evaluate\nmodel performance on the preservation of high-order information. Our evaluation\non four real-world networks of diverse characteristics shows that d-SNEQ\noutperforms a number of state-of-the-art embedding methods in link prediction,\npath prediction, node classification, and node recommendation while being far\nmore space- and time-efficient.",
          "link": "http://arxiv.org/abs/2108.09128",
          "publishedOn": "2021-08-23T01:36:36.903Z",
          "wordCount": 586,
          "title": "Semi-supervised Network Embedding with Differentiable Deep Quantisation. (arXiv:2108.09128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09262",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vakili_S/0/1/0/all/0/1\">Sattar Vakili</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bouziani_N/0/1/0/all/0/1\">Nacime Bouziani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jalali_S/0/1/0/all/0/1\">Sepehr Jalali</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bernacchia_A/0/1/0/all/0/1\">Alberto Bernacchia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shiu_D/0/1/0/all/0/1\">Da-shan Shiu</a>",
          "description": "Consider the sequential optimization of a continuous, possibly non-convex,\nand expensive to evaluate objective function $f$. The problem can be cast as a\nGaussian Process (GP) bandit where $f$ lives in a reproducing kernel Hilbert\nspace (RKHS). The state of the art analysis of several learning algorithms\nshows a significant gap between the lower and upper bounds on the simple regret\nperformance. When $N$ is the number of exploration trials and $\\gamma_N$ is the\nmaximal information gain, we prove an $\\tilde{\\mathcal{O}}(\\sqrt{\\gamma_N/N})$\nbound on the simple regret performance of a pure exploration algorithm that is\nsignificantly tighter than the existing bounds. We show that this bound is\norder optimal up to logarithmic factors for the cases where a lower bound on\nregret is known. To establish these results, we prove novel and sharp\nconfidence intervals for GP models applicable to RKHS elements which may be of\nbroader interest.",
          "link": "http://arxiv.org/abs/2108.09262",
          "publishedOn": "2021-08-23T01:36:36.887Z",
          "wordCount": 582,
          "title": "Optimal Order Simple Regret for Gaussian Process Bandits. (arXiv:2108.09262v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mahdi Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousat_B/0/1/0/all/0/1\">Behzad Ousat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1\">Mahdi Jafari Siavoshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahangir_A/0/1/0/all/0/1\">Amir Hossein Jahangir</a>",
          "description": "The intrusion detection system (IDS) is an essential element of security\nmonitoring in computer networks. An IDS distinguishes the malicious traffic\nfrom the benign one and determines the attack types targeting the assets of the\norganization. The main challenge of an IDS is facing new (i.e., zero-day)\nattacks and separating them from benign traffic and existing types of attacks.\nAlong with the power of the deep learning-based IDSes in auto-extracting\nhigh-level features and its independence from the time-consuming and costly\nsignature extraction process, the mentioned challenge still exists in this new\ngeneration of IDSes.\n\nIn this paper, we propose a framework for deep learning-based IDSes\naddressing new attacks. This framework is the first approach using both deep\nnovelty-based classifiers besides the traditional clustering based on the\nspecialized layer of deep structures, in the security scope. Additionally, we\nintroduce DOC++ as a newer version of DOC as a deep novelty-based classifier.\nWe also employ the Deep Intrusion Detection (DID) framework for the\npreprocessing phase, which improves the ability of deep learning algorithms to\ndetect content-based attacks. We compare four different algorithms (including\nDOC, DOC++, OpenMax, and AutoSVM) as the novelty classifier of the framework\nand use both the CIC-IDS2017 and CSE-CIC-IDS2018 datasets for the evaluation.\nOur results show that DOC++ is the best implementation of the open set\nrecognition module. Besides, the completeness and homogeneity of the clustering\nand post-training phase prove that this model is good enough for the supervised\nlabeling and updating phase.",
          "link": "http://arxiv.org/abs/2108.09199",
          "publishedOn": "2021-08-23T01:36:36.881Z",
          "wordCount": 693,
          "title": "An Adaptable Deep Learning-Based Intrusion Detection System to Zero-Day Attacks. (arXiv:2108.09199v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Renhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Du Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiewen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hangchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zekun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibasaki_R/0/1/0/all/0/1\">Ryosuke Shibasaki</a>",
          "description": "Nowadays, with the rapid development of IoT (Internet of Things) and CPS\n(Cyber-Physical Systems) technologies, big spatiotemporal data are being\ngenerated from mobile phones, car navigation systems, and traffic sensors. By\nleveraging state-of-the-art deep learning technologies on such data, urban\ntraffic prediction has drawn a lot of attention in AI and Intelligent\nTransportation System community. The problem can be uniformly modeled with a 3D\ntensor (T, N, C), where T denotes the total time steps, N denotes the size of\nthe spatial domain (i.e., mesh-grids or graph-nodes), and C denotes the\nchannels of information. According to the specific modeling strategy, the\nstate-of-the-art deep learning models can be divided into three categories:\ngrid-based, graph-based, and multivariate time-series models. In this study, we\nfirst synthetically review the deep traffic models as well as the widely used\ndatasets, then build a standard benchmark to comprehensively evaluate their\nperformances with the same settings and metrics. Our study named DL-Traff is\nimplemented with two most popular deep learning frameworks, i.e., TensorFlow\nand PyTorch, which is already publicly available as two GitHub repositories\nhttps://github.com/deepkashiwa20/DL-Traff-Grid and\nhttps://github.com/deepkashiwa20/DL-Traff-Graph. With DL-Traff, we hope to\ndeliver a useful resource to researchers who are interested in spatiotemporal\ndata analysis.",
          "link": "http://arxiv.org/abs/2108.09091",
          "publishedOn": "2021-08-23T01:36:36.868Z",
          "wordCount": 664,
          "title": "DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction. (arXiv:2108.09091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiacheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiacheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Recently, some works found an interesting phenomenon that adversarially\nrobust classifiers can generate good images comparable to generative models. We\ninvestigate this phenomenon from an energy perspective and provide a novel\nexplanation. We reformulate adversarial example generation, adversarial\ntraining, and image generation in terms of an energy function. We find that\nadversarial training contributes to obtaining an energy function that is flat\nand has low energy around the real data, which is the key for generative\ncapability. Based on our new understanding, we further propose a better\nadversarial training method, Joint Energy Adversarial Training (JEAT), which\ncan generate high-quality images and achieve new state-of-the-art robustness\nunder a wide range of attacks. The Inception Score of the images (CIFAR-10)\ngenerated by JEAT is 8.80, much better than original robust classifiers (7.50).\nIn particular, we achieve new state-of-the-art robustness on CIFAR-10 (from\n57.20% to 62.04%) and CIFAR-100 (from 30.03% to 30.18%) without extra training\ndata.",
          "link": "http://arxiv.org/abs/2108.09093",
          "publishedOn": "2021-08-23T01:36:36.861Z",
          "wordCount": 602,
          "title": "Towards Understanding the Generative Capability of Adversarially Robust Classifiers. (arXiv:2108.09093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1\">Ranjie Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1\">A. K. Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>",
          "description": "Human can easily recognize visual objects with lost information: even losing\nmost details with only contour reserved, e.g. cartoon. However, in terms of\nvisual perception of Deep Neural Networks (DNNs), the ability for recognizing\nabstract objects (visual objects with lost information) is still a challenge.\nIn this work, we investigate this issue from an adversarial viewpoint: will the\nperformance of DNNs decrease even for the images only losing a little\ninformation? Towards this end, we propose a novel adversarial attack, named\n\\textit{AdvDrop}, which crafts adversarial examples by dropping existing\ninformation of images. Previously, most adversarial attacks add extra\ndisturbing information on clean images explicitly. Opposite to previous works,\nour proposed work explores the adversarial robustness of DNN models in a novel\nperspective by dropping imperceptible details to craft adversarial examples. We\ndemonstrate the effectiveness of \\textit{AdvDrop} by extensive experiments, and\nshow that this new type of adversarial examples is more difficult to be\ndefended by current defense systems.",
          "link": "http://arxiv.org/abs/2108.09034",
          "publishedOn": "2021-08-23T01:36:36.853Z",
          "wordCount": 619,
          "title": "AdvDrop: Adversarial Attack to DNNs by Dropping Information. (arXiv:2108.09034v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weicong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hanlin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingshuo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guangxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qiang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xuezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dongying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qiao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>",
          "description": "Real-world recommendation systems often consist of two phases. In the first\nphase, multiple predictive models produce the probability of different\nimmediate user actions. In the second phase, these predictions are aggregated\naccording to a set of 'strategic parameters' to meet a diverse set of business\ngoals, such as longer user engagement, higher revenue potential, or more\ncommunity/network interactions. In addition to building accurate predictive\nmodels, it is also crucial to optimize this set of 'strategic parameters' so\nthat primary goals are optimized while secondary guardrails are not hurt. In\nthis setting with multiple and constrained goals, this paper discovers that a\nprobabilistic strategic parameter regime can achieve better value compared to\nthe standard regime of finding a single deterministic parameter. The new\nprobabilistic regime is to learn the best distribution over strategic parameter\nchoices and sample one strategic parameter from the distribution when each user\nvisits the platform. To pursue the optimal probabilistic solution, we formulate\nthe problem into a stochastic compositional optimization problem, in which the\nunbiased stochastic gradient is unavailable. Our approach is applied in a\npopular social network platform with hundreds of millions of daily users and\nachieves +0.22% lift of user engagement in a recommendation task and +1.7% lift\nin revenue in an advertising optimization scenario comparing to using the best\ndeterministic parameter strategy.",
          "link": "http://arxiv.org/abs/2108.09076",
          "publishedOn": "2021-08-23T01:36:36.836Z",
          "wordCount": 693,
          "title": "PASTO: Strategic Parameter Optimization in Recommendation Systems -- Probabilistic is Better than Deterministic. (arXiv:2108.09076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Howard H. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Deshun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "Implementing federated learning (FL) algorithms in wireless networks has\ngarnered a wide range of attention. However, few works have considered the\nimpact of user mobility on the learning performance. To fill this research gap,\nfirstly, we develop a theoretical model to characterize the hierarchical\nfederated learning (HFL) algorithm in wireless networks where the mobile users\nmay roam across multiple edge access points, leading to incompletion of\ninconsistent FL training. Secondly, we provide the convergence analysis of HFL\nwith user mobility. Our analysis proves that the learning performance of HFL\ndeteriorates drastically with highly-mobile users. And this decline in the\nlearning performance will be exacerbated with small number of participants and\nlarge data distribution divergences among local data of users. To circumvent\nthese issues, we propose a mobility-aware cluster federated learning (MACFL)\nalgorithm by redesigning the access mechanism, local update rule and model\naggregation scheme. Finally, we provide experiments to evaluate the learning\nperformance of HFL and our MACFL. The results show that our MACFL can enhance\nthe learning performance, especially for three different cases, namely, the\ncase of users with non-independent and identical distribution data, the case of\nusers with high mobility, and the cases with a small number of users.",
          "link": "http://arxiv.org/abs/2108.09103",
          "publishedOn": "2021-08-23T01:36:36.828Z",
          "wordCount": 650,
          "title": "Mobility-Aware Cluster Federated Learning in Hierarchical Wireless Networks. (arXiv:2108.09103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_L/0/1/0/all/0/1\">Lin William Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>",
          "description": "We predict asset returns and measure risk premia using a prominent technique\nfrom artificial intelligence -- deep sequence modeling. Because asset returns\noften exhibit sequential dependence that may not be effectively captured by\nconventional time series models, sequence modeling offers a promising path with\nits data-driven approach and superior performance. In this paper, we first\noverview the development of deep sequence models, introduce their applications\nin asset pricing, and discuss their advantages and limitations. We then perform\na comparative analysis of these methods using data on U.S. equities. We\ndemonstrate how sequence modeling benefits investors in general through\nincorporating complex historical path dependence, and that Long- and Short-term\nMemory (LSTM) based models tend to have the best out-of-sample performance.",
          "link": "http://arxiv.org/abs/2108.08999",
          "publishedOn": "2021-08-23T01:36:36.821Z",
          "wordCount": 560,
          "title": "Deep Sequence Modeling: Development and Applications in Asset Pricing. (arXiv:2108.08999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabbani_T/0/1/0/all/0/1\">Tahseen Rabbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Apollo Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_A/0/1/0/all/0/1\">Arjun Rajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "The power method is a classical algorithm with broad applications in machine\nlearning tasks, including streaming PCA, spectral clustering, and low-rank\nmatrix approximation. The distilled purpose of the vanilla power method is to\ndetermine the largest eigenvalue (in absolute modulus) and its eigenvector of a\nmatrix. A momentum-based scheme can be used to accelerate the power method, but\nachieving an optimal convergence rate with existing algorithms critically\nrelies on additional spectral information that is unavailable at run-time, and\nsub-optimal initializations can result in divergence. In this paper, we provide\na pair of novel momentum-based power methods, which we call the delayed\nmomentum power method (DMPower) and a streaming variant, the delayed momentum\nstreaming method (DMStream). Our methods leverage inexact deflation and are\ncapable of achieving near-optimal convergence with far less restrictive\nhyperparameter requirements. We provide convergence analyses for both\nalgorithms through the lens of perturbation theory. Further, we experimentally\ndemonstrate that DMPower routinely outperforms the vanilla power method and\nthat both algorithms match the convergence speed of an oracle running existing\naccelerated methods with perfect spectral knowledge.",
          "link": "http://arxiv.org/abs/2108.09264",
          "publishedOn": "2021-08-23T01:36:36.814Z",
          "wordCount": 608,
          "title": "Practical and Fast Momentum-Based Power Methods. (arXiv:2108.09264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiawei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1\">Yuhan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Weiwei Tu</a>",
          "description": "Tabular data prediction (TDP) is one of the most popular industrial\napplications, and various methods have been designed to improve the prediction\nperformance. However, existing works mainly focus on feature interactions and\nignore sample relations, e.g., users with the same education level might have a\nsimilar ability to repay the debt. In this work, by explicitly and\nsystematically modeling sample relations, we propose a novel framework TabGNN\nbased on recently popular graph neural networks (GNN). Specifically, we firstly\nconstruct a multiplex graph to model the multifaceted sample relations, and\nthen design a multiplex graph neural network to learn enhanced representation\nfor each sample. To integrate TabGNN with the tabular solution in our company,\nwe concatenate the learned embeddings and the original ones, which are then fed\nto prediction models inside the solution. Experiments on eleven TDP datasets\nfrom various domains, including classification and regression ones, show that\nTabGNN can consistently improve the performance compared to the tabular\nsolution AutoFE in 4Paradigm.",
          "link": "http://arxiv.org/abs/2108.09127",
          "publishedOn": "2021-08-23T01:36:36.798Z",
          "wordCount": 596,
          "title": "TabGNN: Multiplex Graph Neural Network for Tabular Data Prediction. (arXiv:2108.09127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1\">Ege Erdogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1\">Alptekin Kupcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_A/0/1/0/all/0/1\">A. Ercument Cicek</a>",
          "description": "Training deep neural networks requires large scale data, which often forces\nusers to work in a distributed or outsourced setting, accompanied with privacy\nconcerns. Split learning framework aims to address this concern by splitting up\nthe model among the client and the server. The idea is that since the server\ndoes not have access to client's part of the model, the scheme supposedly\nprovides privacy. We show that this is not true via two novel attacks. (1) We\nshow that an honest-but-curious split learning server, equipped only with the\nknowledge of the client neural network architecture, can recover the input\nsamples and also obtain a functionally similar model to the client model,\nwithout the client being able to detect the attack. (2) Furthermore, we show\nthat if split learning is used naively to protect the training labels, the\nhonest-but-curious server can infer the labels with perfect accuracy. We test\nour attacks using three benchmark datasets and investigate various properties\nof the overall system that affect the attacks' effectiveness. Our results show\nthat plaintext split learning paradigm can pose serious security risks and\nprovide no more than a false sense of security.",
          "link": "http://arxiv.org/abs/2108.09033",
          "publishedOn": "2021-08-23T01:36:36.792Z",
          "wordCount": 639,
          "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks Against Split Learning. (arXiv:2108.09033v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Issaid_C/0/1/0/all/0/1\">Chaouki Ben Issaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarakoon_S/0/1/0/all/0/1\">Sumudu Samarakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "In this article, we study the problem of robust reconfigurable intelligent\nsurface (RIS)-aided downlink communication over heterogeneous RIS types in the\nsupervised learning setting. By modeling downlink communication over\nheterogeneous RIS designs as different workers that learn how to optimize phase\nconfigurations in a distributed manner, we solve this distributed learning\nproblem using a distributionally robust formulation in a\ncommunication-efficient manner, while establishing its rate of convergence. By\ndoing so, we ensure that the global model performance of the worst-case worker\nis close to the performance of other workers. Simulation results show that our\nproposed algorithm requires fewer communication rounds (about 50% lesser) to\nachieve the same worst-case distribution test accuracy compared to competitive\nbaselines.",
          "link": "http://arxiv.org/abs/2108.09026",
          "publishedOn": "2021-08-23T01:36:36.786Z",
          "wordCount": 566,
          "title": "Federated Distributionally Robust Optimization for Phase Configuration of RISs. (arXiv:2108.09026v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1\">Christos Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1\">Johan Fredin Haslum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1\">Magnus S&#xf6;derberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>",
          "description": "Convolutional Neural Networks (CNNs) have reigned for a decade as the de\nfacto approach to automated medical image diagnosis. Recently, vision\ntransformers (ViTs) have appeared as a competitive alternative to CNNs,\nyielding similar levels of performance while possessing several interesting\nproperties that could prove beneficial for medical imaging tasks. In this work,\nwe explore whether it is time to move to transformer-based models or if we\nshould keep working with CNNs - can we trivially switch to transformers? If so,\nwhat are the advantages and drawbacks of switching to ViTs for medical image\ndiagnosis? We consider these questions in a series of experiments on three\nmainstream medical image datasets. Our findings show that, while CNNs perform\nbetter when trained from scratch, off-the-shelf vision transformers using\ndefault hyperparameters are on par with CNNs when pretrained on ImageNet, and\noutperform their CNN counterparts when pretrained using self-supervision.",
          "link": "http://arxiv.org/abs/2108.09038",
          "publishedOn": "2021-08-23T01:36:36.780Z",
          "wordCount": 608,
          "title": "Is it Time to Replace CNNs with Transformers for Medical Images?. (arXiv:2108.09038v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasmussen_T/0/1/0/all/0/1\">Torbj&#xf8;rn Rasmussen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brovang_B/0/1/0/all/0/1\">Bertram Brovang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Anasua Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuemmeth_F/0/1/0/all/0/1\">Ferdinand Kuemmeth</a>",
          "description": "In spin based quantum dot arrays, a leading technology for quantum\ncomputation applications, material or fabrication imprecisions affect the\nbehaviour of the device, which is compensated via tuning parameters. Automatic\ntuning of these device parameters constitutes a formidable challenge for\nmachine-learning. Here, we present the first practical algorithm for\ncontrolling the transition of electrons in a spin qubit array. We exploit a\nconnection to computational geometry and phrase the task as estimating a convex\npolytope from measurements.\n\nOur proposed algorithm uses active learning, to find the count, shapes and\nsizes of all facets of a given polytope. We test our algorithm on artifical\npolytopes as well as a real 2x2 spin qubit array. Our results show that we can\nreliably find the facets of the polytope, including small facets with sizes on\nthe order of the measurement precision. We discuss the implications of the\nNP-hardness of the underlying estimation problem and outline design\nconsiderations, limitations and tuning strategies for controlling future\nlarge-scale spin qubit devices.",
          "link": "http://arxiv.org/abs/2108.09133",
          "publishedOn": "2021-08-23T01:36:36.773Z",
          "wordCount": 634,
          "title": "Estimation of Convex Polytopes for Automatic Discovery of Charge State Transitions in Quantum Dot Arrays. (arXiv:2108.09133v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1\">Jed Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>",
          "description": "Federated Learning (FL) is a recent development in the field of machine\nlearning that collaboratively trains models without the training data leaving\nclient devices, in order to preserve data-privacy. In realistic settings, the\ntotal training set is distributed over clients in a highly non-Independent and\nIdentically Distributed (non-IID) fashion, which has been shown extensively to\nharm FL convergence speed and final model performance. We propose a novel,\ngeneralised approach for applying adaptive optimisation techniques to FL with\nthe Federated Global Biased Optimiser (FedGBO) algorithm. FedGBO accelerates FL\nby applying a set of global biased optimiser values during the local training\nphase of FL, which helps to reduce `client-drift' from non-IID data, whilst\nalso benefiting from adaptive momentum/learning-rate methods. We show that the\nFedGBO update with a generic optimiser can be viewed as a centralised update\nwith biased gradients and optimiser update, and use this theoretical framework\nto prove the convergence of FedGBO using momentum-Stochastic Gradient Descent.\nWe also perform extensive experiments using 4 realistic benchmark FL datasets\nand 3 popular adaptive optimisers to compare the performance of different\nadaptive-FL approaches, demonstrating that FedGBO has highly competitive\nperformance considering its low communication and computation costs, and\nproviding highly practical insights for the use of adaptive optimisation in FL.",
          "link": "http://arxiv.org/abs/2108.09134",
          "publishedOn": "2021-08-23T01:36:36.767Z",
          "wordCount": 647,
          "title": "Accelerating Federated Learning with a Global Biased Optimiser. (arXiv:2108.09134v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayesha_B/0/1/0/all/0/1\">Buddhi Ayesha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeewanthi_B/0/1/0/all/0/1\">Bhagya Jeewanthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitraranjan_C/0/1/0/all/0/1\">Charith Chitraranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_A/0/1/0/all/0/1\">Amal Shehan Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_A/0/1/0/all/0/1\">Amal S. Kumarage</a>",
          "description": "Understanding human mobility is essential for many fields, including\ntransportation planning. Currently, surveys are the primary source for such\nanalysis. However, in the recent past, many researchers have focused on Call\nDetail Records (CDR) for identifying travel patterns. CDRs have shown\ncorrelation to human mobility behavior. However, one of the main issues in\nusing CDR data is that it is difficult to identify the precise location of the\nuser due to the low spacial resolution of the data and other artifacts such as\nthe load sharing effect. Existing approaches have certain limitations. Previous\nstudies using CDRs do not consider the transmit power of cell towers when\nlocalizing the users and use an oversimplified approach to identify load\nsharing effects. Furthermore, they consider the entire population of users as\none group neglecting the differences in mobility patterns of different segments\nof users. This research introduces a novel methodology to user position\nlocalization from CDRs through improved detection of load sharing effects, by\ntaking the transmit power into account, and segmenting the users into distinct\ngroups for the purpose of learning any parameters of the model. Moreover, this\nresearch uses several methods to address the existing limitations and validate\nthe generated results using nearly 4 billion CDR data points with travel survey\ndata and voluntarily collected mobile data.",
          "link": "http://arxiv.org/abs/2108.09157",
          "publishedOn": "2021-08-23T01:36:36.761Z",
          "wordCount": 667,
          "title": "User Localization Based on Call Detail Records. (arXiv:2108.09157v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>",
          "description": "Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.",
          "link": "http://arxiv.org/abs/2108.09151",
          "publishedOn": "2021-08-23T01:36:36.754Z",
          "wordCount": 711,
          "title": "Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrah_I/0/1/0/all/0/1\">Ismail Ali Afrah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_U/0/1/0/all/0/1\">Utku Kose</a>",
          "description": "Main objective of this study is to introduce an expert system-based mHealth\napplication that takes Artificial Intelligence support by considering\npreviously introduced solutions from the literature and employing possible\nrequirements for a better solution. Thanks to that research study, a mobile\nsoftware system having Artificial Intelligence support and providing dynamic\nsupport against the common health problems in daily life was designed-developed\nand it was evaluated via survey and diagnosis-based evaluation tasks.\nEvaluation tasks indicated positive outcomes for the mHealth system.",
          "link": "http://arxiv.org/abs/2108.09277",
          "publishedOn": "2021-08-23T01:36:36.713Z",
          "wordCount": 533,
          "title": "MHealth: An Artificial Intelligence Oriented Mobile Application for Personal Healthcare Support. (arXiv:2108.09277v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poels_Y/0/1/0/all/0/1\">Yoeri Poels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>",
          "description": "The goal of a classification model is to assign the correct labels to data.\nIn most cases, this data is not fully described by the given set of labels.\nOften a rich set of meaningful concepts exist in the domain that can much more\nprecisely describe each datapoint. Such concepts can also be highly useful for\ninterpreting the model's classifications. In this paper we propose a model,\ndenoted as Variational Autoencoder-based Contrastive Explanation (VAE-CE), that\nrepresents data with high-level concepts and uses this representation for both\nclassification and generating explanations. The explanations are produced in a\ncontrastive manner, conveying why a datapoint is assigned to one class rather\nthan an alternative class. An explanation is specified as a set of\ntransformations of the input datapoint, with each step depicting a concept\nchanging towards the contrastive class. We build the model using a disentangled\nVAE, extended with a new supervised method for disentangling individual\ndimensions. An analysis on synthetic data and MNIST shows that the approaches\nto both disentanglement and explanation provide benefits over other methods.",
          "link": "http://arxiv.org/abs/2108.09159",
          "publishedOn": "2021-08-23T01:36:36.703Z",
          "wordCount": 610,
          "title": "VAE-CE: Visual Contrastive Explanation using Disentangled VAEs. (arXiv:2108.09159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>",
          "description": "Abundant real-world data can be naturally represented by large-scale\nnetworks, which demands efficient and effective learning algorithms. At the\nsame time, labels may only be available for some networks, which demands these\nalgorithms to be able to adapt to unlabeled networks. Domain-adaptive hash\nlearning has enjoyed considerable success in the computer vision community in\nmany practical tasks due to its lower cost in both retrieval time and storage\nfootprint. However, it has not been applied to multiple-domain networks. In\nthis work, we bridge this gap by developing an unsupervised domain-adaptive\nhash learning method for networks, dubbed UDAH. Specifically, we develop four\n{task-specific yet correlated} components: (1) network structure preservation\nvia a hard groupwise contrastive loss, (2) relaxation-free supervised hashing,\n(3) cross-domain intersected discriminators, and (4) semantic center alignment.\nWe conduct a wide range of experiments to evaluate the effectiveness and\nefficiency of our method on a range of tasks including link prediction, node\nclassification, and neighbor recommendation. Our evaluation results demonstrate\nthat our model achieves better performance than the state-of-the-art\nconventional discrete embedding methods over all the tasks.",
          "link": "http://arxiv.org/abs/2108.09136",
          "publishedOn": "2021-08-23T01:36:36.695Z",
          "wordCount": 614,
          "title": "Unsupervised Domain-adaptive Hash for Networks. (arXiv:2108.09136v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koumeri_L/0/1/0/all/0/1\">Lisa Koutsoviti Koumeri</a>",
          "description": "The need to measure bias encoded in tabular data that are used to solve\npattern recognition problems is widely recognized by academia, legislators and\nenterprises alike. In previous work, we proposed a bias quantification measure,\ncalled fuzzy-rough uncer-tainty, which relies on the fuzzy-rough set theory.\nThe intuition dictates that protected features should not change the\nfuzzy-rough boundary regions of a decision class significantly. The extent to\nwhich this happens is a proxy for bias expressed as uncertainty in\nadecision-making context. Our measure's main advantage is that it does not\ndepend on any machine learning prediction model but adistance function. In this\npaper, we extend our study by exploring the existence of bias encoded\nimplicitly in non-protected featuresas defined by the correlation between\nprotected and unprotected attributes. This analysis leads to four scenarios\nthat domain experts should evaluate before deciding how to tackle bias. In\naddition, we conduct a sensitivity analysis to determine the fuzzy operatorsand\ndistance function that best capture change in the boundary regions.",
          "link": "http://arxiv.org/abs/2108.09098",
          "publishedOn": "2021-08-23T01:36:36.684Z",
          "wordCount": 612,
          "title": "A fuzzy-rough uncertainty measure to discover bias encoded explicitly or implicitly in features of structured pattern classification datasets. (arXiv:2108.09098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nodet_P/0/1/0/all/0/1\">Pierre Nodet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1\">Vincent Lemaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1\">Alexis Bondu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornuejols_A/0/1/0/all/0/1\">Antoine Cornu&#xe9;jols</a>",
          "description": "In this paper we show that the combination of a Contrastive representation\nwith a label noise-robust classification head requires fine-tuning the\nrepresentation in order to achieve state-of-the-art performances. Since\nfine-tuned representations are shown to outperform frozen ones, one can\nconclude that noise-robust classification heads are indeed able to promote\nmeaningful representations if provided with a suitable starting point.\nExperiments are conducted to draw a comprehensive picture of performances by\nfeaturing six methods and nine noise instances of three different kinds (none,\nsymmetric, and asymmetric). In presence of noise the experiments show that fine\ntuning of Contrastive representation allows the six methods to achieve better\nresults than end-to-end learning and represent a new reference compare to the\nrecent state of art. Results are also remarkable stable versus the noise level.",
          "link": "http://arxiv.org/abs/2108.09154",
          "publishedOn": "2021-08-23T01:36:36.673Z",
          "wordCount": 563,
          "title": "Contrastive Representations for Label Noise Require Fine-Tuning. (arXiv:2108.09154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Debasrita Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1\">Debayan Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Susmita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ashish Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jonathan H. Chan</a>",
          "description": "The current COVID-19 pandemic has put a huge challenge on the Indian health\ninfrastructure. With more and more people getting affected during the second\nwave, the hospitals were over-burdened, running out of supplies and oxygen. In\nthis scenario, prediction of the number of COVID-19 cases beforehand might have\nhelped in the better utilization of limited resources and supplies. This\nmanuscript deals with the prediction of new COVID-19 cases, new deaths and\ntotal active cases for multiple days in advance. The proposed method uses gated\nrecurrent unit networks as the main predicting model. A study is conducted by\nbuilding four models that are pre-trained on the data from four different\ncountries (United States of America, Brazil, Spain and Bangladesh) and are\nfine-tuned or retrained on India's data. Since the four countries chosen have\nexperienced different types of infection curves, the pre-training provides a\ntransfer learning to the models incorporating diverse situations into account.\nEach of the four models then give a multiple days ahead predictions using\nrecursive learning method for the Indian test data. The final prediction comes\nfrom an ensemble of the predictions of the combination of different models.\nThis method with two countries, Spain and Brazil, is seen to achieve the best\nperformance amongst all the combinations as well as compared to other\ntraditional regression models.",
          "link": "http://arxiv.org/abs/2108.09131",
          "publishedOn": "2021-08-23T01:36:36.658Z",
          "wordCount": 734,
          "title": "Combination of Transfer Learning, Recursive Learning and Ensemble Learning for Multi-Day Ahead COVID-19 Cases Prediction in India using Gated Recurrent Unit Networks. (arXiv:2108.09131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09160",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Heas_P/0/1/0/all/0/1\">Patrick Heas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Herzet_C/0/1/0/all/0/1\">Cedric Herzet</a>",
          "description": "This technical note reviews sate-of-the-art algorithms for linear\napproximation of high-dimensional dynamical systems using low-rank dynamic mode\ndecomposition (DMD). While repeating several parts of our article \"low-rank\ndynamic mode decomposition: an exact and tractable solution\", this work\nprovides additional details useful for building a comprehensive picture of\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.09160",
          "publishedOn": "2021-08-23T01:36:36.632Z",
          "wordCount": 487,
          "title": "State-Of-The-Art Algorithms For Low-Rank Dynamic Mode Decomposition. (arXiv:2108.09160v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Traore_K/0/1/0/all/0/1\">Kalifou Rene Traore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Lots of effort in neural architecture search (NAS) research has been\ndedicated to algorithmic development, aiming at designing more efficient and\nless costly methods. Nonetheless, the investigation of the initialization of\nthese techniques remain scare, and currently most NAS methodologies rely on\nstochastic initialization procedures, because acquiring information prior to\nsearch is costly. However, the recent availability of NAS benchmarks have\nenabled low computational resources prototyping. In this study, we propose to\naccelerate a NAS algorithm using a data-driven initialization technique,\nleveraging the availability of NAS benchmarks. Particularly, we proposed a\ntwo-step methodology. First, a calibrated clustering analysis of the search\nspace is performed. Second, the centroids are extracted and used to initialize\na NAS algorithm. We tested our proposal using Aging Evolution, an evolutionary\nalgorithm, on NAS-bench-101. The results show that, compared to a random\ninitialization, a faster convergence and a better performance of the final\nsolution is achieved.",
          "link": "http://arxiv.org/abs/2108.09126",
          "publishedOn": "2021-08-23T01:36:36.618Z",
          "wordCount": 607,
          "title": "Lessons from the Clustering Analysis of a Search Space: A Centroid-based Approach to Initializing NAS. (arXiv:2108.09126v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1\">Ege Erdogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1\">Alptekin Kupcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_A/0/1/0/all/0/1\">A. Ercument Cicek</a>",
          "description": "Distributed deep learning frameworks, such as split learning, have recently\nbeen proposed to enable a group of participants to collaboratively train a deep\nneural network without sharing their raw data. Split learning in particular\nachieves this goal by dividing a neural network between a client and a server\nso that the client computes the initial set of layers, and the server computes\nthe rest. However, this method introduces a unique attack vector for a\nmalicious server attempting to steal the client's private data: the server can\ndirect the client model towards learning a task of its choice. With a concrete\nexample already proposed, such training-hijacking attacks present a significant\nrisk for the data privacy of split learning clients.\n\nIn this paper, we propose SplitGuard, a method by which a split learning\nclient can detect whether it is being targeted by a training-hijacking attack\nor not. We experimentally evaluate its effectiveness, and discuss in detail\nvarious points related to its use. We conclude that SplitGuard can effectively\ndetect training-hijacking attacks while minimizing the amount of information\nrecovered by the adversaries.",
          "link": "http://arxiv.org/abs/2108.09052",
          "publishedOn": "2021-08-23T01:36:36.593Z",
          "wordCount": 619,
          "title": "SplitGuard: Detecting and Mitigating Training-Hijacking Attacks in Split Learning. (arXiv:2108.09052v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianlei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xucheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>",
          "description": "Federated learning aims to protect users' privacy while performing data\nanalysis from different participants. However, it is challenging to guarantee\nthe training efficiency on heterogeneous systems due to the various\ncomputational capabilities and communication bottlenecks. In this work, we\npropose FedSkel to enable computation-efficient and communication-efficient\nfederated learning on edge devices by only updating the model's essential\nparts, named skeleton networks. FedSkel is evaluated on real edge devices with\nimbalanced datasets. Experimental results show that it could achieve up to\n5.52$\\times$ speedups for CONV layers' back-propagation, 1.82$\\times$ speedups\nfor the whole training process, and reduce 64.8% communication cost, with\nnegligible accuracy loss.",
          "link": "http://arxiv.org/abs/2108.09081",
          "publishedOn": "2021-08-23T01:36:36.544Z",
          "wordCount": 556,
          "title": "FedSkel: Efficient Federated Learning on Heterogeneous Systems with Skeleton Gradients Update. (arXiv:2108.09081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yushu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jayaweera_M/0/1/0/all/0/1\">Malith Jayaweera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaeli_D/0/1/0/all/0/1\">David Kaeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Though recent years have witnessed remarkable progress in single image\nsuper-resolution (SISR) tasks with the prosperous development of deep neural\nnetworks (DNNs), the deep learning methods are confronted with the computation\nand memory consumption issues in practice, especially for resource-limited\nplatforms such as mobile devices. To overcome the challenge and facilitate the\nreal-time deployment of SISR tasks on mobile, we combine neural architecture\nsearch with pruning search and propose an automatic search framework that\nderives sparse super-resolution (SR) models with high image quality while\nsatisfying the real-time inference requirement. To decrease the search cost, we\nleverage the weight sharing strategy by introducing a supernet and decouple the\nsearch problem into three stages, including supernet construction,\ncompiler-aware architecture and pruning search, and compiler-aware pruning\nratio search. With the proposed framework, we are the first to achieve\nreal-time SR inference (with only tens of milliseconds per frame) for\nimplementing 720p resolution with competitive image quality (in terms of PSNR\nand SSIM) on mobile platforms (Samsung Galaxy S20).",
          "link": "http://arxiv.org/abs/2108.08910",
          "publishedOn": "2021-08-23T01:36:36.470Z",
          "wordCount": 639,
          "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search. (arXiv:2108.08910v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.",
          "link": "http://arxiv.org/abs/2108.09105",
          "publishedOn": "2021-08-23T01:36:36.450Z",
          "wordCount": 661,
          "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08993",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1\">Ruidi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paschalidis_I/0/1/0/all/0/1\">Ioannis Ch. Paschalidis</a>",
          "description": "This monograph develops a comprehensive statistical learning framework that\nis robust to (distributional) perturbations in the data using Distributionally\nRobust Optimization (DRO) under the Wasserstein metric. Beginning with\nfundamental properties of the Wasserstein metric and the DRO formulation, we\nexplore duality to arrive at tractable formulations and develop finite-sample,\nas well as asymptotic, performance guarantees. We consider a series of learning\nproblems, including (i) distributionally robust linear regression; (ii)\ndistributionally robust regression with group structure in the predictors;\n(iii) distributionally robust multi-output regression and multiclass\nclassification, (iv) optimal decision making that combines distributionally\nrobust regression with nearest-neighbor estimation; (v) distributionally robust\nsemi-supervised learning, and (vi) distributionally robust reinforcement\nlearning. A tractable DRO relaxation for each problem is being derived,\nestablishing a connection between robustness and regularization, and obtaining\nbounds on the prediction and estimation errors of the solution. Beyond theory,\nwe include numerical experiments and case studies using synthetic and real\ndata. The real data experiments are all associated with various health\ninformatics problems, an application area which provided the initial impetus\nfor this work.",
          "link": "http://arxiv.org/abs/2108.08993",
          "publishedOn": "2021-08-23T01:36:36.444Z",
          "wordCount": 596,
          "title": "Distributionally Robust Learning. (arXiv:2108.08993v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anirban Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patterson_S/0/1/0/all/0/1\">Stacy Patterson</a>",
          "description": "We consider federated learning in tiered communication networks. Our network\nmodel consists of a set of silos, each holding a vertical partition of the\ndata. Each silo contains a hub and a set of clients, with the silo's vertical\ndata shard partitioned horizontally across its clients. We propose Tiered\nDecentralized Coordinate Descent (TDCD), a communication-efficient\ndecentralized training algorithm for such two-tiered networks. To reduce\ncommunication overhead, the clients in each silo perform multiple local\ngradient steps before sharing updates with their hub. Each hub adjusts its\ncoordinates by averaging its workers' updates, and then hubs exchange\nintermediate updates with one another. We present a theoretical analysis of our\nalgorithm and show the dependence of the convergence rate on the number of\nvertical partitions, the number of local updates, and the number of clients in\neach hub. We further validate our approach empirically via simulation-based\nexperiments using a variety of datasets and objectives.",
          "link": "http://arxiv.org/abs/2108.08930",
          "publishedOn": "2021-08-23T01:36:36.423Z",
          "wordCount": 609,
          "title": "Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and Horizontal Data Partitioning. (arXiv:2108.08930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chengzhang Zhu</a>",
          "description": "Automated next-best action recommendation for each customer in a sequential,\ndynamic and interactive context has been widely needed in natural, social and\nbusiness decision-making. Personalized next-best action recommendation must\ninvolve past, current and future customer demographics and circumstances\n(states) and behaviors, long-range sequential interactions between customers\nand decision-makers, multi-sequence interactions between states, behaviors and\nactions, and their reactions to their counterpart's actions. No existing\nmodeling theories and tools, including Markovian decision processes, user and\nbehavior modeling, deep sequential modeling, and personalized sequential\nrecommendation, can quantify such complex decision-making on a personal level.\nWe take a data-driven approach to learn the next-best actions for personalized\ndecision-making by a reinforced coupled recurrent neural network (CRN). CRN\nrepresents multiple coupled dynamic sequences of a customer's historical and\ncurrent states, responses to decision-makers' actions, decision rewards to\nactions, and learns long-term multi-sequence interactions between parties\n(customer and decision-maker). Next-best actions are then recommended on each\ncustomer at a time point to change their state for an optimal decision-making\nobjective. Our study demonstrates the potential of personalized deep learning\nof multi-sequence interactions and automated dynamic intervention for\npersonalized decision-making in complex systems.",
          "link": "http://arxiv.org/abs/2108.08846",
          "publishedOn": "2021-08-23T01:36:36.407Z",
          "wordCount": 636,
          "title": "Personalized next-best action recommendation with multi-party interaction learning for automated decision-making. (arXiv:2108.08846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>",
          "description": "Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.",
          "link": "http://arxiv.org/abs/2108.08988",
          "publishedOn": "2021-08-23T01:36:36.400Z",
          "wordCount": 605,
          "title": "Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhipeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1\">Ozan Sener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "Continual learning is the problem of learning and retaining knowledge through\ntime over multiple tasks and environments. Research has primarily focused on\nthe incremental classification setting, where new tasks/classes are added at\ndiscrete time intervals. Such an \"offline\" setting does not evaluate the\nability of agents to learn effectively and efficiently, since an agent can\nperform multiple learning epochs without any time limitation when a task is\nadded. We argue that \"online\" continual learning, where data is a single\ncontinuous stream without task boundaries, enables evaluating both information\nretention and online learning efficacy. In online continual learning, each\nincoming small batch of data is first used for testing and then added to the\ntraining set, making the problem truly online. Trained models are later\nevaluated on historical data to assess information retention. We introduce a\nnew benchmark for online continual visual learning that exhibits large scale\nand natural distribution shifts. Through a large-scale analysis, we identify\ncritical and previously unobserved phenomena of gradient-based optimization in\ncontinual learning, and propose effective strategies for improving\ngradient-based online continual learning with real data. The source code and\ndataset are available in: https://github.com/IntelLabs/continuallearning.",
          "link": "http://arxiv.org/abs/2108.09020",
          "publishedOn": "2021-08-23T01:36:36.393Z",
          "wordCount": 640,
          "title": "Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data. (arXiv:2108.09020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zecheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Ruby B. Lee</a>",
          "description": "In cloud computing, it is desirable if suspicious activities can be detected\nby automatic anomaly detection systems. Although anomaly detection has been\ninvestigated in the past, it remains unsolved in cloud computing. Challenges\nare: characterizing the normal behavior of a cloud server, distinguishing\nbetween benign and malicious anomalies (attacks), and preventing alert fatigue\ndue to false alarms.\n\nWe propose CloudShield, a practical and generalizable real-time anomaly and\nattack detection system for cloud computing. Cloudshield uses a general,\npretrained deep learning model with different cloud workloads, to predict the\nnormal behavior and provide real-time and continuous detection by examining the\nmodel reconstruction error distributions. Once an anomaly is detected, to\nreduce alert fatigue, CloudShield automatically distinguishes between benign\nprograms, known attacks, and zero-day attacks, by examining the prediction\nerror distributions. We evaluate the proposed CloudShield on representative\ncloud benchmarks. Our evaluation shows that CloudShield, using model\npretraining, can apply to a wide scope of cloud workloads. Especially, we\nobserve that CloudShield can detect the recently proposed speculative execution\nattacks, e.g., Spectre and Meltdown attacks, in milliseconds. Furthermore, we\nshow that CloudShield accurately differentiates and prioritizes known attacks,\nand potential zero-day attacks, from benign programs. Thus, it significantly\nreduces false alarms by up to 99.0%.",
          "link": "http://arxiv.org/abs/2108.08977",
          "publishedOn": "2021-08-23T01:36:36.361Z",
          "wordCount": 633,
          "title": "CloudShield: Real-time Anomaly Detection in the Cloud. (arXiv:2108.08977v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Heyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigas_P/0/1/0/all/0/1\">Paul Grigas</a>",
          "description": "The predict-then-optimize framework is fundamental in practical stochastic\ndecision-making problems: first predict unknown parameters of an optimization\nmodel, then solve the problem using the predicted values. A natural loss\nfunction in this setting is defined by measuring the decision error induced by\nthe predicted parameters, which was named the Smart Predict-then-Optimize (SPO)\nloss by Elmachtoub and Grigas [arXiv:1710.08005]. Since the SPO loss is\ntypically nonconvex and possibly discontinuous, Elmachtoub and Grigas\n[arXiv:1710.08005] introduced a convex surrogate, called the SPO+ loss, that\nimportantly accounts for the underlying structure of the optimization model. In\nthis paper, we greatly expand upon the consistency results for the SPO+ loss\nprovided by Elmachtoub and Grigas [arXiv:1710.08005]. We develop risk bounds\nand uniform calibration results for the SPO+ loss relative to the SPO loss,\nwhich provide a quantitative way to transfer the excess surrogate risk to\nexcess true risk. By combining our risk bounds with generalization bounds, we\nshow that the empirical minimizer of the SPO+ loss achieves low excess true\nrisk with high probability. We first demonstrate these results in the case when\nthe feasible region of the underlying optimization problem is a polyhedron, and\nthen we show that the results can be strengthened substantially when the\nfeasible region is a level set of a strongly convex function. We perform\nexperiments to empirically demonstrate the strength of the SPO+ surrogate, as\ncompared to standard $\\ell_1$ and squared $\\ell_2$ prediction error losses, on\nportfolio allocation and cost-sensitive multi-class classification problems.",
          "link": "http://arxiv.org/abs/2108.08887",
          "publishedOn": "2021-08-23T01:36:36.355Z",
          "wordCount": 683,
          "title": "Risk Bounds and Calibration for a Smart Predict-then-Optimize Method. (arXiv:2108.08887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zihang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sathya N. Ravi</a>",
          "description": "We study how stochastic differential equation (SDE) based ideas can inspire\nnew modifications to existing algorithms for a set of problems in computer\nvision. Loosely speaking, our formulation is related to both explicit and\nimplicit strategies for data augmentation and group equivariance, but is\nderived from new results in the SDE literature on estimating infinitesimal\ngenerators of a class of stochastic processes. If and when there is nominal\nagreement between the needs of an application/task and the inherent properties\nand behavior of the types of processes that we can efficiently handle, we\nobtain a very simple and efficient plug-in layer that can be incorporated\nwithin any existing network architecture, with minimal modification and only a\nfew additional parameters. We show promising experiments on a number of vision\ntasks including few shot learning, point cloud transformers and deep\nvariational segmentation obtaining efficiency or performance improvements.",
          "link": "http://arxiv.org/abs/2108.08891",
          "publishedOn": "2021-08-23T01:36:36.349Z",
          "wordCount": 587,
          "title": "Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators. (arXiv:2108.08891v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tri Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1\">Aiden Nibali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>",
          "description": "Medical image classification is often challenging for two reasons: a lack of\nlabelled examples due to expensive and time-consuming annotation protocols, and\nimbalanced class labels due to the relative scarcity of disease-positive\nindividuals in the wider population. Semi-supervised learning (SSL) methods\nexist for dealing with a lack of labels, but they generally do not address the\nproblem of class imbalance. In this study we propose Adaptive Blended\nConsistency Loss (ABCL), a drop-in replacement for consistency loss in\nperturbation-based SSL methods. ABCL counteracts data skew by adaptively mixing\nthe target class distribution of the consistency loss in accordance with class\nfrequency. Our experiments with ABCL reveal improvements to unweighted average\nrecall on two different imbalanced medical image classification datasets when\ncompared with existing consistency losses that are not designed to counteract\nclass imbalance.",
          "link": "http://arxiv.org/abs/2108.08956",
          "publishedOn": "2021-08-23T01:36:36.343Z",
          "wordCount": 584,
          "title": "Semi-supervised learning for medical image classification using imbalanced training data. (arXiv:2108.08956v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Gokul Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1\">Sumit K. Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannala_M/0/1/0/all/0/1\">Manvitha Pannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1\">Chaitali Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jae-sun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Y. Ogras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>",
          "description": "In-memory computing (IMC) on a monolithic chip for deep learning faces\ndramatic challenges on area, yield, and on-chip interconnection cost due to the\never-increasing model sizes. 2.5D integration or chiplet-based architectures\ninterconnect multiple small chips (i.e., chiplets) to form a large computing\nsystem, presenting a feasible solution beyond a monolithic IMC architecture to\naccelerate large deep learning models. This paper presents a new benchmarking\nsimulator, SIAM, to evaluate the performance of chiplet-based IMC architectures\nand explore the potential of such a paradigm shift in IMC architecture design.\nSIAM integrates device, circuit, architecture, network-on-chip (NoC),\nnetwork-on-package (NoP), and DRAM access models to realize an end-to-end\nsystem. SIAM is scalable in its support of a wide range of deep neural networks\n(DNNs), customizable to various network structures and configurations, and\ncapable of efficient design space exploration. We demonstrate the flexibility,\nscalability, and simulation speed of SIAM by benchmarking different\nstate-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We\nfurther calibrate the simulation results with a published silicon result,\nSIMBA. The chiplet-based IMC architecture obtained through SIAM shows\n130$\\times$ and 72$\\times$ improvement in energy-efficiency for ResNet-50 on\nthe ImageNet dataset compared to Nvidia V100 and T4 GPUs.",
          "link": "http://arxiv.org/abs/2108.08903",
          "publishedOn": "2021-08-23T01:36:36.337Z",
          "wordCount": 643,
          "title": "SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks. (arXiv:2108.08903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08871",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jakobsen_M/0/1/0/all/0/1\">Martin Emil Jakobsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shah_R/0/1/0/all/0/1\">Rajen D. Shah</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1\">Peter B&#xfc;hlmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Peters_J/0/1/0/all/0/1\">Jonas Peters</a>",
          "description": "Knowing the causal structure of a system is of fundamental interest in many\nareas of science and can aid the design of prediction algorithms that work well\nunder manipulations to the system. The causal structure becomes identifiable\nfrom the observational distribution under certain restrictions. To learn the\nstructure from data, score-based methods evaluate different graphs according to\nthe quality of their fits. However, for large nonlinear models, these rely on\nheuristic optimization approaches with no general guarantees of recovering the\ntrue causal structure. In this paper, we consider structure learning of\ndirected trees. We propose a fast and scalable method based on Chu-Liu-Edmonds'\nalgorithm we call causal additive trees (CAT). For the case of Gaussian errors,\nwe prove consistency in an asymptotic regime with a vanishing identifiability\ngap. We also introduce a method for testing substructure hypotheses with\nasymptotic family-wise error rate control that is valid post-selection and in\nunidentified settings. Furthermore, we study the identifiability gap, which\nquantifies how much better the true causal model fits the observational\ndistribution, and prove that it is lower bounded by local properties of the\ncausal model. Simulation studies demonstrate the favorable performance of CAT\ncompared to competing structure learning methods.",
          "link": "http://arxiv.org/abs/2108.08871",
          "publishedOn": "2021-08-23T01:36:36.330Z",
          "wordCount": 637,
          "title": "Structure Learning for Directed Trees. (arXiv:2108.08871v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayala_A/0/1/0/all/0/1\">Angel Ayala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1\">Francisco Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_B/0/1/0/all/0/1\">Bruno Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dazeley_R/0/1/0/all/0/1\">Richard Dazeley</a>",
          "description": "Explainable reinforcement learning allows artificial agents to explain their\nbehavior in a human-like manner aiming at non-expert end-users. An efficient\nalternative of creating explanations is to use an introspection-based method\nthat transforms Q-values into probabilities of success used as the base to\nexplain the agent's decision-making process. This approach has been effectively\nused in episodic and discrete scenarios, however, to compute the probability of\nsuccess in non-episodic and more complex environments has not been addressed\nyet. In this work, we adapt the introspection method to be used in a\nnon-episodic task and try it in a continuous Atari game scenario solved with\nthe Rainbow algorithm. Our initial results show that the probability of success\ncan be computed directly from the Q-values for all possible actions.",
          "link": "http://arxiv.org/abs/2108.08911",
          "publishedOn": "2021-08-23T01:36:36.306Z",
          "wordCount": 562,
          "title": "Explainable Deep Reinforcement Learning Using Introspection in a Non-episodic Task. (arXiv:2108.08911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1\">Shayan Fazeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>",
          "description": "Topic Modeling refers to the problem of discovering the main topics that have\noccurred in corpora of textual data, with solutions finding crucial\napplications in numerous fields. In this work, inspired by the recent\nadvancements in the Natural Language Processing domain, we introduce FAME, an\nopen-source framework enabling an efficient mechanism of extracting and\nincorporating textual features and utilizing them in discovering topics and\nclustering text documents that are semantically similar in a corpus. These\nfeatures range from traditional approaches (e.g., frequency-based) to the most\nrecent auto-encoding embeddings from transformer-based language models such as\nBERT model family. To demonstrate the effectiveness of this library, we\nconducted experiments on the well-known News-Group dataset. The library is\navailable online.",
          "link": "http://arxiv.org/abs/2108.08946",
          "publishedOn": "2021-08-23T01:36:36.300Z",
          "wordCount": 550,
          "title": "A Framework for Neural Topic Modeling of Text Corpora. (arXiv:2108.08946v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1\">Majid Abdolshah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thommen Karimpanal George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "Sample-efficient generalisation of reinforcement learning approaches have\nalways been a challenge, especially, for complex scenes with many components.\nIn this work, we introduce Plug and Play Markov Decision Processes, an\nobject-based representation that allows zero-shot integration of new objects\nfrom known object classes. This is achieved by representing the global\ntransition dynamics as a union of local transition functions, each with respect\nto one active object in the scene. Transition dynamics from an object class can\nbe pre-learnt and thus would be ready to use in a new environment. Each active\nobject is also endowed with its reward function. Since there is no central\nreward function, addition or removal of objects can be handled efficiently by\nonly updating the reward functions of objects involved. A new transfer learning\nmechanism is also proposed to adapt reward function in such cases. Experiments\nshow that our representation can achieve sample-efficiency in a variety of\nset-ups.",
          "link": "http://arxiv.org/abs/2108.08960",
          "publishedOn": "2021-08-23T01:36:36.287Z",
          "wordCount": 582,
          "title": "Plug and Play, Model-Based Reinforcement Learning. (arXiv:2108.08960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Ruihan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harimoto_K/0/1/0/all/0/1\">Keiko Harimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Adversarial training is a method for enhancing neural networks to improve the\nrobustness against adversarial examples. Besides the security concerns of\npotential adversarial examples, adversarial training can also improve the\nperformance of the neural networks, train robust neural networks, and provide\ninterpretability for neural networks. In this work, we take the first step to\nintroduce adversarial training in time series analysis by taking the finance\nfield as an example. Rethinking existing researches of adversarial training, we\npropose the adaptively scaled adversarial training (ASAT) in time series\nanalysis, by treating data at different time slots with time-dependent\nimportance weights. Experimental results show that the proposed ASAT can\nimprove both the accuracy and the adversarial robustness of neural networks.\nBesides enhancing neural networks, we also propose the dimension-wise\nadversarial sensitivity indicator to probe the sensitivities and importance of\ninput dimensions. With the proposed indicator, we can explain the decision\nbases of black box neural networks.",
          "link": "http://arxiv.org/abs/2108.08976",
          "publishedOn": "2021-08-23T01:36:36.269Z",
          "wordCount": 601,
          "title": "ASAT: Adaptively Scaled Adversarial Training in Time Series. (arXiv:2108.08976v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08890",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bogoclu_C/0/1/0/all/0/1\">Can Bogoclu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roos_D/0/1/0/all/0/1\">Dirk Roos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestorovic_T/0/1/0/all/0/1\">Tamara Nestorovi&#x107;</a>",
          "description": "Optimizing the reliability and the robustness of a design is important but\noften unaffordable due to high sample requirements. Surrogate models based on\nstatistical and machine learning methods are used to increase the sample\nefficiency. However, for higher dimensional or multi-modal systems, surrogate\nmodels may also require a large amount of samples to achieve good results. We\npropose a sequential sampling strategy for the surrogate based solution of\nmulti-objective reliability based robust design optimization problems. Proposed\nlocal Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be\ncombined with any surrogate model because there is no free lunch but possibly a\nbudget one. The proposed method is compared to stationary sampling as well as\nother proposed strategies from the literature. Gaussian process and support\nvector regression are both used as surrogate models. Empirical evidence is\npresented, showing that LoLHR achieves on average better results compared to\nother surrogate based strategies on the tested examples.",
          "link": "http://arxiv.org/abs/2108.08890",
          "publishedOn": "2021-08-23T01:36:36.258Z",
          "wordCount": 605,
          "title": "Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization. (arXiv:2108.08890v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Siddhansh Narang</a>",
          "description": "There has been a remarkable progress in learning a model which could\nrecognise novel classes with only a few labeled examples in the last few years.\nFew-shot learning (FSL) for action recognition is a challenging task of\nrecognising novel action categories which are represented by few instances in\nthe training data. We propose a novel variational inference based architectural\nframework (HF-AR) for few shot activity recognition. Our framework leverages\nvolume-preserving Householder Flow to learn a flexible posterior distribution\nof the novel classes. This results in better performance as compared to\nstate-of-the-art few shot approaches for human activity recognition. approach\nconsists of base model and an adapter model. Our architecture consists of a\nbase model and an adapter model. The base model is trained on seen classes and\nit computes an embedding that represent the spatial and temporal insights\nextracted from the input video, e.g. combination of Resnet-152 and LSTM based\nencoder-decoder model. The adapter model applies a series of Householder\ntransformations to compute a flexible posterior distribution that lends higher\naccuracy in the few shot approach. Extensive experiments on three well-known\ndatasets: UCF101, HMDB51 and Something-Something-V2, demonstrate similar or\nbetter performance on 1-shot and 5-shot classification as compared to\nstate-of-the-art few shot approaches that use only RGB frame sequence as input.\nTo the best of our knowledge, we are the first to explore variational inference\nalong with householder transformations to capture the full rank covariance\nmatrix of posterior distribution, for few shot learning in activity\nrecognition.",
          "link": "http://arxiv.org/abs/2108.08990",
          "publishedOn": "2021-08-23T01:36:36.251Z",
          "wordCount": 706,
          "title": "Few Shot Activity Recognition Using Variational Inference. (arXiv:2108.08990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ono_T/0/1/0/all/0/1\">Taiga Ono</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_T/0/1/0/all/0/1\">Takeshi Sugawara</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1\">Jun Sakuma</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1\">Tatsuya Mori</a> (1 and 4) ((1) Waseda University, (2) The University of Electro-Communications, (3) University of Tsukuba, (4) RIKEN AIP)",
          "description": "This work aims to assess the reality and feasibility of the adversarial\nattack against cardiac diagnosis system powered by machine learning algorithms.\nTo this end, we introduce adversarial beats, which are adversarial\nperturbations tailored specifically against electrocardiograms (ECGs)\nbeat-by-beat classification system. We first formulate an algorithm to generate\nadversarial examples for the ECG classification neural network model, and study\nits attack success rate. Next, to evaluate its feasibility in a physical\nenvironment, we mount a hardware attack by designing a malicious signal\ngenerator which injects adversarial beats into ECG sensor readings. To the best\nof our knowledge, our work is the first in evaluating the proficiency of\nadversarial examples for ECGs in a physical setup. Our real-world experiments\ndemonstrate that adversarial beats successfully manipulated the diagnosis\nresults 3-5 times out of 40 attempts throughout the course of 2 minutes.\nFinally, we discuss the overall feasibility and impact of the attack, by\nclearly defining motives and constraints of expected attackers along with our\nexperimental results.",
          "link": "http://arxiv.org/abs/2108.08972",
          "publishedOn": "2021-08-23T01:36:36.245Z",
          "wordCount": 625,
          "title": "Application of Adversarial Examples to Physical ECG Signals. (arXiv:2108.08972v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Porebski_S/0/1/0/all/0/1\">Sebastian Porebski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potempa_R/0/1/0/all/0/1\">Rafal Potempa</a>",
          "description": "While quantum architectures are still under development, when available, they\nwill only be able to process quantum data when machine learning algorithms can\nonly process numerical data. Therefore, in the issues of classification or\nregression, it is necessary to simulate and study quantum systems that will\ntransfer the numerical input data to a quantum form and enable quantum\ncomputers to use the available methods of machine learning. This material\nincludes the results of experiments on training and performance of a hybrid\nquantum-classical neural network developed for the problem of classification of\nhandwritten digits from the MNIST data set. The comparative results of two\nmodels: classical and quantum neural networks of a similar number of training\nparameters, indicate that the quantum network, although its simulation is\ntime-consuming, overcomes the classical network (it has better convergence and\nachieves higher training and testing accuracy).",
          "link": "http://arxiv.org/abs/2108.08875",
          "publishedOn": "2021-08-23T01:36:36.221Z",
          "wordCount": 627,
          "title": "Comparing concepts of quantum and classical neural network models for image classification task. (arXiv:2108.08875v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1\">Miguel Ruiz-Garcia</a>",
          "description": "The work of McCloskey and Cohen popularized the concept of catastrophic\ninterference. They used a neural network that tried to learn addition using two\ngroups of examples as two different tasks. In their case, learning the second\ntask rapidly deteriorated the acquired knowledge about the previous one. This\ncould be a symptom of a fundamental problem: addition is an algorithmic task\nthat should not be learned through pattern recognition. We propose to use a\nneural network with a different architecture that can be trained to recover the\ncorrect algorithm for the addition of binary numbers. We test it in the setting\nproposed by McCloskey and Cohen and training on random examples one by one. The\nneural network not only does not suffer from catastrophic forgetting but it\nimproves its predictive power on unseen pairs of numbers as training\nprogresses. This work emphasizes the importance that neural network\narchitecture has for the emergence of catastrophic forgetting and introduces a\nneural network that is able to learn an algorithm.",
          "link": "http://arxiv.org/abs/2108.03940",
          "publishedOn": "2021-08-23T01:36:36.122Z",
          "wordCount": 626,
          "title": "Some thoughts on catastrophic forgetting and how to learn an algorithm. (arXiv:2108.03940v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qin Yang</a>",
          "description": "Distributed artificial intelligence (DAI) studies artificial intelligence\nentities working together to reason, plan, solve problems, organize behaviors\nand strategies, make collective decisions and learn. This Ph.D. research\nproposes a principled Multi-Agent Systems (MAS) cooperation framework --\nSelf-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap\nbetween perception, communication, planning, execution, decision-making, and\nlearning.",
          "link": "http://arxiv.org/abs/2106.04679",
          "publishedOn": "2021-08-23T01:36:36.104Z",
          "wordCount": 543,
          "title": "Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v4 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1\">Jason Plawinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1\">Sajanth Subramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1\">Aimee Readie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1\">Gregory Ligozio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1\">David Ohlssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1\">Mark Baillie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1\">Thibaud Coroller</a>",
          "description": "Sharing data from clinical studies can facilitate innovative data-driven\nresearch and ultimately lead to better public health. However, sharing\nbiomedical data can put sensitive personal information at risk. This is usually\nsolved by anonymization, which is a slow and expensive process. An alternative\nto anonymization is sharing a synthetic dataset that bears a behaviour similar\nto the real data but preserves privacy. As part of the collaboration between\nNovartis and the Oxford Big Data Institute, we generate a synthetic dataset\nbased on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply\nan Auxiliary Classifier GAN to generate synthetic MRIs of vertebral units. The\nimages are conditioned on the VU location (cervical, thoracic and lumbar). In\nthis paper, we present a method for generating a synthetic dataset and conduct\nan in-depth analysis on its properties along three key metrics: image fidelity,\nsample diversity and dataset privacy.",
          "link": "http://arxiv.org/abs/2106.13199",
          "publishedOn": "2021-08-23T01:36:36.048Z",
          "wordCount": 630,
          "title": "A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, generating pseudo labels or low confidence\nlabels on the entire unlabeled test dataset, and then, (2) filter out quality\ngenerated labels and, (3) combining the generated labels with the previously\navailable high confidence labeled dataset. This assimilated dataset is used for\nthe next round of training ensemble models. This cyclical process is repeated\nuntil the performance improvement plateaus. Additionally, we post process our\nresults with Conditional Random Fields. Our approach sets the second highest\nscore on the public hold-out test leaderboard for the ETCI competition with\n0.7654 IoU. To the best of our knowledge we believe this is one of the first\nworks to try out semi-supervised learning to improve flood segmentation models.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-08-23T01:36:36.041Z",
          "wordCount": 776,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huming Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yansong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1\">Alsharif Abuadbba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Anmin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Sarawi_S/0/1/0/all/0/1\">Said Al-Sarawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_D/0/1/0/all/0/1\">Derek Abbott</a>",
          "description": "There is currently a burgeoning demand for deploying deep learning (DL)\nmodels on ubiquitous edge Internet of Things devices attributing to their low\nlatency and high privacy preservation. However, DL models are often large in\nsize and require large-scale computation, which prevents them from being placed\ndirectly onto IoT devices where resources are constrained and 32-bit\nfloating-point operations are unavailable. Model quantization is a pragmatic\nsolution, which enables DL deployment on mobile devices and embedded systems by\neffortlessly post-quantizing a large high-precision model into a small\nlow-precision model while retaining the model inference accuracy.\n\nThis work reveals that the standard quantization operation can be abused to\nactivate a backdoor. We demonstrate that a full-precision backdoored model that\ndoes not have any backdoor effect in the presence of a trigger -- as the\nbackdoor is dormant -- can be activated by the default TensorFlow-Lite\nquantization, the only product-ready quantization framework to date. We\nascertain that all trained float-32 backdoored models exhibit no backdoor\neffect even in the presence of trigger inputs. State-of-the-art frontend\ndetection approaches, such as Neural Cleanse and STRIP, fail to identify the\nbackdoor in the float-32 models. When each of the float-32 models is converted\ninto an int-8 format model through the standard TFLite post-training\nquantization, the backdoor is activated in the quantized model, which shows a\nstable attack success rate close to 100% upon inputs with the trigger, while\nbehaves normally upon non-trigger inputs. This work highlights that a stealthy\nsecurity threat occurs when end users utilize the on-device post-training model\nquantization toolkits, informing security researchers of cross-platform\noverhaul of DL models post quantization even if they pass frontend inspections.",
          "link": "http://arxiv.org/abs/2108.09187",
          "publishedOn": "2021-08-23T01:36:36.034Z",
          "wordCount": 717,
          "title": "Quantization Backdoors to Deep Learning Models. (arXiv:2108.09187v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tolkova_I/0/1/0/all/0/1\">Irina Tolkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_B/0/1/0/all/0/1\">Brian Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_M/0/1/0/all/0/1\">Marcel Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_S/0/1/0/all/0/1\">Stefan Kahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinck_H/0/1/0/all/0/1\">Holger Klinck</a>",
          "description": "Monitoring of bird populations has played a vital role in conservation\nefforts and in understanding biodiversity loss. The automation of this process\nhas been facilitated by both sensing technologies, such as passive acoustic\nmonitoring, and accompanying analytical tools, such as deep learning. However,\nmachine learning models frequently have difficulty generalizing to examples not\nencountered in the training data. In our work, we present a semi-supervised\napproach to identify characteristic calls and environmental noise. We utilize\nseveral methods to learn a latent representation of audio samples, including a\nconvolutional autoencoder and two pre-trained networks, and group the resulting\nembeddings for a domain expert to identify cluster labels. We show that our\napproach can improve classification precision and provide insight into the\nlatent structure of environmental acoustic datasets.",
          "link": "http://arxiv.org/abs/2108.09203",
          "publishedOn": "2021-08-23T01:36:36.016Z",
          "wordCount": 573,
          "title": "Parsing Birdsong with Deep Audio Embeddings. (arXiv:2108.09203v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>",
          "description": "Social media such as Instagram and Twitter have become important platforms\nfor marketing and selling illicit drugs. Detection of online illicit drug\ntrafficking has become critical to combat the online trade of illicit drugs.\nHowever, the legal status often varies spatially and temporally; even for the\nsame drug, federal and state legislation can have different regulations about\nits legality. Meanwhile, more drug trafficking events are disguised as a novel\nform of advertising commenting leading to information heterogeneity.\nAccordingly, accurate detection of illicit drug trafficking events (IDTEs) from\nsocial media has become even more challenging. In this work, we conduct the\nfirst systematic study on fine-grained detection of IDTEs on Instagram. We\npropose to take a deep multimodal multilabel learning (DMML) approach to detect\nIDTEs and demonstrate its effectiveness on a newly constructed dataset called\nmultimodal IDTE(MM-IDTE). Specifically, our model takes text and image data as\nthe input and combines multimodal information to predict multiple labels of\nillicit drugs. Inspired by the success of BERT, we have developed a\nself-supervised multimodal bidirectional transformer by jointly fine-tuning\npretrained text and image encoders. We have constructed a large-scale dataset\nMM-IDTE with manually annotated multiple drug labels to support fine-grained\ndetection of illicit drugs. Extensive experimental results on the MM-IDTE\ndataset show that the proposed DMML methodology can accurately detect IDTEs\neven in the presence of special characters and style changes attempting to\nevade detection.",
          "link": "http://arxiv.org/abs/2108.08920",
          "publishedOn": "2021-08-23T01:36:36.010Z",
          "wordCount": 693,
          "title": "Detection of Illicit Drug Trafficking Events on Instagram: A Deep Multimodal Multilabel Learning Approach. (arXiv:2108.08920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kavitzky_J/0/1/0/all/0/1\">Jonathan Kavitzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarecki_J/0/1/0/all/0/1\">Jonathan Zarecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brusilovsky_I/0/1/0/all/0/1\">Idan Brusilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>",
          "description": "Recent advances in deep learning have transformed many fields by introducing\ngeneric embedding spaces, capable of achieving great predictive performance\nwith minimal labeling effort. The geology field has not yet met such success.\nIn this work, we introduce an extension for self-supervised learning techniques\ntailored for exploiting the fractal-effect in remote-sensing images. The\nfractal-effect assumes that the same structures (for example rivers, peaks and\nsaddles) will appear in all scales. We demonstrate our method's effectiveness\non elevation data, we also use the effect in inference. We perform an extensive\nanalysis on several classification tasks and emphasize its effectiveness in\ndetecting the same class on different scales. To the best of our knowledge, it\nis the first attempt to build a generic representation for topographic images.",
          "link": "http://arxiv.org/abs/2108.08870",
          "publishedOn": "2021-08-23T01:36:35.990Z",
          "wordCount": 561,
          "title": "Topo2vec: Topography Embedding Using the Fractal Effect. (arXiv:2108.08870v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chug_S/0/1/0/all/0/1\">Sezal Chug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_P/0/1/0/all/0/1\">Priya Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>",
          "description": "Data is expanding at an unimaginable rate, and with this development comes\nthe responsibility of the quality of data. Data Quality refers to the relevance\nof the information present and helps in various operations like decision making\nand planning in a particular organization. Mostly data quality is measured on\nan ad-hoc basis, and hence none of the developed concepts provide any practical\napplication. The current empirical study was undertaken to formulate a concrete\nautomated data quality platform to assess the quality of incoming dataset and\ngenerate a quality label, score and comprehensive report. We utilize various\ndatasets from healthdata.gov, opendata.nhs and Demographics and Health Surveys\n(DHS) Program to observe the variations in the quality score and formulate a\nlabel using Principal Component Analysis(PCA). The results of the current\nempirical study revealed a metric that encompasses nine quality ingredients,\nnamely provenance, dataset characteristics, uniformity, metadata coupling,\npercentage of missing cells and duplicate rows, skewness of data, the ratio of\ninconsistencies of categorical columns, and correlation between these\nattributes. The study also provides an illustrative case study and validation\nof the metric following Mutation Testing approaches. This research study\nprovides an automated platform which takes an incoming dataset and metadata to\nprovide the DQ score, report and label. The results of this study would be\nuseful to data scientists as the value of this quality label would instill\nconfidence before deploying the data for his/her respective practical\napplication.",
          "link": "http://arxiv.org/abs/2108.08905",
          "publishedOn": "2021-08-23T01:36:35.965Z",
          "wordCount": 682,
          "title": "Statistical Learning to Operationalize a Domain Agnostic Data Quality Scoring. (arXiv:2108.08905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rondao_D/0/1/0/all/0/1\">Duarte Rondao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>",
          "description": "Autonomous spacecraft relative navigation technology has been planned for and\napplied to many famous space missions. The development of on-board electronics\nsystems has enabled the use of vision-based and LiDAR-based methods to achieve\nbetter performances. Meanwhile, deep learning has reached great success in\ndifferent areas, especially in computer vision, which has also attracted the\nattention of space researchers. However, spacecraft navigation differs from\nground tasks due to high reliability requirements but lack of large datasets.\nThis survey aims to systematically investigate the current deep learning-based\nautonomous spacecraft relative navigation methods, focusing on concrete orbital\napplications such as spacecraft rendezvous and landing on small bodies or the\nMoon. The fundamental characteristics, primary motivations, and contributions\nof deep learning-based relative navigation algorithms are first summarised from\nthree perspectives of spacecraft rendezvous, asteroid exploration, and terrain\nnavigation. Furthermore, popular visual tracking benchmarks and their\nrespective properties are compared and summarised. Finally, potential\napplications are discussed, along with expected impediments.",
          "link": "http://arxiv.org/abs/2108.08876",
          "publishedOn": "2021-08-23T01:36:35.947Z",
          "wordCount": 611,
          "title": "Deep Learning-based Spacecraft Relative Navigation Methods: A Survey. (arXiv:2108.08876v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Satvik Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1\">Pradyumn Pundir</a>",
          "description": "From the past few years, due to advancements in technologies, the sedentary\nliving style in urban areas is at its peak. This results in individuals getting\na victim of obesity at an early age. There are various health impacts of\nobesity like Diabetes, Heart disease, Blood pressure problems, and many more.\nMachine learning from the past few years is showing its implications in all\nexpertise like forecasting, healthcare, medical imaging, sentiment analysis,\netc. In this work, we aim to provide a framework that uses machine learning\nalgorithms namely, Random Forest, Decision Tree, XGBoost, Extra Trees, and KNN\nto train models that would help predict obesity levels (Classification),\nBodyweight, and fat percentage levels (Regression) using various parameters. We\nalso applied and compared various hyperparameter optimization (HPO) algorithms\nsuch as Genetic algorithm, Random Search, Grid Search, Optuna to further\nimprove the accuracy of the models. The website framework contains various\nother features like making customizable Diet plans, workout plans, and a\ndashboard to track the progress. The framework is built using the Python Flask.\nFurthermore, a weighing scale using the Internet of Things (IoT) is also\nintegrated into the framework to track calories and macronutrients from food\nintake.",
          "link": "http://arxiv.org/abs/2108.08868",
          "publishedOn": "2021-08-23T01:36:35.922Z",
          "wordCount": 669,
          "title": "MOFit: A Framework to reduce Obesity using Machine learning and IoT. (arXiv:2108.08868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammad Mahfujur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>",
          "description": "Domain generalization approaches aim to learn a domain invariant prediction\nmodel for unknown target domains from multiple training source domains with\ndifferent distributions. Significant efforts have recently been committed to\nbroad domain generalization, which is a challenging and topical problem in\nmachine learning and computer vision communities. Most previous domain\ngeneralization approaches assume that the conditional distribution across the\ndomains remain the same across the source domains and learn a domain invariant\nmodel by minimizing the marginal distributions. However, the assumption of a\nstable conditional distribution of the training source domains does not really\nhold in practice. The hyperplane learned from the source domains will easily\nmisclassify samples scattered at the boundary of clusters or far from their\ncorresponding class centres. To address the above two drawbacks, we propose a\ndiscriminative domain-invariant adversarial network (DDIAN) for domain\ngeneralization. The discriminativeness of the features are guaranteed through a\ndiscriminative feature module and domain-invariant features are guaranteed\nthrough the global domain and local sub-domain alignment modules. Extensive\nexperiments on several benchmarks show that DDIAN achieves better prediction on\nunseen target data during training compared to state-of-the-art domain\ngeneralization approaches.",
          "link": "http://arxiv.org/abs/2108.08995",
          "publishedOn": "2021-08-23T01:36:35.916Z",
          "wordCount": 643,
          "title": "Discriminative Domain-Invariant Adversarial Network for Deep Domain Generalization. (arXiv:2108.08995v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Luo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingqing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Recommender system plays a crucial role in modern E-commerce platform. Due to\nthe lack of historical interactions between users and items, cold-start\nrecommendation is a challenging problem. In order to alleviate the cold-start\nissue, most existing methods introduce content and contextual information as\nthe auxiliary information. Nevertheless, these methods assume the recommended\nitems behave steadily over time, while in a typical E-commerce scenario, items\ngenerally have very different performances throughout their life period. In\nsuch a situation, it would be beneficial to consider the long-term return from\nthe item perspective, which is usually ignored in conventional methods.\nReinforcement learning (RL) naturally fits such a long-term optimization\nproblem, in which the recommender could identify high potential items,\nproactively allocate more user impressions to boost their growth, therefore\nimprove the multi-period cumulative gains. Inspired by this idea, we model the\nprocess as a Partially Observable and Controllable Markov Decision Process\n(POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the\nitem lifetime values (LTV) into the recommendation. In RL-LTV, the critic\nstudies historical trajectories of items and predict the future LTV of fresh\nitem, while the actor suggests a score-based policy which maximizes the future\nLTV expectation. Scores suggested by the actor are then combined with classical\nranking scores in a dual-rank framework, therefore the recommendation is\nbalanced with the LTV consideration. Our method outperforms the strong live\nbaseline with a relative improvement of 8.67% and 18.03% on IPV and GMV of\ncold-start items, on one of the largest E-commerce platform.",
          "link": "http://arxiv.org/abs/2108.09141",
          "publishedOn": "2021-08-23T01:36:35.910Z",
          "wordCount": 696,
          "title": "Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. (arXiv:2108.09141v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atzmon_M/0/1/0/all/0/1\">Matan Atzmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Implicit neural representation is a recent approach to learn shape\ncollections as zero level-sets of neural networks, where each shape is\nrepresented by a latent code. So far, the focus has been shape reconstruction,\nwhile shape generalization was mostly left to generic encoder-decoder or\nauto-decoder regularization.\n\nIn this paper we advocate deformation-aware regularization for implicit\nneural representations, aiming at producing plausible deformations as latent\ncode changes. The challenge is that implicit representations do not capture\ncorrespondences between different shapes, which makes it difficult to represent\nand regularize their deformations. Thus, we propose to pair the implicit\nrepresentation of the shapes with an explicit, piecewise linear deformation\nfield, learned as an auxiliary function. We demonstrate that, by regularizing\nthese deformation fields, we can encourage the implicit neural representation\nto induce natural deformations in the learned shape space, such as\nas-rigid-as-possible deformations.",
          "link": "http://arxiv.org/abs/2108.08931",
          "publishedOn": "2021-08-23T01:36:35.896Z",
          "wordCount": 584,
          "title": "Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields. (arXiv:2108.08931v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sifat Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>",
          "description": "Over the past decade, the number of wildfire has increased significantly\naround the world, especially in the State of California. The high-level\nconcentration of greenhouse gas (GHG) emitted by wildfires aggravates global\nwarming that further increases the risk of more fires. Therefore, an accurate\nprediction of wildfire occurrence greatly helps in preventing large-scale and\nlong-lasting wildfires and reducing the consequent GHG emissions. Various\nmethods have been explored for wildfire risk prediction. However, the complex\ncorrelations among a lot of natural and human factors and wildfire ignition\nmake the prediction task very challenging. In this paper, we develop a deep\nlearning based data augmentation approach for wildfire risk prediction. We\nbuild a dataset consisting of diverse features responsible for fire ignition\nand utilize a conditional tabular generative adversarial network to explore the\nunderlying patterns between the target value of risk levels and all involved\nfeatures. For fair and comprehensive comparisons, we compare our proposed\nscheme with five other baseline methods where the former outperformed most of\nthem. To corroborate the robustness, we have also tested the performance of our\nmethod with another dataset that also resulted in better efficiency. By\nadopting the proposed method, we can take preventive strategies of wildfire\nmitigation to reduce global GHG emissions.",
          "link": "http://arxiv.org/abs/2108.08952",
          "publishedOn": "2021-08-23T01:36:35.864Z",
          "wordCount": 645,
          "title": "Mitigating Greenhouse Gas Emissions Through Generative Adversarial Networks Based Wildfire Prediction. (arXiv:2108.08952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08895",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yazdekhasty_P/0/1/0/all/0/1\">Parham Yazdekhasty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zindari_A/0/1/0/all/0/1\">Ali Zindari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nabizadeh_ShahreBabak_Z/0/1/0/all/0/1\">Zahra Nabizadeh-ShahreBabak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Coronavirus has caused hundreds of thousands of deaths. Fatalities could\ndecrease if every patient could get suitable treatment by the healthcare\nsystem. Machine learning, especially computer vision methods based on deep\nlearning, can help healthcare professionals diagnose and treat COVID-19\ninfected cases more efficiently. Hence, infected patients can get better\nservice from the healthcare system and decrease the number of deaths caused by\nthe coronavirus. This research proposes a method for segmenting infected lung\nregions in a CT image. For this purpose, a convolutional neural network with an\nattention mechanism is used to detect infected areas with complex patterns.\nAttention blocks improve the segmentation accuracy by focusing on informative\nparts of the image. Furthermore, a generative adversarial network generates\nsynthetic images for data augmentation and expansion of small available\ndatasets. Experimental results show the superiority of the proposed method\ncompared to some existing procedures.",
          "link": "http://arxiv.org/abs/2108.08895",
          "publishedOn": "2021-08-23T01:36:35.851Z",
          "wordCount": 653,
          "title": "Segmentation of Lungs COVID Infected Regions by Attention Mechanism and Synthetic Data. (arXiv:2108.08895v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Elia Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Francesco Silvestri</a>",
          "description": "A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it anywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these zones is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.",
          "link": "http://arxiv.org/abs/2107.00761",
          "publishedOn": "2021-08-20T01:53:53.880Z",
          "wordCount": 695,
          "title": "On the Bike Spreading Problem. (arXiv:2107.00761v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuben Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haipeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiafa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>",
          "description": "Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support\nextensive applications in next-generation wireless networks in both civil and\nmilitary fields. Empowering UAVs networks intelligence by artificial\nintelligence (AI) especially machine learning (ML) techniques is inevitable and\nappealing to enable the aforementioned applications. To solve the problems of\ntraditional cloud-centric ML for UAV networks such as privacy concern,\nunacceptable latency, and resource burden, a distributed ML technique,\n\\textit(i.e.), federated learning (FL), has been recently proposed to enable\nmultiple UAVs to collaboratively train ML model without letting out raw data.\nHowever, almost all existing FL paradigms are still centralized, \\textit{i.e.},\na central entity is in charge of ML model aggregation and fusion over the whole\nnetwork, which could result in the issue of a single point of failure and are\ninappropriate to UAV networks with both unreliable nodes and links. Thus\nmotivated, in this article, we propose a novel architecture called DFL-UN\n(\\underline{D}ecentralized \\underline{F}ederated \\underline{L}earning for\n\\underline{U}AV \\underline{N}etworks), which enables FL within UAV networks\nwithout a central entity. We also conduct a preliminary simulation study to\nvalidate the feasibility and effectiveness of the DFL-UN architecture. Finally,\nwe discuss the main challenges and potential research directions in the DFL-UN.",
          "link": "http://arxiv.org/abs/2104.07557",
          "publishedOn": "2021-08-20T01:53:53.870Z",
          "wordCount": 690,
          "title": "Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities. (arXiv:2104.07557v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-08-20T01:53:53.864Z",
          "wordCount": 666,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "A myriad of recent breakthroughs in hand-crafted neural architectures for\nvisual recognition have highlighted the urgent need to explore hybrid\narchitectures consisting of diversified building blocks. Meanwhile, neural\narchitecture search methods are surging with an expectation to reduce human\nefforts. However, whether NAS methods can efficiently and effectively handle\ndiversified search spaces with disparate candidates (e.g. CNNs and\ntransformers) is still an open question. In this work, we present Block-wisely\nSelf-supervised Neural Architecture Search (BossNAS), an unsupervised NAS\nmethod that addresses the problem of inaccurate architecture rating caused by\nlarge weight-sharing space and biased supervision in previous methods. More\nspecifically, we factorize the search space into blocks and utilize a novel\nself-supervised training scheme, named ensemble bootstrapping, to train each\nblock separately before searching them as a whole towards the population\ncenter. Additionally, we present HyTra search space, a fabric-like hybrid\nCNN-transformer search space with searchable down-sampling positions. On this\nchallenging search space, our searched model, BossNet-T, achieves up to 82.5%\naccuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute\ntime. Moreover, our method achieves superior architecture rating accuracy with\n0.78 and 0.76 Spearman correlation on the canonical MBConv search space with\nImageNet and on NATS-Bench size search space with CIFAR-100, respectively,\nsurpassing state-of-the-art NAS methods. Code:\nhttps://github.com/changlin31/BossNAS",
          "link": "http://arxiv.org/abs/2103.12424",
          "publishedOn": "2021-08-20T01:53:53.857Z",
          "wordCount": 705,
          "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search. (arXiv:2103.12424v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1\">Raviteja Vemulapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1\">Lior Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Collecting labeled data for the task of semantic segmentation is expensive\nand time-consuming, as it requires dense pixel-level annotations. While recent\nConvolutional Neural Network (CNN) based semantic segmentation approaches have\nachieved impressive results by using large amounts of labeled training data,\ntheir performance drops significantly as the amount of labeled data decreases.\nThis happens because deep CNNs trained with the de facto cross-entropy loss can\neasily overfit to small amounts of labeled data. To address this issue, we\npropose a simple and effective contrastive learning-based training strategy in\nwhich we first pretrain the network using a pixel-wise, label-based contrastive\nloss, and then fine-tune it using the cross-entropy loss. This approach\nincreases intra-class compactness and inter-class separability, thereby\nresulting in a better pixel classifier. We demonstrate the effectiveness of the\nproposed training strategy using the Cityscapes and PASCAL VOC 2012\nsegmentation datasets. Our results show that pretraining with the proposed\ncontrastive loss results in large performance gains (more than 20% absolute\nimprovement in some settings) when the amount of labeled data is limited. In\nmany settings, the proposed contrastive pretraining strategy, which does not\nuse any additional data, is able to match or outperform the widely-used\nImageNet pretraining strategy that uses more than a million additional labeled\nimages.",
          "link": "http://arxiv.org/abs/2012.06985",
          "publishedOn": "2021-08-20T01:53:53.839Z",
          "wordCount": 710,
          "title": "Contrastive Learning for Label-Efficient Semantic Segmentation. (arXiv:2012.06985v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soorki_M/0/1/0/all/0/1\">Mehdi Naderi Soorki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Choong Seon Hong</a>",
          "description": "In this paper, a novel framework for guaranteeing ultra-reliable millimeter\nwave (mmW) communications using multiple artificial intelligence (AI)-enabled\nreconfigurable intelligent surfaces (RISs) is proposed. The use of multiple\nAI-powered RISs allows changing the propagation direction of the signals\ntransmitted from a mmW access point (AP) thereby improving coverage\nparticularly for non-line-of-sight (NLoS) areas. However, due to the\npossibility of highly stochastic blockage over mmW links, designing an\nintelligent controller to jointly optimize the mmW AP beam and RIS phase shifts\nis a daunting task. In this regard, first, a parametric risk-sensitive episodic\nreturn is proposed to maximize the expected bit rate and mitigate the risk of\nmmW link blockage. Then, a closed-form approximation of the policy gradient of\nthe risk-sensitive episodic return is analytically derived. Next, the problem\nof joint beamforming for mmW AP and phase shift control for mmW RISs is modeled\nas an identical payoff stochastic game within a cooperative multi-agent\nenvironment, in which the agents are the mmW AP and the RISs. Two centralized\nand distributed controllers are proposed to control the policies of the mmW AP\nand RISs. To directly find an optimal solution, the parametric functional-form\npolicies for these controllers are modeled using deep recurrent neural networks\n(RNNs). Simulation results show that the error between policies of the optimal\nand the RNN-based controllers is less than 1.5%. Moreover, the variance of the\nachievable rates resulting from the deep RNN-based controllers is 60% less than\nthe variance of the risk-averse baseline.",
          "link": "http://arxiv.org/abs/2104.00075",
          "publishedOn": "2021-08-20T01:53:53.831Z",
          "wordCount": 722,
          "title": "Ultra-Reliable Indoor Millimeter Wave Communications using Multiple Artificial Intelligence-Powered Intelligent Surfaces. (arXiv:2104.00075v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaman_A/0/1/0/all/0/1\">Anadi Chaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belius_D/0/1/0/all/0/1\">David Belius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Convolutional neural networks (CNNs) have been tremendously successful in\nsolving imaging inverse problems. To understand their success, an effective\nstrategy is to construct simpler and mathematically more tractable\nconvolutional sparse coding (CSC) models that share essential ingredients with\nCNNs. Existing CSC methods, however, underperform leading CNNs in challenging\ninverse problems. We hypothesize that the performance gap may be attributed in\npart to how they process images at different spatial scales: While many CNNs\nuse multiscale feature representations, existing CSC models mostly rely on\nsingle-scale dictionaries. To close the performance gap, we thus propose a\nmultiscale convolutional dictionary structure. The proposed dictionary\nstructure is derived from the U-Net, arguably the most versatile and widely\nused CNN for image-to-image learning problems. We show that incorporating the\nproposed multiscale dictionary in an otherwise standard CSC framework yields\nperformance competitive with state-of-the-art CNNs across a range of\nchallenging inverse problems including CT and MRI reconstruction. Our work thus\ndemonstrates the effectiveness and scalability of the multiscale CSC approach\nin solving challenging inverse problems.",
          "link": "http://arxiv.org/abs/2011.12815",
          "publishedOn": "2021-08-20T01:53:53.824Z",
          "wordCount": 641,
          "title": "Learning Multiscale Convolutional Dictionaries for Image Reconstruction. (arXiv:2011.12815v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1\">Anum Talpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1\">Mohan Gurusamy</a>",
          "description": "Machine Learning (ML) has emerged as an attractive and viable technique to\nprovide effective solutions for a wide range of application domains. An\nimportant application domain is vehicular networks wherein ML-based approaches\nare found to be very useful to address various problems. The use of wireless\ncommunication between vehicular nodes and/or infrastructure makes it vulnerable\nto different types of attacks. In this regard, ML and its variants are gaining\npopularity to detect attacks and deal with different kinds of security issues\nin vehicular communication. In this paper, we present a comprehensive survey of\nML-based techniques for different security issues in vehicular networks. We\nfirst briefly introduce the basics of vehicular networks and different types of\ncommunications. Apart from the traditional vehicular networks, we also consider\nmodern vehicular network architectures. We propose a taxonomy of security\nattacks in vehicular networks and discuss various security challenges and\nrequirements. We classify the ML techniques developed in the literature\naccording to their use in vehicular network applications. We explain the\nsolution approaches and working principles of these ML techniques in addressing\nvarious security challenges and provide insightful discussion. The limitations\nand challenges in using ML-based methods in vehicular networks are discussed.\nFinally, we present observations and lessons learned before we conclude our\nwork.",
          "link": "http://arxiv.org/abs/2105.15035",
          "publishedOn": "2021-08-20T01:53:53.815Z",
          "wordCount": 684,
          "title": "Machine Learning for Security in Vehicular Networks: A Comprehensive Survey. (arXiv:2105.15035v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reiersen_G/0/1/0/all/0/1\">Gyri Reiersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1\">David Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by up to 10-times for tropical\nreforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.",
          "link": "http://arxiv.org/abs/2107.11320",
          "publishedOn": "2021-08-20T01:53:53.809Z",
          "wordCount": 693,
          "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery. (arXiv:2107.11320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arkabandhu Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingchao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>",
          "description": "Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.",
          "link": "http://arxiv.org/abs/2101.00562",
          "publishedOn": "2021-08-20T01:53:53.788Z",
          "wordCount": 596,
          "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier. (arXiv:2101.00562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie P. Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>",
          "description": "Pushing is a fundamental robotic skill. Existing work has shown how to\nexploit models of pushing to achieve a variety of tasks, including grasping\nunder uncertainty, in-hand manipulation and clearing clutter. Such models,\nhowever, are approximate, which limits their applicability. Learning-based\nmethods can reason directly from raw sensory data with accuracy, and have the\npotential to generalize to a wider diversity of scenarios. However, developing\nand testing such methods requires rich-enough datasets. In this paper we\nintroduce Omnipush, a dataset with high variety of planar pushing behavior. In\nparticular, we provide 250 pushes for each of 250 objects, all recorded with\nRGB-D and a high precision tracking system. The objects are constructed so as\nto systematically explore key factors that affect pushing -- the shape of the\nobject and its mass distribution -- which have not been broadly explored in\nprevious datasets, and allow to study generalization in model learning.\nOmnipush includes a benchmark for meta-learning dynamic models, which requires\nalgorithms that make good predictions and estimate their own uncertainty. We\nalso provide an RGB video prediction benchmark and propose other relevant tasks\nthat can be suited with this dataset.\n\nData and code are available at\n\\url{https://web.mit.edu/mcube/omnipush-dataset/}.",
          "link": "http://arxiv.org/abs/1910.00618",
          "publishedOn": "2021-08-20T01:53:53.782Z",
          "wordCount": 697,
          "title": "Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video. (arXiv:1910.00618v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1\">Vasu Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "Adversarial training is one of the most effective defenses against\nadversarial attacks. Previous works suggest that overfitting is a dominant\nphenomenon in adversarial training leading to a large generalization gap\nbetween test and train accuracy in neural networks. In this work, we show that\nthe observed generalization gap is closely related to the choice of the\nactivation function. In particular, we show that using activation functions\nwith low (exact or approximate) curvature values has a regularization effect\nthat significantly reduces both the standard and robust generalization gaps in\nadversarial training. We observe this effect for both differentiable/smooth\nactivations such as SiLU as well as non-differentiable/non-smooth activations\nsuch as LeakyReLU. In the latter case, the \"approximate\" curvature of the\nactivation is low. Finally, we show that for activation functions with low\ncurvature, the double descent phenomenon for adversarially trained models does\nnot occur.",
          "link": "http://arxiv.org/abs/2102.07861",
          "publishedOn": "2021-08-20T01:53:53.775Z",
          "wordCount": 605,
          "title": "Low Curvature Activations Reduce Overfitting in Adversarial Training. (arXiv:2102.07861v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1\">Ezekiel Soremekun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1\">Sakshi Udeshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a>",
          "description": "Software often produces biased outputs. In particular, machine learning (ML)\nbased software are known to produce erroneous predictions when processing\ndiscriminatory inputs. Such unfair program behavior can be caused by societal\nbias. In the last few years, Amazon, Microsoft and Google have provided\nsoftware services that produce unfair outputs, mostly due to societal bias\n(e.g. gender or race). In such events, developers are saddled with the task of\nconducting fairness testing. Fairness testing is challenging; developers are\ntasked with generating discriminatory inputs that reveal and explain biases.\n\nWe propose a grammar-based fairness testing approach (called ASTRAEA) which\nleverages context-free grammars to generate discriminatory inputs that reveal\nfairness violations in software systems. Using probabilistic grammars, ASTRAEA\nalso provides fault diagnosis by isolating the cause of observed software bias.\nASTRAEA's diagnoses facilitate the improvement of ML fairness.\n\nASTRAEA was evaluated on 18 software systems that provide three major natural\nlanguage processing (NLP) services. In our evaluation, ASTRAEA generated\nfairness violations with a rate of ~18%. ASTRAEA generated over 573K\ndiscriminatory test cases and found over 102K fairness violations. Furthermore,\nASTRAEA improves software fairness by ~76%, via model-retraining.",
          "link": "http://arxiv.org/abs/2010.02542",
          "publishedOn": "2021-08-20T01:53:53.768Z",
          "wordCount": 650,
          "title": "Astraea: Grammar-based Fairness Testing. (arXiv:2010.02542v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OLuing_M/0/1/0/all/0/1\">Mervyn O&#x27;Luing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1\">Steven Prestwich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarim_S/0/1/0/all/0/1\">S. Armagan Tarim</a>",
          "description": "This study combines simulated annealing with delta evaluation to solve the\njoint stratification and sample allocation problem. In this problem, atomic\nstrata are partitioned into mutually exclusive and collectively exhaustive\nstrata. Each stratification is a solution, the quality of which is measured by\nits cost. The Bell number of possible solutions is enormous for even a moderate\nnumber of atomic strata and an additional layer of complexity is added with the\nevaluation time of each solution. Many larger scale combinatorial optimisation\nproblems cannot be solved to optimality because the search for an optimum\nsolution requires a prohibitive amount of computation time; a number of local\nsearch heuristic algorithms have been designed for this problem but these can\nbecome trapped in local minima preventing any further improvements. We add to\nthe existing suite of local search algorithms a simulated annealing algorithm\nthat allows for an escape from local minima and uses delta evaluation to\nexploit the similarity between consecutive solutions and thereby reduce the\nevaluation time. We compare the simulated annealing algorithm with two recent\nalgorithms. In both cases the SAA attains a solution of comparable quality in\nconsiderably less computation time.",
          "link": "http://arxiv.org/abs/2011.13006",
          "publishedOn": "2021-08-20T01:53:53.762Z",
          "wordCount": 669,
          "title": "A Simulated Annealing Algorithm for Joint Stratification and Sample Allocation Designs. (arXiv:2011.13006v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14410",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>",
          "description": "Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.",
          "link": "http://arxiv.org/abs/2107.14410",
          "publishedOn": "2021-08-20T01:53:53.744Z",
          "wordCount": 550,
          "title": "The Adaptive Multi-Factor Model and the Financial Market. (arXiv:2107.14410v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09235",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_G/0/1/0/all/0/1\">Guanyang Wang</a>",
          "description": "The exchange algorithm is one of the most popular extensions of the\nMetropolis--Hastings algorithm to sample from doubly-intractable distributions.\nHowever, the theoretical exploration of the exchange algorithm is very limited.\nFor example, natural questions like `Does exchange algorithm converge at a\ngeometric rate?' or `Does the exchange algorithm admit a Central Limit\nTheorem?' have not been answered yet. In this paper, we study the theoretical\nproperties of the exchange algorithm, in terms of asymptotic variance and\nconvergence speed. We compare the exchange algorithm with the original\nMetropolis--Hastings algorithm and provide both necessary and sufficient\nconditions for the geometric ergodicity of the exchange algorithm. Moreover, we\nprove that our results can be applied to various practical applications such as\nlocation models, Gaussian models, Poisson models, and a large class of\nexponential families, which includes most of the practical applications of the\nexchange algorithm. A central limit theorem for the exchange algorithm is also\nestablished. Our results justify the theoretical usefulness of the exchange\nalgorithm.",
          "link": "http://arxiv.org/abs/2005.09235",
          "publishedOn": "2021-08-20T01:53:53.737Z",
          "wordCount": 651,
          "title": "On the Theoretical Properties of the Exchange Algorithm. (arXiv:2005.09235v4 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen-Hsuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>",
          "description": "Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.",
          "link": "http://arxiv.org/abs/2104.06405",
          "publishedOn": "2021-08-20T01:53:53.731Z",
          "wordCount": 657,
          "title": "BARF: Bundle-Adjusting Neural Radiance Fields. (arXiv:2104.06405v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-08-20T01:53:53.721Z",
          "wordCount": 703,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>",
          "description": "This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.",
          "link": "http://arxiv.org/abs/2108.04351",
          "publishedOn": "2021-08-20T01:53:53.681Z",
          "wordCount": 664,
          "title": "Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_C/0/1/0/all/0/1\">Chaitanya K. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1\">Quentin Cappart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1\">Louis-Martin Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_T/0/1/0/all/0/1\">Thomas Laurent</a>",
          "description": "End-to-end training of neural network solvers for combinatorial optimization\nproblems such as the Travelling Salesman Problem is intractable and inefficient\nbeyond a few hundreds of nodes. While state-of-the-art Machine Learning\napproaches perform closely to classical solvers when trained on trivially small\nsizes, they are unable to generalize the learnt policy to larger instances of\npractical scales. Towards leveraging transfer learning to solve large-scale\nTSPs, this paper identifies inductive biases, model architectures and learning\nalgorithms that promote generalization to instances larger than those seen in\ntraining. Our controlled experiments provide the first principled investigation\ninto such zero-shot generalization, revealing that extrapolating beyond\ntraining data requires rethinking the neural combinatorial optimization\npipeline, from network layers and learning paradigms to evaluation protocols.",
          "link": "http://arxiv.org/abs/2006.07054",
          "publishedOn": "2021-08-20T01:53:53.633Z",
          "wordCount": 604,
          "title": "Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiaheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "We show when maximizing a properly defined $f$-divergence measure with\nrespect to a classifier's predictions and the supervised labels is robust with\nlabel noise. Leveraging its variational form, we derive a nice decoupling\nproperty for a family of $f$-divergence measures when label noise presents,\nwhere the divergence is shown to be a linear combination of the variational\ndifference defined on the clean distribution and a bias term introduced due to\nthe noise. The above derivation helps us analyze the robustness of different\n$f$-divergence functions. With established robustness, this family of\n$f$-divergence functions arises as useful metrics for the problem of learning\nwith noisy labels, which do not require the specification of the labels' noise\nrate. When they are possibly not robust, we propose fixes to make them so. In\naddition to the analytical results, we present thorough experimental evidence.\nOur code is available at\nhttps://github.com/UCSC-REAL/Robust-f-divergence-measures.",
          "link": "http://arxiv.org/abs/2011.03687",
          "publishedOn": "2021-08-20T01:53:53.627Z",
          "wordCount": 622,
          "title": "When Optimizing $f$-divergence is Robust with Label Noise. (arXiv:2011.03687v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Choi_Y/0/1/0/all/0/1\">Yeunju Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suh_Y/0/1/0/all/0/1\">Youngjoo Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>",
          "description": "Although recent end-to-end text-to-speech (TTS) systems have achieved\nhigh-quality speech synthesis, there are still several factors that degrade the\nquality of synthesized speech, including lack of training data or information\nloss during knowledge distillation. To address the problem, we propose a novel\nway to train a TTS model under the supervision of perceptual loss, which\nmeasures the distance between the maximum speech quality score and the\npredicted one. We first pre-train a mean opinion score (MOS) prediction model\nand then train a TTS model in the direction of maximizing the MOS of\nsynthesized speech predicted by the pre-trained MOS prediction model. Through\nthis method, we can improve the quality of synthesized speech universally\n(i.e., regardless of the network architecture or the cause of the speech\nquality degradation) and efficiently (i.e., without increasing the inference\ntime or the model complexity). The evaluation results for MOS and phone error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.",
          "link": "http://arxiv.org/abs/2011.01174",
          "publishedOn": "2021-08-20T01:53:53.619Z",
          "wordCount": 639,
          "title": "Perceptually Guided End-to-End Text-to-Speech With MOS Prediction. (arXiv:2011.01174v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03465",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1\">Bikram Sahoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1\">Naimat Ullah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1\">Alexander Zelikovskiy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>",
          "description": "With the rapid spread of the novel coronavirus (COVID-19) across the globe\nand its continuous mutation, it is of pivotal importance to design a system to\nidentify different known (and unknown) variants of SARS-CoV-2. Identifying\nparticular variants helps to understand and model their spread patterns, design\neffective mitigation strategies, and prevent future outbreaks. It also plays a\ncrucial role in studying the efficacy of known vaccines against each variant\nand modeling the likelihood of breakthrough infections. It is well known that\nthe spike protein contains most of the information/variation pertaining to\ncoronavirus variants.\n\nIn this paper, we use spike sequences to classify different variants of the\ncoronavirus in humans. We show that preserving the order of the amino acids\nhelps the underlying classifiers to achieve better performance. We also show\nthat we can train our model to outperform the baseline algorithms using only a\nsmall number of training samples ($1\\%$ of the data). Finally, we show the\nimportance of the different amino acids which play a key role in identifying\nvariants and how they coincide with those reported by the USA's Centers for\nDisease Control and Prevention (CDC).",
          "link": "http://arxiv.org/abs/2108.03465",
          "publishedOn": "2021-08-20T01:53:53.613Z",
          "wordCount": 688,
          "title": "A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1\">Maithra Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>",
          "description": "Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.",
          "link": "http://arxiv.org/abs/2108.08810",
          "publishedOn": "2021-08-20T01:53:53.605Z",
          "wordCount": 622,
          "title": "Do Vision Transformers See Like Convolutional Neural Networks?. (arXiv:2108.08810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weyns_D/0/1/0/all/0/1\">Danny Weyns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1\">Thomas B&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe8; Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belbachir_A/0/1/0/all/0/1\">Ahmed Nabil Belbachir</a>",
          "description": "Computing systems form the backbone of many aspects of our life, hence they\nare becoming as vital as water, electricity, and road infrastructures for our\nsociety. Yet, engineering long running computing systems that achieve their\ngoals in ever-changing environments pose significant challenges. Currently, we\ncan build computing systems that adjust or learn over time to match changes\nthat were anticipated. However, dealing with unanticipated changes, such as\nanomalies, novelties, new goals or constraints, requires system evolution,\nwhich remains in essence a human-driven activity. Given the growing complexity\nof computing systems and the vast amount of highly complex data to process,\nthis approach will eventually become unmanageable. To break through the status\nquo, we put forward a new paradigm for the design and operation of computing\nsystems that we coin \"lifelong computing.\" The paradigm starts from\ncomputing-learning systems that integrate computing/service modules and\nlearning modules. Computing warehouses offer such computing elements together\nwith data sheets and usage guides. When detecting anomalies, novelties, new\ngoals or constraints, a lifelong computing system activates an evolutionary\nself-learning engine that runs online experiments to determine how the\ncomputing-learning system needs to evolve to deal with the changes, thereby\nchanging its architecture and integrating new computing elements from computing\nwarehouses as needed. Depending on the domain at hand, some activities of\nlifelong computing systems can be supported by humans. We motivate the need for\nlifelong computing with a future fish farming scenario, outline a blueprint\narchitecture for lifelong computing systems, and highlight key research\nchallenges to realise the vision of lifelong computing.",
          "link": "http://arxiv.org/abs/2108.08802",
          "publishedOn": "2021-08-20T01:53:53.590Z",
          "wordCount": 683,
          "title": "Lifelong Computing. (arXiv:2108.08802v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bateni_P/0/1/0/all/0/1\">Peyman Bateni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Jarred Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state-of-the-art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks.",
          "link": "http://arxiv.org/abs/2006.12245",
          "publishedOn": "2021-08-20T01:53:53.584Z",
          "wordCount": 587,
          "title": "Enhancing Few-Shot Image Classification with Unlabelled Examples. (arXiv:2006.12245v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04921",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wellawatte_G/0/1/0/all/0/1\">Geemi P. Wellawatte</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chakraborty_M/0/1/0/all/0/1\">Maghesree Chakraborty</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gandhi_H/0/1/0/all/0/1\">Heta A. Gandhi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+White_A/0/1/0/all/0/1\">Andrew D. White</a>",
          "description": "The selection of coarse-grained (CG) mapping operators is a critical step for\nCG molecular dynamics (MD) simulation. It is still an open question about what\nis optimal for this choice and there is a need for theory. The current\nstate-of-the art method is mapping operators manually selected by experts. In\nthis work, we demonstrate an automated approach by viewing this problem as\nsupervised learning where we seek to reproduce the mapping operators produced\nby experts. We present a graph neural network based CG mapping predictor called\nDEEP SUPERVISED GRAPH PARTITIONING MODEL(DSGPM) that treats mapping operators\nas a graph segmentation problem. DSGPM is trained on a novel dataset,\nHuman-annotated Mappings (HAM), consisting of 1,206 molecules with expert\nannotated mapping operators. HAM can be used to facilitate further research in\nthis area. Our model uses a novel metric learning objective to produce\nhigh-quality atomic features that are used in spectral clustering. The results\nshow that the DSGPM outperforms state-of-the-art methods in the field of graph\nsegmentation. Finally, we find that predicted CG mapping operators indeed\nresult in good CG MD models when used in simulation.",
          "link": "http://arxiv.org/abs/2007.04921",
          "publishedOn": "2021-08-20T01:53:53.577Z",
          "wordCount": 656,
          "title": "Graph Neural Network Based Coarse-Grained Mapping Prediction. (arXiv:2007.04921v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10253",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyi Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Leng_J/0/1/0/all/0/1\">Jiaqi Leng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_T/0/1/0/all/0/1\">Tongyang Li</a>",
          "description": "We initiate the study of quantum algorithms for escaping from saddle points\nwith provable guarantee. Given a function $f\\colon\\mathbb{R}^{n}\\to\\mathbb{R}$,\nour quantum algorithm outputs an $\\epsilon$-approximate second-order stationary\npoint using $\\tilde{O}(\\log^{2} (n)/\\epsilon^{1.75})$ queries to the quantum\nevaluation oracle (i.e., the zeroth-order oracle). Compared to the classical\nstate-of-the-art algorithm by Jin et al. with $\\tilde{O}(\\log^{6}\n(n)/\\epsilon^{1.75})$ queries to the gradient oracle (i.e., the first-order\noracle), our quantum algorithm is polynomially better in terms of $\\log n$ and\nmatches its complexity in terms of $1/\\epsilon$. Technically, our main\ncontribution is the idea of replacing the classical perturbations in gradient\ndescent methods by simulating quantum wave equations, which constitutes the\nimprovement in the quantum query complexity with $\\log n$ factors for escaping\nfrom saddle points. We also show how to use a quantum gradient computation\nalgorithm due to Jordan to replace the classical gradient queries by quantum\nevaluation queries with the same complexity. Finally, we also perform numerical\nexperiments that support our theoretical findings.",
          "link": "http://arxiv.org/abs/2007.10253",
          "publishedOn": "2021-08-20T01:53:53.569Z",
          "wordCount": 634,
          "title": "Quantum algorithms for escaping from saddle points. (arXiv:2007.10253v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08752",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Feng_D/0/1/0/all/0/1\">Dai Feng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baumgartner_R/0/1/0/all/0/1\">Richard Baumgartner</a>",
          "description": "Kernels ensuing from tree ensembles such as random forest (RF) or gradient\nboosted trees (GBT), when used for kernel learning, have been shown to be\ncompetitive to their respective tree ensembles (particularly in higher\ndimensional scenarios). On the other hand, it has been also shown that\nperformance of the kernel algorithms depends on the degree of the kernel-target\nalignment. However, the kernel-target alignment for kernel learning based on\nthe tree ensembles has not been investigated and filling this gap is the main\ngoal of our work.\n\nUsing the eigenanalysis of the kernel matrix, we demonstrate that for\ncontinuous targets good performance of the tree-based kernel learning is\nassociated with strong kernel-target alignment. Moreover, we show that well\nperforming tree ensemble based kernels are characterized by strong target\naligned components that are expressed through scalar products between the\neigenvectors of the kernel matrix and the target. This suggests that when tree\nensemble based kernel learning is successful, relevant information for the\nsupervised problem is concentrated near lower dimensional manifold spanned by\nthe target aligned components. Persistence of the strong target aligned\ncomponents in tree ensemble based kernels is further supported by sensitivity\nanalysis via landmark learning. In addition to a comprehensive simulation\nstudy, we also provide experimental results from several real life data sets\nthat are in line with the simulations.",
          "link": "http://arxiv.org/abs/2108.08752",
          "publishedOn": "2021-08-20T01:53:53.562Z",
          "wordCount": 661,
          "title": "A Framework for an Assessment of the Kernel-target Alignment in Tree Ensemble Kernel Learning. (arXiv:2108.08752v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00570",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_T/0/1/0/all/0/1\">Tommy Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Malik_A/0/1/0/all/0/1\">Avinash Malik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bear_L/0/1/0/all/0/1\">Laura R. Bear</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Trew_M/0/1/0/all/0/1\">Mark L. Trew</a>",
          "description": "The proposed method re-frames traditional inverse problems of\nelectrocardiography into regression problems, constraining the solution space\nby decomposing signals with multidimensional Gaussian impulse basis functions.\nImpulse HSPs were generated with single Gaussian basis functions at discrete\nheart surface locations and projected to corresponding BSPs using a volume\nconductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to\nregular 2D surface meshes and used to train a neural network. Predictive\ncapabilities of the network were tested with unseen synthetic and experimental\ndata. A dense full connected single hidden layer neural network was trained to\nmap body surface impulses to heart surface Gaussian basis functions for\nreconstructing HSP. Synthetic pulses moving across the heart surface were\npredicted from the neural network with root mean squared error of $9.1\\pm1.4$%.\nPredicted signals were robust to noise up to 20 dB and errors due to\ndisplacement and rotation of the heart within the torso were bounded and\npredictable. A shift of the heart 40 mm toward the spine resulted in a 4\\%\nincrease in signal feature localization error. The set of training impulse\nfunction data could be reduced and prediction error remained bounded. Recorded\nHSPs from in-vitro pig hearts were reliably decomposed using space-time\nGaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean\nabsolute error of $10.4\\pm11.4$ ms. Other pacing scenarios were analyzed with\nsimilar success. Conclusion: Impulses from Gaussian basis functions are\npotentially an effective and robust way to train simple neural network data\nmodels for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the\nneural network can be used to generate activation maps that non-invasively\nidentify features of cardiac electrical dysfunction and can guide subsequent\ntreatment options.",
          "link": "http://arxiv.org/abs/2102.00570",
          "publishedOn": "2021-08-20T01:53:53.545Z",
          "wordCount": 755,
          "title": "Impulse data models for the inverse problem of electrocardiography. (arXiv:2102.00570v3 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>",
          "description": "In the past few years, we have witnessed remarkable breakthroughs in\nself-supervised representation learning. Despite the success and adoption of\nrepresentations learned through this paradigm, much is yet to be understood\nabout how different training methods and datasets influence performance on\ndownstream tasks. In this paper, we analyze contrastive approaches as one of\nthe most successful and popular variants of self-supervised representation\nlearning. We perform this analysis from the perspective of the training\nalgorithms, pre-training datasets and end tasks. We examine over 700 training\nexperiments including 30 encoders, 4 pre-training datasets and 20 diverse\ndownstream tasks. Our experiments address various questions regarding the\nperformance of self-supervised models compared to their supervised\ncounterparts, current benchmarks used for evaluation, and the effect of the\npre-training data on end task performance. Our Visual Representation Benchmark\n(ViRB) is available at: https://github.com/allenai/virb.",
          "link": "http://arxiv.org/abs/2103.14005",
          "publishedOn": "2021-08-20T01:53:53.538Z",
          "wordCount": 608,
          "title": "Contrasting Contrastive Self-Supervised Representation Learning Pipelines. (arXiv:2103.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1\">Megh Shukla</a>",
          "description": "Active learning algorithms select a subset of data for annotation to maximize\nthe model performance on a budget. One such algorithm is Expected Gradient\nLength, which as the name suggests uses the approximate gradient induced per\nexample in the sampling process. While Expected Gradient Length has been\nsuccessfully used for classification and regression, the formulation for\nregression remains intuitively driven. Hence, our theoretical contribution\ninvolves deriving this formulation, thereby supporting the experimental\nevidence. Subsequently, we show that expected gradient length in regression is\nequivalent to Bayesian uncertainty. If certain assumptions are infeasible, our\nalgorithmic contribution (EGL++) approximates the effect of ensembles with a\nsingle deterministic network. Instead of computing multiple possible inferences\nper input, we leverage previously annotated samples to quantify the probability\nof previous labels being the true label. Such an approach allows us to extend\nexpected gradient length to a new task: human pose estimation. We perform\nexperimental validation on two human pose datasets (MPII and LSP/LSPET),\nhighlighting the interpretability and competitiveness of EGL++ with different\nactive learning algorithms for human pose estimation.",
          "link": "http://arxiv.org/abs/2104.09493",
          "publishedOn": "2021-08-20T01:53:53.531Z",
          "wordCount": 649,
          "title": "Bayesian Uncertainty and Expected Gradient Length - Regression: Two Sides Of The Same Coin?. (arXiv:2104.09493v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1\">Shay Vargaftik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basat_R/0/1/0/all/0/1\">Ran Ben Basat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portnoy_A/0/1/0/all/0/1\">Amit Portnoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_G/0/1/0/all/0/1\">Gal Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1\">Yaniv Ben-Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1\">Michael Mitzenmacher</a>",
          "description": "Federated learning commonly relies on algorithms such as distributed\n(mini-batch) SGD, where multiple clients compute their gradients and send them\nto a central coordinator for averaging and updating the model. To optimize the\ntransmission time and the scalability of the training process, clients often\nuse lossy compression to reduce the message sizes. DRIVE is a recent state of\nthe art algorithm that compresses gradients using one bit per coordinate (with\nsome lower-order overhead). In this technical report, we generalize DRIVE to\nsupport any bandwidth constraint as well as extend it to support heterogeneous\nclient resources and make it robust to packet loss.",
          "link": "http://arxiv.org/abs/2108.08842",
          "publishedOn": "2021-08-20T01:53:53.524Z",
          "wordCount": 561,
          "title": "Communication-Efficient Federated Learning via Robust Distributed Mean Estimation. (arXiv:2108.08842v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08775",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankar_R/0/1/0/all/0/1\">Rahul Sankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhudev_A/0/1/0/all/0/1\">Amithash M Prabhudev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahesh_P/0/1/0/all/0/1\">P A Mahesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prakashini_K/0/1/0/all/0/1\">K Prakashini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudha Kiran Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "The world is going through a challenging phase due to the disastrous effect\ncaused by the COVID-19 pandemic on the healthcare system and the economy. The\nrate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of\nCOVID-19 have put the healthcare systems in disruption across the globe. Due to\nthis, the task of accurately screening COVID-19 cases has become of utmost\npriority. Since the virus infects the respiratory system, Chest X-Ray is an\nimaging modality that is adopted extensively for the initial screening. We have\nperformed a comprehensive study that uses CXR images to identify COVID-19 cases\nand realized the necessity of having a more generalizable model. We utilize\nMobileNetV2 architecture as the feature extractor and integrate it into Capsule\nNetworks to construct a fully automated and lightweight model termed as\nMobileCaps. MobileCaps is trained and evaluated on the publicly available\ndataset with the model ensembling and Bayesian optimization strategies to\nefficiently classify CXR images of patients with COVID-19 from non-COVID-19\npneumonia and healthy cases. The proposed model is further evaluated on two\nadditional RT-PCR confirmed datasets to demonstrate the generalizability. We\nalso introduce MobileCaps-S and leverage it for performing severity assessment\nof CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema\n(RALE) scoring technique. Our classification model achieved an overall recall\nof 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,\nnon-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity\nassessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that\nthe proposed models have fewer trainable parameters than the state-of-the-art\nmodels reported in the literature, we believe our models will go a long way in\naiding healthcare systems in the battle against the pandemic.",
          "link": "http://arxiv.org/abs/2108.08775",
          "publishedOn": "2021-08-20T01:53:53.518Z",
          "wordCount": 814,
          "title": "MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images. (arXiv:2108.08775v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_T/0/1/0/all/0/1\">Tom Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafi_N/0/1/0/all/0/1\">Nasik Nafi</a>",
          "description": "Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n\nIn the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n\nOur approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium",
          "link": "http://arxiv.org/abs/2104.04162",
          "publishedOn": "2021-08-20T01:53:53.501Z",
          "wordCount": 675,
          "title": "eGAN: Unsupervised approach to class imbalance using transfer learning. (arXiv:2104.04162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cawood_P/0/1/0/all/0/1\">Pieter Cawood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L. van Zyl</a>",
          "description": "We investigate ensembling techniques in forecasting and examine their\npotential for use in nonseasonal time-series similar to those in the early days\nof the COVID-19 pandemic. Developing improved forecast methods is essential as\nthey provide data-driven decisions to organisations and decision-makers during\ncritical phases. We propose using late data fusion, using a stacked ensemble of\ntwo forecasting models and two meta-features that prove their predictive power\nduring a preliminary forecasting stage. The final ensembles include a Prophet\nand long short term memory (LSTM) neural network as base models. The base\nmodels are combined by a multilayer perceptron (MLP), taking into account\nmeta-features that indicate the highest correlation with each base model's\nforecast accuracy. We further show that the inclusion of meta-features\ngenerally improves the ensemble's forecast accuracy across two forecast\nhorizons of seven and fourteen days. This research reinforces previous work and\ndemonstrates the value of combining traditional statistical models with deep\nlearning models to produce more accurate forecast models for time-series across\ndomains.",
          "link": "http://arxiv.org/abs/2108.08723",
          "publishedOn": "2021-08-20T01:53:53.495Z",
          "wordCount": 654,
          "title": "Feature-weighted Stacking for Nonseasonal Time Series Forecasts: A Case Study of the COVID-19 Epidemic Curves. (arXiv:2108.08723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_A/0/1/0/all/0/1\">Alexander Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>",
          "description": "Large-scale, two-sided matching platforms must find market outcomes that\nalign with user preferences while simultaneously learning these preferences\nfrom data. However, since preferences are inherently uncertain during learning,\nthe classical notion of stability (Gale and Shapley, 1962; Shapley and Shubik,\n1971) is unattainable in these settings. To bridge this gap, we develop a\nframework and algorithms for learning stable market outcomes under uncertainty.\nOur primary setting is matching with transferable utilities, where the platform\nboth matches agents and sets monetary transfers between them. We design an\nincentive-aware learning objective that captures the distance of a market\noutcome from equilibrium. Using this objective, we analyze the complexity of\nlearning as a function of preference structure, casting learning as a\nstochastic multi-armed bandit problem. Algorithmically, we show that \"optimism\nin the face of uncertainty,\" the principle underlying many bandit algorithms,\napplies to a primal-dual formulation of matching with transfers and leads to\nnear-optimal regret bounds. Our work takes a first step toward elucidating when\nand how stable matchings arise in large, data-driven marketplaces.",
          "link": "http://arxiv.org/abs/2108.08843",
          "publishedOn": "2021-08-20T01:53:53.488Z",
          "wordCount": 616,
          "title": "Learning Equilibria in Matching Markets from Bandit Feedback. (arXiv:2108.08843v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1805.05052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1\">Alexander Jung</a>",
          "description": "Machine learning (ML) has become a commodity in our every-day lives. We\nroutinely ask ML empowered smartphones to suggest lovely food places or to\nguide us through a strange place. ML methods have also become standard tools in\nmany fields of science and engineering. A plethora of ML applications transform\nhuman lives at unprecedented pace and scale. This book portrays ML as the\ncombination of three basic components: data, model and loss. ML methods combine\nthese three components within computationally efficient implementations of the\nbasic scientific principle \"trial and error\". This principle consists of the\ncontinuous adaptation of a hypothesis about a phenomenon that generates data.\nML methods use a hypothesis to compute predictions for future events. We\nbelieve that thinking about ML as combinations of three components given by\ndata, model, and loss helps to navigate the steadily growing offer for\nready-to-use ML methods. Our three-component picture of ML allows a unified\ntreatment of a wide range of concepts and techniques which seem quite unrelated\nat first sight. The regularization effect of early stopping in iterative\nmethods is due to the shrinking of the effective hypothesis space.\nPrivacy-preserving ML is obtained by particular choices for the features of\ndata points. Explainable ML methods are characterized by particular choices for\nthe hypothesis space. To make good use of ML tools it is instrumental to\nunderstand its underlying principles at different levels of detail. On a lower\nlevel, this tutorial helps ML engineers to choose suitable methods for the\napplication at hand. The book also offers a higher-level view on the\nimplementation of ML methods which is typically required to manage a team of ML\nengineers and data scientists.",
          "link": "http://arxiv.org/abs/1805.05052",
          "publishedOn": "2021-08-20T01:53:53.480Z",
          "wordCount": 842,
          "title": "Machine Learning: The Basics. (arXiv:1805.05052v14 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1\">Ozioma Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McSharry_P/0/1/0/all/0/1\">Patrick McSharry</a>",
          "description": "Modelling, simulation, and forecasting offer a means of facilitating better\nplanning and decision-making. These quantitative approaches can add value\nbeyond traditional methods that do not rely on data and are particularly\nrelevant for public transportation. Lagos is experiencing rapid urbanization\nand currently has a population of just under 15 million. Both long waiting\ntimes and uncertain travel times has driven many people to acquire their own\nvehicle or use alternative modes of transport. This has significantly increased\nthe number of vehicles on the roads leading to even more traffic and greater\ntraffic congestion. This paper investigates urban travel demand in Lagos and\nexplores passenger dynamics in time and space. Using individual commuter trip\ndata from tickets purchased from the Lagos State Bus Rapid Transit (BRT), the\ndemand patterns through the hours of the day, days of the week and bus stations\nare analysed. This study aims to quantify demand from actual passenger trips\nand estimate the impact that dynamic scheduling could have on passenger waiting\ntimes. Station segmentation is provided to cluster stations by their demand\ncharacteristics in order to tailor specific bus schedules. Intra-day public\ntransportation demand in Lagos BRT is analysed and predictions are compared.\nSimulations using fixed and dynamic bus scheduling demonstrate that the average\nwaiting time could be reduced by as much as 80%. The load curves, insights and\nthe approach developed will be useful for informing policymaking in Lagos and\nsimilar African cities facing the challenges of rapid urbanization.",
          "link": "http://arxiv.org/abs/2105.11816",
          "publishedOn": "2021-08-20T01:53:53.472Z",
          "wordCount": 697,
          "title": "Public Transportation Demand Analysis: A Case Study of Metropolitan Lagos. (arXiv:2105.11816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15245",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dai_W/0/1/0/all/0/1\">Wenhan Dai</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhi Zeng</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Dou_D/0/1/0/all/0/1\">Daowei Dou</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1\">Jianping Chen</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Junli Li</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a> (1) ((1) Department of Engineering Physics, Tsinghua University, Beijing, China, (2) College of Nuclear Science and Technology, Beijing Normal University, Beijing, China)",
          "description": "The monitoring of Cs-137 in seawater using scintillation detector relies on\nthe spectrum analysis method to extract the Cs-137 concentration. And when in\npoor statistic situation, the calculation result of the traditional net peak\narea (NPA) method has a large uncertainty. We present a machine learning based\nmethod to better analyze the gamma-ray spectrum with low Cs-137 concentration.\nWe apply multilayer perceptron (MLP) to analyze the 662 keV full energy peak of\nCs-137 in the seawater spectrum. And the MLP can be trained with a few measured\nbackground spectrums by combining the simulated Cs-137 signal with measured\nbackground spectrums. Thus, it can save the time of preparing and measuring the\nstandard samples for generating the training dataset. To validate the MLP-based\nmethod, we use Geant4 and background gamma-ray spectrums measured by a seaborne\nmonitoring device to generate an independent test dataset to test the result by\nour method and the traditional NPA method. We find that the MLP-based method\nachieves a root mean squared error of 0.159, 2.3 times lower than that of the\ntraditional net peak area method, indicating the MLP-based method improves the\nprecision of Cs-137 concentration calculation",
          "link": "http://arxiv.org/abs/2010.15245",
          "publishedOn": "2021-08-20T01:53:53.466Z",
          "wordCount": 719,
          "title": "A marine radioisotope gamma-ray spectrum analysis method based on Monte Carlo simulation and MLP neural network. (arXiv:2010.15245v2 [physics.ins-det] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00568",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Clavijo_J/0/1/0/all/0/1\">Jose M. Clavijo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Glaysher_P/0/1/0/all/0/1\">Paul Glaysher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katzy_J/0/1/0/all/0/1\">Judith M. Katzy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>",
          "description": "We apply adversarial domain adaptation in unsupervised setting to reduce\nsample bias in a supervised high energy physics events classifier training. We\nmake use of a neural network containing event and domain classifier with a\ngradient reversal layer to simultaneously enable signal versus background\nevents classification on the one hand, while on the other hand minimising the\ndifference in response of the network to background samples originating from\ndifferent MC models via adversarial domain classification loss. We show the\nsuccessful bias removal on the example of simulated events at the LHC with\n$t\\bar{t}H$ signal versus $t\\bar{t}b\\bar{b}$ background classification and\ndiscuss implications and limitations of the method",
          "link": "http://arxiv.org/abs/2005.00568",
          "publishedOn": "2021-08-20T01:53:53.449Z",
          "wordCount": 600,
          "title": "Adversarial domain adaptation to reduce sample bias of a high energy physics classifier. (arXiv:2005.00568v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-08-20T01:53:53.442Z",
          "wordCount": 594,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1\">David LeBauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnette_M/0/1/0/all/0/1\">Max Burnette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahlgren_N/0/1/0/all/0/1\">Noah Fahlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooper_R/0/1/0/all/0/1\">Rob Kooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1\">Kenton McHenry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the evaluation of sensing technologies to study plants\nunder field conditions. The TERRA-REF program deployed a suite of\nhigh-resolution, cutting edge technology sensors on a gantry system with the\naim of scanning 1 hectare (10$^4$) at around 1 mm$^2$ spatial resolution\nmultiple times per week. The system contains co-located sensors including a\nstereo-pair RGB camera, a thermal imager, a laser scanner to capture 3D\nstructure, and two hyperspectral cameras covering wavelengths of 300-2500nm.\nThis sensor data is provided alongside over sixty types of traditional plant\nphenotype measurements that can be used to train new machine learning models.\nAssociated weather and environmental measurements, information about agronomic\nmanagement and experimental design, and the genomic sequences of hundreds of\nplant varieties have been collected and are available alongside the sensor and\nplant phenotype data.\n\nOver the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n\nThe focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.",
          "link": "http://arxiv.org/abs/2107.14072",
          "publishedOn": "2021-08-20T01:53:52.906Z",
          "wordCount": 737,
          "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public Domain Data Offer the Computer Vision Community?. (arXiv:2107.14072v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>",
          "description": "Actor-critic methods are widely used in offline reinforcement learning\npractice, but are not so well-understood theoretically. We propose a new\noffline actor-critic algorithm that naturally incorporates the pessimism\nprinciple, leading to several key advantages compared to the state of the art.\nThe algorithm can operate when the Bellman evaluation operator is closed with\nrespect to the action value function of the actor's policies; this is a more\ngeneral setting than the low-rank MDP model. Despite the added generality, the\nprocedure is computationally tractable as it involves the solution of a\nsequence of second-order programs. We prove an upper bound on the suboptimality\ngap of the policy returned by the procedure that depends on the data coverage\nof any arbitrary, possibly data dependent comparator policy. The achievable\nguarantee is complemented with a minimax lower bound that is matching up to\nlogarithmic factors.",
          "link": "http://arxiv.org/abs/2108.08812",
          "publishedOn": "2021-08-20T01:53:52.892Z",
          "wordCount": 588,
          "title": "Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning. (arXiv:2108.08812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "Adversarial attack algorithms are dominated by penalty methods, which are\nslow in practice, or more efficient distance-customized methods, which are\nheavily tailored to the properties of the distance considered. We propose a\nwhite-box attack algorithm to generate minimally perturbed adversarial examples\nbased on Augmented Lagrangian principles. We bring several algorithmic\nmodifications, which have a crucial effect on performance. Our attack enjoys\nthe generality of penalty methods and the computational efficiency of\ndistance-customized algorithms, and can be readily used for a wide set of\ndistances. We compare our attack to state-of-the-art methods on three datasets\nand several models, and consistently obtain competitive performances with\nsimilar or lower computational complexity.",
          "link": "http://arxiv.org/abs/2011.11857",
          "publishedOn": "2021-08-20T01:53:52.886Z",
          "wordCount": 578,
          "title": "Augmented Lagrangian Adversarial Attacks. (arXiv:2011.11857v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02283",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Sun_N/0/1/0/all/0/1\">Ningning Sun</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wells_M/0/1/0/all/0/1\">Martin T. Wells</a>",
          "description": "This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting the stock returns. In a 10-second time\nfrequency, we study the clustering structure of different measures to find out\nthe best ones for predicting. In this way, we can predict more accurately with\na limited number of predictors, which removes the noise and makes the model\nmore interpretable.",
          "link": "http://arxiv.org/abs/2107.02283",
          "publishedOn": "2021-08-20T01:53:52.880Z",
          "wordCount": 514,
          "title": "Clustering Structure of Microstructure Measures. (arXiv:2107.02283v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>",
          "description": "Deep networks often make confident, yet incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models are a candidate metric for\noutlier detection with unlabeled data. Yet, previous studies have shown that\nsuch likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest class of deep generative\nmodels. First, we show that a theoretically-grounded correction readily\nameliorates a key bias with VAE likelihood estimates. The bias correction is\nmodel-free, sample-specific, and accurately computed with the Bernoulli and\ncontinuous Bernoulli visible distributions. Second, we show that a well-known\npreprocessing technique, contrast normalization, extends the effectiveness of\nbias correction to natural image datasets. Third, we show that the variance of\nthe likelihoods computed over an ensemble of VAEs also enables robust outlier\ndetection. We perform a comprehensive evaluation of our remedies with nine\n(grayscale and natural) image datasets, and demonstrate significant advantages,\nin terms of both speed and accuracy, over four other state-of-the-art methods.\nOur lightweight remedies are biologically inspired and may serve to achieve\nefficient outlier detection with many types of deep generative models.",
          "link": "http://arxiv.org/abs/2108.08760",
          "publishedOn": "2021-08-20T01:53:52.869Z",
          "wordCount": 645,
          "title": "Efficient remedies for outlier detection with variational autoencoders. (arXiv:2108.08760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perumal_R/0/1/0/all/0/1\">Rylan Perumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L van Zyl</a>",
          "description": "Parameter calibration is a significant challenge in agent-based modelling and\nsimulation (ABMS). An agent-based model's (ABM) complexity grows as the number\nof parameters required to be calibrated increases. This parameter expansion\nleads to the ABMS equivalent of the \\say{curse of dimensionality}. In\nparticular, infeasible computational requirements searching an infinite\nparameter space. We propose a more comprehensive and adaptive ABMS Framework\nthat can effectively swap out parameterisation strategies and surrogate models\nto parameterise an infectious disease ABM. This framework allows us to evaluate\ndifferent strategy-surrogate combinations' performance in accuracy and\nefficiency (speedup). We show that we achieve better than parity in accuracy\nacross the surrogate assisted sampling strategies and the baselines. Also, we\nidentify that the Metric Stochastic Response Surface strategy combined with the\nSupport Vector Machine surrogate is the best overall in getting closest to the\ntrue synthetic parameters. Also, we show that DYnamic COOrdindate Search Using\nResponse Surface Models with XGBoost as a surrogate attains in combination the\nhighest probability of approximating a cumulative synthetic daily infection\ndata distribution and achieves the most significant speedup with regards to our\nanalysis. Lastly, we show in a real-world setting that DYCORS XGBoost and MSRS\nSVM can approximate the real world cumulative daily infection distribution with\n$97.12$\\% and $96.75$\\% similarity respectively.",
          "link": "http://arxiv.org/abs/2108.08809",
          "publishedOn": "2021-08-20T01:53:52.863Z",
          "wordCount": 651,
          "title": "Surrogate Assisted Strategies (The Parameterisation of an Infectious Disease Agent-Based Model). (arXiv:2108.08809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1\">Konstantin Klemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neill_D/0/1/0/all/0/1\">Daniel B. Neill</a>",
          "description": "Machine learning is gaining popularity in a broad range of areas working with\ngeographic data, such as ecology or atmospheric sciences. Here, data often\nexhibit spatial effects, which can be difficult to learn for neural networks.\nIn this study, we propose SXL, a method for embedding information on the\nautoregressive nature of spatial data directly into the learning process using\nauxiliary tasks. We utilize the local Moran's I, a popular measure of local\nspatial autocorrelation, to \"nudge\" the model to learn the direction and\nmagnitude of local spatial effects, complementing the learning of the primary\ntask. We further introduce a novel expansion of Moran's I to multiple\nresolutions, thus capturing spatial interactions over longer and shorter\ndistances simultaneously. The novel multi-resolution Moran's I can be\nconstructed easily and as a multi-dimensional tensor offers seamless\nintegration into existing machine learning frameworks. Throughout a range of\nexperiments using real-world data, we highlight how our method consistently\nimproves the training of neural networks in unsupervised and supervised\nlearning tasks. In generative spatial modeling experiments, we propose a novel\nloss for auxiliary task GANs utilizing task uncertainty weights. Our proposed\nmethod outperforms domain-specific spatial interpolation benchmarks,\nhighlighting its potential for downstream applications. This study bridges\nexpertise from geographic information science and machine learning, showing how\nthis integration of disciplines can help to address domain-specific challenges.\nThe code for our experiments is available on Github:\nhttps://github.com/konstantinklemmer/sxl.",
          "link": "http://arxiv.org/abs/2006.10461",
          "publishedOn": "2021-08-20T01:53:52.856Z",
          "wordCount": 712,
          "title": "Auxiliary-task learning for geographic data with autoregressive embeddings. (arXiv:2006.10461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL",
          "link": "http://arxiv.org/abs/2108.08728",
          "publishedOn": "2021-08-20T01:53:52.849Z",
          "wordCount": 601,
          "title": "Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification. (arXiv:2108.08728v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xinjian Luo</a>",
          "description": "As a decentralized model training method, federated learning is designed to\nintegrate the isolated data islands and protect data privacy. Recent studies,\nhowever, have demonstrated that the Generative Adversarial Network (GAN) based\nattacks can be used in federated learning to learn the distribution of the\nvictim's private dataset and accordingly reconstruct human-distinguishable\nimages. In this paper, we exploit defenses against GAN-based attacks in\nfederated learning, and propose a framework, Anti-GAN, to prevent attackers\nfrom learning the real distribution of the victim's data. The core idea of\nAnti-GAN is to corrupt the visual features of the victim's private training\nimages, such that the images restored by the attacker are indistinguishable to\nhuman eyes. Specifically, in Anti-GAN, the victim first projects the personal\ndataset onto a GAN's generator, then mixes the fake images generated by the\ngenerator with the real images to obtain the training dataset, which will be\nfed into the federated model for training. We redesign the structure of the\nvictim's GAN to encourage it to learn the classification features (instead of\nthe visual features) of the real images. We further introduce an unsupervised\ntask to the GAN model for obfuscating the visual features of the generated\nimages. The experiments demonstrate that Anti-GAN can effectively prevent the\nattacker from learning the distribution of the private images, meanwhile\ncausing little harm to the accuracy of the federated model.",
          "link": "http://arxiv.org/abs/2004.12571",
          "publishedOn": "2021-08-20T01:53:52.842Z",
          "wordCount": 687,
          "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning. (arXiv:2004.12571v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey Savchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korovko_A/0/1/0/all/0/1\">Arseny Korovko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherstyuk_L/0/1/0/all/0/1\">Leonid Sherstyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severin_N/0/1/0/all/0/1\">Nikita Severin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikheev_A/0/1/0/all/0/1\">Aleksandr Mikheev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1\">Dmitrii Babaev</a>",
          "description": "Many tasks in graph machine learning, such as link prediction and node\nclassification, are typically solved by using representation learning, in which\neach node or edge in the network is encoded via an embedding. Though there\nexists a lot of network embeddings for static graphs, the task becomes much\nmore complicated when the dynamic (i.e. temporal) network is analyzed. In this\npaper, we propose a novel approach for dynamic network representation learning\nbased on Temporal Graph Network by using a highly custom message generating\nfunction by extracting Causal Anonymous Walks. For evaluation, we provide a\nbenchmark pipeline for the evaluation of temporal network embeddings. This work\nprovides the first comprehensive comparison framework for temporal network\nrepresentation learning in every available setting for graph machine learning\nproblems involving node classification and link prediction. The proposed model\noutperforms state-of-the-art baseline models. The work also justifies the\ndifference between them based on evaluation in various transductive/inductive\nedge/node classification tasks. In addition, we show the applicability and\nsuperior performance of our model in the real-world downstream graph machine\nlearning task provided by one of the top European banks, involving credit\nscoring based on transaction data.",
          "link": "http://arxiv.org/abs/2108.08754",
          "publishedOn": "2021-08-20T01:53:52.797Z",
          "wordCount": 634,
          "title": "Temporal Graph Network Embedding with Causal Anonymous Walks Representations. (arXiv:2108.08754v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "In this work, we aim to calibrate the score outputs of an estimator for the\nbinary classification problem by finding an 'optimal' mapping to class\nprobabilities, where the 'optimal' mapping is in the sense that minimizes the\nclassification error (or equivalently, maximizes the accuracy). We show that\nfor the given target variables and the score outputs of an estimator, an\n'optimal' soft mapping, which monotonically maps the score values to\nprobabilities, is a hard mapping that maps the score values to $0$ and $1$. We\nshow that for class weighted (where the accuracy for one class is more\nimportant) and sample weighted (where the samples' accurate classifications are\nnot equally important) errors, or even general linear losses; this hard mapping\ncharacteristic is preserved. We propose a sequential recursive merger approach,\nwhich produces an 'optimal' hard mapping (for the observed samples so far)\nsequentially with each incoming new sample. Our approach has a logarithmic in\nsample size time complexity, which is optimally efficient.",
          "link": "http://arxiv.org/abs/2108.08780",
          "publishedOn": "2021-08-20T01:53:52.787Z",
          "wordCount": 599,
          "title": "Optimally Efficient Sequential Calibration of Binary Classifiers to Minimize Classification Error. (arXiv:2108.08780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Craig_K/0/1/0/all/0/1\">Katy Craig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trillos_N/0/1/0/all/0/1\">Nicol&#xe1;s Garc&#xed;a Trillos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Slepcev_D/0/1/0/all/0/1\">Dejan Slep&#x10d;ev</a>",
          "description": "In this work we build a unifying framework to interpolate between\ndensity-driven and geometry-based algorithms for data clustering, and\nspecifically, to connect the mean shift algorithm with spectral clustering at\ndiscrete and continuum levels. We seek this connection through the introduction\nof Fokker-Planck equations on data graphs. Besides introducing new forms of\nmean shift algorithms on graphs, we provide new theoretical insights on the\nbehavior of the family of diffusion maps in the large sample limit as well as\nprovide new connections between diffusion maps and mean shift dynamics on a\nfixed graph. Several numerical examples illustrate our theoretical findings and\nhighlight the benefits of interpolating density-driven and geometry-based\nclustering algorithms.",
          "link": "http://arxiv.org/abs/2108.08687",
          "publishedOn": "2021-08-20T01:53:52.779Z",
          "wordCount": 571,
          "title": "Clustering dynamics on graphs: from spectral clustering to mean shift through Fokker-Planck interpolation. (arXiv:2108.08687v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuyue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>",
          "description": "In generative adversarial imitation learning (GAIL), the agent aims to learn\na policy from an expert demonstration so that its performance cannot be\ndiscriminated from the expert policy on a certain predefined reward set. In\nthis paper, we study GAIL in both online and offline settings with linear\nfunction approximation, where both the transition and reward function are\nlinear in the feature maps. Besides the expert demonstration, in the online\nsetting the agent can interact with the environment, while in the offline\nsetting the agent only accesses an additional dataset collected by a prior. For\nonline GAIL, we propose an optimistic generative adversarial policy\noptimization algorithm (OGAP) and prove that OGAP achieves\n$\\widetilde{\\mathcal{O}}(H^2 d^{3/2}K^{1/2}+KH^{3/2}dN_1^{-1/2})$ regret. Here\n$N_1$ represents the number of trajectories of the expert demonstration, $d$ is\nthe feature dimension, and $K$ is the number of episodes.\n\nFor offline GAIL, we propose a pessimistic generative adversarial policy\noptimization algorithm (PGAP). For an arbitrary additional dataset, we obtain\nthe optimality gap of PGAP, achieving the minimax lower bound in the\nutilization of the additional dataset. Assuming sufficient coverage on the\nadditional dataset, we show that PGAP achieves\n$\\widetilde{\\mathcal{O}}(H^{2}dK^{-1/2}\n+H^2d^{3/2}N_2^{-1/2}+H^{3/2}dN_1^{-1/2} \\ )$ optimality gap. Here $N_2$\nrepresents the number of trajectories of the additional dataset with sufficient\ncoverage.",
          "link": "http://arxiv.org/abs/2108.08765",
          "publishedOn": "2021-08-20T01:53:52.766Z",
          "wordCount": 674,
          "title": "Provably Efficient Generative Adversarial Imitation Learning for Online and Offline Setting with Linear Function Approximation. (arXiv:2108.08765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08818",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "We introduce the notion of Point in Time Economic Scenario Generation (PiT\nESG) with a clear mathematical problem formulation to unify and compare\neconomic scenario generation approaches conditional on forward looking market\ndata. Such PiT ESGs should provide quicker and more flexible reactions to\nsudden economic changes than traditional ESGs calibrated solely to long periods\nof historical data. We specifically take as economic variable the S&P500 Index\nwith the VIX Index as forward looking market data to compare the nonparametric\nfiltered historical simulation, GARCH model with joint likelihood estimation\n(parametric), Restricted Boltzmann Machine and the conditional Variational\nAutoencoder (Generative Networks) for their suitability as PiT ESG. Our\nevaluation consists of statistical tests for model fit and benchmarking the out\nof sample forecasting quality with a strategy backtest using model output as\nstop loss criterion. We find that both Generative Networks outperform the\nnonparametric and classic parametric model in our tests, but that the CVAE\nseems to be particularly well suited for our purposes: yielding more robust\nperformance and being computationally lighter.",
          "link": "http://arxiv.org/abs/2108.08818",
          "publishedOn": "2021-08-20T01:53:52.748Z",
          "wordCount": 617,
          "title": "Discriminating modelling approaches for Point in Time Economic Scenario Generation. (arXiv:2108.08818v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>",
          "description": "Graph neural networks (GNNs), has been widely used for supervised learning\ntasks in graphs reaching state-of-the-art results. However, little work was\ndedicated to creating unbiased GNNs, i.e., where the classification is\nuncorrelated with sensitive attributes, such as race or gender. Some ignore the\nsensitive attributes or optimize for the criteria of statistical parity for\nfairness. However, it has been shown that neither approaches ensure fairness,\nbut rather cripple the utility of the prediction task. In this work, we present\na GNN framework that allows optimizing representations for the notion of\nEqualized Odds fairness criteria. The architecture is composed of three\ncomponents: (1) a GNN classifier predicting the utility class, (2) a sampler\nlearning the distribution of the sensitive attributes of the nodes given their\nlabels. It generates samples fed into a (3) discriminator that discriminates\nbetween true and sampled sensitive attributes using a novel \"permutation loss\"\nfunction. Using these components, we train a model to neglect information\nregarding the sensitive attribute only with respect to its label. To the best\nof our knowledge, we are the first to optimize GNNs for the equalized odds\ncriteria. We evaluate our classifier over several graph datasets and sensitive\nattributes and show our algorithm reaches state-of-the-art results.",
          "link": "http://arxiv.org/abs/2108.08800",
          "publishedOn": "2021-08-20T01:53:52.734Z",
          "wordCount": 637,
          "title": "EqGNN: Equalized Node Opportunity in Graphs. (arXiv:2108.08800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1\">Tobias Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangelova_S/0/1/0/all/0/1\">Stanislava Rangelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flutura_S/0/1/0/all/0/1\">Simon Flutura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>",
          "description": "Virtual Reality (VR) games that feature physical activities have been shown\nto increase players' motivation to do physical exercise. However, for such\nexercises to have a positive healthcare effect, they have to be repeated\nseveral times a week. To maintain player motivation over longer periods of\ntime, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the\ngame's challenge according to the player's capabilities. For exercise games,\nthis is mostly done by tuning specific in-game parameters like the speed of\nobjects. In this work, we propose to use experience-driven Procedural Content\nGeneration for DDA in VR exercise games by procedurally generating levels that\nmatch the player's current capabilities. Not only finetuning specific\nparameters but creating completely new levels has the potential to decrease\nrepetition over longer time periods and allows for the simultaneous adaptation\nof the cognitive and physical challenge of the exergame. As a proof-of-concept,\nwe implement an initial prototype in which the player must traverse a maze that\nincludes several exercise rooms, whereby the generation of the maze is realized\nby a neural network. Passing those exercise rooms requires the player to\nperform physical activities. To match the player's capabilities, we use Deep\nReinforcement Learning to adjust the structure of the maze and to decide which\nexercise rooms to include in the maze. We evaluate our prototype in an\nexploratory user study utilizing both biodata and subjective questionnaires.",
          "link": "http://arxiv.org/abs/2108.08762",
          "publishedOn": "2021-08-20T01:53:52.715Z",
          "wordCount": 681,
          "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation. (arXiv:2108.08762v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>",
          "description": "In this paper, we study the problem of Novel Class Discovery (NCD). NCD aims\nat inferring novel object categories in an unlabeled set by leveraging from\nprior knowledge of a labeled set containing different, but related classes.\nExisting approaches tackle this problem by considering multiple objective\nfunctions, usually involving specialized loss terms for the labeled and the\nunlabeled samples respectively, and often requiring auxiliary regularization\nterms. In this paper, we depart from this traditional scheme and introduce a\nUNified Objective function (UNO) for discovering novel classes, with the\nexplicit purpose of favoring synergy between supervised and unsupervised\nlearning. Using a multi-view self-labeling strategy, we generate pseudo-labels\nthat can be treated homogeneously with ground truth labels. This leads to a\nsingle classification objective operating on both known and unknown classes.\nDespite its simplicity, UNO outperforms the state of the art by a significant\nmargin on several benchmarks (~+10% on CIFAR-100 and +8% on ImageNet). The\nproject page is available at: \\url{https://ncd-uno.github.io}.",
          "link": "http://arxiv.org/abs/2108.08536",
          "publishedOn": "2021-08-20T01:53:52.709Z",
          "wordCount": 608,
          "title": "A Unified Objective for Novel Class Discovery. (arXiv:2108.08536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vignesh Nanda Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edakunni_N/0/1/0/all/0/1\">Narayanan U Edakunni</a>",
          "description": "In recent years, gradient boosted decision trees have become popular in\nbuilding robust machine learning models on big data. The primary technique that\nhas enabled these algorithms success has been distributing the computation\nwhile building the decision trees. A distributed decision tree building, in\nturn, has been enabled by building quantiles of the big datasets and choosing\nthe candidate split points from these quantile sets. In XGBoost, for instance,\na sophisticated quantile building algorithm is employed to identify the\ncandidate split points for the decision trees. This method is often projected\nto yield better results when the computation is distributed. In this paper, we\ndispel the notion that these methods provide more accurate and scalable methods\nfor building decision trees in a distributed manner. In a significant\ncontribution, we show theoretically and empirically that choosing the split\npoints uniformly at random provides the same or even better performance in\nterms of accuracy and computational efficiency. Hence, a simple random\nselection of points suffices for decision tree building compared to more\nsophisticated methods.",
          "link": "http://arxiv.org/abs/2108.08790",
          "publishedOn": "2021-08-20T01:53:52.702Z",
          "wordCount": 605,
          "title": "Simple is better: Making Decision Trees faster using random sampling. (arXiv:2108.08790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Changwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_K/0/1/0/all/0/1\">Kyeong-Joong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Won-Yong Shin</a>",
          "description": "In recent years, many recommender systems using network embedding (NE) such\nas graph neural networks (GNNs) have been extensively studied in the sense of\nimproving recommendation accuracy. However, such attempts have focused mostly\non utilizing only the information of positive user-item interactions with high\nratings. Thus, there is a challenge on how to make use of low rating scores for\nrepresenting users' preferences since low ratings can be still informative in\ndesigning NE-based recommender systems. In this study, we present SiReN, a new\nsign-aware recommender system based on GNN models. Specifically, SiReN has\nthree key components: 1) constructing a signed bipartite graph for more\nprecisely representing users' preferences, which is split into two\nedge-disjoint graphs with positive and negative edges each, 2) generating two\nembeddings for the partitioned graphs with positive and negative edges via a\nGNN model and a multi-layer perceptron (MLP), respectively, and then using an\nattention model to obtain the final embeddings, and 3) establishing a\nsign-aware Bayesian personalized ranking (BPR) loss function in the process of\noptimization. Through comprehensive experiments, we empirically demonstrate\nthat SiReN consistently outperforms state-of-the-art NE-aided recommendation\nmethods.",
          "link": "http://arxiv.org/abs/2108.08735",
          "publishedOn": "2021-08-20T01:53:52.696Z",
          "wordCount": 640,
          "title": "SiReN: Sign-Aware Recommendation Using Graph Neural Networks. (arXiv:2108.08735v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1\">Armin Salimi-Badr</a>",
          "description": "In this paper, a new interval type-2 fuzzy neural network able to construct\nnon-separable fuzzy rules with adaptive shapes is introduced. To reflect the\nuncertainty, the shape of fuzzy sets considered to be uncertain. Therefore, a\nnew form of interval type-2 fuzzy sets based on a general Gaussian model able\nto construct different shapes (including triangular, bell-shaped, trapezoidal)\nis proposed. To consider the interactions among input variables, input vectors\nare transformed to new feature spaces with uncorrelated variables proper for\ndefining each fuzzy rule. Next, the new features are fed to a fuzzification\nlayer using proposed interval type-2 fuzzy sets with adaptive shape.\nConsequently, interval type-2 non-separable fuzzy rules with proper shapes,\nconsidering the local interactions of variables and the uncertainty are formed.\nFor type reduction the contribution of the upper and lower firing strengths of\neach fuzzy rule are adaptively selected separately. To train different\nparameters of the network, the Levenberg-Marquadt optimization method is\nutilized. The performance of the proposed method is investigated on clean and\nnoisy datasets to show the ability to consider the uncertainty. Moreover, the\nproposed paradigm, is successfully applied to real-world time-series\npredictions, regression problems, and nonlinear system identification.\nAccording to the experimental results, the performance of our proposed model\noutperforms other methods with a more parsimonious structure.",
          "link": "http://arxiv.org/abs/2108.08704",
          "publishedOn": "2021-08-20T01:53:52.689Z",
          "wordCount": 668,
          "title": "IT2CFNN: An Interval Type-2 Correlation-Aware Fuzzy Neural Network to Construct Non-Separable Fuzzy Rules with Uncertain and Adaptive Shapes for Nonlinear Function Approximation. (arXiv:2108.08704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Mhafuzul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Mashrur Chowdhury</a>",
          "description": "This paper presents a sensor fusion based Global Navigation Satellite System\n(GNSS) spoofing attack detection framework for autonomous vehicles (AV) that\nconsists of two concurrent strategies: (i) detection of vehicle state using\npredicted location shift -- i.e., distance traveled between two consecutive\ntimestamps -- and monitoring of vehicle motion state -- i.e., standstill/ in\nmotion; and (ii) detection and classification of turns (i.e., left or right).\nData from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering\nangle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural\nnetwork model, which is a long short-term memory (LSTM) network for predicting\nthe location shift, i.e., the distance that an AV travels between two\nconsecutive timestamps. This location shift is then compared with the\nGNSS-based location shift to detect an attack. We have then combined k-Nearest\nNeighbors (k-NN) and Dynamic Time Warping (DTW) algorithms to detect and\nclassify left and right turns using data from the steering angle sensor. To\nprove the efficacy of the sensor fusion-based attack detection framework,\nattack datasets are created for four unique and sophisticated spoofing\nattacks-turn-by-turn, overshoot, wrong turn, and stop, using the publicly\navailable real-world Honda Research Institute Driving Dataset (HDD). Our\nanalysis reveals that the sensor fusion-based detection framework successfully\ndetects all four types of spoofing attacks within the required computational\nlatency threshold.",
          "link": "http://arxiv.org/abs/2108.08635",
          "publishedOn": "2021-08-20T01:53:52.671Z",
          "wordCount": 675,
          "title": "A Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2108.08635v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>",
          "description": "Clustered Federated Multitask Learning (CFL) was introduced as an efficient\nscheme to obtain reliable specialized models when data is imbalanced and\ndistributed in a non-i.i.d. (non-independent and identically distributed)\nfashion amongst clients. While a similarity measure metric, like the cosine\nsimilarity, can be used to endow groups of the client with a specialized model,\nthis process can be arduous as the server should involve all clients in each of\nthe federated learning rounds. Therefore, it is imperative that a subset of\nclients is selected periodically due to the limited bandwidth and latency\nconstraints at the network edge. To this end, this paper proposes a new client\nselection algorithm that aims to accelerate the convergence rate for obtaining\nspecialized machine learning models that achieve high test accuracies for all\nclient groups. Specifically, we introduce a client selection approach that\nleverages the devices' heterogeneity to schedule the clients based on their\nround latency and exploits the bandwidth reuse for clients that consume more\ntime to update the model. Then, the server performs model averaging and\nclusters the clients based on predefined thresholds. When a specific cluster\nreaches a stationary point, the proposed algorithm uses a greedy scheduling\nalgorithm for that group by selecting the clients with less latency to update\nthe model. Extensive experiments show that the proposed approach lowers the\ntraining time and accelerates the convergence rate by up to 50% while imbuing\neach client with a specialized model that is fit for its local data\ndistribution.",
          "link": "http://arxiv.org/abs/2108.08768",
          "publishedOn": "2021-08-20T01:53:52.665Z",
          "wordCount": 701,
          "title": "Client Selection Approach in Support of Clustered Federated Learning over Wireless Edge Networks. (arXiv:2108.08768v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1\">Maria-Florina Balcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dravyansh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "We analyze the meta-learning of the initialization and step-size of learning\nalgorithms for piecewise-Lipschitz functions, a non-convex setting with\napplications to both machine learning and algorithms. Starting from recent\nregret bounds for the exponential forecaster on losses with dispersed\ndiscontinuities, we generalize them to be initialization-dependent and then use\nthis result to propose a practical meta-learning procedure that learns both the\ninitialization and the step-size of the algorithm from multiple online learning\ntasks. Asymptotically, we guarantee that the average regret across tasks scales\nwith a natural notion of task-similarity that measures the amount of overlap\nbetween near-optimal regions of different tasks. Finally, we instantiate the\nmethod and its guarantee in two important settings: robust meta-learning and\nmulti-task data-driven algorithm design.",
          "link": "http://arxiv.org/abs/2108.08770",
          "publishedOn": "2021-08-20T01:53:52.658Z",
          "wordCount": 541,
          "title": "Learning-to-learn non-convex piecewise-Lipschitz functions. (arXiv:2108.08770v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daoyi Dong</a>",
          "description": "Tensor Train (TT) approach has been successfully applied in the modelling of\nthe multilinear interaction of features. Nevertheless, the existing models lack\nflexibility and generalizability, as they only model a single type of\nhigh-order correlation. In practice, multiple multilinear correlations may\nexist within the features. In this paper, we present a novel Residual Tensor\nTrain (ResTT) which integrates the merits of TT and residual structure to\ncapture the multilinear feature correlations, from low to higher orders, within\nthe same model. In particular, we prove that the fully-connected layer in\nneural networks and the Volterra series can be taken as special cases of ResTT.\nFurthermore, we derive the rule for weight initialization that stabilizes the\ntraining of ResTT based on a mean-field analysis. We prove that such a rule is\nmuch more relaxed than that of TT, which means ResTT can easily address the\nvanishing and exploding gradient problem that exists in the current TT models.\nNumerical experiments demonstrate that ResTT outperforms the state-of-the-art\ntensor network approaches, and is competitive with the benchmark deep learning\nmodels on MNIST and Fashion-MNIST datasets.",
          "link": "http://arxiv.org/abs/2108.08659",
          "publishedOn": "2021-08-20T01:53:52.651Z",
          "wordCount": 622,
          "title": "Residual Tensor Train: a Flexible and Efficient Approach for Learning Multiple Multilinear Correlations. (arXiv:2108.08659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr",
          "link": "http://arxiv.org/abs/2108.08839",
          "publishedOn": "2021-08-20T01:53:52.644Z",
          "wordCount": 663,
          "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers. (arXiv:2108.08839v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lennon_K/0/1/0/all/0/1\">Kyle Lennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_K/0/1/0/all/0/1\">Katharina Fransen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1\">Alexander O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yumeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1\">Matthew Beveridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Although LEGO sets have entertained generations of children and adults, the\nchallenge of designing customized builds matching the complexity of real-world\nor imagined scenes remains too great for the average enthusiast. In order to\nmake this feat possible, we implement a system that generates a LEGO brick\nmodel from 2D images. We design a novel solution to this problem that uses an\noctree-structured autoencoder trained on 3D voxelized models to obtain a\nfeasible latent representation for model reconstruction, and a separate network\ntrained to predict this latent representation from 2D images. LEGO models are\nobtained by algorithmic conversion of the 3D voxelized model to bricks. We\ndemonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An\noctree architecture enables the flexibility to produce multiple resolutions to\nbest fit a user's creative vision or design needs. In order to demonstrate the\nbroad applicability of our system, we generate step-by-step building\ninstructions and animations for LEGO models of objects and human faces.\nFinally, we test these automatically generated LEGO sets by constructing\nphysical builds using real LEGO bricks.",
          "link": "http://arxiv.org/abs/2108.08477",
          "publishedOn": "2021-08-20T01:53:52.636Z",
          "wordCount": 627,
          "title": "Image2Lego: Customized LEGO Set Generation from Images. (arXiv:2108.08477v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Welle_M/0/1/0/all/0/1\">Michael C. Welle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poklukar_P/0/1/0/all/0/1\">Petra Poklukar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1\">Danica Kragic</a>",
          "description": "The state-of-the-art unsupervised contrastive visual representation learning\nmethods that have emerged recently (SimCLR, MoCo, SwAV) all make use of data\naugmentations in order to construct a pretext task of instant discrimination\nconsisting of similar and dissimilar pairs of images. Similar pairs are\nconstructed by randomly extracting patches from the same image and applying\nseveral other transformations such as color jittering or blurring, while\ntransformed patches from different image instances in a given batch are\nregarded as dissimilar pairs. We argue that this approach can result similar\npairs that are \\textit{semantically} dissimilar. In this work, we address this\nproblem by introducing a \\textit{batch curation} scheme that selects batches\nduring the training process that are more inline with the underlying\ncontrastive objective. We provide insights into what constitutes beneficial\nsimilar and dissimilar pairs as well as validate \\textit{batch curation} on\nCIFAR10 by integrating it in the SimCLR model.",
          "link": "http://arxiv.org/abs/2108.08643",
          "publishedOn": "2021-08-20T01:53:52.618Z",
          "wordCount": 573,
          "title": "Batch Curation for Unsupervised Contrastive Representation Learning. (arXiv:2108.08643v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "Meta-learning has been the most common framework for few-shot learning in\nrecent years. It learns the model from collections of few-shot classification\ntasks, which is believed to have a key advantage of making the training\nobjective consistent with the testing objective. However, some recent works\nreport that by training for whole-classification, i.e. classification on the\nwhole label-set, it can get comparable or even better embedding than many\nmeta-learning algorithms. The edge between these two lines of works has yet\nbeen underexplored, and the effectiveness of meta-learning in few-shot learning\nremains unclear. In this paper, we explore a simple process: meta-learning over\na whole-classification pre-trained model on its evaluation metric. We observe\nthis simple method achieves competitive performance to state-of-the-art methods\non standard benchmarks. Our further analysis shed some light on understanding\nthe trade-offs between the meta-learning objective and the whole-classification\nobjective in few-shot learning.",
          "link": "http://arxiv.org/abs/2003.04390",
          "publishedOn": "2021-08-20T01:53:52.611Z",
          "wordCount": 639,
          "title": "Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning. (arXiv:2003.04390v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kontolati_K/0/1/0/all/0/1\">Katiana Kontolati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_N/0/1/0/all/0/1\">Natalie Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_N/0/1/0/all/0/1\">Nishant Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>",
          "description": "Constructing probability densities for inference in high-dimensional spectral\ndata is often intractable. In this work, we use normalizing flows on structured\nspectral latent spaces to estimate such densities, enabling downstream\ninference tasks. In addition, we evaluate a method for uncertainty\nquantification when predicting unobserved state vectors associated with each\nspectrum. We demonstrate the capability of this approach on laser-induced\nbreakdown spectroscopy data collected by the ChemCam instrument on the Mars\nrover Curiosity. Using our approach, we are able to generate realistic spectral\nsamples and to accurately predict state vectors with associated well-calibrated\nuncertainties. We anticipate that this methodology will enable efficient\nprobabilistic modeling of spectral data, leading to potential advances in\nseveral areas, including out-of-distribution detection and sensitivity\nanalysis.",
          "link": "http://arxiv.org/abs/2108.08709",
          "publishedOn": "2021-08-20T01:53:52.604Z",
          "wordCount": 572,
          "title": "Neural density estimation and uncertainty quantification for laser induced breakdown spectroscopy spectra. (arXiv:2108.08709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08631",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Inui_K/0/1/0/all/0/1\">Koji Inui</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kato_Y/0/1/0/all/0/1\">Yasuyuki Kato</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Motome_Y/0/1/0/all/0/1\">Yukitoshi Motome</a>",
          "description": "We propose a general framework for finding the ground state of many-body\nfermionic systems by using feed-forward neural networks. The anticommutation\nrelation for fermions is usually implemented to a variational wave function by\nthe Slater determinant (or Pfaffian), which is a computational bottleneck\nbecause of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this\nbottleneck by explicitly calculating the sign changes associated with particle\nexchanges in real space and using fully connected neural networks for\noptimizing the rest parts of the wave function. This reduces the computational\ncost to $O(N^2)$ or less. We show that the accuracy of the approximation can be\nimproved by optimizing the \"variance\" of the energy simultaneously with the\nenergy itself. We also find that a reweighting method in Monte Carlo sampling\ncan stabilize the calculation. These improvements can be applied to other\napproaches based on variational Monte Carlo methods. Moreover, we show that the\naccuracy can be further improved by using the symmetry of the system, the\nrepresentative states, and an additional neural network implementing a\ngeneralized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the\nmethod by applying it to a two-dimensional Hubbard model.",
          "link": "http://arxiv.org/abs/2108.08631",
          "publishedOn": "2021-08-20T01:53:52.597Z",
          "wordCount": 643,
          "title": "Determinant-free fermionic wave function using feed-forward neural network. (arXiv:2108.08631v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08473",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jonathan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1\">Bowen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_R/0/1/0/all/0/1\">Rahul Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David Liang</a>",
          "description": "In this work, deep learning algorithms are used to classify fundus images in\nterms of diabetic retinopathy severity. Six different combinations of two model\narchitectures, the Dense Convolutional Network-121 and the Residual Neural\nNetwork-50 and three image types, RGB, Green, and High Contrast, were tested to\nfind the highest performing combination. We achieved an average validation loss\nof 0.17 and a max validation accuracy of 85 percent. By testing out multiple\ncombinations, certain combinations of parameters performed better than others,\nthough minimal variance was found overall. Green filtration was shown to\nperform the poorest, while amplified contrast appeared to have a negligible\neffect in comparison to RGB analysis. ResNet50 proved to be less of a robust\nmodel as opposed to DenseNet121.",
          "link": "http://arxiv.org/abs/2108.08473",
          "publishedOn": "2021-08-20T01:53:52.591Z",
          "wordCount": 603,
          "title": "Classification of Diabetic Retinopathy Severity in Fundus Images with DenseNet121 and ResNet50. (arXiv:2108.08473v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">H. Eric Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Baljeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filev_D/0/1/0/all/0/1\">Dimitar Filev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>",
          "description": "The field of Meta Reinforcement Learning (Meta-RL) has seen substantial\nadvancements recently. In particular, off-policy methods were developed to\nimprove the data efficiency of Meta-RL techniques. \\textit{Probabilistic\nembeddings for actor-critic RL} (PEARL) is currently one of the leading\napproaches for multi-MDP adaptation problems. A major drawback of many existing\nMeta-RL methods, including PEARL, is that they do not explicitly consider the\nsafety of the prior policy when it is exposed to a new task for the very first\ntime. This is very important for some real-world applications, including field\nrobots and Autonomous Vehicles (AVs). In this paper, we develop the PEARL PLUS\n(PEARL$^+$) algorithm, which optimizes the policy for both prior safety and\nposterior adaptation. Building on top of PEARL, our proposed PEARL$^+$\nalgorithm introduces a prior regularization term in the reward function and a\nnew Q-network for recovering the state-action value with prior context\nassumption, to improve the robustness and safety of the trained network\nexposing to a new task for the first time. The performance of the PEARL$^+$\nmethod is demonstrated by solving three safety-critical decision-making\nproblems related to robots and AVs, including two MuJoCo benchmark problems.\nFrom the simulation experiments, we show that the safety of the prior policy is\nsignificantly improved compared to that of the original PEARL method.",
          "link": "http://arxiv.org/abs/2108.08448",
          "publishedOn": "2021-08-20T01:53:52.571Z",
          "wordCount": 670,
          "title": "Prior Is All You Need to Improve the Robustness and Safety for the First Time Deployment of Meta RL. (arXiv:2108.08448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1\">Hamsa Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinchaisri_W/0/1/0/all/0/1\">Wichinpong Park Sinchaisri</a>",
          "description": "A key aspect of human intelligence is their ability to convey their knowledge\nto others in succinct forms. However, despite their predictive power, current\nmachine learning models are largely blackboxes, making it difficult for humans\nto extract useful insights. Focusing on sequential decision-making, we design a\nnovel machine learning algorithm that conveys its insights to humans in the\nform of interpretable \"tips\". Our algorithm selects the tip that best bridges\nthe gap in performance between human users and the optimal policy. We evaluate\nour approach through a series of randomized controlled user studies where\nparticipants manage a virtual kitchen. Our experiments show that the tips\ngenerated by our algorithm can significantly improve human performance relative\nto intuitive baselines. In addition, we discuss a number of empirical insights\nthat can help inform the design of algorithms intended for human-AI\ncollaboration. For instance, we find evidence that participants do not simply\nblindly follow our tips; instead, they combine them with their own experience\nto discover additional strategies for improving performance.",
          "link": "http://arxiv.org/abs/2108.08454",
          "publishedOn": "2021-08-20T01:53:52.565Z",
          "wordCount": 598,
          "title": "Improving Human Decision-Making with Machine Learning. (arXiv:2108.08454v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.11722",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zenati_H/0/1/0/all/0/1\">Houssam Zenati</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1\">Alberto Bietti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martin_M/0/1/0/all/0/1\">Matthieu Martin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Diemert_E/0/1/0/all/0/1\">Eustache Diemert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>",
          "description": "Counterfactual reasoning from logged data has become increasingly important\nfor many applications such as web advertising or healthcare. In this paper, we\naddress the problem of learning stochastic policies with continuous actions\nfrom the viewpoint of counterfactual risk minimization (CRM). While the CRM\nframework is appealing and well studied for discrete actions, the continuous\naction case raises new challenges about modelization, optimization, and~offline\nmodel selection with real data which turns out to be particularly challenging.\nOur paper contributes to these three aspects of the CRM estimation pipeline.\nFirst, we introduce a modelling strategy based on a joint kernel embedding of\ncontexts and actions, which overcomes the shortcomings of previous\ndiscretization approaches. Second, we empirically show that the optimization\naspect of counterfactual learning is important, and we demonstrate the benefits\nof proximal point algorithms and differentiable estimators. Finally, we propose\nan evaluation protocol for offline policies in real-world logged systems, which\nis challenging since policies cannot be replayed on test data, and we release a\nnew large-scale dataset along with multiple synthetic, yet realistic,\nevaluation setups.",
          "link": "http://arxiv.org/abs/2004.11722",
          "publishedOn": "2021-08-20T01:53:52.559Z",
          "wordCount": 654,
          "title": "Counterfactual Learning of Stochastic Policies withContinuous Actions: from Models to Offline Evaluation. (arXiv:2004.11722v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sohns_J/0/1/0/all/0/1\">Jan-Tobias Sohns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michaela Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jirasek_F/0/1/0/all/0/1\">Fabian Jirasek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasse_H/0/1/0/all/0/1\">Hans Hasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leitte_H/0/1/0/all/0/1\">Heike Leitte</a>",
          "description": "Embeddings of high-dimensional data are widely used to explore data, to\nverify analysis results, and to communicate information. Their explanation, in\nparticular with respect to the input attributes, is often difficult. With\nlinear projects like PCA the axes can still be annotated meaningfully. With\nnon-linear projections this is no longer possible and alternative strategies\nsuch as attribute-based color coding are required. In this paper, we review\nexisting augmentation techniques and discuss their limitations. We present the\nNon-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation\nstrategy for projected data (rangesets) with interactive analysis in a small\nmultiples setting. Rangesets use a set-based visualization approach for binned\nattribute values that enable the user to quickly observe structure and detect\noutliers. We detail the link between algebraic topology and rangesets and\ndemonstrate the utility of NoLiES in case studies with various challenges\n(complex attribute value distribution, many attributes, many data points) and a\nreal-world application to understand latent features of matrix completion in\nthermodynamics.",
          "link": "http://arxiv.org/abs/2108.08706",
          "publishedOn": "2021-08-20T01:53:52.551Z",
          "wordCount": 602,
          "title": "Attribute-based Explanations of Non-Linear Embeddings of High-Dimensional Data. (arXiv:2108.08706v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollis_C/0/1/0/all/0/1\">Courtland Hollis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkison_T/0/1/0/all/0/1\">Travis Atkison</a>",
          "description": "An adaptive traffic signal controller (ATSC) combined with a connected\nvehicle (CV) concept uses real-time vehicle trajectory data to regulate green\ntime and has the ability to reduce intersection waiting time significantly and\nthereby improve travel time in a signalized corridor. However, the CV-based\nATSC increases the size of the surface vulnerable to potential cyber-attack,\nallowing an attacker to generate disastrous traffic congestion in a roadway\nnetwork. An attacker can congest a route by generating fake vehicles by\nmaintaining traffic and car-following rules at a slow rate so that the signal\ntiming and phase change without having any abrupt changes in number of\nvehicles. Because of the adaptive nature of ATSC, it is a challenge to model\nthis kind of attack and also to develop a strategy for detection. This paper\nintroduces an innovative \"slow poisoning\" cyberattack for a waiting time based\nATSC algorithm and a corresponding detection strategy. Thus, the objectives of\nthis paper are to: (i) develop a \"slow poisoning\" attack generation strategy\nfor an ATSC, and (ii) develop a prediction-based \"slow poisoning\" attack\ndetection strategy using a recurrent neural network -- i.e., long short-term\nmemory model. We have generated a \"slow poisoning\" attack modeling strategy\nusing a microscopic traffic simulator -- Simulation of Urban Mobility (SUMO) --\nand used generated data from the simulation to develop both the attack model\nand detection model. Our analyses revealed that the attack strategy is\neffective in creating a congestion in an approach and detection strategy is\nable to flag the attack.",
          "link": "http://arxiv.org/abs/2108.08627",
          "publishedOn": "2021-08-20T01:53:52.545Z",
          "wordCount": 695,
          "title": "An Innovative Attack Modelling and Attack Detection Approach for a Waiting Time-based Adaptive Traffic Signal Controller. (arXiv:2108.08627v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehdy_A/0/1/0/all/0/1\">A K M Nuhil Mehdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrpouyan_H/0/1/0/all/0/1\">Hoda Mehrpouyan</a>",
          "description": "The concern regarding users' data privacy has risen to its highest level due\nto the massive increase in communication platforms, social networking sites,\nand greater users' participation in online public discourse. An increasing\nnumber of people exchange private information via emails, text messages, and\nsocial media without being aware of the risks and implications. Researchers in\nthe field of Natural Language Processing (NLP) have concentrated on creating\ntools and strategies to identify, categorize, and sanitize private information\nin text data since a substantial amount of data is exchanged in textual form.\nHowever, most of the detection methods solely rely on the existence of\npre-identified keywords in the text and disregard the inference of the\nunderlying meaning of the utterance in a specific context. Hence, in some\nsituations, these tools and algorithms fail to detect disclosure, or the\nproduced results are miss-classified. In this paper, we propose a multi-input,\nmulti-output hybrid neural network which utilizes transfer-learning,\nlinguistics, and metadata to learn the hidden patterns. Our goal is to better\nclassify disclosure/non-disclosure content in terms of the context of\nsituation. We trained and evaluated our model on a human-annotated ground truth\ndataset, containing a total of 5,400 tweets. The results show that the proposed\nmodel was able to identify privacy disclosure through tweets with an accuracy\nof 77.4% while classifying the information type of those tweets with an\nimpressive accuracy of 99%, by jointly learning for two separate tasks.",
          "link": "http://arxiv.org/abs/2108.08483",
          "publishedOn": "2021-08-20T01:53:52.539Z",
          "wordCount": 699,
          "title": "A Multi-input Multi-output Transformer-based Hybrid Neural Network for Multi-class Privacy Disclosure Detection. (arXiv:2108.08483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03193",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Khatri_S/0/1/0/all/0/1\">Sumeet Khatri</a>",
          "description": "Distributing entanglement over long distances is one of the central tasks in\nquantum networks. An important problem, especially for near-term quantum\nnetworks, is to develop optimal entanglement distribution protocols that take\ninto account the limitations of current and near-term hardware, such as quantum\nmemories with limited coherence time. We address this problem by initiating the\nstudy of quantum network protocols for entanglement distribution using the\ntheory of decision processes, such that optimal protocols (referred to as\npolicies in the context of decision processes) can be found using dynamic\nprogramming or reinforcement learning algorithms. As a first step, in this work\nwe focus exclusively on the elementary link level. We start by defining a\nquantum decision process for elementary links, along with figures of merit for\nevaluating policies. We then provide two algorithms for determining policies,\none of which we prove to be optimal (with respect to fidelity and success\nprobability) among all policies. Then we show that the previously-studied\nmemory-cutoff protocol can be phrased as a policy within our decision process\nframework, allowing us to obtain several new fundamental results about it. The\nconceptual developments and results of this work pave the way for the\nsystematic study of the fundamental limitations of near-term quantum networks,\nand the requirements for physically realizing them.",
          "link": "http://arxiv.org/abs/2007.03193",
          "publishedOn": "2021-08-20T01:53:52.519Z",
          "wordCount": 684,
          "title": "Policies for elementary links in a quantum network. (arXiv:2007.03193v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08628",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sagar Dasgupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_T/0/1/0/all/0/1\">Tonmoy Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>",
          "description": "A resilient and robust positioning, navigation, and timing (PNT) system is a\nnecessity for the navigation of autonomous vehicles (AVs). Global Navigation\nSatelite System (GNSS) provides satellite-based PNT services. However, a\nspoofer can temper an authentic GNSS signal and could transmit wrong position\ninformation to an AV. Therefore, a GNSS must have the capability of real-time\ndetection and feedback-correction of spoofing attacks related to PNT receivers,\nwhereby it will help the end-user (autonomous vehicle in this case) to navigate\nsafely if it falls into any compromises. This paper aims to develop a deep\nreinforcement learning (RL)-based turn-by-turn spoofing attack detection using\nlow-cost in-vehicle sensor data. We have utilized Honda Driving Dataset to\ncreate attack and non-attack datasets, develop a deep RL model, and evaluate\nthe performance of the RL-based attack detection model. We find that the\naccuracy of the RL model ranges from 99.99% to 100%, and the recall value is\n100%. However, the precision ranges from 93.44% to 100%, and the f1 score\nranges from 96.61% to 100%. Overall, the analyses reveal that the RL model is\neffective in turn-by-turn spoofing attack detection.",
          "link": "http://arxiv.org/abs/2108.08628",
          "publishedOn": "2021-08-20T01:53:52.513Z",
          "wordCount": 626,
          "title": "A Reinforcement Learning Approach for GNSS Spoofing Attack Detection of Autonomous Vehicles. (arXiv:2108.08628v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1\">Florian Merkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1\">Maximilian Samsinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1\">Pascal Sch&#xf6;ttle</a>",
          "description": "The vulnerability of deep neural networks against adversarial examples -\ninputs with small imperceptible perturbations - has gained a lot of attention\nin the research community recently. Simultaneously, the number of parameters of\nstate-of-the-art deep learning models has been growing massively, with\nimplications on the memory and computational resources required to train and\ndeploy such models. One approach to control the size of neural networks is\nretrospectively reducing the number of parameters, so-called neural network\npruning. Available research on the impact of neural network pruning on the\nadversarial robustness is fragmentary and often does not adhere to established\nprinciples of robustness evaluation. We close this gap by evaluating the\nrobustness of pruned models against L-0, L-2 and L-infinity attacks for a wide\nrange of attack strengths, several architectures, data sets, pruning methods,\nand compression rates. Our results confirm that neural network pruning and\nadversarial robustness are not mutually exclusive. Instead, sweet spots can be\nfound that are favorable in terms of model size and adversarial robustness.\nFurthermore, we extend our analysis to situations that incorporate additional\nassumptions on the adversarial scenario and show that depending on the\nsituation, different strategies are optimal.",
          "link": "http://arxiv.org/abs/2108.08560",
          "publishedOn": "2021-08-20T01:53:52.507Z",
          "wordCount": 621,
          "title": "Pruning in the Face of Adversaries. (arXiv:2108.08560v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1\">Jakub Grudzien Kuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Muning Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Linghui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shangding Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1\">David Henry Mguni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Policy gradient (PG) methods are popular reinforcement learning (RL) methods\nwhere a baseline is often applied to reduce the variance of gradient estimates.\nIn multi-agent RL (MARL), although the PG theorem can be naturally extended,\nthe effectiveness of multi-agent PG (MAPG) methods degrades as the variance of\ngradient estimates increases rapidly with the number of agents. In this paper,\nwe offer a rigorous analysis of MAPG methods by, firstly, quantifying the\ncontributions of the number of agents and agents' explorations to the variance\nof MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)\nthat achieves the minimal variance. In comparison to the OB, we measure the\nexcess variance of existing MARL algorithms such as vanilla MAPG and COMA.\nConsidering using deep neural networks, we also propose a surrogate version of\nOB, which can be seamlessly plugged into any existing PG methods in MARL. On\nbenchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique\neffectively stabilises training and improves the performance of multi-agent PPO\nand COMA algorithms by a significant margin.",
          "link": "http://arxiv.org/abs/2108.08612",
          "publishedOn": "2021-08-20T01:53:52.500Z",
          "wordCount": 621,
          "title": "Settling the Variance of Multi-Agent Policy Gradients. (arXiv:2108.08612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Existing self-supervised learning methods learn representation by means of\npretext tasks which are either (1) discriminating that explicitly specify which\nfeatures should be separated or (2) aligning that precisely indicate which\nfeatures should be closed together, but ignore the fact how to jointly and\nprincipally define which features to be repelled and which ones to be\nattracted. In this work, we combine the positive aspects of the discriminating\nand aligning methods, and design a hybrid method that addresses the above\nissue. Our method explicitly specifies the repulsion and attraction mechanism\nrespectively by discriminative predictive task and concurrently maximizing\nmutual information between paired views sharing redundant information. We\nqualitatively and quantitatively show that our proposed model learns better\nfeatures that are more effective for the diverse downstream tasks ranging from\nclassification to semantic segmentation. Our experiments on nine established\nbenchmarks show that the proposed model consistently outperforms the existing\nstate-of-the-art results of self-supervised and transfer learning protocol.",
          "link": "http://arxiv.org/abs/2108.08562",
          "publishedOn": "2021-08-20T01:53:52.493Z",
          "wordCount": 602,
          "title": "Concurrent Discrimination and Alignment for Self-Supervised Feature Learning. (arXiv:2108.08562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_D/0/1/0/all/0/1\">David Schn&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>",
          "description": "The Turing mechanism describes the emergence of spatial patterns due to\nspontaneous symmetry breaking in reaction-diffusion processes and underlies\nmany developmental processes. Identifying Turing mechanisms in biological\nsystems defines a challenging problem. This paper introduces an approach to the\nprediction of Turing parameter values from observed Turing patterns. The\nparameter values correspond to a parametrized system of reaction-diffusion\nequations that generate Turing patterns as steady state. The Gierer-Meinhardt\nmodel with four parameters is chosen as a case study. A novel invariant pattern\nrepresentation based on resistance distance histograms is employed, along with\nWasserstein kernels, in order to cope with the highly variable arrangement of\nlocal pattern structure that depends on the initial conditions which are\nassumed to be unknown. This enables to compute physically plausible distances\nbetween patterns, to compute clusters of patterns and, above all, model\nparameter prediction: for small training sets, classical state-of-the-art\nmethods including operator-valued kernels outperform neural networks that are\napplied to raw pattern data, whereas for large training sets the latter are\nmore accurate. Excellent predictions are obtained for single parameter values\nand reasonably accurate results for jointly predicting all parameter values.",
          "link": "http://arxiv.org/abs/2108.08542",
          "publishedOn": "2021-08-20T01:53:52.468Z",
          "wordCount": 618,
          "title": "Learning System Parameters from Turing Patterns. (arXiv:2108.08542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suin_M/0/1/0/all/0/1\">Maitreya Suin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "We present a general learning-based solution for restoring images suffering\nfrom spatially-varying degradations. Prior approaches are typically\ndegradation-specific and employ the same processing across different images and\ndifferent pixels within. However, we hypothesize that such spatially rigid\nprocessing is suboptimal for simultaneously restoring the degraded pixels as\nwell as reconstructing the clean regions of the image. To overcome this\nlimitation, we propose SPAIR, a network design that harnesses\ndistortion-localization information and dynamically adjusts computation to\ndifficult regions in the image. SPAIR comprises of two components, (1) a\nlocalization network that identifies degraded pixels, and (2) a restoration\nnetwork that exploits knowledge from the localization network in filter and\nfeature domain to selectively and adaptively restore degraded pixels. Our key\nidea is to exploit the non-uniformity of heavy degradations in spatial-domain\nand suitably embed this knowledge within distortion-guided modules performing\nsparse normalization, feature extraction and attention. Our architecture is\nagnostic to physical formation model and generalizes across several types of\nspatially-varying degradations. We demonstrate the efficacy of SPAIR\nindividually on four restoration tasks-removal of rain-streaks, raindrops,\nshadows and motion blur. Extensive qualitative and quantitative comparisons\nwith prior art on 11 benchmark datasets demonstrate that our\ndegradation-agnostic network design offers significant performance gains over\nstate-of-the-art degradation-specific architectures. Code available at\nhttps://github.com/human-analysis/spatially-adaptive-image-restoration.",
          "link": "http://arxiv.org/abs/2108.08617",
          "publishedOn": "2021-08-20T01:53:52.461Z",
          "wordCount": 653,
          "title": "Spatially-Adaptive Image Restoration using Distortion-Guided Networks. (arXiv:2108.08617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krokotsch_T/0/1/0/all/0/1\">Tilman Krokotsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knaak_M/0/1/0/all/0/1\">Mirko Knaak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guhmann_C/0/1/0/all/0/1\">Clemens G&#xfc;hmann</a>",
          "description": "RUL estimation suffers from a server data imbalance where data from machines\nnear their end of life is rare. Additionally, the data produced by a machine\ncan only be labeled after the machine failed. Semi-Supervised Learning (SSL)\ncan incorporate the unlabeled data produced by machines that did not yet fail.\nPrevious work on SSL evaluated their approaches under unrealistic conditions\nwhere the data near failure was still available. Even so, only moderate\nimprovements were made. This paper proposes a novel SSL approach based on\nself-supervised pre-training. The method can outperform two competing\napproaches from the literature and a supervised baseline under realistic\nconditions on the NASA C-MAPSS dataset. Nevertheless, we observe degraded\nperformance in some circumstances and discuss possible causes.",
          "link": "http://arxiv.org/abs/2108.08721",
          "publishedOn": "2021-08-20T01:53:52.454Z",
          "wordCount": 558,
          "title": "Improving Semi-Supervised Learning for Remaining Useful Lifetime Estimation Through Self-Supervision. (arXiv:2108.08721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingjun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zikui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>",
          "description": "Vision systems that deploy Deep Neural Networks (DNNs) are known to be\nvulnerable to adversarial examples. Recent research has shown that checking the\nintrinsic consistencies in the input data is a promising way to detect\nadversarial attacks (e.g., by checking the object co-occurrence relationships\nin complex scenes). However, existing approaches are tied to specific models\nand do not offer generalizability. Motivated by the observation that language\ndescriptions of natural scene images have already captured the object\nco-occurrence relationships that can be learned by a language model, we develop\na novel approach to perform context consistency checks using such language\nmodels. The distinguishing aspect of our approach is that it is independent of\nthe deployed object detector and yet offers very high accuracy in terms of\ndetecting adversarial examples in practical scenes with multiple objects.",
          "link": "http://arxiv.org/abs/2108.08421",
          "publishedOn": "2021-08-20T01:53:52.435Z",
          "wordCount": 590,
          "title": "Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes. (arXiv:2108.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "The performance of a computer vision model depends on the size and quality of\nits training data. Recent studies have unveiled previously-unknown composition\nbiases in common image datasets which then lead to skewed model outputs, and\nhave proposed methods to mitigate these biases. However, most existing works\nassume that human-generated annotations can be considered gold-standard and\nunbiased. In this paper, we reveal that this assumption can be problematic, and\nthat special care should be taken to prevent models from learning such\nannotation biases. We focus on facial expression recognition and compare the\nlabel biases between lab-controlled and in-the-wild datasets. We demonstrate\nthat many expression datasets contain significant annotation biases between\ngenders, especially when it comes to the happy and angry expressions, and that\ntraditional methods cannot fully mitigate such biases in trained models. To\nremove expression annotation bias, we propose an AU-Calibrated Facial\nExpression Recognition (AUC-FER) framework that utilizes facial action units\n(AUs) and incorporates the triplet loss into the objective function.\nExperimental results suggest that the proposed method is more effective in\nremoving expression annotation bias than existing techniques.",
          "link": "http://arxiv.org/abs/2108.08504",
          "publishedOn": "2021-08-20T01:53:52.429Z",
          "wordCount": 631,
          "title": "Understanding and Mitigating Annotation Bias in Facial Expression Recognition. (arXiv:2108.08504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Heat management plays an important role in engineering. Temperature field\nreconstruction of heat source systems (TFR-HSS) with limited monitoring\ntensors, performs an essential role in heat management. However, prior methods\nwith common interpolations usually cannot provide accurate reconstruction. In\naddition, there exists no public dataset for widely research of reconstruction\nmethods to further boost the field reconstruction in engineering. To overcome\nthis problem, this work construct a specific dataset, namely TFRD, for TFR-HSS\ntask with commonly used methods, including the interpolation methods and the\nsurrogate model based methods, as baselines to advance the research over\ntemperature field reconstruction. First, the TFR-HSS task is mathematically\nmodelled from real-world engineering problem and three types of numerically\nmodellings have been constructed to transform the problem into discrete mapping\nforms. Besides, this work selects four typical reconstruction problem with\ndifferent heat source information and boundary conditions and generate the\nstandard samples as training and testing samples for further research. Finally,\na comprehensive review of the prior methods for TFR-HSS task as well as recent\nwidely used deep learning methods is given and we provide a performance\nanalysis of typical methods on TFRD, which can be served as the baseline\nresults on this benchmark.",
          "link": "http://arxiv.org/abs/2108.08298",
          "publishedOn": "2021-08-20T01:53:52.409Z",
          "wordCount": 649,
          "title": "TFRD: A Benchmark Dataset for Research on Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2108.08298v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1\">Faezeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematollahi_M/0/1/0/all/0/1\">Mohammad Ali Nematollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>",
          "description": "Coronary heart disease (CAD) is one of the crucial reasons for cardiovascular\nmortality in middle-aged people worldwide. The most typical tool is angiography\nfor diagnosing CAD. The challenges of CAD diagnosis using angiography are\ncostly and have side effects. One of the alternative solutions is the use of\nmachine learning-based patterns for CAD diagnosis. Hence, this paper provides a\nnew hybrid machine learning model called Genetic Support Vector Machine and\nAnalysis of Variance (GSVMA). The ANOVA is known as the kernel function for\nSVM. The proposed model is performed based on the Z-Alizadeh Sani dataset. A\ngenetic optimization algorithm is used to select crucial features. In addition,\nSVM with Anova, Linear SVM, and LibSVM with radial basis function methods were\napplied to classify the dataset. As a result, the GSVMA hybrid method performs\nbetter than other methods. This proposed method has the highest accuracy of\n89.45% through a 10-fold cross-validation technique with 35 selected features\non the Z-Alizadeh Sani dataset. Therefore, the genetic optimization algorithm\nis very effective for improving accuracy. The computer-aided GSVMA method can\nbe helped clinicians with CAD diagnosis.",
          "link": "http://arxiv.org/abs/2108.08292",
          "publishedOn": "2021-08-20T01:53:52.311Z",
          "wordCount": 634,
          "title": "GSVMA: A Genetic-Support Vector Machine-Anova method for CAD diagnosis based on Z-Alizadeh Sani dataset. (arXiv:2108.08292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>",
          "description": "Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.",
          "link": "http://arxiv.org/abs/2108.08712",
          "publishedOn": "2021-08-20T01:53:52.302Z",
          "wordCount": 550,
          "title": "Teaching Uncertainty Quantification in Machine Learning through Use Cases. (arXiv:2108.08712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garau_N/0/1/0/all/0/1\">Nicola Garau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisagno_N/0/1/0/all/0/1\">Niccol&#xf2; Bisagno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodka_P/0/1/0/all/0/1\">Piotr Br&#xf3;dka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conci_N/0/1/0/all/0/1\">Nicola Conci</a>",
          "description": "Human Pose Estimation (HPE) aims at retrieving the 3D position of human\njoints from images or videos. We show that current 3D HPE methods suffer a lack\nof viewpoint equivariance, namely they tend to fail or perform poorly when\ndealing with viewpoints unseen at training time. Deep learning methods often\nrely on either scale-invariant, translation-invariant, or rotation-invariant\noperations, such as max-pooling. However, the adoption of such procedures does\nnot necessarily improve viewpoint generalization, rather leading to more\ndata-dependent methods. To tackle this issue, we propose a novel capsule\nautoencoder network with fast Variational Bayes capsule routing, named DECA. By\nmodeling each joint as a capsule entity, combined with the routing algorithm,\nour approach can preserve the joints' hierarchical and geometrical structure in\nthe feature space, independently from the viewpoint. By achieving viewpoint\nequivariance, we drastically reduce the network data dependency at training\ntime, resulting in an improved ability to generalize for unseen viewpoints. In\nthe experimental validation, we outperform other methods on depth images from\nboth seen and unseen viewpoints, both top-view, and front-view. In the RGB\ndomain, the same network gives state-of-the-art results on the challenging\nviewpoint transfer task, also establishing a new framework for top-view HPE.\nThe code can be found at https://github.com/mmlab-cv/DECA.",
          "link": "http://arxiv.org/abs/2108.08557",
          "publishedOn": "2021-08-20T01:53:52.296Z",
          "wordCount": 668,
          "title": "DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders. (arXiv:2108.08557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yilin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haozhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinggao Liu</a>",
          "description": "The effectiveness of shortcut/skip-connection has been widely verified, which\ninspires massive explorations on neural architecture design. This work attempts\nto find an effective way to design new network architectures. It is discovered\nthat the main difference between network architectures can be reflected in\ntheir recursion formulas. Based on this, a methodology is proposed to design\nnovel network architectures from the perspective of mathematical formulas.\nAfterwards, a case study is provided to generate an improved architecture based\non ResNet. Furthermore, the new architecture is compared with ResNet and then\ntested on ResNet-based networks. Massive experiments are conducted on CIFAR and\nImageNet, which witnesses the significant performance improvements provided by\nthe architecture.",
          "link": "http://arxiv.org/abs/2108.08689",
          "publishedOn": "2021-08-20T01:53:52.269Z",
          "wordCount": 562,
          "title": "Analyze and Design Network Architectures by Recursion Formulas. (arXiv:2108.08689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08670",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1\">Kushal Chakrabarti</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1\">Nikhil Chopra</a>",
          "description": "This paper studies a distributed multi-agent convex optimization problem. The\nsystem comprises multiple agents in this problem, each with a set of local data\npoints and an associated local cost function. The agents are connected to a\nserver, and there is no inter-agent communication. The agents' goal is to learn\na parameter vector that optimizes the aggregate of their local costs without\nrevealing their local data points. In principle, the agents can solve this\nproblem by collaborating with the server using the traditional distributed\ngradient-descent method. However, when the aggregate cost is ill-conditioned,\nthe gradient-descent method (i) requires a large number of iterations to\nconverge, and (ii) is highly unstable against process noise. We propose an\niterative pre-conditioning technique to mitigate the deleterious effects of the\ncost function's conditioning on the convergence rate of distributed\ngradient-descent. Unlike the conventional pre-conditioning techniques, the\npre-conditioner matrix in our proposed technique updates iteratively to\nfacilitate implementation on the distributed network. In the distributed\nsetting, we provably show that the proposed algorithm converges linearly with\nan improved rate of convergence than the traditional and adaptive\ngradient-descent methods. Additionally, for the special case when the minimizer\nof the aggregate cost is unique, our algorithm converges superlinearly. We\ndemonstrate our algorithm's superior performance compared to prominent\ndistributed algorithms for solving real logistic regression problems and\nemulating neural network training via a noisy quadratic model, thereby\nsignifying the proposed algorithm's efficiency for distributively solving\nnon-convex optimization. Moreover, we empirically show that the proposed\nalgorithm results in faster training without compromising the generalization\nperformance.",
          "link": "http://arxiv.org/abs/2108.08670",
          "publishedOn": "2021-08-20T01:53:52.263Z",
          "wordCount": 696,
          "title": "On Accelerating Distributed Convex Optimizations. (arXiv:2108.08670v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_S/0/1/0/all/0/1\">Shen-Lung Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>",
          "description": "Spatial-temporal prediction is a critical problem for intelligent\ntransportation, which is helpful for tasks such as traffic control and accident\nprevention. Previous studies rely on large-scale traffic data collected from\nsensors. However, it is unlikely to deploy sensors in all regions due to the\ndevice and maintenance costs. This paper addresses the problem via outdoor\ncellular traffic distilled from over two billion records per day in a telecom\ncompany, because outdoor cellular traffic induced by user mobility is highly\nrelated to transportation traffic. We study road intersections in urban and aim\nto predict future outdoor cellular traffic of all intersections given historic\noutdoor cellular traffic. Furthermore, We propose a new model for multivariate\nspatial-temporal prediction, mainly consisting of two extending graph attention\nnetworks (GAT). First GAT is used to explore correlations among multivariate\ncellular traffic. Another GAT leverages the attention mechanism into graph\npropagation to increase the efficiency of capturing spatial dependency.\nExperiments show that the proposed model significantly outperforms the\nstate-of-the-art methods on our dataset.",
          "link": "http://arxiv.org/abs/2108.08307",
          "publishedOn": "2021-08-20T01:53:52.250Z",
          "wordCount": 612,
          "title": "Multivariate and Propagation Graph Attention Network for Spatial-Temporal Prediction with Outdoor Cellular Traffic. (arXiv:2108.08307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "Multi-view network embedding aims at projecting nodes in the network to\nlow-dimensional vectors, while preserving their multiple relations and\nattribute information. Contrastive learning-based methods have preliminarily\nshown promising performance in this task. However, most contrastive\nlearning-based methods mostly rely on high-quality graph embedding and explore\nless on the relationships between different graph views. To deal with these\ndeficiencies, we design a novel node-to-node Contrastive learning framework for\nMulti-view network Embedding (CREME), which mainly contains two contrastive\nobjectives: Multi-view fusion InfoMax and Inter-view InfoMin. The former\nobjective distills information from embeddings generated from different graph\nviews, while the latter distinguishes different graph views better to capture\nthe complementary information between them. Specifically, we first apply a view\nencoder to generate each graph view representation and utilize a multi-view\naggregator to fuse these representations. Then, we unify the two contrastive\nobjectives into one learning objective for training. Extensive experiments on\nthree real-world datasets show that CREME outperforms existing methods\nconsistently.",
          "link": "http://arxiv.org/abs/2108.08296",
          "publishedOn": "2021-08-20T01:53:52.244Z",
          "wordCount": 594,
          "title": "Deep Contrastive Learning for Multi-View Network Embedding. (arXiv:2108.08296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyi Ye</a>",
          "description": "Federated Learning (FL) is a privacy-protected machine learning paradigm that\nallows model to be trained directly at the edge without uploading data. One of\nthe biggest challenges faced by FL in practical applications is the\nheterogeneity of edge node data, which will slow down the convergence speed and\ndegrade the performance of the model. For the above problems, a representative\nsolution is to add additional constraints in the local training, such as\nFedProx, FedCurv and FedCL. However, the above algorithms still have room for\nimprovement. We propose to use the aggregation of all models obtained in the\npast as new constraint target to further improve the performance of such\nalgorithms. Experiments in various settings demonstrate that our method\nsignificantly improves the convergence speed and performance of the model.",
          "link": "http://arxiv.org/abs/2108.08577",
          "publishedOn": "2021-08-20T01:53:52.237Z",
          "wordCount": 563,
          "title": "Towards More Efficient Federated Learning with Better Optimization Objects. (arXiv:2108.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>",
          "description": "This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).",
          "link": "http://arxiv.org/abs/2108.08708",
          "publishedOn": "2021-08-20T01:53:52.222Z",
          "wordCount": 594,
          "title": "Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sunwoong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanga Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kwanjung Yee</a>",
          "description": "Though inverse approach is computationally efficient in aerodynamic design as\nthe desired target performance distribution is specified, it has some\nsignificant limitations that prevent full efficiency from being achieved.\nFirst, the iterative procedure should be repeated whenever the specified target\ndistribution changes. Target distribution optimization can be performed to\nclarify the ambiguity in specifying this distribution, but several additional\nproblems arise in this process such as loss of the representation capacity due\nto parameterization of the distribution, excessive constraints for a realistic\ndistribution, inaccuracy of quantities of interest due to theoretical/empirical\npredictions, and the impossibility of explicitly imposing geometric\nconstraints. To deal with these issues, a novel inverse design optimization\nframework with a two-step deep learning approach is proposed. A variational\nautoencoder and multi-layer perceptron are used to generate a realistic target\ndistribution and predict the quantities of interest and shape parameters from\nthe generated distribution, respectively. Then, target distribution\noptimization is performed as the inverse design optimization. The proposed\nframework applies active learning and transfer learning techniques to improve\naccuracy and efficiency. Finally, the framework is validated through\naerodynamic shape optimizations of the airfoil of a wind turbine blade, where\ninverse design is actively being applied. The results of the optimizations show\nthat this framework is sufficiently accurate, efficient, and flexible to be\napplied to other inverse design engineering applications.",
          "link": "http://arxiv.org/abs/2108.08500",
          "publishedOn": "2021-08-20T01:53:52.215Z",
          "wordCount": 680,
          "title": "Inverse design optimization framework via a two-step deep learning approach: application to a wind turbine airfoil. (arXiv:2108.08500v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanfang Ye</a>",
          "description": "Illicit drug trafficking via social media sites such as Instagram has become\na severe problem, thus drawing a great deal of attention from law enforcement\nand public health agencies. How to identify illicit drug dealers from social\nmedia data has remained a technical challenge due to the following reasons. On\nthe one hand, the available data are limited because of privacy concerns with\ncrawling social media sites; on the other hand, the diversity of drug dealing\npatterns makes it difficult to reliably distinguish drug dealers from common\ndrug users. Unlike existing methods that focus on posting-based detection, we\npropose to tackle the problem of illicit drug dealer identification by\nconstructing a large-scale multimodal dataset named Identifying Drug Dealers on\nInstagram (IDDIG). Totally nearly 4,000 user accounts, of which over 1,400 are\ndrug dealers, have been collected from Instagram with multiple data sources\nincluding post comments, post images, homepage bio, and homepage images. We\nthen design a quadruple-based multimodal fusion method to combine the multiple\ndata sources associated with each user account for drug dealer identification.\nExperimental results on the constructed IDDIG dataset demonstrate the\neffectiveness of the proposed method in identifying drug dealers (almost 95%\naccuracy). Moreover, we have developed a hashtag-based community detection\ntechnique for discovering evolving patterns, especially those related to\ngeography and drug types.",
          "link": "http://arxiv.org/abs/2108.08301",
          "publishedOn": "2021-08-20T01:53:52.200Z",
          "wordCount": 662,
          "title": "Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion. (arXiv:2108.08301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1\">Pavel Dolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dHauthuille_L/0/1/0/all/0/1\">Luc d&#x27;Hauthuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vattani_A/0/1/0/all/0/1\">Andrea Vattani</a>",
          "description": "Twitch chats pose a unique problem in natural language understanding due to a\nlarge presence of neologisms, specifically emotes. There are a total of 8.06\nmillion emotes, over 400k of which were used in the week studied. There is\nvirtually no information on the meaning or sentiment of emotes, and with a\nconstant influx of new emotes and drift in their frequencies, it becomes\nimpossible to maintain an updated manually-labeled dataset. Our paper makes a\ntwo fold contribution. First we establish a new baseline for sentiment analysis\non Twitch data, outperforming the previous supervised benchmark by 7.9% points.\nSecondly, we introduce a simple but powerful unsupervised framework based on\nword embeddings and k-NN to enrich existing models with out-of-vocabulary\nknowledge. This framework allows us to auto-generate a pseudo-dictionary of\nemotes and we show that we can nearly match the supervised benchmark above even\nwhen injecting such emote knowledge into sentiment classifiers trained on\nextraneous datasets such as movie reviews or Twitter.",
          "link": "http://arxiv.org/abs/2108.08411",
          "publishedOn": "2021-08-20T01:53:52.168Z",
          "wordCount": 591,
          "title": "FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_R/0/1/0/all/0/1\">Reyan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turja_M/0/1/0/all/0/1\">Md Asadullah Turja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahneh_F/0/1/0/all/0/1\">Faryad Darabi Sahneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1\">Mithun Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1\">Stephen Kobourov</a>",
          "description": "Graph neural networks have been successful in many learning problems and\nreal-world applications. A recent line of research explores the power of graph\nneural networks to solve combinatorial and graph algorithmic problems such as\nsubgraph isomorphism, detecting cliques, and the traveling salesman problem.\nHowever, many NP-complete problems are as of yet unexplored using this method.\nIn this paper, we tackle the Steiner Tree Problem. We employ four learning\nframeworks to compute low cost Steiner trees: feed-forward neural networks,\ngraph neural networks, graph convolutional networks, and a graph attention\nmodel. We use these frameworks in two fundamentally different ways: 1) to train\nthe models to learn the actual Steiner tree nodes, 2) to train the model to\nlearn good Steiner point candidates to be connected to the constructed tree\nusing a shortest path in a greedy fashion. We illustrate the robustness of our\nheuristics on several random graph generation models as well as the SteinLib\ndata library. Our finding suggests that the out-of-the-box application of GNN\nmethods does worse than the classic 2-approximation method. However, when\ncombined with a greedy shortest path construction, it even does slightly better\nthan the 2-approximation algorithm. This result sheds light on the fundamental\ncapabilities and limitations of graph learning techniques on classical\nNP-complete problems.",
          "link": "http://arxiv.org/abs/2108.08368",
          "publishedOn": "2021-08-20T01:53:52.160Z",
          "wordCount": 643,
          "title": "Computing Steiner Trees using Graph Neural Networks. (arXiv:2108.08368v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yushan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seversky_L/0/1/0/all/0/1\">Lee Seversky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengtao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dahai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Houbing Song</a>",
          "description": "Anomaly detection has been a challenging task given high-dimensional\nmultivariate time series data generated by networked sensors and actuators in\nCyber-Physical Systems (CPS). Besides the highly nonlinear, complex, and\ndynamic natures of such time series, the lack of labeled data impedes data\nexploitation in a supervised manner and thus prevents an accurate detection of\nabnormal phenomenons. On the other hand, the collected data at the edge of the\nnetwork is often privacy sensitive and large in quantity, which may hinder the\ncentralized training at the main server. To tackle these issues, we propose an\nunsupervised time series anomaly detection framework in a federated fashion to\ncontinuously monitor the behaviors of interconnected devices within a network\nand alerts for abnormal incidents so that countermeasures can be taken before\nundesired consequences occur. To be specific, we leave the training data\ndistributed at the edge to learn a shared Variational Autoencoder (VAE) based\non Convolutional Gated Recurrent Unit (ConvGRU) model, which jointly captures\nfeature and temporal dependencies in the multivariate time series data for\nrepresentation learning and downstream anomaly detection tasks. Experiments on\nthree real-world networked sensor datasets illustrate the advantage of our\napproach over other state-of-the-art models. We also conduct extensive\nexperiments to demonstrate the effectiveness of our detection framework under\nnon-federated and federated settings in terms of overall performance and\ndetection latency.",
          "link": "http://arxiv.org/abs/2108.08404",
          "publishedOn": "2021-08-20T01:53:52.146Z",
          "wordCount": 674,
          "title": "Federated Variational Learning for Anomaly Detection in Multivariate Time Series. (arXiv:2108.08404v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08305",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lichuan Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1\">Royson Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed S. Abdelfattah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_H/0/1/0/all/0/1\">Hongkai Wen</a>",
          "description": "Deep learning-based blind super-resolution (SR) methods have recently\nachieved unprecedented performance in upscaling frames with unknown\ndegradation. These models are able to accurately estimate the unknown\ndownscaling kernel from a given low-resolution (LR) image in order to leverage\nthe kernel during restoration. Although these approaches have largely been\nsuccessful, they are predominantly image-based and therefore do not exploit the\ntemporal properties of the kernels across multiple video frames. In this paper,\nwe investigated the temporal properties of the kernels and highlighted its\nimportance in the task of blind video super-resolution. Specifically, we\nmeasured the kernel temporal consistency of real-world videos and illustrated\nhow the estimated kernels might change per frame in videos of varying\ndynamicity of the scene and its objects. With this new insight, we revisited\nprevious popular video SR approaches, and showed that previous assumptions of\nusing a fixed kernel throughout the restoration process can lead to visual\nartifacts when upscaling real-world videos. In order to counteract this, we\ntailored existing single-image and video SR techniques to leverage kernel\nconsistency during both kernel estimation and video upscaling processes.\nExtensive experiments on synthetic and real-world videos show substantial\nrestoration gains quantitatively and qualitatively, achieving the new\nstate-of-the-art in blind video SR and underlining the potential of exploiting\nkernel temporal consistency.",
          "link": "http://arxiv.org/abs/2108.08305",
          "publishedOn": "2021-08-20T01:53:52.138Z",
          "wordCount": 665,
          "title": "Temporal Kernel Consistency for Blind Video Super-Resolution. (arXiv:2108.08305v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niyas_S/0/1/0/all/0/1\">S. Niyas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pawan_S/0/1/0/all/0/1\">S J Pawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1\">M Anand Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>",
          "description": "Computer-aided medical image analysis plays a significant role in assisting\nmedical practitioners for expert clinical diagnosis and deciding the optimal\ntreatment plan. At present, convolutional neural networks (CNN) are the\npreferred choice for medical image analysis. In addition, with the rapid\nadvancements in three-dimensional (3D) imaging systems and the availability of\nexcellent hardware and software support to process large volumes of data, 3D\ndeep learning methods are gaining popularity in medical image analysis. Here,\nwe present an extensive review of the recently evolved 3D deep learning methods\nin medical image segmentation. Furthermore, the research gaps and future\ndirections in 3D medical image segmentation are discussed.",
          "link": "http://arxiv.org/abs/2108.08467",
          "publishedOn": "2021-08-20T01:53:52.112Z",
          "wordCount": 574,
          "title": "Medical Image Segmentation using 3D Convolutional Neural Networks: A Review. (arXiv:2108.08467v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "Federated learning (FL) can protect data privacy in distributed learning\nsince it merely collects local gradients from users without access to their\ndata. However, FL is fragile in the presence of heterogeneity that is commonly\nencountered in practical settings, e.g., non-IID data over different users.\nExisting FL approaches usually update a single global model to capture the\nshared knowledge of all users by aggregating their gradients, regardless of the\ndiscrepancy between their data distributions. By comparison, a mixture of\nmultiple global models could capture the heterogeneity across various users if\nassigning the users to different global models (i.e., centers) in FL. To this\nend, we propose a novel multi-center aggregation mechanism . It learns multiple\nglobal models from data, and simultaneously derives the optimal matching\nbetween users and centers. We then formulate it as a bi-level optimization\nproblem that can be efficiently solved by a stochastic expectation maximization\n(EM) algorithm. Experiments on multiple benchmark datasets of FL show that our\nmethod outperforms several popular FL competitors. The source code are open\nsource on Github.",
          "link": "http://arxiv.org/abs/2108.08647",
          "publishedOn": "2021-08-20T01:53:52.105Z",
          "wordCount": 610,
          "title": "Multi-Center Federated Learning. (arXiv:2108.08647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharifnassab_A/0/1/0/all/0/1\">Arsalan Sharifnassab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1\">Saber Salehkaleybar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golestani_S/0/1/0/all/0/1\">S. Jamaloddin Golestani</a>",
          "description": "We consider the problem of federated learning in a one-shot setting in which\nthere are $m$ machines, each observing $n$ samples function from an unknown\ndistribution on non-convex loss functions. Let $F:[-1,1]^d\\to\\mathbb{R}$ be the\nexpected loss function with respect to this unknown distribution. The goal is\nto find an estimate of the minimizer of $F$. Based on its observations, each\nmachine generates a signal of bounded length $B$ and sends it to a server. The\nsever collects signals of all machines and outputs an estimate of the minimizer\nof $F$. We propose a distributed learning algorithm, called Multi-Resolution\nEstimator for Non-Convex loss function (MRE-NC), whose expected error is\nbounded by $\\max\\big(1/\\sqrt{n}(mB)^{1/d}, 1/\\sqrt{mn}\\big)$, up to\npolylogarithmic factors. We also provide a matching lower bound on the\nperformance of any algorithm, showing that MRE-NC is order optimal in terms of\n$n$ and $m$. Experiments on synthetic and real data show the effectiveness of\nMRE-NC in distributed learning of model's parameters for non-convex loss\nfunctions.",
          "link": "http://arxiv.org/abs/2108.08677",
          "publishedOn": "2021-08-20T01:53:52.092Z",
          "wordCount": 592,
          "title": "Order Optimal One-Shot Federated Learning for non-Convex Loss Functions. (arXiv:2108.08677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junna~Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuisheng~Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_%7E/0/1/0/all/0/1\">~Cui~Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuan Zhang</a>",
          "description": "Kernel logistic regression (KLR) is a classical nonlinear classifier in\nstatistical machine learning. Newton method with quadratic convergence rate can\nsolve KLR problem more effectively than the gradient method. However, an\nobvious limitation of Newton method for training large-scale problems is the\n$O(n^{3})$ time complexity and $O(n^{2})$ space complexity, where $n$ is the\nnumber of training instances. In this paper, we employ the multilevel circulant\nmatrix (MCM) approximate kernel matrix to save in storage space and accelerate\nthe solution of the KLR. Combined with the characteristics of MCM and our\ningenious design, we propose an MCM approximate Newton iterative method. We\nfirst simplify the Newton direction according to the semi-positivity of the\nkernel matrix and then perform a two-step approximation of the Newton direction\nby using MCM. Our method reduces the time complexity of each iteration to $O(n\n\\log n)$ by using the multidimensional fast Fourier transform (mFFT). In\naddition, the space complexity can be reduced to $O(n)$ due to the built-in\nperiodicity of MCM. Experimental results on some large-scale binary and\nmulti-classification problems show that our method makes KLR scalable for\nlarge-scale problems, with less memory consumption, and converges to test\naccuracy without sacrifice in a shorter time.",
          "link": "http://arxiv.org/abs/2108.08605",
          "publishedOn": "2021-08-20T01:53:52.085Z",
          "wordCount": 631,
          "title": "Using Multilevel Circulant Matrix Approximate to Speed Up Kernel Logistic Regression. (arXiv:2108.08605v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "This paper studies the relative importance of attention heads in\nTransformer-based models to aid their interpretability in cross-lingual and\nmulti-lingual tasks. Prior research has found that only a few attention heads\nare important in each mono-lingual Natural Language Processing (NLP) task and\npruning the remaining heads leads to comparable or improved performance of the\nmodel. However, the impact of pruning attention heads is not yet clear in\ncross-lingual and multi-lingual tasks. Through extensive experiments, we show\nthat (1) pruning a number of attention heads in a multi-lingual\nTransformer-based model has, in general, positive effects on its performance in\ncross-lingual and multi-lingual tasks and (2) the attention heads to be pruned\ncan be ranked using gradients and identified with a few trial experiments. Our\nexperiments focus on sequence labeling tasks, with potential applicability on\nother cross-lingual and multi-lingual tasks. For comprehensiveness, we examine\ntwo pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and\nXLM-R, on three tasks across 9 languages each. We also discuss the validity of\nour findings and their extensibility to truly resource-scarce languages and\nother task settings.",
          "link": "http://arxiv.org/abs/2108.08375",
          "publishedOn": "2021-08-20T01:53:52.068Z",
          "wordCount": 626,
          "title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samajdar_A/0/1/0/all/0/1\">Ananda Samajdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1\">Jan Moritz Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_M/0/1/0/all/0/1\">Matthew Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tushar Krishna</a>",
          "description": "Design space exploration is an important but costly step involved in the\ndesign/deployment of custom architectures to squeeze out maximum possible\nperformance and energy efficiency. Conventionally, optimizations require\niterative sampling of the design space using simulation or heuristic tools. In\nthis paper we investigate the possibility of learning the optimization task\nusing machine learning and hence using the learnt model to predict optimal\nparameters for the design and mapping space of custom architectures, bypassing\nany exploration step. We use three case studies involving the optimal array\ndesign, SRAM buffer sizing, mapping, and schedule determination for\nsystolic-array-based custom architecture design and mapping space. Within the\npurview of these case studies, we show that it is possible to capture the\ndesign space and train a model to \"generalize\" prediction the optimal design\nand mapping parameters when queried with workload and design constraints. We\nperform systematic design-aware and statistical analysis of the optimization\nspace for our case studies and highlight the patterns in the design space. We\nformulate the architecture design and mapping as a machine learning problem\nthat allows us to leverage existing ML models for training and inference. We\ndesign and train a custom network architecture called AIRCHITECT, which is\ncapable of learning the architecture design space with as high as 94.3% test\naccuracy and predicting optimal configurations which achieve on average\n(GeoMean) of 99.9% the best possible performance on a test dataset with $10^5$\nGEMM workloads.",
          "link": "http://arxiv.org/abs/2108.08295",
          "publishedOn": "2021-08-20T01:53:52.060Z",
          "wordCount": 674,
          "title": "AIRCHITECT: Learning Custom Architecture Design and Mapping Space. (arXiv:2108.08295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-08-20T01:53:52.022Z",
          "wordCount": 701,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirignano_J/0/1/0/all/0/1\">Justin Sirignano</a>",
          "description": "Actor-critic algorithms are widely used in reinforcement learning, but are\nchallenging to mathematically analyze due to the online arrival of non-i.i.d.\ndata samples. The distribution of the data samples dynamically changes as the\nmodel is updated, introducing a complex feedback loop between the data\ndistribution and the reinforcement learning algorithm. We prove that, under a\ntime rescaling, the online actor-critic algorithm with tabular parametrization\nconverges to an ordinary differential equations (ODEs) as the number of updates\nbecomes large. The proof first establishes the geometric ergodicity of the data\nsamples under a fixed actor policy. Then, using a Poisson equation, we prove\nthat the fluctuations of the data samples around a dynamic probability measure,\nwhich is a function of the evolving actor model, vanish as the number of\nupdates become large. Once the ODE limit has been derived, we study its\nconvergence properties using a two time-scale analysis which asymptotically\nde-couples the critic ODE from the actor ODE. The convergence of the critic to\nthe solution of the Bellman equation and the actor to the optimal policy are\nproven. In addition, a convergence rate to this global minimum is also\nestablished. Our convergence analysis holds under specific choices for the\nlearning rates and exploration rates in the actor-critic algorithm, which could\nprovide guidance for the implementation of actor-critic algorithms in practice.",
          "link": "http://arxiv.org/abs/2108.08655",
          "publishedOn": "2021-08-20T01:53:52.000Z",
          "wordCount": 668,
          "title": "Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Recently, the generalization behavior of Convolutional Neural Networks (CNN)\nis gradually transparent through explanation techniques with the frequency\ncomponents decomposition. However, the importance of the phase spectrum of the\nimage for a robust vision system is still ignored. In this paper, we notice\nthat the CNN tends to converge at the local optimum which is closely related to\nthe high-frequency components of the training images, while the amplitude\nspectrum is easily disturbed such as noises or common corruptions. In contrast,\nmore empirical studies found that humans rely on more phase components to\nachieve robust recognition. This observation leads to more explanations of the\nCNN's generalization behaviors in both robustness to common perturbations and\nout-of-distribution detection, and motivates a new perspective on data\naugmentation designed by re-combing the phase spectrum of the current image and\nthe amplitude spectrum of the distracter image. That is, the generated samples\nforce the CNN to pay more attention to the structured information from phase\ncomponents and keep robust to the variation of the amplitude. Experiments on\nseveral image datasets indicate that the proposed method achieves\nstate-of-the-art performances on multiple generalizations and calibration\ntasks, including adaptability for common corruptions and surface variations,\nout-of-distribution detection, and adversarial attack.",
          "link": "http://arxiv.org/abs/2108.08487",
          "publishedOn": "2021-08-20T01:53:51.969Z",
          "wordCount": 654,
          "title": "Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain. (arXiv:2108.08487v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08350",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Shanny Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>",
          "description": "Accurately modeling power distribution grids is crucial for designing\neffective monitoring and decision making algorithms. This paper addresses the\npartial observability issue of data-driven distribution modeling in order to\nimprove the accuracy of line parameter estimation. Inspired by the sparse\nchanges in residential loads, we advocate to regularize the group sparsity of\nthe unobservable injections in a bi-linear estimation problem. The alternating\nminimization scheme of guaranteed convergence is proposed to take advantage of\nconvex subproblems with efficient solutions. Numerical results using real-world\nload data on the single-phase equivalent of the IEEE 123-bus test case have\ndemonstrated the accuracy improvements of the proposed solution over existing\nwork for both parameter estimation and voltage modeling.",
          "link": "http://arxiv.org/abs/2108.08350",
          "publishedOn": "2021-08-20T01:53:51.955Z",
          "wordCount": 547,
          "title": "Data-driven Modeling for Distribution Grids Under Partial Observability. (arXiv:2108.08350v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tauscher_Z/0/1/0/all/0/1\">Zachary Tauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yushan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Houbing Song</a>",
          "description": "With massive data being generated daily and the ever-increasing\ninterconnectivity of the world's Internet infrastructures, a machine learning\nbased intrusion detection system (IDS) has become a vital component to protect\nour economic and national security. In this paper, we perform a comprehensive\nstudy on NSL-KDD, a network traffic dataset, by visualizing patterns and\nemploying different learning-based models to detect cyber attacks. Unlike\nprevious shallow learning and deep learning models that use the single learning\nmodel approach for intrusion detection, we adopt a hierarchy strategy, in which\nthe intrusion and normal behavior are classified firstly, and then the specific\ntypes of attacks are classified. We demonstrate the advantage of the\nunsupervised representation learning model in binary intrusion detection tasks.\nBesides, we alleviate the data imbalance problem with SVM-SMOTE oversampling\ntechnique in 4-class classification and further demonstrate the effectiveness\nand the drawback of the oversampling mechanism with a deep neural network as a\nbase model.",
          "link": "http://arxiv.org/abs/2108.08394",
          "publishedOn": "2021-08-20T01:53:51.948Z",
          "wordCount": 610,
          "title": "Learning to Detect: A Data-driven Approach for Network Intrusion Detection. (arXiv:2108.08394v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-08-20T01:53:51.828Z",
          "wordCount": 613,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haghtalab_N/0/1/0/all/0/1\">Nika Haghtalab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roughgarden_T/0/1/0/all/0/1\">Tim Roughgarden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1\">Abhishek Shetty</a>",
          "description": "We prove novel algorithmic guarantees for several online problems in the\nsmoothed analysis model. In this model, at each time an adversary chooses an\ninput distribution with density function bounded above by $\\tfrac{1}{\\sigma}$\ntimes that of the uniform distribution; nature then samples an input from this\ndistribution. Crucially, our results hold for {\\em adaptive} adversaries that\ncan choose an input distribution based on the decisions of the algorithm and\nthe realizations of the inputs in the previous time steps.\n\nThis paper presents a general technique for proving smoothed algorithmic\nguarantees against adaptive adversaries, in effect reducing the setting of\nadaptive adversaries to the simpler case of oblivious adversaries. We apply\nthis technique to prove strong smoothed guarantees for three problems:\n\n-Online learning: We consider the online prediction problem, where instances\nare generated from an adaptive sequence of $\\sigma$-smooth distributions and\nthe hypothesis class has VC dimension $d$. We bound the regret by\n$\\tilde{O}\\big(\\sqrt{T d\\ln(1/\\sigma)} + d\\sqrt{\\ln(T/\\sigma)}\\big)$. This\nanswers open questions of [RST11,Hag18].\n\n-Online discrepancy minimization: We consider the online Koml\\'os problem,\nwhere the input is generated from an adaptive sequence of $\\sigma$-smooth and\nisotropic distributions on the $\\ell_2$ unit ball. We bound the $\\ell_\\infty$\nnorm of the discrepancy vector by $\\tilde{O}\\big(\\ln^2\\!\\big(\n\\frac{nT}{\\sigma}\\big) \\big)$.\n\n-Dispersion in online optimization: We consider online optimization of\npiecewise Lipschitz functions where functions with $\\ell$ discontinuities are\nchosen by a smoothed adaptive adversary and show that the resulting sequence is\n$\\big( {\\sigma}/{\\sqrt{T\\ell}}, \\tilde O\\big(\\sqrt{T\\ell}\n\\big)\\big)$-dispersed. This matches the parameters of [BDV18] for oblivious\nadversaries, up to log factors.",
          "link": "http://arxiv.org/abs/2102.08446",
          "publishedOn": "2021-08-20T01:53:51.809Z",
          "wordCount": 719,
          "title": "Smoothed Analysis with Adaptive Adversaries. (arXiv:2102.08446v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1\">Daniel M. Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1\">Vasilis Kontonis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1\">Christos Tzamos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1\">Nikos Zarifis</a>",
          "description": "We study the problem of PAC learning halfspaces on $\\mathbb{R}^d$ with\nMassart noise under Gaussian marginals. In the Massart noise model, an\nadversary is allowed to flip the label of each point $\\mathbf{x}$ with\nprobability $\\eta(\\mathbf{x}) \\leq \\eta$, for some parameter $\\eta \\in\n[0,1/2]$.\n\nThe goal of the learner is to output a hypothesis with missclassification\nerror $\\mathrm{opt} + \\epsilon$, where $\\mathrm{opt}$ is the error of the\ntarget halfspace. Prior work studied this problem assuming that the target\nhalfspace is homogeneous and that the parameter $\\eta$ is strictly smaller than\n$1/2$. We explore how the complexity of the problem changes when either of\nthese assumptions is removed, establishing the following threshold phenomena:\n\nFor $\\eta = 1/2$, we prove a lower bound of $d^{\\Omega (\\log(1/\\epsilon))}$\non the complexity of any Statistical Query (SQ) algorithm for the problem,\nwhich holds even for homogeneous halfspaces. On the positive side, we give a\nnew learning algorithm for arbitrary halfspaces in this regime with sample\ncomplexity and running time $O_\\epsilon(1) \\, d^{O(\\log(1/\\epsilon))}$.\n\nFor $\\eta <1/2$, we establish a lower bound of $d^{\\Omega(\\log(1/\\gamma))}$\non the SQ complexity of the problem, where $\\gamma = \\max\\{\\epsilon,\n\\min\\{\\mathbf{Pr}[f(\\mathbf{x}) = 1], \\mathbf{Pr}[f(\\mathbf{x}) = -1]\\} \\}$ and\n$f$ is the target halfspace. In particular, this implies an SQ lower bound of\n$d^{\\Omega (\\log(1/\\epsilon) )}$ for learning arbitrary Massart halfspaces\n(even for small constant $\\eta$). We complement this lower bound with a new\nlearning algorithm for this regime with sample complexity and runtime\n$d^{O_{\\eta}(\\log(1/\\gamma))} \\mathrm{poly}(1/\\epsilon)$.\n\nTaken together, our results qualitatively characterize the complexity of\nlearning halfspaces in the Massart model.",
          "link": "http://arxiv.org/abs/2108.08767",
          "publishedOn": "2021-08-20T01:53:51.753Z",
          "wordCount": 709,
          "title": "Threshold Phenomena in Learning Halfspaces with Massart Noise. (arXiv:2108.08767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youn Kyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyi Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>",
          "description": "In modern deep learning research, finding optimal (or near optimal) neural\nnetwork models is one of major research directions and it is widely studied in\nmany applications. In this paper, the main research trends of neural\narchitecture search (NAS) are classified as neuro-evolutionary algorithms,\nreinforcement learning based algorithms, and one-shot architecture search\napproaches. Furthermore, each research trend is introduced and finally all the\nmajor three trends are compared. Lastly, the future research directions of NAS\nresearch trends are discussed.",
          "link": "http://arxiv.org/abs/2108.08474",
          "publishedOn": "2021-08-20T01:53:51.742Z",
          "wordCount": 534,
          "title": "Trends in Neural Architecture Search: Towards the Acceleration of Search. (arXiv:2108.08474v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1\">Nikola Kovachki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Burigede Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kaushik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew Stuart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "The classical development of neural networks has primarily focused on\nlearning mappings between finite dimensional Euclidean spaces or finite sets.\nWe propose a generalization of neural networks tailored to learn operators\nmapping between infinite dimensional function spaces. We formulate the\napproximation of operators by composition of a class of linear integral\noperators and nonlinear activation functions, so that the composed operator can\napproximate complex nonlinear operators. Furthermore, we introduce four classes\nof operator parameterizations: graph-based operators, low-rank operators,\nmultipole graph-based operators, and Fourier operators and describe efficient\nalgorithms for computing with each one. The proposed neural operators are\nresolution-invariant: they share the same network parameters between different\ndiscretizations of the underlying function spaces and can be used for zero-shot\nsuper-resolutions. Numerically, the proposed models show superior performance\ncompared to existing machine learning based methodologies on Burgers' equation,\nDarcy flow, and the Navier-Stokes equation, while being several order of\nmagnitude faster compared to conventional PDE solvers.",
          "link": "http://arxiv.org/abs/2108.08481",
          "publishedOn": "2021-08-20T01:53:51.735Z",
          "wordCount": 595,
          "title": "Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Although automated metrics are commonly used to evaluate NLG systems, they\noften correlate poorly with human judgements. Newer metrics such as BERTScore\nhave addressed many weaknesses in prior metrics such as BLEU and ROUGE, which\nrely on n-gram matching. These newer methods, however, are still limited in\nthat they do not consider the generation context, so they cannot properly\nreward generated text that is correct but deviates from the given reference.\n\nIn this paper, we propose Language Model Augmented Relevance Score (MARS), a\nnew context-aware metric for NLG evaluation. MARS leverages off-the-shelf\nlanguage models, guided by reinforcement learning, to create augmented\nreferences that consider both the generation context and available human\nreferences, which are then used as additional references to score generated\ntext. Compared with seven existing metrics in three common NLG tasks, MARS not\nonly achieves higher correlation with human reference judgements, but also\ndifferentiates well-formed candidates from adversarial samples to a larger\ndegree.",
          "link": "http://arxiv.org/abs/2108.08485",
          "publishedOn": "2021-08-20T01:53:51.717Z",
          "wordCount": 589,
          "title": "Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Sen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weishen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "Federated learning (FL) has gain growing interests for its capability of\nlearning from distributed data sources collectively without the need of\naccessing the raw data samples across different sources. So far FL research has\nmostly focused on improving the performance, how the algorithmic disparity will\nbe impacted for the model learned from FL and the impact of algorithmic\ndisparity on the utility inconsistency are largely unexplored. In this paper,\nwe propose an FL framework to jointly consider performance consistency and\nalgorithmic fairness across different local clients (data sources). We derive\nour framework from a constrained multi-objective optimization perspective, in\nwhich we learn a model satisfying fairness constraints on all clients with\nconsistent performance. Specifically, we treat the algorithm prediction loss at\neach local client as an objective and maximize the worst-performing client with\nfairness constraints through optimizing a surrogate maximum function with all\nobjectives involved. A gradient-based procedure is employed to achieve the\nPareto optimality of this optimization problem. Theoretical analysis is\nprovided to prove that our method can converge to a Pareto solution that\nachieves the min-max performance with fairness constraints on all clients.\nComprehensive experiments on synthetic and real-world datasets demonstrate the\nsuperiority that our approach over baselines and its effectiveness in achieving\nboth fairness and consistency across all local clients.",
          "link": "http://arxiv.org/abs/2108.08435",
          "publishedOn": "2021-08-20T01:53:51.710Z",
          "wordCount": 639,
          "title": "Fair and Consistent Federated Learning. (arXiv:2108.08435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>",
          "description": "Self-supervised learning has been successfully applied to pre-train video\nrepresentations, which aims at efficient adaptation from pre-training domain to\ndownstream tasks. Existing approaches merely leverage contrastive loss to learn\ninstance-level discrimination. However, lack of category information will lead\nto hard-positive problem that constrains the generalization ability of this\nkind of methods. We find that the multi-task process of meta learning can\nprovide a solution to this problem. In this paper, we propose a\nMeta-Contrastive Network (MCN), which combines the contrastive learning and\nmeta learning, to enhance the learning ability of existing self-supervised\napproaches. Our method contains two training stages based on model-agnostic\nmeta learning (MAML), each of which consists of a contrastive branch and a meta\nbranch. Extensive evaluations demonstrate the effectiveness of our method. For\ntwo downstream tasks, i.e., video action recognition and video retrieval, MCN\noutperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be\nmore specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8%\nand 54.5% for video action recognition, as well as 52.5% and 23.7% for video\nretrieval.",
          "link": "http://arxiv.org/abs/2108.08426",
          "publishedOn": "2021-08-20T01:53:51.703Z",
          "wordCount": 618,
          "title": "Self-Supervised Video Representation Learning with Meta-Contrastive Network. (arXiv:2108.08426v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>",
          "description": "With the popularity of blockchain technology, the financial security issues\nof blockchain transaction networks have become increasingly serious. Phishing\nscam detection methods will protect possible victims and build a healthier\nblockchain ecosystem. Usually, the existing works define phishing scam\ndetection as a node classification task by learning the potential features of\nusers through graph embedding methods such as random walk or graph neural\nnetwork (GNN). However, these detection methods are suffered from high\ncomplexity due to the large scale of the blockchain transaction network,\nignoring temporal information of the transaction. Addressing this problem, we\ndefined the transaction pattern graphs for users and transformed the phishing\nscam detection into a graph classification task. To extract richer information\nfrom the input graph, we proposed a multi-channel graph classification model\n(MCGC) with multiple feature extraction channels for GNN. The transaction\npattern graphs and MCGC are more able to detect potential phishing scammers by\nextracting the transaction pattern features of the target users. Extensive\nexperiments on seven benchmark and Ethereum datasets demonstrate that the\nproposed MCGC can not only achieve state-of-the-art performance in the graph\nclassification task but also achieve effective phishing scam detection based on\nthe target users' transaction pattern graphs.",
          "link": "http://arxiv.org/abs/2108.08456",
          "publishedOn": "2021-08-20T01:53:51.697Z",
          "wordCount": 624,
          "title": "Blockchain Phishing Scam Detection via Multi-channel Graph Classification. (arXiv:2108.08456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Site Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Pengyi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1\">Jane You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "The unsupervised domain adaptation (UDA) has been widely adopted to alleviate\nthe data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is adapted for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vector that violates the poset constraints by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.",
          "link": "http://arxiv.org/abs/2107.13467",
          "publishedOn": "2021-08-19T01:35:03.309Z",
          "wordCount": 651,
          "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation. (arXiv:2107.13467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07958",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuksel_O/0/1/0/all/0/1\">Oguz Kaan Yuksel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1\">Tatjana Chavdarova</a>",
          "description": "Data augmentation is a widely adopted technique for avoiding overfitting when\ntraining deep neural networks. However, this approach requires domain-specific\nknowledge and is often limited to a fixed set of hard-coded transformations.\nRecently, several works proposed to use generative models for generating\nsemantically meaningful perturbations to train a classifier. However, because\naccurate encoding and decoding are critical, these methods, which use\narchitectures that approximate the latent-variable inference, remained limited\nto pilot studies on small datasets.\n\nExploiting the exactly reversible encoder-decoder structure of normalizing\nflows, we perform on-manifold perturbations in the latent space to define fully\nunsupervised data augmentations. We demonstrate that such perturbations match\nthe performance of advanced data augmentation techniques -- reaching 96.6% test\naccuracy for CIFAR-10 using ResNet-18 and outperform existing methods,\nparticularly in low data regimes -- yielding 10--25% relative improvement of\ntest accuracy from classical training. We find that our latent adversarial\nperturbations adaptive to the classifier throughout its training are most\neffective, yielding the first test accuracy improvement results on real-world\ndatasets -- CIFAR-10/100 -- via latent-space perturbations.",
          "link": "http://arxiv.org/abs/2108.07958",
          "publishedOn": "2021-08-19T01:35:03.169Z",
          "wordCount": 620,
          "title": "Semantic Perturbations with Normalizing Flows for Improved Generalization. (arXiv:2108.07958v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yizeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "In this paper, we explore the spatial redundancy in video recognition with\nthe aim to improve the computational efficiency. It is observed that the most\ninformative region in each frame of a video is usually a small image patch,\nwhich shifts smoothly across frames. Therefore, we model the patch localization\nproblem as a sequential decision task, and propose a reinforcement learning\nbased approach for efficient spatially adaptive video recognition (AdaFocus).\nIn specific, a light-weighted ConvNet is first adopted to quickly process the\nfull video sequence, whose features are used by a recurrent policy network to\nlocalize the most task-relevant regions. Then the selected patches are inferred\nby a high-capacity network for the final prediction. During offline inference,\nonce the informative patch sequence has been generated, the bulk of computation\ncan be done in parallel, and is efficient on modern GPU devices. In addition,\nwe demonstrate that the proposed method can be easily extended by further\nconsidering the temporal redundancy, e.g., dynamically skipping less valuable\nframes. Extensive experiments on five benchmark datasets, i.e., ActivityNet,\nFCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is\nsignificantly more efficient than the competitive baselines. Code is available\nat https://github.com/blackfeather-wang/AdaFocus.",
          "link": "http://arxiv.org/abs/2105.03245",
          "publishedOn": "2021-08-19T01:35:03.162Z",
          "wordCount": 675,
          "title": "Adaptive Focus for Efficient Video Recognition. (arXiv:2105.03245v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1\">Saber Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1\">Alexander Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1\">Anton V. Proskurnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1\">Francesco Bullo</a>",
          "description": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of\nimplicit-depth learning models where function evaluation is performed by\nsolving a fixed point equation. They generalize classic feedforward models and\nare equivalent to infinite-depth weight-tied feedforward networks. While\nimplicit models show improved accuracy and significant reduction in memory\nconsumption, they can suffer from ill-posedness and convergence instability.\n\nThis paper provides a new framework to design well-posed and robust implicit\nneural networks based upon contraction theory for the non-Euclidean norm\n$\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness\nbased on one-sided Lipschitz constants, (ii) an average iteration for computing\nfixed-points, and (iii) explicit estimates on input-output Lipschitz constants.\nAdditionally, we design a training problem with the well-posedness condition\nand the average iteration as constraints and, to achieve robust models, with\nthe input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$\nwell-posedness condition leads to a larger polytopic training search space than\nexisting conditions and our average iteration enjoys accelerated convergence.\nFinally, we perform several numerical experiments for function estimation and\ndigit classification through the MNIST data set. Our numerical results\ndemonstrate improved accuracy and robustness of the implicit models with\nsmaller input-output Lipschitz bounds.",
          "link": "http://arxiv.org/abs/2106.03194",
          "publishedOn": "2021-08-19T01:35:03.115Z",
          "wordCount": 678,
          "title": "Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaodian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanhang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuihai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>",
          "description": "In recent years, federated learning (FL) has been widely applied for\nsupporting decentralized collaborative learning scenarios. Among existing FL\nmodels, federated logistic regression (FLR) is a widely used statistic model\nand has been used in various industries. To ensure data security and user\nprivacy, FLR leverages homomorphic encryption (HE) to protect the exchanged\ndata among different collaborative parties. However, HE introduces significant\ncomputational overhead (i.e., the cost of data encryption/decryption and\ncalculation over encrypted data), which eventually becomes the performance\nbottleneck of the whole system. In this paper, we propose HAFLO, a GPU-based\nsolution to improve the performance of FLR. The core idea of HAFLO is to\nsummarize a set of performance-critical homomorphic operators (HO) used by FLR\nand accelerate the execution of these operators through a joint optimization of\nstorage, IO, and computation. The preliminary results show that our\nacceleration on FATE, a popular FL framework, achieves a 49.9$\\times$ speedup\nfor heterogeneous LR and 88.4$\\times$ for homogeneous LR.",
          "link": "http://arxiv.org/abs/2107.13797",
          "publishedOn": "2021-08-19T01:35:02.928Z",
          "wordCount": 606,
          "title": "HAFLO: GPU-Based Acceleration for Federated Logistic Regression. (arXiv:2107.13797v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunrui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camporese_G/0/1/0/all/0/1\">Guglielmo Camporese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>",
          "description": "In open set recognition, a classifier has to detect unknown classes that are\nnot known at training time. In order to recognize new categories, the\nclassifier has to project the input samples of known classes in very compact\nand separated regions of the features space for discriminating samples of\nunknown classes. Recently proposed Capsule Networks have shown to outperform\nalternatives in many fields, particularly in image recognition, however they\nhave not been fully applied yet to open-set recognition. In capsule networks,\nscalar neurons are replaced by capsule vectors or matrices, whose entries\nrepresent different properties of objects. In our proposal, during training,\ncapsules features of the same known class are encouraged to match a pre-defined\ngaussian, one for each class. To this end, we use the variational autoencoder\nframework, with a set of gaussian priors as the approximation for the posterior\ndistribution. In this way, we are able to control the compactness of the\nfeatures of the same class around the center of the gaussians, thus controlling\nthe ability of the classifier in detecting samples from unknown classes. We\nconducted several experiments and ablation of our model, obtaining state of the\nart results on different datasets in the open set recognition and unknown\ndetection tasks.",
          "link": "http://arxiv.org/abs/2104.09159",
          "publishedOn": "2021-08-19T01:35:02.807Z",
          "wordCount": 683,
          "title": "Conditional Variational Capsule Network for Open Set Recognition. (arXiv:2104.09159v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shota Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Learning from positive and unlabeled (PU) data is an important problem in\nvarious applications. Most of the recent approaches for PU classification\nassume that the class-prior (the ratio of positive samples) in the training\nunlabeled dataset is identical to that of the test data, which does not hold in\nmany practical cases. In addition, we usually do not know the class-priors of\nthe training and test data, thus we have no clue on how to train a classifier\nwithout them. To address these problems, we propose a novel PU classification\nmethod based on density ratio estimation. A notable advantage of our proposed\nmethod is that it does not require the class-priors in the training phase;\nclass-prior shift is incorporated only in the test phase. We theoretically\njustify our proposed method and experimentally demonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.05045",
          "publishedOn": "2021-08-19T01:35:02.609Z",
          "wordCount": 593,
          "title": "Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>",
          "description": "Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.",
          "link": "http://arxiv.org/abs/2105.15176",
          "publishedOn": "2021-08-19T01:35:02.518Z",
          "wordCount": 668,
          "title": "Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models can synthesize photorealistic images of human faces\nwith novel identities. However, a key challenge to the wide applicability of\nsuch techniques is to provide independent control over semantically meaningful\nparameters: appearance, head pose, face shape, and facial expressions. In this\npaper, we propose VariTex - to the best of our knowledge the first method that\nlearns a variational latent feature space of neural face textures, which allows\nsampling of novel identities. We combine this generative model with a\nparametric face model and gain explicit control over head pose and facial\nexpressions. To generate complete images of human heads, we propose an additive\ndecoder that adds plausible details such as hair. A novel training scheme\nenforces a pose-independent latent space and in consequence, allows learning a\none-to-many mapping between latent codes and pose-conditioned exterior regions.\nThe resulting method can generate geometrically consistent images of novel\nidentities under fine-grained control over head pose, face shape, and facial\nexpressions. This facilitates a broad range of downstream tasks, like sampling\nnovel identities, changing the head pose, expression transfer, and more. Code\nand models are available for research on https://mcbuehler.github.io/VariTex.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-08-19T01:35:02.499Z",
          "wordCount": 694,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13721",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Saparbayeva_B/0/1/0/all/0/1\">Bayan Saparbayeva</a>",
          "description": "Manifold-valued functional data analysis (FDA) recently becomes an active\narea of research motivated by the raising availability of trajectories or\nlongitudinal data observed on non-linear manifolds. The challenges of analyzing\nsuch data come from many aspects, including infinite dimensionality and\nnonlinearity, as well as time-domain or phase variability. In this paper, we\nstudy the amplitude part of manifold-valued functions on $\\mathbb{S}^2$, which\nis invariant to random time warping or re-parameterization. Utilizing the nice\ngeometry of $\\mathbb{S}^2$, we develop a set of efficient and accurate tools\nfor temporal alignment of functions, geodesic computing, and sample mean\ncalculation. At the heart of these tools, they rely on gradient descent\nalgorithms with carefully derived gradients. We show the advantages of these\nnewly developed tools over its competitors with extensive simulations and real\ndata and demonstrate the importance of considering the amplitude part of\nfunctions instead of mixing it with phase variability in manifold-valued FDA.",
          "link": "http://arxiv.org/abs/2107.13721",
          "publishedOn": "2021-08-19T01:35:02.460Z",
          "wordCount": 613,
          "title": "Amplitude Mean of Functional Data on $\\mathbb{S}^2$. (arXiv:2107.13721v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adikari_T/0/1/0/all/0/1\">Tharindu B. Adikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draper_S/0/1/0/all/0/1\">Stark C. Draper</a>",
          "description": "An increasing bottleneck in decentralized optimization is communication.\nBigger models and growing datasets mean that decentralization of computation is\nimportant and that the amount of information exchanged is quickly growing.\nWhile compression techniques have been introduced to cope with the latter, none\nhas considered leveraging the temporal correlations that exist in consecutive\nvector updates. An important example is distributed momentum-SGD where temporal\ncorrelation is enhanced by the low-pass-filtering effect of applying momentum.\nIn this paper we design and analyze compression methods that exploit temporal\ncorrelation in systems both with and without error-feedback. Experiments with\nthe ImageNet dataset demonstrate that our proposed methods offer significant\nreduction in the rate of communication at only a negligible increase in\ncomputation complexity. We further analyze the convergence of SGD when\ncompression is applied with error-feedback. In the literature, convergence\nguarantees are developed only for compressors that provide error-bounds\npoint-wise, i.e., for each input to the compressor. In contrast, many important\ncodes (e.g. rate-distortion codes) provide error-bounds only in expectation and\nthus provide a more general guarantee. In this paper we prove the convergence\nof SGD under an expected error assumption by establishing a bound for the\nminimum gradient norm.",
          "link": "http://arxiv.org/abs/2108.07827",
          "publishedOn": "2021-08-19T01:35:02.434Z",
          "wordCount": 700,
          "title": "Compressing gradients by exploiting temporal correlation in momentum-SGD. (arXiv:2108.07827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04379",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1\">Ziming Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_S/0/1/0/all/0/1\">Sitian Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yan_Y/0/1/0/all/0/1\">Yuxuan Yan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1\">Tianyi Yang</a>",
          "description": "Principal component analysis (PCA) has achieved great success in unsupervised\nlearning by identifying covariance correlations among features. If the data\ncollection fails to capture the covariance information, PCA will not be able to\ndiscover meaningful modes. In particular, PCA will fail the spatial Gaussian\nProcess (GP) model in the undersampling regime, i.e. the averaged distance of\nneighboring anchor points (spatial features) is greater than the correlation\nlength of GP. Counterintuitively, by drawing the connection between PCA and\nSchr\\\"odinger equation, we can not only attack the undersampling challenge but\nalso compute in an efficient and decoupled way with the proposed algorithm\ncalled Schr\\\"odinger PCA. Our algorithm only requires variances of features and\nestimated correlation length as input, constructs the corresponding\nSchr\\\"odinger equation, and solves it to obtain the energy eigenstates, which\ncoincide with principal components. We will also establish the connection of\nour algorithm to the model reduction techniques in the partial differential\nequation (PDE) community, where the steady-state Schr\\\"odinger operator is\nidentified as a second-order approximation to the covariance function.\nNumerical experiments are implemented to testify the validity and efficiency of\nthe proposed algorithm, showing its potential for unsupervised learning tasks\non general graphs and manifolds.",
          "link": "http://arxiv.org/abs/2006.04379",
          "publishedOn": "2021-08-19T01:35:02.420Z",
          "wordCount": 672,
          "title": "Schr\\\"{o}dinger PCA: On the Duality between Principal Component Analysis and Schr\\\"{o}dinger Equation. (arXiv:2006.04379v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1\">Sekitoshi Kanai</a>",
          "description": "Generative adversarial networks built from deep convolutional neural networks\n(GANs) lack the ability to exactly replicate the high-frequency components of\nnatural images. To alleviate this issue, we introduce two novel training\ntechniques called frequency dropping (F-Drop) and frequency matching (F-Match).\nThe key idea of F-Drop is to filter out unnecessary high-frequency components\nfrom the input images of the discriminators. This simple modification prevents\nthe discriminators from being confused by perturbations of the high-frequency\ncomponents. In addition, F-Drop makes the GANs focus on fitting in the\nlow-frequency domain, in which there are the dominant components of natural\nimages. F-Match minimizes the difference between real and fake images in the\nfrequency domain for generating more realistic images. F-Match is implemented\nas a regularization term in the objective functions of the generators; it\npenalizes the batch mean error in the frequency domain. F-Match helps the\ngenerators to fit in the high-frequency domain filtered out by F-Drop to the\nreal image. We experimentally demonstrate that the combination of F-Drop and\nF-Match improves the generative performance of GANs in both the frequency and\nspatial domain on multiple image benchmarks.",
          "link": "http://arxiv.org/abs/2106.02343",
          "publishedOn": "2021-08-19T01:35:02.331Z",
          "wordCount": 666,
          "title": "F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08143",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tamkanat-E-Ali/0/1/0/all/0/1\">Tamkanat-E-Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Asad Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>",
          "description": "SARS-CoV-2, like any other virus, continues to mutate as it spreads,\naccording to an evolutionary process. Unlike any other virus, the number of\ncurrently available sequences of SARS-CoV-2 in public databases such as GISAID\nis already several million. This amount of data has the potential to uncover\nthe evolutionary dynamics of a virus like never before. However, a million is\nalready several orders of magnitude beyond what can be processed by the\ntraditional methods designed to reconstruct a virus's evolutionary history,\nsuch as those that build a phylogenetic tree. Hence, new and scalable methods\nwill need to be devised in order to make use of the ever increasing number of\nviral sequences being collected.\n\nSince identifying variants is an important part of understanding the\nevolution of a virus, in this paper, we propose an approach based on clustering\nsequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer\nbased feature vector generation and efficient feature selection methods, our\napproach is effective in identifying variants, as well as being efficient and\nscalable to millions of sequences. Such a clustering method allows us to show\nthe relative proportion of each variant over time, giving the rate of spread of\neach variant in different locations -- something which is important for vaccine\ndevelopment and distribution. We also compute the importance of each amino acid\nposition of the spike protein in identifying a given variant in terms of\ninformation gain. Positions of high variant-specific importance tend to agree\nwith those reported by the USA's Centers for Disease Control and Prevention\n(CDC), further demonstrating our approach.",
          "link": "http://arxiv.org/abs/2108.08143",
          "publishedOn": "2021-08-19T01:35:02.269Z",
          "wordCount": 743,
          "title": "Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v1 [q-bio.PE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1\">Benjamin Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyu &quot;Sherwin&quot; Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Jeremy Straub</a>",
          "description": "Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.",
          "link": "http://arxiv.org/abs/2108.08264",
          "publishedOn": "2021-08-19T01:35:02.246Z",
          "wordCount": 604,
          "title": "Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hindy_H/0/1/0/all/0/1\">Hanan Hindy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachtatzis_C/0/1/0/all/0/1\">Christos Tachtatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_R/0/1/0/all/0/1\">Robert Atkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brosset_D/0/1/0/all/0/1\">David Brosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bures_M/0/1/0/all/0/1\">Miroslav Bures</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonovic_I/0/1/0/all/0/1\">Ivan Andonovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michie_C/0/1/0/all/0/1\">Craig Michie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellekens_X/0/1/0/all/0/1\">Xavier Bellekens</a>",
          "description": "The use of supervised Machine Learning (ML) to enhance Intrusion Detection\nSystems has been the subject of significant research. Supervised ML is based\nupon learning by example, demanding significant volumes of representative\ninstances for effective training and the need to re-train the model for every\nunseen cyber-attack class. However, retraining the models in-situ renders the\nnetwork susceptible to attacks owing to the time-window required to acquire a\nsufficient volume of data. Although anomaly detection systems provide a\ncoarse-grained defence against unseen attacks, these approaches are\nsignificantly less accurate and suffer from high false-positive rates. Here, a\ncomplementary approach referred to as 'One-Shot Learning', whereby a limited\nnumber of examples of a new attack-class is used to identify a new attack-class\n(out of many) is detailed. The model grants a new cyber-attack classification\nwithout retraining. A Siamese Network is trained to differentiate between\nclasses based on pairs similarities, rather than features, allowing to identify\nnew and previously unseen attacks. The performance of a pre-trained model to\nclassify attack-classes based only on one example is evaluated using three\ndatasets. Results confirm the adaptability of the model in classifying unseen\nattacks and the trade-off between performance and the need for distinctive\nclass representation.",
          "link": "http://arxiv.org/abs/2006.15343",
          "publishedOn": "2021-08-19T01:35:02.152Z",
          "wordCount": 681,
          "title": "Leveraging Siamese Networks for One-Shot Intrusion Detection Model. (arXiv:2006.15343v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>",
          "description": "A multilayer perceptron (MLP) is typically made of multiple fully connected\nlayers with nonlinear activation functions. There have been several approaches\nto make them better (e.g. faster convergence, better convergence limit, etc.).\nBut the researches lack in more structured ways to test them. We test different\nMLP architectures by carrying out the experiments on the age and gender\ndatasets. We empirically show that by whitening inputs before every linear\nlayer and adding skip connections, our proposed MLP architecture can result in\nbetter performance. Since the whitening process includes dropouts, it can also\nbe used to approximate Bayesian inference. We have open sourced our code\nreleased models and docker images at https://github.com/tae898/age-gender/.",
          "link": "http://arxiv.org/abs/2108.08186",
          "publishedOn": "2021-08-19T01:35:02.040Z",
          "wordCount": 544,
          "title": "Generalizing MLPs With Dropouts, Batch Normalization, and Skip Connections. (arXiv:2108.08186v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1\">Olivier Bousquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1\">Mark Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efremenko_K/0/1/0/all/0/1\">Klim Efremenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kol_G/0/1/0/all/0/1\">Gillat Kol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "Hypothesis Selection is a fundamental distribution learning problem where\ngiven a comparator-class $Q=\\{q_1,\\ldots, q_n\\}$ of distributions, and a\nsampling access to an unknown target distribution $p$, the goal is to output a\ndistribution $q$ such that $\\mathsf{TV}(p,q)$ is close to $opt$, where $opt =\n\\min_i\\{\\mathsf{TV}(p,q_i)\\}$ and $\\mathsf{TV}(\\cdot, \\cdot)$ denotes the\ntotal-variation distance. Despite the fact that this problem has been studied\nsince the 19th century, its complexity in terms of basic resources, such as\nnumber of samples and approximation guarantees, remains unsettled (this is\ndiscussed, e.g., in the charming book by Devroye and Lugosi `00). This is in\nstark contrast with other (younger) learning settings, such as PAC learning,\nfor which these complexities are well understood.\n\nWe derive an optimal $2$-approximation learning strategy for the Hypothesis\nSelection problem, outputting $q$ such that $\\mathsf{TV}(p,q) \\leq2 \\cdot opt +\n\\eps$, with a (nearly) optimal sample complexity of~$\\tilde O(\\log\nn/\\epsilon^2)$. This is the first algorithm that simultaneously achieves the\nbest approximation factor and sample complexity: previously, Bousquet, Kane,\nand Moran (COLT `19) gave a learner achieving the optimal $2$-approximation,\nbut with an exponentially worse sample complexity of $\\tilde\nO(\\sqrt{n}/\\epsilon^{2.5})$, and Yatracos~(Annals of Statistics `85) gave a\nlearner with optimal sample complexity of $O(\\log n /\\epsilon^2)$ but with a\nsub-optimal approximation factor of $3$.",
          "link": "http://arxiv.org/abs/2108.07880",
          "publishedOn": "2021-08-19T01:35:02.033Z",
          "wordCount": 657,
          "title": "Statistically Near-Optimal Hypothesis Selection. (arXiv:2108.07880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10437",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sieun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eunho Lee</a>",
          "description": "Recently, there has been discussions on the ill-posed nature of\nsuper-resolution that multiple possible reconstructions exist for a given\nlow-resolution image. Using normalizing flows, SRflow[23] achieves\nstate-of-the-art perceptual quality by learning the distribution of the output\ninstead of a deterministic output to one estimate. In this paper, we adapt the\nconcepts of SRFlow to improve GAN-based super-resolution by properly\nimplementing the one-to-many property. We modify the generator to estimate a\ndistribution as a mapping from random noise. We improve the content loss that\nhampers the perceptual training objectives. We also propose additional training\ntechniques to further enhance the perceptual quality of generated images. Using\nour proposed methods, we were able to improve the performance of ESRGAN[1] in\nx4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual\nextreme SR by applying our methods to RFB-ESRGAN[21].",
          "link": "http://arxiv.org/abs/2106.10437",
          "publishedOn": "2021-08-19T01:35:01.930Z",
          "wordCount": 601,
          "title": "One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongmei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "Due to the over-parameterization nature, neural networks are a powerful tool\nfor nonlinear function approximation. In order to achieve good generalization\non unseen data, a suitable inductive bias is of great importance for neural\nnetworks. One of the most straightforward ways is to regularize the neural\nnetwork with some additional objectives. L2 regularization serves as a standard\nregularization for neural networks. Despite its popularity, it essentially\nregularizes one dimension of the individual neuron, which is not strong enough\nto control the capacity of highly over-parameterized neural networks. Motivated\nby this, hyperspherical uniformity is proposed as a novel family of relational\nregularizations that impact the interaction among neurons. We consider several\ngeometrically distinct ways to achieve hyperspherical uniformity. The\neffectiveness of hyperspherical uniformity is justified by theoretical insights\nand empirical evaluations.",
          "link": "http://arxiv.org/abs/2103.01649",
          "publishedOn": "2021-08-19T01:35:01.871Z",
          "wordCount": 598,
          "title": "Learning with Hyperspherical Uniformity. (arXiv:2103.01649v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altman_R/0/1/0/all/0/1\">Russ Altman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arx_S/0/1/0/all/0/1\">Sydney von Arx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael S. Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brynjolfsson_E/0/1/0/all/0/1\">Erik Brynjolfsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1\">Rodrigo Castellon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri Chatterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Annie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creel_K/0/1/0/all/0/1\">Kathleen Creel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jared Quincy Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1\">Dora Demszky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doumbouya_M/0/1/0/all/0/1\">Moussa Doumbouya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etchemendy_J/0/1/0/all/0/1\">John Etchemendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1\">Trevor Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1\">Lauren Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossman_S/0/1/0/all/0/1\">Shelby Grossman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenny Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1\">Kyle Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1\">Geoff Keeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khani_F/0/1/0/all/0/1\">Fereshte Khani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohd_P/0/1/0/all/0/1\">Pang Wei Kohd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1\">Rohith Kuditipudi</a>, et al. (62 additional authors not shown)",
          "description": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\nDALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\nwide range of downstream tasks. We call these models foundation models to\nunderscore their critically central yet incomplete character. This report\nprovides a thorough account of the opportunities and risks of foundation\nmodels, ranging from their capabilities (e.g., language, vision, robotics,\nreasoning, human interaction) and technical principles(e.g., model\narchitectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal\nimpact (e.g., inequity, misuse, economic and environmental impact, legal and\nethical considerations). Though foundation models are based on standard deep\nlearning and transfer learning, their scale results in new emergent\ncapabilities,and their effectiveness across so many tasks incentivizes\nhomogenization. Homogenization provides powerful leverage but demands caution,\nas the defects of the foundation model are inherited by all the adapted models\ndownstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and\nwhat they are even capable of due to their emergent properties. To tackle these\nquestions, we believe much of the critical research on foundation models will\nrequire deep interdisciplinary collaboration commensurate with their\nfundamentally sociotechnical nature.",
          "link": "http://arxiv.org/abs/2108.07258",
          "publishedOn": "2021-08-19T01:35:01.830Z",
          "wordCount": 937,
          "title": "On the Opportunities and Risks of Foundation Models. (arXiv:2108.07258v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>",
          "description": "Model inversion (MI) attacks are aimed at reconstructing training data from\nmodel parameters. Such attacks have triggered increasing concerns about\nprivacy, especially given a growing number of online model repositories.\nHowever, existing MI attacks against deep neural networks (DNNs) have large\nroom for performance improvement. We present a novel inversion-specific GAN\nthat can better distill knowledge useful for performing attacks on private\nmodels from public data. In particular, we train the discriminator to\ndifferentiate not only the real and fake samples but the soft-labels provided\nby the target model. Moreover, unlike previous work that directly searches for\na single data point to represent a target class, we propose to model a private\ndata distribution for each target class. Our experiments show that the\ncombination of these techniques can significantly boost the success rate of the\nstate-of-the-art MI attacks by 150%, and generalize better to a variety of\ndatasets and models. Our code is available at\nhttps://github.com/SCccc21/Knowledge-Enriched-DMI.",
          "link": "http://arxiv.org/abs/2010.04092",
          "publishedOn": "2021-08-19T01:35:01.813Z",
          "wordCount": 619,
          "title": "Improved Techniques for Model Inversion Attacks. (arXiv:2010.04092v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Solving geometric tasks involving point clouds by using machine learning is a\nchallenging problem. Standard feed-forward neural networks combine linear or,\nif the bias parameter is included, affine layers and activation functions.\nTheir geometric modeling is limited, which motivated the prior work introducing\nthe multilayer hypersphere perceptron (MLHP). Its constituent part, i.e., the\nhypersphere neuron, is obtained by applying a conformal embedding of Euclidean\nspace. By virtue of Clifford algebra, it can be implemented as the Cartesian\ndot product of inputs and weights. If the embedding is applied in a manner\nconsistent with the dimensionality of the input space geometry, the decision\nsurfaces of the model units become combinations of hyperspheres and make the\ndecision-making process geometrically interpretable for humans. Our extension\nof the MLHP model, the multilayer geometric perceptron (MLGP), and its\nrespective layer units, i.e., geometric neurons, are consistent with the 3D\ngeometry and provide a geometric handle of the learned coefficients. In\nparticular, the geometric neuron activations are isometric in 3D, which is\nnecessary for rotation and translation equivariance. When classifying the 3D\nTetris shapes, we quantitatively show that our model requires no activation\nfunction in the hidden layers other than the embedding to outperform the\nvanilla multilayer perceptron. In the presence of noise in the data, our model\nis also superior to the MLHP.",
          "link": "http://arxiv.org/abs/2006.06507",
          "publishedOn": "2021-08-19T01:35:01.794Z",
          "wordCount": 690,
          "title": "Embed Me If You Can: A Geometric Perceptron. (arXiv:2006.06507v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yimin Wen</a>",
          "description": "In many practical data mining scenarios, such as network intrusion detection,\nTwitter spam detection, and computer-aided diagnosis, a source domain that is\ndifferent from but related to a target domain is very common. In addition, a\nlarge amount of unlabeled data is available in both source and target domains,\nbut labeling each of them is difficult, expensive, time-consuming, and sometime\nunnecessary. Therefore, it is very important and worthwhile to fully explore\nthe labeled and unlabeled data in source and target domains to settle the task\nin target domain. In this paper, a new semi-supervised inductive transfer\nlearning framework, named \\emph{Co-Transfer} is proposed. Co-Transfer first\ngenerates three TrAdaBoost classifiers for transfer learning from the source\ndomain to the target domain, and meanwhile another three TrAdaBoost classifiers\nare generated for transfer learning from the target domain to the source\ndomain, using bootstraped samples from the original labeled data. In each round\nof co-transfer, each group of TrAdaBoost classifiers are refined using the\ncarefully labeled data. Finally, the group of TrAdaBoost classifiers learned to\ntransfer from the source domain to the target domain produce the final\nhypothesis. Experiments results illustrate Co-Transfer can effectively exploit\nand reuse the labeled and unlabeled data in source and target domains.",
          "link": "http://arxiv.org/abs/2108.07930",
          "publishedOn": "2021-08-19T01:35:01.778Z",
          "wordCount": 629,
          "title": "A new semi-supervised inductive transfer learning framework: Co-Transfer. (arXiv:2108.07930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1\">Binay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Anirban Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matha_H/0/1/0/all/0/1\">Harika Matha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_K/0/1/0/all/0/1\">Kunal Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsai_L/0/1/0/all/0/1\">Lalitdutt Parsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1\">Vijay Agneeswaran</a>",
          "description": "Reducing the number of failures in a production system is one of the most\nchallenging problems in technology driven industries, such as, the online\nretail industry. To address this challenge, change management has emerged as a\npromising sub-field in operations that manages and reviews the changes to be\ndeployed in production in a systematic manner. However, it is practically\nimpossible to manually review a large number of changes on a daily basis and\nassess the risk associated with them. This warrants the development of an\nautomated system to assess the risk associated with a large number of changes.\nThere are a few commercial solutions available to address this problem but\nthose solutions lack the ability to incorporate domain knowledge and continuous\nfeedback from domain experts into the risk assessment process. As part of this\nwork, we aim to bridge the gap between model-driven risk assessment of change\nrequests and the assessment of domain experts by building a continuous feedback\nloop into the risk assessment process. Here we present our work to build an\nend-to-end machine learning system along with the discussion of some of\npractical challenges we faced related to extreme skewness in class\ndistribution, concept drift, estimation of the uncertainty associated with the\nmodel's prediction and the overall scalability of the system.",
          "link": "http://arxiv.org/abs/2108.07951",
          "publishedOn": "2021-08-19T01:35:01.766Z",
          "wordCount": 661,
          "title": "Look Before You Leap! Designing a Human-Centered AI System for Change Risk Assessment. (arXiv:2108.07951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-08-19T01:35:01.748Z",
          "wordCount": 602,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Sen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weishen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "In this paper, we focus on effective learning over a collaborative research\nnetwork involving multiple clients. Each client has its own sample population\nwhich may not be shared with other clients due to privacy concerns. The goal is\nto learn a model for each client, which behaves better than the one learned\nfrom its own data, through secure collaborations with other clients in the\nnetwork. Due to the discrepancies of the sample distributions across different\nclients, it is not necessarily that collaborating with everyone will lead to\nthe best local models. We propose a learning to collaborate framework, where\neach client can choose to collaborate with certain members in the network to\nachieve a \"collaboration equilibrium\", where smaller collaboration coalitions\nare formed within the network so that each client can obtain the model with the\nbest utility. We propose the concept of benefit graph which describes how each\nclient can benefit from collaborating with other clients and develop a Pareto\noptimization approach to obtain it. Finally the collaboration coalitions can be\nderived from it based on graph operations. Our framework provides a new way of\nsetting up collaborations in a research network. Experiments on both synthetic\nand real world data sets are provided to demonstrate the effectiveness of our\nmethod.",
          "link": "http://arxiv.org/abs/2108.07926",
          "publishedOn": "2021-08-19T01:35:01.742Z",
          "wordCount": 634,
          "title": "Learning to Collaborate. (arXiv:2108.07926v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1\">Matthieu Bloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>",
          "description": "We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.",
          "link": "http://arxiv.org/abs/2106.09485",
          "publishedOn": "2021-08-19T01:35:01.728Z",
          "wordCount": 680,
          "title": "Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>",
          "description": "Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a standard\n20-second long microvascular video takes on average 20 minutes and requires\nextensive training. Several studies have reported that manual analysis hinders\nthe application of microvascular microscopy in a clinical setting. In this\npaper, we present a fully automated state-of-the-art system, called\nCapillaryNet, that can quantify skin nutritive capillary density and red blood\ncell velocity from handheld microscopy videos. Moreover, CapillaryNet measures\nseveral novel microvascular parameters that researchers were previously unable\nto quantify, i.e. capillary hematocrit and Intra-capillary flow velocity\nheterogeneity. Our system has been used to analyze skin microcirculation videos\nfrom various patient groups (COVID-19, pancreatitis, and acute heart diseases).\nOur proposed system excels from existing capillary detection systems as it\ncombines the speed of traditional computer vision algorithms and the accuracy\nof convolutional neural networks.",
          "link": "http://arxiv.org/abs/2104.11574",
          "publishedOn": "2021-08-19T01:35:01.721Z",
          "wordCount": 740,
          "title": "CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanfeng Liu</a>",
          "description": "Cross-Domain Recommendation (CDR) and Cross-System Recommendation (CSR) have\nbeen proposed to improve the recommendation accuracy in a target dataset\n(domain/system) with the help of a source one with relatively richer\ninformation. However, most existing CDR and CSR approaches are single-target,\nnamely, there is a single target dataset, which can only help the target\ndataset and thus cannot benefit the source dataset. In this paper, we focus on\nthree new scenarios, i.e., Dual-Target CDR (DTCDR), Multi-Target CDR (MTCDR),\nand CDR+CSR, and aim to improve the recommendation accuracy in all datasets\nsimultaneously for all scenarios. To do this, we propose a unified framework,\ncalled GA (based on Graph embedding and Attention techniques), for all three\nscenarios. In GA, we first construct separate heterogeneous graphs to generate\nmore representative user and item embeddings. Then, we propose an element-wise\nattention mechanism to effectively combine the embeddings of common entities\n(users/items) learned from different datasets. Moreover, to avoid negative\ntransfer, we further propose a Personalized training strategy to minimize the\nembedding difference of common entities between a richer dataset and a sparser\ndataset, deriving three new models, i.e., GA-DTCDR-P, GA-MTCDR-P, and\nGA-CDR+CSR-P, for the three scenarios respectively. Extensive experiments\nconducted on four real-world datasets demonstrate that our proposed GA models\nsignificantly outperform the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2108.07976",
          "publishedOn": "2021-08-19T01:35:01.684Z",
          "wordCount": 676,
          "title": "A Unified Framework for Cross-Domain and Cross-System Recommendations. (arXiv:2108.07976v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinsung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_M/0/1/0/all/0/1\">Minju Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seunghyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyoung Song</a>",
          "description": "Mobile digital billboards are an effective way to augment brand-awareness.\nAmong various such mobile billboards, taxicab rooftop devices are emerging in\nthe market as a brand new media. Motov is a leading company in South Korea in\nthe taxicab rooftop advertising market. In this work, we present a lightweight\nyet accurate deep learning-based method to predict taxicabs' next locations to\nbetter prepare for targeted advertising based on demographic information of\nlocations. Considering the fact that next POI recommendation datasets are\nfrequently sparse, we design our presented model based on neural ordinary\ndifferential equations (NODEs), which are known to be robust to\nsparse/incorrect input, with several enhancements. Our model, which we call\nLightMove, has a larger prediction accuracy, a smaller number of parameters,\nand/or a smaller training/inference time, when evaluating with various\ndatasets, in comparison with state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.04993",
          "publishedOn": "2021-08-19T01:35:01.647Z",
          "wordCount": 605,
          "title": "LightMove: A Lightweight Next-POI Recommendation for Taxicab Rooftop Advertising. (arXiv:2108.04993v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isaac_Medina_B/0/1/0/all/0/1\">Brian K. S. Isaac-Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poyser_M/0/1/0/all/0/1\">Matt Poyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1\">Daniel Organisciak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>",
          "description": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due\nto both negligent and malicious use. For this reason, the automated detection\nand tracking of UAV is a fundamental task in aerial security systems. Common\ntechnologies for UAV detection include visible-band and thermal infrared\nimaging, radio frequency and radar. Recent advances in deep neural networks\n(DNNs) for image-based object detection open the possibility to use visual\ninformation for this detection and tracking task. Furthermore, these detection\narchitectures can be implemented as backbones for visual tracking systems,\nthereby enabling persistent tracking of UAV incursions. To date, no\ncomprehensive performance benchmark exists that applies DNNs to visible-band\nimagery for UAV detection and tracking. To this end, three datasets with varied\nenvironmental conditions for UAV detection and tracking, comprising a total of\n241 videos (331,486 images), are assessed using four detection architectures\nand three tracking frameworks. The best performing detector architecture\nobtains an mAP of 98.6% and the best performing tracking framework obtains a\nMOTA of 96.3%. Cross-modality evaluation is carried out between visible and\ninfrared spectrums, achieving a maximal 82.8% mAP on visible images when\ntraining in the infrared modality. These results provide the first public\nmulti-approach benchmark for state-of-the-art deep learning-based methods and\ngive insight into which detection and tracking architectures are effective in\nthe UAV domain.",
          "link": "http://arxiv.org/abs/2103.13933",
          "publishedOn": "2021-08-19T01:35:01.628Z",
          "wordCount": 709,
          "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural Networks: A Performance Benchmark. (arXiv:2103.13933v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopper_M/0/1/0/all/0/1\">Marisa Klopper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warren_R/0/1/0/all/0/1\">Robin Warren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>",
          "description": "We present an experimental investigation into the effectiveness of transfer\nlearning and bottleneck feature extraction in detecting COVID-19 from audio\nrecordings of cough, breath and speech.\n\nThis type of screening is non-contact, does not require specialist medical\nexpertise or laboratory facilities and can be deployed on inexpensive consumer\nhardware.\n\nWe use datasets that contain recordings of coughing, sneezing, speech and\nother noises, but do not contain COVID-19 labels, to pre-train three deep\nneural networks: a CNN, an LSTM and a Resnet50.\n\nThese pre-trained networks are subsequently either fine-tuned using smaller\ndatasets of coughing with COVID-19 labels in the process of transfer learning,\nor are used as bottleneck feature extractors.\n\nResults show that a Resnet50 classifier trained by this transfer learning\nprocess delivers optimal or near-optimal performance across all datasets\nachieving areas under the receiver operating characteristic (ROC AUC) of 0.98,\n0.94 and 0.92 respectively for all three sound classes (coughs, breaths and\nspeech).\n\nThis indicates that coughs carry the strongest COVID-19 signature, followed\nby breath and speech.\n\nOur results also show that applying transfer learning and extracting\nbottleneck features using the larger datasets without COVID-19 labels led not\nonly to improve performance, but also to minimise the standard deviation of the\nclassifier AUCs among the outer folds of the leave-$p$-out cross-validation,\nindicating better generalisation.\n\nWe conclude that deep transfer learning and bottleneck feature extraction can\nimprove COVID-19 cough, breath and speech audio classification, yielding\nautomatic classifiers with higher accuracy.",
          "link": "http://arxiv.org/abs/2104.02477",
          "publishedOn": "2021-08-19T01:35:01.620Z",
          "wordCount": 778,
          "title": "COVID-19 Detection in Cough, Breath and Speech using Deep Transfer Learning and Bottleneck Features. (arXiv:2104.02477v4 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>",
          "description": "Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.",
          "link": "http://arxiv.org/abs/2105.03761",
          "publishedOn": "2021-08-19T01:35:01.609Z",
          "wordCount": 681,
          "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhongxi Zheng</a>",
          "description": "Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make some corrections corresponding\nto potentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present\none-step abductive multi-target learning (OSAMTL) that imposes a one-step\nlogical reasoning upon machine learning via a multi-target learning procedure\nto constrain the predictions of the learning model to be subject to our prior\nknowledge about the true target. For the problem 2), we propose a logical\nassessment formula (LAF) that evaluates the logical rationality of the outputs\nof an approach by estimating the consistencies between the predictions of the\nlearning model and the logical facts narrated from the results of the one-step\nlogical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori\n(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable\nthe machine learning model achieving logically more rational predictions, which\nis beyond various state-of-the-art approaches in handling complex noisy labels.",
          "link": "http://arxiv.org/abs/2011.14956",
          "publishedOn": "2021-08-19T01:35:01.593Z",
          "wordCount": 767,
          "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>",
          "description": "Machine learning has the potential to fuel further advances in data science,\nbut it is greatly hindered by an ad hoc design process, poor data hygiene, and\na lack of statistical rigor in model evaluation. Recently, these issues have\nbegun to attract more attention as they have caused public and embarrassing\nissues in research and development. Drawing from our experience as machine\nlearning researchers, we follow the machine learning process from algorithm\ndesign to data collection to model evaluation, drawing attention to common\npitfalls and providing practical recommendations for improvements. At each\nstep, case studies are introduced to highlight how these pitfalls occur in\npractice, and where things could be improved.",
          "link": "http://arxiv.org/abs/2011.02832",
          "publishedOn": "2021-08-19T01:35:01.572Z",
          "wordCount": 589,
          "title": "Pitfalls in Machine Learning Research: Reexamining the Development Cycle. (arXiv:2011.02832v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03408",
          "author": "<a href=\"http://arxiv.org/find/hep-ex/1/au:+Hong_T/0/1/0/all/0/1\">Tae Min Hong</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Carlson_B/0/1/0/all/0/1\">Benjamin Carlson</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Eubanks_B/0/1/0/all/0/1\">Brandon Eubanks</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Racz_S/0/1/0/all/0/1\">Stephen Racz</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Roche_S/0/1/0/all/0/1\">Stephen Roche</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Stelzer_J/0/1/0/all/0/1\">Joerg Stelzer</a>, <a href=\"http://arxiv.org/find/hep-ex/1/au:+Stumpp_D/0/1/0/all/0/1\">Daniel Stumpp</a>",
          "description": "We present a novel implementation of classification using the machine\nlearning / artificial intelligence method called boosted decision trees (BDT)\non field programmable gate arrays (FPGA). The firmware implementation of binary\nclassification requiring 100 training trees with a maximum depth of 4 using\nfour input variables gives a latency value of about 10 ns, independent of the\nclock speed from 100 to 320 MHz in our setup. The low timing values are\nachieved by restructuring the BDT layout and reconfiguring its parameters. The\nFPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our\nsetup. A software package called fwXmachina achieves this implementation. Our\nintended user is an expert of custom electronics-based trigger systems in high\nenergy physics experiments or anyone that needs decisions at the lowest latency\nvalues for real-time event classification. Two problems from high energy\nphysics are considered, in the separation of electrons vs. photons and in the\nselection of vector boson fusion-produced Higgs bosons vs. the rejection of the\nmultijet processes.",
          "link": "http://arxiv.org/abs/2104.03408",
          "publishedOn": "2021-08-19T01:35:01.564Z",
          "wordCount": 677,
          "title": "Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics. (arXiv:2104.03408v3 [hep-ex] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajihassani_O/0/1/0/all/0/1\">Omid Hajihassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardakanian_O/0/1/0/all/0/1\">Omid Ardakanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khazaei_H/0/1/0/all/0/1\">Hamzeh Khazaei</a>",
          "description": "The abundance of data collected by sensors in Internet of Things (IoT)\ndevices, and the success of deep neural networks in uncovering hidden patterns\nin time series data have led to mounting privacy concerns. This is because\nprivate and sensitive information can be potentially learned from sensor data\nby applications that have access to this data. In this paper, we aim to examine\nthe tradeoff between utility and privacy loss by learning low-dimensional\nrepresentations that are useful for data obfuscation. We propose deterministic\nand probabilistic transformations in the latent space of a variational\nautoencoder to synthesize time series data such that intrusive inferences are\nprevented while desired inferences can still be made with sufficient accuracy.\nIn the deterministic case, we use a linear transformation to move the\nrepresentation of input data in the latent space such that the reconstructed\ndata is likely to have the same public attribute but a different private\nattribute than the original input data. In the probabilistic case, we apply the\nlinear transformation to the latent representation of input data with some\nprobability. We compare our technique with autoencoder-based anonymization\ntechniques and additionally show that it can anonymize data in real time on\nresource-constrained edge devices.",
          "link": "http://arxiv.org/abs/2011.08315",
          "publishedOn": "2021-08-19T01:35:01.539Z",
          "wordCount": 669,
          "title": "Privacy-preserving Data Analysis through Representation Learning and Transformation. (arXiv:2011.08315v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yikun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Manan Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos Theodorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1\">Naira Hovakimyan</a>",
          "description": "A reinforcement learning (RL) policy trained in a nominal environment could\nfail in a new/perturbed environment due to the existence of dynamic variations.\nExisting robust methods try to obtain a fixed policy for all envisioned dynamic\nvariation scenarios through robust or adversarial training. These methods could\nlead to conservative performance due to emphasis on the worst case, and often\ninvolve tedious modifications to the training environment. We propose an\napproach to robustifying a pre-trained non-robust RL policy with\n$\\mathcal{L}_1$ adaptive control. Leveraging the capability of an\n$\\mathcal{L}_1$ control law in the fast estimation of and active compensation\nfor dynamic variations, our approach can significantly improve the robustness\nof an RL policy trained in a standard (i.e., non-robust) way, either in a\nsimulator or in the real world. Numerical experiments are provided to validate\nthe efficacy of the proposed approach.",
          "link": "http://arxiv.org/abs/2106.02249",
          "publishedOn": "2021-08-19T01:35:01.527Z",
          "wordCount": 630,
          "title": "Robustifying Reinforcement Learning Policies with $\\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bailey_A/0/1/0/all/0/1\">Andrew Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1\">Mark D. Plumbley</a>",
          "description": "Depression is a large-scale mental health problem and a challenging area for\nmachine learning researchers in detection of depression. Datasets such as\nDistress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created\nto aid research in this area. However, on top of the challenges inherent in\naccurately detecting depression, biases in datasets may result in skewed\nclassification performance. In this paper we examine gender bias in the\nDAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an\noverreporting of performance. By different concepts from Fair Machine Learning,\nsuch as data re-distribution, and using raw audio features, we can mitigate\nagainst the harmful effects of bias.",
          "link": "http://arxiv.org/abs/2010.15120",
          "publishedOn": "2021-08-19T01:35:01.507Z",
          "wordCount": 597,
          "title": "Gender Bias in Depression Detection Using Audio Features. (arXiv:2010.15120v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_N/0/1/0/all/0/1\">Nikhila Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>",
          "description": "We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code are available at https://worldsheet.github.io.",
          "link": "http://arxiv.org/abs/2012.09854",
          "publishedOn": "2021-08-19T01:35:01.500Z",
          "wordCount": 668,
          "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a Single Image. (arXiv:2012.09854v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Sheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Ju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junshan Zhang</a>",
          "description": "In order to meet the requirements for performance, safety, and latency in\nmany IoT applications, intelligent decisions must be made right here right now\nat the network edge. However, the constrained resources and limited local data\namount pose significant challenges to the development of edge AI. To overcome\nthese challenges, we explore continual edge learning capable of leveraging the\nknowledge transfer from previous tasks. Aiming to achieve fast and continual\nedge learning, we propose a platform-aided federated meta-learning architecture\nwhere edge nodes collaboratively learn a meta-model, aided by the knowledge\ntransfer from prior tasks. The edge learning problem is cast as a regularized\noptimization problem, where the valuable knowledge learned from previous tasks\nis extracted as regularization. Then, we devise an ADMM based federated\nmeta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural\nmechanism to decompose the original problem into many subproblems which can be\nsolved in parallel across edge nodes and the platform. Further, a variant of\ninexact-ADMM method is employed where the subproblems are `solved' via linear\napproximation as well as Hessian estimation to reduce the computational cost\nper round to $\\mathcal{O}(n)$. We provide a comprehensive analysis of\nADMM-FedMeta, in terms of the convergence properties, the rapid adaptation\nperformance, and the forgetting effect of prior knowledge transfer, for the\ngeneral non-convex case. Extensive experimental studies demonstrate the\neffectiveness and efficiency of ADMM-FedMeta, and showcase that it\nsubstantially outperforms the existing baselines.",
          "link": "http://arxiv.org/abs/2012.08677",
          "publishedOn": "2021-08-19T01:35:01.493Z",
          "wordCount": 722,
          "title": "Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rempe_D/0/1/0/all/0/1\">Davis Rempe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1\">Aaron Hertzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>",
          "description": "We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal\npose and shape. Though substantial progress has been made in estimating 3D\nhuman motion and shape from dynamic observations, recovering plausible pose\nsequences in the presence of noise and occlusions remains a challenge. For this\npurpose, we propose an expressive generative model in the form of a conditional\nvariational autoencoder, which learns a distribution of the change in pose at\neach step of a motion sequence. Furthermore, we introduce a flexible\noptimization-based approach that leverages HuMoR as a motion prior to robustly\nestimate plausible pose and shape from ambiguous observations. Through\nextensive evaluations, we demonstrate that our model generalizes to diverse\nmotions and body shapes after training on a large motion capture dataset, and\nenables motion reconstruction from multiple input modalities including 3D\nkeypoints and RGB(-D) videos.",
          "link": "http://arxiv.org/abs/2105.04668",
          "publishedOn": "2021-08-19T01:35:01.464Z",
          "wordCount": 623,
          "title": "HuMoR: 3D Human Motion Model for Robust Pose Estimation. (arXiv:2105.04668v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiorini_S/0/1/0/all/0/1\">Stefano Fiorini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciavotta_M/0/1/0/all/0/1\">Michele Ciavotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurino_A/0/1/0/all/0/1\">Andrea Maurino</a>",
          "description": "In recent years, studying and predicting alternative mobility (e.g., sharing\nservices) patterns in urban environments has become increasingly important as\naccurate and timely information on current and future vehicle flows can\nsuccessfully increase the quality and availability of transportation services.\nThis need is aggravated during the current pandemic crisis, which pushes\npolicymakers and private citizens to seek social-distancing compliant urban\nmobility services, such as electric bikes and scooter sharing offerings.\nHowever, predicting the number of incoming and outgoing vehicles for different\ncity areas is challenging due to the nonlinear spatial and temporal\ndependencies typical of urban mobility patterns. In this work, we propose\nSTREED-Net, a novel deep learning network with a multi-attention (spatial and\ntemporal) mechanism that effectively captures and exploits complex spatial and\ntemporal patterns in mobility data. The results of a thorough experimental\nanalysis using real-life data are reported, indicating that the proposed model\nimproves the state-of-the-art for this task.",
          "link": "http://arxiv.org/abs/2103.00983",
          "publishedOn": "2021-08-19T01:35:01.444Z",
          "wordCount": 640,
          "title": "Listening to the city, attentively: A Spatio-Temporal Attention Boosted Autoencoder for the Short-Term Flow Prediction Problem. (arXiv:2103.00983v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Prarthana Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1\">Krzysztof Czarnecki</a>",
          "description": "Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.",
          "link": "http://arxiv.org/abs/2101.02672",
          "publishedOn": "2021-08-19T01:35:01.437Z",
          "wordCount": 694,
          "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection. (arXiv:2101.02672v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Arghyadip Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "In the regret-based formulation of Multi-armed Bandit (MAB) problems, except\nin rare instances, much of the literature focuses on arms with i.i.d. rewards.\nIn this paper, we consider the problem of obtaining regret guarantees for MAB\nproblems in which the rewards of each arm form a Markov chain which may not\nbelong to a single parameter exponential family. To achieve logarithmic regret\nin such problems is not difficult: a variation of standard Kullback-Leibler\nUpper Confidence Bound (KL-UCB) does the job. However, the constants obtained\nfrom such an analysis are poor for the following reason: i.i.d. rewards are a\nspecial case of Markov rewards and it is difficult to design an algorithm that\nworks well independent of whether the underlying model is truly Markovian or\ni.i.d. To overcome this issue, we introduce a novel algorithm that identifies\nwhether the rewards from each arm are truly Markovian or i.i.d. using a total\nvariation distance-based test. Our algorithm then switches from using a\nstandard KL-UCB to a specialized version of KL-UCB when it determines that the\narm reward is Markovian, thus resulting in low regret for both i.i.d. and\nMarkovian settings.",
          "link": "http://arxiv.org/abs/2009.06606",
          "publishedOn": "2021-08-19T01:35:01.429Z",
          "wordCount": 663,
          "title": "Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Regenwetter_L/0/1/0/all/0/1\">Lyle Regenwetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_B/0/1/0/all/0/1\">Brent Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>",
          "description": "In this paper, we present \"BIKED,\" a dataset comprised of 4500 individually\ndesigned bicycle models sourced from hundreds of designers. We expect BIKED to\nenable a variety of data-driven design applications for bicycles and support\nthe development of data-driven design methods. The dataset is comprised of a\nvariety of design information including assembly images, component images,\nnumerical design parameters, and class labels. In this paper, we first discuss\nthe processing of the dataset, then highlight some prominent research questions\nthat BIKED can help address. Of these questions, we further explore the\nfollowing in detail: 1) Are there prominent gaps in the current bicycle market\nand design space? We explore the design space using unsupervised dimensionality\nreduction methods. 2) How does one identify the class of a bicycle and what\nfactors play a key role in defining it? We address the bicycle classification\ntask by training a multitude of classifiers using different forms of design\ndata and identifying parameters of particular significance through\npermutation-based interpretability analysis. 3) How does one synthesize new\nbicycles using different representation methods? We consider numerous machine\nlearning methods to generate new bicycle models as well as interpolate between\nand extrapolate from existing models using Variational Autoencoders. The\ndataset and code are available at this http URL",
          "link": "http://arxiv.org/abs/2103.05844",
          "publishedOn": "2021-08-19T01:35:01.422Z",
          "wordCount": 686,
          "title": "BIKED: A Dataset for Computational Bicycle Design with Machine Learning Benchmarks. (arXiv:2103.05844v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shafran_A/0/1/0/all/0/1\">Avital Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peleg_S/0/1/0/all/0/1\">Shmuel Peleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Membership inference attacks (MIA) try to detect if data samples were used to\ntrain a neural network model, e.g. to detect copyright abuses. We show that\nmodels with higher dimensional input and output are more vulnerable to MIA, and\naddress in more detail models for image translation and semantic segmentation,\nincluding medical image segmentation. We show that reconstruction-errors can\nlead to very effective MIA attacks as they are indicative of memorization.\nUnfortunately, reconstruction error alone is less effective at discriminating\nbetween non-predictable images used in training and easy to predict images that\nwere never seen before. To overcome this, we propose using a novel\npredictability error that can be computed for each sample, and its computation\ndoes not require a training set. Our membership error, obtained by subtracting\nthe predictability error from the reconstruction error, is shown to achieve\nhigh MIA accuracy on an extensive number of benchmarks.",
          "link": "http://arxiv.org/abs/2102.07762",
          "publishedOn": "2021-08-19T01:35:01.397Z",
          "wordCount": 617,
          "title": "Membership Inference Attacks are Easier on Difficult Problems. (arXiv:2102.07762v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Te-Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubfar_A/0/1/0/all/0/1\">Ata Mahjoubfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prusinski_D/0/1/0/all/0/1\">Daniel Prusinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_L/0/1/0/all/0/1\">Luis Stevens</a>",
          "description": "Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in content-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 2.5 times more energy-efficient compared with an ARM Cortex-A72 CPU and\n12.5 times more energy-efficient compared with NVIDIA T4 GPU for inference by a\nlightweight convolutional neural network without batching while maintaining the\nsame level of matching accuracy. The study validates the potential of\nneuromorphic computing in low-power image retrieval, as a complementary\nparadigm to the existing von Neumann architectures.",
          "link": "http://arxiv.org/abs/2008.01380",
          "publishedOn": "2021-08-19T01:35:01.375Z",
          "wordCount": 604,
          "title": "Neuromorphic Computing for Content-based Image Retrieval. (arXiv:2008.01380v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelrahman_G/0/1/0/all/0/1\">Ghodai Abdelrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Tracing a student's knowledge is vital for tailoring the learning experience.\nRecent knowledge tracing methods tend to respond to these challenges by\nmodelling knowledge state dynamics across learning concepts. However, they\nstill suffer from several inherent challenges including: modelling forgetting\nbehaviours and identifying relationships among latent concepts. To address\nthese challenges, in this paper, we propose a novel knowledge tracing model,\nnamely \\emph{Deep Graph Memory Network} (DGMN). In this model, we incorporate a\nforget gating mechanism into an attention memory structure in order to capture\nforgetting behaviours dynamically during the knowledge tracing process.\nParticularly, this forget gating mechanism is built upon attention forgetting\nfeatures over latent concepts considering their mutual dependencies. Further,\nthis model has the capability of learning relationships between latent concepts\nfrom a dynamic latent concept graph in light of a student's evolving knowledge\nstates. A comprehensive experimental evaluation has been conducted using four\nwell-established benchmark datasets. The results show that DGMN consistently\noutperforms the state-of-the-art KT models over all the datasets. The\neffectiveness of modelling forgetting behaviours and learning latent concept\ngraphs has also been analyzed in our experiments.",
          "link": "http://arxiv.org/abs/2108.08105",
          "publishedOn": "2021-08-19T01:35:01.365Z",
          "wordCount": 607,
          "title": "Deep Graph Memory Networks for Forgetting-Robust Knowledge Tracing. (arXiv:2108.08105v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hellkvist_M/0/1/0/all/0/1\">Martin Hellkvist</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ozcelikkale_A/0/1/0/all/0/1\">Ay&#xe7;a &#xd6;z&#xe7;elikkale</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahlen_A/0/1/0/all/0/1\">Anders Ahl&#xe9;n</a>",
          "description": "Distributed learning provides an attractive framework for scaling the\nlearning task by sharing the computational load over multiple nodes in a\nnetwork. Here, we investigate the performance of distributed learning for\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\nare distributed over the network. We adopt a statistical learning approach. In\ncontrast to works that focus on the performance on the training data, we focus\non the generalization error, i.e., the performance on unseen data. We provide\nhigh-probability bounds on the generalization error for both isotropic and\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\ndependence of the generalization performance on the partitioning of the model\nover the network. In particular, our results show that the generalization error\nof the distributed solution can be substantially higher than that of the\ncentralized solution even when the error on the training data is at the same\nlevel for both the centralized and distributed approaches. Our numerical\nresults illustrate the performance with both real-world image data as well as\nsynthetic data.",
          "link": "http://arxiv.org/abs/2101.09001",
          "publishedOn": "2021-08-19T01:35:01.355Z",
          "wordCount": 640,
          "title": "Linear Regression with Distributed Learning: A Generalization Error Perspective. (arXiv:2101.09001v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1\">Robert Birke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1\">Aditya Kunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y. Chen</a>",
          "description": "Generative Adversarial Networks (GANs) are typically trained to synthesize\ndata, from images and more recently tabular data, under the assumption of\ndirectly accessible training data. Recently, federated learning (FL) is an\nemerging paradigm that features decentralized learning on client's local data\nwith a privacy-preserving capability. And, while learning GANs to synthesize\nimages on FL systems has just been demonstrated, it is unknown if GANs for\ntabular data can be learned from decentralized data sources. Moreover, it\nremains unclear which distributed architecture suits them best. Different from\nimage GANs, state-of-the-art tabular GANs require prior knowledge on the data\ndistribution of each (discrete and continuous) column to agree on a common\nencoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the\nfirst Federated learning framework for Tabular GANs. To effectively learn a\ncomplex tabular GAN on non-identical participants, Fed-TGAN designs two novel\nfeatures: (i) a privacy-preserving multi-source feature encoding for model\ninitialization; and (ii) table similarity aware weighting strategies to\naggregate local models for countering data skew. We extensively evaluate the\nproposed Fed-TGAN against variants of decentralized learning architectures on\nfour widely used datasets. Results show that Fed-TGAN accelerates training time\nper epoch up to 200% compared to the alternative architectures, for both IID\nand Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but\nalso achieves better similarity between generated and original data.",
          "link": "http://arxiv.org/abs/2108.07927",
          "publishedOn": "2021-08-19T01:35:01.345Z",
          "wordCount": 656,
          "title": "Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data. (arXiv:2108.07927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhatib_N/0/1/0/all/0/1\">Natasha Alkhatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1\">Hadi Ghauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danger_J/0/1/0/all/0/1\">Jean-Luc Danger</a>",
          "description": "Intrusion Detection Systems are widely used to detect cyberattacks,\nespecially on protocols vulnerable to hacking attacks such as SOME/IP. In this\npaper, we present a deep learning-based sequential model for offline intrusion\ndetection on SOME/IP application layer protocol. To assess our intrusion\ndetection system, we have generated and labeled a dataset with several classes\nrepresenting realistic intrusions, and a normal class - a significant\ncontribution due to the absence of such publicly available datasets.\nFurthermore, we also propose a simple recurrent neural network (RNN), as an\ninstance of deep learning-based sequential model, that we apply to our\ngenerated dataset. The numerical results show that RNN excel at predicting\nin-vehicle intrusions, with F1 Scores and AUC values of 0.99 for each type of\nintrusion.",
          "link": "http://arxiv.org/abs/2108.08262",
          "publishedOn": "2021-08-19T01:35:01.291Z",
          "wordCount": 565,
          "title": "SOME/IP Intrusion Detection using Deep Learning-based Sequential Models in Automotive Ethernet Networks. (arXiv:2108.08262v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Z/0/1/0/all/0/1\">Zicun Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1\">Pei Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.",
          "link": "http://arxiv.org/abs/2108.07915",
          "publishedOn": "2021-08-19T01:35:01.264Z",
          "wordCount": 624,
          "title": "Data Pricing in Machine Learning Pipelines. (arXiv:2108.07915v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08038",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+OLuing_M/0/1/0/all/0/1\">Mervyn O&#x27;Luing</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Prestwich_S/0/1/0/all/0/1\">Steven Prestwich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tarim_S/0/1/0/all/0/1\">S. Armagan Tarim</a>",
          "description": "In this paper we combine the k-means and/or k-means type algorithms with a\nhill climbing algorithm in stages to solve the joint stratification and sample\nallocation problem. This is a combinatorial optimisation problem in which we\nsearch for the optimal stratification from the set of all possible\nstratifications of basic strata. Each stratification being a solution the\nquality of which is measured by its cost. This problem is intractable for\nlarger sets. Furthermore evaluating the cost of each solution is expensive. A\nnumber of heuristic algorithms have already been developed to solve this\nproblem with the aim of finding acceptable solutions in reasonable computation\ntimes. However, the heuristics for these algorithms need to be trained in order\nto optimise performance in each instance. We compare the above multi-stage\ncombination of algorithms with three recent algorithms and report the solution\ncosts, evaluation times and training times. The multi-stage combinations\ngenerally compare well with the recent algorithms both in the case of atomic\nand continuous strata and provide the survey designer with a greater choice of\nalgorithms to choose from.",
          "link": "http://arxiv.org/abs/2108.08038",
          "publishedOn": "2021-08-19T01:35:01.256Z",
          "wordCount": 630,
          "title": "Combining K-means type algorithms with Hill Climbing for Joint Stratification and Sample Allocation Designs. (arXiv:2108.08038v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjinappa_C/0/1/0/all/0/1\">Chethan K. Anjinappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guvenc_I/0/1/0/all/0/1\">Ismail Guvenc</a>",
          "description": "The utilization of millimeter-wave (mmWave) bands in 5G networks poses new\nchallenges to network planning. Vulnerability to blockages at mmWave bands can\ncause coverage holes (CHs) in the radio environment, leading to radio link\nfailure when a user enters these CHs. Detection of the CHs carries critical\nimportance so that necessary remedies can be introduced to improve coverage. In\nthis letter, we propose a novel approach to identify the CHs in an unsupervised\nfashion using a state-of-the-art manifold learning technique: uniform manifold\napproximation and projection. The key idea is to preserve the\nlocal-connectedness structure inherent in the collected unlabelled channel\nsamples, such that the CHs from the service area are detectable. Our results on\nthe DeepMIMO dataset scenario demonstrate that the proposed method can learn\nthe structure within the data samples and provide visual holes in the\nlow-dimensional embedding while preserving the CH boundaries. Once the CH\nboundary is determined in the low-dimensional embedding, channel-based\nlocalization techniques can be applied to these samples to obtain the\ngeographical boundaries of the CHs.",
          "link": "http://arxiv.org/abs/2108.07854",
          "publishedOn": "2021-08-19T01:35:01.249Z",
          "wordCount": 621,
          "title": "Coverage Hole Detection for mmWave Networks: An Unsupervised Learning Approach. (arXiv:2108.07854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10643",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>",
          "description": "Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.",
          "link": "http://arxiv.org/abs/2101.10643",
          "publishedOn": "2021-08-19T01:35:01.238Z",
          "wordCount": 661,
          "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v8 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>",
          "description": "Benford's Law (BL) or the Significant Digit Law defines the probability\ndistribution of the first digit of numerical values in a data sample. This Law\nis observed in many naturally occurring datasets. It can be seen as a measure\nof naturalness of a given distribution and finds its application in areas like\nanomaly and fraud detection. In this work, we address the following question:\nIs the distribution of the Neural Network parameters related to the network's\ngeneralization capability? To that end, we first define a metric, MLH (Model\nEnthalpy),that measures the closeness of a set of numbers to Benford's Law and\nwe show empirically that it is a strong predictor of Validation Accuracy.\nSecond, we use MLH as an alternative to Validation Accuracy for Early Stopping,\nremoving the need for a Validation set. We provide experimental evidence that\neven if the optimal size of the validation set is known before-hand, the peak\ntest accuracy attained is lower than not using a validation set at all.\nFinally, we investigate the connection of BL to Free Energy Principle and First\nLaw of Thermodynamics, showing that MLH is a component of the internal energy\nof the learning system and optimization as an analogy to minimizing the total\nenergy to attain equilibrium.",
          "link": "http://arxiv.org/abs/2102.03313",
          "publishedOn": "2021-08-19T01:35:01.228Z",
          "wordCount": 652,
          "title": "Rethinking Neural Networks With Benford's Law. (arXiv:2102.03313v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hin_D/0/1/0/all/0/1\">David Hin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croft_R/0/1/0/all/0/1\">Roland Croft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "It is increasingly suggested to identify Software Vulnerabilities (SVs) in\ncode commits to give early warnings about potential security risks. However,\nthere is a lack of effort to assess vulnerability-contributing commits right\nafter they are detected to provide timely information about the exploitability,\nimpact and severity of SVs. Such information is important to plan and\nprioritize the mitigation for the identified SVs. We propose a novel Deep\nmulti-task learning model, DeepCVA, to automate seven Commit-level\nVulnerability Assessment tasks simultaneously based on Common Vulnerability\nScoring System (CVSS) metrics. We conduct large-scale experiments on 1,229\nvulnerability-contributing commits containing 542 different SVs in 246\nreal-world software projects to evaluate the effectiveness and efficiency of\nour model. We show that DeepCVA is the best-performing model with 38% to 59.8%\nhigher Matthews Correlation Coefficient than many supervised and unsupervised\nbaseline models. DeepCVA also requires 6.3 times less training and validation\ntime than seven cumulative assessment models, leading to significantly less\nmodel maintenance cost as well. Overall, DeepCVA presents the first effective\nand efficient solution to automatically assess SVs early in software systems.",
          "link": "http://arxiv.org/abs/2108.08041",
          "publishedOn": "2021-08-19T01:35:01.210Z",
          "wordCount": 638,
          "title": "DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning. (arXiv:2108.08041v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08180",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jinhua Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>",
          "description": "In this paper, sparsification techniques aided online prediction algorithms\nin a reproducing kernel Hilbert space are studied for nonstationary time\nseries. The online prediction algorithms as usual consist of the selection of\nkernel structure parameters and the kernel weight vector updating. For\nstructure parameters, the kernel dictionary is selected by some sparsification\ntechniques with online selective modeling criteria, and moreover the kernel\ncovariance matrix is intermittently optimized in the light of the covariance\nmatrix adaptation evolution strategy (CMA-ES). Optimizing the real symmetric\ncovariance matrix can not only improve the kernel structure's flexibility by\nthe cross relatedness of the input variables, but also partly alleviate the\nprediction uncertainty caused by the kernel dictionary selection for\nnonstationary time series. In order to sufficiently capture the underlying\ndynamic characteristics in prediction-error time series, a generalized\noptimization strategy is designed to construct the kernel dictionary\nsequentially in multiple kernel connection modes. The generalized optimization\nstrategy provides a more self-contained way to construct the entire kernel\nconnections, which enhances the ability to adaptively track the changing\ndynamic characteristics. Numerical simulations have demonstrated that the\nproposed approach has superior prediction performance for nonstationary time\nseries.",
          "link": "http://arxiv.org/abs/2108.08180",
          "publishedOn": "2021-08-19T01:35:01.204Z",
          "wordCount": 649,
          "title": "Structure Parameter Optimized Kernel Based Online Prediction with a Generalized Optimization Strategy for Nonstationary Time Series. (arXiv:2108.08180v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08077",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Wadhawan_K/0/1/0/all/0/1\">Kahini Wadhawan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Han_B/0/1/0/all/0/1\">Barbara A. Han</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Fischhoff_I/0/1/0/all/0/1\">Ilya R. Fischhoff</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Castellanos_A/0/1/0/all/0/1\">Adrian C. Castellanos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Varsani_A/0/1/0/all/0/1\">Arvind Varsani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>",
          "description": "Current methods for viral discovery target evolutionarily conserved proteins\nthat accurately identify virus families but remain unable to distinguish the\nzoonotic potential of newly discovered viruses. Here, we apply an\nattention-enhanced long-short-term memory (LSTM) deep neural net classifier to\na highly conserved viral protein target to predict zoonotic potential across\nbetacoronaviruses. The classifier performs with a 94% accuracy. Analysis and\nvisualization of attention at the sequence and structure-level features\nindicate possible association between important protein-protein interactions\ngoverning viral replication in zoonotic betacoronaviruses and zoonotic\ntransmission.",
          "link": "http://arxiv.org/abs/2108.08077",
          "publishedOn": "2021-08-19T01:35:01.193Z",
          "wordCount": 595,
          "title": "Towards Interpreting Zoonotic Potential of Betacoronavirus Sequences With Attention. (arXiv:2108.08077v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09499",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1\">Jiaxin Hu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_C/0/1/0/all/0/1\">Chanwoo Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1\">Miaoyan Wang</a>",
          "description": "Higher-order tensors have received increased attention across science and\nengineering. While most tensor decomposition methods are developed for a single\ntensor observation, scientific studies often collect side information, in the\nform of node features and interactions thereof, together with the tensor data.\nSuch data problems are common in neuroimaging, network analysis, and\nspatial-temporal modeling. Identifying the relationship between a\nhigh-dimensional tensor and side information is important yet challenging.\nHere, we develop a tensor decomposition method that incorporates multiple\nfeature matrices as side information. Unlike unsupervised tensor decomposition,\nour supervised decomposition captures the effective dimension reduction of the\ndata tensor confined to feature space of interest. An efficient alternating\noptimization algorithm with provable spectral initialization is further\ndeveloped. Our proposal handles a broad range of data types, including\ncontinuous, count, and binary observations. We apply the method to diffusion\ntensor imaging data from human connectome project and multi-relational\npolitical network data. We identify the key global connectivity pattern and\npinpoint the local regions that are associated with available features. Our\nsimulation code, R-package tensorregress, and datasets used in the paper are\navailable at https://CRAN.R-project.org/package=tensorregress.",
          "link": "http://arxiv.org/abs/1910.09499",
          "publishedOn": "2021-08-19T01:35:01.183Z",
          "wordCount": 650,
          "title": "Supervised tensor decomposition with features on multiple modes. (arXiv:1910.09499v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmen_M/0/1/0/all/0/1\">Melanie Dohmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guarino_V/0/1/0/all/0/1\">Vanessa Emanuela Guarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokarian_A/0/1/0/all/0/1\">Ashkan Mokarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mais_L/0/1/0/all/0/1\">Lisa Mais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_J/0/1/0/all/0/1\">Jan Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>",
          "description": "Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.",
          "link": "http://arxiv.org/abs/2101.05846",
          "publishedOn": "2021-08-19T01:35:01.167Z",
          "wordCount": 737,
          "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation. (arXiv:2101.05846v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Predictor-based algorithms have achieved remarkable performance in the Neural\nArchitecture Search (NAS) tasks. However, these methods suffer from high\ncomputation costs, as training the performance predictor usually requires\ntraining and evaluating hundreds of architectures from scratch. Previous works\nalong this line mainly focus on reducing the number of architectures required\nto fit the predictor. In this work, we tackle this challenge from a different\nperspective - improve search efficiency by cutting down the computation budget\nof architecture training. We propose NOn-uniform Successive Halving (NOSH), a\nhierarchical scheduling algorithm that terminates the training of\nunderperforming architectures early to avoid wasting budget. To effectively\nleverage the non-uniform supervision signals produced by NOSH, we formulate\npredictor-based architecture search as learning to rank with pairwise\ncomparisons. The resulting method - RANK-NOSH, reduces the search budget by ~5x\nwhile achieving competitive or even better performance than previous\nstate-of-the-art predictor-based methods on various spaces and datasets.",
          "link": "http://arxiv.org/abs/2108.08019",
          "publishedOn": "2021-08-19T01:35:01.156Z",
          "wordCount": 609,
          "title": "RANK-NOSH: Efficient Predictor-Based Architecture Search via Non-Uniform Successive Halving. (arXiv:2108.08019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fitzke_M/0/1/0/all/0/1\">Michael Fitzke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitley_D/0/1/0/all/0/1\">Derick Whitley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yau_W/0/1/0/all/0/1\">Wilson Yau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_F/0/1/0/all/0/1\">Fernando Rodrigues Jr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadeev_V/0/1/0/all/0/1\">Vladimir Fadeev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacmeister_C/0/1/0/all/0/1\">Cindy Bacmeister</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carter_C/0/1/0/all/0/1\">Chris Carter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1\">Jeffrey Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_M/0/1/0/all/0/1\">Mark Parkinson</a>",
          "description": "Background: Histopathology is an important modality for the diagnosis and\nmanagement of many diseases in modern healthcare, and plays a critical role in\ncancer care. Pathology samples can be large and require multi-site sampling,\nleading to upwards of 20 slides for a single tumor, and the human-expert tasks\nof site selection and and quantitative assessment of mitotic figures are time\nconsuming and subjective. Automating these tasks in the setting of a digital\npathology service presents significant opportunities to improve workflow\nefficiency and augment human experts in practice. Approach: Multiple\nstate-of-the-art deep learning techniques for histopathology image\nclassification and mitotic figure detection were used in the development of\nOncoPetNet. Additionally, model-free approaches were used to increase speed and\naccuracy. The robust and scalable inference engine leverages Pytorch's\nperformance optimizations as well as specifically developed speed up techniques\nin inference. Results: The proposed system, demonstrated significantly improved\nmitotic counting performance for 41 cancer cases across 14 cancer types\ncompared to human expert baselines. In 21.9% of cases use of OncoPetNet led to\nchange in tumor grading compared to human expert evaluation. In deployment, an\neffective 0.27 min/slide inference was achieved in a high throughput veterinary\ndiagnostic pathology service across 2 centers processing 3,323 digital whole\nslide images daily. Conclusion: This work represents the first successful\nautomated deployment of deep learning systems for real-time expert-level\nperformance on important histopathology tasks at scale in a high volume\nclinical practice. The resulting impact outlines important considerations for\nmodel development, deployment, clinical decision making, and informs best\npractices for implementation of deep learning systems in digital histopathology\npractices.",
          "link": "http://arxiv.org/abs/2108.07856",
          "publishedOn": "2021-08-19T01:35:01.148Z",
          "wordCount": 758,
          "title": "OncoPetNet: A Deep Learning based AI system for mitotic figure counting on H&E stained whole slide digital images in a large veterinary diagnostic lab setting. (arXiv:2108.07856v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07872",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_P/0/1/0/all/0/1\">Priya Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Han_C/0/1/0/all/0/1\">Cuize Han</a>",
          "description": "E-commerce websites use machine learned ranking models to serve shopping\nresults to customers. Typically, the websites log the customer search events,\nwhich include the query entered and the resulting engagement with the shopping\nresults, such as clicks and purchases. Each customer search event serves as\ninput training data for the models, and the individual customer engagement\nserves as a signal for customer preference. So a purchased shopping result, for\nexample, is perceived to be more important than one that is not. However, new\nor under-impressed products do not have enough customer engagement signals and\nend up at a disadvantage when being ranked alongside popular products. In this\npaper, we propose a novel method for data curation that aggregates all customer\nengagements within a day for the same query to use as input training data. This\naggregated customer engagement gives the models a complete picture of the\nrelative importance of shopping results. Training models on this aggregated\ndata leads to less reliance on behavioral features. This helps mitigate the\ncold start problem and boosted relevant new products to top search results. In\nthis paper, we present the offline and online analysis and results comparing\nthe individual and aggregated customer engagement models trained on e-commerce\ndata.",
          "link": "http://arxiv.org/abs/2108.07872",
          "publishedOn": "2021-08-19T01:35:01.137Z",
          "wordCount": 626,
          "title": "Aggregated Customer Engagement Model. (arXiv:2108.07872v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simic_I/0/1/0/all/0/1\">Ilija &#x160;imi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabol_V/0/1/0/all/0/1\">Vedran Sabol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veas_E/0/1/0/all/0/1\">Eduardo Veas</a>",
          "description": "Deep learning models have recently demonstrated remarkable results in a\nvariety of tasks, which is why they are being increasingly applied in\nhigh-stake domains, such as industry, medicine, and finance. Considering that\nautomatic predictions in these domains might have a substantial impact on the\nwell-being of a person, as well as considerable financial and legal\nconsequences to an individual or a company, all actions and decisions that\nresult from applying these models have to be accountable. Given that a\nsubstantial amount of data that is collected in high-stake domains are in the\nform of time series, in this paper we examine the current state of eXplainable\nAI (XAI) methods with a focus on approaches for opening up deep learning black\nboxes for the task of time series classification. Finally, our contribution\nalso aims at deriving promising directions for future work, to advance XAI for\ndeep learning on time series data.",
          "link": "http://arxiv.org/abs/2108.08009",
          "publishedOn": "2021-08-19T01:35:01.116Z",
          "wordCount": 609,
          "title": "XAI Methods for Neural Time Series Classification: A Brief Review. (arXiv:2108.08009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_L/0/1/0/all/0/1\">Luca Parisi</a>",
          "description": "This study presents the m-arcsinh Kernel ('m-ar-K') Fast Independent\nComponent Analysis ('FastICA') method ('m-ar-K-FastICA') for feature\nextraction. The kernel trick has enabled dimensionality reduction techniques to\ncapture a higher extent of non-linearity in the data; however, reproducible,\nopen-source kernels to aid with feature extraction are still limited and may\nnot be reliable when projecting features from entropic data. The m-ar-K\nfunction, freely available in Python and compatible with its open-source\nlibrary 'scikit-learn', is hereby coupled with FastICA to achieve more reliable\nfeature extraction in presence of a high extent of randomness in the data,\nreducing the need for pre-whitening. Different classification tasks were\nconsidered, as related to five (N = 5) open access datasets of various degrees\nof information entropy, available from scikit-learn and the University\nCalifornia Irvine (UCI) Machine Learning repository. Experimental results\ndemonstrate improvements in the classification performance brought by the\nproposed feature extraction. The novel m-ar-K-FastICA dimensionality reduction\napproach is compared to the 'FastICA' gold standard method, supporting its\nhigher reliability and computational efficiency, regardless of the underlying\nuncertainty in the data.",
          "link": "http://arxiv.org/abs/2108.07908",
          "publishedOn": "2021-08-19T01:35:01.071Z",
          "wordCount": 654,
          "title": "M-ar-K-Fast Independent Component Analysis. (arXiv:2108.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_L/0/1/0/all/0/1\">Lin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1\">Ellie X. Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>",
          "description": "Deep retrieval models are widely used for learning entity representations and\nrecommendations. Federated learning provides a privacy-preserving way to train\nthese models without requiring centralization of user data. However, federated\ndeep retrieval models usually perform much worse than their centralized\ncounterparts due to non-IID (independent and identically distributed) training\ndata on clients, an intrinsic property of federated learning that limits\nnegatives available for training. We demonstrate that this issue is distinct\nfrom the commonly studied client drift problem. This work proposes\nbatch-insensitive losses as a way to alleviate the non-IID negatives issue for\nfederated movie recommendation. We explore a variety of techniques and identify\nthat batch-insensitive losses can effectively improve the performance of\nfederated deep retrieval models, increasing the relative recall of the\nfederated model by up to 93.15% and reducing the relative gap in recall between\nit and a centralized model from 27.22% - 43.14% to 0.53% - 2.42%. We\nopen-source our code framework to accelerate further research and applications\nof federated deep retrieval models.",
          "link": "http://arxiv.org/abs/2108.07931",
          "publishedOn": "2021-08-19T01:35:01.046Z",
          "wordCount": 604,
          "title": "Learning Federated Representations and Recommendations with Limited Negatives. (arXiv:2108.07931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cholakov_R/0/1/0/all/0/1\">Radostin Cholakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolev_T/0/1/0/all/0/1\">Todor Kolev</a>",
          "description": "Recurrent Neural Networks were, until recently, one of the best ways to\ncapture the timely dependencies in sequences. However, with the introduction of\nthe Transformer, it has been proven that an architecture with only\nattention-mechanisms without any RNN can improve on the results in various\nsequence processing tasks (e.g. NLP). Multiple studies since then have shown\nthat similar approaches can be applied for images, point clouds, video, audio\nor time series forecasting. Furthermore, solutions such as the Perceiver or the\nInformer have been introduced to expand on the applicability of the\nTransformer. Our main objective is testing and evaluating the effectiveness of\napplying Transformer-like models on time series data, tackling susceptibility\nto anomalies, context awareness and space complexity by fine-tuning the\nhyperparameters, preprocessing the data, applying dimensionality reduction or\nconvolutional encodings, etc. We are also looking at the problem of next-frame\nprediction and exploring ways to modify existing solutions in order to achieve\nhigher performance and learn generalized knowledge.",
          "link": "http://arxiv.org/abs/2108.08224",
          "publishedOn": "2021-08-19T01:35:01.037Z",
          "wordCount": 606,
          "title": "Transformers predicting the future. Applying attention in next-frame and time series forecasting. (arXiv:2108.08224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimibeni_M/0/1/0/all/0/1\">Mohammad Salimibeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiakhondi_Meybodi_Z/0/1/0/all/0/1\">Zohreh Hajiakhondi-Meybodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingxu Wang</a>",
          "description": "Recently, as a consequence of the COVID-19 pandemic, dependence on Contact\nTracing (CT) models has significantly increased to prevent spread of this\nhighly contagious virus and be prepared for the potential future ones. Since\nthe spreading probability of the novel coronavirus in indoor environments is\nmuch higher than that of the outdoors, there is an urgent and unmet quest to\ndevelop/design efficient, autonomous, trustworthy, and secure indoor CT\nsolutions. Despite such an urgency, this field is still in its infancy. The\npaper addresses this gap and proposes the Trustworthy Blockchain-enabled system\nfor Indoor Contact Tracing (TB-ICT) framework. The TB-ICT framework is proposed\nto protect privacy and integrity of the underlying CT data from unauthorized\naccess. More specifically, it is a fully distributed and innovative blockchain\nplatform exploiting the proposed dynamic Proof of Work (dPoW) credit-based\nconsensus algorithm coupled with Randomized Hash Window (W-Hash) and dynamic\nProof of Credit (dPoC) mechanisms to differentiate between honest and dishonest\nnodes. The TB-ICT not only provides a decentralization in data replication but\nalso quantifies the node's behavior based on its underlying credit-based\nmechanism. For achieving high localization performance, we capitalize on\navailability of Internet of Things (IoT) indoor localization infrastructures,\nand develop a data driven localization model based on Bluetooth Low Energy\n(BLE) sensor measurements. The simulation results show that the proposed TB-ICT\nprevents the COVID-19 from spreading by implementation of a highly accurate\ncontact tracing model while improving the users' privacy and security.",
          "link": "http://arxiv.org/abs/2108.08275",
          "publishedOn": "2021-08-19T01:35:01.031Z",
          "wordCount": 729,
          "title": "TB-ICT: A Trustworthy Blockchain-Enabled System for Indoor COVID-19 Contact Tracing. (arXiv:2108.08275v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-08-19T01:35:01.014Z",
          "wordCount": 690,
          "title": "Off-Belief Learning. (arXiv:2103.04000v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdoli_M/0/1/0/all/0/1\">Mahsan Abdoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahrabi_J/0/1/0/all/0/1\">Jamal Shahrabi</a>",
          "description": "Credit scoring models, which are among the most potent risk management tools\nthat banks and financial institutes rely on, have been a popular subject for\nresearch in the past few decades. Accordingly, many approaches have been\ndeveloped to address the challenges in classifying loan applicants and improve\nand facilitate decision-making. The imbalanced nature of credit scoring\ndatasets, as well as the heterogeneous nature of features in credit scoring\ndatasets, pose difficulties in developing and implementing effective credit\nscoring models, targeting the generalization power of classification models on\nunseen data. In this paper, we propose the Bagging Supervised Autoencoder\nClassifier (BSAC) that mainly leverages the superior performance of the\nSupervised Autoencoder, which learns low-dimensional embeddings of the input\ndata exclusively with regards to the ultimate classification task of credit\nscoring, based on the principles of multi-task learning. BSAC also addresses\nthe data imbalance problem by employing a variant of the Bagging process based\non the undersampling of the majority class. The obtained results from our\nexperiments on the benchmark and real-life credit scoring datasets illustrate\nthe robustness and effectiveness of the Bagging Supervised Autoencoder\nClassifier in the classification of loan applicants that can be regarded as a\npositive development in credit scoring models.",
          "link": "http://arxiv.org/abs/2108.07800",
          "publishedOn": "2021-08-19T01:35:01.007Z",
          "wordCount": 633,
          "title": "Bagging Supervised Autoencoder Classifier for Credit Scoring. (arXiv:2108.07800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.06592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>",
          "description": "Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.",
          "link": "http://arxiv.org/abs/1907.06592",
          "publishedOn": "2021-08-19T01:35:00.997Z",
          "wordCount": 700,
          "title": "Sparsely Activated Networks. (arXiv:1907.06592v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raman_R/0/1/0/all/0/1\">Raja CSP Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_R/0/1/0/all/0/1\">Rohith Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perumal_D/0/1/0/all/0/1\">Divya Perumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_V/0/1/0/all/0/1\">Vedha Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1\">Talha Abdur Rahman</a>",
          "description": "The Prevalence of Community support and engagement for different domains in\nthe tech industry has changed and evolved throughout the years. In this study,\nwe aim to understand, analyze and predict the trends of technology in a\nscientific manner, having collected data on numerous topics and their growth\nthroughout the years in the past decade. We apply machine learning models on\ncollected data, to understand, analyze and forecast the trends in the\nadvancement of different fields. We show that certain technical concepts such\nas python, machine learning, and Keras have an undisputed uptrend, finally\nconcluding that the Stackindex model forecasts with high accuracy and can be a\nviable tool for forecasting different tech domains.",
          "link": "http://arxiv.org/abs/2108.08120",
          "publishedOn": "2021-08-19T01:35:00.989Z",
          "wordCount": 549,
          "title": "Stack Index Prediction Using Time-Series Analysis. (arXiv:2108.08120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girase_H/0/1/0/all/0/1\">Harshayu Girase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1\">Srikanth Malla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehara_A/0/1/0/all/0/1\">Akira Kanehara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>",
          "description": "Recent advances in trajectory prediction have shown that explicit reasoning\nabout agents' intent is important to accurately forecast their motion. However,\nthe current research activities are not directly applicable to intelligent and\nsafety critical systems. This is mainly because very few public datasets are\navailable, and they only consider pedestrian-specific intents for a short\ntemporal horizon from a restricted egocentric view. To this end, we propose\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\nLOKI dataset is created to discover several factors that may affect intention,\nincluding i) agent's own will, ii) social interactions, iii) environmental\nconstraints, and iv) contextual information. We also propose a model that\njointly performs trajectory and intention prediction, showing that recurrently\nreasoning about intention can assist with trajectory prediction. We show our\nmethod outperforms state-of-the-art trajectory prediction methods by upto\n$27\\%$ and also provide a baseline for frame-wise intention estimation.",
          "link": "http://arxiv.org/abs/2108.08236",
          "publishedOn": "2021-08-19T01:35:00.982Z",
          "wordCount": 640,
          "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction. (arXiv:2108.08236v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08214",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Silva_M/0/1/0/all/0/1\">Mariana Da Silva</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garcia_K/0/1/0/all/0/1\">Kara Garcia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bass_C/0/1/0/all/0/1\">Cher Bass</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>",
          "description": "Biomechanical modeling of tissue deformation can be used to simulate\ndifferent scenarios of longitudinal brain evolution. In this work,we present a\ndeep learning framework for hyper-elastic strain modelling of brain atrophy,\nduring healthy ageing and in Alzheimer's Disease. The framework directly models\nthe effects of age, disease status, and scan interval to regress regional\npatterns of atrophy, from which a strain-based model estimates deformations.\nThis model is trained and validated using 3D structural magnetic resonance\nimaging data from the ADNI cohort. Results show that the framework can estimate\nrealistic deformations, following the known course of Alzheimer's disease, that\nclearly differentiate between healthy and demented patterns of ageing. This\nsuggests the framework has potential to be incorporated into explainable models\nof disease, for the exploration of interventions and counterfactual examples.",
          "link": "http://arxiv.org/abs/2108.08214",
          "publishedOn": "2021-08-19T01:35:00.974Z",
          "wordCount": 601,
          "title": "Distinguishing Healthy Ageing from Dementia: a Biomechanical Simulation of Brain Atrophy using Deep Networks. (arXiv:2108.08214v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08230",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nauck_C/0/1/0/all/0/1\">Christian Nauck</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lindner_M/0/1/0/all/0/1\">Michael Lindner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schurholt_K/0/1/0/all/0/1\">Konstantin Sch&#xfc;rholt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1\">Haoming Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schultz_P/0/1/0/all/0/1\">Paul Schultz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kurths_J/0/1/0/all/0/1\">J&#xfc;rgen Kurths</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Isenhardt_I/0/1/0/all/0/1\">Ingrid Isenhardt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hellmann_F/0/1/0/all/0/1\">Frank Hellmann</a>",
          "description": "The prediction of dynamical stability of power grids becomes more important\nand challenging with increasing shares of renewable energy sources due to their\ndecentralized structure, reduced inertia and volatility. We investigate the\nfeasibility of applying graph neural networks (GNN) to predict dynamic\nstability of synchronisation in complex power grids using the single-node basin\nstability (SNBS) as a measure. To do so, we generate two synthetic datasets for\ngrids with 20 and 100 nodes respectively and estimate SNBS using Monte-Carlo\nsampling. Those datasets are used to train and evaluate the performance of\neight different GNN-models. All models use the full graph without\nsimplifications as input and predict SNBS in a nodal-regression-setup. We show\nthat SNBS can be predicted in general and the performance significantly changes\nusing different GNN-models. Furthermore, we observe interesting transfer\ncapabilities of our approach: GNN-models trained on smaller grids can directly\nbe applied on larger grids without the need of retraining.",
          "link": "http://arxiv.org/abs/2108.08230",
          "publishedOn": "2021-08-19T01:35:00.968Z",
          "wordCount": 620,
          "title": "Predicting Dynamic Stability of Power Grids using Graph Neural Networks. (arXiv:2108.08230v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1\">Pengfei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>",
          "description": "Differentiable architecture search (DARTS) marks a milestone in Neural\nArchitecture Search (NAS), boasting simplicity and small search costs. However,\nDARTS still suffers from frequent performance collapse, which happens when some\noperations, such as skip connections, zeroes and poolings, dominate the\narchitecture. In this paper, we are the first to point out that the phenomenon\nis attributed to bi-level optimization. We propose Single-DARTS which merely\nuses single-level optimization, updating network weights and architecture\nparameters simultaneously with the same data batch. Even single-level\noptimization has been previously attempted, no literature provides a systematic\nexplanation on this essential point. Replacing the bi-level optimization,\nSingle-DARTS obviously alleviates performance collapse as well as enhances the\nstability of architecture search. Experiment results show that Single-DARTS\nachieves state-of-the-art performance on mainstream search spaces. For\ninstance, on NAS-Benchmark-201, the searched architectures are nearly optimal\nones. We also validate that the single-level optimization framework is much\nmore stable than the bi-level one. We hope that this simple yet effective\nmethod will give some insights on differential architecture search. The code is\navailable at https://github.com/PencilAndBike/Single-DARTS.git.",
          "link": "http://arxiv.org/abs/2108.08128",
          "publishedOn": "2021-08-19T01:35:00.961Z",
          "wordCount": 616,
          "title": "Single-DARTS: Towards Stable Architecture Search. (arXiv:2108.08128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattapalli_S/0/1/0/all/0/1\">Sai Mattapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athavale_R/0/1/0/all/0/1\">Rishi Athavale</a>",
          "description": "Due to morphological similarity at the microscopic level, making an accurate\nand time-sensitive distinction between blood cells affected by Acute\nLymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage\nof machine learning architectures. However, three of the most common models,\nVGG, ResNet, and Inception, each come with their own set of flaws with room for\nimprovement which demands the need for a superior model. ALLNet, the proposed\nhybrid convolutional neural network architecture, consists of a combination of\nthe VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019\n(available here) contains 10,691 images of white blood cells which were used to\ntrain and test the models. 7,272 of the images in the dataset are of cells with\nALL and 3,419 of them are of healthy cells. Of the images, 60% were used to\ntrain the model, 20% were used for the cross-validation set, and 20% were used\nfor the test set. ALLNet outperformed the VGG, ResNet, and the Inception models\nacross the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,\na specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803\nin the cross-validation set. In the test set, ALLNet achieved an accuracy of\n92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of\n0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the\nclinical workspace can better treat the thousands of people suffering from ALL\nacross the world, many of whom are children.",
          "link": "http://arxiv.org/abs/2108.08195",
          "publishedOn": "2021-08-19T01:35:00.911Z",
          "wordCount": 711,
          "title": "ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.",
          "link": "http://arxiv.org/abs/2108.08217",
          "publishedOn": "2021-08-19T01:35:00.905Z",
          "wordCount": 678,
          "title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Bin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwen Yu</a>",
          "description": "The prediction of express delivery sequence, i.e., modeling and estimating\nthe volumes of daily incoming and outgoing parcels for delivery, is critical\nfor online business, logistics, and positive customer experience, and\nspecifically for resource allocation optimization and promotional activity\narrangement. A precise estimate of consumer delivery requests has to involve\nsequential factors such as shopping behaviors, weather conditions, events,\nbusiness campaigns, and their couplings. Besides, conventional sequence\nprediction assumes a stable sequence evolution, failing to address complex\nnonlinear sequences and various feature effects in the above multi-source data.\nAlthough deep networks and attention mechanisms demonstrate the potential of\ncomplex sequence modeling, extant networks ignore the heterogeneous and\ncoupling situation between features and sequences, resulting in weak prediction\naccuracy. To address these issues, we propose DeepExpress - a deep-learning\nbased express delivery sequence prediction model, which extends the classic\nseq2seq framework to learning complex coupling between sequence and features.\nDeepExpress leverages an express delivery seq2seq learning, a\ncarefully-designed heterogeneous feature representation, and a novel joint\ntraining attention mechanism to adaptively map heterogeneous data, and capture\nsequence-feature coupling for precise estimation. Experimental results on\nreal-world data demonstrate that the proposed method outperforms both shallow\nand deep baseline models.",
          "link": "http://arxiv.org/abs/2108.08170",
          "publishedOn": "2021-08-19T01:35:00.897Z",
          "wordCount": 638,
          "title": "DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction. (arXiv:2108.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-08-19T01:35:00.890Z",
          "wordCount": 531,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_P/0/1/0/all/0/1\">Peyman Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">Ingrid Chieh Yu</a>",
          "description": "Counterfactual explanation methods interpret the outputs of a machine\nlearning model in the form of \"what-if scenarios\" without compromising the\nfidelity-interpretability trade-off. They explain how to obtain a desired\nprediction from the model by recommending small changes to the input features,\naka recourse. We believe an actionable recourse should be created based on\nsound counterfactual explanations originating from the distribution of the\nground-truth data and linked to the domain knowledge. Moreover, it needs to\npreserve the coherency between changed/unchanged features while satisfying\nuser/domain-specified constraints. This paper introduces CARE, a modular\nexplanation framework that addresses the model- and user-level desiderata in a\nconsecutive and structured manner. We tackle the existing requirements by\nproposing novel and efficient solutions that are formulated in a\nmulti-objective optimization framework. The designed framework enables\nincluding arbitrary requirements and generating counterfactual explanations and\nactionable recourse by choice. As a model-agnostic approach, CARE generates\nmultiple, diverse explanations for any black-box model in tabular\nclassification and regression settings. Several experiments on standard data\nsets and black-box models demonstrate the effectiveness of our modular\nframework and its superior performance compared to the baselines.",
          "link": "http://arxiv.org/abs/2108.08197",
          "publishedOn": "2021-08-19T01:35:00.883Z",
          "wordCount": 615,
          "title": "CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations. (arXiv:2108.08197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "For all the ways convolutional neural nets have revolutionized computer\nvision in recent years, one important aspect has received surprisingly little\nattention: the effect of image size on the accuracy of tasks being trained for.\nTypically, to be efficient, the input images are resized to a relatively small\nspatial resolution (e.g. 224x224), and both training and inference are carried\nout at this resolution. The actual mechanism for this re-scaling has been an\nafterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic\nare commonly used in most machine learning software frameworks. But do these\nresizers limit the on task performance of the trained networks? The answer is\nyes. Indeed, we show that the typical linear resizer can be replaced with\nlearned resizers that can substantially improve performance. Importantly, while\nthe classical resizers typically result in better perceptual quality of the\ndownscaled images, our proposed learned resizers do not necessarily give better\nvisual quality, but instead improve task performance. Our learned image resizer\nis jointly trained with a baseline vision model. This learned CNN-based resizer\ncreates machine friendly visual manipulations that lead to a consistent\nimprovement of the end task metric over the baseline model. Specifically, here\nwe focus on the classification task with the ImageNet dataset, and experiment\nwith four different models to learn resizers adapted to each model. Moreover,\nwe show that the proposed resizer can also be useful for fine-tuning the\nclassification baselines for other vision tasks. To this end, we experiment\nwith three different baselines to develop image quality assessment (IQA) models\non the AVA dataset.",
          "link": "http://arxiv.org/abs/2103.09950",
          "publishedOn": "2021-08-19T01:35:00.863Z",
          "wordCount": 727,
          "title": "Learning to Resize Images for Computer Vision Tasks. (arXiv:2103.09950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>",
          "description": "Generating context-aware language that embodies diverse emotions is an\nimportant step towards building empathetic NLP systems. In this paper, we\npropose a formulation of modulated layer normalization -- a technique inspired\nby computer vision -- that allows us to use large-scale language models for\nemotional response generation. In automatic and human evaluation on the\nMojiTalk dataset, our proposed modulated layer normalization method outperforms\nprior baseline methods while maintaining diversity, fluency, and coherence. Our\nmethod also obtains competitive performance even when using only 10% of the\navailable training data.",
          "link": "http://arxiv.org/abs/2108.07886",
          "publishedOn": "2021-08-19T01:35:00.855Z",
          "wordCount": 525,
          "title": "Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.00400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>",
          "description": "Recent literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features, but without considering\nthe description length of the representations. In this thesis, first we\nintroduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined metric $\\varphi$. We lastly present Sparsely Activated\nNetworks (SANs) that consist of kernels with shared weights that, during\nencoding, are convolved with the input and then passed through a sparse\nactivation function. During decoding, the same weights are convolved with the\nsparse activation map and subsequently the partial reconstructions from each\nweight are summed to reconstruct the input. We compare SANs using the five\npreviously defined activation functions on a variety of datasets (Physionet,\nUCI-epilepsy, MNIST, FMNIST) and show that models that are selected using\n$\\varphi$ have small description representation length and consist of\ninterpretable kernels.",
          "link": "http://arxiv.org/abs/1911.00400",
          "publishedOn": "2021-08-19T01:35:00.846Z",
          "wordCount": 660,
          "title": "Sparsely Activated Networks: A new method for decomposing and compressing data. (arXiv:1911.00400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seong Jin Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myoung Ho Kim</a>",
          "description": "Link prediction is one of the key problems for graph-structured data. With\nthe advancement of graph neural networks, graph autoencoders (GAEs) and\nvariational graph autoencoders (VGAEs) have been proposed to learn graph\nembeddings in an unsupervised way. It has been shown that these methods are\neffective for link prediction tasks. However, they do not work well in link\npredictions when a node whose degree is zero (i.g., isolated node) is involved.\nWe have found that GAEs/VGAEs make embeddings of isolated nodes close to zero\nregardless of their content features. In this paper, we propose a novel\nVariational Graph Normalized AutoEncoder (VGNAE) that utilize\n$L_2$-normalization to derive better embeddings for isolated nodes. We show\nthat our VGNAEs outperform the existing state-of-the-art models for link\nprediction tasks. The code is available at\nhttps://github.com/SeongJinAhn/VGNAE.",
          "link": "http://arxiv.org/abs/2108.08046",
          "publishedOn": "2021-08-19T01:35:00.836Z",
          "wordCount": 557,
          "title": "Variational Graph Normalized Auto-Encoders. (arXiv:2108.08046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08253",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kavakli_K/0/1/0/all/0/1\">Koray Kavakl&#x131;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Urey_H/0/1/0/all/0/1\">Hakan Urey</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Aksit_K/0/1/0/all/0/1\">Kaan Ak&#x15f;it</a>",
          "description": "Computer-Generated Holography (CGH) algorithms often fall short in matching\nsimulations with results from a physical holographic display. Our work\naddresses this mismatch by learning the holographic light transport in\nholographic displays. Using a camera and a holographic display, we capture the\nimage reconstructions of optimized holograms that rely on ideal simulations to\ngenerate a dataset. Inspired by the ideal simulations, we learn a\ncomplex-valued convolution kernel that can propagate given holograms to\ncaptured photographs in our dataset. Our method can dramatically improve\nsimulation accuracy and image quality in holographic displays while paving the\nway for physically informed learning approaches.",
          "link": "http://arxiv.org/abs/2108.08253",
          "publishedOn": "2021-08-19T01:35:00.823Z",
          "wordCount": 523,
          "title": "Learned holographic light transport. (arXiv:2108.08253v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaei_A/0/1/0/all/0/1\">Ali Aghdaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuo Feng</a>",
          "description": "Hypergraphs allow modeling problems with multi-way high-order relationships.\nHowever, the computational cost of most existing hypergraph-based algorithms\ncan be heavily dependent upon the input hypergraph sizes. To address the\never-increasing computational challenges, graph coarsening can be potentially\napplied for preprocessing a given hypergraph by aggressively aggregating its\nvertices (nodes). However, state-of-the-art hypergraph partitioning\n(clustering) methods that incorporate heuristic graph coarsening techniques are\nnot optimized for preserving the structural (global) properties of hypergraphs.\nIn this work, we propose an efficient spectral hypergraph coarsening scheme\n(HyperSF) for well preserving the original spectral (structural) properties of\nhypergraphs. Our approach leverages a recent strongly-local max-flow-based\nclustering algorithm for detecting the sets of hypergraph vertices that\nminimize ratio cut. To further improve the algorithm efficiency, we propose a\ndivide-and-conquer scheme by leveraging spectral clustering of the bipartite\ngraphs corresponding to the original hypergraphs. Our experimental results for\na variety of hypergraphs extracted from real-world VLSI design benchmarks show\nthat the proposed hypergraph coarsening algorithm can significantly improve the\nmulti-way conductance of hypergraph clustering as well as runtime efficiency\nwhen compared with existing state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2108.07901",
          "publishedOn": "2021-08-19T01:35:00.797Z",
          "wordCount": 607,
          "title": "HyperSF: Spectral Hypergraph Coarsening via Flow-based Local Clustering. (arXiv:2108.07901v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chia-Hsiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yu-Shin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yuan-Yao Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai-Chiang Wu</a>",
          "description": "Neural architecture search can discover neural networks with good\nperformance, and One-Shot approaches are prevalent. One-Shot approaches\ntypically require a supernet with weight sharing and predictors that predict\nthe performance of architecture. However, the previous methods take much time\nto generate performance predictors thus are inefficient. To this end, we\npropose FOX-NAS that consists of fast and explainable predictors based on\nsimulated annealing and multivariate regression. Our method is\nquantization-friendly and can be efficiently deployed to the edge. The\nexperiments on different hardware show that FOX-NAS models outperform some\nother popular neural network architectures. For example, FOX-NAS matches\nMobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on\nthe edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer\nVision Challenge (LPCVC), DSP classification track. See all evaluation results\nat https://lpcv.ai/competitions/2020. Search code and pre-trained models are\nreleased at https://github.com/great8nctu/FOX-NAS.",
          "link": "http://arxiv.org/abs/2108.08189",
          "publishedOn": "2021-08-19T01:35:00.789Z",
          "wordCount": 598,
          "title": "FOX-NAS: Fast, On-device and Explainable Neural Architecture Search. (arXiv:2108.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diers_J/0/1/0/all/0/1\">Jan Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pigorsch_C/0/1/0/all/0/1\">Christian Pigorsch</a>",
          "description": "Out-of-distribution detection (OOD) deals with anomalous input to neural\nnetworks. In the past, specialized methods have been proposed to reject\npredictions on anomalous input. We use outlier detection algorithms to detect\nanomalous input as reliable as specialized methods from the field of OOD. No\nneural network adaptation is required; detection is based on the model's\nsoftmax score. Our approach works unsupervised with an Isolation Forest or with\nsupervised classifiers such as a Gradient Boosting machine.",
          "link": "http://arxiv.org/abs/2108.08218",
          "publishedOn": "2021-08-19T01:35:00.781Z",
          "wordCount": 496,
          "title": "Out-of-Distribution Detection using Outlier Detection Methods. (arXiv:2108.08218v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.03021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1\">Martin Rinard</a>",
          "description": "Researchers have developed neural network verification algorithms motivated\nby the need to characterize the robustness of deep neural networks. The\nverifiers aspire to answer whether a neural network guarantees certain\nproperties with respect to all inputs in a space. However, many verifiers\ninaccurately model floating point arithmetic but do not thoroughly discuss the\nconsequences.\n\nWe show that the negligence of floating point error leads to unsound\nverification that can be systematically exploited in practice. For a pretrained\nneural network, we present a method that efficiently searches inputs as\nwitnesses for the incorrectness of robustness claims made by a complete\nverifier. We also present a method to construct neural network architectures\nand weights that induce wrong results of an incomplete verifier. Our results\nhighlight that, to achieve practically reliable verification of neural\nnetworks, any verification system must accurately (or conservatively) model the\neffects of any floating point computations in the network inference or\nverification system.",
          "link": "http://arxiv.org/abs/2003.03021",
          "publishedOn": "2021-08-19T01:35:00.761Z",
          "wordCount": 631,
          "title": "Exploiting Verified Neural Networks via Floating Point Numerical Error. (arXiv:2003.03021v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08052",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rozen_N/0/1/0/all/0/1\">Noam Rozen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nickel_M/0/1/0/all/0/1\">Maximilian Nickel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "We are interested in learning generative models for complex geometries\ndescribed via manifolds, such as spheres, tori, and other implicit surfaces.\nCurrent extensions of existing (Euclidean) generative models are restricted to\nspecific geometries and typically suffer from high computational costs. We\nintroduce Moser Flow (MF), a new class of generative models within the family\nof continuous normalizing flows (CNF). MF also produces a CNF via a solution to\nthe change-of-variable formula, however differently from other CNF methods, its\nmodel (learned) density is parameterized as the source (prior) density minus\nthe divergence of a neural network (NN). The divergence is a local, linear\ndifferential operator, easy to approximate and calculate on manifolds.\nTherefore, unlike other CNFs, MF does not require invoking or backpropagating\nthrough an ODE solver during training. Furthermore, representing the model\ndensity explicitly as the divergence of a NN rather than as a solution of an\nODE facilitates learning high fidelity densities. Theoretically, we prove that\nMF constitutes a universal density approximator under suitable assumptions.\nEmpirically, we demonstrate for the first time the use of flow models for\nsampling from general curved surfaces and achieve significant improvements in\ndensity estimation, sample quality, and training complexity over existing CNFs\non challenging synthetic geometries and real-world benchmarks from the earth\nand climate sciences.",
          "link": "http://arxiv.org/abs/2108.08052",
          "publishedOn": "2021-08-19T01:35:00.707Z",
          "wordCount": 648,
          "title": "Moser Flow: Divergence-based Generative Modeling on Manifolds. (arXiv:2108.08052v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1\">Maja J Matari&#x107;</a>",
          "description": "Automated systems that detect the social behavior of deception can enhance\nhuman well-being across medical, social work, and legal domains. Labeled\ndatasets to train supervised deception detection models can rarely be collected\nfor real-world, high-stakes contexts. To address this challenge, we propose the\nfirst unsupervised approach for detecting real-world, high-stakes deception in\nvideos without requiring labels. This paper presents our novel approach for\naffect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative\nrepresentations of deceptive and truthful behavior. Drawing on psychology\ntheories that link affect and deception, we experimented with unimodal and\nmultimodal DBN-based approaches trained on facial valence, facial arousal,\naudio, and visual features. In addition to using facial affect as a feature on\nwhich DBN models are trained, we also introduce a DBN training procedure that\nuses facial affect as an aligner of audio-visual representations. We conducted\nclassification experiments with unsupervised Gaussian Mixture Model clustering\nto evaluate our approaches. Our best unsupervised approach (trained on facial\nvalence and visual features) achieved an AUC of 80%, outperforming human\nability and performing comparably to fully-supervised models. Our results\nmotivate future work on unsupervised, affect-aware computational approaches for\ndetecting deception and other social behaviors in the wild.",
          "link": "http://arxiv.org/abs/2108.07897",
          "publishedOn": "2021-08-19T01:35:00.683Z",
          "wordCount": 637,
          "title": "Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection. (arXiv:2108.07897v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tianhong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hengyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1\">Kai Arulkumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1\">Anil Anthony Bharath</a>",
          "description": "Hindsight experience replay (HER) is a goal relabelling technique typically\nused with off-policy deep reinforcement learning algorithms to solve\ngoal-oriented tasks; it is well suited to robotic manipulation tasks that\ndeliver only sparse rewards. In HER, both trajectories and transitions are\nsampled uniformly for training. However, not all of the agent's experiences\ncontribute equally to training, and so naive uniform sampling may lead to\ninefficient learning. In this paper, we propose diversity-based trajectory and\ngoal selection with HER (DTGSH). Firstly, trajectories are sampled according to\nthe diversity of the goal states as modelled by determinantal point processes\n(DPPs). Secondly, transitions with diverse goal states are selected from the\ntrajectories by using k-DPPs. We evaluate DTGSH on five challenging robotic\nmanipulation tasks in simulated robot environments, where we show that our\nmethod can learn more quickly and reach higher performance than other\nstate-of-the-art approaches on all tasks.",
          "link": "http://arxiv.org/abs/2108.07887",
          "publishedOn": "2021-08-19T01:35:00.640Z",
          "wordCount": 584,
          "title": "Diversity-based Trajectory and Goal Selection with Hindsight Experience Replay. (arXiv:2108.07887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06412",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vretos_N/0/1/0/all/0/1\">Nicholas Vretos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>",
          "description": "Recently there has been an explosion in the use of Deep Learning (DL) methods\nfor medical image segmentation. However the field's reliability is hindered by\nthe lack of a common base of reference for accuracy/performance evaluation and\nthe fact that previous research uses different datasets for evaluation. In this\npaper, an extensive comparison of DL models for lung and COVID-19 lesion\nsegmentation in Computerized Tomography (CT) scans is presented, which can also\nbe used as a benchmark for testing medical image segmentation models. Four DL\narchitectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly\ninitialized and pretrained encoders (variations of VGG, DenseNet, ResNet,\nResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct\n200 tested models. Three experimental setups are conducted for lung\nsegmentation, lesion segmentation and lesion segmentation using the original\nlung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20\nfor validation) is used for training/validation and a different public dataset\nconsisting of 829 images from 9 CT scan volumes for testing. Multiple findings\nare provided including the best architecture-encoder models for each experiment\nas well as mean Dice results for each experiment, architecture and encoder\nindependently. Finally, the upper bounds improvements when using lung masks as\na preprocessing step or when using pretrained models are quantified. The source\ncode and 600 pretrained models for the three experiments are provided, suitable\nfor fine-tuning in experimental setups without GPU capabilities.",
          "link": "http://arxiv.org/abs/2009.06412",
          "publishedOn": "2021-08-19T01:35:00.632Z",
          "wordCount": 767,
          "title": "Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans. (arXiv:2009.06412v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "We initiate the study of multi-stage episodic reinforcement learning under\nadversarial corruptions in both the rewards and the transition probabilities of\nthe underlying system extending recent results for the special case of\nstochastic bandits. We provide a framework which modifies the aggressive\nexploration enjoyed by existing reinforcement learning approaches based on\n\"optimism in the face of uncertainty\", by complementing them with principles\nfrom \"action elimination\". Importantly, our framework circumvents the major\nchallenges posed by naively applying action elimination in the RL setting, as\nformalized by a lower bound we demonstrate. Our framework yields efficient\nalgorithms which (a) attain near-optimal regret in the absence of corruptions\nand (b) adapt to unknown levels corruption, enjoying regret guarantees which\ndegrade gracefully in the total corruption encountered. To showcase the\ngenerality of our approach, we derive results for both tabular settings (where\nstates and actions are finite) as well as linear-function-approximation\nsettings (where the dynamics and rewards admit a linear underlying\nrepresentation). Notably, our work provides the first sublinear regret\nguarantee which accommodates any deviation from purely i.i.d. transitions in\nthe bandit-feedback model for episodic reinforcement learning.",
          "link": "http://arxiv.org/abs/1911.08689",
          "publishedOn": "2021-08-19T01:35:00.625Z",
          "wordCount": 669,
          "title": "Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1\">Martin Rinard</a>",
          "description": "Deep neural networks are an attractive tool for compressing the control\npolicy lookup tables in systems such as the Airborne Collision Avoidance System\n(ACAS). It is vital to ensure the safety of such neural controllers via\nverification techniques. The problem of analyzing ACAS Xu networks has\nmotivated many successful neural network verifiers. These verifiers typically\nanalyze the internal computation of neural networks to decide whether a\nproperty regarding the input/output holds. The intrinsic complexity of neural\nnetwork computation renders such verifiers slow to run and vulnerable to\nfloating-point error.\n\nThis paper revisits the original problem of verifying ACAS Xu networks. The\nnetworks take low-dimensional sensory inputs with training data provided by a\nprecomputed lookup table. We propose to prepend an input quantization layer to\nthe network. Quantization allows efficient verification via input state\nenumeration, whose complexity is bounded by the size of the quantization space.\nQuantization is equivalent to nearest-neighbor interpolation at run time, which\nhas been shown to provide acceptable accuracy for ACAS in simulation. Moreover,\nour technique can deliver exact verification results immune to floating-point\nerror if we directly enumerate the network outputs on the target inference\nimplementation or on an accurate simulation of the target implementation.",
          "link": "http://arxiv.org/abs/2108.07961",
          "publishedOn": "2021-08-19T01:35:00.574Z",
          "wordCount": 634,
          "title": "Verifying Low-dimensional Input Neural Networks via Input Quantization. (arXiv:2108.07961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11589",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rawlinson_D/0/1/0/all/0/1\">David Rawlinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ahmed_A/0/1/0/all/0/1\">Abdelrahman Ahmed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kowadlo_G/0/1/0/all/0/1\">Gideon Kowadlo</a>",
          "description": "We present a recurrent neural network memory that uses sparse coding to\ncreate a combinatoric encoding of sequential inputs. Using several examples, we\nshow that the network can associate distant causes and effects in a discrete\nstochastic process, predict partially-observable higher-order sequences, and\nenable a DQN agent to navigate a maze by giving it memory. The network uses\nonly biologically-plausible, local and immediate credit assignment. Memory\nrequirements are typically one order of magnitude less than existing LSTM, GRU\nand autoregressive feed-forward sequence learning models. The most significant\nlimitation of the memory is generalization to unseen input sequences. We\nexplore this limitation by measuring next-word prediction perplexity on the\nPenn Treebank dataset.",
          "link": "http://arxiv.org/abs/1905.11589",
          "publishedOn": "2021-08-19T01:35:00.470Z",
          "wordCount": 590,
          "title": "Learning distant cause and effect using only local and immediate credit assignment. (arXiv:1905.11589v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Pengcheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tei_K/0/1/0/all/0/1\">Kenji Tei</a>",
          "description": "Users interacting with a UI-embedded machine or system are typically obliged\nto perform their actions in a pre-determined order, to successfully achieve\ncertain functional goals. However, such obligations are often not followed\nstrictly by users, which may lead to the violation to security properties,\nespecially in security-critical systems. In order to improve the security with\nthe awareness of unexpected user behaviors, a system can be redesigned to a\nmore robust one by changing the order of actions in its specification.\nMeanwhile, we anticipate that the functionalities would remain consistent\nfollowing the modifications. In this paper, we propose an efficient algorithm\nto automatically produce specification revisions tackling with attack scenarios\ncaused by the weakened user obligations. By our algorithm, all the revisions\nmaintain the integrity of the functionalities as the original specification,\nwhich are generated using a novel recomposition approach. Then, the qualified\nrevisions that can satisfy the security requirements would be efficiently\nspotted by a hybrid approach combining model checking and machine learning\ntechniques. We evaluate our algorithm by comparing its performance with a\nstate-of-the-art approach regarding their coverage and searching speed of the\ndesirable revisions.",
          "link": "http://arxiv.org/abs/2108.08282",
          "publishedOn": "2021-08-19T01:35:00.454Z",
          "wordCount": 635,
          "title": "OACAL: Finding Module-Consistent Solutions to Weaken User Obligations. (arXiv:2108.08282v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weier Wan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kubendran_R/0/1/0/all/0/1\">Rajkumar Kubendran</a> (2 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1\">Clemens Schaefer</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Eryilmaz_S/0/1/0/all/0/1\">S. Burc Eryilmaz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dabin Wu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Deiss_S/0/1/0/all/0/1\">Stephen Deiss</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Raina_P/0/1/0/all/0/1\">Priyanka Raina</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">He Qian</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin Gao</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Siddharth Joshi</a> (4 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huaqiang Wu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wong_H/0/1/0/all/0/1\">H.-S. Philip Wong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cauwenberghs_G/0/1/0/all/0/1\">Gert Cauwenberghs</a> (2) ((1) Stanford University, (2) University of California San Diego, (3) Tsinghua University, (4) University of Notre Dame, (5) University of Pittsburgh)",
          "description": "Realizing today's cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental \"memory wall\": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.",
          "link": "http://arxiv.org/abs/2108.07879",
          "publishedOn": "2021-08-19T01:35:00.445Z",
          "wordCount": 778,
          "title": "Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory. (arXiv:2108.07879v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Otness_K/0/1/0/all/0/1\">Karl Otness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gjoka_A/0/1/0/all/0/1\">Arvi Gjoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_T/0/1/0/all/0/1\">Teseo Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>",
          "description": "Simulating physical systems is a core component of scientific computing,\nencompassing a wide range of physical domains and applications. Recently, there\nhas been a surge in data-driven methods to complement traditional numerical\nsimulations methods, motivated by the opportunity to reduce computational costs\nand/or learn new physical models leveraging access to large collections of\ndata. However, the diversity of problem settings and applications has led to a\nplethora of approaches, each one evaluated on a different setup and with\ndifferent evaluation metrics. We introduce a set of benchmark problems to take\na step towards unified benchmarks and evaluation protocols. We propose four\nrepresentative physical systems, as well as a collection of both widely used\nclassical time integrators and representative data-driven methods\n(kernel-based, MLP, CNN, nearest neighbors). Our framework allows evaluating\nobjectively and systematically the stability, accuracy, and computational\nefficiency of data-driven methods. Additionally, it is configurable to permit\nadjustments for accommodating other learning tasks and for establishing a\nfoundation for future developments in machine learning for scientific\ncomputing.",
          "link": "http://arxiv.org/abs/2108.07799",
          "publishedOn": "2021-08-19T01:35:00.423Z",
          "wordCount": 622,
          "title": "An Extensible Benchmark Suite for Learning to Simulate Physical Systems. (arXiv:2108.07799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>",
          "description": "Recently, FCNs have attracted widespread attention in the CD field. In\npursuit of better CD performance, it has become a tendency to design deeper and\nmore complicated FCNs, which inevitably brings about huge numbers of parameters\nand an unbearable computational burden. With the goal of designing a quite deep\narchitecture to obtain more precise CD results while simultaneously decreasing\nparameter numbers to improve efficiency, in this work, we present a very deep\nand efficient CD network, entitled EffCDNet. In EffCDNet, to reduce the\nnumerous parameters associated with deep architecture, an efficient convolution\nconsisting of depth-wise convolution and group convolution with a channel\nshuffle mechanism is introduced to replace standard convolutional layers. In\nterms of the specific network architecture, EffCDNet does not use mainstream\nUNet-like architecture, but rather adopts the architecture with a very deep\nencoder and a lightweight decoder. In the very deep encoder, two very deep\nsiamese streams stacked by efficient convolution first extract two highly\nrepresentative and informative feature maps from input image-pairs.\nSubsequently, an efficient ASPP module is designed to capture multi-scale\nchange information. In the lightweight decoder, a recurrent criss-cross\nself-attention (RCCA) module is applied to efficiently utilize non-local\nsimilar feature representations to enhance discriminability for each pixel,\nthus effectively separating the changed and unchanged regions. Moreover, to\ntackle the optimization problem in confused pixels, two novel loss functions\nbased on information entropy are presented. On two challenging CD datasets, our\napproach outperforms other SOTA FCN-based methods, with only benchmark-level\nparameter numbers and quite low computational overhead.",
          "link": "http://arxiv.org/abs/2108.08157",
          "publishedOn": "2021-08-19T01:35:00.250Z",
          "wordCount": 716,
          "title": "Towards Deep and Efficient: A Deep Siamese Self-Attention Fully Efficient Convolutional Network for Change Detection in VHR Images. (arXiv:2108.08157v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1\">Bojia Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Adversarial training is one effective approach for training robust deep\nneural networks against adversarial attacks. While being able to bring reliable\nrobustness, adversarial training (AT) methods in general favor high capacity\nmodels, i.e., the larger the model the better the robustness. This tends to\nlimit their effectiveness on small models, which are more preferable in\nscenarios where storage or computing resources are very limited (e.g., mobile\ndevices). In this paper, we leverage the concept of knowledge distillation to\nimprove the robustness of small models by distilling from adversarially trained\nlarge models. We first revisit several state-of-the-art AT methods from a\ndistillation perspective and identify one common technique that can lead to\nimproved robustness: the use of robust soft labels -- predictions of a robust\nmodel. Following this observation, we propose a novel adversarial robustness\ndistillation method called Robust Soft Label Adversarial Distillation (RSLAD)\nto train robust small student models. RSLAD fully exploits the robust soft\nlabels produced by a robust (adversarially-trained) large teacher model to\nguide the student's learning on both natural and adversarial examples in all\nloss terms. We empirically demonstrate the effectiveness of our RSLAD approach\nover existing adversarial training and distillation methods in improving the\nrobustness of small models against state-of-the-art attacks including the\nAutoAttack. We also provide a set of understandings on our RSLAD and the\nimportance of robust soft labels for adversarial robustness distillation.",
          "link": "http://arxiv.org/abs/2108.07969",
          "publishedOn": "2021-08-19T01:35:00.225Z",
          "wordCount": 668,
          "title": "Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better. (arXiv:2108.07969v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belton_N/0/1/0/all/0/1\">Niamh Belton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welaratne_I/0/1/0/all/0/1\">Ivan Welaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlan_A/0/1/0/all/0/1\">Adil Dahlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearne_R/0/1/0/all/0/1\">Ronan T Hearne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1\">Misgina Tsighe Hagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1\">Aonghus Lawlor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1\">Kathleen M. Curran</a>",
          "description": "This work employs a pre-trained, multi-view Convolutional Neural Network\n(CNN) with a spatial attention block to optimise knee injury detection. An\nopen-source Magnetic Resonance Imaging (MRI) data set with image-level labels\nwas leveraged for this analysis. As MRI data is acquired from three planes, we\ncompare our technique using data from a single-plane and multiple planes\n(multi-plane). For multi-plane, we investigate various methods of fusing the\nplanes in the network. This analysis resulted in the novel 'MPFuseNet' network\nand state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior\nCruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977\nand 0.957 respectively. We then developed an objective metric, Penalised\nLocalisation Accuracy (PLA), to validate the model's localisation ability. This\nmetric compares binary masks generated from Grad-Cam output and the\nradiologist's annotations on a sample of MRIs. We also extracted explainability\nfeatures in a model-agnostic approach that were then verified as clinically\nrelevant by the radiologist.",
          "link": "http://arxiv.org/abs/2108.08136",
          "publishedOn": "2021-08-19T01:35:00.194Z",
          "wordCount": 623,
          "title": "Optimising Knee Injury Detection with Spatial Attention and Validating Localisation Ability. (arXiv:2108.08136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olson_M/0/1/0/all/0/1\">Matthew L. Olson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuy-Vy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_G/0/1/0/all/0/1\">Gaurav Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratzlaff_N/0/1/0/all/0/1\">Neale Ratzlaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng-Keen Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1\">Minsuk Kahng</a>",
          "description": "Identifying covariate shift is crucial for making machine learning systems\nrobust in the real world and for detecting training data biases that are not\nreflected in test data. However, detecting covariate shift is challenging,\nespecially when the data consists of high-dimensional images, and when multiple\ntypes of localized covariate shift affect different subspaces of the data.\nAlthough automated techniques can be used to detect the existence of covariate\nshift, our goal is to help human users characterize the extent of covariate\nshift in large image datasets with interfaces that seamlessly integrate\ninformation obtained from the detection algorithms. In this paper, we design\nand evaluate a new visual interface that facilitates the comparison of the\nlocal distributions of training and test data. We conduct a quantitative user\nstudy on multi-attribute facial data to compare two different learned\nlow-dimensional latent representations (pretrained ImageNet CNN vs. density\nratio) and two user analytic workflows (nearest-neighbor vs.\ncluster-to-cluster). Our results indicate that the latent representation of our\ndensity ratio model, combined with a nearest-neighbor comparison, is the most\neffective at helping humans identify covariate shift.",
          "link": "http://arxiv.org/abs/2108.08000",
          "publishedOn": "2021-08-19T01:35:00.187Z",
          "wordCount": 622,
          "title": "Contrastive Identification of Covariate Shift in Image Data. (arXiv:2108.08000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08129",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin De Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "We establish the uniform in time stability, w.r.t. the marginals, of the\nIterative Proportional Fitting Procedure, also known as Sinkhorn algorithm,\nused to solve entropy-regularised Optimal Transport problems. Our result is\nquantitative and stated in terms of the 1-Wasserstein metric. As a corollary we\nestablish a quantitative stability result for Schr\\\"odinger bridges.",
          "link": "http://arxiv.org/abs/2108.08129",
          "publishedOn": "2021-08-19T01:35:00.075Z",
          "wordCount": 499,
          "title": "Quantitative Uniform Stability of the Iterative Proportional Fitting Procedure. (arXiv:2108.08129v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhirong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedov_D/0/1/0/all/0/1\">Denis Sedov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corander_J/0/1/0/all/0/1\">Jukka Corander</a>",
          "description": "Neighbor Embedding (NE) that aims to preserve pairwise similarities between\ndata items has been shown to yield an effective principle for data\nvisualization. However, even the currently best NE methods such as Stochastic\nNeighbor Embedding (SNE) may leave large-scale patterns such as clusters hidden\ndespite of strong signals being present in the data. To address this, we\npropose a new cluster visualization method based on Neighbor Embedding. We\nfirst present a family of Neighbor Embedding methods which generalizes SNE by\nusing non-normalized Kullback-Leibler divergence with a scale parameter. In\nthis family, much better cluster visualizations often appear with a parameter\nvalue different from the one corresponding to SNE. We also develop an efficient\nsoftware which employs asynchronous stochastic block coordinate descent to\noptimize the new family of objective functions. The experimental results\ndemonstrate that our method consistently and substantially improves\nvisualization of data clusters compared with the state-of-the-art NE\napproaches.",
          "link": "http://arxiv.org/abs/2108.08003",
          "publishedOn": "2021-08-19T01:35:00.028Z",
          "wordCount": 572,
          "title": "Stochastic Cluster Embedding. (arXiv:2108.08003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberle_S/0/1/0/all/0/1\">Simon Eberle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Georg S. Weiss</a>",
          "description": "The training of artificial neural networks (ANNs) with rectified linear unit\n(ReLU) activation via gradient descent (GD) type optimization schemes is\nnowadays a common industrially relevant procedure. Till this day in the\nscientific literature there is in general no mathematical convergence analysis\nwhich explains the numerical success of GD type optimization schemes in the\ntraining of ANNs with ReLU activation. GD type optimization schemes can be\nregarded as temporal discretization methods for the gradient flow (GF)\ndifferential equations associated to the considered optimization problem and,\nin view of this, it seems to be a natural direction of research to first aim to\ndevelop a mathematical convergence theory for time-continuous GF differential\nequations and, thereafter, to aim to extend such a time-continuous convergence\ntheory to implementable time-discrete GD type optimization methods. In this\narticle we establish two basic results for GF differential equations in the\ntraining of fully-connected feedforward ANNs with one hidden layer and ReLU\nactivation. In the first main result of this article we establish in the\ntraining of such ANNs under the assumption that the probability distribution of\nthe input data of the considered supervised learning problem is absolutely\ncontinuous with a bounded density function that every GF differential equation\nadmits for every initial value a solution which is also unique among a suitable\nclass of solutions. In the second main result of this article we prove in the\ntraining of such ANNs under the assumption that the target function and the\ndensity function of the probability distribution of the input data are\npiecewise polynomial that every non-divergent GF trajectory converges with an\nappropriate rate of convergence to a critical point and that the risk of the\nnon-divergent GF trajectory converges with rate 1 to the risk of the critical\npoint.",
          "link": "http://arxiv.org/abs/2108.08106",
          "publishedOn": "2021-08-19T01:34:59.912Z",
          "wordCount": 764,
          "title": "Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2108.08106v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08095",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taha_T/0/1/0/all/0/1\">Thiab Taha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>",
          "description": "Medical Imaging is one of the growing fields in the world of computer vision.\nIn this study, we aim to address the Diabetic Retinopathy (DR) problem as one\nof the open challenges in medical imaging. In this research, we propose a new\nlesion detection architecture, comprising of two sub-modules, which is an\noptimal solution to detect and find not only the type of lesions caused by DR,\ntheir corresponding bounding boxes, and their masks; but also the severity\nlevel of the overall case. Aside from traditional accuracy, we also use two\npopular evaluation criteria to evaluate the outputs of our models, which are\nintersection over union (IOU) and mean average precision (mAP). We hypothesize\nthat this new solution enables specialists to detect lesions with high\nconfidence and estimate the severity of the damage with high accuracy.",
          "link": "http://arxiv.org/abs/2108.08095",
          "publishedOn": "2021-08-19T01:34:59.662Z",
          "wordCount": 620,
          "title": "DRDrV3: Complete Lesion Detection in Fundus Images Using Mask R-CNN, Transfer Learning, and LSTM. (arXiv:2108.08095v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07992",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Le_K/0/1/0/all/0/1\">Khang Le</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Nguyen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1\">Tung Pham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>",
          "description": "We study the multi-marginal partial optimal transport (POT) problem between\n$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove\nthat we can obtain two equivalence forms of the multimarginal POT problem in\nterms of the multimarginal optimal transport problem via novel extensions of\ncost tensor. The first equivalence form is derived under the assumptions that\nthe total masses of each measure are sufficiently close while the second\nequivalence form does not require any conditions on these masses but at the\nprice of more sophisticated extended cost tensor. Our proof techniques for\nobtaining these equivalence forms rely on novel procedures of moving mass in\ngraph theory to push transportation plan into appropriate regions. Finally,\nbased on the equivalence forms, we develop optimization algorithm, named\nApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the\nentropic regularized multimarginal optimal transport. We demonstrate that the\nApproxMPOT algorithm can approximate the optimal value of multimarginal POT\nproblem with a computational complexity upper bound of the order\n$\\tilde{\\mathcal{O}}(m^3(n+1)^{m}/ \\varepsilon^2)$ where $\\varepsilon > 0$\nstands for the desired tolerance.",
          "link": "http://arxiv.org/abs/2108.07992",
          "publishedOn": "2021-08-19T01:34:59.592Z",
          "wordCount": 645,
          "title": "On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity. (arXiv:2108.07992v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>",
          "description": "Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.",
          "link": "http://arxiv.org/abs/2105.07674",
          "publishedOn": "2021-08-18T01:55:00.944Z",
          "wordCount": 581,
          "title": "Continual Learning with Echo State Networks. (arXiv:2105.07674v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.",
          "link": "http://arxiv.org/abs/2108.07253",
          "publishedOn": "2021-08-18T01:55:00.937Z",
          "wordCount": 609,
          "title": "Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coma_Puig_B/0/1/0/all/0/1\">Bernat Coma-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmona_J/0/1/0/all/0/1\">Josep Carmona</a>",
          "description": "Implementing systems based on Machine Learning to detect fraud and other\nNon-Technical Losses (NTL) is challenging: the data available is biased, and\nthe algorithms currently used are black-boxes that cannot be either easily\ntrusted or understood by stakeholders. This work explains our human-in-the-loop\napproach to mitigate these problems in a real system that uses a supervised\nmodel to detect Non-Technical Losses (NTL) for an international utility company\nfrom Spain. This approach exploits human knowledge (e.g. from the data\nscientists or the company's stakeholders) and the information provided by\nexplanatory methods to guide the system during the training process. This\nsimple, efficient method that can be easily implemented in other industrial\nprojects is tested in a real dataset and the results show that the derived\nprediction model is better in terms of accuracy, interpretability, robustness\nand flexibility.",
          "link": "http://arxiv.org/abs/2009.13437",
          "publishedOn": "2021-08-18T01:55:00.903Z",
          "wordCount": 605,
          "title": "A Human-in-the-Loop Approach based on Explainability to Improve NTL Detection. (arXiv:2009.13437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1\">Martin Svedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steven W. D. Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.",
          "link": "http://arxiv.org/abs/2107.06676",
          "publishedOn": "2021-08-18T01:55:00.895Z",
          "wordCount": 620,
          "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "The past decade has seen the rapid development of Reinforcement Learning,\nwhich acquires impressive performance with numerous training resources.\nHowever, one of the greatest challenges in RL is generalization efficiency\n(i.e., generalization performance in a unit time). This paper proposes a\nframework of Active Reinforcement Learning (ARL) over MDPs to improve\ngeneralization efficiency in a limited resource by instance selection. Given a\nnumber of instances, the algorithm chooses out valuable instances as training\nsets while training the policy, thereby costing fewer resources. Unlike\nexisting approaches, we attempt to actively select and use training data rather\nthan train on all the given data, thereby costing fewer resources. Furthermore,\nwe introduce a general instance evaluation metrics and selection mechanism into\nthe framework. Experiments results reveal that the proposed framework with\nProximal Policy Optimization as policy optimizer can effectively improve\ngeneralization efficiency than unselect-ed and unbiased selected methods.",
          "link": "http://arxiv.org/abs/2108.02323",
          "publishedOn": "2021-08-18T01:55:00.860Z",
          "wordCount": 620,
          "title": "Active Reinforcement Learning over MDPs. (arXiv:2108.02323v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1\">Charles Vorbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Imitation learning enables high-fidelity, vision-based learning of policies\nwithin rich, photorealistic environments. However, such techniques often rely\non traditional discrete-time neural models and face difficulties in\ngeneralizing to domain shifts by failing to account for the causal\nrelationships between the agent and the environment. In this paper, we propose\na theoretical and experimental framework for learning causal representations\nusing continuous-time neural networks, specifically over their discrete-time\ncounterparts. We evaluate our method in the context of visual-control learning\nof drones over a series of complex tasks, ranging from short- and long-term\nnavigation, to chasing static and dynamic objects through photorealistic\nenvironments. Our results demonstrate that causal continuous-time deep models\ncan perform robust navigation tasks, where advanced recurrent models fail.\nThese models learn complex causal control representations directly from raw\nvisual inputs and scale to solve a variety of tasks using imitation learning.",
          "link": "http://arxiv.org/abs/2106.08314",
          "publishedOn": "2021-08-18T01:55:00.852Z",
          "wordCount": 614,
          "title": "Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>",
          "description": "People navigating in unfamiliar buildings take advantage of myriad visual,\nspatial and semantic cues to efficiently achieve their navigation goals.\nTowards equipping computational agents with similar capabilities, we introduce\nPathdreamer, a visual world model for agents navigating in novel indoor\nenvironments. Given one or more previous visual observations, Pathdreamer\ngenerates plausible high-resolution 360 visual observations (RGB, semantic\nsegmentation and depth) for viewpoints that have not been visited, in buildings\nnot seen during training. In regions of high uncertainty (e.g. predicting\naround corners, imagining the contents of an unseen room), Pathdreamer can\npredict diverse scenes, allowing an agent to sample multiple realistic outcomes\nfor a given trajectory. We demonstrate that Pathdreamer encodes useful and\naccessible visual, spatial and semantic knowledge about human environments by\nusing it in the downstream task of Vision-and-Language Navigation (VLN).\nSpecifically, we show that planning ahead with Pathdreamer brings about half\nthe benefit of looking ahead at actual observations from unobserved parts of\nthe environment. We hope that Pathdreamer will help unlock model-based\napproaches to challenging embodied navigation tasks such as navigating to\nspecified objects and VLN.",
          "link": "http://arxiv.org/abs/2105.08756",
          "publishedOn": "2021-08-18T01:55:00.800Z",
          "wordCount": 657,
          "title": "Pathdreamer: A World Model for Indoor Navigation. (arXiv:2105.08756v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme\nbrings greater efficiency by limiting self-attention computation to\nnon-overlapping local windows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model at various scales\nand has linear computational complexity with respect to image size. These\nqualities of Swin Transformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and\ndense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP\non COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its\nperformance surpasses the previous state-of-the-art by a large margin of +2.7\nbox AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the\npotential of Transformer-based models as vision backbones. The hierarchical\ndesign and the shifted window approach also prove beneficial for all-MLP\narchitectures. The code and models are publicly available\nat~\\url{https://github.com/microsoft/Swin-Transformer}.",
          "link": "http://arxiv.org/abs/2103.14030",
          "publishedOn": "2021-08-18T01:55:00.791Z",
          "wordCount": 701,
          "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. (arXiv:2103.14030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>",
          "description": "With the fast development of quantum computing and deep learning, quantum\nneural networks have attracted great attention recently. By leveraging the\npower of quantum computing, deep neural networks can potentially overcome\ncomputational power limitations in classic machine learning. However, when\nmultiple quantum machines wish to train a global model using the local data on\neach machine, it may be very difficult to copy the data into one machine and\ntrain the model. Therefore, a collaborative quantum neural network framework is\nnecessary. In this article, we borrow the core idea of federated learning to\npropose QuantumFed, a quantum federated learning framework to have multiple\nquantum nodes with local quantum data train a mode together. Our experiments\nshow the feasibility and robustness of our framework.",
          "link": "http://arxiv.org/abs/2106.09109",
          "publishedOn": "2021-08-18T01:55:00.785Z",
          "wordCount": 601,
          "title": "QuantumFed: A Federated Learning Framework for Collaborative Quantum Training. (arXiv:2106.09109v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomlinson_K/0/1/0/all/0/1\">Kiran Tomlinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugander_J/0/1/0/all/0/1\">Johan Ugander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1\">Austin R. Benson</a>",
          "description": "Standard methods in preference learning involve estimating the parameters of\ndiscrete choice models from data of selections (choices) made by individuals\nfrom a discrete set of alternatives (the choice set). While there are many\nmodels for individual preferences, existing learning methods overlook how\nchoice set assignment affects the data. Often, the choice set itself is\ninfluenced by an individual's preferences; for instance, a consumer choosing a\nproduct from an online retailer is often presented with options from a\nrecommender system that depend on information about the consumer's preferences.\nIgnoring these assignment mechanisms can mislead choice models into making\nbiased estimates of preferences, a phenomenon that we call choice set\nconfounding; we demonstrate the presence of such confounding in widely-used\nchoice datasets.\n\nTo address this issue, we adapt methods from causal inference to the discrete\nchoice setting. We use covariates of the chooser for inverse probability\nweighting and/or regression controls, accurately recovering individual\npreferences in the presence of choice set confounding under certain\nassumptions. When such covariates are unavailable or inadequate, we develop\nmethods that take advantage of structured choice set assignment to improve\nprediction. We demonstrate the effectiveness of our methods on real-world\nchoice data, showing, for example, that accounting for choice set confounding\nmakes choices observed in hotel booking and commute transportation more\nconsistent with rational utility-maximization.",
          "link": "http://arxiv.org/abs/2105.07959",
          "publishedOn": "2021-08-18T01:55:00.778Z",
          "wordCount": 689,
          "title": "Choice Set Confounding in Discrete Choice. (arXiv:2105.07959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hanxiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1\">Helena Kotthaus</a>",
          "description": "In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose a point cloud-applicable explainability approach\nbased on local surrogate model-based method to show which components contribute\nto the classification. Moreover, we propose quantitative fidelity validations\nfor generated explanations that enhance the persuasive power of explainability\nand compare the plausibility of different existing point cloud-applicable\nexplainability methods. Our new explainability approach provides a fairly\naccurate, more semantically coherent and widely applicable explanation for\npoint cloud classification tasks. Our code is available at\nhttps://github.com/Explain3D/LIME-3D",
          "link": "http://arxiv.org/abs/2107.13459",
          "publishedOn": "2021-08-18T01:55:00.760Z",
          "wordCount": 597,
          "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs. (arXiv:2107.13459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhikang T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Despite the empirical success of the deep Q network (DQN) reinforcement\nlearning algorithm and its variants, DQN is still not well understood and it\ndoes not guarantee convergence. In this work, we show that DQN can diverge and\ncease to operate in realistic settings. Although there exist gradient-based\nconvergent methods, we show that they actually have inherent problems in\nlearning behaviour and elucidate why they often fail in practice. To overcome\nthese problems, we propose a convergent DQN algorithm (C-DQN) by carefully\nmodifying DQN, and we show that the algorithm is convergent and can work with\nlarge discount factors (0.9998). It learns robustly in difficult settings and\ncan learn several difficult games in the Atari 2600 benchmark where DQN fail,\nwithin a moderate computational budget. Our codes have been publicly released\nand can be used to reproduce our results.",
          "link": "http://arxiv.org/abs/2106.15419",
          "publishedOn": "2021-08-18T01:55:00.753Z",
          "wordCount": 597,
          "title": "A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1\">Kritika Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1\">Andrew Trask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1\">Rickmer Braren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>",
          "description": "In recent years, formal methods of privacy protection such as differential\nprivacy (DP), capable of deployment to data-driven tasks such as machine\nlearning (ML), have emerged. Reconciling large-scale ML with the closed-form\nreasoning required for the principled analysis of individual privacy loss\nrequires the introduction of new tools for automatic sensitivity analysis and\nfor tracking an individual's data and their features through the flow of\ncomputation. For this purpose, we introduce a novel \\textit{hybrid} automatic\ndifferentiation (AD) system which combines the efficiency of reverse-mode AD\nwith an ability to obtain a closed-form expression for any given quantity in\nthe computational graph. This enables modelling the sensitivity of arbitrary\ndifferentiable function compositions, such as the training of neural networks\non private data. We demonstrate our approach by analysing the individual DP\nguarantees of statistical database queries. Moreover, we investigate the\napplication of our technique to the training of DP neural networks. Our\napproach can enable the principled reasoning about privacy loss in the setting\nof data processing, and further the development of automatic sensitivity\nanalysis and privacy budgeting systems.",
          "link": "http://arxiv.org/abs/2107.04265",
          "publishedOn": "2021-08-18T01:55:00.747Z",
          "wordCount": 675,
          "title": "Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Norcliffe_A/0/1/0/all/0/1\">Alexander Norcliffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1\">Cristian Bodnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Day_B/0/1/0/all/0/1\">Ben Day</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moss_J/0/1/0/all/0/1\">Jacob Moss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "Neural Ordinary Differential Equations (NODEs) use a neural network to model\nthe instantaneous rate of change in the state of a system. However, despite\ntheir apparent suitability for dynamics-governed time-series, NODEs present a\nfew disadvantages. First, they are unable to adapt to incoming data points, a\nfundamental requirement for real-time applications imposed by the natural\ndirection of time. Second, time series are often composed of a sparse set of\nmeasurements that could be explained by many possible underlying dynamics.\nNODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are\na family of models providing uncertainty estimation and fast data adaptation\nbut lack an explicit treatment of the flow of time. To address these problems,\nwe introduce Neural ODE Processes (NDPs), a new class of stochastic processes\ndetermined by a distribution over Neural ODEs. By maintaining an adaptive\ndata-dependent distribution over the underlying ODE, we show that our model can\nsuccessfully capture the dynamics of low-dimensional systems from just a few\ndata points. At the same time, we demonstrate that NDPs scale up to challenging\nhigh-dimensional time-series with unknown latent dynamics such as rotating\nMNIST digits.",
          "link": "http://arxiv.org/abs/2103.12413",
          "publishedOn": "2021-08-18T01:55:00.716Z",
          "wordCount": 648,
          "title": "Neural ODE Processes. (arXiv:2103.12413v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>",
          "description": "Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.",
          "link": "http://arxiv.org/abs/2101.06069",
          "publishedOn": "2021-08-18T01:55:00.708Z",
          "wordCount": 702,
          "title": "Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Causal graphs (CGs) are compact representations of the knowledge of the data\ngenerating processes behind the data distributions. When a CG is available,\ne.g., from the domain knowledge, we can infer the conditional independence (CI)\nrelations that should hold in the data distribution. However, it is not\nstraightforward how to incorporate this knowledge into predictive modeling. In\nthis work, we propose a model-agnostic data augmentation method that allows us\nto exploit the prior knowledge of the CI encoded in a CG for supervised machine\nlearning. We theoretically justify the proposed method by providing an excess\nrisk bound indicating that the proposed method suppresses overfitting by\nreducing the apparent complexity of the predictor hypothesis class. Using\nreal-world data with CGs provided by domain experts, we experimentally show\nthat the proposed method is effective in improving the prediction accuracy,\nespecially in the small-data regime.",
          "link": "http://arxiv.org/abs/2103.00136",
          "publishedOn": "2021-08-18T01:55:00.699Z",
          "wordCount": 637,
          "title": "Incorporating Causal Graphical Prior Knowledge into Predictive Modeling via Simple Data Augmentation. (arXiv:2103.00136v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jacek Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jakub Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1\">Witold Kraskiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1\">Mateusz Topolewski</a>",
          "description": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT, Helpdesk and BPI'12 datasets.",
          "link": "http://arxiv.org/abs/2107.08687",
          "publishedOn": "2021-08-18T01:55:00.690Z",
          "wordCount": 504,
          "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1\">Wilson E. Marc&#xed;lio-Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1\">Danilo M. Eler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Rog&#xe9;rio E. Garcia</a>",
          "description": "Cluster interpretation after dimensionality reduction (DR) is a ubiquitous\npart of exploring multidimensional datasets. DR results are frequently\nrepresented by scatterplots, where spatial proximity encodes similarity among\ndata samples. In the literature, techniques support the understanding of\nscatterplots' organization by visualizing the importance of the features for\ncluster definition with layout enrichment strategies. However, current\napproaches usually focus on global information, hampering the analysis whenever\nthe focus is to understand the differences among clusters. Thus, this paper\nintroduces a methodology to visually explore DR results and interpret clusters'\nformation based on contrastive analysis. We also introduce a bipartite graph to\nvisually interpret and explore the relationship between the statistical\nvariables employed to understand how the data features influence cluster\nformation. Our approach is demonstrated through case studies, in which we\nexplore two document collections related to news articles and tweets about\nCOVID-19 symptoms. Finally, we evaluate our approach through quantitative\nresults to demonstrate its robustness to support multidimensional analysis.",
          "link": "http://arxiv.org/abs/2101.12044",
          "publishedOn": "2021-08-18T01:55:00.684Z",
          "wordCount": 668,
          "title": "Contrastive analysis for scatterplot-based representations of dimensionality reduction. (arXiv:2101.12044v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Samuel G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "Automatic augmentation methods have recently become a crucial pillar for\nstrong model performance in vision tasks. While existing automatic augmentation\nmethods need to trade off simplicity, cost and performance, we present a most\nsimple baseline, TrivialAugment, that outperforms previous methods for almost\nfree. TrivialAugment is parameter-free and only applies a single augmentation\nto each image. Thus, TrivialAugment's effectiveness is very unexpected to us\nand we performed very thorough experiments to study its performance. First, we\ncompare TrivialAugment to previous state-of-the-art methods in a variety of\nimage classification scenarios. Then, we perform multiple ablation studies with\ndifferent augmentation spaces, augmentation methods and setups to understand\nthe crucial requirements for its performance. Additionally, we provide a simple\ninterface to facilitate the widespread adoption of automatic augmentation\nmethods, as well as our full code base for reproducibility. Since our work\nreveals a stagnation in many parts of automatic augmentation research, we end\nwith a short proposal of best practices for sustained future progress in\nautomatic augmentation methods.",
          "link": "http://arxiv.org/abs/2103.10158",
          "publishedOn": "2021-08-18T01:55:00.664Z",
          "wordCount": 630,
          "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation. (arXiv:2103.10158v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1\">Benjamin Bloem-Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Most data is automatically collected and only ever \"seen\" by algorithms. Yet,\ndata compressors preserve perceptual fidelity rather than just the information\nneeded by algorithms performing downstream tasks. In this paper, we\ncharacterize the bit-rate required to ensure high performance on all predictive\ntasks that are invariant under a set of transformations, such as data\naugmentations. Based on our theory, we design unsupervised objectives for\ntraining neural compressors. Using these objectives, we train a generic image\ncompressor that achieves substantial rate savings (more than $1000\\times$ on\nImageNet) compared to JPEG on 8 datasets, without decreasing downstream\nclassification performance.",
          "link": "http://arxiv.org/abs/2106.10800",
          "publishedOn": "2021-08-18T01:55:00.657Z",
          "wordCount": 572,
          "title": "Lossy Compression for Lossless Prediction. (arXiv:2106.10800v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1\">Aurelien Lucchi</a>",
          "description": "Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning. We release our code at\nhttps://github.com/dariopavllo/textured-3d-gan",
          "link": "http://arxiv.org/abs/2103.15627",
          "publishedOn": "2021-08-18T01:55:00.648Z",
          "wordCount": 642,
          "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images. (arXiv:2103.15627v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ledent_A/0/1/0/all/0/1\">Antoine Ledent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Rodrigo Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>",
          "description": "We propose orthogonal inductive matrix completion (OMIC), an interpretable\napproach to matrix completion based on a sum of multiple orthonormal side\ninformation terms, together with nuclear-norm regularization. The approach\nallows us to inject prior knowledge about the singular vectors of the ground\ntruth matrix. We optimize the approach by a provably converging algorithm,\nwhich optimizes all components of the model simultaneously. We study the\ngeneralization capabilities of our method in both the distribution-free setting\nand in the case where the sampling distribution admits uniform marginals,\nyielding learning guarantees that improve with the quality of the injected\nknowledge in both cases. As particular cases of our framework, we present\nmodels which can incorporate user and item biases or community information in a\njoint and additive fashion. We analyse the performance of OMIC on several\nsynthetic and real datasets. On synthetic datasets with a sliding scale of user\nbias relevance, we show that OMIC better adapts to different regimes than other\nmethods. On real-life datasets containing user/items recommendations and\nrelevant side information, we find that OMIC surpasses the state-of-the-art,\nwith the added benefit of greater interpretability.",
          "link": "http://arxiv.org/abs/2004.01653",
          "publishedOn": "2021-08-18T01:55:00.640Z",
          "wordCount": 676,
          "title": "Orthogonal Inductive Matrix Completion. (arXiv:2004.01653v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02480",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guitart_A/0/1/0/all/0/1\">Anna Guitart</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rio_A/0/1/0/all/0/1\">Ana Fern&#xe1;ndez del R&#xed;o</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Perianez_A/0/1/0/all/0/1\">&#xc1;frica Peri&#xe1;&#xf1;ez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bellhouse_L/0/1/0/all/0/1\">Lauren Bellhouse</a>",
          "description": "Every day, 800 women and 6,700 newborns die from complications related to\npregnancy or childbirth. A well-trained midwife can prevent most of these\nmaternal and newborn deaths. Data science models together with logs generated\nby users of online learning applications for midwives can help to improve their\nlearning competencies. The goal is to use these rich behavioral data to push\ndigital learning towards personalized content and to provide an adaptive\nlearning journey. In this work, we evaluate various forecasting methods to\ndetermine the interest of future users on the different kind of contents\navailable in the app, broken down by profession and region.",
          "link": "http://arxiv.org/abs/2107.02480",
          "publishedOn": "2021-08-18T01:55:00.630Z",
          "wordCount": 565,
          "title": "Midwifery Learning and Forecasting: Predicting Content Demand with User-Generated Logs. (arXiv:2107.02480v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Omari_R/0/1/0/all/0/1\">Rollin Omari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_R/0/1/0/all/0/1\">R. I. McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Raven's Progressive Matrices have been widely used for measuring abstract\nreasoning and intelligence in humans. However for artificial learning systems,\nabstract reasoning remains a challenging problem. In this paper we investigate\nhow neural networks augmented with biologically inspired spiking modules gain a\nsignificant advantage in solving this problem. To illustrate this, we first\ninvestigate the performance of our networks with supervised learning, then with\nunsupervised learning. Experiments on the RAVEN dataset show that the overall\naccuracy of our supervised networks surpass human-level performance, while our\nunsupervised networks significantly outperform existing unsupervised methods.\nFinally, our results from both supervised and unsupervised learning illustrate\nthat, unlike their non-augmented counterparts, networks with spiking modules\nare able to extract and encode temporal features without any explicit\ninstruction, do not heavily rely on training data, and generalise more readily\nto new problems. In summary, the results reported here indicate that artificial\nneural networks with spiking modules are well suited to solving abstract\nreasoning.",
          "link": "http://arxiv.org/abs/2010.06746",
          "publishedOn": "2021-08-18T01:55:00.622Z",
          "wordCount": 672,
          "title": "Analogical and Relational Reasoning with Spiking Neural Networks. (arXiv:2010.06746v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>",
          "description": "Imitation Learning (IL) is an effective framework to learn visuomotor skills\nfrom offline demonstration data. However, IL methods often fail to generalize\nto new scene configurations not covered by training data. On the other hand,\nhumans can manipulate objects in varying conditions. Key to such capability is\nhand-eye coordination, a cognitive ability that enables humans to adaptively\ndirect their movements at task-relevant objects and be invariant to the\nobjects' absolute spatial location. In this work, we present a learnable action\nspace, Hand-eye Action Networks (HAN), that can approximate human's hand-eye\ncoordination behaviors by learning from human teleoperated demonstrations.\nThrough a set of challenging multi-stage manipulation tasks, we show that a\nvisuomotor policy equipped with HAN is able to inherit the key spatial\ninvariance property of hand-eye coordination and achieve zero-shot\ngeneralization to new scene configurations. Additional materials available at\nhttps://sites.google.com/stanford.edu/han",
          "link": "http://arxiv.org/abs/2103.00375",
          "publishedOn": "2021-08-18T01:55:00.604Z",
          "wordCount": 627,
          "title": "Generalization Through Hand-Eye Coordination: An Action Space for Learning Spatially-Invariant Visuomotor Control. (arXiv:2103.00375v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1\">Timothee Masquelier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have attracted enormous research interest due\nto temporal information processing capability, low power consumption, and high\nbiological plausibility. However, the formulation of efficient and\nhigh-performance learning algorithms for SNNs is still challenging. Most\nexisting learning methods learn weights only, and require manual tuning of the\nmembrane-related parameters that determine the dynamics of a single spiking\nneuron. These parameters are typically chosen to be the same for all neurons,\nwhich limits the diversity of neurons and thus the expressiveness of the\nresulting SNNs. In this paper, we take inspiration from the observation that\nmembrane-related parameters are different across brain regions, and propose a\ntraining algorithm that is capable of learning not only the synaptic weights\nbut also the membrane time constants of SNNs. We show that incorporating\nlearnable membrane time constants can make the network less sensitive to\ninitial values and can speed up learning. In addition, we reevaluate the\npooling methods in SNNs and find that max-pooling will not lead to significant\ninformation loss and have the advantage of low computation cost and binary\ncompatibility. We evaluate the proposed method for image classification tasks\non both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and\nneuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment\nresults show that the proposed method outperforms the state-of-the-art accuracy\non nearly all datasets, using fewer time-steps. Our codes are available at\nhttps://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.",
          "link": "http://arxiv.org/abs/2007.05785",
          "publishedOn": "2021-08-18T01:55:00.595Z",
          "wordCount": 750,
          "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks. (arXiv:2007.05785v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-08-18T01:55:00.587Z",
          "wordCount": 671,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1\">Christoph Molnar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Herbinger_J/0/1/0/all/0/1\">Julia Herbinger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dandl_S/0/1/0/all/0/1\">Susanne Dandl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholbeck_C/0/1/0/all/0/1\">Christian A. Scholbeck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "An increasing number of model-agnostic interpretation techniques for machine\nlearning (ML) models such as partial dependence plots (PDP), permutation\nfeature importance (PFI) and Shapley values provide insightful model\ninterpretations, but can lead to wrong conclusions if applied incorrectly. We\nhighlight many general pitfalls of ML model interpretation, such as using\ninterpretation techniques in the wrong context, interpreting models that do not\ngeneralize well, ignoring feature dependencies, interactions, uncertainty\nestimates and issues in high-dimensional settings, or making unjustified causal\ninterpretations, and illustrate them with examples. We focus on pitfalls for\nglobal methods that describe the average model behavior, but many pitfalls also\napply to local methods that explain individual predictions. Our paper addresses\nML practitioners by raising awareness of pitfalls and identifying solutions for\ncorrect model interpretation, but also addresses ML researchers by discussing\nopen issues for further research.",
          "link": "http://arxiv.org/abs/2007.04131",
          "publishedOn": "2021-08-18T01:55:00.580Z",
          "wordCount": 603,
          "title": "General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models. (arXiv:2007.04131v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>",
          "description": "Graph matching (GM) has been a building block in many areas including\ncomputer vision and pattern recognition. Despite the recent impressive\nprogress, existing deep GM methods often have difficulty in handling outliers\nin both graphs, which are ubiquitous in practice. We propose a deep\nreinforcement learning (RL) based approach RGM for weighted graph matching,\nwhose sequential node matching scheme naturally fits with the strategy for\nselective inlier matching against outliers, and supports seed graph matching. A\nrevocable action scheme is devised to improve the agent's flexibility against\nthe complex constrained matching task. Moreover, we propose a quadratic\napproximation technique to regularize the affinity matrix, in the presence of\noutliers. As such, the RL agent can finish inlier matching timely when the\nobjective score stop growing, for which otherwise an additional hyperparameter\ni.e. the number of common inliers is needed to avoid matching outliers. In this\npaper, we are focused on learning the back-end solver for the most general form\nof GM: the Lawler's QAP, whose input is the affinity matrix. Our approach can\nalso boost other solvers using the affinity input. Experimental results on both\nsynthetic and real-world datasets showcase its superior performance regarding\nboth matching accuracy and robustness.",
          "link": "http://arxiv.org/abs/2012.08950",
          "publishedOn": "2021-08-18T01:55:00.520Z",
          "wordCount": 686,
          "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching. (arXiv:2012.08950v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1\">Martin Abadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1\">Gordon Plotkin</a>",
          "description": "Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and, increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n\nIn the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional, rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n\nWe establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.",
          "link": "http://arxiv.org/abs/2007.08926",
          "publishedOn": "2021-08-18T01:55:00.513Z",
          "wordCount": 721,
          "title": "Smart Choices and the Selection Monad. (arXiv:2007.08926v6 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frederic Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>",
          "description": "We address the problem of detecting human-object interactions in images using\ngraphical neural networks. Unlike conventional methods, where nodes send scaled\nbut otherwise identical messages to each of their neighbours, we propose to\ncondition messages between pairs of nodes on their spatial relationships,\nresulting in different messages going to neighbours of the same node. To this\nend, we explore various ways of applying spatial conditioning under a\nmulti-branch structure. Through extensive experimentation we demonstrate the\nadvantages of spatial conditioning for the computation of the adjacency\nstructure, messages and the refined graph features. In particular, we\nempirically show that as the quality of the bounding boxes increases, their\ncoarse appearance features contribute relatively less to the disambiguation of\ninteractions compared to the spatial information. Our method achieves an mAP of\n31.33% on HICO-DET and 54.2% on V-COCO, significantly outperforming\nstate-of-the-art on fine-tuned detections.",
          "link": "http://arxiv.org/abs/2012.06060",
          "publishedOn": "2021-08-18T01:55:00.506Z",
          "wordCount": 624,
          "title": "Spatially Conditioned Graphs for Detecting Human-Object Interactions. (arXiv:2012.06060v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1\">Michail Chatzianastasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1\">George Dasoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1\">Georgios Siolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "Neural Architecture Search (NAS) has recently gained increased attention, as\na class of approaches that automatically searches in an input space of network\narchitectures. A crucial part of the NAS pipeline is the encoding of the\narchitecture that consists of the applied computational blocks, namely the\noperations and the links between them. Most of the existing approaches either\nfail to capture the structural properties of the architectures or use\nhand-engineered vector to encode the operator information. In this paper, we\npropose the replacement of fixed operator encoding with learnable\nrepresentations in the optimization process. This approach, which effectively\ncaptures the relations of different operations, leads to smoother and more\naccurate representations of the architectures and consequently to improved\nperformance of the end task. Our extensive evaluation in ENAS benchmark\ndemonstrates the effectiveness of the proposed operation embeddings to the\ngeneration of highly accurate models, achieving state-of-the-art performance.\nFinally, our method produces top-performing architectures that share similar\noperation and graph patterns, highlighting a strong correlation between the\nstructural properties of the architecture and its performance.",
          "link": "http://arxiv.org/abs/2105.04885",
          "publishedOn": "2021-08-18T01:55:00.497Z",
          "wordCount": 645,
          "title": "Graph-based Neural Architecture Search with Operation Embeddings. (arXiv:2105.04885v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaren_M/0/1/0/all/0/1\">Mitchell McLaren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brummer_N/0/1/0/all/0/1\">Niko Brummer</a>",
          "description": "In this paper, we address the problem of speaker verification in conditions\nunseen or unknown during development. A standard method for speaker\nverification consists of extracting speaker embeddings with a deep neural\nnetwork and processing them through a backend composed of probabilistic linear\ndiscriminant analysis (PLDA) and global logistic regression score calibration.\nThis method is known to result in systems that work poorly on conditions\ndifferent from those used to train the calibration model. We propose to modify\nthe standard backend, introducing an adaptive calibrator that uses duration and\nother automatically extracted side-information to adapt to the conditions of\nthe inputs. The backend is trained discriminatively to optimize binary\ncross-entropy. When trained on a number of diverse datasets that are labeled\nonly with respect to speaker, the proposed backend consistently and, in some\ncases, dramatically improves calibration, compared to the standard PLDA\napproach, on a number of held-out datasets, some of which are markedly\ndifferent from the training data. Discrimination performance is also\nconsistently improved. We show that joint training of the PLDA and the adaptive\ncalibrator is essential -- the same benefits cannot be achieved when freezing\nPLDA and fine-tuning the calibrator. To our knowledge, the results in this\npaper are the first evidence in the literature that it is possible to develop a\nspeaker verification system with robust out-of-the-box performance on a large\nvariety of conditions.",
          "link": "http://arxiv.org/abs/2102.01760",
          "publishedOn": "2021-08-18T01:55:00.490Z",
          "wordCount": 699,
          "title": "A Speaker Verification Backend with Robust Performance across Conditions. (arXiv:2102.01760v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Abdullah Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Multi-view projection methods have demonstrated their ability to reach\nstate-of-the-art performance on 3D shape recognition. Those methods learn\ndifferent ways to aggregate information from multiple views. However, the\ncamera view-points for those views tend to be heuristically set and fixed for\nall shapes. To circumvent the lack of dynamism of current multi-view methods,\nwe propose to learn those view-points. In particular, we introduce the\nMulti-View Transformation Network (MVTN) that regresses optimal view-points for\n3D shape recognition, building upon advances in differentiable rendering. As a\nresult, MVTN can be trained end-to-end along with any multi-view network for 3D\nshape classification. We integrate MVTN in a novel adaptive multi-view pipeline\nthat can render either 3D meshes or point clouds. MVTN exhibits clear\nperformance gains in the tasks of 3D shape classification and 3D shape\nretrieval without the need for extra training supervision. In these tasks, MVTN\nachieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the\nmost recent and realistic ScanObjectNN dataset (up to 6% improvement).\nInterestingly, we also show that MVTN can provide network robustness against\nrotation and occlusion in the 3D domain. The code is available at\nhttps://github.com/ajhamdi/MVTN .",
          "link": "http://arxiv.org/abs/2011.13244",
          "publishedOn": "2021-08-18T01:55:00.482Z",
          "wordCount": 675,
          "title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition. (arXiv:2011.13244v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jaehui Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jun-Hyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "The video-based action recognition task has been extensively studied in\nrecent years. In this paper, we study the structural vulnerability of deep\nlearning-based action recognition models against the adversarial attack using\nthe one frame attack that adds an inconspicuous perturbation to only a single\nframe of a given video clip. Our analysis shows that the models are highly\nvulnerable against the one frame attack due to their structural properties.\nExperiments demonstrate high fooling rates and inconspicuous characteristics of\nthe attack. Furthermore, we show that strong universal one frame perturbations\ncan be obtained under various scenarios. Our work raises the serious issue of\nadversarial vulnerability of the state-of-the-art action recognition models in\nvarious perspectives.",
          "link": "http://arxiv.org/abs/2011.14585",
          "publishedOn": "2021-08-18T01:55:00.371Z",
          "wordCount": 596,
          "title": "Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack. (arXiv:2011.14585v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.",
          "link": "http://arxiv.org/abs/2008.01377",
          "publishedOn": "2021-08-18T01:55:00.345Z",
          "wordCount": 718,
          "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a> (PANAMA, DANTE), <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (LMO), <a href=\"http://arxiv.org/find/cs/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a> (GIPSA-GAIA), <a href=\"http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1\">Yann Traonmilin</a> (IMB)",
          "description": "We provide statistical learning guarantees for two unsupervised learning\ntasks in the context of compressive statistical learning, a general framework\nfor resource-efficient large-scale learning that we introduced in a companion\npaper.The principle of compressive statistical learning is to compress a\ntraining collection, in one pass, into a low-dimensional sketch (a vector of\nrandom empirical generalized moments) that captures the information relevant to\nthe considered learning task. We explicitly describe and analyze random feature\nfunctions which empirical averages preserve the needed information for\ncompressive clustering and compressive Gaussian mixture modeling with fixed\nknown variance, and establish sufficient sketch sizes given the problem\ndimensions.",
          "link": "http://arxiv.org/abs/2004.08085",
          "publishedOn": "2021-08-18T01:55:00.326Z",
          "wordCount": 629,
          "title": "Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling. (arXiv:2004.08085v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.08543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1\">Chris Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1\">Uladzimir Yahorau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1\">Rory Coles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Kyle Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1\">Stephen Whitelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1\">Isaac Tamblyn</a>",
          "description": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters.",
          "link": "http://arxiv.org/abs/1903.08543",
          "publishedOn": "2021-08-18T01:55:00.306Z",
          "wordCount": 647,
          "title": "Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v5 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mahdi Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1\">Mahdi Jafari Siavoshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahangir_A/0/1/0/all/0/1\">Amir Hossein Jahangir</a>",
          "description": "The growing number of Internet users and the prevalence of web applications\nmake it necessary to deal with very complex software and applications in the\nnetwork. This results in an increasing number of new vulnerabilities in the\nsystems, and leading to an increase in cyber threats and, in particular,\nzero-day attacks. The cost of generating appropriate signatures for these\nattacks is a potential motive for using machine learning-based methodologies.\nAlthough there are many studies on using learning-based methods for attack\ndetection, they generally use extracted features and overlook raw contents.\nThis approach can lessen the performance of detection systems against\ncontent-based attacks like SQL injection, Cross-site Scripting (XSS), and\nvarious viruses.\n\nIn this work, we propose a framework, called deep intrusion detection (DID)\nsystem, that uses the pure content of traffic flows in addition to traffic\nmetadata in the learning and detection phases of a passive DNN IDS. To this\nend, we deploy and evaluate an offline IDS following the framework using LSTM\nas a deep learning technique. Due to the inherent nature of deep learning, it\ncan process high dimensional data content and, accordingly, discover the\nsophisticated relations between the auto extracted features of the traffic. To\nevaluate the proposed DID system, we use the CIC-IDS2017 and CSE-CIC-IDS2018\ndatasets. The evaluation metrics, such as precision and recall, reach $0.992$\nand $0.998$ on CIC-IDS2017, and $0.933$ and $0.923$ on CSE-CIC-IDS2018\nrespectively, which show the high performance of the proposed DID method.",
          "link": "http://arxiv.org/abs/2001.05009",
          "publishedOn": "2021-08-18T01:55:00.238Z",
          "wordCount": 710,
          "title": "A Content-Based Deep Intrusion Detection System. (arXiv:2001.05009v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "3D point cloud understanding has made great progress in recent years.\nHowever, one major bottleneck is the scarcity of annotated real datasets,\nespecially compared to 2D object detection tasks, since a large amount of labor\nis involved in annotating the real scans of a scene. A promising solution to\nthis problem is to make better use of the synthetic dataset, which consists of\nCAD object models, to boost the learning on real datasets. This can be achieved\nby the pre-training and fine-tuning procedure. However, recent work on 3D\npre-training exhibits failure when transfer features learned on synthetic\nobjects to other real-world applications. In this work, we put forward a new\nmethod called RandomRooms to accomplish this objective. In particular, we\npropose to generate random layouts of a scene by making use of the objects in\nthe synthetic CAD dataset and learn the 3D scene representation by applying\nobject-level contrastive learning on two random scenes generated from the same\nset of synthetic objects. The model pre-trained in this way can serve as a\nbetter initialization when later fine-tuning on the 3D object detection task.\nEmpirically, we show consistent improvement in downstream 3D detection tasks on\nseveral base models, especially when less training data are used, which\nstrongly demonstrates the effectiveness and generalization of our method.\nBenefiting from the rich semantic knowledge and diverse objects from synthetic\ndata, our method establishes the new state-of-the-art on widely-used 3D\ndetection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide\na new perspective for bridging object and scene-level 3D understanding.",
          "link": "http://arxiv.org/abs/2108.07794",
          "publishedOn": "2021-08-18T01:55:00.217Z",
          "wordCount": 720,
          "title": "RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection. (arXiv:2108.07794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qafari_M/0/1/0/all/0/1\">Mahnaz Sadat Qafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1\">Wil van der Aalst</a>",
          "description": "Process mining techniques can help organizations to improve their operational\nprocesses. Organizations can benefit from process mining techniques in finding\nand amending the root causes of performance or compliance problems. Considering\nthe volume of the data and the number of features captured by the information\nsystem of today's companies, the task of discovering the set of features that\nshould be considered in root cause analysis can be quite involving. In this\npaper, we propose a method for finding the set of (aggregated) features with a\npossible effect on the problem.\n\nThe root cause analysis task is usually done by applying a machine learning\ntechnique to the data gathered from the information system supporting the\nprocesses. To prevent mixing up correlation and causation, which may happen\nbecause of interpreting the findings of machine learning techniques as causal,\nwe propose a method for discovering the structural equation model of the\nprocess that can be used for root cause analysis. We have implemented the\nproposed method as a plugin in ProM and we have evaluated it using two real and\nsynthetic event logs. These experiments show the validity and effectiveness of\nthe proposed methods.",
          "link": "http://arxiv.org/abs/2108.07795",
          "publishedOn": "2021-08-18T01:55:00.210Z",
          "wordCount": 633,
          "title": "Feature Recommendation for Structural Equation Model Discovery in Process Mining. (arXiv:2108.07795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Assessing action quality is challenging due to the subtle differences between\nvideos and large variations in scores. Most existing approaches tackle this\nproblem by regressing a quality score from a single video, suffering a lot from\nthe large inter-video score variations. In this paper, we show that the\nrelations among videos can provide important clues for more accurate action\nquality assessment during both training and inference. Specifically, we\nreformulate the problem of action quality assessment as regressing the relative\nscores with reference to another video that has shared attributes (e.g.,\ncategory and difficulty), instead of learning unreferenced scores. Following\nthis formulation, we propose a new Contrastive Regression (CoRe) framework to\nlearn the relative scores by pair-wise comparison, which highlights the\ndifferences between videos and guides the models to learn the key hints for\nassessment. In order to further exploit the relative information between two\nvideos, we devise a group-aware regression tree to convert the conventional\nscore regression into two easier sub-problems: coarse-to-fine classification\nand regression in small intervals. To demonstrate the effectiveness of CoRe, we\nconduct extensive experiments on three mainstream AQA datasets including AQA-7,\nMTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large\nmargin and establishes new state-of-the-art on all three benchmarks.",
          "link": "http://arxiv.org/abs/2108.07797",
          "publishedOn": "2021-08-18T01:55:00.203Z",
          "wordCount": 654,
          "title": "Group-aware Contrastive Regression for Action Quality Assessment. (arXiv:2108.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.02326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charlier_J/0/1/0/all/0/1\">Jeremy Charlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarenkov_V/0/1/0/all/0/1\">Vladimir Makarenkov</a>",
          "description": "Bootstrap aggregation, known as bagging, is one of the most popular ensemble\nmethods used in machine learning (ML). An ensemble method is a ML method that\ncombines multiple hypotheses to form a single hypothesis used for prediction. A\nbagging algorithm combines multiple classifiers modeled on different\nsub-samples of the same data set to build one large classifier. Banks, and\ntheir retail banking activities, are nowadays using the power of ML algorithms,\nincluding decision trees and random forests, to optimize their processes.\nHowever, banks have to comply with regulators and governance and, hence,\ndelivering effective ML solutions is a challenging task. It starts with the\nbank's validation and governance department, followed by the deployment of the\nsolution in a production environment up to the external validation of the\nnational financial regulator. Each proposed ML model has to be validated and\nclear rules for every algorithm-based decision must be justified. In this\ncontext, we propose XtracTree, an algorithm capable of efficiently converting\nan ML bagging classifier, such as a random forest, into simple \"if-then\" rules\nsatisfying the requirements of model validation. We use a public loan data set\nfrom Kaggle to illustrate the usefulness of our approach. Our experiments\ndemonstrate that using XtracTree, one can convert an ML model into a rule-based\nalgorithm, leading to easier model validation by national financial regulators\nand the bank's validation department. The proposed approach allowed our banking\ninstitution to reduce up to 50% the time of delivery of our AI solutions to the\nend-user.",
          "link": "http://arxiv.org/abs/2004.02326",
          "publishedOn": "2021-08-18T01:55:00.191Z",
          "wordCount": 735,
          "title": "XtracTree: a Simple and Effective Method for Regulator Validation of Bagging Methods Used in Retail Banking. (arXiv:2004.02326v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anshul Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_R/0/1/0/all/0/1\">Roger Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_L/0/1/0/all/0/1\">Lisa Walker</a>",
          "description": "Introduction: When a learner fails to reach a milestone, educators often\nwonder if there had been any warning signs that could have allowed them to\nintervene sooner. Machine learning is used to predict which students are at\nrisk of failing a national certifying exam. Predictions are made well in\nadvance of the exam, such that educators can meaningfully intervene before\nstudents take the exam.\n\nMethods: Using already-collected, first-year student assessment data from\nfour cohorts in a Master of Physician Assistant Studies program, the authors\nimplement an \"adaptive minimum match\" version of the k-nearest neighbors\nalgorithm (AMMKNN), using changing numbers of neighbors to predict each\nstudent's future exam scores on the Physician Assistant National Certifying\nExamination (PANCE). Leave-one-out cross validation (LOOCV) was used to\nevaluate the practical capabilities of this model, before making predictions\nfor new students.\n\nResults: The best predictive model has an accuracy of 93%, sensitivity of\n69%, and specificity of 94%. It generates a predicted PANCE score for each\nstudent, one year before they are scheduled to take the exam. Students can then\nbe prospectively categorized into groups that need extra support, optional\nextra support, or no extra support. The educator then has one year to provide\nthe appropriate customized support to each type of student.\n\nConclusions: Predictive analytics can help health professions educators\nallocate scarce time and resources across their students. Interprofessional\neducators can use the included methods and code to generate predicted test\noutcomes for students. The authors recommend that educators using this or\nsimilar predictive methods act responsibly and transparently.",
          "link": "http://arxiv.org/abs/2108.07709",
          "publishedOn": "2021-08-18T01:55:00.180Z",
          "wordCount": 697,
          "title": "The application of predictive analytics to identify at-risk students in health professions education. (arXiv:2108.07709v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonsior_J/0/1/0/all/0/1\">Julius Gonsior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_M/0/1/0/all/0/1\">Maik Thiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_W/0/1/0/all/0/1\">Wolfgang Lehner</a>",
          "description": "One of the biggest challenges that complicates applied supervised machine\nlearning is the need for huge amounts of labeled data. Active Learning (AL) is\na well-known standard method for efficiently obtaining labeled data by first\nlabeling the samples that contain the most information based on a query\nstrategy. Although many methods for query strategies have been proposed in the\npast, no clear superior method that works well in general for all domains has\nbeen found yet. Additionally, many strategies are computationally expensive\nwhich further hinders the widespread use of AL for large-scale annotation\nprojects.\n\nWe, therefore, propose ImitAL, a novel query strategy, which encodes AL as a\nlearning-to-rank problem. For training the underlying neural network we chose\nImitation Learning. The required demonstrative expert experience for training\nis generated from purely synthetic data.\n\nTo show the general and superior applicability of \\ImitAL{}, we perform an\nextensive evaluation comparing our strategy on 15 different datasets, from a\nwide range of domains, with 10 different state-of-the-art query strategies. We\nalso show that our approach is more runtime performant than most other\nstrategies, especially on very large datasets.",
          "link": "http://arxiv.org/abs/2108.07670",
          "publishedOn": "2021-08-18T01:55:00.173Z",
          "wordCount": 615,
          "title": "ImitAL: Learning Active Learning Strategies from Synthetic Data. (arXiv:2108.07670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samsinger_M/0/1/0/all/0/1\">Maximilian Samsinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merkle_F/0/1/0/all/0/1\">Florian Merkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schottle_P/0/1/0/all/0/1\">Pascal Sch&#xf6;ttle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1\">Tomas Pevny</a>",
          "description": "Adversarial machine learning, i.e., increasing the robustness of machine\nlearning algorithms against so-called adversarial examples, is now an\nestablished field. Yet, newly proposed methods are evaluated and compared under\nunrealistic scenarios where costs for adversary and defender are not considered\nand either all samples are attacked or no sample is attacked. We scrutinize\nthese assumptions and propose the advanced adversarial classification game,\nwhich incorporates all relevant parameters of an adversary and a defender in\nadversarial classification. Especially, we take into account economic factors\non both sides and the fact that all so far proposed countermeasures against\nadversarial examples reduce accuracy on benign samples. Analyzing the scenario\nin detail, where both players have two pure strategies, we identify all best\nresponses and conclude that in practical settings, the most influential factor\nmight be the maximum amount of adversarial examples.",
          "link": "http://arxiv.org/abs/2108.07602",
          "publishedOn": "2021-08-18T01:55:00.143Z",
          "wordCount": 589,
          "title": "When Should You Defend Your Classifier -- A Game-theoretical Analysis of Countermeasures against Adversarial Examples. (arXiv:2108.07602v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Penghua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1\">Huaiwei Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>",
          "description": "With the renaissance of deep learning, automatic diagnostic systems for\ncomputed tomography (CT) have achieved many successful applications. However,\nthey are mostly attributed to careful expert annotations, which are often\nscarce in practice. This drives our interest to the unsupervised representation\nlearning. Recent studies have shown that self-supervised learning is an\neffective approach for learning representations, but most of them rely on the\nempirical design of transformations and pretext tasks. To avoid the\nsubjectivity associated with these methods, we propose the MVCNet, a novel\nunsupervised three dimensional (3D) representation learning method working in a\ntransformation-free manner. We view each 3D lesion from different orientations\nto collect multiple two dimensional (2D) views. Then, an embedding function is\nlearned by minimizing a contrastive loss so that the 2D views of the same 3D\nlesion are aggregated, and the 2D views of different lesions are separated. We\nevaluate the representations by training a simple classification head upon the\nembedding layer. Experimental results show that MVCNet achieves\nstate-of-the-art accuracies on the LIDC-IDRI (89.55%), LNDb (77.69%) and\nTianChi (79.96%) datasets for unsupervised representation learning. When\nfine-tuned on 10% of the labeled data, the accuracies are comparable to the\nsupervised learning model (89.46% vs. 85.03%, 73.85% vs. 73.44%, 83.56% vs.\n83.34% on the three datasets, respectively), indicating the superiority of\nMVCNet in learning representations with limited annotations. Code is released\nat: https://github.com/penghuazhai/MVCNet.",
          "link": "http://arxiv.org/abs/2108.07662",
          "publishedOn": "2021-08-18T01:55:00.116Z",
          "wordCount": 696,
          "title": "MVCNet: Multiview Contrastive Network for Unsupervised Representation Learning for 3D CT Lesions. (arXiv:2108.07662v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chomiak_K/0/1/0/all/0/1\">Krzysztof Chomiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miktus_M/0/1/0/all/0/1\">Micha&#x142; Miktus</a>",
          "description": "The paper introduces concepts of fairness and explainability (XAI) in\nartificial intelligence, oriented to solve a sophisticated business problems.\nFor fairness, the authors discuss the bias-inducing specifics, as well as\nrelevant mitigation methods, concluding with a set of recipes for introducing\nfairness in data-driven organizations. Additionally, for XAI, the authors audit\nspecific algorithms paired with demonstrational business use-cases, discuss a\nplethora of techniques of explanations quality quantification and provide an\noverview of future research avenues.",
          "link": "http://arxiv.org/abs/2108.07714",
          "publishedOn": "2021-08-18T01:55:00.094Z",
          "wordCount": 516,
          "title": "Harnessing value from data science in business: ensuring explainability and fairness of solutions. (arXiv:2108.07714v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07636",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Prado_E/0/1/0/all/0/1\">Estev&#xe3;o B. Prado</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew C. Parnell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McJames_N/0/1/0/all/0/1\">Nathan McJames</a>, <a href=\"http://arxiv.org/find/stat/1/au:+OShea_A/0/1/0/all/0/1\">Ann O&#x27;Shea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moral_R/0/1/0/all/0/1\">Rafael A. Moral</a>",
          "description": "We propose a new semi-parametric model based on Bayesian Additive Regression\nTrees (BART). In our approach, the response variable is approximated by a\nlinear predictor and a BART model, where the first component is responsible for\nestimating the main effects and BART accounts for the non-specified\ninteractions and non-linearities. The novelty in our approach lies in the way\nwe change tree generation moves in BART to deal with confounding between the\nparametric and non-parametric components when they have covariates in common.\nThrough synthetic and real-world examples, we demonstrate that the performance\nof the new semi-parametric BART is competitive when compared to regression\nmodels and other tree-based methods. The implementation of the proposed method\nis available at https://github.com/ebprado/SP-BART.",
          "link": "http://arxiv.org/abs/2108.07636",
          "publishedOn": "2021-08-18T01:55:00.088Z",
          "wordCount": 553,
          "title": "Semi-parametric Bayesian Additive Regression Trees. (arXiv:2108.07636v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-08-18T01:55:00.067Z",
          "wordCount": 597,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07749",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1\">Joshua Yao-Yu Lin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pandya_S/0/1/0/all/0/1\">Sneh Pandya</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Pratap_D/0/1/0/all/0/1\">Devanshi Pratap</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kind_M/0/1/0/all/0/1\">Matias Carrasco Kind</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kindratenko_V/0/1/0/all/0/1\">Volodymyr Kindratenko</a>",
          "description": "Supermassive black holes (SMBHs) are ubiquitously found at the centers of\nmost massive galaxies. Measuring SMBH mass is important for understanding the\norigin and evolution of SMBHs. However, traditional methods require\nspectroscopic data which is expensive to gather. We present an algorithm that\nweighs SMBHs using quasar light time series, circumventing the need for\nexpensive spectra. We train, validate, and test neural networks that directly\nlearn from the Sloan Digital Sky Survey (SDSS) Stripe 82 light curves for a\nsample of $38,939$ spectroscopically confirmed quasars to map out the nonlinear\nencoding between SMBH mass and multi-color optical light curves. We find a\n1$\\sigma$ scatter of 0.37 dex between the predicted SMBH mass and the fiducial\nvirial mass estimate based on SDSS single-epoch spectra, which is comparable to\nthe systematic uncertainty in the virial mass estimate. Our results have direct\nimplications for more efficient applications with future observations from the\nVera C. Rubin Observatory. Our code, \\textsf{AGNet}, is publicly available at\n\n{\\color{red} \\url{https://github.com/snehjp2/AGNet}}.",
          "link": "http://arxiv.org/abs/2108.07749",
          "publishedOn": "2021-08-18T01:55:00.057Z",
          "wordCount": 622,
          "title": "AGNet: Weighing Black Holes with Deep Learning. (arXiv:2108.07749v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yubo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hai Lin</a>",
          "description": "This paper proposes a novel model for predicting subgraphs in dynamic graphs,\nan extension of traditional link prediction. This proposed end-to-end model\nlearns a mapping from the subgraph structures in the current snapshot to the\nsubgraph structures in the next snapshot directly, i.e., edge existence among\nmultiple nodes in the subgraph. A new mechanism named cross-attention with a\ntwin-tower module is designed to integrate node attribute information and\ntopology information collaboratively for learning subgraph evolution. We\ncompare our model with several state-of-the-art methods for subgraph prediction\nand subgraph pattern prediction in multiple real-world homogeneous and\nheterogeneous dynamic graphs, respectively. Experimental results demonstrate\nthat our model outperforms other models in these two tasks, with a gain\nincrease from 5.02% to 10.88%.",
          "link": "http://arxiv.org/abs/2108.07776",
          "publishedOn": "2021-08-18T01:55:00.045Z",
          "wordCount": 563,
          "title": "SPAN: Subgraph Prediction Attention Network for Dynamic Graphs. (arXiv:2108.07776v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/1902.01687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukai_B/0/1/0/all/0/1\">Ben Boukai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Zuofeng Shang</a>",
          "description": "Deep neural network is a state-of-art method in modern science and\ntechnology. Much statistical literature have been devoted to understanding its\nperformance in nonparametric estimation, whereas the results are suboptimal due\nto a redundant logarithmic sacrifice. In this paper, we show that such\nlog-factors are not necessary. We derive upper bounds for the $L^2$ minimax\nrisk in nonparametric estimation. Sufficient conditions on network\narchitectures are provided such that the upper bounds become optimal (without\nlog-sacrifice). Our proof relies on an explicitly constructed network estimator\nbased on tensor product B-splines. We also derive asymptotic distributions for\nthe constructed network and a relating hypothesis testing procedure. The\ntesting procedure is further proven as minimax optimal under suitable network\narchitectures.",
          "link": "http://arxiv.org/abs/1902.01687",
          "publishedOn": "2021-08-18T01:55:00.038Z",
          "wordCount": 576,
          "title": "Optimal Nonparametric Inference via Deep Neural Network. (arXiv:1902.01687v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jinyung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1\">Theodore P. Pavlic</a>",
          "description": "Fruit flies are established model systems for studying olfactory learning as\nthey will readily learn to associate odors with both electric shock or sugar\nrewards. The mechanisms of the insect brain apparently responsible for odor\nlearning form a relatively shallow neuronal architecture. Olfactory inputs are\nreceived by the antennal lobe (AL) of the brain, which produces an encoding of\neach odor mixture across ~50 sub-units known as glomeruli. Each of these\nglomeruli then project its component of this feature vector to several of ~2000\nso-called Kenyon Cells (KCs) in a region of the brain known as the mushroom\nbody (MB). Fly responses to odors are generated by small downstream neuropils\nthat decode the higher-order representation from the MB. Research has shown\nthat there is no recognizable pattern in the glomeruli--KC connections (and\nthus the particular higher-order representations); they are akin to\nfingerprints~-- even isogenic flies have different projections. Leveraging\ninsights from this architecture, we propose KCNet, a single-hidden-layer neural\nnetwork that contains sparse, randomized, binary weights between the input\nlayer and the hidden layer and analytically learned weights between the hidden\nlayer and the output layer. Furthermore, we also propose a dynamic optimization\nalgorithm that enables the KCNet to increase performance beyond its structural\nlimits by searching a more efficient set of inputs. For odorant-perception\ntasks that predict perceptual properties of an odorant, we show that KCNet\noutperforms existing data-driven approaches, such as XGBoost. For\nimage-classification tasks, KCNet achieves reasonable performance on benchmark\ndatasets (MNIST, Fashion-MNIST, and EMNIST) without any data-augmentation\nmethods or convolutional layers and shows particularly fast running time. Thus,\nneural networks inspired by the insect brain can be both economical and perform\nwell.",
          "link": "http://arxiv.org/abs/2108.07554",
          "publishedOn": "2021-08-18T01:55:00.029Z",
          "wordCount": 730,
          "title": "KCNet: An Insect-Inspired Single-Hidden-Layer Neural Network with Randomized Binary Weights for Prediction and Classification Tasks. (arXiv:2108.07554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odena_A/0/1/0/all/0/1\">Augustus Odena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1\">Maxwell Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1\">Ellen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>",
          "description": "This paper explores the limits of the current generation of large language\nmodels for program synthesis in general purpose programming languages. We\nevaluate a collection of such models (with between 244M and 137B parameters) on\ntwo new benchmarks, MBPP and MathQA-Python, in both the few-shot and\nfine-tuning regimes. Our benchmarks are designed to measure the ability of\nthese models to synthesize short Python programs from natural language\ndescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974\nprogramming tasks, designed to be solvable by entry-level programmers. The\nMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914\nproblems that evaluate the ability of the models to synthesize code from more\ncomplex text. On both datasets, we find that synthesis performance scales\nlog-linearly with model size. Our largest models, even without finetuning on a\ncode dataset, can synthesize solutions to 59.6 percent of the problems from\nMBPP using few-shot learning with a well-designed prompt. Fine-tuning on a\nheld-out portion of the dataset improves performance by about 10 percentage\npoints across most model sizes. On the MathQA-Python dataset, the largest\nfine-tuned model achieves 83.8 percent accuracy. Going further, we study the\nmodel's ability to engage in dialog about code, incorporating human feedback to\nimprove its solutions. We find that natural language feedback from a human\nhalves the error rate compared to the model's initial prediction. Additionally,\nwe conduct an error analysis to shed light on where these models fall short and\nwhat types of programs are most difficult to generate. Finally, we explore the\nsemantic grounding of these models by fine-tuning them to predict the results\nof program execution. We find that even our best models are generally unable to\npredict the output of a program given a specific input.",
          "link": "http://arxiv.org/abs/2108.07732",
          "publishedOn": "2021-08-18T01:55:00.022Z",
          "wordCount": 739,
          "title": "Program Synthesis with Large Language Models. (arXiv:2108.07732v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1\">Somjit Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranwal_M/0/1/0/all/0/1\">Mayank Baranwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1\">Harshad Khadilkar</a>",
          "description": "Several real-world scenarios, such as remote control and sensing, are\ncomprised of action and observation delays. The presence of delays degrades the\nperformance of reinforcement learning (RL) algorithms, often to such an extent\nthat algorithms fail to learn anything substantial. This paper formally\ndescribes the notion of Markov Decision Processes (MDPs) with stochastic delays\nand shows that delayed MDPs can be transformed into equivalent standard MDPs\n(without delays) with significantly simplified cost structure. We employ this\nequivalence to derive a model-free Delay-Resolved RL framework and show that\neven a simple RL algorithm built upon this framework achieves near-optimal\nrewards in environments with stochastic delays in actions and observations. The\ndelay-resolved deep Q-network (DRDQN) algorithm is bench-marked on a variety of\nenvironments comprising of multi-step and stochastic delays and results in\nbetter performance, both in terms of achieving near-optimal rewards and\nminimizing the computational overhead thereof, with respect to the currently\nestablished algorithms.",
          "link": "http://arxiv.org/abs/2108.07555",
          "publishedOn": "2021-08-18T01:54:59.999Z",
          "wordCount": 606,
          "title": "Revisiting State Augmentation methods for Reinforcement Learning with Stochastic Delays. (arXiv:2108.07555v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07772",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petratos_A/0/1/0/all/0/1\">Aidan Petratos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ting_A/0/1/0/all/0/1\">Allen Ting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Shankar Padmanabhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kristina Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hageman_D/0/1/0/all/0/1\">Dylan Hageman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pisel_J/0/1/0/all/0/1\">Jesse R. Pisel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pyrcz_M/0/1/0/all/0/1\">Michael J. Pyrcz</a>",
          "description": "The placement of charging stations in areas with developing charging\ninfrastructure is a critical component of the future success of electric\nvehicles (EVs). In Albany County in New York, the expected rise in the EV\npopulation requires additional charging stations to maintain a sufficient level\nof efficiency across the charging infrastructure. A novel application of\nReinforcement Learning (RL) is able to find optimal locations for new charging\nstations given the predicted charging demand and current charging locations.\nThe most important factors that influence charging demand prediction include\nthe conterminous traffic density, EV registrations, and proximity to certain\ntypes of public buildings. The proposed RL framework can be refined and applied\nto cities across the world to optimize charging station placement.",
          "link": "http://arxiv.org/abs/2108.07772",
          "publishedOn": "2021-08-18T01:54:59.990Z",
          "wordCount": 581,
          "title": "Optimal Placement of Public Electric Vehicle Charging Stations Using Deep Reinforcement Learning. (arXiv:2108.07772v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifah_T/0/1/0/all/0/1\">Tariq Alkhalifah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovcharenko_O/0/1/0/all/0/1\">Oleg Ovcharenko</a>",
          "description": "We propose a direct domain adaptation (DDA) approach to enrich the training\nof supervised neural networks on synthetic data by features from real-world\ndata. The process involves a series of linear operations on the input features\nto the NN model, whether they are from the source or target domains, as\nfollows: 1) A cross-correlation of the input data (i.e. images) with a randomly\npicked sample pixel (or pixels) of all images from that domain or the mean of\nall randomly picked sample pixel (or pixels) of all images. 2) The convolution\nof the resulting data with the mean of the autocorrelated input images from the\nother domain. In the training stage, as expected, the input images are from the\nsource domain, and the mean of auto-correlated images are evaluated from the\ntarget domain. In the inference/application stage, the input images are from\nthe target domain, and the mean of auto-correlated images are evaluated from\nthe source domain. The proposed method only manipulates the data from the\nsource and target domains and does not explicitly interfere with the training\nworkflow and network architecture. An application that includes training a\nconvolutional neural network on the MNIST dataset and testing the network on\nthe MNIST-M dataset achieves a 70% accuracy on the test data. A principal\ncomponent analysis (PCA), as well as t-SNE, show that the input features from\nthe source and target domains, after the proposed direct transformations, share\nsimilar properties along with the principal components as compared to the\noriginal MNIST and MNIST-M input features.",
          "link": "http://arxiv.org/abs/2108.07600",
          "publishedOn": "2021-08-18T01:54:59.979Z",
          "wordCount": 691,
          "title": "Direct domain adaptation through reciprocal linear transformations. (arXiv:2108.07600v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "The recent breakthroughs and prohibitive complexities of Deep Neural Networks\n(DNNs) have excited extensive interest in domain-specific DNN accelerators,\namong which optical DNN accelerators are particularly promising thanks to their\nunprecedented potential of achieving superior performance-per-watt. However,\nthe development of optical DNN accelerators is much slower than that of\nelectrical DNN accelerators. One key challenge is that while many techniques\nhave been developed to facilitate the development of electrical DNN\naccelerators, techniques that support or expedite optical DNN accelerator\ndesign remain much less explored, limiting both the achievable performance and\nthe innovation development of optical DNN accelerators. To this end, we develop\nthe first-of-its-kind framework dubbed O-HAS, which for the first time\ndemonstrates automated Optical Hardware Accelerator Search for boosting both\nthe acceleration efficiency and development speed of optical DNN accelerators.\nSpecifically, our O-HAS consists of two integrated enablers: (1) an O-Cost\nPredictor, which can accurately yet efficiently predict an optical\naccelerator's energy and latency based on the DNN model parameters and the\noptical accelerator design; and (2) an O-Search Engine, which can automatically\nexplore the large design space of optical DNN accelerators and identify the\noptimal accelerators (i.e., the micro-architectures and\nalgorithm-to-accelerator mapping methods) in order to maximize the target\nacceleration efficiency. Extensive experiments and ablation studies\nconsistently validate the effectiveness of both our O-Cost Predictor and\nO-Search Engine as well as the excellent efficiency of O-HAS generated optical\naccelerators.",
          "link": "http://arxiv.org/abs/2108.07538",
          "publishedOn": "2021-08-18T01:54:59.970Z",
          "wordCount": 686,
          "title": "O-HAS: Optical Hardware Accelerator Search for Boosting Both Acceleration Performance and Development Speed. (arXiv:2108.07538v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kehinde_A/0/1/0/all/0/1\">Adeniyi Jide Kehinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeniyi_A/0/1/0/all/0/1\">Abidemi Emmanuel Adeniyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundokun_R/0/1/0/all/0/1\">Roseline Oluwaseun Ogundokun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1\">Sanjay Misra</a>",
          "description": "Many researchers have studied student academic performance in supervised and\nunsupervised learning using numerous data mining techniques. Neural networks\noften need a greater collection of observations to achieve enough predictive\nability. Due to the increase in the rate of poor graduates, it is necessary to\ndesign a system that helps to reduce this menace as well as reduce the\nincidence of students having to repeat due to poor performance or having to\ndrop out of school altogether in the middle of the pursuit of their career. It\nis therefore necessary to study each one as well as their advantages and\ndisadvantages, so as to determine which is more efficient in and in what case\none should be preferred over the other. The study aims to develop a system to\npredict student performance with Artificial Neutral Network using the student\ndemographic traits so as to assist the university in selecting candidates\n(students) with a high prediction of success for admission using previous\nacademic records of students granted admissions which will eventually lead to\nquality graduates of the institution. The model was developed based on certain\nselected variables as the input. It achieved an accuracy of over 92.3 percent,\nshowing Artificial Neural Network potential effectiveness as a predictive tool\nand a selection criterion for candidates seeking admission to a university.",
          "link": "http://arxiv.org/abs/2108.07717",
          "publishedOn": "2021-08-18T01:54:59.962Z",
          "wordCount": 684,
          "title": "Prediction of Students performance with Artificial Neural Network using Demographic Traits. (arXiv:2108.07717v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1\">Leonardo Enzo Brito da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayapati_N/0/1/0/all/0/1\">Nagasharath Rayapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1\">Donald C. Wunsch II</a>",
          "description": "In streaming data applications incoming samples are processed and discarded,\ntherefore, intelligent decision-making is crucial for the performance of\nlifelong learning systems. In addition, the order in which samples arrive may\nheavily affect the performance of online (and offline) incremental learners.\nThe recently introduced incremental cluster validity indices (iCVIs) provide\nvaluable aid in addressing such class of problems. Their primary use-case has\nbeen cluster quality monitoring; nonetheless, they have been very recently\nintegrated in a streaming clustering method to assist the clustering task\nitself. In this context, the work presented here introduces the first adaptive\nresonance theory (ART)-based model that uses iCVIs for unsupervised and\nsemi-supervised online learning. Moreover, it shows for the first time how to\nuse iCVIs to regulate ART vigilance via an iCVI-based match tracking mechanism.\nThe model achieves improved accuracy and robustness to ordering effects by\nintegrating an online iCVI framework as module B of a topological adaptive\nresonance theory predictive mapping (TopoARTMAP) -- thereby being named\niCVI-TopoARTMAP -- and by employing iCVI-driven post-processing heuristics at\nthe end of each learning step. The online iCVI framework provides assignments\nof input samples to clusters at each iteration in accordance to any of several\niCVIs. The iCVI-TopoARTMAP maintains useful properties shared by ARTMAP models,\nsuch as stability, immunity to catastrophic forgetting, and the many-to-one\nmapping capability via the map field module. The performance (unsupervised and\nsemi-supervised) and robustness to presentation order (unsupervised) of\niCVI-TopoARTMAP were evaluated via experiments with a synthetic data set and\ndeep embeddings of a real-world face image data set.",
          "link": "http://arxiv.org/abs/2108.07743",
          "publishedOn": "2021-08-18T01:54:59.936Z",
          "wordCount": 701,
          "title": "Incremental cluster validity index-guided online learning for performance and robustness to presentation order. (arXiv:2108.07743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "Machine Learning (ML) is about computational methods that enable machines to\nlearn concepts from experiences. In handling a wide variety of experiences\nranging from data instances, knowledge, constraints, to rewards, adversaries,\nand lifelong interplay in an ever-growing spectrum of tasks, contemporary ML/AI\nresearch has resulted in a multitude of learning paradigms and methodologies.\nDespite the continual progresses on all different fronts, the disparate\nnarrowly-focused methods also make standardized, composable, and reusable\ndevelopment of learning solutions difficult, and make it costly if possible to\nbuild AI agents that panoramically learn from all types of experiences. This\npaper presents a standardized ML formalism, in particular a standard equation\nof the learning objective, that offers a unifying understanding of diverse ML\nalgorithms, making them special cases due to different choices of modeling\ncomponents. The framework also provides guidance for mechanic design of new ML\nsolutions, and serves as a promising vehicle towards panoramic learning with\nall experiences.",
          "link": "http://arxiv.org/abs/2108.07783",
          "publishedOn": "2021-08-18T01:54:59.905Z",
          "wordCount": 583,
          "title": "Panoramic Learning with A Standardized Machine Learning Formalism. (arXiv:2108.07783v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>",
          "description": "Running faster will only get you so far -- it is generally advisable to first\nunderstand where the roads lead, then get a car ...\n\nThe renaissance of machine learning (ML) and deep learning (DL) over the last\ndecade is accompanied by an unscalable computational cost, limiting its\nadvancement and weighing on the field in practice. In this thesis we take a\nsystematic approach to address the algorithmic and methodological limitations\nat the root of these costs. We first demonstrate that DL training and pruning\nare predictable and governed by scaling laws -- for state of the art models and\ntasks, spanning image classification and language modeling, as well as for\nstate of the art model compression via iterative pruning. Predictability, via\nthe establishment of these scaling laws, provides the path for principled\ndesign and trade-off reasoning, currently largely lacking in the field. We then\ncontinue to analyze the sources of the scaling laws, offering an\napproximation-theoretic view and showing through the exploration of a noiseless\nrealizable case that DL is in fact dominated by error sources very far from the\nlower error limit. We conclude by building on the gained theoretical\nunderstanding of the scaling laws' origins. We present a conjectural path to\neliminate one of the current dominant error sources -- through a data bandwidth\nlimiting hypothesis and the introduction of Nyquist learners -- which can, in\nprinciple, reach the generalization error lower limit (e.g. 0 in the noiseless\ncase), at finite dataset size.",
          "link": "http://arxiv.org/abs/2108.07686",
          "publishedOn": "2021-08-18T01:54:59.898Z",
          "wordCount": 668,
          "title": "Scaling Laws for Deep Learning. (arXiv:2108.07686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glimsdal_S/0/1/0/all/0/1\">Sondre Glimsdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>",
          "description": "Using finite-state machines to learn patterns, Tsetlin machines (TMs) have\nobtained competitive accuracy and learning speed across several benchmarks,\nwith frugal memory- and energy footprint. A TM represents patterns as\nconjunctive clauses in propositional logic (AND-rules), each clause voting for\nor against a particular output. While efficient for single-output problems, one\nneeds a separate TM per output for multi-output problems. Employing multiple\nTMs hinders pattern reuse because each TM then operates in a silo. In this\npaper, we introduce clause sharing, merging multiple TMs into a single one.\nEach clause is related to each output by using a weight. A positive weight\nmakes the clause vote for output $1$, while a negative weight makes the clause\nvote for output $0$. The clauses thus coalesce to produce multiple outputs. The\nresulting coalesced Tsetlin Machine (CoTM) simultaneously learns both the\nweights and the composition of each clause by employing interacting Stochastic\nSearching on the Line (SSL) and Tsetlin Automata (TA) teams. Our empirical\nresults on MNIST, Fashion-MNIST, and Kuzushiji-MNIST show that CoTM obtains\nsignificantly higher accuracy than TM on $50$- to $1$K-clause configurations,\nindicating an ability to repurpose clauses. E.g., accuracy goes from $71.99$%\nto $89.66$% on Fashion-MNIST when employing $50$ clauses per class (22 Kb\nmemory). While TM and CoTM accuracy is similar when using more than $1$K\nclauses per class, CoTM reaches peak accuracy $3\\times$ faster on MNIST with\n$8$K clauses. We further investigate robustness towards imbalanced training\ndata. Our evaluations on imbalanced versions of IMDb- and CIFAR10 data show\nthat CoTM is robust towards high degrees of class imbalance. Being able to\nshare clauses, we believe CoTM will enable new TM application domains that\ninvolve multiple outputs, such as learning language models and auto-encoding.",
          "link": "http://arxiv.org/abs/2108.07594",
          "publishedOn": "2021-08-18T01:54:59.890Z",
          "wordCount": 718,
          "title": "Coalesced Multi-Output Tsetlin Machines with Clause Sharing. (arXiv:2108.07594v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Ajay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkountouna_O/0/1/0/all/0/1\">Olga Gkountouna</a>",
          "description": "We present a demonstration of REACT, a new Real-time Educational AI-powered\nClassroom Tool that employs EDM techniques for supporting the decision-making\nprocess of educators. REACT is a data-driven tool with a user-friendly\ngraphical interface. It analyzes students' performance data and provides\ncontext-based alerts as well as recommendations to educators for course\nplanning. Furthermore, it incorporates model-agnostic explanations for bringing\nexplainability and interpretability in the process of decision making. This\npaper demonstrates a use case scenario of our proposed tool using a real-world\ndataset and presents the design of its architecture and user interface. This\ndemonstration focuses on the agglomerative clustering of students based on\ntheir performance (i.e., incorrect responses and hints used) during an in-class\nactivity. This formation of clusters of students with similar strengths and\nweaknesses may help educators to improve their course planning by identifying\nat-risk students, forming study groups, or encouraging tutoring between\nstudents of different strengths.",
          "link": "http://arxiv.org/abs/2108.07693",
          "publishedOn": "2021-08-18T01:54:59.883Z",
          "wordCount": 596,
          "title": "Demonstrating REACT: a Real-time Educational AI-powered Classroom Tool. (arXiv:2108.07693v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1\">Yanli Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oesterle_J/0/1/0/all/0/1\">Jonathan Oesterle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Euler_T/0/1/0/all/0/1\">Thomas Euler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1\">Philipp Berens</a>",
          "description": "Spatio-temporal receptive field (STRF) models are frequently used to\napproximate the computation implemented by a sensory neuron. Typically, such\nSTRFs are assumed to be smooth and sparse. Current state-of-the-art approaches\nfor estimating STRFs based on empirical Bayes are often not computationally\nefficient in high-dimensional settings, as encountered in sensory neuroscience.\nHere we pursued an alternative approach and encode prior knowledge for\nestimation of STRFs by choosing a set of basis functions with the desired\nproperties: natural cubic splines. Our method is computationally efficient and\ncan be easily applied to a wide range of existing models. We compared the\nperformance of spline-based methods to non-spline ones on simulated and\nexperimental data, showing that spline-based methods consistently outperform\nthe non-spline versions.",
          "link": "http://arxiv.org/abs/2108.07537",
          "publishedOn": "2021-08-18T01:54:59.861Z",
          "wordCount": 558,
          "title": "Estimating smooth and sparse neural receptive fields with a flexible spline basis. (arXiv:2108.07537v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongji Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caihua Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1\">Khaled B. Letaief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>",
          "description": "Graph convolutional networks (GCNs) have recently enabled a popular class of\nalgorithms for collaborative filtering (CF). Nevertheless, the theoretical\nunderpinnings of their empirical successes remain elusive. In this paper, we\nendeavor to obtain a better understanding of GCN-based CF methods via the lens\nof graph signal processing. By identifying the critical role of smoothness, a\nkey concept in graph signal processing, we develop a unified graph\nconvolution-based framework for CF. We prove that many existing CF methods are\nspecial cases of this framework, including the neighborhood-based methods,\nlow-rank matrix factorization, linear auto-encoders, and LightGCN,\ncorresponding to different low-pass filters. Based on our framework, we then\npresent a simple and computationally efficient CF baseline, which we shall\nrefer to as Graph Filter based Collaborative Filtering (GF-CF). Given an\nimplicit feedback matrix, GF-CF can be obtained in a closed form instead of\nexpensive training with back-propagation. Experiments will show that GF-CF\nachieves competitive or better performance against deep learning-based methods\non three well-known datasets, notably with a $70\\%$ performance gain over\nLightGCN on the Amazon-book dataset.",
          "link": "http://arxiv.org/abs/2108.07567",
          "publishedOn": "2021-08-18T01:54:59.852Z",
          "wordCount": 619,
          "title": "How Powerful is Graph Convolution for Recommendation?. (arXiv:2108.07567v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaibo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Graph-based anomaly detection has been widely used for detecting malicious\nactivities in real-world applications. Existing attempts to address this\nproblem have thus far focused on structural feature engineering or learning in\nthe binary classification regime. In this work, we propose to leverage graph\ncontrastive coding and present the supervised GCCAD model for contrasting\nabnormal nodes with normal ones in terms of their distances to the global\ncontext (e.g., the average of all nodes). To handle scenarios with scarce\nlabels, we further enable GCCAD as a self-supervised framework by designing a\ngraph corrupting strategy for generating synthetic node labels. To achieve the\ncontrastive objective, we design a graph neural network encoder that can infer\nand further remove suspicious links during message passing, as well as learn\nthe global context of the input graph. We conduct extensive experiments on four\npublic datasets, demonstrating that 1) GCCAD significantly and consistently\noutperforms various advanced baselines and 2) its self-supervised version\nwithout fine-tuning can achieve comparable performance with its fully\nsupervised version.",
          "link": "http://arxiv.org/abs/2108.07516",
          "publishedOn": "2021-08-18T01:54:59.845Z",
          "wordCount": 610,
          "title": "GCCAD: Graph Contrastive Coding for Anomaly Detection. (arXiv:2108.07516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>",
          "description": "It's challenging to customize transducer-based automatic speech recognition\n(ASR) system with context information which is dynamic and unavailable during\nmodel training. In this work, we introduce a light-weight contextual spelling\ncorrection model to correct context-related recognition errors in\ntransducer-based ASR systems. We incorporate the context information into the\nspelling correction model with a shared context encoder and use a filtering\nalgorithm to handle large-size context lists. Experiments show that the model\nimproves baseline ASR model performance with about 50% relative word error rate\nreduction, which also significantly outperforms the baseline method such as\ncontextual LM biasing. The model also shows excellent performance for\nout-of-vocabulary terms not seen during training.",
          "link": "http://arxiv.org/abs/2108.07493",
          "publishedOn": "2021-08-18T01:54:59.838Z",
          "wordCount": 566,
          "title": "A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Feng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_A/0/1/0/all/0/1\">Ajith Kumar V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanci Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qikui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ansi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1\">Dhruv Makwana</a>",
          "description": "To further improve the performance and the self-learning ability of GCNs, in\nthis paper, we propose an efficient self-supervised learning strategy of GCNs,\nnamed randomly removed links with a fixed step at one region (RRLFSOR). In\naddition, we also propose another self-supervised learning strategy of GCNs,\nnamed randomly removing links with a fixed step at some blocks (RRLFSSB), to\nsolve the problem that adjacent nodes have no selected step. Experiments on\ntransductive link prediction tasks show that our strategies outperform the\nbaseline models consistently by up to 21.34% in terms of accuracy on three\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2108.07481",
          "publishedOn": "2021-08-18T01:54:59.830Z",
          "wordCount": 538,
          "title": "RRLFSOR: An Efficient Self-Supervised Learning Strategy of Graph Convolutional Networks. (arXiv:2108.07481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Puyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>",
          "description": "Randomized coordinate descent (RCD) is a popular optimization algorithm with\nwide applications in solving various machine learning problems, which motivates\na lot of theoretical analysis on its convergence behavior. As a comparison,\nthere is no work studying how the models trained by RCD would generalize to\ntest examples. In this paper, we initialize the generalization analysis of RCD\nby leveraging the powerful tool of algorithmic stability. We establish argument\nstability bounds of RCD for both convex and strongly convex objectives, from\nwhich we develop optimal generalization bounds by showing how to early-stop the\nalgorithm to tradeoff the estimation and optimization. Our analysis shows that\nRCD enjoys better stability as compared to stochastic gradient descent.",
          "link": "http://arxiv.org/abs/2108.07414",
          "publishedOn": "2021-08-18T01:54:59.803Z",
          "wordCount": 544,
          "title": "Stability and Generalization for Randomized Coordinate Descent. (arXiv:2108.07414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongyoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Successful sequential recommendation systems rely on accurately capturing the\nuser's short-term and long-term interest. Although Transformer-based models\nachieved state-of-the-art performance in the sequential recommendation task,\nthey generally require quadratic memory and time complexity to the sequence\nlength, making it difficult to extract the long-term interest of users. On the\nother hand, Multi-Layer Perceptrons (MLP)-based models, renowned for their\nlinear memory and time complexity, have recently shown competitive results\ncompared to Transformer in various tasks. Given the availability of a massive\namount of the user's behavior history, the linear memory and time complexity of\nMLP-based models make them a promising alternative to explore in the sequential\nrecommendation task. To this end, we adopted MLP-based models in sequential\nrecommendation but consistently observed that MLP-based methods obtain lower\nperformance than those of Transformer despite their computational benefits.\nFrom experiments, we observed that introducing explicit high-order interactions\nto MLP layers mitigates such performance gap. In response, we propose the\nMulti-Order Interaction (MOI) layer, which is capable of expressing an\narbitrary order of interactions within the inputs while maintaining the memory\nand time complexity of the MLP layer. By replacing the MLP layer with the MOI\nlayer, our model was able to achieve comparable performance with\nTransformer-based models while retaining the MLP-based models' computational\nbenefits.",
          "link": "http://arxiv.org/abs/2108.07505",
          "publishedOn": "2021-08-18T01:54:59.797Z",
          "wordCount": 658,
          "title": "MOI-Mixer: Improving MLP-Mixer with Multi Order Interactions in Sequential Recommendation. (arXiv:2108.07505v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jessie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_B/0/1/0/all/0/1\">Blanca Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_S/0/1/0/all/0/1\">Sebastiano Barbieri</a>",
          "description": "In this study we propose the Learning to Defer with Uncertainty (LDU)\nalgorithm, an approach which considers the model's predictive uncertainty when\nidentifying the patient group to be evaluated by human experts. Our aim is to\nensure patient safety when ML models are deployed in healthcare settings.",
          "link": "http://arxiv.org/abs/2108.07392",
          "publishedOn": "2021-08-18T01:54:59.784Z",
          "wordCount": 484,
          "title": "Incorporating Uncertainty in Learning to Defer Algorithms for Safe Computer-Aided Diagnosis. (arXiv:2108.07392v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1\">Chiranjibi Sitaula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinyuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadarshi_A/0/1/0/all/0/1\">Archana Priyadarshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tracy_M/0/1/0/all/0/1\">Mark Tracy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehei_O/0/1/0/all/0/1\">Omid Kavehei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinder_M/0/1/0/all/0/1\">Murray Hinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Withana_A/0/1/0/all/0/1\">Anusha Withana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McEwan_A/0/1/0/all/0/1\">Alistair McEwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzbanrad_F/0/1/0/all/0/1\">Faezeh Marzbanrad</a>",
          "description": "Abdominal auscultation is a convenient, safe and inexpensive method to assess\nbowel conditions, which is essential in neonatal care. It helps early detection\nof neonatal bowel dysfunctions and allows timely intervention. This paper\npresents a neonatal bowel sound detection method to assist the auscultation.\nSpecifically, a Convolutional Neural Network (CNN) is proposed to classify\nperistalsis and non-peristalsis sounds. The classification is then optimized\nusing a Laplace Hidden Semi-Markov Model (HSMM). The proposed method is\nvalidated on abdominal sounds from 49 newborn infants admitted to our tertiary\nNeonatal Intensive Care Unit (NICU). The results show that the method can\neffectively detect bowel sounds with accuracy and area under curve (AUC) score\nbeing 89.81% and 83.96% respectively, outperforming 13 baseline methods.\nFurthermore, the proposed Laplace HSMM refinement strategy is proven capable to\nenhance other bowel sound detection models. The outcomes of this work have the\npotential to facilitate future telehealth applications for neonatal care. The\nsource code of our work can be found at:\nhttps://bitbucket.org/chirudeakin/neonatal-bowel-sound-classification/",
          "link": "http://arxiv.org/abs/2108.07467",
          "publishedOn": "2021-08-18T01:54:59.742Z",
          "wordCount": 637,
          "title": "Neonatal Bowel Sound Detection Using Convolutional Neural Network and Laplace Hidden Semi-Markov Model. (arXiv:2108.07467v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen-Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>",
          "description": "While information delivery in industrial Internet of things demands\nreliability and latency guarantees, the freshness of the controller's available\ninformation, measured by the age of information (AoI), is paramount for\nhigh-performing industrial automation. The problem in this work is cast as a\nsensor's transmit power minimization subject to the peak-AoI requirement and a\nprobabilistic constraint on queuing latency. We further characterize the tail\nbehavior of the latency by a generalized Pareto distribution (GPD) for solving\nthe power allocation problem through Lyapunov optimization. As each sensor\nutilizes its own data to locally train the GPD model, we incorporate federated\nlearning and propose a local-model selection approach which accounts for\ncorrelation among the sensor's training data. Numerical results show the\ntradeoff between the transmit power, peak AoI, and delay's tail distribution.\nFurthermore, we verify the superiority of the proposed correlation-aware\napproach for selecting the local models in federated learning over an existing\nbaseline.",
          "link": "http://arxiv.org/abs/2108.07504",
          "publishedOn": "2021-08-18T01:54:59.732Z",
          "wordCount": 605,
          "title": "Federated Learning with Correlated Data: Taming the Tail for Age-Optimal Industrial IoT. (arXiv:2108.07504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angelakis_A/0/1/0/all/0/1\">A. Angelakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulioti_I/0/1/0/all/0/1\">I. Soulioti</a>",
          "description": "We train a machine learning model on a dataset of 2177 individuals using as\nfeatures 26 probe sets and their age in order to classify if someone has acute\nmyeloid leukaemia or is healthy. The dataset is multicentric and consists of\ndata from 27 organisations, 25 cities, 15 countries and 4 continents. The\naccuracy or our model is 99.94\\% and its F1-score 0.9996. To the best of our\nknowledge the performance of our model is the best one in the literature, as\nregards the prediction of AML using similar or not data. Moreover, there has\nnot been any bibliographic reference associated with acute myeloid leukaemia\nfor the 26 probe sets we used as features in our model.",
          "link": "http://arxiv.org/abs/2108.07396",
          "publishedOn": "2021-08-18T01:54:59.695Z",
          "wordCount": 547,
          "title": "Diagnosis of Acute Myeloid Leukaemia Using Machine Learning. (arXiv:2108.07396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhijian Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaotie Deng</a>",
          "description": "In this paper, we propose a general meta learning approach to computing\napproximate Nash equilibrium for finite $n$-player normal-form games. Unlike\nexisting solutions that approximate or learn a Nash equilibrium from scratch\nfor each of the games, our meta solver directly constructs a mapping from a\ngame utility matrix to a joint strategy profile. The mapping is parameterized\nand learned in a self-supervised fashion by a proposed Nash equilibrium\napproximation metric without ground truth data informing any Nash equilibrium.\nAs such, it can immediately predict the joint strategy profile that\napproximates a Nash equilibrium for any unseen new game under the same game\ndistribution. Moreover, the meta-solver can be further fine-tuned and adaptive\nto a new game if iteration updates are allowed. We theoretically prove that our\nmeta-solver is not affected by the non-smoothness of exact Nash equilibrium\nsolutions, and derive a sample complexity bound to demonstrate its\ngeneralization ability across normal-form games. Experimental results\ndemonstrate its substantial approximation power against other strong baselines\nin both adaptive and non-adaptive cases.",
          "link": "http://arxiv.org/abs/2108.07472",
          "publishedOn": "2021-08-18T01:54:59.657Z",
          "wordCount": 616,
          "title": "Learning to Compute Approximate Nash Equilibrium for Normal-form Games. (arXiv:2108.07472v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Didong Li</a>",
          "description": "Over a complete Riemannian manifold of finite dimension, Greene and Wu\nintroduced a convolution, known as Greene-Wu (GW) convolution. In this paper,\nwe introduce a reformulation of the GW convolution. Using our reformulation,\nmany properties of the GW convolution can be easily derived, including a new\nformula for how the curvature of the space would affect the curvature of the\nfunction through the GW convolution. Also enabled by our new reformulation, an\nimproved method for gradient estimation over Riemannian manifolds is\nintroduced. Theoretically, our gradient estimation method improves the order of\nestimation error from $O \\left( \\left( n + 3 \\right)^{3/2} \\right)$ to $O\n\\left( n^{3/2} \\right)$, where $n$ is the dimension of the manifold.\nEmpirically, our method outperforms the best existing method for gradient\nestimation over Riemannian manifolds, as evidenced by thorough experimental\nevaluations.",
          "link": "http://arxiv.org/abs/2108.07406",
          "publishedOn": "2021-08-18T01:54:59.577Z",
          "wordCount": 578,
          "title": "From the Greene--Wu Convolution to Gradient Estimation over Riemannian Manifolds. (arXiv:2108.07406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>",
          "description": "With the increasing demands for privacy protection, privacy-preserving\nmachine learning has been drawing much attention in both academia and industry.\nHowever, most existing methods have their limitations in practical\napplications. On the one hand, although most cryptographic methods are provable\nsecure, they bring heavy computation and communication. On the other hand, the\nsecurity of many relatively efficient private methods (e.g., federated learning\nand split learning) is being questioned, since they are non-provable secure.\nInspired by previous work on privacy-preserving machine learning, we build a\nprivacy-preserving machine learning framework by combining random permutation\nand arithmetic secret sharing via our compute-after-permutation technique.\nSince our method reduces the cost for element-wise function computation, it is\nmore efficient than existing cryptographic methods. Moreover, by adopting\ndistance correlation as a metric for privacy leakage, we demonstrate that our\nmethod is more secure than previous non-provable secure methods. Overall, our\nproposal achieves a good balance between security and efficiency. Experimental\nresults show that our method not only is up to 6x faster and reduces up to 85%\nnetwork traffic compared with state-of-the-art cryptographic methods, but also\nleaks less privacy during the training process compared with non-provable\nsecure methods.",
          "link": "http://arxiv.org/abs/2108.07463",
          "publishedOn": "2021-08-18T01:54:59.550Z",
          "wordCount": 632,
          "title": "Towards Secure and Practical Machine Learning via Secret Sharing and Random Permutation. (arXiv:2108.07463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.",
          "link": "http://arxiv.org/abs/2108.07435",
          "publishedOn": "2021-08-18T01:54:59.531Z",
          "wordCount": 615,
          "title": "Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yu Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Liang Lan</a>",
          "description": "Factorization Machines (FM), a general predictor that can efficiently model\nfeature interactions in linear time, was primarily proposed for collaborative\nrecommendation and have been broadly used for regression, classification and\nranking tasks. Subspace Encoding Factorization Machine (SEFM) has been proposed\nrecently to overcome the expressiveness limitation of Factorization Machines\n(FM) by applying explicit nonlinear feature mapping for both individual\nfeatures and feature interactions through one-hot encoding to each input\nfeature. Despite the effectiveness of SEFM, it increases the memory cost of FM\nby $b$ times, where $b$ is the number of bins when applying one-hot encoding on\neach input feature. To reduce the memory cost of SEFM, we propose a new method\ncalled Binarized FM which constraints the model parameters to be binary values\n(i.e., 1 or $-1$). Then each parameter value can be efficiently stored in one\nbit. Our proposed method can significantly reduce the memory cost of SEFM\nmodel. In addition, we propose a new algorithm to effectively and efficiently\nlearn proposed FM with binary constraints using Straight Through Estimator\n(STE) with Adaptive Gradient Descent (Adagrad). Finally, we evaluate the\nperformance of our proposed method on eight different classification datasets.\nOur experimental results have demonstrated that our proposed method achieves\ncomparable accuracy with SEFM but with much less memory cost.",
          "link": "http://arxiv.org/abs/2108.07421",
          "publishedOn": "2021-08-18T01:54:59.491Z",
          "wordCount": 642,
          "title": "Memory-Efficient Factorization Machines via Binarizing both Data and Model Coefficients. (arXiv:2108.07421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duta_I/0/1/0/all/0/1\">Ionut Cosmin Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "We propose contextual convolution (CoConv) for visual recognition. CoConv is\na direct replacement of the standard convolution, which is the core component\nof convolutional neural networks. CoConv is implicitly equipped with the\ncapability of incorporating contextual information while maintaining a similar\nnumber of parameters and computational cost compared to the standard\nconvolution. CoConv is inspired by neuroscience studies indicating that (i)\nneurons, even from the primary visual cortex (V1 area), are involved in\ndetection of contextual cues and that (ii) the activity of a visual neuron can\nbe influenced by the stimuli placed entirely outside of its theoretical\nreceptive field. On the one hand, we integrate CoConv in the widely-used\nresidual networks and show improved recognition performance over baselines on\nthe core tasks and benchmarks for visual recognition, namely image\nclassification on the ImageNet data set and object detection on the MS COCO\ndata set. On the other hand, we introduce CoConv in the generator of a\nstate-of-the-art Generative Adversarial Network, showing improved generative\nresults on CIFAR-10 and CelebA. Our code is available at\nhttps://github.com/iduta/coconv.",
          "link": "http://arxiv.org/abs/2108.07387",
          "publishedOn": "2021-08-18T01:54:59.481Z",
          "wordCount": 621,
          "title": "Contextual Convolutional Neural Networks. (arXiv:2108.07387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Eliana Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfaro_L/0/1/0/all/0/1\">Luca de Alfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1\">Elena Baralis</a>",
          "description": "When analyzing the behavior of machine learning algorithms, it is important\nto identify specific data subgroups for which the considered algorithm shows\ndifferent performance with respect to the entire dataset. The intervention of\ndomain experts is normally required to identify relevant attributes that define\nthese subgroups.\n\nWe introduce the notion of divergence to measure this performance difference\nand we exploit it in the context of (i) classification models and (ii) ranking\napplications to automatically detect data subgroups showing a significant\ndeviation in their behavior. Furthermore, we quantify the contribution of all\nattributes in the data subgroup to the divergent behavior by means of Shapley\nvalues, thus allowing the identification of the most impacting attributes.",
          "link": "http://arxiv.org/abs/2108.07450",
          "publishedOn": "2021-08-18T01:54:59.474Z",
          "wordCount": 564,
          "title": "Identifying Biased Subgroups in Ranking and Classification. (arXiv:2108.07450v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+AlQuabeh_H/0/1/0/all/0/1\">Hilal AlQuabeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawazeer_A/0/1/0/all/0/1\">Ameera Bawazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashmi_A/0/1/0/all/0/1\">Abdulateef Alhashmi</a>",
          "description": "Data labeling in supervised learning is considered an expensive and\ninfeasible tool in some conditions. The self-supervised learning method is\nproposed to tackle the learning effectiveness with fewer labeled data, however,\nthere is a lack of confidence in the size of labeled data needed to achieve\nadequate results. This study aims to draw a baseline on the proportion of the\nlabeled data that models can appreciate to yield competent accuracy when\ncompared to training with additional labels. The study implements the\nkaggle.com' cats-vs-dogs dataset, Mnist and Fashion-Mnist to investigate the\nself-supervised learning task by implementing random rotations augmentation on\nthe original datasets. To reveal the true effectiveness of the pretext process\nin self-supervised learning, the original dataset is divided into smaller\nbatches, and learning is repeated on each batch with and without the pretext\npre-training. Results show that the pretext process in the self-supervised\nlearning improves the accuracy around 15% in the downstream classification task\nwhen compared to the plain supervised learning.",
          "link": "http://arxiv.org/abs/2108.07464",
          "publishedOn": "2021-08-18T01:54:59.466Z",
          "wordCount": 613,
          "title": "Investigating a Baseline Of Self Supervised Learning Towards Reducing Labeling Costs For Image Classification. (arXiv:2108.07464v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>",
          "description": "Federated learning is a distributed machine learning paradigm where multiple\ndata owners (clients) collaboratively train one machine learning model while\nkeeping data on their own devices. The heterogeneity of client datasets is one\nof the most important challenges of federated learning algorithms. Studies have\nfound performance reduction with standard federated algorithms, such as FedAvg,\non non-IID data. Many existing works on handling non-IID data adopt the same\naggregation framework as FedAvg and focus on improving model updates either on\nthe server side or on clients. In this work, we tackle this challenge in a\ndifferent view by introducing redistribution rounds that delay the aggregation.\nWe perform experiments on multiple tasks and show that the proposed framework\nsignificantly improves the performance on non-IID data.",
          "link": "http://arxiv.org/abs/2108.07433",
          "publishedOn": "2021-08-18T01:54:59.457Z",
          "wordCount": 550,
          "title": "Aggregation Delayed Federated Learning. (arXiv:2108.07433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07339",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fredieu_C/0/1/0/all/0/1\">C. Tanner Fredieu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bui_J/0/1/0/all/0/1\">Justin Bui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martone_A/0/1/0/all/0/1\">Anthony Martone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marks_R/0/1/0/all/0/1\">Robert J. Marks II</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baylis_C/0/1/0/all/0/1\">Charles Baylis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>",
          "description": "In this paper, we examine the use of a deep multi-layer perceptron model\narchitecture to classify received signal samples as coming from one of four\ncommon waveforms, Single Carrier (SC), Single-Carrier Frequency Division\nMultiple Access (SC-FDMA), Orthogonal Frequency Division Multiplexing (OFDM),\nand Linear Frequency Modulation (LFM), used in communication and radar\nnetworks. Synchronization of the signals is not needed as we assume there is an\nunknown and uncompensated time and frequency offset. An autoencoder with a deep\nCNN architecture is also examined to create a new fifth classification category\nof an unknown waveform type. This is accomplished by calculating a minimum and\nmaximum threshold values from the root mean square error (RMSE) of the radar\nand communication waveforms. The classifier and autoencoder work together to\nmonitor a spectrum area to identify the common waveforms inside the area of\noperation along with detecting unknown waveforms. Results from testing showed\nthe classifier had 100\\% classification rate above 0 dB with accuracy of 83.2\\%\nand 94.7\\% at -10 dB and -5 dB, respectively, with signal impairments present.\nResults for the anomaly detector showed 85.3\\% accuracy at 0 dB with 100\\% at\nSNR greater than 0 dB with signal impairments present when using a high-value\nFast Fourier Transform (FFT) size. Accurate detection rates decline as\nadditional noise is introduced to the signals, with 78.1\\% at -5 dB and 56.5\\%\nat -10 dB. However, these low rates seen can be potentially mitigated by using\neven higher FFT sizes also shown in our results.",
          "link": "http://arxiv.org/abs/2108.07339",
          "publishedOn": "2021-08-18T01:54:59.435Z",
          "wordCount": 698,
          "title": "Classification of Common Waveforms Including a Watchdog for Unknown Signals. (arXiv:2108.07339v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07453",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yankun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hemmings Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sawan_M/0/1/0/all/0/1\">Mohamad Sawan</a>",
          "description": "An accurate seizure prediction system enables early warnings before seizure\nonset of epileptic patients. It is extremely important for drug-refractory\npatients. Conventional seizure prediction works usually rely on features\nextracted from Electroencephalography (EEG) recordings and classification\nalgorithms such as regression or support vector machine (SVM) to locate the\nshort time before seizure onset. However, such methods cannot achieve\nhigh-accuracy prediction due to information loss of the hand-crafted features\nand the limited classification ability of regression and SVM algorithms. We\npropose an end-to-end deep learning solution using a convolutional neural\nnetwork (CNN) in this paper. One and two dimensional kernels are adopted in the\nearly- and late-stage convolution and max-pooling layers, respectively. The\nproposed CNN model is evaluated on Kaggle intracranial and CHB-MIT scalp EEG\ndatasets. Overall sensitivity, false prediction rate, and area under receiver\noperating characteristic curve reaches 93.5%, 0.063/h, 0.981 and 98.8%,\n0.074/h, 0.988 on two datasets respectively. Comparison with state-of-the-art\nworks indicates that the proposed model achieves exceeding prediction\nperformance.",
          "link": "http://arxiv.org/abs/2108.07453",
          "publishedOn": "2021-08-18T01:54:59.425Z",
          "wordCount": 614,
          "title": "An End-to-End Deep Learning Approach for Epileptic Seizure Prediction. (arXiv:2108.07453v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Aritra Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>",
          "description": "Computerized adaptive testing (CAT) refers to a form of tests that are\npersonalized to every student/test taker. CAT methods adaptively select the\nnext most informative question/item for each student given their responses to\nprevious questions, effectively reducing test length. Existing CAT methods use\nitem response theory (IRT) models to relate student ability to their responses\nto questions and static question selection algorithms designed to reduce the\nability estimation error as quickly as possible; therefore, these algorithms\ncannot improve by learning from large-scale student response data. In this\npaper, we propose BOBCAT, a Bilevel Optimization-Based framework for CAT to\ndirectly learn a data-driven question selection algorithm from training data.\nBOBCAT is agnostic to the underlying student response model and is\ncomputationally efficient during the adaptive testing process. Through\nextensive experiments on five real-world student response datasets, we show\nthat BOBCAT outperforms existing CAT methods (sometimes significantly) at\nreducing test length.",
          "link": "http://arxiv.org/abs/2108.07386",
          "publishedOn": "2021-08-18T01:54:59.416Z",
          "wordCount": 581,
          "title": "BOBCAT: Bilevel Optimization-Based Computerized Adaptive Testing. (arXiv:2108.07386v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07380",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Subhadeep Mukhopadhyay</a>",
          "description": "We have entered a new era of machine learning (ML), where the most accurate\nalgorithm with superior predictive power may not even be deployable, unless it\nis admissible under the regulatory constraints. This has led to great interest\nin developing fair, transparent and trustworthy ML methods. The purpose of this\narticle is to introduce a new information-theoretic learning framework\n(admissible machine learning) and algorithmic risk-management tools (InfoGram,\nL-features, ALFA-testing) that can guide an analyst to redesign off-the-shelf\nML methods to be regulatory compliant, while maintaining good prediction\naccuracy. We have illustrated our approach using several real-data examples\nfrom financial sectors, biomedical research, marketing campaigns, and the\ncriminal justice system.",
          "link": "http://arxiv.org/abs/2108.07380",
          "publishedOn": "2021-08-18T01:54:59.410Z",
          "wordCount": 554,
          "title": "InfoGram and Admissible Machine Learning. (arXiv:2108.07380v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1\">Jeremy C. Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1\">Wolfgang Nejdl</a>",
          "description": "As Artificial Intelligence (AI) is used in more applications, the need to\nconsider and mitigate biases from the learned models has followed. Most works\nin developing fair learning algorithms focus on the offline setting. However,\nin many real-world applications data comes in an online fashion and needs to be\nprocessed on the fly. Moreover, in practical application, there is a trade-off\nbetween accuracy and fairness that needs to be accounted for, but current\nmethods often have multiple hyperparameters with non-trivial interaction to\nachieve fairness. In this paper, we propose a flexible ensemble algorithm for\nfair decision-making in the more challenging context of evolving online\nsettings. This algorithm, called FARF (Fair and Adaptive Random Forests), is\nbased on using online component classifiers and updating them according to the\ncurrent distribution, that also accounts for fairness and a single\nhyperparameters that alters fairness-accuracy balance. Experiments on\nreal-world discriminated data streams demonstrate the utility of FARF.",
          "link": "http://arxiv.org/abs/2108.07403",
          "publishedOn": "2021-08-18T01:54:59.402Z",
          "wordCount": 589,
          "title": "FARF: A Fair and Adaptive Random Forests Classifier. (arXiv:2108.07403v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qin Zhang</a>",
          "description": "We study the problem of learning to cluster data points using an oracle which\ncan answer same-cluster queries. Different from previous approaches, we do not\nassume that the total number of clusters is known at the beginning and do not\nrequire that the true clusters are consistent with a predefined objective\nfunction such as the K-means. These relaxations are critical from the practical\nperspective and, meanwhile, make the problem more challenging. We propose two\nalgorithms with provable theoretical guarantees and verify their effectiveness\nvia an extensive set of experiments on both synthetic and real-world data.",
          "link": "http://arxiv.org/abs/2108.07383",
          "publishedOn": "2021-08-18T01:54:59.380Z",
          "wordCount": 520,
          "title": "Learning to Cluster via Same-Cluster Queries. (arXiv:2108.07383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Guruprasad Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rahul Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "In many applications, finding adequate labeled data to train predictive\nmodels is a major challenge. In this work, we propose methods to use\ngroup-level binary labels as weak supervision to train instance-level binary\nclassification models. Aggregate labels are common in several domains where\nannotating on a group-level might be cheaper or might be the only way to\nprovide annotated data without infringing on privacy. We model group-level\nlabels as Class Conditional Noisy (CCN) labels for individual instances and use\nthe noisy labels to regularize predictions of the model trained on the\nstrongly-labeled instances. Our experiments on real-world application of land\ncover mapping shows the utility of the proposed method in leveraging\ngroup-level labels, both in the presence and absence of class imbalance.",
          "link": "http://arxiv.org/abs/2108.07330",
          "publishedOn": "2021-08-18T01:54:59.372Z",
          "wordCount": 554,
          "title": "Weakly Supervised Classification Using Group-Level Labels. (arXiv:2108.07330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torfah_H/0/1/0/all/0/1\">Hazem Torfah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shetal Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Supratik Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akshay_S/0/1/0/all/0/1\">S. Akshay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1\">Sanjit A. Seshia</a>",
          "description": "We present a new multi-objective optimization approach for synthesizing\ninterpretations that \"explain\" the behavior of black-box machine learning\nmodels. Constructing human-understandable interpretations for black-box models\noften requires balancing conflicting objectives. A simple interpretation may be\neasier to understand for humans while being less precise in its predictions\nvis-a-vis a complex interpretation. Existing methods for synthesizing\ninterpretations use a single objective function and are often optimized for a\nsingle class of interpretations. In contrast, we provide a more general and\nmulti-objective synthesis framework that allows users to choose (1) the class\nof syntactic templates from which an interpretation should be synthesized, and\n(2) quantitative measures on both the correctness and explainability of an\ninterpretation. For a given black-box, our approach yields a set of\nPareto-optimal interpretations with respect to the correctness and\nexplainability measures. We show that the underlying multi-objective\noptimization problem can be solved via a reduction to quantitative constraint\nsolving, such as weighted maximum satisfiability. To demonstrate the benefits\nof our approach, we have applied it to synthesize interpretations for black-box\nneural-network classifiers. Our experiments show that there often exists a rich\nand varied set of choices for interpretations that are missed by existing\napproaches.",
          "link": "http://arxiv.org/abs/2108.07307",
          "publishedOn": "2021-08-18T01:54:59.364Z",
          "wordCount": 644,
          "title": "Synthesizing Pareto-Optimal Interpretations for Black-Box Models. (arXiv:2108.07307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>",
          "description": "The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nCurrent metrics suggest that contextualized word embedding models do not\nuniformly utilize all dimensions when embedding tokens in vector space. Here we\nargue that existing metrics are fragile and tend to obfuscate the true spatial\ndistribution of point clouds. To ameliorate this issue, we propose IsoScore: a\nnovel metric which quantifies the degree to which a point cloud uniformly\nutilizes the ambient vector space. We demonstrate that IsoScore has several\ndesirable properties such as mean invariance and direct correspondence to the\nnumber of dimensions used, which are properties that existing scores do not\npossess. Furthermore, IsoScore is conceptually intuitive and computationally\nefficient, making it well suited for analyzing the distribution of point clouds\nin arbitrary vector spaces, not necessarily limited to those of word embeddings\nalone. Additionally, we use IsoScore to demonstrate that a number of recent\nconclusions in the NLP literature that have been derived using brittle metrics\nof spatial distribution, such as average cosine similarity, may be incomplete\nor altogether inaccurate.",
          "link": "http://arxiv.org/abs/2108.07344",
          "publishedOn": "2021-08-18T01:54:59.357Z",
          "wordCount": 620,
          "title": "IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gary Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_K/0/1/0/all/0/1\">Karan Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>",
          "description": "We study the performance of federated learning algorithms and their variants\nin an asymptotic framework. Our starting point is the formulation of federated\nlearning as a multi-criterion objective, where the goal is to minimize each\nclient's loss using information from all of the clients. We propose a linear\nregression model, where, for a given client, we theoretically compare the\nperformance of various algorithms in the high-dimensional asymptotic limit.\nThis asymptotic multi-criterion approach naturally models the high-dimensional,\nmany-device nature of federated learning and suggests that personalization is\ncentral to federated learning. Our theory suggests that Fine-tuned Federated\nAveraging (FTFA), i.e., Federated Averaging followed by local training, and the\nridge regularized variant Ridge-tuned Federated Averaging (RTFA) are\ncompetitive with more sophisticated meta-learning and proximal-regularized\napproaches. In addition to being conceptually simpler, FTFA and RTFA are\ncomputationally more efficient than its competitors. We corroborate our\ntheoretical claims with extensive experiments on federated versions of the\nEMNIST, CIFAR-100, Shakespeare, and Stack Overflow datasets.",
          "link": "http://arxiv.org/abs/2108.07313",
          "publishedOn": "2021-08-18T01:54:59.348Z",
          "wordCount": 612,
          "title": "Fine-tuning is Fine in Federated Learning. (arXiv:2108.07313v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07356",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cutler_J/0/1/0/all/0/1\">Joshua Cutler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1\">Dmitriy Drusvyatskiy</a>, <a href=\"http://arxiv.org/find/math/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "We consider the problem of minimizing a convex function that is evolving in\ntime according to unknown and possibly stochastic dynamics. Such problems\nabound in the machine learning and signal processing literature, under the\nnames of concept drift and stochastic tracking. We provide novel non-asymptotic\nconvergence guarantees for stochastic algorithms with iterate averaging,\nfocusing on bounds valid both in expectation and with high probability.\nNotably, we show that the tracking efficiency of the proximal stochastic\ngradient method depends only logarithmically on the initialization quality,\nwhen equipped with a step-decay schedule. The results moreover naturally extend\nto settings where the dynamics depend jointly on time and on the decision\nvariable itself, as in the performative prediction framework.",
          "link": "http://arxiv.org/abs/2108.07356",
          "publishedOn": "2021-08-18T01:54:59.316Z",
          "wordCount": 570,
          "title": "Stochastic optimization under time drift: iterate averaging, step decay, and high probability guarantees. (arXiv:2108.07356v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07316",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Constantin_A/0/1/0/all/0/1\">Andrei Constantin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Harvey_T/0/1/0/all/0/1\">Thomas R. Harvey</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Lukas_A/0/1/0/all/0/1\">Andre Lukas</a>",
          "description": "We use reinforcement learning as a means of constructing string\ncompactifications with prescribed properties. Specifically, we study heterotic\nSO(10) GUT models on Calabi-Yau three-folds with monad bundles, in search of\nphenomenologically promising examples. Due to the vast number of bundles and\nthe sparseness of viable choices, methods based on systematic scanning are not\nsuitable for this class of models. By focusing on two specific manifolds with\nPicard numbers two and three, we show that reinforcement learning can be used\nsuccessfully to explore monad bundles. Training can be accomplished with\nminimal computing resources and leads to highly efficient policy networks. They\nproduce phenomenologically promising states for nearly 100% of episodes and\nwithin a small number of steps. In this way, hundreds of new candidate standard\nmodels are found.",
          "link": "http://arxiv.org/abs/2108.07316",
          "publishedOn": "2021-08-18T01:54:59.307Z",
          "wordCount": 585,
          "title": "Heterotic String Model Building with Monad Bundles and Reinforcement Learning. (arXiv:2108.07316v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gavali_S/0/1/0/all/0/1\">Sachin Gavali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowart_J/0/1/0/all/0/1\">Julie Cowart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shanshan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cathy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1\">Tammy Anderson</a>",
          "description": "In recent years, the US has experienced an opioid epidemic with an\nunprecedented number of drugs overdose deaths. Research finds such overdose\ndeaths are linked to neighborhood-level traits, thus providing opportunity to\nidentify effective interventions. Typically, techniques such as Ordinary Least\nSquares (OLS) or Maximum Likelihood Estimation (MLE) are used to document\nneighborhood-level factors significant in explaining such adverse outcomes.\nThese techniques are, however, less equipped to ascertain non-linear\nrelationships between confounding factors. Hence, in this study we apply\nmachine learning based techniques to identify opioid risks of neighborhoods in\nDelaware and explore the correlation of these factors using Shapley Additive\nexplanations (SHAP). We discovered that the factors related to neighborhoods\nenvironment, followed by education and then crime, were highly correlated with\nhigher opioid risk. We also explored the change in these correlations over the\nyears to understand the changing dynamics of the epidemic. Furthermore, we\ndiscovered that, as the epidemic has shifted from legal (i.e., prescription\nopioids) to illegal (e.g.,heroin and fentanyl) drugs in recent years, the\ncorrelation of environment, crime and health related variables with the opioid\nrisk has increased significantly while the correlation of economic and\nsocio-demographic variables has decreased. The correlation of education related\nfactors has been higher from the start and has increased slightly in recent\nyears suggesting a need for increased awareness about the opioid epidemic.",
          "link": "http://arxiv.org/abs/2108.07301",
          "publishedOn": "2021-08-18T01:54:59.255Z",
          "wordCount": 676,
          "title": "Understanding the factors driving the opioid epidemic using machine learning. (arXiv:2108.07301v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}