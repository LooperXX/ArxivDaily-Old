{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "In recent years, Vietnam witnesses the mass development of social network\nusers on different social platforms such as Facebook, Youtube, Instagram, and\nTiktok. On social medias, hate speech has become a critical problem for social\nnetwork users. To solve this problem, we introduce the ViHSD - a\nhuman-annotated dataset for automatically detecting hate speech on the social\nnetwork. This dataset contains over 30,000 comments, each comment in the\ndataset has one of three labels: CLEAN, OFFENSIVE, or HATE. Besides, we\nintroduce the data creation process for annotating and evaluating the quality\nof the dataset. Finally, we evaluated the dataset by deep learning models and\ntransformer models.",
          "link": "http://arxiv.org/abs/2103.11528",
          "publishedOn": "2021-07-21T02:01:34.365Z",
          "wordCount": 605,
          "title": "A Large-scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts. (arXiv:2103.11528v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Samridhi Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1\">Joseph P. McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>",
          "description": "End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous work in\nthis area has focused on targeted tasks in fixed domains, where the output\nsemantic structure is assumed a priori and the input speech is of limited\ncomplexity. In this work we present our approach to developing an E2E model for\ngeneralized SLU in commercial voice assistants (VAs). We propose a fully\ndifferentiable, transformer-based, hierarchical system that can be pretrained\nat both the ASR and NLU levels. This is then fine-tuned on both transcription\nand semantic classification losses to handle a diverse set of intent and\nargument combinations. This leads to an SLU system that achieves significant\nimprovements over baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark on\nthe popular Fluent Speech Commands dataset. We further evaluate our model on a\nhard test set, exclusively containing slot arguments unseen in training, and\ndemonstrate a nearly 20% improvement, showing the efficacy of our approach in\ntruly demanding VA scenarios.",
          "link": "http://arxiv.org/abs/2106.09009",
          "publishedOn": "2021-07-21T02:01:34.309Z",
          "wordCount": 665,
          "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1\">Amartya Hatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rakesh M. Verma</a>",
          "description": "This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.",
          "link": "http://arxiv.org/abs/2103.08001",
          "publishedOn": "2021-07-21T02:01:33.929Z",
          "wordCount": 598,
          "title": "Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidarysafa_M/0/1/0/all/0/1\">Mojtaba Heidarysafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowsari_K/0/1/0/all/0/1\">Kamran Kowsari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashiri_M/0/1/0/all/0/1\">Masoud Bashiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Donald E. Brown</a>",
          "description": "The growth of the data science field requires better tools to understand such\na fast-paced growing domain. Moreover, individuals from different backgrounds\nbecame interested in following a career as data scientists. Therefore,\nproviding a quantitative guide for individuals and organizations to understand\nthe skills required in the job market would be crucial. This paper introduces a\nframework to analyze the job market for data science-related jobs within the US\nwhile providing an interface to access insights in this market. The proposed\nframework includes three sub-modules allowing continuous data collection,\ninformation extraction, and a web-based dashboard visualization to investigate\nthe spatial and temporal distribution of data science-related jobs and skills.\nThe result of this work shows important skills for the main branches of data\nscience jobs and attempts to provide a skill-based definition of these data\nscience branches. The current version of this application is deployed on the\nweb and allows individuals and institutes to investigate skills required for\ndata science positions through the industry lens.",
          "link": "http://arxiv.org/abs/2106.11077",
          "publishedOn": "2021-07-21T02:01:33.913Z",
          "wordCount": 645,
          "title": "Toward a Knowledge Discovery Framework for Data Science Job Market in the United States. (arXiv:2106.11077v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>",
          "description": "Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.",
          "link": "http://arxiv.org/abs/2107.09356",
          "publishedOn": "2021-07-21T02:01:33.868Z",
          "wordCount": 665,
          "title": "Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zeeshan Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akella_K/0/1/0/all/0/1\">Kartheek Akella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "This work studies the long-standing problems of model capacity and negative\ninterference in multilingual neural machine translation MNMT. We use network\npruning techniques and observe that pruning 50-70% of the parameters from a\ntrained MNMT model results only in a 0.29-1.98 drop in the BLEU score.\nSuggesting that there exist large redundancies even in MNMT models. These\nobservations motivate us to use the redundant parameters and counter the\ninterference problem efficiently. We propose a novel adaptation strategy, where\nwe iteratively prune and retrain the redundant parameters of an MNMT to improve\nbilingual representations while retaining the multilinguality. Negative\ninterference severely affects high resource languages, and our method\nalleviates it without any additional adapter modules. Hence, we call it\nparameter-free adaptation strategy, paving way for the efficient adaptation of\nMNMT. We demonstrate the effectiveness of our method on a 9 language MNMT\ntrained on TED talks, and report an average improvement of +1.36 BLEU on high\nresource pairs. Code will be released here.",
          "link": "http://arxiv.org/abs/2107.09622",
          "publishedOn": "2021-07-21T02:01:33.837Z",
          "wordCount": 587,
          "title": "More Parameters? No Thanks!. (arXiv:2107.09622v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique Ferraz de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da Fontoura Costa</a>",
          "description": "To a good extent, words can be understood as corresponding to patterns or\ncategories that appeared in order to represent concepts and structures that are\nparticularly important or useful in a given time and space. Words are\ncharacterized by not being completely general nor specific, in the sense that\nthe same word can be instantiated or related to several different contexts,\ndepending on specific situations. Indeed, the way in which words are\ninstantiated and associated represents a particularly interesting aspect that\ncan substantially help to better understand the context in which they are\nemployed. Scientific words are no exception to that. In the present work, we\napproach the associations between a set of particularly relevant words in the\nsense of being not only frequently used in several areas, but also representing\nconcepts that are currently related to some of the main standing challenges in\nscience. More specifically, the study reported here takes into account the\nwords \"prediction\", \"model\", \"optimization\", \"complex\", \"entropy\", \"random\",\n\"deterministic\", \"pattern\", and \"database\". In order to complement the\nanalysis, we also obtain a network representing the relationship between the\nadopted areas. Many interesting results were found. First and foremost, several\nof the words were observed to have markedly distinct associations in different\nareas. Biology was found to be related to computer science, sharing\nassociations with databases. Furthermore, for most of the cases, the words\n\"complex\", \"model\", and \"prediction\" were observed to have several strong\nassociations.",
          "link": "http://arxiv.org/abs/2106.14610",
          "publishedOn": "2021-07-21T02:01:33.805Z",
          "wordCount": 696,
          "title": "A keyword-driven approach to science. (arXiv:2106.14610v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach to conformal prediction when the target task has\nlimited data available for training. Conformal prediction identifies a small\nset of promising output candidates in place of a single prediction, with\nguarantees that the set contains the correct answer with high probability. When\ntraining data is limited, however, the predicted set can easily become unusably\nlarge. In this work, we obtain substantially tighter prediction sets while\nmaintaining desirable marginal guarantees by casting conformal prediction as a\nmeta-learning paradigm over exchangeable collections of auxiliary tasks. Our\nconformalization algorithm is simple, fast, and agnostic to the choice of\nunderlying model, learning algorithm, or dataset. We demonstrate the\neffectiveness of this approach across a number of few-shot classification and\nregression tasks in natural language processing, computer vision, and\ncomputational chemistry for drug discovery.",
          "link": "http://arxiv.org/abs/2102.08898",
          "publishedOn": "2021-07-21T02:01:33.797Z",
          "wordCount": 603,
          "title": "Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Romila Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhinaba Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaya_N/0/1/0/all/0/1\">Niyati Chhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We address the problem of recognizing emotion cause in conversations, define\ntwo novel sub-tasks of this problem, and provide a corresponding dialogue-level\ndataset, along with strong Transformer-based baselines. The dataset is\navailable at https://github.com/declare-lab/RECCON.\n\nIntroduction: Recognizing the cause behind emotions in text is a fundamental\nyet under-explored area of research in NLP. Advances in this area hold the\npotential to improve interpretability and performance in affect-based models.\nIdentifying emotion causes at the utterance level in conversations is\nparticularly challenging due to the intermingling dynamics among the\ninterlocutors.\n\nMethod: We introduce the task of Recognizing Emotion Cause in CONversations\nwith an accompanying dataset named \\RECCONDA, containing over 1,000 dialogues\nand 10,000 utterance cause-effect pairs. Furthermore, we define different cause\ntypes based on the source of the causes, and establish strong Transformer-based\nbaselines to address two different sub-tasks on this dataset: causal span\nextraction and causal emotion entailment.\n\nResult: Our Transformer-based baselines, which leverage contextual\npre-trained embeddings, such as RoBERTa, outperform the state-of-the-art\nemotion cause extraction approaches\n\nConclusion: We introduce a new task highly relevant for (explainable)\nemotion-aware artificial intelligence: recognizing emotion cause in\nconversations, provide a new highly challenging publicly available\ndialogue-level dataset for this task, and give strong baseline results on this\ndataset.",
          "link": "http://arxiv.org/abs/2012.11820",
          "publishedOn": "2021-07-21T02:01:33.787Z",
          "wordCount": 692,
          "title": "Recognizing Emotion Cause in Conversations. (arXiv:2012.11820v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>",
          "description": "We study open-domain question answering with structured, unstructured and\nsemi-structured knowledge sources, including text, tables, lists and knowledge\nbases. Departing from prior work, we propose a unifying approach that\nhomogenizes all sources by reducing them to text and applies the\nretriever-reader model which has so far been limited to text sources only. Our\napproach greatly improves the results on knowledge-base QA tasks by 11 points,\ncompared to latest graph-based methods. More importantly, we demonstrate that\nour unified knowledge (UniK-QA) model is a simple and yet effective way to\ncombine heterogeneous sources of knowledge, advancing the state-of-the-art\nresults on two popular question answering benchmarks, NaturalQuestions and\nWebQuestions, by 3.5 and 2.6 points, respectively.",
          "link": "http://arxiv.org/abs/2012.14610",
          "publishedOn": "2021-07-21T02:01:33.780Z",
          "wordCount": 593,
          "title": "UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_H/0/1/0/all/0/1\">Harshita Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1\">Aniket Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Ridam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashima/0/1/0/all/0/1\">Ashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Ananya Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>",
          "description": "Social media plays a pivotal role in disseminating news globally and acts as\na platform for people to express their opinions on various topics. A wide\nvariety of views accompanies COVID-19 vaccination drives across the globe,\noften colored by emotions, which change along with rising cases, approval of\nvaccines, and multiple factors discussed online. This study aims at analyzing\nthe temporal evolution of different Emotion categories: Hesitation, Rage,\nSorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine\nRollout, Misinformation, Health Effects, and Inequities as lexical categories\ncreated from Tweets belonging to five countries with vital vaccine roll-out\nprograms, namely, India, United States of America, Brazil, United Kingdom, and\nAustralia. We extracted a corpus of nearly 1.8 million Twitter posts related to\nCOVID-19 vaccination. Using cosine distance from selected seed words, we\nexpanded the vocabulary of each category and tracked the longitudinal change in\ntheir strength from June 2020 to April 2021. We used community detection\nalgorithms to find modules in positive correlation networks. Our findings\nsuggest that tweets expressing hesitancy towards vaccines contain the highest\nmentions of health-related effects in all countries. Our results indicated that\nthe patterns of hesitancy were variable across geographies and can help us\nlearn targeted interventions. We also observed a significant change in the\nlinear trends of categories like hesitation and contentment before and after\napproval of vaccines. Negative emotions like rage and sorrow gained the highest\nimportance in the alluvial diagram. They formed a significant module with all\nthe influencing factors in April 2021, when India observed the second wave of\nCOVID-19 cases. The relationship between Emotions and Influencing Factors was\nfound to be variable across the countries.",
          "link": "http://arxiv.org/abs/2104.01131",
          "publishedOn": "2021-07-21T02:01:33.773Z",
          "wordCount": 790,
          "title": "Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical Embeddings. (arXiv:2104.01131v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-07-21T02:01:33.752Z",
          "wordCount": 614,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrera_Casado_D/0/1/0/all/0/1\">David Carrera-Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "Biosemiosis is a process of choice-making between simultaneously alternative\noptions. It is well-known that, when sufficiently young children encounter a\nnew word, they tend to interpret it as pointing to a meaning that does not have\na word yet in their lexicon rather than to a meaning that already has a word\nattached. In previous research, the strategy was shown to be optimal from an\ninformation theoretic standpoint. In that framework, interpretation is\nhypothesized to be driven by the minimization of a cost function: the option of\nleast communication cost is chosen. However, the information theoretic model\nemployed in that research neither explains the weakening of that vocabulary\nlearning bias in older children or polylinguals nor reproduces Zipf's\nmeaning-frequency law, namely the non-linear relationship between the number of\nmeanings of a word and its frequency. Here we consider a generalization of the\nmodel that is channeled to reproduce that law. The analysis of the new model\nreveals regions of the phase space where the bias disappears consistently with\nthe weakening or loss of the bias in older children or polylinguals. The model\nis abstract enough to support future research on other levels of life that are\nrelevant to biosemiotics. In the deep learning era, the model is a transparent\nlow-dimensional tool for future experimental research and illustrates the\npredictive power of a theoretical framework originally designed to shed light\non the origins of Zipf's rank-frequency law.",
          "link": "http://arxiv.org/abs/2105.11519",
          "publishedOn": "2021-07-21T02:01:33.745Z",
          "wordCount": 731,
          "title": "The advent and fall of a vocabulary learning bias from communicative efficiency. (arXiv:2105.11519v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hexu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.",
          "link": "http://arxiv.org/abs/2105.14686",
          "publishedOn": "2021-07-21T02:01:33.737Z",
          "wordCount": 624,
          "title": "Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1\">Megan D. Bardolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>",
          "description": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.",
          "link": "http://arxiv.org/abs/2107.09648",
          "publishedOn": "2021-07-21T02:01:33.727Z",
          "wordCount": 554,
          "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ao_S/0/1/0/all/0/1\">Shuang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_X/0/1/0/all/0/1\">Xeno Acharya</a>",
          "description": "A medical dialogue system is essential for healthcare service as providing\nprimary clinical advice and diagnoses. It has been gradually adopted and\npracticed in medical organizations in the form of a conversational bot, largely\ndue to the advancement of NLP. In recent years, the introduction of\nstate-of-the-art deep learning models and transfer learning techniques like\nUniversal Language Model Fine Tuning (ULMFiT) and Knowledge Distillation (KD)\nlargely contributes to the performance of NLP tasks. However, some deep neural\nnetworks are poorly calibrated and wrongly estimate the uncertainty. Hence the\nmodel is not trustworthy, especially in sensitive medical decision-making\nsystems and safety tasks. In this paper, we investigate the well-calibrated\nmodel for ULMFiT and self-distillation (SD) in a medical dialogue system. The\ncalibrated ULMFiT (CULMFiT) is obtained by incorporating label smoothing (LS),\na commonly used regularization technique to achieve a well-calibrated model.\nMoreover, we apply the technique to recalibrate the confidence score called\ntemperature scaling (TS) with KD to observe its correlation with network\ncalibration. To further understand the relation between SD and calibration, we\nuse both fixed and optimal temperatures to fine-tune the whole model. All\nexperiments are conducted on the consultation backpain dataset collected by\nexperts then further validated using a large publicly medial dialogue corpus.\nWe empirically show that our proposed methodologies outperform conventional\nmethods in terms of accuracy and robustness.",
          "link": "http://arxiv.org/abs/2107.09625",
          "publishedOn": "2021-07-21T02:01:33.710Z",
          "wordCount": 657,
          "title": "Learning ULMFiT and Self-Distillation with Calibration for Medical Dialogue System. (arXiv:2107.09625v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bezati_E/0/1/0/all/0/1\">Endri Bezati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emami_M/0/1/0/all/0/1\">Mahyar Emami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janneck_J/0/1/0/all/0/1\">J&#xf6;rn Janneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larus_J/0/1/0/all/0/1\">James Larus</a>",
          "description": "To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n\nThis work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.",
          "link": "http://arxiv.org/abs/2107.09333",
          "publishedOn": "2021-07-21T02:01:33.687Z",
          "wordCount": 610,
          "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical report). (arXiv:2107.09333v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr",
          "link": "http://arxiv.org/abs/2107.09609",
          "publishedOn": "2021-07-21T02:01:33.677Z",
          "wordCount": 680,
          "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weile Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>",
          "description": "While named entity recognition (NER) is a key task in natural language\nprocessing, most approaches only target flat entities, ignoring nested\nstructures which are common in many scenarios. Most existing nested NER methods\ntraverse all sub-sequences which is both expensive and inefficient, and also\ndon't well consider boundary knowledge which is significant for nested\nentities. In this paper, we propose a joint entity mention detection and typing\nmodel via prior boundary knowledge (BoningKnife) to better handle nested NER\nextraction and recognition tasks. BoningKnife consists of two modules,\nMentionTagger and TypeClassifier. MentionTagger better leverages boundary\nknowledge beyond just entity start/end to improve the handling of nesting\nlevels and longer spans, while generating high quality mention candidates.\nTypeClassifier utilizes a two-level attention mechanism to decouple different\nnested level representations and better distinguish entity types. We jointly\ntrain both modules sharing a common representation and a new dual-info\nattention layer, which leads to improved representation focus on entity-related\ninformation. Experiments over different datasets show that our approach\noutperforms previous state of the art methods and achieves 86.41, 85.46, and\n94.2 F1 scores on ACE2004, ACE2005, and NNE, respectively.",
          "link": "http://arxiv.org/abs/2107.09429",
          "publishedOn": "2021-07-21T02:01:33.660Z",
          "wordCount": 641,
          "title": "BoningKnife: Joint Entity Mention Detection and Typing for Nested NER via prior Boundary Knowledge. (arXiv:2107.09429v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1\">Roberto Gretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1\">Marco Matassoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1\">Daniele Falavigna</a>",
          "description": "We address the problem of language model customization in applications where\nthe ASR component needs to manage domain-specific terminology; although current\nstate-of-the-art speech recognition technology provides excellent results for\ngeneric domains, the adaptation to specialized dictionaries or glossaries is\nstill an open issue. In this work we present an approach for automatically\nselecting sentences, from a text corpus, that match, both semantically and\nmorphologically, a glossary of terms (words or composite words) furnished by\nthe user. The final goal is to rapidly adapt the language model of an hybrid\nASR system with a limited amount of in-domain text data in order to\nsuccessfully cope with the linguistic domain at hand; the vocabulary of the\nbaseline model is expanded and tailored, reducing the resulting OOV rate. Data\nselection strategies based on shallow morphological seeds and semantic\nsimilarity viaword2vec are introduced and discussed; the experimental setting\nconsists in a simultaneous interpreting scenario, where ASRs in three languages\nare designed to recognize the domain-specific terms (i.e. dentistry). Results\nusing different metrics (OOV rate, WER, precision and recall) show the\neffectiveness of the proposed techniques.",
          "link": "http://arxiv.org/abs/2107.09433",
          "publishedOn": "2021-07-21T02:01:33.651Z",
          "wordCount": 615,
          "title": "Seed Words Based Data Selection for Language Model Adaptation. (arXiv:2107.09433v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "In voice conversion (VC), an approach showing promising results in the latest\nvoice conversion challenge (VCC) 2020 is to first use an automatic speech\nrecognition (ASR) model to transcribe the source speech into the underlying\nlinguistic contents; these are then used as input by a text-to-speech (TTS)\nsystem to generate the converted speech. Such a paradigm, referred to as\nASR+TTS, overlooks the modeling of prosody, which plays an important role in\nspeech naturalness and conversion similarity. Although some researchers have\nconsidered transferring prosodic clues from the source speech, there arises a\nspeaker mismatch during training and conversion. To address this issue, in this\nwork, we propose to directly predict prosody from the linguistic representation\nin a target-speaker-dependent manner, referred to as target text prediction\n(TTP). We evaluate both methods on the VCC2020 benchmark and consider different\nlinguistic representations. The results demonstrate the effectiveness of TTP in\nboth objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2107.09477",
          "publishedOn": "2021-07-21T02:01:33.628Z",
          "wordCount": 598,
          "title": "On Prosody Modeling for ASR+TTS based Voice Conversion. (arXiv:2107.09477v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yujia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslan_O/0/1/0/all/0/1\">Ozlem Aslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "We present a new dataset of Wikipedia articles each paired with a knowledge\ngraph, to facilitate the research in conditional text generation, graph\ngeneration and graph representation learning. Existing graph-text paired\ndatasets typically contain small graphs and short text (1 or few sentences),\nthus limiting the capabilities of the models that can be learned on the data.\nOur new dataset WikiGraphs is collected by pairing each Wikipedia article from\nthe established WikiText-103 benchmark (Merity et al., 2016) with a subgraph\nfrom the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy\nto benchmark against other state-of-the-art text generative models that are\ncapable of generating long paragraphs of coherent text. Both the graphs and the\ntext data are of significantly larger scale compared to prior graph-text paired\ndatasets. We present baseline graph neural network and transformer model\nresults on our dataset for 3 tasks: graph -> text generation, graph -> text\nretrieval and text -> graph retrieval. We show that better conditioning on the\ngraph provides gains in generation and retrieval quality but there is still\nlarge room for improvement.",
          "link": "http://arxiv.org/abs/2107.09556",
          "publishedOn": "2021-07-21T02:01:33.596Z",
          "wordCount": 618,
          "title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset. (arXiv:2107.09556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongsik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Harksoo Kim</a>",
          "description": "The sentence-level relation extraction mainly aims to classify the relation\nbetween two entities in a sentence. The sentence-level relation extraction\ncorpus is often containing data of difficulty for the model to infer or noise\ndata. In this paper, we propose a curriculum learning-based relation extraction\nmodel that split data by difficulty and utilize it for learning. In the\nexperiments with the representative sentence-level relation extraction\ndatasets, TACRED and Re-TACRED, the proposed method showed good performances.",
          "link": "http://arxiv.org/abs/2107.09332",
          "publishedOn": "2021-07-21T02:01:33.587Z",
          "wordCount": 503,
          "title": "Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>",
          "description": "We present a simple and effective way to generate a variety of paraphrases\nand find a good quality paraphrase among them. As in previous studies, it is\ndifficult to ensure that one generation method always generates the best\nparaphrase in various domains. Therefore, we focus on finding the best\ncandidate from multiple candidates, rather than assuming that there is only one\ncombination of generative models and decoding options. Our approach shows that\nit is easy to apply in various domains and has sufficiently good performance\ncompared to previous methods. In addition, our approach can be used for data\nagumentation that extends the downstream corpus, showing that it can help\nimprove performance in English and Korean datasets.",
          "link": "http://arxiv.org/abs/2107.09274",
          "publishedOn": "2021-07-21T02:01:33.251Z",
          "wordCount": 538,
          "title": "Paraphrasing via Ranking Many Candidates. (arXiv:2107.09274v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.09293",
          "publishedOn": "2021-07-21T02:01:33.170Z",
          "wordCount": 663,
          "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>",
          "description": "Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.",
          "link": "http://arxiv.org/abs/2107.09278",
          "publishedOn": "2021-07-21T02:01:33.158Z",
          "wordCount": 593,
          "title": "Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09055",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1\">Priyank Sonkiya</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1\">Vikas Bajpai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1\">Anukriti Bansal</a>",
          "description": "The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.",
          "link": "http://arxiv.org/abs/2107.09055",
          "publishedOn": "2021-07-21T02:01:33.143Z",
          "wordCount": 699,
          "title": "Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:33.123Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>",
          "description": "Typically, a linearly orthogonal transformation mapping is learned by\naligning static type-level embeddings to build a shared semantic space. In view\nof the analysis that contextual embeddings contain richer semantic features, we\ninvestigate a context-aware and dictionary-free mapping approach by leveraging\nparallel corpora. We illustrate that our contextual embedding space mapping\nsignificantly outperforms previous multilingual word embedding methods on the\nbilingual dictionary induction (BDI) task by providing a higher degree of\nisomorphism. To improve the quality of mapping, we also explore sense-level\nembeddings that are split from type-level representations, which can align\nspaces in a finer resolution and yield more precise mapping. Moreover, we\nreveal that contextual embedding spaces suffer from their natural properties --\nanisotropy and anisometry. To mitigate these two problems, we introduce the\niterative normalization algorithm as an imperative preprocessing step. Our\nfindings unfold the tight relationship between isotropy, isometry, and\nisomorphism in normalized contextual embedding spaces.",
          "link": "http://arxiv.org/abs/2107.09186",
          "publishedOn": "2021-07-21T02:01:33.080Z",
          "wordCount": 584,
          "title": "Cross-Lingual BERT Contextual Embedding Space Mapping with Isotropic and Isometric Conditions. (arXiv:2107.09186v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:33.069Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1\">Kaylee Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "Although virtual agents are increasingly situated in environments where\nnatural language is the most effective mode of interaction with humans, these\nexchanges are rarely used as an opportunity for learning. Leveraging language\ninteractions effectively requires addressing limitations in the two most common\napproaches to language grounding: semantic parsers built on top of fixed object\ncategories are precise but inflexible and end-to-end models are maximally\nexpressive, but fickle and opaque. Our goal is to develop a system that\nbalances the strengths of each approach so that users can teach agents new\ninstructions that generalize broadly from a single example. We introduce the\nidea of neural abstructions: a set of constraints on the inference procedure of\na label-conditioned generative model that can affect the meaning of the label\nin context. Starting from a core programming language that operates over\nabstructions, users can define increasingly complex mappings from natural\nlanguage to actions. We show that with this method a user population is able to\nbuild a semantic parser for an open-ended house modification task in Minecraft.\nThe semantic parser that results is both flexible and expressive: the\npercentage of utterances sourced from redefinitions increases steadily over the\ncourse of 191 total exchanges, achieving a final value of 28%.",
          "link": "http://arxiv.org/abs/2107.09285",
          "publishedOn": "2021-07-21T02:01:33.033Z",
          "wordCount": 651,
          "title": "Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning. (arXiv:2107.09285v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>",
          "description": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.",
          "link": "http://arxiv.org/abs/2107.09099",
          "publishedOn": "2021-07-21T02:01:33.001Z",
          "wordCount": 554,
          "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-20T02:04:41.716Z",
          "wordCount": 602,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:41.578Z",
          "wordCount": 846,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:41.119Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1\">Nihal Potdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yiran Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>",
          "description": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.",
          "link": "http://arxiv.org/abs/2105.10042",
          "publishedOn": "2021-07-20T02:04:41.090Z",
          "wordCount": 701,
          "title": "A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:41.071Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>",
          "description": "This paper models unsupervised learning of an identity-based pattern (or\ncopying) in speech called reduplication from raw continuous data with deep\nconvolutional neural networks. We use the ciwGAN architecture Begu\\v{s} (2021a;\narXiv:2006.02951) in which learning of meaningful representations in speech\nemerges from a requirement that the CNNs generate informative data. We propose\na technique to wug-test CNNs trained on speech and, based on four generative\ntests, argue that the network learns to represent an identity-based pattern in\nits latent space. By manipulating only two categorical variables in the latent\nspace, we can actively turn an unreduplicated form into a reduplicated form\nwith no other substantial changes to the output in the majority of cases. We\nalso argue that the network extends the identity-based pattern to unobserved\ndata. Exploration of how meaningful representations of identity-based patterns\nemerge in CNNs and how the latent space variables outside of the training range\ncorrelate with identity-based patterns in the output has general implications\nfor neural network interpretability.",
          "link": "http://arxiv.org/abs/2009.06110",
          "publishedOn": "2021-07-20T02:04:41.052Z",
          "wordCount": 628,
          "title": "Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication. (arXiv:2009.06110v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:41.021Z",
          "wordCount": 635,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Quan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Knowledge distillation has been proven to be effective in model acceleration\nand compression. It allows a small network to learn to generalize in the same\nway as a large network. Recent successes in pre-training suggest the\neffectiveness of transferring model parameters. Inspired by this, we\ninvestigate methods of model acceleration and compression in another line of\nresearch. We propose Weight Distillation to transfer the knowledge in the large\nnetwork parameters through a parameter generator. Our experiments on WMT16\nEn-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight\ndistillation can train a small network that is 1.88~2.94x faster than the large\nnetwork but with competitive performance. With the same sized small network,\nweight distillation can outperform knowledge distillation by 0.51~1.82 BLEU\npoints.",
          "link": "http://arxiv.org/abs/2009.09152",
          "publishedOn": "2021-07-20T02:04:40.957Z",
          "wordCount": 606,
          "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters. (arXiv:2009.09152v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.",
          "link": "http://arxiv.org/abs/2101.00542",
          "publishedOn": "2021-07-20T02:04:40.927Z",
          "wordCount": 580,
          "title": "An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_J/0/1/0/all/0/1\">Jieh Hsiang</a>",
          "description": "Generative models, such as GPT-2, have demonstrated impressive results\nrecently. A fundamental question we'd like to address is: where did the\ngenerated text come from? This work is our initial effort toward answering the\nquestion by using prior art search. The purpose of the prior art search is to\nfind the most similar prior text in the training data of GPT-2. We take a\nreranking approach and apply it to the patent domain. Specifically, we\npre-train GPT-2 models from scratch by using the patent data from the USPTO.\nThe input for the prior art search is the patent text generated by the GPT-2\nmodel. We also pre-trained BERT models from scratch for converting patent text\nto embeddings. The steps of reranking are: (1) search the most similar text in\nthe training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2)\nconvert the search results in text format to BERT embeddings, and (3) provide\nthe final result by ranking the BERT embeddings based on their similarities\nwith the patent text generated by GPT-2. The experiments in this work show that\nsuch reranking is better than ranking with embeddings alone. However, our mixed\nresults also indicate that calculating the semantic similarities among long\ntext spans is still challenging. To our knowledge, this work is the first to\nimplement a reranking system to identify retrospectively the most similar\ninputs to a GPT model based on its output.",
          "link": "http://arxiv.org/abs/2009.09132",
          "publishedOn": "2021-07-20T02:04:40.905Z",
          "wordCount": 728,
          "title": "Prior Art Search and Reranking for Generated Patent Text. (arXiv:2009.09132v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00858",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1\">Wenning Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>",
          "description": "In this paper, several works are proposed to address practical challenges for\ndeploying RNN Transducer (RNN-T) based speech recognition system. These\nchallenges are adapting a well-trained RNN-T model to a new domain without\ncollecting the audio data, obtaining time stamps and confidence scores at word\nlevel. The first challenge is solved with a splicing data method which\nconcatenates the speech segments extracted from the source domain data. To get\nthe time stamp, a phone prediction branch is added to the RNN-T model by\nsharing the encoder for the purpose of force alignment. Finally, we obtain\nword-level confidence scores by utilizing several types of features calculated\nduring decoding and from confusion network. Evaluated with Microsoft production\ndata, the splicing data adaptation method improves the baseline and adaptation\nwith the text to speech method by 58.03% and 15.25% relative word error rate\nreduction, respectively. The proposed time stamping method can get less than\n50ms word timing difference from the ground truth alignment on average while\nmaintaining the recognition accuracy of the RNN-T model. We also obtain high\nconfidence annotation performance with limited computation cost.",
          "link": "http://arxiv.org/abs/2105.00858",
          "publishedOn": "2021-07-20T02:04:40.886Z",
          "wordCount": 657,
          "title": "On Addressing Practical Challenges for RNN-Transducer. (arXiv:2105.00858v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1\">Cristiano Chesi</a>",
          "description": "A cognitively plausible parsing algorithm should perform like the human\nparser in critical contexts. Here I propose an adaptation of Earley's parsing\nalgorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that\nis able to predict complexity effects in performance. Focusing on self-paced\nreading experiments of object clefts sentences (Warren & Gibson 2005) I will\nassociate to parsing a complexity metric based on cued features to be retrieved\nat the verb segment (Feature Retrieval & Encoding Cost, FREC). FREC is\ncrucially based on the usage of memory predicted by the discussed parsing\nalgorithm and it correctly fits with the reading time revealed.",
          "link": "http://arxiv.org/abs/1906.00908",
          "publishedOn": "2021-07-20T02:04:40.864Z",
          "wordCount": 580,
          "title": "Phase-based Minimalist Parsing and complexity in non-local dependencies. (arXiv:1906.00908v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>",
          "description": "The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nFurthermore, a generative autoencoder learning algorithm was proposed to\nleverage the sparsity reduction that achieved 91% accuracy, 91% recall, and 91%\nprecision. This study successfully applied learning representation and machine\nlearning algorithms to detect heart failure from clinical natural language in a\nsingle French institution. Further work is needed to use the same methodology\nin other institutions and other languages.",
          "link": "http://arxiv.org/abs/2104.03969",
          "publishedOn": "2021-07-20T02:04:40.806Z",
          "wordCount": 716,
          "title": "Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.",
          "link": "http://arxiv.org/abs/2106.08484",
          "publishedOn": "2021-07-20T02:04:40.786Z",
          "wordCount": 636,
          "title": "Generative Conversational Networks. (arXiv:2106.08484v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Deokgun Park</a>",
          "description": "Despite recent advances in many application-specific domains, we do not know\nhow to build a human-level artificial intelligence (HLAI). We conjecture that\nlearning from others' experience with the language is the essential\ncharacteristic that distinguishes human intelligence from the rest. Humans can\nupdate the action-value function with the verbal description as if they\nexperience states, actions, and corresponding rewards sequences firsthand. In\nthis paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.",
          "link": "http://arxiv.org/abs/2011.09410",
          "publishedOn": "2021-07-20T02:04:40.762Z",
          "wordCount": 575,
          "title": "A Definition and a Test for Human-Level Artificial Intelligence. (arXiv:2011.09410v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "With the increased audiovisualisation of communication, the need for live\nsubtitles in multilingual events is more relevant than ever. In an attempt to\nautomatise the process, we aim at exploring the feasibility of simultaneous\nspeech translation (SimulST) for live subtitling. However, the word-for-word\nrate of generation of SimulST systems is not optimal for displaying the\nsubtitles in a comprehensible and readable way. In this work, we adapt SimulST\nsystems to predict subtitle breaks along with the translation. We then propose\na display mode that exploits the predicted break structure by presenting the\nsubtitles in scrolling lines. We compare our proposed mode with a display 1)\nword-for-word and 2) in blocks, in terms of reading speed and delay.\nExperiments on three language pairs (en$\\rightarrow$it, de, fr) show that\nscrolling lines is the only mode achieving an acceptable reading speed while\nkeeping delay close to a 4-second threshold. We argue that simultaneous\ntranslation for readable live subtitles still faces challenges, the main one\nbeing poor translation quality, and propose directions for steering future\nresearch.",
          "link": "http://arxiv.org/abs/2107.08807",
          "publishedOn": "2021-07-20T02:04:40.743Z",
          "wordCount": 622,
          "title": "Simultaneous Speech Translation for Live Subtitling: from Delay to Display. (arXiv:2107.08807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:40.723Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-07-20T02:04:40.640Z",
          "wordCount": 657,
          "title": "An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.",
          "link": "http://arxiv.org/abs/2106.13155",
          "publishedOn": "2021-07-20T02:04:40.621Z",
          "wordCount": 575,
          "title": "Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>",
          "description": "For most language combinations, parallel data is either scarce or simply\nunavailable. To address this, unsupervised machine translation (UMT) exploits\nlarge amounts of monolingual data by using synthetic data generation techniques\nsuch as back-translation and noising, while self-supervised NMT (SSNMT)\nidentifies parallel sentences in smaller comparable data and trains on them. To\ndate, the inclusion of UMT data generation techniques in SSNMT has not been\ninvestigated. We show that including UMT techniques into SSNMT significantly\noutperforms SSNMT and UMT on all tested language pairs, with improvements of up\nto +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT,\nrespectively, on Afrikaans to English. We further show that the combination of\nmultilingual denoising autoencoding, SSNMT with backtranslation and bilingual\nfinetuning enables us to learn machine translation even for distant language\npairs for which only small amounts of monolingual data are available, e.g.\nyielding BLEU scores of 11.6 (English to Swahili).",
          "link": "http://arxiv.org/abs/2107.08772",
          "publishedOn": "2021-07-20T02:04:40.442Z",
          "wordCount": 604,
          "title": "Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages. (arXiv:2107.08772v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.01670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>",
          "description": "Data-driven analysis and detection of abusive online content covers many\ndifferent tasks, phenomena, contexts, and methodologies. This paper\nsystematically reviews abusive language dataset creation and content in\nconjunction with an open website for cataloguing abusive language data. This\ncollection of knowledge leads to a synthesis providing evidence-based\nrecommendations for practitioners working with this complex and highly diverse\ndata.",
          "link": "http://arxiv.org/abs/2004.01670",
          "publishedOn": "2021-07-20T02:04:40.423Z",
          "wordCount": 533,
          "title": "Directions in Abusive Language Training Data: Garbage In, Garbage Out. (arXiv:2004.01670v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slavnov_S/0/1/0/all/0/1\">Sergey Slavnov</a>",
          "description": "We propose a concrete surface representation of abstract categorial grammars\nin the category of word cobordisms or cowordisms for short, which are certain\nbipartite graphs decorated with words in a given alphabet, generalizing linear\nlogic proof-nets. We also introduce and study linear logic grammars, directly\nbased on cobordisms and using classical multiplicative linear logic as a typing\nsystem.",
          "link": "http://arxiv.org/abs/2107.08728",
          "publishedOn": "2021-07-20T02:04:40.362Z",
          "wordCount": 519,
          "title": "Cobordisms and commutative categorial grammars. (arXiv:2107.08728v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jane L. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">Joshua R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewhurst_D/0/1/0/all/0/1\">David R. Dewhurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">Andrew J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>",
          "description": "In real-time, social media data strongly imprints world events, popular\nculture, and day-to-day conversations by millions of ordinary people at a scale\nthat is scarcely conventionalized and recorded. Vitally, and absent from many\nstandard corpora such as books and news archives, sharing and commenting\nmechanisms are native to social media platforms, enabling us to quantify social\namplification (i.e., popularity) of trending storylines and contemporary\ncultural phenomena. Here, we describe Storywrangler, a natural language\nprocessing instrument designed to carry out an ongoing, day-scale curation of\nover 100 billion tweets containing roughly 1 trillion 1-grams from 2008 to\n2021. For each day, we break tweets into unigrams, bigrams, and trigrams\nspanning over 100 languages. We track n-gram usage frequencies, and generate\nZipf distributions, for words, hashtags, handles, numerals, symbols, and\nemojis. We make the data set available through an interactive time series\nviewer, and as downloadable time series and daily distributions. Although\nStorywrangler leverages Twitter data, our method of extracting and tracking\ndynamic changes of n-grams can be extended to any similar social media\nplatform. We showcase a few examples of the many possible avenues of study we\naim to enable including how social amplification can be visualized through\n'contagiograms'. We also present some example case studies that bridge n-gram\ntime series with disparate data sources to explore sociotechnical dynamics of\nfamous individuals, box office success, and social unrest.",
          "link": "http://arxiv.org/abs/2007.12988",
          "publishedOn": "2021-07-20T02:04:40.344Z",
          "wordCount": 786,
          "title": "Storywrangler: A massive exploratorium for sociolinguistic, cultural, socioeconomic, and political timelines using Twitter. (arXiv:2007.12988v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H.R. Hahnloser</a>",
          "description": "We introduce MemSum (Multi-step Episodic Markov decision process extractive\nSUMmarizer), a reinforcement-learning-based extractive summarizer enriched at\nany given time step with information on the current extraction history. Similar\nto previous models in this vein, MemSum iteratively selects sentences into the\nsummary. Our innovation is in considering a broader information set when\nsummarizing that would intuitively also be used by humans in this task: 1) the\ntext content of the sentence, 2) the global text context of the rest of the\ndocument, and 3) the extraction history consisting of the set of sentences that\nhave already been extracted. With a lightweight architecture, MemSum\nnonetheless obtains state-of-the-art test-set performance (ROUGE score) on long\ndocument datasets (PubMed, arXiv, and GovReport). Supporting analysis\ndemonstrates that the added awareness of extraction history gives MemSum\nrobustness against redundancy in the source document.",
          "link": "http://arxiv.org/abs/2107.08929",
          "publishedOn": "2021-07-20T02:04:40.322Z",
          "wordCount": 575,
          "title": "MemSum: Extractive Summarization of Long Documents using Multi-step Episodic Markov Decision Processes. (arXiv:2107.08929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:40.303Z",
          "wordCount": 590,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1\">Francesco Fabiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenchner_J/0/1/0/all/0/1\">Jonathan Lenchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Francesca Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapini_M/0/1/0/all/0/1\">Marianna Bergamaschi Ganapini</a>",
          "description": "Epistemic Planning (EP) refers to an automated planning setting where the\nagent reasons in the space of knowledge states and tries to find a plan to\nreach a desirable state from the current state. Its general form, the\nMulti-agent Epistemic Planning (MEP) problem involves multiple agents who need\nto reason about both the state of the world and the information flow between\nagents. In a MEP problem, multiple approaches have been developed recently with\nvarying restrictions, such as considering only the concept of knowledge while\nnot allowing the idea of belief, or not allowing for ``complex\" modal operators\nsuch as those needed to handle dynamic common knowledge. While the diversity of\napproaches has led to a deeper understanding of the problem space, the lack of\na standardized way to specify MEP problems independently of solution approaches\nhas created difficulties in comparing performance of planners, identifying\npromising techniques, exploring new strategies like ensemble methods, and\nmaking it easy for new researchers to contribute to this research area. To\naddress the situation, we propose a unified way of specifying EP problems - the\nEpistemic Planning Domain Definition Language, E-PDDL. We show that E-PPDL can\nbe supported by leading MEP planners and provide corresponding parser code that\ntranslates EP problems specified in E-PDDL into (M)EP problems that can be\nhandled by several planners. This work is also useful in building more general\nepistemic planning environments where we envision a meta-cognitive module that\ntakes a planning problem in E-PDDL, identifies and assesses some of its\nfeatures, and autonomously decides which planner is the best one to solve it.",
          "link": "http://arxiv.org/abs/2107.08739",
          "publishedOn": "2021-07-20T02:04:40.282Z",
          "wordCount": 718,
          "title": "E-PDDL: A Standardized Way of Defining Epistemic Planning Problems. (arXiv:2107.08739v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>",
          "description": "A proactive dialogue system has the ability to proactively lead the\nconversation. Different from the general chatbots which only react to the user,\nproactive dialogue systems can be used to achieve some goals, e.g., to\nrecommend some items to the user. Background knowledge is essential to enable\nsmooth and natural transitions in dialogue. In this paper, we propose a new\nmulti-task learning framework for retrieval-based knowledge-grounded proactive\ndialogue. To determine the relevant knowledge to be used, we frame knowledge\nprediction as a complementary task and use explicit signals to supervise its\nlearning. The final response is selected according to the predicted knowledge,\nthe goal to achieve, and the context. Experimental results show that explicit\nmodeling of knowledge prediction and goal selection can greatly improve the\nfinal response selection. Our code is available at\nhttps://github.com/DaoD/KPN/.",
          "link": "http://arxiv.org/abs/2107.08329",
          "publishedOn": "2021-07-20T02:04:40.260Z",
          "wordCount": 581,
          "title": "Proactive Retrieval-based Chatbots based on Relevant Knowledge and Goals. (arXiv:2107.08329v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hegel_A/0/1/0/all/0/1\">Allison Hegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Marina Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peaslee_G/0/1/0/all/0/1\">Genevieve Peaslee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roof_B/0/1/0/all/0/1\">Brendan Roof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elwany_E/0/1/0/all/0/1\">Emad Elwany</a>",
          "description": "Large, pre-trained transformer models like BERT have achieved\nstate-of-the-art results on document understanding tasks, but most\nimplementations can only consider 512 tokens at a time. For many real-world\napplications, documents can be much longer, and the segmentation strategies\ntypically used on longer documents miss out on document structure and\ncontextual information, hurting their results on downstream tasks. In our work\non legal agreements, we find that visual cues such as layout, style, and\nplacement of text in a document are strong features that are crucial to\nachieving an acceptable level of accuracy on long documents. We measure the\nimpact of incorporating such visual cues, obtained via computer vision methods,\non the accuracy of document understanding tasks including document\nsegmentation, entity extraction, and attribute classification. Our method of\nsegmenting documents based on structural metadata out-performs existing methods\non four long-document understanding tasks as measured on the Contract\nUnderstanding Atticus Dataset.",
          "link": "http://arxiv.org/abs/2107.08128",
          "publishedOn": "2021-07-20T02:04:40.191Z",
          "wordCount": 602,
          "title": "The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues. (arXiv:2107.08128v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Rudolf A. Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madikeri_S/0/1/0/all/0/1\">Srikanth Madikeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>",
          "description": "A common problem for automatic speech recognition systems is how to recognize\nwords that they did not see during training. Currently there is no established\nmethod of evaluating different techniques for tackling this problem. We propose\nusing the CommonVoice dataset to create test sets for multiple languages which\nhave a high out-of-vocabulary (OOV) ratio relative to a training set and\nrelease a new tool for calculating relevant performance metrics. We then\nevaluate, within the context of a hybrid ASR system, how much better subword\nmodels are at recognizing OOVs, and how much benefit one can get from\nincorporating OOV-word information into an existing system by modifying WFSTs.\nAdditionally, we propose a new method for modifying a subword-based language\nmodel so as to better recognize OOV-words. We showcase very large improvements\nin OOV-word recognition and make both the data and code available.",
          "link": "http://arxiv.org/abs/2107.08091",
          "publishedOn": "2021-07-20T02:04:40.151Z",
          "wordCount": 592,
          "title": "A Comparison of Methods for OOV-word Recognition on a New Public Dataset. (arXiv:2107.08091v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>",
          "description": "Classifying and resolving coreferences of objects (e.g., product names) and\nattributes (e.g., product aspects) in opinionated reviews is crucial for\nimproving the opinion mining performance. However, the task is challenging as\none often needs to consider domain-specific knowledge (e.g., iPad is a tablet\nand has aspect resolution) to identify coreferences in opinionated reviews.\nAlso, compiling a handcrafted and curated domain-specific knowledge base for\neach domain is very time consuming and arduous. This paper proposes an approach\nto automatically mine and leverage domain-specific knowledge for classifying\nobjects and attribute coreferences. The approach extracts domain-specific\nknowledge from unlabeled review data and trains a knowledgeaware neural\ncoreference classification model to leverage (useful) domain knowledge together\nwith general commonsense knowledge for the task. Experimental evaluation on\nrealworld datasets involving five domains (product types) shows the\neffectiveness of the approach.",
          "link": "http://arxiv.org/abs/2010.05357",
          "publishedOn": "2021-07-20T02:04:40.129Z",
          "wordCount": 615,
          "title": "A Knowledge-Driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining. (arXiv:2010.05357v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanton_M/0/1/0/all/0/1\">Margherita Fanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonaldi_H/0/1/0/all/0/1\">Helena Bonaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Undermining the impact of hateful content with informed and non-aggressive\nresponses, called counter narratives, has emerged as a possible solution for\nhaving healthier online communities. Thus, some NLP studies have started\naddressing the task of counter narrative generation. Although such studies have\nmade an effort to build hate speech / counter narrative (HS/CN) datasets for\nneural generation, they fall short in reaching either high-quality and/or\nhigh-quantity. In this paper, we propose a novel human-in-the-loop data\ncollection methodology in which a generative language model is refined\niteratively by using its own data from the previous loops to generate new\ntraining samples that experts review and/or post-edit. Our experiments\ncomprised several loops including dynamic variations. Results show that the\nmethodology is scalable and facilitates diverse, novel, and cost-effective data\ncollection. To our knowledge, the resulting dataset is the only expert-based\nmulti-target HS/CN dataset available to the community.",
          "link": "http://arxiv.org/abs/2107.08720",
          "publishedOn": "2021-07-20T02:04:40.103Z",
          "wordCount": 603,
          "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech. (arXiv:2107.08720v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:40.071Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nyoungwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Ho-Jin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyun Myaeng</a>",
          "description": "In multi-modal dialogue systems, it is important to allow the use of images\nas part of a multi-turn conversation. Training such dialogue systems generally\nrequires a large-scale dataset consisting of multi-turn dialogues that involve\nimages, but such datasets rarely exist. In response, this paper proposes a 45k\nmulti-modal dialogue dataset created with minimal human intervention. Our\nmethod to create such a dataset consists of (1) preparing and pre-processing\ntext dialogue datasets, (2) creating image-mixed dialogues by using a\ntext-to-image replacement technique, and (3) employing a\ncontextual-similarity-based filtering step to ensure the contextual coherence\nof the dataset. To evaluate the validity of our dataset, we devise a simple\nretrieval model for dialogue sentence prediction tasks. Automatic metrics and\nhuman evaluation results on such tasks show that our dataset can be effectively\nused as training data for multi-modal dialogue systems which require an\nunderstanding of images and text in a context-aware manner. Our dataset and\ngeneration code is available at\nhttps://github.com/shh1574/multi-modal-dialogue-dataset.",
          "link": "http://arxiv.org/abs/2107.08685",
          "publishedOn": "2021-07-20T02:04:40.007Z",
          "wordCount": 606,
          "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images. (arXiv:2107.08685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>",
          "description": "Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires understanding and interaction\nusing natural language in a partially observable environment. In this paper, we\nimprove the semantic understanding of the agent by proposing a simple RL with\nLM framework where we use transformer-based language models with Deep RL\nmodels. We perform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1, to achieve a\nscore of 44.7, which is 1.6 higher than the state-of-the-art model. Our\nproposed approach also performs comparably to the state-of-the-art models on\nthe other set of text games.",
          "link": "http://arxiv.org/abs/2107.08408",
          "publishedOn": "2021-07-20T02:04:39.987Z",
          "wordCount": 581,
          "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. (arXiv:2107.08408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>",
          "description": "Semantic role labeling (SRL) -- identifying the semantic relationships\nbetween a predicate and other constituents in the same sentence -- is a\nwell-studied task in natural language understanding (NLU). However, many of\nthese relationships are evident only at the level of the document, as a role\nfor a predicate in one sentence may often be filled by an argument in a\ndifferent one. This more general task, known as implicit semantic role labeling\nor argument linking, has received increased attention in recent years, as\nresearchers have recognized its centrality to information extraction and NLU.\nThis paper surveys the literature on argument linking and identifies several\nnotable shortcomings of existing approaches that indicate the paths along which\nfuture research effort could most profitably be spent.",
          "link": "http://arxiv.org/abs/2107.08523",
          "publishedOn": "2021-07-20T02:04:39.968Z",
          "wordCount": 549,
          "title": "Argument Linking: A Survey and Forecast. (arXiv:2107.08523v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1\">Binzong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiancheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "This ability to learn consecutive tasks without forgetting how to perform\npreviously trained problems is essential for developing an online dialogue\nsystem. This paper proposes an effective continual learning for the\ntask-oriented dialogue system with iterative network pruning, expanding and\nmasking (TPEM), which preserves performance on previously encountered tasks\nwhile accelerating learning progress on subsequent tasks. Specifically, TPEM\n(i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts\nnetwork expanding to create free weights for new tasks, and (iii) introduces\ntask-specific network masking to alleviate the negative impact of fixed weights\nof old tasks on new tasks. We conduct extensive experiments on seven different\ntasks from three benchmark datasets and show empirically that TPEM leads to\nsignificantly improved results over the strong competitors. For\nreproducibility, we submit the code and data at:\nhttps://github.com/siat-nlp/TPEM",
          "link": "http://arxiv.org/abs/2107.08173",
          "publishedOn": "2021-07-20T02:04:39.940Z",
          "wordCount": 598,
          "title": "Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. (arXiv:2107.08173v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_N/0/1/0/all/0/1\">Ning Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Ben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>",
          "description": "Despite recent success in machine reading comprehension (MRC), learning\nhigh-quality MRC models still requires large-scale labeled training data, even\nusing strong pre-trained language models (PLMs). The pre-training tasks for\nPLMs are not question-answering or MRC-based tasks, making existing PLMs unable\nto be directly used for unsupervised MRC. Specifically, MRC aims to spot an\naccurate answer span from the given document, but PLMs focus on token filling\nin sentences. In this paper, we propose a new framework for unsupervised MRC.\nFirstly, we propose to learn to spot answer spans in documents via\nself-supervised learning, by designing a self-supervision pretext task for MRC\n- Spotting-MLM. Solving this task requires capturing deep interactions between\nsentences in documents. Secondly, we apply a simple sentence rewriting strategy\nin the inference stage to alleviate the expression mismatch between questions\nand documents. Experiments show that our method achieves a new state-of-the-art\nperformance for unsupervised MRC.",
          "link": "http://arxiv.org/abs/2107.08582",
          "publishedOn": "2021-07-20T02:04:39.921Z",
          "wordCount": 593,
          "title": "Bridging the Gap between Language Model and Reading Comprehension: Unsupervised MRC via Self-Supervision. (arXiv:2107.08582v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:39.649Z",
          "wordCount": 546,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:39.589Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arruda_H/0/1/0/all/0/1\">Henrique F. de Arruda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reia_S/0/1/0/all/0/1\">Sandro M. Reia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1\">Filipi N. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1\">Diego R. Amancio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1\">Luciano da F. Costa</a>",
          "description": "Poetry and prose are written artistic expressions that help us to appreciate\nthe reality we live. Each of these styles has its own set of subjective\nproperties, such as rhyme and rhythm, which are easily caught by a human\nreader's eye and ear. With the recent advances in artificial intelligence, the\ngap between humans and machines may have decreased, and today we observe\nalgorithms mastering tasks that were once exclusively performed by humans. In\nthis paper, we propose an automated method to distinguish between poetry and\nprose based solely on aural and rhythmic properties. In other to compare prose\nand poetry rhythms, we represent the rhymes and phones as temporal sequences\nand thus we propose a procedure for extracting rhythmic features from these\nsequences. The classification of the considered texts using the set of features\nextracted resulted in a best accuracy of 0.78, obtained with a neural network.\nInterestingly, by using an approach based on complex networks to visualize the\nsimilarities between the different texts considered, we found that the patterns\nof poetry vary much more than prose. Consequently, a much richer and complex\nset of rhythmic possibilities tends to be found in that modality.",
          "link": "http://arxiv.org/abs/2107.08512",
          "publishedOn": "2021-07-20T02:04:39.557Z",
          "wordCount": 646,
          "title": "A pattern recognition approach for distinguishing between prose and poetry. (arXiv:2107.08512v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>",
          "description": "Mistranslated numbers have the potential to cause serious effects, such as\nfinancial loss or medical misinformation. In this work we develop comprehensive\nassessments of the robustness of neural machine translation systems to\nnumerical text via behavioural testing. We explore a variety of numerical\ntranslation capabilities a system is expected to exhibit and design effective\ntest examples to expose system underperformance. We find that numerical\nmistranslation is a general issue: major commercial systems and\nstate-of-the-art research models fail on many of our test examples, for high-\nand low-resource languages. Our tests reveal novel errors that have not\npreviously been reported in NMT systems, to the best of our knowledge. Lastly,\nwe discuss strategies to mitigate numerical mistranslation.",
          "link": "http://arxiv.org/abs/2107.08357",
          "publishedOn": "2021-07-20T02:04:39.369Z",
          "wordCount": 577,
          "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation. (arXiv:2107.08357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:39.289Z",
          "wordCount": 651,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chingacham_A/0/1/0/all/0/1\">Anupama Chingacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Listening in noisy environments can be difficult even for individuals with a\nnormal hearing thresholds. The speech signal can be masked by noise, which may\nlead to word misperceptions on the side of the listener, and overall difficulty\nto understand the message. To mitigate hearing difficulties on listeners, a\nco-operative speaker utilizes voice modulation strategies like Lombard speech\nto generate noise-robust utterances, and similar solutions have been developed\nfor speech synthesis systems. In this work, we propose an alternate solution of\nchoosing noise-robust lexical paraphrases to represent an intended meaning. Our\nresults show that lexical paraphrases differ in their intelligibility in noise.\nWe evaluate the intelligibility of synonyms in context and find that choosing a\nlexical unit that is less risky to be misheard than its synonym introduced an\naverage gain in comprehension of 37% at SNR -5 dB and 21% at SNR 0 dB for\nbabble noise.",
          "link": "http://arxiv.org/abs/2107.08337",
          "publishedOn": "2021-07-20T02:04:39.266Z",
          "wordCount": 598,
          "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors. (arXiv:2107.08337v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:39.238Z",
          "wordCount": 626,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:39.186Z",
          "wordCount": 611,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1\">Xin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Deval Mehta</a>",
          "description": "Transcending the binary categorization of racist and xenophobic texts, this\nresearch takes cues from social science theories to develop a four dimensional\ncategory for racism and xenophobia detection, namely stigmatization,\noffensiveness, blame, and exclusion. With the aid of deep learning techniques,\nthis categorical detection enables insights into the nuances of emergent topics\nreflected in racist and xenophobic expression on Twitter. Moreover, a stage\nwise analysis is applied to capture the dynamic changes of the topics across\nthe stages of early development of Covid-19 from a domestic epidemic to an\ninternational public health emergency, and later to a global pandemic. The main\ncontributions of this research include, first the methodological advancement.\nBy bridging the state-of-the-art computational methods with social science\nperspective, this research provides a meaningful approach for future research\nto gain insight into the underlying subtlety of racist and xenophobic\ndiscussion on digital platforms. Second, by enabling a more accurate\ncomprehension and even prediction of public opinions and actions, this research\npaves the way for the enactment of effective intervention policies to combat\nracist crimes and social exclusion under Covid-19.",
          "link": "http://arxiv.org/abs/2107.08347",
          "publishedOn": "2021-07-20T02:04:39.148Z",
          "wordCount": 688,
          "title": "Beyond a binary of (non)racist tweets: A four-dimensional categorical detection and analysis of racist and xenophobic opinions on Twitter in early Covid-19. (arXiv:2107.08347v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "We present an overview of the SciVer shared task, presented at the 2nd\nScholarly Document Processing (SDP) workshop at NAACL 2021. In this shared\ntask, systems were provided a scientific claim and a corpus of research\nabstracts, and asked to identify which articles SUPPORT or REFUTE the claim as\nwell as provide evidentiary sentences justifying those labels. 11 teams made a\ntotal of 14 submissions to the shared task leaderboard, leading to an\nimprovement of more than +23 F1 on the primary task evaluation metric. In\naddition to surveying the participating systems, we provide several insights\ninto modeling approaches to support continued progress and future research on\nthe important and challenging task of scientific claim verification.",
          "link": "http://arxiv.org/abs/2107.08188",
          "publishedOn": "2021-07-20T02:04:39.083Z",
          "wordCount": 567,
          "title": "Overview and Insights from the SciVer Shared Task on Scientific Claim Verification. (arXiv:2107.08188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>",
          "description": "Tamarian, a fictional language introduced in the Star Trek episode Darmok,\ncommunicates meaning through utterances of metaphorical references, such as\n\"Darmok and Jalad at Tanagra\" instead of \"We should work together.\" This work\nassembles a Tamarian-English dictionary of utterances from the original episode\nand several follow-on novels, and uses this to construct a parallel corpus of\n456 English-Tamarian utterances. A machine translation system based on a large\nlanguage model (T5) is trained using this parallel corpus, and is shown to\nproduce an accuracy of 76% when translating from English to Tamarian on known\nutterances.",
          "link": "http://arxiv.org/abs/2107.08146",
          "publishedOn": "2021-07-20T02:04:39.061Z",
          "wordCount": 527,
          "title": "Darmok and Jalad at Tanagra: A Dataset and Model for English-to-Tamarian Translation. (arXiv:2107.08146v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1\">Oskar Wysocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florea_M/0/1/0/all/0/1\">Malina Florea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">Donal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>",
          "description": "This paper proposes a novel statistical corpus analysis framework targeted\ntowards the interpretation of Natural Language Processing (NLP) architectural\npatterns at scale. The proposed approach combines saturation-based lexicon\nconstruction, statistical corpus analysis methods and graph collocations to\ninduce a synthesis representation of NLP architectural patterns from corpora.\nThe framework is validated in the full corpus of Semeval tasks and demonstrated\ncoherent architectural patterns which can be used to answer architectural\nquestions on a data-driven fashion, providing a systematic mechanism to\ninterpret a largely dynamic and exponentially growing field.",
          "link": "http://arxiv.org/abs/2107.08124",
          "publishedOn": "2021-07-20T02:04:39.034Z",
          "wordCount": 546,
          "title": "Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems. (arXiv:2107.08124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Andrew J. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Christopher DuBois</a>",
          "description": "Existing methods for open-retrieval question answering in lower resource\nlanguages (LRLs) lag significantly behind English. They not only suffer from\nthe shortcomings of non-English document retrieval, but are reliant on\nlanguage-specific supervision for either the task or translation. We formulate\na task setup more realistic to available resources, that circumvents document\nretrieval to reliably transfer knowledge from English to lower resource\nlanguages. Assuming a strong English question answering model or database, we\ncompare and analyze methods that pivot through English: to map foreign queries\nto English and then English answers back to target language answers. Within\nthis task setup we propose Reranked Multilingual Maximal Inner Product Search\n(RM-MIPS), akin to semantic similarity retrieval over the English training set\nwith reranking, which outperforms the strongest baselines by 2.7% on XQuAD and\n6.2% on MKQA. Analysis demonstrates the particular efficacy of this strategy\nover state-of-the-art alternatives in challenging settings: low-resource\nlanguages, with extensive distractor data and query distribution misalignment.\nCircumventing retrieval, our analysis shows this approach offers rapid answer\ngeneration to almost any language off-the-shelf, without the need for any\nadditional training data in the target language.",
          "link": "http://arxiv.org/abs/2012.14094",
          "publishedOn": "2021-07-19T00:49:05.908Z",
          "wordCount": 651,
          "title": "Pivot Through English: Reliably Answering Multilingual Questions without Document Retrieval. (arXiv:2012.14094v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yiran Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakemeyer_G/0/1/0/all/0/1\">Gerhard Lakemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.",
          "link": "http://arxiv.org/abs/2101.00419",
          "publishedOn": "2021-07-19T00:49:05.844Z",
          "wordCount": 612,
          "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation. (arXiv:2101.00419v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:05.837Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
          "link": "http://arxiv.org/abs/2106.11410",
          "publishedOn": "2021-07-19T00:49:05.830Z",
          "wordCount": 620,
          "title": "A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Learning effective language representations from crowdsourced labels is\ncrucial for many real-world machine learning tasks. A challenging aspect of\nthis problem is that the quality of crowdsourced labels suffer high intra- and\ninter-observer variability. Since the high-capacity deep neural networks can\neasily memorize all disagreements among crowdsourced labels, directly applying\nexisting supervised language representation learning algorithms may yield\nsuboptimal solutions. In this paper, we propose \\emph{TACMA}, a\n\\underline{t}emporal-\\underline{a}ware language representation learning\nheuristic for \\underline{c}rowdsourced labels with \\underline{m}ultiple\n\\underline{a}nnotators. The proposed approach (1) explicitly models the\nintra-observer variability with attention mechanism; (2) computes and\naggregates per-sample confidence scores from multiple workers to address the\ninter-observer disagreements. The proposed heuristic is extremely easy to\nimplement in around 5 lines of code. The proposed heuristic is evaluated on\nfour synthetic and four real-world data sets. The results show that our\napproach outperforms a wide range of state-of-the-art baselines in terms of\nprediction accuracy and AUC. To encourage the reproducible results, we make our\ncode publicly available at \\url{https://github.com/CrowdsourcingMining/TACMA}.",
          "link": "http://arxiv.org/abs/2107.07958",
          "publishedOn": "2021-07-19T00:49:05.801Z",
          "wordCount": 621,
          "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels. (arXiv:2107.07958v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_M/0/1/0/all/0/1\">Martha Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drenth_E/0/1/0/all/0/1\">Eduard Drenth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijffels_J/0/1/0/all/0/1\">Jan Wijffels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_H/0/1/0/all/0/1\">Hans Van de Velde</a>",
          "description": "We present a lemmatizer/POS-tagger/dependency parser for West Frisian using a\ncorpus of 44,714 words in 3,126 sentences that were annotated according to the\nguidelines of Universal Dependency version 2. POS tags were assigned to words\nby using a Dutch POS tagger that was applied to a literal word-by-word\ntranslation, or to sentences of a Dutch parallel text. Best results were\nobtained when using literal translations that were created by using the Frisian\ntranslation program Oersetter. Morphologic and syntactic annotations were\ngenerated on the basis of a literal Dutch translation as well. The performance\nof the lemmatizer/tagger/annotator when it was trained using default parameters\nwas compared to the performance that was obtained when using the parameter\nvalues that were used for training the LassySmall UD 2.5 corpus. A significant\nimprovement was found for `lemma'. The Frisian lemmatizer/PoS tagger/dependency\nparser is released as a web app and as a web service.",
          "link": "http://arxiv.org/abs/2107.07974",
          "publishedOn": "2021-07-19T00:49:05.794Z",
          "wordCount": 606,
          "title": "POS tagging, lemmatization and dependency parsing of West Frisian. (arXiv:2107.07974v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>",
          "description": "End-to-End intelligent neural dialogue systems suffer from the problems of\ngenerating inconsistent and repetitive responses. Existing dialogue models pay\nattention to unilaterally incorporating personal knowledge into the dialog\nwhile ignoring the fact that incorporating the personality-related conversation\ninformation into personal knowledge taken as the bilateral information flow\nboosts the quality of the subsequent conversation. Besides, it is indispensable\nto control personal knowledge utilization over the conversation level. In this\npaper, we propose a conversation-adaption multi-view persona aware response\ngeneration model that aims at enhancing conversation consistency and\nalleviating the repetition from two folds. First, we consider conversation\nconsistency from multiple views. From the view of the persona profile, we\ndesign a novel interaction module that not only iteratively incorporates\npersonalized knowledge into each turn conversation but also captures the\npersonality-related information from conversation to enhance personalized\nknowledge semantic representation. From the view of speaking style, we\nintroduce the speaking style vector and feed it into the decoder to keep the\nspeaking style consistency. To avoid conversation repetition, we devise a\ncoverage mechanism to keep track of the activation of personal knowledge\nutilization. Experiments on both automatic and human evaluation verify the\nsuperiority of our model over previous models.",
          "link": "http://arxiv.org/abs/2107.07771",
          "publishedOn": "2021-07-19T00:49:05.774Z",
          "wordCount": 639,
          "title": "Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for Open-domain Dialogue Generation. (arXiv:2107.07771v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koenders_C/0/1/0/all/0/1\">Camille Koenders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filla_J/0/1/0/all/0/1\">Johannes Filla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nicolai Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woloszyn_V/0/1/0/all/0/1\">Vinicius Woloszyn</a>",
          "description": "As the spread of false information on the internet has increased dramatically\nin recent years, more and more attention is being paid to automated fake news\ndetection. Some fake news detection methods are already quite successful.\nNevertheless, there are still many vulnerabilities in the detection algorithms.\nThe reason for this is that fake news publishers can structure and formulate\ntheir texts in such a way that a detection algorithm does not expose this text\nas fake news. This paper shows that it is possible to automatically attack\nstate-of-the-art models that have been trained to detect Fake News, making\nthese vulnerable. For this purpose, corresponding models were first trained\nbased on a dataset. Then, using Text-Attack, an attempt was made to manipulate\nthe trained models in such a way that previously correctly identified fake news\nwas classified as true news. The results show that it is possible to\nautomatically bypass Fake News detection mechanisms, leading to implications\nconcerning existing policy initiatives.",
          "link": "http://arxiv.org/abs/2107.07970",
          "publishedOn": "2021-07-19T00:49:05.715Z",
          "wordCount": 604,
          "title": "How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?. (arXiv:2107.07970v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengju Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yonghui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Muhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>",
          "description": "Recent studies on Knowledge Base Question Answering (KBQA) have shown great\nprogress on this task via better question understanding. Previous works for\nencoding questions mainly focus on the word sequences, but seldom consider the\ninformation from syntactic trees.In this paper, we propose an approach to learn\nsyntax-based representations for KBQA. First, we encode path-based syntax by\nconsidering the shortest dependency paths between keywords. Then, we propose\ntwo encoding strategies to mode the information of whole syntactic trees to\nobtain tree-based syntax. Finally, we combine both path-based and tree-based\nsyntax representations for KBQA. We conduct extensive experiments on a widely\nused benchmark dataset and the experimental results show that our syntax-aware\nsystems can make full use of syntax information in different settings and\nachieve state-of-the-art performance of KBQA.",
          "link": "http://arxiv.org/abs/2107.07940",
          "publishedOn": "2021-07-19T00:49:05.695Z",
          "wordCount": 565,
          "title": "Exploiting Rich Syntax for Better Knowledge Base Question Answering. (arXiv:2107.07940v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:05.680Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.656Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonet_O/0/1/0/all/0/1\">Ona de Gibert Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melero_M/0/1/0/all/0/1\">Maite Melero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "Multilingual language models have been a crucial breakthrough as they\nconsiderably reduce the need of data for under-resourced languages.\nNevertheless, the superiority of language-specific models has already been\nproven for languages having access to large amounts of data. In this work, we\nfocus on Catalan with the aim to explore to what extent a medium-sized\nmonolingual language model is competitive with state-of-the-art large\nmultilingual models. For this, we: (1) build a clean, high-quality textual\nCatalan corpus (CaText), the largest to date (but only a fraction of the usual\nsize of the previous work in monolingual language models), (2) train a\nTransformer-based language model for Catalan (BERTa), and (3) devise a thorough\nevaluation in a diversity of settings, comprising a complete array of\ndownstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and\nClassification, Text Classification, Question Answering, and Semantic Textual\nSimilarity, with most of the corresponding datasets being created ex novo. The\nresult is a new benchmark, the Catalan Language Understanding Benchmark (CLUB),\nwhich we publish as an open resource, together with the clean textual corpus,\nthe language model, and the cleaning pipeline. Using state-of-the-art\nmultilingual models and a monolingual model trained only on Wikipedia as\nbaselines, we consistently observe the superiority of our model across tasks\nand settings.",
          "link": "http://arxiv.org/abs/2107.07903",
          "publishedOn": "2021-07-19T00:49:05.605Z",
          "wordCount": 674,
          "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan. (arXiv:2107.07903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gelderloos_L/0/1/0/all/0/1\">Lieke Gelderloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>",
          "description": "Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech.",
          "link": "http://arxiv.org/abs/2005.02721",
          "publishedOn": "2021-07-19T00:49:05.581Z",
          "wordCount": 671,
          "title": "Learning to Understand Child-directed and Adult-directed Speech. (arXiv:2005.02721v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "The quality of vocal delivery is one of the key indicators for evaluating\nteacher enthusiasm, which has been widely accepted to be connected to the\noverall course qualities. However, existing evaluation for vocal delivery is\nmainly conducted with manual ratings, which faces two core challenges:\nsubjectivity and time-consuming. In this paper, we present a novel machine\nlearning approach that utilizes pairwise comparisons and a multimodal\northogonal fusing algorithm to generate large-scale objective evaluation\nresults of the teacher vocal delivery in terms of fluency and passion. We\ncollect two datasets from real-world education scenarios and the experiment\nresults demonstrate the effectiveness of our algorithm. To encourage\nreproducible results, we make our code public available at\n\\url{https://github.com/tal-ai/ML4VocalDelivery.git}.",
          "link": "http://arxiv.org/abs/2107.07956",
          "publishedOn": "2021-07-19T00:49:05.566Z",
          "wordCount": 577,
          "title": "A Multimodal Machine Learning Framework for Teacher Vocal Delivery Evaluation. (arXiv:2107.07956v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shiting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Peilei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Task requirements (TRs) writing is an important question type in Key English\nTest and Preliminary English Test. A TR writing question may include multiple\nrequirements and a high-quality essay must respond to each requirement\nthoroughly and accurately. However, the limited teacher resources prevent\nstudents from getting detailed grading instantly. The majority of existing\nautomatic essay scoring systems focus on giving a holistic score but rarely\nprovide reasons to support it. In this paper, we proposed an end-to-end\nframework based on machine reading comprehension (MRC) to address this problem\nto some extent. The framework not only detects whether an essay responds to a\nrequirement question, but clearly marks where the essay answers the question.\nOur framework consists of three modules: question normalization module, ELECTRA\nbased MRC module and response locating module. We extensively explore\nstate-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and\n0.85 F1 score on a real-world educational dataset. To encourage reproducible\nresults, we make our code publicly available at\n\\url{https://github.com/aied2021TRMRC/AIED_2021_TRMRC_code}.",
          "link": "http://arxiv.org/abs/2107.07957",
          "publishedOn": "2021-07-19T00:49:05.545Z",
          "wordCount": 619,
          "title": "Automatic Task Requirements Writing Evaluation via Machine Reading Comprehension. (arXiv:2107.07957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:05.525Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:05.500Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:05.441Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other's interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.",
          "link": "http://arxiv.org/abs/2107.07567",
          "publishedOn": "2021-07-19T00:49:05.430Z",
          "wordCount": 556,
          "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation. (arXiv:2107.07567v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>",
          "description": "This paper improves the robustness of the pretrained language model BERT\nagainst word substitution-based adversarial attacks by leveraging\nself-supervised contrastive learning with adversarial perturbations. One\nadvantage of our method compared to previous works is that it is capable of\nimproving model robustness without using any labels. Additionally, we also\ncreate an adversarial attack for word-level adversarial training on BERT. The\nattack is efficient, allowing adversarial training for BERT on adversarial\nexamples generated on the fly during training. Experimental results on four\ndatasets show that our method improves the robustness of BERT against four\ndifferent word substitution-based adversarial attacks. Furthermore, to\nunderstand why our method can improve the model robustness against adversarial\nattacks, we study vector representations of clean examples and their\ncorresponding adversarial examples before and after applying our method. As our\nmethod improves model robustness with unlabeled raw data, it opens up the\npossibility of using large text datasets to train robust language models.",
          "link": "http://arxiv.org/abs/2107.07610",
          "publishedOn": "2021-07-19T00:49:05.402Z",
          "wordCount": 596,
          "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-guang Lou</a>",
          "description": "Recent years pre-trained language models hit a success on modeling natural\nlanguage sentences and (semi-)structured tables. However, existing table\npre-training techniques always suffer from low data quality and low\npre-training efficiency. In this paper, we show that table pre-training can be\nrealized by learning a neural SQL executor over a synthetic corpus, which is\nobtained by automatically synthesizing executable SQL queries. By pre-training\non the synthetic corpus, our approach TAPEX dramatically improves the\nperformance on downstream tasks, boosting existing language models by at most\n19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields\nstrong results when using a small pre-trained corpus. Experimental results\ndemonstrate that TAPEX outperforms previous table pre-training approaches by a\nlarge margin, and our model achieves new state-of-the-art results on four\nwell-known datasets, including improving the WikiSQL denotation accuracy to\n89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the\nSQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%\n(+3.6%). Our work opens the way to reason over structured data by pre-training\non synthetic executable programs.",
          "link": "http://arxiv.org/abs/2107.07653",
          "publishedOn": "2021-07-19T00:49:05.328Z",
          "wordCount": 624,
          "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).",
          "link": "http://arxiv.org/abs/2107.07566",
          "publishedOn": "2021-07-19T00:49:05.279Z",
          "wordCount": 594,
          "title": "Internet-Augmented Dialogue Generation. (arXiv:2107.07566v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07509",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings without\nvoice activity detection (VAD), based on monotonic chunkwise attention (MoChA)\nwith an auxiliary connectionist temporal classification (CTC) objective. We\npropose a block-synchronous beam search decoding to take advantage of efficient\nbatched output-synchronous and low-latency input-synchronous searches. We also\npropose a VAD-free inference algorithm that leverages CTC probabilities to\ndetermine a suitable timing to reset the model states to tackle the\nvulnerability to long-form data. Experimental evaluations demonstrate that the\nblock-synchronous decoding achieves comparable accuracy to the\nlabel-synchronous one. Moreover, the VAD-free inference can recognize long-form\nspeech robustly for up to a few hours.",
          "link": "http://arxiv.org/abs/2107.07509",
          "publishedOn": "2021-07-16T00:48:23.310Z",
          "wordCount": 560,
          "title": "VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording. (arXiv:2107.07509v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.",
          "link": "http://arxiv.org/abs/2107.00635",
          "publishedOn": "2021-07-16T00:48:23.271Z",
          "wordCount": 662,
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:23.256Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.230Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.",
          "link": "http://arxiv.org/abs/2107.00368",
          "publishedOn": "2021-07-16T00:48:23.222Z",
          "wordCount": 625,
          "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaojing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>",
          "description": "Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods. This work first\nintroduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first\ncomprehensive small sample evaluation benchmark in Chinese. It includes nine\ntasks, ranging from single-sentence and sentence-pair classification tasks to\nmachine reading comprehension tasks. Given the high variance of the few-shot\nlearning performance, we provide multiple training/validation sets to\nfacilitate a more accurate and stable evaluation of few-shot modeling. An\nunlabeled training set with up to 20,000 additional samples per task is\nprovided, allowing researchers to explore better ways of using unlabeled\nsamples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning\nmethods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark.Our results show that: 1) all five few-shot\nlearning methods exhibit better performance than fine-tuning or zero-shot\nlearning; 2) among the five methods, PET is the best performing few-shot\nmethod; 3) few-shot learning performance is highly dependent on the specific\ntask. Our benchmark and code are available at\nhttps://github.com/CLUEbenchmark/FewCLUE",
          "link": "http://arxiv.org/abs/2107.07498",
          "publishedOn": "2021-07-16T00:48:23.183Z",
          "wordCount": 671,
          "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Abul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1\">Mark Levene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1\">David Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1\">Renate Fromson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1\">Nicolas Koslover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1\">Tamara Levene</a>",
          "description": "Objective: This study aims to develop an end-to-end natural language\nprocessing pipeline for triage and diagnosis of COVID-19 from patient-authored\nsocial media posts, in order to provide researchers and other interested\nparties with additional information on the symptoms, severity and prevalence of\nthe disease. Materials and Methods: The text processing pipeline first extracts\nCOVID-19 symptoms and related concepts such as severity, duration, negations,\nand body parts from patients posts using conditional random fields. An\nunsupervised rule-based algorithm is then applied to establish relations\nbetween concepts in the next step of the pipeline. The extracted concepts and\nrelations are subsequently used to construct two different vector\nrepresentations of each post. These vectors are applied separately to build\nsupport vector machine learning models to triage patients into three categories\nand diagnose them for COVID-19. Results: We report that macro- and\nmicro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for\nthe triage and diagnosis of COVID-19, when the models are trained on human\nlabelled data. Our experimental results indicate that similar performance can\nbe achieved when the models are trained using predicted labels from concept\nextraction and rule-based classifiers, thus yielding end-to-end machine\nlearning. Also, we highlight important features uncovered by our diagnostic\nmachine learning models and compare them with the most frequent symptoms\nrevealed in another COVID-19 dataset. In particular, we found that the most\nimportant features are not always the most frequent ones.",
          "link": "http://arxiv.org/abs/2103.11850",
          "publishedOn": "2021-07-16T00:48:23.138Z",
          "wordCount": 757,
          "title": "Triage and diagnosis of COVID-19 from medical social media. (arXiv:2103.11850v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:23.079Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1\">Momcilo Vasilijevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nis publicly available at https://github.com/Dod-o/VT-SSum to support the future\nresearch of video transcript segmentation and summarization tasks.",
          "link": "http://arxiv.org/abs/2106.05606",
          "publishedOn": "2021-07-16T00:48:23.072Z",
          "wordCount": 605,
          "title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:22.991Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "Making controlled perturbations is essential for various tasks (e.g., data\naugmentation), but building task-specific generators can be expensive. We\nintroduce Tailor, a task-agnostic generation system that perturbs text in a\nsemantically-controlled way. With unlikelihood training, we design Tailor's\ngenerator to follow a series of control codes derived from semantic roles.\nThrough modifications of these control codes, Tailor can produce fine-grained\nperturbations. We implement a set of operations on control codes that can be\ncomposed into complex perturbation strategies, and demonstrate their\neffectiveness in three distinct applications: First, Tailor facilitates the\nconstruction of high-quality contrast sets that are lexically diverse, and less\nbiased than original task test data. Second, paired with automated labeling\nheuristics, Tailor helps improve model generalization through data\naugmentation: We obtain an average gain of 1.73 on an NLI challenge set by\nperturbing just 5% of training data. Third, without any finetuning overhead,\nTailor's perturbations effectively improve compositionality in fine-grained\nstyle transfer, outperforming fine-tuned baselines on 6 transfers.",
          "link": "http://arxiv.org/abs/2107.07150",
          "publishedOn": "2021-07-16T00:48:22.906Z",
          "wordCount": 594,
          "title": "Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:22.899Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Weiping Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Many real-world applications involve the use of Optical Character Recognition\n(OCR) engines to transform handwritten images into transcripts on which\ndownstream Natural Language Processing (NLP) models are applied. In this\nprocess, OCR engines may introduce errors and inputs to downstream NLP models\nbecome noisy. Despite that pre-trained models achieve state-of-the-art\nperformance in many NLP benchmarks, we prove that they are not robust to noisy\ntexts generated by real OCR engines. This greatly limits the application of NLP\nmodels in real-world scenarios. In order to improve model performance on noisy\nOCR transcripts, it is natural to train the NLP model on labelled noisy texts.\nHowever, in most cases there are only labelled clean texts. Since there is no\nhandwritten pictures corresponding to the text, it is impossible to directly\nuse the recognition model to obtain noisy labelled data. Human resources can be\nemployed to copy texts and take pictures, but it is extremely expensive\nconsidering the size of data for model training. Consequently, we are\ninterested in making NLP models intrinsically robust to OCR errors in a low\nresource manner. We propose a novel robust training framework which 1) employs\nsimple but effective methods to directly simulate natural OCR noises from clean\ntexts and 2) iteratively mines the hard examples from a large number of\nsimulated samples for optimal performance. 3) To make our model learn\nnoise-invariant representations, a stability loss is employed. Experiments on\nthree real-world datasets show that the proposed framework boosts the\nrobustness of pre-trained models by a large margin. We believe that this work\ncan greatly promote the application of NLP models in actual scenarios, although\nthe algorithm we use is simple and straightforward. We make our codes and three\ndatasets publicly\navailable\\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.",
          "link": "http://arxiv.org/abs/2107.07113",
          "publishedOn": "2021-07-16T00:48:22.816Z",
          "wordCount": 754,
          "title": "Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining. (arXiv:2107.07113v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>",
          "description": "Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.",
          "link": "http://arxiv.org/abs/2107.06963",
          "publishedOn": "2021-07-16T00:48:22.800Z",
          "wordCount": 609,
          "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features. (arXiv:2107.06963v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:22.784Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:22.760Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019.",
          "link": "http://arxiv.org/abs/2107.07253",
          "publishedOn": "2021-07-16T00:48:22.751Z",
          "wordCount": 497,
          "title": "Spanish Language Models. (arXiv:2107.07253v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:22.730Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luckin_R/0/1/0/all/0/1\">Rose Luckin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "In this work, we study computational approaches to detect online dialogic\ninstructions, which are widely used to help students understand learning\nmaterials, and build effective study habits. This task is rather challenging\ndue to the widely-varying quality and pedagogical styles of dialogic\ninstructions. To address these challenges, we utilize pre-trained language\nmodels, and propose a multi-task paradigm which enhances the ability to\ndistinguish instances of different classes by enlarging the margin between\ncategories via contrastive loss. Furthermore, we design a strategy to fully\nexploit the misclassified examples during the training stage. Extensive\nexperiments on a real-world online educational data set demonstrate that our\napproach achieves superior performance compared to representative baselines. To\nencourage reproducible results, we make our implementation online available at\n\\url{https://github.com/AIED2021/multitask-dialogic-instruction}.",
          "link": "http://arxiv.org/abs/2107.07119",
          "publishedOn": "2021-07-16T00:48:22.722Z",
          "wordCount": 586,
          "title": "Multi-Task Learning based Online Dialogic Instruction Detection with Pre-trained Language Models. (arXiv:2107.07119v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>",
          "description": "In this paper, we describe our end-to-end multilingual speech translation\nsystem submitted to the IWSLT 2021 evaluation campaign on the Multilingual\nSpeech Translation shared task. Our system is built by leveraging transfer\nlearning across modalities, tasks and languages. First, we leverage\ngeneral-purpose multilingual modules pretrained with large amounts of\nunlabelled and labelled data. We further enable knowledge transfer from the\ntext task to the speech task by training two tasks jointly. Finally, our\nmultilingual model is finetuned on speech translation task-specific data to\nachieve the best translation results. Experimental results show our system\noutperforms the reported systems, including both end-to-end and cascaded based\napproaches, by a large margin.\n\nIn some translation directions, our speech translation results evaluated on\nthe public Multilingual TEDx test set are even comparable with the ones from a\nstrong text-to-text translation system, which uses the oracle speech\ntranscripts as input.",
          "link": "http://arxiv.org/abs/2107.06959",
          "publishedOn": "2021-07-16T00:48:22.698Z",
          "wordCount": 610,
          "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Luke Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>",
          "description": "As neural language models grow in effectiveness, they are increasingly being\napplied in real-world settings. However these applications tend to be limited\nin the modes of interaction they support. In this extended abstract, we propose\nWordcraft, an AI-assisted editor for story writing in which a writer and a\ndialog system collaborate to write a story. Our novel interface uses few-shot\nlearning and the natural affordances of conversation to support a variety of\ninteractions. Our editor provides a sandbox for writers to probe the boundaries\nof transformer-based language models and paves the way for future\nhuman-in-the-loop training pipelines and novel evaluation methods.",
          "link": "http://arxiv.org/abs/2107.07430",
          "publishedOn": "2021-07-16T00:48:22.690Z",
          "wordCount": 550,
          "title": "Wordcraft: a Human-AI Collaborative Editor for Story Writing. (arXiv:2107.07430v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mawalim_C/0/1/0/all/0/1\">Candy Olivia Mawalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unoki_M/0/1/0/all/0/1\">Masashi Unoki</a>",
          "description": "Speaker anonymization aims to suppress speaker individuality to protect\nprivacy in speech while preserving the other aspects, such as speech content.\nOne effective solution for anonymization is to modify the McAdams coefficient.\nIn this work, we propose a method to improve the security for speaker\nanonymization based on the McAdams coefficient by using a speech watermarking\napproach. The proposed method consists of two main processes: one for embedding\nand one for detection. In embedding process, two different McAdams coefficients\nrepresent binary bits ``0\" and ``1\". The watermarked speech is then obtained by\nframe-by-frame bit inverse switching. Subsequently, the detection process is\ncarried out by a power spectrum comparison. We conducted objective evaluations\nwith reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech\nwatermarking with reference to the Information Hiding Challenge (IHC) and found\nthat our method could satisfy the blind detection, inaudibility, and robustness\nrequirements in watermarking. It also significantly improved the anonymization\nperformance in comparison to the secondary baseline system in VP2020.",
          "link": "http://arxiv.org/abs/2107.07223",
          "publishedOn": "2021-07-16T00:48:22.664Z",
          "wordCount": 603,
          "title": "Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method. (arXiv:2107.07223v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:22.621Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiongqiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiafu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL) and building computational approaches to automatically solve such\nquestions is beneficial to language learners. In this work, we propose a neural\nframework to solve SC questions in English examinations by utilizing\npre-trained language models. We conduct extensive experiments on a real-world\nK-12 ESL SC question dataset and the results demonstrate the superiority of our\nmodel in terms of prediction accuracy. Furthermore, we run precision-recall\ntrade-off analysis to discuss the practical issues when deploying it in\nreal-life scenarios. To encourage reproducible results, we make our code\npublicly available at \\url{https://github.com/AIED2021/ESL-SentenceCompletion}.",
          "link": "http://arxiv.org/abs/2107.07122",
          "publishedOn": "2021-07-16T00:48:22.592Z",
          "wordCount": 595,
          "title": "Solving ESL Sentence Completion Questions via Pre-trained Neural Language Models. (arXiv:2107.07122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:22.510Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We propose a transition-based bubble parser to perform coordination structure\nidentification and dependency-based syntactic analysis simultaneously. Bubble\nrepresentations were proposed in the formal linguistics literature decades ago;\nthey enhance dependency trees by encoding coordination boundaries and internal\nrelationships within coordination structures explicitly. In this paper, we\nintroduce a transition system and neural models for parsing these\nbubble-enhanced structures. Experimental results on the English Penn Treebank\nand the English GENIA corpus show that our parsers beat previous\nstate-of-the-art approaches on the task of coordination structure prediction,\nespecially for the subset of sentences with complex coordination structures.",
          "link": "http://arxiv.org/abs/2107.06905",
          "publishedOn": "2021-07-16T00:48:22.501Z",
          "wordCount": 536,
          "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction. (arXiv:2107.06905v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>",
          "description": "We present our contribution to the IWPT 2021 shared task on parsing into\nenhanced Universal Dependencies. Our main system component is a hybrid\ntree-graph parser that integrates (a) predictions of spanning trees for the\nenhanced graphs with (b) additional graph edges not present in the spanning\ntrees. We also adopt a finetuning strategy where we first train a\nlanguage-generic parser on the concatenation of data from all available\nlanguages, and then, in a second step, finetune on each individual language\nseparately. Additionally, we develop our own complete set of pre-processing\nmodules relevant to the shared task, including tokenization, sentence\nsegmentation, and multiword token expansion, based on pre-trained XLM-R models\nand our own pre-training of character-level language models. Our submission\nreaches a macro-average ELAS of 89.24 on the test set. It ranks top among all\nteams, with a margin of more than 2 absolute ELAS over the next best-performing\nsubmission, and best score on 16 out of 17 languages.",
          "link": "http://arxiv.org/abs/2107.06907",
          "publishedOn": "2021-07-16T00:48:22.340Z",
          "wordCount": 610,
          "title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning. (arXiv:2107.06907v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.09558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">I-Ling Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_F/0/1/0/all/0/1\">Farokh Bastani</a>",
          "description": "In this paper, we consider the IoT data discovery data objects to specific\nnodes in the network. They are very problem in very large and growing scale\nnetworks. Specifically, we investigate in depth the routing table summarization\ntechniques to support effective and space-efficient IoT data discovery routing.\nNovel summarization algorithms, including alphabetical based, hash based, and\nmeaning based summarization and their corresponding coding schemes are\nproposed. The issue of potentially misleading routing due to summarization is\nalso investigated. Subsequently, we analyze the strategy of when to summarize\nin order to balance the tradeoff especially in handling MAA based lookups.\nbetween the routing table compression rate and the chance of Unstructured\ndiscovery routing approaches, such as [4] [5], causing misleading routing. For\nexperimental study, we have collected 100K IoT data streams from various IoT\ndatabases as the input dataset. Experimental results show that our\nsummarization solution can reduce the routing table size by 20 to 30 folds with\n2-5% increase in latency when compared with similar peer-to-peer discovery\nrouting algorithms without summarization. Also, our approach outperforms DHT\nbased approaches by 2 to 6 folds in terms of latency and traffic.",
          "link": "http://arxiv.org/abs/2107.09558",
          "publishedOn": "2021-07-21T02:01:33.669Z",
          "wordCount": 658,
          "title": "Into Summarization Techniques for IoT Data Discovery Routing. (arXiv:2107.09558v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets.",
          "link": "http://arxiv.org/abs/2010.12537",
          "publishedOn": "2021-07-21T02:01:32.983Z",
          "wordCount": 662,
          "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1\">Domenico Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1\">Raffaele Giancarlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1\">Giosu&#xe8; Lo Bosco</a>",
          "description": "Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.",
          "link": "http://arxiv.org/abs/2107.09480",
          "publishedOn": "2021-07-21T02:01:32.669Z",
          "wordCount": 696,
          "title": "Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanci Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianming Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donohue_L/0/1/0/all/0/1\">Lawrence Donohue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>",
          "description": "The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.",
          "link": "http://arxiv.org/abs/2104.11783",
          "publishedOn": "2021-07-20T02:04:39.008Z",
          "wordCount": 601,
          "title": "Form 10-Q Itemization. (arXiv:2104.11783v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_N/0/1/0/all/0/1\">Niloofar Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouvelas_N/0/1/0/all/0/1\">Nikolaos Kouvelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1\">R Venkatesha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucani_D/0/1/0/all/0/1\">Daniel E. Lucani</a>",
          "description": "High frame-corruption is widely observed in Long Range Wide Area Networks\n(LoRaWAN) due to the coexistence with other networks in ISM bands and an\nAloha-like MAC layer. LoRa's Forward Error Correction (FEC) mechanism is often\ninsufficient to retrieve corrupted data. In fact, real-life measurements show\nthat at least one-fourth of received transmissions are corrupted. When more\nframes are dropped, LoRa nodes usually switch over to higher spreading factors\n(SF), thus increasing transmission times and increasing the required energy.\nThis paper introduces ReDCoS, a novel coding technique at the application layer\nthat improves recovery of corrupted LoRa frames, thus reducing the overall\ntransmission time and energy invested by LoRa nodes by several-fold. ReDCoS\nutilizes lightweight coding techniques to pre-encode the transmitted data.\nTherefore, the inbuilt Cyclic Redundancy Check (CRC) that follows is computed\nbased on an already encoded data. At the receiver, we use both the CRC and the\ncoded data to recover data from a corrupted frame beyond the built-in Error\nCorrecting Code (ECC). We compare the performance of ReDCoS to (I) the standard\nFEC of vanilla-LoRaWAN, and to (ii) RS coding applied as ECC to the data of\nLoRaWAN. The results indicated a 54x and 13.5x improvement of decoding ratio,\nrespectively, when 20 data symbols were sent. Furthermore, we evaluated ReDCoS\non-field using LoRa SX1261 transceivers showing that it outperformed RS-coding\nby factor of at least 2x (and up to 6x) in terms of the decoding ratio while\nconsuming 38.5% less energy per correctly received transmission.",
          "link": "http://arxiv.org/abs/2107.08868",
          "publishedOn": "2021-07-20T02:04:38.982Z",
          "wordCount": 688,
          "title": "Energy Efficient Data Recovery from Corrupted LoRa Frames. (arXiv:2107.08868v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:38.396Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourkamali_F/0/1/0/all/0/1\">Farzad Pourkamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1\">Nicolas Macris</a>",
          "description": "We consider the estimation of an n-dimensional vector s from the noisy\nelement-wise measurements of $\\mathbf{s}\\mathbf{s}^T$, a generic problem that\narises in statistics and machine learning. We study a mismatched Bayesian\ninference setting, where some of the parameters are not known to the\nstatistician. We derive the full exact analytic expression of the asymptotic\nmean squared error (MSE) in the large system size limit for the particular case\nof Gaussian priors and additive noise. From our formulas, we see that\nestimation is still possible in the mismatched case; and also that the minimum\nMSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our\ntechnique relies on the asymptotics of the spherical integrals and can be\napplied as long as the statistician chooses a rotationally invariant prior.",
          "link": "http://arxiv.org/abs/2107.08927",
          "publishedOn": "2021-07-20T02:04:38.368Z",
          "wordCount": 562,
          "title": "Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:38.289Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:38.147Z",
          "wordCount": 618,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lakshya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sagnik Sarkar</a>",
          "description": "Typical e-commerce platforms contain millions of products in the catalog.\nUsers visit these platforms and enter search queries to retrieve their desired\nproducts. Therefore, showing the relevant products at the top is essential for\nthe success of e-commerce platforms. We approach this problem by learning low\ndimension representations for queries and product descriptions by leveraging\nuser click-stream data as our main source of signal for product relevance.\nStarting from GRU-based architectures as our baseline model, we move towards a\nmore advanced transformer-based architecture. This helps the model to learn\ncontextual representations of queries and products to serve better search\nresults and understand the user intent in an efficient manner. We perform\nexperiments related to pre-training of the Transformer based RoBERTa model\nusing a fashion corpus and fine-tuning it over the triplet loss. Our\nexperiments on the product ranking task show that the RoBERTa model is able to\ngive an improvement of 7.8% in Mean Reciprocal Rank(MRR), 15.8% in Mean Average\nPrecision(MAP) and 8.8% in Normalized Discounted Cumulative Gain(NDCG), thus\noutperforming our GRU based baselines. For the product retrieval task, RoBERTa\nmodel is able to outperform other two models with an improvement of 164.7% in\nPrecision@50 and 145.3% in Recall@50. In order to highlight the importance of\npre-training RoBERTa for fashion domain, we qualitatively compare already\npre-trained RoBERTa on standard datasets with our custom pre-trained RoBERTa\nover a fashion corpus for the query token prediction task. Finally, we also\nshow a qualitative comparison between GRU and RoBERTa results for product\nretrieval task for some test queries.",
          "link": "http://arxiv.org/abs/2107.08291",
          "publishedOn": "2021-07-20T02:04:38.103Z",
          "wordCount": 692,
          "title": "Neural Search: Learning Query and Product Representations in Fashion E-commerce. (arXiv:2107.08291v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yinqiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Similar question retrieval is a core task in community-based question\nanswering (CQA) services. To balance the effectiveness and efficiency, the\nquestion retrieval system is typically implemented as multi-stage rankers: The\nfirst-stage ranker aims to recall potentially relevant questions from a large\nrepository, and the latter stages attempt to re-rank the retrieved results.\nMost existing works on question retrieval mainly focused on the re-ranking\nstages, leaving the first-stage ranker to some traditional term-based methods.\nHowever, term-based methods often suffer from the vocabulary mismatch problem,\nespecially on short texts, which may block the re-rankers from relevant\nquestions at the very beginning. An alternative is to employ embedding-based\nmethods for the first-stage ranker, which compress texts into dense vectors to\nenhance the semantic matching. However, these methods often lose the\ndiscriminative power as term-based methods, thus introduce noise during\nretrieval and hurt the recall performance. In this work, we aim to tackle the\ndilemma of the first-stage ranker, and propose a discriminative semantic\nranker, namely DenseTrans, for high-recall retrieval. Specifically, DenseTrans\nis a densely connected Transformer, which learns semantic embeddings for texts\nbased on Transformer layers. Meanwhile, DenseTrans promotes low-level features\nthrough dense connections to keep the discriminative power of the learned\nrepresentations. DenseTrans is inspired by DenseNet in computer vision (CV),\nbut poses a new way to use the dense connectivity which is totally different\nfrom its original design purpose. Experimental results over two question\nretrieval benchmark datasets show that our model can obtain significant gain on\nrecall against strong term-based methods as well as state-of-the-art\nembedding-based methods.",
          "link": "http://arxiv.org/abs/2107.08345",
          "publishedOn": "2021-07-20T02:04:38.062Z",
          "wordCount": 691,
          "title": "A Discriminative Semantic Ranker for Question Retrieval. (arXiv:2107.08345v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:05.316Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_J/0/1/0/all/0/1\">Jean-Francis Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassani_G/0/1/0/all/0/1\">Giovanni Cassani</a>",
          "description": "The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for\n\"In-session prediction for purchase intent and recommendations\". The challenge\naddresses the growing need for reliable predictions within the boundaries of a\nshopping session, as customer intentions can be different depending on the\noccasion. The need for efficient procedures for personalization is even clearer\nif we consider the e-commerce landscape more broadly: outside of giant digital\nretailers, the constraints of the problem are stricter, due to smaller user\nbases and the realization that most users are not frequently returning\ncustomers. We release a new session-based dataset including more than 30M\nfine-grained browsing events (product detail, add, purchase), enriched by\nlinguistic behavior (queries made by shoppers, with items clicked and items not\nclicked after the query) and catalog meta-data (images, text, pricing\ninformation). On this dataset, we ask participants to showcase innovative\nsolutions for two open problems: a recommendation task (where a model is shown\nsome events at the start of a session, and it is asked to predict future\nproduct interactions); an intent prediction task, where a model is shown a\nsession containing an add-to-cart event, and it is asked to predict whether the\nitem will be bought before the end of the session.",
          "link": "http://arxiv.org/abs/2104.09423",
          "publishedOn": "2021-07-19T00:49:05.195Z",
          "wordCount": 695,
          "title": "SIGIR 2021 E-Commerce Workshop Data Challenge. (arXiv:2104.09423v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:05.156Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shivani Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthra_T/0/1/0/all/0/1\">Tarun Luthra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashima Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajat Singh</a>",
          "description": "Knowledge Graph embedding provides a versatile technique for representing\nknowledge. These techniques can be used in a variety of applications such as\ncompletion of knowledge graph to predict missing information, recommender\nsystems, question answering, query expansion, etc. The information embedded in\nKnowledge graph though being structured is challenging to consume in a\nreal-world application. Knowledge graph embedding enables the real-world\napplication to consume information to improve performance. Knowledge graph\nembedding is an active research area. Most of the embedding methods focus on\nstructure-based information. Recent research has extended the boundary to\ninclude text-based information and image-based information in entity embedding.\nEfforts have been made to enhance the representation with context information.\nThis paper introduces growth in the field of KG embedding from simple\ntranslation-based models to enrichment-based models. This paper includes the\nutility of the Knowledge graph in real-world applications.",
          "link": "http://arxiv.org/abs/2107.07842",
          "publishedOn": "2021-07-19T00:49:05.141Z",
          "wordCount": 580,
          "title": "A Survey of Knowledge Graph Embedding and Their Applications. (arXiv:2107.07842v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:05.131Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:04.905Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "Dense retrieval conducts text retrieval in the embedding space and has shown\nmany advantages compared to sparse retrieval. Existing dense retrievers\noptimize representations of queries and documents with contrastive training and\nmap them to the embedding space. The embedding space is optimized by aligning\nthe matched query-document pairs and pushing the negative documents away from\nthe query. However, in such training paradigm, the queries are only optimized\nto align to the documents and are coarsely positioned, leading to an\nanisotropic query embedding space. In this paper, we analyze the embedding\nspace distributions and propose an effective training paradigm, Contrastive\nDual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained\nquery representations for dense retrieval. DANCE incorporates an additional\ndual training object of query retrieval, inspired by the classic information\nretrieval training axiom, query likelihood. With contrastive learning, the dual\ntraining object of DANCE learns more tailored representations for queries and\ndocuments to keep the embedding space smooth and uniform, thriving on the\nranking performance of DANCE on the MS MARCO document retrieval task. Different\nfrom ANCE that only optimized with the document retrieval task, DANCE\nconcentrates the query embeddings closer to document representations while\nmaking the document distribution more discriminative. Such concentrated query\nembedding distribution assigns more uniform negative sampling probabilities to\nqueries and helps to sufficiently optimize query representations in the query\nretrieval task. Our codes are released at https://github.com/thunlp/DANCE.",
          "link": "http://arxiv.org/abs/2107.07773",
          "publishedOn": "2021-07-19T00:49:04.885Z",
          "wordCount": 669,
          "title": "More Robust Dense Retrieval with Contrastive Dual Learning. (arXiv:2107.07773v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.212Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:22.196Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chintoo Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_C/0/1/0/all/0/1\">C. Ravindranath Chowdary</a>",
          "description": "In general, recommender systems are designed to provide personalized items to\na user. But in few cases, items are recommended for a group, and the challenge\nis to aggregate the individual user preferences to infer the recommendation to\na group. It is also important to consider the similarity of characteristics\namong the members of a group to generate a better recommendation. Members of an\nautomatically identified group will have similar characteristics, and reaching\na consensus with a decision-making process is preferable in this case. It\nrequires users-items and their rating interactions over a utility matrix to\nauto-detect the groups in group recommendations. We may not overlook other\nintrinsic information to form a group. The textual information also plays a\npivotal role in user clustering. In this paper, we auto-detect the groups based\non the textual similarity of the metadata (review texts). We consider the order\nin user preferences in our models. We have conducted extensive experiments over\ntwo real-world datasets to check the efficacy of the proposed models. We have\nalso conducted a competitive comparison with a baseline model to show the\nimprovements in the quality of recommendations.",
          "link": "http://arxiv.org/abs/2107.07284",
          "publishedOn": "2021-07-16T00:48:22.160Z",
          "wordCount": 618,
          "title": "Auto-detecting groups based on textual similarity for group recommendations. (arXiv:2107.07284v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenzhuo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shoujin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengsheng Wang</a>",
          "description": "The changing preferences of users towards items trigger the emergence of\nsession-based recommender systems (SBRSs), which aim to model the dynamic\npreferences of users for next-item recommendations. However, most of the\nexisting studies on SBRSs are based on long sessions only for recommendations,\nignoring short sessions, though short sessions, in fact, account for a large\nproportion in most of the real-world datasets. As a result, the applicability\nof existing SBRSs solutions is greatly reduced. In a short session, quite\nlimited contextual information is available, making the next-item\nrecommendation very challenging. To this end, in this paper, inspired by the\nsuccess of few-shot learning (FSL) in effectively learning a model with limited\ninstances, we formulate the next-item recommendation as an FSL problem.\nAccordingly, following the basic idea of a representative approach for FSL,\ni.e., meta-learning, we devise an effective SBRS called INter-SEssion\ncollaborative Recommender netTwork (INSERT) for next-item recommendations in\nshort sessions. With the carefully devised local module and global module,\nINSERT is able to learn an optimal preference representation of the current\nuser in a given short session. In particular, in the global module, a similar\nsession retrieval network (SSRN) is designed to find out the sessions similar\nto the current short session from the historical sessions of both the current\nuser and other users, respectively. The obtained similar sessions are then\nutilized to complement and optimize the preference representation learned from\nthe current short session by the local module for more accurate next-item\nrecommendations in this short session. Extensive experiments conducted on two\nreal-world datasets demonstrate the superiority of our proposed INSERT over the\nstate-of-the-art SBRSs when making next-item recommendations in short sessions.",
          "link": "http://arxiv.org/abs/2107.07453",
          "publishedOn": "2021-07-16T00:48:22.148Z",
          "wordCount": 707,
          "title": "Next-item Recommendations in Short Sessions. (arXiv:2107.07453v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>",
          "description": "Sequential recommender systems (SRS) have become a research hotspot due to\nits power in modeling user dynamic interests and sequential behavioral\npatterns. To maximize model expressive ability, a default choice is to apply a\nlarger and deeper network architecture, which, however, often brings high\nnetwork latency when generating online recommendations. Naturally, we argue\nthat compressing the heavy recommendation models into middle- or light- weight\nneural networks is of great importance for practical production systems. To\nrealize such a goal, we propose AdaRec, a knowledge distillation (KD) framework\nwhich compresses knowledge of a teacher model into a student model adaptively\naccording to its recommendation scene by using differentiable Neural\nArchitecture Search (NAS). Specifically, we introduce a target-oriented\ndistillation loss to guide the structure search process for finding the student\nnetwork architecture, and a cost-sensitive loss as constraints for model size,\nwhich achieves a superior trade-off between recommendation effectiveness and\nefficiency. In addition, we leverage Earth Mover's Distance (EMD) to realize\nmany-to-many layer mapping during knowledge distillation, which enables each\nintermediate student layer to learn from other intermediate teacher layers\nadaptively. Extensive experiments on real-world recommendation datasets\ndemonstrate that our model achieves competitive or better accuracy with notable\ninference speedup comparing to strong counterparts, while discovering diverse\nneural architectures for sequential recommender models under different\nrecommendation scenes.",
          "link": "http://arxiv.org/abs/2107.07173",
          "publishedOn": "2021-07-16T00:48:22.123Z",
          "wordCount": 654,
          "title": "Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search. (arXiv:2107.07173v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:22.054Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:21.994Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:33.208Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Ying Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>",
          "description": "This paper investigates adaptive streaming of one or multiple tiled 360\nvideos from a multi-antenna base station (BS) to one or multiple single-antenna\nusers, respectively, in a multi-carrier wireless system. We aim to maximize the\nvideo quality while keeping rebuffering time small via encoding rate adaptation\nat each group of pictures (GOP) and transmission adaptation at each\n(transmission) slot. To capture the impact of field-of-view (FoV) prediction,\nwe consider three cases of FoV viewing probability distributions, i.e.,\nperfect, imperfect, and unknown FoV viewing probability distributions, and use\nthe average total utility, worst average total utility, and worst total utility\nas the respective performance metrics. In the single-user scenario, we optimize\nthe encoding rates of the tiles, encoding rates of the FoVs, and transmission\nbeamforming vectors for all subcarriers to maximize the total utility in each\ncase. In the multi-user scenario, we adopt rate splitting with successive\ndecoding and optimize the encoding rates of the tiles, encoding rates of the\nFoVs, rates of the common and private messages, and transmission beamforming\nvectors for all subcarriers to maximize the total utility in each case. Then,\nwe separate the challenging optimization problem into multiple tractable\nproblems in each scenario. In the single-user scenario, we obtain a globally\noptimal solution of each problem using transformation techniques and the\nKarush-Kuhn-Tucker (KKT) conditions. In the multi-user scenario, we obtain a\nKKT point of each problem using the concave-convex procedure (CCCP). Finally,\nnumerical results demonstrate that the proposed solutions achieve notable gains\nover existing schemes in all three cases. To the best of our knowledge, this is\nthe first work revealing the impact of FoV prediction on the performance of\nadaptive streaming of tiled 360 videos.",
          "link": "http://arxiv.org/abs/2107.09491",
          "publishedOn": "2021-07-21T02:01:33.193Z",
          "wordCount": 744,
          "title": "Adaptive Streaming of 360 Videos with Perfect, Imperfect, and Unknown FoV Viewing Probabilities in Wireless Networks. (arXiv:2107.09491v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gunjan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "Dance and music typically go hand in hand. The complexities in dance, music,\nand their synchronisation make them fascinating to study from a computational\ncreativity perspective. While several works have looked at generating dance for\na given music, automatically generating music for a given dance remains\nunder-explored. This capability could have several creative expression and\nentertainment applications. We present some early explorations in this\ndirection. We present a search-based offline approach that generates music\nafter processing the entire dance video and an online approach that uses a deep\nneural network to generate music on-the-fly as the video proceeds. We compare\nthese approaches to a strong heuristic baseline via human studies and present\nour findings. We have integrated our online approach in a live demo! A video of\nthe demo can be found here:\nhttps://sites.google.com/view/dance2music/live-demo.",
          "link": "http://arxiv.org/abs/2107.06252",
          "publishedOn": "2021-07-21T02:01:33.048Z",
          "wordCount": 581,
          "title": "Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yinzhe Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>",
          "description": "In order to protect the intellectual property (IP) of deep neural networks\n(DNNs), many existing DNN watermarking techniques either embed watermarks\ndirectly into the DNN parameters or insert backdoor watermarks by fine-tuning\nthe DNN parameters, which, however, cannot resist against various attack\nmethods that remove watermarks by altering DNN parameters. In this paper, we\nbypass such attacks by introducing a structural watermarking scheme that\nutilizes channel pruning to embed the watermark into the host DNN architecture\ninstead of crafting the DNN parameters. To be specific, during watermark\nembedding, we prune the internal channels of the host DNN with the channel\npruning rates controlled by the watermark. During watermark extraction, the\nwatermark is retrieved by identifying the channel pruning rates from the\narchitecture of the target DNN model. Due to the superiority of pruning\nmechanism, the performance of the DNN model on its original task is reserved\nduring watermark embedding. Experimental results have shown that, the proposed\nwork enables the embedded watermark to be reliably recovered and provides a\nhigh watermark capacity, without sacrificing the usability of the DNN model. It\nis also demonstrated that the work is robust against common transforms and\nattacks designed for conventional watermarking approaches.",
          "link": "http://arxiv.org/abs/2107.08688",
          "publishedOn": "2021-07-20T02:04:38.180Z",
          "wordCount": 640,
          "title": "Structural Watermarking to Deep Neural Networks via Network Channel Pruning. (arXiv:2107.08688v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:37.964Z",
          "wordCount": 668,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:37.888Z",
          "wordCount": 685,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:37.706Z",
          "wordCount": 689,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:04.940Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:04.774Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:22.536Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HajiAkhondi_Meybodi_Z/0/1/0/all/0/1\">Zohreh HajiAkhondi-Meybodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouei_J/0/1/0/all/0/1\">Jamshid Abouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Ming Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "Recently, as a consequence of the COVID-19 pandemic, dependence on\ntelecommunication for remote working and telemedicine has significantly\nincreased. In cellular networks, incorporation of Unmanned Aerial Vehicles\n(UAVs) can result in enhanced connectivity for outdoor users due to the high\nprobability of establishing Line of Sight (LoS) links. The UAV's limited\nbattery life and its signal attenuation in indoor areas, however, make it\ninefficient to manage users' requests in indoor environments. Referred to as\nthe Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the\nnetwork's coverage in both indoor and outdoor environments increases via a\ntwo-phase clustering for FAPs' formation and UAVs' deployment. First objective\nis to increase the content diversity. In this context, we propose a coded\ncontent placement in a cluster-centric cellular network, which is integrated\nwith the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference\nin edge areas. Then, we compute, experimentally, the number of coded contents\nto be stored in each caching node to increase the cache-hit ratio,\nSignal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and\ndecrease the users' access delay and cache redundancy for different content\npopularity profiles. Capitalizing on clustering, our second objective is to\nassign the best caching node to indoor/outdoor users for managing their\nrequests. In this regard, we define the movement speed of ground users as the\ndecision metric of the transmission scheme for serving outdoor users' requests\nto avoid frequent handovers between FAPs and increase the battery life of UAVs.\nSimulation results illustrate that the proposed CCUF implementation increases\nthe cache hit-ratio, SINR, and cache diversity and decrease the users' access\ndelay, cache redundancy and UAVs' energy consumption.",
          "link": "http://arxiv.org/abs/2101.11787",
          "publishedOn": "2021-07-16T00:48:22.525Z",
          "wordCount": 790,
          "title": "Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks. (arXiv:2101.11787v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiayi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.",
          "link": "http://arxiv.org/abs/2107.07268",
          "publishedOn": "2021-07-16T00:48:22.242Z",
          "wordCount": 606,
          "title": "Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:22.137Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobbers_S/0/1/0/all/0/1\">Sebastian L&#xf6;bbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.",
          "link": "http://arxiv.org/abs/2107.07360",
          "publishedOn": "2021-07-16T00:48:22.090Z",
          "wordCount": 599,
          "title": "Sketching sounds: an exploratory study on sound-shape associations. (arXiv:2107.07360v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.09609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr",
          "link": "http://arxiv.org/abs/2107.09609",
          "publishedOn": "2021-07-21T02:01:37.296Z",
          "wordCount": 680,
          "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. (arXiv:2107.09609v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion and Gaussian perturbations improves, while even improving the\nperformance on clean images slightly without performing any data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-07-21T02:01:37.289Z",
          "wordCount": 556,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenbin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1\">Qi Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>",
          "description": "Human activity recognition (HAR) in ubiquitous computing has been beginning\nto incorporate attention into the context of deep neural networks (DNNs), in\nwhich the rich sensing data from multimodal sensors such as accelerometer and\ngyroscope is used to infer human activities. Recently, two attention methods\nare proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term\nMemory (LSTM) network, which can capture the dependencies of sensing signals in\nboth spatial and temporal domains simultaneously. However, recurrent networks\noften have a weak feature representing power compared with convolutional neural\nnetworks (CNNs). On the other hand, two attention, i.e., hard attention and\nsoft attention, are applied in temporal domains via combining with CNN, which\npay more attention to the target activity from a long sequence. However, they\ncan only tell where to focus and miss channel information, which plays an\nimportant role in deciding what to focus. As a result, they fail to address the\nspatial-temporal dependencies of multimodal sensing signals, compared with\nattention-based GRU or LSTM. In the paper, we propose a novel dual attention\nmethod called DanHAR, which introduces the framework of blending channel\nattention and temporal attention on a CNN, demonstrating superiority in\nimproving the comprehensibility for multimodal HAR. Extensive experiments on\nfour public HAR datasets and weakly labeled dataset show that DanHAR achieves\nstate-of-the-art performance with negligible overhead of parameters.\nFurthermore, visualizing analysis is provided to show that our attention can\namplifies more important sensor modalities and timesteps during classification,\nwhich agrees well with human common intuition.",
          "link": "http://arxiv.org/abs/2006.14435",
          "publishedOn": "2021-07-21T02:01:36.612Z",
          "wordCount": 734,
          "title": "DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors. (arXiv:2006.14435v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_H/0/1/0/all/0/1\">H. Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_W/0/1/0/all/0/1\">W. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">J. S. Cardoso</a>",
          "description": "The use of Deep Learning in the medical field is hindered by the lack of\ninterpretability. Case-based interpretability strategies can provide intuitive\nexplanations for deep learning models' decisions, thus, enhancing trust.\nHowever, the resulting explanations threaten patient privacy, motivating the\ndevelopment of privacy-preserving methods compatible with the specifics of\nmedical data. In this work, we analyze existing privacy-preserving methods and\ntheir respective capacity to anonymize medical data while preserving\ndisease-related semantic features. We find that the PPRL-VGAN deep learning\nmethod was the best at preserving the disease-related semantic features while\nguaranteeing a high level of privacy among the compared state-of-the-art\nmethods. Nevertheless, we emphasize the need to improve privacy-preserving\nmethods for medical imaging, as we identified relevant drawbacks in all\nexisting privacy-preserving approaches.",
          "link": "http://arxiv.org/abs/2107.09652",
          "publishedOn": "2021-07-21T02:01:36.586Z",
          "wordCount": 571,
          "title": "Towards Privacy-preserving Explanations in Medical Image Analysis. (arXiv:2107.09652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chongyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>",
          "description": "End-to-end text-spotting, which aims to integrate detection and recognition\nin a unified framework, has attracted increasing attention due to its\nsimplicity of the two complimentary tasks. It remains an open problem\nespecially when processing arbitrarily-shaped text instances. Previous methods\ncan be roughly categorized into two groups: character-based and\nsegmentation-based, which often require character-level annotations and/or\ncomplex post-processing due to the unstructured output. Here, we tackle\nend-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet\nv2). Our main contributions are four-fold: 1) For the first time, we adaptively\nfit arbitrarily-shaped text by a parameterized Bezier curve, which, compared\nwith segmentation-based methods, can not only provide structured output but\nalso controllable representation. 2) We design a novel BezierAlign layer for\nextracting accurate convolution features of a text instance of arbitrary\nshapes, significantly improving the precision of recognition over previous\nmethods. 3) Different from previous methods, which often suffer from complex\npost-processing and sensitive hyper-parameters, our ABCNet v2 maintains a\nsimple pipeline with the only post-processing non-maximum suppression (NMS). 4)\nAs the performance of text recognition closely depends on feature alignment,\nABCNet v2 further adopts a simple yet effective coordinate convolution to\nencode the position of the convolutional filters, which leads to a considerable\nimprovement with negligible computation overhead. Comprehensive experiments\nconducted on various bilingual (English and Chinese) benchmark datasets\ndemonstrate that ABCNet v2 can achieve state-of-the-art performance while\nmaintaining very high efficiency.",
          "link": "http://arxiv.org/abs/2105.03620",
          "publishedOn": "2021-07-21T02:01:36.398Z",
          "wordCount": 728,
          "title": "ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting. (arXiv:2105.03620v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Ye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Multi-person pose estimation is an attractive and challenging task. Existing\nmethods are mostly based on two-stage frameworks, which include top-down and\nbottom-up methods. Two-stage methods either suffer from high computational\nredundancy for additional person detectors or they need to group keypoints\nheuristically after predicting all the instance-agnostic keypoints. The\nsingle-stage paradigm aims to simplify the multi-person pose estimation\npipeline and receives a lot of attention. However, recent single-stage methods\nhave the limitation of low performance due to the difficulty of regressing\nvarious full-body poses from a single feature vector. Different from previous\nsolutions that involve complex heuristic designs, we present a simple yet\neffective solution by employing instance-aware dynamic networks. Specifically,\nwe propose an instance-aware module to adaptively adjust (part of) the network\nparameters for each instance. Our solution can significantly increase the\ncapacity and adaptive-ability of the network for recognizing various poses,\nwhile maintaining a compact end-to-end trainable pipeline. Extensive\nexperiments on the MS-COCO dataset demonstrate that our method achieves\nsignificant improvement over existing single-stage methods, and makes a better\nbalance of accuracy and efficiency compared to the state-of-the-art two-stage\napproaches.",
          "link": "http://arxiv.org/abs/2107.08982",
          "publishedOn": "2021-07-21T02:01:36.213Z",
          "wordCount": 662,
          "title": "InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1\">Steve Borkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1\">Adam Crespi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1\">Saurav Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1\">Sujoy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1\">Jonathan Hogins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1\">You-Cyuan Jhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1\">Mohsen Kamalzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1\">Steven Leal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1\">Pete Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1\">Cesar Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">Wesley Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1\">Alex Thaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1\">Samuel Warren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1\">Nupur Yadav</a>",
          "description": "We introduce the Unity Perception package which aims to simplify and\naccelerate the process of generating synthetic datasets for computer vision\ntasks by offering an easy-to-use and highly customizable toolset. This\nopen-source package extends the Unity Editor and engine components to generate\nperfectly annotated examples for several common computer vision tasks.\nAdditionally, it offers an extensible Randomization framework that lets the\nuser quickly construct and configure randomized simulation parameters in order\nto introduce variation into the generated datasets. We provide an overview of\nthe provided tools and how they work, and demonstrate the value of the\ngenerated synthetic datasets by training a 2D object detection model. The model\ntrained with mostly synthetic data outperforms the model trained using only\nreal data.",
          "link": "http://arxiv.org/abs/2107.04259",
          "publishedOn": "2021-07-21T02:01:36.164Z",
          "wordCount": 613,
          "title": "Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-21T02:01:36.138Z",
          "wordCount": 687,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.00384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "Weakly-supervised object detection attempts to limit the amount of\nsupervision by dispensing the need for bounding boxes, but still assumes\nimage-level labels on the entire training set. In this work, we study the\nproblem of training an object detector from one or few images with image-level\nlabels and a larger set of completely unlabeled images. This is an extreme case\nof semi-supervised learning where the labeled data are not enough to bootstrap\nthe learning of a detector. Our solution is to train a weakly-supervised\nstudent detector model from image-level pseudo-labels generated on the\nunlabeled set by a teacher classifier model, bootstrapped by region-level\nsimilarities to labeled images. Building upon the recent representative\nweakly-supervised pipeline PCL, our method can use more unlabeled images to\nachieve performance competitive or superior to many recent weakly-supervised\ndetection solutions.",
          "link": "http://arxiv.org/abs/1912.00384",
          "publishedOn": "2021-07-21T02:01:35.994Z",
          "wordCount": 633,
          "title": "Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images. (arXiv:1912.00384v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "In recent years, virtual makeup applications have become more and more\npopular. However, it is still challenging to propose a robust makeup transfer\nmethod in the real-world environment. Current makeup transfer methods mostly\nwork well on good-conditioned clean makeup images, but transferring makeup that\nexhibits shadow and occlusion is not satisfying. To alleviate it, we propose a\nnovel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN\n(SOGAN). Given the source and the reference faces, we first fit a 3D face model\nand then disentangle the faces into shape and texture. In the texture branch,\nwe map the texture to the UV space and design a UV texture generator to\ntransfer the makeup. Since human faces are symmetrical in the UV space, we can\nconveniently remove the undesired shadow and occlusion from the reference image\nby carefully designing a Flip Attention Module (FAM). After obtaining cleaner\nmakeup features from the reference image, a Makeup Transfer Module (MTM) is\nintroduced to perform accurate makeup transfer. The qualitative and\nquantitative experiments demonstrate that our SOGAN not only achieves superior\nresults in shadow and occlusion situations but also performs well in large pose\nand expression variations.",
          "link": "http://arxiv.org/abs/2104.10567",
          "publishedOn": "2021-07-21T02:01:35.279Z",
          "wordCount": 676,
          "title": "SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer. (arXiv:2104.10567v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shaodi You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qiu Shen</a>",
          "description": "High-resolution hyperspectral images (HSIs) contain the response of each\npixel in different spectral bands, which can be used to effectively distinguish\nvarious objects in complex scenes. While HSI cameras have become low cost,\nalgorithms based on it have not been well exploited. In this paper, we focus on\na novel topic, weakly-supervised semantic segmentation in cityscape via HSIs.\nIt is based on the idea that high-resolution HSIs in city scenes contain rich\nspectral information, which can be easily associated to semantics without\nmanual labeling. Therefore, it enables low cost, highly reliable semantic\nsegmentation in complex scenes. Specifically, in this paper, we theoretically\nanalyze the HSIs and introduce a weakly-supervised HSI semantic segmentation\nframework, which utilizes spectral information to improve the coarse labels to\na finer degree. The experimental results show that our method can obtain highly\ncompetitive labels and even have higher edge fineness than artificial fine\nlabels in some classes. At the same time, the results also show that the\nrefined labels can effectively improve the effect of semantic segmentation. The\ncombination of HSIs and semantic segmentation proves that HSIs have great\npotential in high-level visual tasks.",
          "link": "http://arxiv.org/abs/2012.10122",
          "publishedOn": "2021-07-21T02:01:35.265Z",
          "wordCount": 652,
          "title": "Weakly-supervised Semantic Segmentation in Cityscape via Hyperspectral Image. (arXiv:2012.10122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinzhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Facial affect analysis (FAA) using visual signals is important in\nhuman-computer interaction. Early methods focus on extracting appearance and\ngeometry features associated with human affects, while ignoring the latent\nsemantic information among individual facial changes, leading to limited\nperformance and generalization. Recent work attempts to establish a graph-based\nrepresentation to model these semantic relationships and develop frameworks to\nleverage them for various FAA tasks. In this paper, we provide a comprehensive\nreview of graph-based FAA, including the evolution of algorithms and their\napplications. First, the FAA background knowledge is introduced, especially on\nthe role of the graph. We then discuss approaches that are widely used for\ngraph-based affective representation in literature and show a trend towards\ngraph construction. For the relational reasoning in graph-based FAA, existing\nstudies are categorized according to their usage of traditional methods or deep\nmodels, with a special emphasis on the latest graph neural networks.\nPerformance comparisons of the state-of-the-art graph-based FAA methods are\nalso summarized. Finally, we discuss the challenges and potential directions.\nAs far as we know, this is the first survey of graph-based FAA methods. Our\nfindings can serve as a reference for future research in this field.",
          "link": "http://arxiv.org/abs/2103.15599",
          "publishedOn": "2021-07-21T02:01:35.259Z",
          "wordCount": 707,
          "title": "Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges. (arXiv:2103.15599v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-07-21T02:01:35.229Z",
          "wordCount": 674,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-07-21T02:01:35.220Z",
          "wordCount": 584,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>",
          "description": "We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2012.02924",
          "publishedOn": "2021-07-21T02:01:35.206Z",
          "wordCount": 737,
          "title": "iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_V/0/1/0/all/0/1\">Vien Ngoc Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galati_F/0/1/0/all/0/1\">Francesco Galati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cortese_R/0/1/0/all/0/1\">Rosa Cortese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1\">Giuseppe Di Giacomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marconetto_V/0/1/0/all/0/1\">Viola Marconetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prateek Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prados_F/0/1/0/all/0/1\">Ferran Prados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_M/0/1/0/all/0/1\">Maria A. Zuluaga</a>",
          "description": "Deep learning techniques for 3D brain vessel image segmentation have not been\nas successful as in the segmentation of other organs and tissues. This can be\nexplained by two factors. First, deep learning techniques tend to show poor\nperformances at the segmentation of relatively small objects compared to the\nsize of the full image. Second, due to the complexity of vascular trees and the\nsmall size of vessels, it is challenging to obtain the amount of annotated\ntraining data typically needed by deep learning methods. To address these\nproblems, we propose a novel annotation-efficient deep learning vessel\nsegmentation framework. The framework avoids pixel-wise annotations, only\nrequiring weak patch-level labels to discriminate between vessel and non-vessel\n2D patches in the training set, in a setup similar to the CAPTCHAs used to\ndifferentiate humans from bots in web applications. The user-provided weak\nannotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels\nfor vessels and background in each patch, which are used to train a\nsegmentation network, and 2) to train a classifier network. The classifier\nnetwork allows to generate additional weak patch labels, further reducing the\nannotation burden, and it acts as a noise filter for poor quality images. We\nuse this framework for the segmentation of the cerebrovascular tree in\nTime-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The\nresults show that the framework achieves state-of-the-art accuracy, while\nreducing the annotation time by ~77% w.r.t. learning-based segmentation methods\nusing pixel-wise labels for training.",
          "link": "http://arxiv.org/abs/2101.09321",
          "publishedOn": "2021-07-21T02:01:35.199Z",
          "wordCount": 746,
          "title": "Vessel-CAPTCHA: an efficient learning framework for vessel annotation and segmentation. (arXiv:2101.09321v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Edward J. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1\">Roberto Calandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Adriana Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>",
          "description": "Humans build 3D understandings of the world through active object\nexploration, using jointly their senses of vision and touch. However, in 3D\nshape reconstruction, most recent progress has relied on static datasets of\nlimited sensory data such as RGB images, depth maps or haptic readings, leaving\nthe active exploration of the shape largely unexplored. In active touch sensing\nfor 3D reconstruction, the goal is to actively select the tactile readings that\nmaximize the improvement in shape reconstruction accuracy. However, the\ndevelopment of deep learning-based active touch models is largely limited by\nthe lack of frameworks for shape exploration. In this paper, we focus on this\nproblem and introduce a system composed of: 1) a haptic simulator leveraging\nhigh spatial resolution vision-based tactile sensors for active touching of 3D\nobjects; 2) a mesh-based 3D shape reconstruction model that relies on tactile\nor visuotactile signals; and 3) a set of data-driven solutions with either\ntactile or visuotactile priors to guide the shape exploration. Our framework\nenables the development of the first fully data-driven solutions to active\ntouch on top of learned models for object understanding. Our experiments show\nthe benefits of such solutions in the task of 3D shape understanding where our\nmodels consistently outperform natural baselines. We provide our framework as a\ntool to foster future research in this direction.",
          "link": "http://arxiv.org/abs/2107.09584",
          "publishedOn": "2021-07-21T02:01:35.174Z",
          "wordCount": 664,
          "title": "Active 3D Shape Reconstruction from Vision and Touch. (arXiv:2107.09584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Emma Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Andy Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rayan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.",
          "link": "http://arxiv.org/abs/2103.09957",
          "publishedOn": "2021-07-21T02:01:35.167Z",
          "wordCount": 697,
          "title": "CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1\">Levente J. Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M. Albrecht</a>",
          "description": "Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide\nfrom the atmosphere. However, the lack of efficient quantification methods of\ncarbon stored in trees renders it difficult to track the process. We present an\napproach to estimate the carbon storage in trees based on fusing multi-spectral\naerial imagery and LiDAR data to identify tree coverage, geometric shape, and\ntree species -- key attributes to carbon storage quantification. We demonstrate\nthat tree species information and their three-dimensional geometric shapes can\nbe estimated from aerial imagery in order to determine the tree's biomass.\nSpecifically, we estimate a total of $52,000$ tons of carbon sequestered in\ntrees for New York City's borough Manhattan.",
          "link": "http://arxiv.org/abs/2106.00182",
          "publishedOn": "2021-07-21T02:01:35.159Z",
          "wordCount": 587,
          "title": "Quantification of Carbon Sequestration in Urban Forests. (arXiv:2106.00182v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.",
          "link": "http://arxiv.org/abs/2103.05108",
          "publishedOn": "2021-07-21T02:01:35.152Z",
          "wordCount": 587,
          "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1\">Ansheng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kuiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Graph-based convolutional model such as non-local block has shown to be\neffective for strengthening the context modeling ability in convolutional\nneural networks (CNNs). However, its pixel-wise computational overhead is\nprohibitive which renders it unsuitable for high resolution imagery. In this\npaper, we explore the efficiency of context graph reasoning and propose a novel\nframework called Squeeze Reasoning. Instead of propagating information on the\nspatial map, we first learn to squeeze the input feature into a channel-wise\nglobal vector and perform reasoning within the single vector where the\ncomputation cost can be significantly reduced. Specifically, we build the node\ngraph in the vector where each node represents an abstract semantic concept.\nThe refined feature within the same semantic category results to be consistent,\nwhich is thus beneficial for downstream tasks. We show that our approach can be\nmodularized as an end-to-end trained block and can be easily plugged into\nexisting networks. {Despite its simplicity and being lightweight, the proposed\nstrategy allows us to establish the considerable results on different semantic\nsegmentation datasets and shows significant improvements with respect to strong\nbaselines on various other scene understanding tasks including object\ndetection, instance segmentation and panoptic segmentation.} Code is available\nat \\url{https://github.com/lxtGH/SFSegNets}.",
          "link": "http://arxiv.org/abs/2011.03308",
          "publishedOn": "2021-07-21T02:01:35.132Z",
          "wordCount": 687,
          "title": "Towards Efficient Scene Understanding via Squeeze Reasoning. (arXiv:2011.03308v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaunet_T/0/1/0/all/0/1\">Theo Jaunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1\">Corentin Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1\">Romain Vuillemot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1\">Grigory Antipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1\">Moez Baccouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "Visual Question Answering systems target answering open-ended textual\nquestions given input images. They are a testbed for learning high-level\nreasoning with a primary use in HCI, for instance assistance for the visually\nimpaired. Recent research has shown that state-of-the-art models tend to\nproduce answers exploiting biases and shortcuts in the training data, and\nsometimes do not even look at the input image, instead of performing the\nrequired reasoning steps. We present VisQA, a visual analytics tool that\nexplores this question of reasoning vs. bias exploitation. It exposes the key\nelement of state-of-the-art neural models -- attention maps in transformers.\nOur working hypothesis is that reasoning steps leading to model predictions are\nobservable from attention distributions, which are particularly useful for\nvisualization. The design process of VisQA was motivated by well-known bias\nexamples from the fields of deep learning and vision-language reasoning and\nevaluated in two ways. First, as a result of a collaboration of three fields,\nmachine learning, vision and language reasoning, and data analytics, the work\nlead to a better understanding of bias exploitation of neural models for VQA,\nwhich eventually resulted in an impact on its design and training through the\nproposition of a method for the transfer of reasoning patterns from an oracle\nmodel. Second, we also report on the design of VisQA, and a goal-oriented\nevaluation of VisQA targeting the analysis of a model decision process from\nmultiple experts, providing evidence that it makes the inner workings of models\naccessible to users.",
          "link": "http://arxiv.org/abs/2104.00926",
          "publishedOn": "2021-07-21T02:01:35.119Z",
          "wordCount": 720,
          "title": "VisQA: X-raying Vision and Language Reasoning in Transformers. (arXiv:2104.00926v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kishor Datta Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_D/0/1/0/all/0/1\">Dipankar Dasgupta</a>",
          "description": "Developing secure machine learning models from adversarial examples is\nchallenging as various methods are continually being developed to generate\nadversarial attacks. In this work, we propose an evolutionary approach to\nautomatically determine Image Processing Techniques Sequence (IPTS) for\ndetecting malicious inputs. Accordingly, we first used a diverse set of attack\nmethods including adaptive attack methods (on our defense) to generate\nadversarial samples from the clean dataset. A detection framework based on a\ngenetic algorithm (GA) is developed to find the optimal IPTS, where the\noptimality is estimated by different fitness measures such as Euclidean\ndistance, entropy loss, average histogram, local binary pattern and loss\nfunctions. The \"image difference\" between the original and processed images is\nused to extract the features, which are then fed to a classification scheme in\norder to determine whether the input sample is adversarial or clean. This paper\ndescribed our methodology and performed experiments using multiple data-sets\ntested with several adversarial attacks. For each attack-type and dataset, it\ngenerates unique IPTS. A set of IPTS selected dynamically in testing time which\nworks as a filter for the adversarial attack. Our empirical experiments\nexhibited promising results indicating the approach can efficiently be used as\nprocessing for any AI model.",
          "link": "http://arxiv.org/abs/2007.00337",
          "publishedOn": "2021-07-21T02:01:35.112Z",
          "wordCount": 675,
          "title": "Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks. (arXiv:2007.00337v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Deep neural networks often have a huge number of parameters, which posts\nchallenges in deployment in application scenarios with limited memory and\ncomputation capacity. Knowledge distillation is one approach to derive compact\nmodels from bigger ones. However, it has been observed that a converged heavy\nteacher model is strongly constrained for learning a compact student network\nand could make the optimization subject to poor local optima. In this paper, we\npropose ProKT, a new model-agnostic method by projecting the supervision\nsignals of a teacher model into the student's parameter space. Such projection\nis implemented by decomposing the training objective into local intermediate\ntargets with an approximate mirror descent technique. The proposed method could\nbe less sensitive with the quirks during optimization which could result in a\nbetter local optimum. Experiments on both image and text datasets show that our\nproposed ProKT consistently achieves superior performance compared to other\nexisting knowledge distillation methods.",
          "link": "http://arxiv.org/abs/2107.09305",
          "publishedOn": "2021-07-21T02:01:35.104Z",
          "wordCount": 597,
          "title": "Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1\">Ziwen Ke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhuo-Xu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhilang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1\">Leslie Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjie Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>",
          "description": "In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)\ndecomposition, or robust principal component analysis (PCA), has achieved\nstunning performance. However, the selection of the parameters of L+S is\nempirical, and the acceleration rate is limited, which are common failings of\niterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many\ndeep learning approaches have been proposed to address these issues, but few of\nthem use a low-rank prior. In this paper, a model-based low-rank plus sparse\nnetwork, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In\nparticular, we use an alternating linearized minimization method to solve the\noptimization problem with low-rank and sparse regularization. Learned soft\nsingular value thresholding is introduced to ensure the clear separation of the\nL component and S component. Then, the iterative steps are unrolled into a\nnetwork in which the regularization parameters are learnable. We prove that the\nproposed L+S-Net achieves global convergence under two standard assumptions.\nExperiments on retrospective and prospective cardiac cine datasets show that\nthe proposed model outperforms state-of-the-art CS and existing deep learning\nmethods and has great potential for extremely high acceleration factors (up to\n24x).",
          "link": "http://arxiv.org/abs/2010.13677",
          "publishedOn": "2021-07-21T02:01:35.085Z",
          "wordCount": 677,
          "title": "Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Deep Metric Learning (DML) aims to find representations suitable for\nzero-shot transfer to a priori unknown test distributions. However, common\nevaluation protocols only test a single, fixed data split in which train and\ntest classes are assigned randomly. More realistic evaluations should consider\na broad spectrum of distribution shifts with potentially varying degree and\ndifficulty. In this work, we systematically construct train-test splits of\nincreasing difficulty and present the ooDML benchmark to characterize\ngeneralization under out-of-distribution shifts in DML. ooDML is designed to\nprobe the generalization performance on much more challenging, diverse\ntrain-to-test distribution shifts. Based on our new benchmark, we conduct a\nthorough empirical analysis of state-of-the-art DML methods. We find that while\ngeneralization tends to consistently degrade with difficulty, some methods are\nbetter at retaining performance as the distribution shift increases. Finally,\nwe propose few-shot DML as an efficient way to consistently improve\ngeneralization in response to unknown test shifts presented in ooDML. Code\navailable here:\nhttps://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.",
          "link": "http://arxiv.org/abs/2107.09562",
          "publishedOn": "2021-07-21T02:01:35.078Z",
          "wordCount": 604,
          "title": "Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.",
          "link": "http://arxiv.org/abs/2107.01151",
          "publishedOn": "2021-07-21T02:01:35.071Z",
          "wordCount": 613,
          "title": "Collaborative Visual Navigation. (arXiv:2107.01151v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnaerens_M/0/1/0/all/0/1\">Maxim Bonnaerens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1\">Joni Dambre</a>",
          "description": "This paper proposes anchor pruning for object detection in one-stage\nanchor-based detectors. While pruning techniques are widely used to reduce the\ncomputational cost of convolutional neural networks, they tend to focus on\noptimizing the backbone networks where often most computations are. In this\nwork we demonstrate an additional pruning technique, specifically for object\ndetection: anchor pruning. With more efficient backbone networks and a growing\ntrend of deploying object detectors on embedded systems where post-processing\nsteps such as non-maximum suppression can be a bottleneck, the impact of the\nanchors used in the detection head is becoming increasingly more important. In\nthis work, we show that many anchors in the object detection head can be\nremoved without any loss in accuracy. With additional retraining, anchor\npruning can even lead to improved accuracy. Extensive experiments on SSD and MS\nCOCO show that the detection head can be made up to 44% more efficient while\nsimultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL\nVOC show the general effectiveness of our approach. We also introduce\n`overanchorized' models that can be used together with anchor pruning to\neliminate hyperparameters related to the initial shape of anchors.",
          "link": "http://arxiv.org/abs/2104.00432",
          "publishedOn": "2021-07-21T02:01:35.063Z",
          "wordCount": 645,
          "title": "Anchor Pruning for Object Detection. (arXiv:2104.00432v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zihao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zimu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>",
          "description": "Rigid registration of partial observations is a fundamental problem in\nvarious applied fields. In computer graphics, special attention has been given\nto the registration between two partial point clouds generated by scanning\ndevices. State-of-the-art registration techniques still struggle when the\noverlap region between the two point clouds is small, and completely fail if\nthere is no overlap between the scan pairs. In this paper, we present a\nlearning-based technique that alleviates this problem, and allows registration\nbetween point clouds, presented in arbitrary poses, and having little or even\nno overlap, a setting that has been referred to as tele-registration. Our\ntechnique is based on a novel neural network design that learns a prior of a\nclass of shapes and can complete a partial shape. The key idea is combining the\nregistration and completion tasks in a way that reinforces each other. In\nparticular, we simultaneously train the registration network and completion\nnetwork using two coupled flows, one that register-and-complete, and one that\ncomplete-and-register, and encourage the two flows to produce a consistent\nresult. We show that, compared with each separate flow, this two-flow training\nleads to robust and reliable tele-registration, and hence to a better point\ncloud prediction that completes the registered scans. It is also worth\nmentioning that each of the components in our neural network outperforms\nstate-of-the-art methods in both completion and registration. We further\nanalyze our network with several ablation studies and demonstrate its\nperformance on a large number of partial point clouds, both synthetic and\nreal-world, that have only small or no overlap.",
          "link": "http://arxiv.org/abs/2106.00329",
          "publishedOn": "2021-07-21T02:01:35.054Z",
          "wordCount": 736,
          "title": "Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>",
          "description": "Distortion quantification of point clouds plays a stealth, yet vital role in\na wide range of human and machine perception tasks. For human perception tasks,\na distortion quantification can substitute subjective experiments to guide 3D\nvisualization; while for machine perception tasks, a distortion quantification\ncan work as a loss function to guide the training of deep neural networks for\nunsupervised learning tasks. To handle a variety of demands in many\napplications, a distortion quantification needs to be distortion discriminable,\ndifferentiable, and have a low computational complexity. Currently, however,\nthere is a lack of a general distortion quantification that can satisfy all\nthree conditions. To fill this gap, this work proposes multiscale potential\nenergy discrepancy (MPED), a distortion quantification to measure point cloud\ngeometry and color difference. By evaluating at various neighborhood sizes, the\nproposed MPED achieves global-local tradeoffs, capturing distortion in a\nmultiscale fashion. Extensive experimental studies validate MPED's superiority\nfor both human and machine perception tasks.",
          "link": "http://arxiv.org/abs/2103.02850",
          "publishedOn": "2021-07-21T02:01:35.036Z",
          "wordCount": 647,
          "title": "Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception. (arXiv:2103.02850v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Li Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.",
          "link": "http://arxiv.org/abs/2107.09600",
          "publishedOn": "2021-07-21T02:01:35.029Z",
          "wordCount": 674,
          "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation. (arXiv:2107.09600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.",
          "link": "http://arxiv.org/abs/2107.09011",
          "publishedOn": "2021-07-21T02:01:35.018Z",
          "wordCount": 653,
          "title": "Image Fusion Transformer. (arXiv:2107.09011v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1\">V.B. Surya Prasath</a>",
          "description": "The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.",
          "link": "http://arxiv.org/abs/2107.09602",
          "publishedOn": "2021-07-21T02:01:35.002Z",
          "wordCount": 729,
          "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Junbao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>",
          "description": "Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain\nby utilizing transferable information learned from the available source domain\nand a few labeled target data. However, source data is not always accessible in\npractical scenarios, which restricts the application of SSDA in real world\ncircumstances. In this paper, we propose a novel task named Semi-supervised\nSource Hypothesis Transfer (SSHT), which performs domain adaptation based on\nsource trained model, to generalize well in target domain with a few\nsupervisions. In SSHT, we are facing two challenges: (1) The insufficient\nlabeled target data may result in target features near the decision boundary,\nwith the increased risk of mis-classification; (2) The data are usually\nimbalanced in source domain, so the model trained with these data is biased.\nThe biased model is prone to categorize samples of minority categories into\nmajority ones, resulting in low prediction diversity. To tackle the above\nissues, we propose Consistency and Diversity Learning (CDL), a simple but\neffective framework for SSHT by facilitating prediction consistency between two\nrandomly augmented unlabeled data and maintaining the prediction diversity when\nadapting model to target domain. Encouraging consistency regularization brings\ndifficulty to memorize the few labeled target data and thus enhances the\ngeneralization ability of the learned model. We further integrate Batch\nNuclear-norm Maximization into our method to enhance the discriminability and\ndiversity. Experimental results show that our method outperforms existing SSDA\nmethods and unsupervised model adaptation methods on DomainNet, Office-Home and\nOffice-31 datasets. The code is available at\nhttps://github.com/Wang-xd1899/SSHT.",
          "link": "http://arxiv.org/abs/2107.03008",
          "publishedOn": "2021-07-21T02:01:34.994Z",
          "wordCount": 717,
          "title": "Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer. (arXiv:2107.03008v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.13201",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1\">Ashwin Raju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chi-Tung Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yunakai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jinzheng Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1\">ChienHuang Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1\">Adam P Harrison</a>",
          "description": "In medical imaging, organ/pathology segmentation models trained on current\npublicly available and fully-annotated datasets usually do not well-represent\nthe heterogeneous modalities, phases, pathologies, and clinical scenarios\nencountered in real environments. On the other hand, there are tremendous\namounts of unlabelled patient imaging scans stored by many modern clinical\ncenters. In this work, we present a novel segmentation strategy,\nco-heterogenous and adaptive segmentation (CHASe), which only requires a small\nlabeled cohort of single phase imaging data to adapt to any unlabeled cohort of\nheterogenous multi-phase data with possibly new clinical scenarios and\npathologies. To do this, we propose a versatile framework that fuses appearance\nbased semi-supervision, mask based adversarial domain adaptation, and\npseudo-labeling. We also introduce co-heterogeneous training, which is a novel\nintegration of co-training and hetero modality learning. We have evaluated\nCHASe using a clinically comprehensive and challenging dataset of multi-phase\ncomputed tomography (CT) imaging studies (1147 patients and 4577 3D volumes).\nCompared to previous state-of-the-art baselines, CHASe can further improve\npathological liver mask Dice-Sorensen coefficients by ranges of $4.2\\% \\sim\n9.4\\%$, depending on the phase combinations: e.g., from $84.6\\%$ to $94.0\\%$ on\nnon-contrast CTs.",
          "link": "http://arxiv.org/abs/2005.13201",
          "publishedOn": "2021-07-21T02:01:34.976Z",
          "wordCount": 703,
          "title": "Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation. (arXiv:2005.13201v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most of methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduced a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. Moreover, to boost the\nperformance, we argue that weak augmentations matter to represent a more\nreliable relation, and leverage momentum strategy for practical efficiency.\nExperimental results show that our proposed ReSSL significantly outperforms the\nprevious state-of-the-art algorithms in terms of both performance and training\nefficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.",
          "link": "http://arxiv.org/abs/2107.09282",
          "publishedOn": "2021-07-21T02:01:34.957Z",
          "wordCount": 615,
          "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinhai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>",
          "description": "Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.",
          "link": "http://arxiv.org/abs/2007.05675",
          "publishedOn": "2021-07-21T02:01:34.947Z",
          "wordCount": 674,
          "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Figueroa_Flores_C/0/1/0/all/0/1\">Carola Figueroa-Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berga_D/0/1/0/all/0/1\">David Berga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van der Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>",
          "description": "Saliency is the perceptual capacity of our visual system to focus our\nattention (i.e. gaze) on relevant objects. Neural networks for saliency\nestimation require ground truth saliency maps for training which are usually\nachieved via eyetracking experiments. In the current paper, we demonstrate that\nsaliency maps can be generated as a side-effect of training an object\nrecognition deep neural network that is endowed with a saliency branch. Such a\nnetwork does not require any ground-truth saliency maps for training.Extensive\nexperiments carried out on both real and synthetic saliency datasets\ndemonstrate that our approach is able to generate accurate saliency maps,\nachieving competitive results on both synthetic and real datasets when compared\nto methods that do require ground truth data.",
          "link": "http://arxiv.org/abs/2107.09628",
          "publishedOn": "2021-07-21T02:01:34.925Z",
          "wordCount": 575,
          "title": "Saliency for free: Saliency prediction as a side-effect of object recognition. (arXiv:2107.09628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>",
          "description": "Table structure recognition is an essential part for making machines\nunderstand tables. Its main task is to recognize the internal structure of a\ntable. However, due to the complexity and diversity in their structure and\nstyle, it is very difficult to parse the tabular data into the structured\nformat which machines can understand easily, especially for complex tables. In\nthis paper, we introduce Split, Embed and Merge (SEM), an accurate table\nstructure recognizer. Our model takes table images as input and can correctly\nrecognize the structure of tables, whether they are simple or a complex tables.\nSEM is mainly composed of three parts, splitter, embedder and merger. In the\nfirst stage, we apply the splitter to predict the potential regions of the\ntable row (column) separators, and obtain the fine grid structure of the table.\nIn the second stage, by taking a full consideration of the textual information\nin the table, we fuse the output features for each table grid from both vision\nand language modalities. Moreover, we achieve a higher precision in our\nexperiments through adding additional semantic features. Finally, we process\nthe merging of these basic table grids in a self-regression manner. The\ncorrespondent merging results is learned through the attention mechanism. In\nour experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR\ndataset which outperforms other methods by a large margin. We also won the\nfirst place in the complex table and third place in all tables in ICDAR 2021\nCompetition on Scientific Literature Parsing, Task-B. Extensive experiments on\nother publicly available datasets demonstrate that our model achieves\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.05214",
          "publishedOn": "2021-07-21T02:01:34.907Z",
          "wordCount": 716,
          "title": "Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>",
          "description": "The task of action recognition in dark videos is useful in various scenarios,\ne.g., night surveillance and self-driving at night. Though progress has been\nmade in the action recognition task for videos in normal illumination, few have\nstudied action recognition in the dark. This is partly due to the lack of\nsufficient datasets for such a task. In this paper, we explored the task of\naction recognition in dark videos. We bridge the gap of the lack of data for\nthis task by collecting a new dataset: the Action Recognition in the Dark\n(ARID) dataset. It consists of over 3,780 video clips with 11 action\ncategories. To the best of our knowledge, it is the first dataset focused on\nhuman actions in dark videos. To gain further understandings of our ARID\ndataset, we analyze the ARID dataset in detail and exhibited its necessity over\nsynthetic dark videos. Additionally, we benchmarked the performance of several\ncurrent action recognition models on our dataset and explored potential methods\nfor increasing their performances. Our results show that current action\nrecognition models and frame enhancement methods may not be effective solutions\nfor the task of action recognition in dark videos.",
          "link": "http://arxiv.org/abs/2006.03876",
          "publishedOn": "2021-07-21T02:01:34.849Z",
          "wordCount": 697,
          "title": "ARID: A Comprehensive Study on Recognizing Actions in the Dark and A New Benchmark Dataset. (arXiv:2006.03876v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-21T02:01:34.842Z",
          "wordCount": 713,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09442",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bortsova_G/0/1/0/all/0/1\">Gerda Bortsova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bos_D/0/1/0/all/0/1\">Daniel Bos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1\">Florian Dubost</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vernooij_M/0/1/0/all/0/1\">Meike W. Vernooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikram_M/0/1/0/all/0/1\">M. Kamran Ikram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulder_G/0/1/0/all/0/1\">Gijs van Tulder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>",
          "description": "Purpose: To evaluate a fully-automated deep-learning-based method for\nassessment of intracranial carotid artery calcification (ICAC). Methods: Two\nobservers manually delineated ICAC in non-contrast CT scans of 2,319\nparticipants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study,\nprospectively collected between 2003 and 2006. These data were used to\nretrospectively develop and validate a deep-learning-based method for automated\nICAC delineation and volume measurement. To evaluate the method, we compared\nmanual and automatic assessment (computed using ten-fold cross-validation) with\nrespect to 1) the agreement with an independent observer's assessment\n(available in a random subset of 47 scans); 2) the accuracy in delineating ICAC\nas judged via blinded visual comparison by an expert; 3) the association with\nfirst stroke incidence from the scan date until 2012. All method performance\nmetrics were computed using 10-fold cross-validation. Results: The automated\ndelineation of ICAC reached sensitivity of 83.8% and positive predictive value\n(PPV) of 88%. The intraclass correlation between automatic and manual ICAC\nvolume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset).\nMeasured between the assessments of independent observers, sensitivity was\n73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95;\ncomputed in the 47-scan subset). In the blinded visual comparisons, automatic\ndelineations were more accurate than manual ones (p-value = 0.01). The\nassociation of ICAC volume with incident stroke was similarly strong for both\nautomated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured\nvolumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed\nmodel was capable of automated segmentation and volume quantification of ICAC\nwith accuracy comparable to human experts.",
          "link": "http://arxiv.org/abs/2107.09442",
          "publishedOn": "2021-07-21T02:01:34.809Z",
          "wordCount": 756,
          "title": "Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT. (arXiv:2107.09442v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1\">Kazuma Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiki_S/0/1/0/all/0/1\">Suehiro Daiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazuya_N/0/1/0/all/0/1\">Nishimura Kazuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoma_B/0/1/0/all/0/1\">Bise Ryoma</a>",
          "description": "Cell detection is an essential task in cell image analysis. Recent deep\nlearning-based detection methods have achieved very promising results. In\ngeneral, these methods require exhaustively annotating the cells in an entire\nimage. If some of the cells are not annotated (imperfect annotation), the\ndetection performance significantly degrades due to noisy labels. This often\noccurs in real collaborations with biologists and even in public data-sets. Our\nproposed method takes a pseudo labeling approach for cell detection from\nimperfect annotated data. A detection convolutional neural network (CNN)\ntrained using such missing labeled data often produces over-detection. We treat\npartially labeled cells as positive samples and the detected positions except\nfor the labeled cell as unlabeled samples. Then we select reliable pseudo\nlabels from unlabeled data using recent machine learning techniques;\npositive-and-unlabeled (PU) learning and P-classification. Experiments using\nmicroscopy images for five different conditions demonstrate the effectiveness\nof the proposed method.",
          "link": "http://arxiv.org/abs/2107.09289",
          "publishedOn": "2021-07-21T02:01:34.802Z",
          "wordCount": 599,
          "title": "Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification. (arXiv:2107.09289v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spetter_Goldstein_B/0/1/0/all/0/1\">Benjamin Spetter-Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nataniel Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>",
          "description": "The modern open internet contains billions of public images of human faces\nacross the web, especially on social media websites used by half the world's\npopulation. In this context, Face Recognition (FR) systems have the potential\nto match faces to specific names and identities, creating glaring privacy\nconcerns. Adversarial attacks are a promising way to grant users privacy from\nFR systems by disrupting their capability to recognize faces. Yet, such attacks\ncan be perceptible to human observers, especially under the more challenging\nblack-box threat model. In the literature, the justification for the\nimperceptibility of such attacks hinges on bounding metrics such as $\\ell_p$\nnorms. However, there is not much research on how these norms match up with\nhuman perception. Through examining and measuring both the effectiveness of\nrecent black-box attacks in the face recognition setting and their\ncorresponding human perceptibility through survey data, we demonstrate the\ntrade-offs in perceptibility that occur as attacks become more aggressive. We\nalso show how the $\\ell_2$ norm and other metrics do not correlate with human\nperceptibility in a linear fashion, thus making these norms suboptimal at\nmeasuring adversarial attack perceptibility.",
          "link": "http://arxiv.org/abs/2107.09126",
          "publishedOn": "2021-07-21T02:01:34.606Z",
          "wordCount": 638,
          "title": "Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition. (arXiv:2107.09126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaohao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yuqiao Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>",
          "description": "The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1),\nmaking the DNNs-based systems collapsed byadding the inconspicuous\nperturbations to the images. Most of the existing works for adversarial attack\nare gradient-based and suf-fer from the latency efficiencies and the load on\nGPU memory. Thegenerative-based adversarial attacks can get rid of this\nlimitation,and some relative works propose the approaches based on GAN.However,\nsuffering from the difficulty of the convergence of train-ing a GAN, the\nadversarial examples have either bad attack abilityor bad visual quality. In\nthis work, we find that the discriminatorcould be not necessary for\ngenerative-based adversarial attack, andpropose theSymmetric Saliency-based\nAuto-Encoder (SSAE)to generate the perturbations, which is composed of the\nsaliencymap module and the angle-norm disentanglement of the featuresmodule.\nThe advantage of our proposed method lies in that it is notdepending on\ndiscriminator, and uses the generative saliency map to pay more attention to\nlabel-relevant regions. The extensive exper-iments among the various tasks,\ndatasets, and models demonstratethat the adversarial examples generated by SSAE\nnot only make thewidely-used models collapse, but also achieves good visual\nquality.The code is available at https://github.com/BravoLu/SSAE.",
          "link": "http://arxiv.org/abs/2107.09225",
          "publishedOn": "2021-07-21T02:01:34.599Z",
          "wordCount": 627,
          "title": "Discriminator-Free Generative Adversarial Attack. (arXiv:2107.09225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:34.592Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaqub_W/0/1/0/all/0/1\">Waheeb Yaqub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1\">Manoranjan Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleiman_B/0/1/0/all/0/1\">Basem Suleiman</a>",
          "description": "Online proctoring has become a necessity in online teaching. Video-based\ncrowd-sourced online proctoring solutions are being used, where an exam-taking\nstudent's video is monitored by third parties, leading to privacy concerns. In\nthis paper, we propose a privacy-preserving online proctoring system. The\nproposed image-hashing-based system can detect the student's excessive face and\nbody movement (i.e., anomalies) that is resulted when the student tries to\ncheat in the exam. The detection can be done even if the student's face is\nblurred or masked in video frames. Experiment with an in-house dataset shows\nthe usability of the proposed system.",
          "link": "http://arxiv.org/abs/2107.09373",
          "publishedOn": "2021-07-21T02:01:34.586Z",
          "wordCount": 535,
          "title": "Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring. (arXiv:2107.09373v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Truong T. N. Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_N/0/1/0/all/0/1\">Ngoc T. T. Than</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prange_A/0/1/0/all/0/1\">Alexander Prange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>",
          "description": "This paper investigates the problem of domain adaptation for diabetic\nretinopathy (DR) grading. We learn invariant target-domain features by defining\na novel self-supervised task based on retinal vessel image reconstructions,\ninspired by medical domain knowledge. Then, a benchmark of current\nstate-of-the-art unsupervised domain adaptation methods on the DR problem is\nprovided. It can be shown that our approach outperforms existing domain\nadaption strategies. Furthermore, when utilizing entire training data in the\ntarget domain, we are able to compete with several state-of-the-art approaches\nin final classification accuracy just by applying standard network\narchitectures and using image-level labels.",
          "link": "http://arxiv.org/abs/2107.09372",
          "publishedOn": "2021-07-21T02:01:34.567Z",
          "wordCount": 547,
          "title": "Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction. (arXiv:2107.09372v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1805.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1\">Shahar Mahpod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rig Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorana_E/0/1/0/all/0/1\">Emanuele Maiorana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campisi_P/0/1/0/all/0/1\">Patrizio Campisi</a>",
          "description": "The accurate localization of facial landmarks is at the core of face analysis\ntasks, such as face recognition and facial expression analysis, to name a few.\nIn this work, we propose a novel localization approach based on a deep learning\narchitecture that utilizes cascaded subnetworks with convolutional neural\nnetwork units. The cascaded units of the first subnetwork estimate\nheatmap-based encodings of the landmarks locations, while the cascaded units of\nthe second subnetwork receive as input the output of the corresponding heatmap\nestimation units, and refine them through regression. The proposed scheme is\nexperimentally shown to compare favorably with contemporary state-of-the-art\nschemes, especially when applied to images depicting challenging localization\nconditions.",
          "link": "http://arxiv.org/abs/1805.01760",
          "publishedOn": "2021-07-21T02:01:34.559Z",
          "wordCount": 587,
          "title": "Facial Landmarks Localization using Cascaded Neural Networks. (arXiv:1805.01760v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.00184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Image-to-image translation models have shown remarkable ability on\ntransferring images among different domains. Most of existing work follows the\nsetting that the source domain and target domain keep the same at training and\ninference phases, which cannot be generalized to the scenarios for translating\nan image from an unseen domain to another unseen domain. In this work, we\npropose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem,\nwhich aims to learn a model that can translate samples from image domains that\nare not observed during training. Accordingly, we propose a framework called\nZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model\neach domain with domain-specific feature distribution that is semantically\nconsistent on vision and attribute modalities. Then the domain-invariant\nfeatures are disentangled with an shared encoder for image generation. We carry\nout extensive experiments on CUB and FLO datasets, and the results demonstrate\nthe effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows\nsignificant accuracy improvements over state-of-the-art zero-shot learning\nmethods on CUB and FLO.",
          "link": "http://arxiv.org/abs/1906.00184",
          "publishedOn": "2021-07-21T02:01:34.552Z",
          "wordCount": 641,
          "title": "ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot Image-to-Image Translation. (arXiv:1906.00184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadev_R/0/1/0/all/0/1\">Rohan Mahadev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarti_A/0/1/0/all/0/1\">Anindya Chakravarti</a>",
          "description": "Large scale image classification models trained on top of popular datasets\nsuch as Imagenet have shown to have a distributional skew which leads to\ndisparities in prediction accuracies across different subsections of population\ndemographics. A lot of approaches have been made to solve for this\ndistributional skew using methods that alter the model pre, post and during\ntraining. We investigate one such approach - which uses a multi-label softmax\nloss with cross-entropy as the loss function instead of a binary cross-entropy\non a multi-label classification problem on the Inclusive Images dataset which\nis a subset of the OpenImages V6 dataset. We use the MR2 dataset, which\ncontains images of people with self-identified gender and race attributes to\nevaluate the fairness in the model outcomes and try to interpret the mistakes\nby looking at model activations and suggest possible fixes.",
          "link": "http://arxiv.org/abs/2107.09211",
          "publishedOn": "2021-07-21T02:01:34.545Z",
          "wordCount": 572,
          "title": "Understanding Gender and Racial Disparities in Image Recognition Models. (arXiv:2107.09211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaobo Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>",
          "description": "Electronic Line Calling is an auxiliary referee system used for tennis\nmatches based on binocular vision technology. While ELC has been widely used,\nthere are still many problems, such as complex installation and maintenance,\nhigh cost and etc. We propose a monocular vision technology based ELC method.\nThe method has the following steps. First, locate the tennis ball's trajectory.\nWe propose a multistage tennis ball positioning approach combining background\nsubtraction and color area filtering. Then we propose a bouncing point\nprediction method by minimizing the fitting loss of the uncertain point.\nFinally, we find out whether the bouncing point of the ball is out of bounds or\nnot according to the relative position between the bouncing point and the court\nside line in the two dimensional image. We collected and tagged 394 samples\nwith an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing\npoints.The experimental results show that our method is feasible to judge if a\nball is out of the court with monocular vision and significantly reduce complex\ninstallation and costs of ELC system with binocular vision.",
          "link": "http://arxiv.org/abs/2107.09255",
          "publishedOn": "2021-07-21T02:01:34.539Z",
          "wordCount": 625,
          "title": "Monocular Visual Analysis for Electronic Line Calling of Tennis Games. (arXiv:2107.09255v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yim_M/0/1/0/all/0/1\">Moonbin Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Han-Cheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "For successful scene text recognition (STR) models, synthetic text image\ngenerators have alleviated the lack of annotated text images from the real\nworld. Specifically, they generate multiple text images with diverse\nbackgrounds, font styles, and text shapes and enable STR models to learn visual\npatterns that might not be accessible from manually annotated data. In this\npaper, we introduce a new synthetic text image generator, SynthTIGER, by\nanalyzing techniques used for text image synthesis and integrating effective\nones under a single algorithm. Moreover, we propose two techniques that\nalleviate the long-tail problem in length and character distributions of\ntraining data. In our experiments, SynthTIGER achieves better STR performance\nthan the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).\nOur ablation study demonstrates the benefits of using sub-components of\nSynthTIGER and the guideline on generating synthetic text images for STR\nmodels. Our implementation is publicly available at\nhttps://github.com/clovaai/synthtiger.",
          "link": "http://arxiv.org/abs/2107.09313",
          "publishedOn": "2021-07-21T02:01:34.521Z",
          "wordCount": 599,
          "title": "SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models. (arXiv:2107.09313v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Existing long-tailed recognition methods, aiming to train class-balance\nmodels from long-tailed data, generally assume the models would be evaluated on\nthe uniform test class distribution. However, the practical test class\ndistribution often violates such an assumption (e.g., being long-tailed or even\ninversely long-tailed), which would lead existing methods to fail in real-world\napplications. In this work, we study a more practical task setting, called\ntest-agnostic long-tailed recognition, where the training class distribution is\nlong-tailed while the test class distribution is unknown and can be skewed\narbitrarily. In addition to the issue of class imbalance, this task poses\nanother challenge: the class distribution shift between the training and test\nsamples is unidentified. To address this task, we propose a new method, called\nTest-time Aggregating Diverse Experts (TADE), that presents two solution\nstrategies: (1) a novel skill-diverse expert learning strategy that trains\ndiverse experts to excel at handling different test distributions from a single\nlong-tailed training distribution; (2) a novel test-time expert aggregation\nstrategy that leverages self-supervision to aggregate multiple experts for\nhandling various test distributions. Moreover, we theoretically show that our\nmethod has provable ability to simulate unknown test class distributions.\nPromising results on both vanilla and test-agnostic long-tailed recognition\nverify the effectiveness of TADE. Code is available at\nhttps://github.com/Vanint/TADE-AgnosticLT.",
          "link": "http://arxiv.org/abs/2107.09249",
          "publishedOn": "2021-07-21T02:01:34.514Z",
          "wordCount": 651,
          "title": "Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision. (arXiv:2107.09249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schirris_Y/0/1/0/all/0/1\">Yoni Schirris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nederlof_I/0/1/0/all/0/1\">Iris Nederlof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horlings_H/0/1/0/all/0/1\">Hugo Mark Horlings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>",
          "description": "We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.",
          "link": "http://arxiv.org/abs/2107.09405",
          "publishedOn": "2021-07-21T02:01:34.507Z",
          "wordCount": 654,
          "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&E whole-slide images. (arXiv:2107.09405v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "Since production-level trained deep neural networks (DNNs) are of a great\nbusiness value, protecting such DNN models against copyright infringement and\nunauthorized access is in a rising demand. However, conventional model\nprotection methods focused only the image classification task, and these\nprotection methods were never applied to semantic segmentation although it has\nan increasing number of applications. In this paper, we propose to protect\nsemantic segmentation models from unauthorized access by utilizing block-wise\ntransformation with a secret key for the first time. Protected models are\ntrained by using transformed images. Experiment results show that the proposed\nprotection method allows rightful users with the correct key to access the\nmodel to full capacity and deteriorate the performance for unauthorized users.\nHowever, protected models slightly drop the segmentation performance compared\nto non-protected models.",
          "link": "http://arxiv.org/abs/2107.09362",
          "publishedOn": "2021-07-21T02:01:34.500Z",
          "wordCount": 606,
          "title": "Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Generative Adversarial Networks (GAN) have demonstrated the potential to\nrecover realistic details for single image super-resolution (SISR). To further\nimprove the visual quality of super-resolved results, PIRM2018-SR Challenge\nemployed perceptual metrics to assess the perceptual quality, such as PI, NIQE,\nand Ma. However, existing methods cannot directly optimize these\nindifferentiable perceptual metrics, which are shown to be highly correlated\nwith human ratings. To address the problem, we propose Super-Resolution\nGenerative Adversarial Networks with Ranker (RankSRGAN) to optimize generator\nin the direction of different perceptual metrics. Specifically, we first train\na Ranker which can learn the behaviour of perceptual metrics and then introduce\na novel rank-content loss to optimize the perceptual quality. The most\nappealing part is that the proposed method can combine the strengths of\ndifferent SR methods to generate better results. Furthermore, we extend our\nmethod to multiple Rankers to provide multi-dimension constraints for the\ngenerator. Extensive experiments show that RankSRGAN achieves visually pleasing\nresults and reaches state-of-the-art performance in perceptual metrics and\nquality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN",
          "link": "http://arxiv.org/abs/2107.09427",
          "publishedOn": "2021-07-21T02:01:34.493Z",
          "wordCount": 621,
          "title": "RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank. (arXiv:2107.09427v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donghai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jiabao Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shouye Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bitao Jiang</a>",
          "description": "Collecting large-scale annotated satellite imagery datasets is essential for\ndeep-learning-based global building change surveillance. In particular, the\nscroll imaging mode of optical satellites enables larger observation ranges and\nshorter revisit periods, facilitating efficient global surveillance. However,\nthe images in recent satellite change detection datasets are mainly captured at\nnear-nadir viewing angles. In this paper, we introduce S2Looking, a building\nchange detection dataset that contains large-scale side-looking satellite\nimages captured at varying off-nadir angles. Our S2Looking dataset consists of\n5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)\nof rural areas throughout the world and more than 65,920 annotated change\ninstances. We provide two label maps to separately indicate the newly built and\ndemolished building regions for each sample in the dataset. We establish a\nbenchmark task based on this dataset, i.e., identifying the pixel-level\nbuilding changes in the bi-temporal images. We test several state-of-the-art\nmethods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.\nThe experimental results show that recent change detection methods exhibit much\npoorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking\ndataset presents three main challenges: 1) large viewing angle changes, 2)\nlarge illumination variances and 3) various complex scene characteristics\nencountered in rural areas. Our proposed dataset may promote the development of\nalgorithms for satellite image change detection and registration under\nconditions of large off-nadir angles. The dataset is available at\nhttps://github.com/AnonymousForACMMM/.",
          "link": "http://arxiv.org/abs/2107.09244",
          "publishedOn": "2021-07-21T02:01:34.485Z",
          "wordCount": 696,
          "title": "S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Face recognition remains a challenging task in unconstrained scenarios,\nespecially when faces are partially occluded. To improve the robustness against\nocclusion, augmenting the training images with artificial occlusions has been\nproved as a useful approach. However, these artificial occlusions are commonly\ngenerated by adding a black rectangle or several object templates including\nsunglasses, scarfs and phones, which cannot well simulate the realistic\nocclusions. In this paper, based on the argument that the occlusion essentially\ndamages a group of neurons, we propose a novel and elegant occlusion-simulation\nmethod via dropping the activations of a group of neurons in some elaborately\nselected channel. Specifically, we first employ a spatial regularization to\nencourage each feature channel to respond to local and different face regions.\nIn this way, the activations affected by an occlusion in a local region are\nmore likely to be located in a single feature channel. Then, the locality-aware\nchannel-wise dropout (LCD) is designed to simulate the occlusion by dropping\nout the entire feature channel. Furthermore, by randomly dropping out several\nfeature channels, our method can well simulate the occlusion of larger area.\nThe proposed LCD can encourage its succeeding layers to minimize the\nintra-class feature variance caused by occlusions, thus leading to improved\nrobustness against occlusion. In addition, we design an auxiliary spatial\nattention module by learning a channel-wise attention vector to reweight the\nfeature channels, which improves the contributions of non-occluded regions.\nExtensive experiments on various benchmarks show that the proposed method\noutperforms state-of-the-art methods with a remarkable improvement.",
          "link": "http://arxiv.org/abs/2107.09270",
          "publishedOn": "2021-07-21T02:01:34.462Z",
          "wordCount": 688,
          "title": "Locality-aware Channel-wise Dropout for Occluded Face Recognition. (arXiv:2107.09270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:34.453Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-07-21T02:01:34.445Z",
          "wordCount": 691,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byrnes_O/0/1/0/all/0/1\">Olivia Byrnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_W/0/1/0/all/0/1\">Wendy La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>",
          "description": "Data hiding is the process of embedding information into a noise-tolerant\nsignal such as a piece of audio, video, or image. Digital watermarking is a\nform of data hiding where identifying data is robustly embedded so that it can\nresist tampering and be used to identify the original owners of the media.\nSteganography, another form of data hiding, embeds data for the purpose of\nsecure and secret communication. This survey summarises recent developments in\ndeep learning techniques for data hiding for the purposes of watermarking and\nsteganography, categorising them based on model architectures and noise\ninjection methods. The objective functions, evaluation metrics, and datasets\nused for training these data hiding models are comprehensively summarised.\nFinally, we propose and discuss possible future directions for research into\ndeep data hiding techniques.",
          "link": "http://arxiv.org/abs/2107.09287",
          "publishedOn": "2021-07-21T02:01:34.418Z",
          "wordCount": 577,
          "title": "Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography. (arXiv:2107.09287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount\nof interest in the past few decades, while one of the most critical operations\nin these systems is the perception of the environment. Deep learning and,\nespecially, the use of Deep Neural Networks (DNNs) provides impressive results\nin analyzing and understanding complex and dynamic scenes from visual data. The\nprediction horizons for those perception systems are very short and inference\nmust often be performed in real time, stressing the need of transforming the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Our goal in this work is to\ninvestigate best practices for appropriately applying novel weight sharing\ntechniques, optimizing the available variables and the training procedures\ntowards the significant acceleration of widely adopted DNNs. Extensive\nevaluation studies carried out using various state-of-the-art DNN models in\nobject detection and tracking experiments, provide details about the type of\nerrors that manifest after the application of weight sharing techniques,\nresulting in significant acceleration gains with negligible accuracy losses.",
          "link": "http://arxiv.org/abs/2107.09101",
          "publishedOn": "2021-07-21T02:01:34.411Z",
          "wordCount": 620,
          "title": "Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1\">Oula Puonti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thielscher_A/0/1/0/all/0/1\">Axel Thielscher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>",
          "description": "Despite advances in data augmentation and transfer learning, convolutional\nneural networks (CNNs) have difficulties generalising to unseen target domains.\nWhen applied to segmentation of brain MRI scans, CNNs are highly sensitive to\nchanges in resolution and contrast: even within the same MR modality, decreases\nin performance can be observed across datasets. We introduce SynthSeg, the\nfirst segmentation CNN agnostic to brain MRI scans of any contrast and\nresolution. SynthSeg is trained with synthetic data sampled from a generative\nmodel inspired by Bayesian segmentation. Crucially, we adopt a \\textit{domain\nrandomisation} strategy where we fully randomise the generation parameters to\nmaximise the variability of the training data. Consequently, SynthSeg can\nsegment preprocessed and unpreprocessed real scans of any target domain,\nwithout retraining or fine-tuning. Because SynthSeg only requires segmentations\nto be trained (no images), it can learn from label maps obtained automatically\nfrom existing datasets of different populations (e.g., with atrophy and\nlesions), thus achieving robustness to a wide range of morphological\nvariability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10\nresolutions, where it exhibits unparalleled generalisation compared to\nsupervised CNNs, test time adaptation, and Bayesian segmentation. The code and\ntrained model are available at https://github.com/BBillot/SynthSeg.",
          "link": "http://arxiv.org/abs/2107.09559",
          "publishedOn": "2021-07-21T02:01:34.386Z",
          "wordCount": 676,
          "title": "SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of any Contrast and Resolution. (arXiv:2107.09559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1\">Vincent Wilmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sauraj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1\">Tabea Redl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1\">H&#xe5;kon Sandaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>",
          "description": "Anomaly detection in images plays a significant role for many applications\nacross all industries, such as disease diagnosis in healthcare or quality\nassurance in manufacturing. Manual inspection of images, when extended over a\nmonotonously repetitive period of time is very time consuming and can lead to\nanomalies being overlooked.Artificial neural networks have proven themselves\nvery successful on simple, repetitive tasks, in some cases even outperforming\nhumans. Therefore, in this paper we investigate different methods of deep\nlearning, including supervised and unsupervised learning, for anomaly detection\napplied to a quality assurance use case. We utilize the MVTec anomaly dataset\nand develop three different models, a CNN for supervised anomaly detection,\nKD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly\ndetection and a DCGAN for generating reconstructed images. By experiments, we\nfound that KD-CAE performs better on the anomaly datasets compared to CNN and\nNI-CAE, with NI-CAE performing the best on the Transistor dataset. We also\nimplemented a DCGAN for the creation of new training data but due to\ncomputational limitation and lack of extrapolating the mechanics of AnoGAN, we\nrestricted ourselves just to the generation of GAN based images. We conclude\nthat unsupervised methods are more powerful for anomaly detection in images,\nespecially in a setting where only a small amount of anomalous data is\navailable, or the data is unlabeled.",
          "link": "http://arxiv.org/abs/2107.09204",
          "publishedOn": "2021-07-21T02:01:34.379Z",
          "wordCount": 686,
          "title": "A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zatsarynna_O/0/1/0/all/0/1\">Olga Zatsarynna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "Anticipating human actions is an important task that needs to be addressed\nfor the development of reliable intelligent agents, such as self-driving cars\nor robot assistants. While the ability to make future predictions with high\naccuracy is crucial for designing the anticipation approaches, the speed at\nwhich the inference is performed is not less important. Methods that are\naccurate but not sufficiently fast would introduce a high latency into the\ndecision process. Thus, this will increase the reaction time of the underlying\nsystem. This poses a problem for domains such as autonomous driving, where the\nreaction time is crucial. In this work, we propose a simple and effective\nmulti-modal architecture based on temporal convolutions. Our approach stacks a\nhierarchy of temporal convolutional layers and does not rely on recurrent\nlayers to ensure a fast prediction. We further introduce a multi-modal fusion\nmechanism that captures the pairwise interactions between RGB, flow, and object\nmodalities. Results on two large-scale datasets of egocentric videos,\nEPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves\ncomparable performance to the state-of-the-art approaches while being\nsignificantly faster.",
          "link": "http://arxiv.org/abs/2107.09504",
          "publishedOn": "2021-07-21T02:01:34.357Z",
          "wordCount": 622,
          "title": "Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos. (arXiv:2107.09504v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suzhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>",
          "description": "We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.09293",
          "publishedOn": "2021-07-21T02:01:34.351Z",
          "wordCount": 663,
          "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion. (arXiv:2107.09293v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.",
          "link": "http://arxiv.org/abs/2107.09242",
          "publishedOn": "2021-07-21T02:01:34.331Z",
          "wordCount": 578,
          "title": "Boosting few-shot classification with view-learnable contrastive learning. (arXiv:2107.09242v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1\">Andrew Melnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harter_A/0/1/0/all/0/1\">Augustin Harter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limberg_C/0/1/0/all/0/1\">Christian Limberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1\">Krishan Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenderhauf_N/0/1/0/all/0/1\">Niko Suenderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1\">Helge Ritter</a>",
          "description": "This work discusses a learning approach to mask rewarding objects in images\nusing sparse reward signals from an imitation learning dataset. For that, we\ntrain an Hourglass network using only feedback from a critic model. The\nHourglass network learns to produce a mask to decrease the critic's score of a\nhigh score image and increase the critic's score of a low score image by\nswapping the masked areas between these two images. We trained the model on an\nimitation learning dataset from the NeurIPS 2020 MineRL Competition Track,\nwhere our model learned to mask rewarding objects in a complex interactive 3D\nenvironment with a sparse reward signal. This approach was part of the 1st\nplace winning solution in this competition. Video demonstration and code:\nhttps://rebrand.ly/critic-guided-segmentation",
          "link": "http://arxiv.org/abs/2107.09540",
          "publishedOn": "2021-07-21T02:01:34.324Z",
          "wordCount": 572,
          "title": "Critic Guided Segmentation of Rewarding Objects in First-Person Views. (arXiv:2107.09540v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>",
          "description": "Near infrared (NIR) imaging has been widely applied in low-light imaging\nscenarios; however, it is difficult for human and algorithms to perceive the\nreal scene in the colorless NIR domain. While Generative Adversarial Network\n(GAN) has been widely employed in various image colorization tasks, it is\nchallenging for a direct mapping mechanism, such as a conventional GAN, to\ntransform an image from the NIR to the RGB domain with correct semantic\nreasoning, well-preserved textures, and vivid color combinations concurrently.\nIn this work, we propose a novel Attention-based NIR image colorization\nframework via Adaptive Fusion of Semantic and Texture clues, aiming at\nachieving these goals within the same framework. The tasks of texture transfer\nand semantic reasoning are carried out in two separate network blocks.\nSpecifically, the Texture Transfer Block (TTB) aims at extracting texture\nfeatures from the NIR image's Laplacian component and transferring them for\nsubsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic\nclues and maps the NIR pixel values to the RGB domain. Finally, a Fusion\nAttention Block (FAB) is proposed to adaptively fuse the features from the two\nbranches and generate an optimized colorization result. In order to enhance the\nnetwork's learning capacity in semantic reasoning as well as mapping precision\nin texture transfer, we have proposed the Residual Coordinate Attention Block\n(RCAB), which incorporates coordinate attention into a residual learning\nframework, enabling the network to capture long-range dependencies along the\nchannel direction and meanwhile precise positional information can be preserved\nalong spatial directions. RCAB is also incorporated into FAB to facilitate\naccurate texture alignment during fusion. Both quantitative and qualitative\nevaluations show that the proposed method outperforms state-of-the-art NIR\nimage colorization methods.",
          "link": "http://arxiv.org/abs/2107.09237",
          "publishedOn": "2021-07-21T02:01:34.317Z",
          "wordCount": 721,
          "title": "Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1\">Juan Pablo de Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>",
          "description": "Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.",
          "link": "http://arxiv.org/abs/2107.09170",
          "publishedOn": "2021-07-21T02:01:34.291Z",
          "wordCount": 624,
          "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09179",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bidgoli_N/0/1/0/all/0/1\">Navid Mahmoudian Bidgoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azevedo_R/0/1/0/all/0/1\">Roberto G. de A. Azevedo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maugey_T/0/1/0/all/0/1\">Thomas Maugey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roumy_A/0/1/0/all/0/1\">Aline Roumy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "State-of-the-art 2D image compression schemes rely on the power of\nconvolutional neural networks (CNNs). Although CNNs offer promising\nperspectives for 2D image compression, extending such models to omnidirectional\nimages is not straightforward. First, omnidirectional images have specific\nspatial and statistical properties that can not be fully captured by current\nCNN models. Second, basic mathematical operations composing a CNN architecture,\ne.g., translation and sampling, are not well-defined on the sphere. In this\npaper, we study the learning of representation models for omnidirectional\nimages and propose to use the properties of HEALPix uniform sampling of the\nsphere to redefine the mathematical tools used in deep learning models for\nomnidirectional images. In particular, we: i) propose the definition of a new\nconvolution operation on the sphere that keeps the high expressiveness and the\nlow complexity of a classical 2D convolution; ii) adapt standard CNN techniques\nsuch as stride, iterative aggregation, and pixel shuffling to the spherical\ndomain; and then iii) apply our new framework to the task of omnidirectional\nimage compression. Our experiments show that our proposed on-the-sphere\nsolution leads to a better compression gain that can save 13.7% of the bit rate\ncompared to similar learned models applied to equirectangular images. Also,\ncompared to learning models based on graph convolutional networks, our solution\nsupports more expressive filters that can preserve high frequencies and provide\na better perceptual quality of the compressed images. Such results demonstrate\nthe efficiency of the proposed framework, which opens new research venues for\nother omnidirectional vision tasks to be effectively implemented on the sphere\nmanifold.",
          "link": "http://arxiv.org/abs/2107.09179",
          "publishedOn": "2021-07-21T02:01:34.082Z",
          "wordCount": 719,
          "title": "OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression. (arXiv:2107.09179v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09136",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dick_J/0/1/0/all/0/1\">Jo&#xe3;o Dick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abreu_B/0/1/0/all/0/1\">Brunno Abreu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grellert_M/0/1/0/all/0/1\">Mateus Grellert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bampi_S/0/1/0/all/0/1\">Sergio Bampi</a>",
          "description": "This work presents an analysis of state-of-the-art learning-based image\ncompression techniques. We compare 8 models available in the Tensorflow\nCompression package in terms of visual quality metrics and processing time,\nusing the KODAK data set. The results are compared with the Better Portable\nGraphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the\nlowest execution times compared with the fastest learning-based model, with a\nspeedup of 1.46x in compression and 30x in decompression. However, the\nlearning-based models achieved improvements over JPEG2000 in terms of quality,\nspecially for lower bitrates. Our findings also show that BPG is more efficient\nin terms of PSNR, but the learning models are better for other quality metrics,\nand sometimes even faster. The results indicate that learning-based techniques\nare promising solutions towards a future mainstream compression method.",
          "link": "http://arxiv.org/abs/2107.09136",
          "publishedOn": "2021-07-21T02:01:33.936Z",
          "wordCount": 584,
          "title": "Quality and Complexity Assessment of Learning-Based Image Compression Solutions. (arXiv:2107.09136v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:33.922Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09134",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1\">Daniel Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graves_C/0/1/0/all/0/1\">Catharine Graves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1\">Marco Gutierrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brandoli_B/0/1/0/all/0/1\">Bruno Brandoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jose_J/0/1/0/all/0/1\">Jose Rodrigues-Jr</a>",
          "description": "Magnetic resonance imaging (MRI) is a widely known medical imaging technique\nused to assess the heart function. Deep learning (DL) models perform several\ntasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,\nestimation, and detection of diseases. Many DL models based on convolutional\nneural networks (CNN) were improved by detecting regions-of-interest (ROI)\neither automatically or by hand. In this paper we describe Visual-Motion-Focus\n(VMF), a module that detects the heart motion in the 4D MRI sequence, and\nhighlights ROIs by focusing a Radial Basis Function (RBF) on the estimated\nmotion field. We experimented and evaluated VMF on three CMR datasets,\nobserving that the proposed ROIs cover 99.7% of data labels (Recall score),\nimproved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI\nextraction, and improved the overall training speed by 2.5 times (+150%).",
          "link": "http://arxiv.org/abs/2107.09134",
          "publishedOn": "2021-07-21T02:01:33.906Z",
          "wordCount": 609,
          "title": "Convolutional module for heart localization and segmentation in MRI. (arXiv:2107.09134v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09060",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiazhen Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haikun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gastao Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1\">Christopher Gilliam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1\">Thierry Blu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1\">Ren&#xe9; Botnar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>",
          "description": "Physiological motion, such as cardiac and respiratory motion, during Magnetic\nResonance (MR) image acquisition can cause image artifacts. Motion correction\ntechniques have been proposed to compensate for these types of motion during\nthoracic scans, relying on accurate motion estimation from undersampled\nmotion-resolved reconstruction. A particular interest and challenge lie in the\nderivation of reliable non-rigid motion fields from the undersampled\nmotion-resolved data. Motion estimation is usually formulated in image space\nvia diffusion, parametric-spline, or optical flow methods. However, image-based\nregistration can be impaired by remaining aliasing artifacts due to the\nundersampled motion-resolved reconstruction. In this work, we describe a\nformalism to perform non-rigid registration directly in the sampled Fourier\nspace, i.e. k-space. We propose a deep-learning based approach to perform fast\nand accurate non-rigid registration from the undersampled k-space data. The\nbasic working principle originates from the Local All-Pass (LAP) technique, a\nrecently introduced optical flow-based registration. The proposed LAPNet is\ncompared against traditional and deep learning image-based registrations and\ntested on fully-sampled and highly-accelerated (with two undersampling\nstrategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients\nwith suspected liver or lung metastases and 25 healthy subjects. The proposed\nLAPNet provided consistent and superior performance to image-based approaches\nthroughout different sampling trajectories and acceleration factors.",
          "link": "http://arxiv.org/abs/2107.09060",
          "publishedOn": "2021-07-21T02:01:33.895Z",
          "wordCount": 677,
          "title": "LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09118",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1\">Donya Khaledyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1\">Reza Sarkhosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Deep learning (DL) models have received particular attention in medical\nimaging due to their promising pattern recognition capabilities. However, Deep\nNeural Networks (DNNs) require a huge amount of data, and because of the lack\nof sufficient data in this field, transfer learning can be a great solution.\nDNNs used for disease diagnosis meticulously concentrate on improving the\naccuracy of predictions without providing a figure about their confidence of\npredictions. Knowing how much a DNN model is confident in a computer-aided\ndiagnosis model is necessary for gaining clinicians' confidence and trust in\nDL-based solutions. To address this issue, this work presents three different\nmethods for quantifying uncertainties for skin cancer detection from images. It\nalso comprehensively evaluates and compares performance of these DNNs using\nnovel uncertainty-related metrics. The obtained results reveal that the\npredictive uncertainty estimation methods are capable of flagging risky and\nerroneous predictions with a high uncertainty estimate. We also demonstrate\nthat ensemble approaches are more reliable in capturing uncertainties through\ninference.",
          "link": "http://arxiv.org/abs/2107.09118",
          "publishedOn": "2021-07-21T02:01:33.852Z",
          "wordCount": 625,
          "title": "Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Manu Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1\">Emmanuel Baltsavias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Fusing satellite imagery acquired with different sensors has been a\nlong-standing challenge of Earth observation, particularly across different\nmodalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we\nexplore the joint analysis of imagery from different sensors in the light of\nrepresentation learning: we propose to learn a joint, sensor-invariant\nembedding (feature representation) within a deep neural network. Our\napplication problem is the monitoring of lake ice on Alpine lakes. To reach the\ntemporal resolution requirement of the Swiss Global Climate Observing System\n(GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra\nMODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains\nand between the sensor resolutions make this a challenging instance of the\nsensor fusion problem. Our approach can be classified as a feature-level fusion\nthat is learnt in a data-driven manner. The proposed network architecture has\nseparate encoding branches for each image sensor, which feed into a single\nlatent embedding. I.e., a common feature representation shared by all inputs,\nsuch that subsequent processing steps deliver comparable output irrespective of\nwhich sort of input image was used. By fusing satellite data, we map lake ice\nat a temporal resolution of <1.5 days. The network produces spatially explicit\nlake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores\n>60.7%) and generalises well across different lakes and winters. Moreover, it\nsets a new state-of-the-art for determining the important ice-on and ice-off\ndates for the target lakes, in many cases meeting the GCOS requirement.",
          "link": "http://arxiv.org/abs/2107.09092",
          "publishedOn": "2021-07-21T02:01:33.823Z",
          "wordCount": 703,
          "title": "Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring. (arXiv:2107.09092v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:48.562Z",
          "wordCount": 607,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "In this paper, we propose a novel training strategy for convolutional neural\nnetwork(CNN) named Feature Mining, that aims to strengthen the network's\nlearning of the local feature. Through experiments, we find that semantic\ncontained in different parts of the feature is different, while the network\nwill inevitably lose the local information during feedforward propagation. In\norder to enhance the learning of local feature, Feature Mining divides the\ncomplete feature into two complementary parts and reuse these divided feature\nto make the network learn more local information, we call the two steps as\nfeature segmentation and feature reusing. Feature Mining is a parameter-free\nmethod and has plug-and-play nature, and can be applied to any CNN models.\nExtensive experiments demonstrate the wide applicability, versatility, and\ncompatibility of our method.",
          "link": "http://arxiv.org/abs/2107.08421",
          "publishedOn": "2021-07-20T02:04:48.428Z",
          "wordCount": 571,
          "title": "Feature Mining: A Novel Training Strategy for Convolutional Neural Network. (arXiv:2107.08421v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1\">Xing Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep convolutional neural networks (DCNNs) have shown\nimpressive performance improvements on thermal to visible face synthesis and\nmatching problems. However, current DCNN-based synthesis models do not perform\nwell on thermal faces with large pose variations. In order to deal with this\nproblem, heterogeneous face frontalization methods are needed in which a model\ntakes a thermal profile face image and generates a frontal visible face. This\nis an extremely difficult problem due to the large domain as well as large pose\ndiscrepancies between the two modalities. Despite its applications in\nbiometrics and surveillance, this problem is relatively unexplored in the\nliterature. We propose a domain agnostic learning-based generative adversarial\nnetwork (DAL-GAN) which can synthesize frontal views in the visible domain from\nthermal faces with pose variations. DAL-GAN consists of a generator with an\nauxiliary classifier and two discriminators which capture both local and global\ntexture discriminations for better synthesis. A contrastive constraint is\nenforced in the latent space of the generator with the help of a dual-path\ntraining strategy, which improves the feature vector discrimination. Finally, a\nmulti-purpose loss function is utilized to guide the network in synthesizing\nidentity preserving cross-domain frontalization. Extensive experimental results\ndemonstrate that DAL-GAN can generate better quality frontal views compared to\nthe other baseline methods.",
          "link": "http://arxiv.org/abs/2107.08311",
          "publishedOn": "2021-07-20T02:04:48.375Z",
          "wordCount": 661,
          "title": "Heterogeneous Face Frontalization via Domain Agnostic Learning. (arXiv:2107.08311v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Ruben Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.",
          "link": "http://arxiv.org/abs/2008.08899",
          "publishedOn": "2021-07-20T02:04:47.750Z",
          "wordCount": 597,
          "title": "Document Visual Question Answering Challenge 2020. (arXiv:2008.08899v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ali_A/0/1/0/all/0/1\">Ali Al-Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imene_B/0/1/0/all/0/1\">Bakour Imene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesma_B/0/1/0/all/0/1\">Bouchali Hadia Nesma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenzhen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castiello_C/0/1/0/all/0/1\">Ciro Castiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencar_C/0/1/0/all/0/1\">Corrado Mencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Florian Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1\">Gennaro Vessio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1\">Giovanna Castellano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jieru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abualsaud_K/0/1/0/all/0/1\">Khalid Abualsaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laihui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cianciotta_M/0/1/0/all/0/1\">Marco Cianciotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almaadeed_N/0/1/0/all/0/1\">Noor Almaadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elharrouss_O/0/1/0/all/0/1\">Omar Elharrouss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_P/0/1/0/all/0/1\">Pei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1\">Somaya Al-Maadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sultan Daud Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_T/0/1/0/all/0/1\">Tamer Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golda_T/0/1/0/all/0/1\">Thomas Golda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingnan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuehan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhijian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhiyuan Zhao</a>",
          "description": "Crowd counting on the drone platform is an interesting topic in computer\nvision, which brings new challenges such as small object inference, background\nclutter and wide viewpoint. However, there are few algorithms focusing on crowd\ncounting on the drone-captured data due to the lack of comprehensive datasets.\nTo this end, we collect a large-scale dataset and organize the Vision Meets\nDrone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th\nEuropean Conference on Computer Vision (ECCV 2020) to promote the developments\nin the related fields. The collected dataset is formed by $3,360$ images,\nincluding $2,460$ images for training, and $900$ images for testing.\nSpecifically, we manually annotate persons with points in each video frame.\nThere are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020\nChallenge. We provide a detailed analysis of the evaluation results and\nconclude the challenge. More information can be found at the website:\n\\url{this http URL}.",
          "link": "http://arxiv.org/abs/2107.08766",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results. (arXiv:2107.08766v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Han-Bin Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Deep learning has played a major role in the interpretation of dermoscopic\nimages for detecting skin defects and abnormalities. However, current deep\nlearning solutions for dermatological lesion analysis are typically limited in\nproviding probabilistic predictions which highlights the importance of\nconcerning uncertainties. This concept of uncertainty can provide a confidence\nlevel for each feature which prevents overconfident predictions with poor\ngeneralization on unseen data. In this paper, we propose an overall framework\nthat jointly considers dermatological classification and uncertainty estimation\ntogether. The estimated confidence of each feature to avoid uncertain feature\nand undesirable shift, which are caused by environmental difference of input\nimage, in the latent space is pooled from confidence network. Our qualitative\nresults show that modeling uncertainties not only helps to quantify model\nconfidence for each prediction but also helps classification layers to focus on\nconfident features, therefore, improving the accuracy for dermatological lesion\nclassification. We demonstrate the potential of the proposed approach in two\nstate-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).",
          "link": "http://arxiv.org/abs/2107.08770",
          "publishedOn": "2021-07-20T02:04:47.020Z",
          "wordCount": null,
          "title": "Joint Dermatological Lesion Classification and Confidence Modeling with Uncertainty Estimation. (arXiv:2107.08770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thuy_H/0/1/0/all/0/1\">Hang Duong Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1\">Phi Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1\">Long Tran Quoc</a>",
          "description": "Nowadays, cardiac diagnosis largely depends on left ventricular function\nassessment. With the help of the segmentation deep learning model, the\nassessment of the left ventricle becomes more accessible and accurate. However,\ndeep learning technique still faces two main obstacles: the difficulty in\nacquiring sufficient training data and time-consuming in developing quality\nmodels. In the ordinary data acquisition process, the dataset was selected\nrandomly from a large pool of unlabeled images for labeling, leading to massive\nlabor time to annotate those images. Besides that, hand-designed model\ndevelopment is laborious and also costly. This paper introduces a pipeline that\nrelies on Active Learning to ease the labeling work and utilizes Neural\nArchitecture Search's idea to design the adequate deep learning model\nautomatically. We called this Fully automated machine learning pipeline for\nechocardiogram segmentation. The experiment results show that our method\nobtained the same IOU accuracy with only two-fifths of the original training\ndataset, and the searched model got the same accuracy as the hand-designed\nmodel given the same training dataset.",
          "link": "http://arxiv.org/abs/2107.08440",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yueyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>",
          "description": "Recently, most siamese network based trackers locate targets via object\nclassification and bounding-box regression. Generally, they select the\nbounding-box with maximum classification confidence as the final prediction.\nThis strategy may miss the right result due to the accuracy misalignment\nbetween classification and regression. In this paper, we propose a novel\nsiamese tracking algorithm called SiamRCR, addressing this problem with a\nsimple, light and effective solution. It builds reciprocal links between\nclassification and regression branches, which can dynamically re-weight their\nlosses for each positive sample. In addition, we add a localization branch to\npredict the localization accuracy, so that it can work as the replacement of\nthe regression assistance link during inference. This branch makes the training\nand inference more consistent. Extensive experimental results demonstrate the\neffectiveness of SiamRCR and its superiority over the state-of-the-art\ncompetitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.\nMoreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.",
          "link": "http://arxiv.org/abs/2105.11237",
          "publishedOn": "2021-07-20T02:04:46.771Z",
          "wordCount": null,
          "title": "SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1\">Emilian Radoi</a>",
          "description": "The use of gait for person identification has important advantages such as\nbeing non-invasive, unobtrusive, not requiring cooperation and being less\nlikely to be obscured compared to other biometrics. Existing methods for gait\nrecognition require cooperative gait scenarios, in which a single person is\nwalking multiple times in a straight line in front of a camera. We aim to\naddress the hard challenges of real-world scenarios in which camera feeds\ncapture multiple people, who in most cases pass in front of the camera only\nonce. We address privacy concerns by using only the motion information of\nwalking individuals, with no identifiable appearance-based information. As\nsuch, we propose a novel weakly supervised learning framework, WildGait, which\nconsists of training a Spatio-Temporal Graph Convolutional Network on a large\nnumber of automatically annotated skeleton sequences obtained from raw,\nreal-world, surveillance streams to learn useful gait signatures. Our results\nshow that, with fine-tuning, we surpass in terms of recognition accuracy the\ncurrent state-of-the-art pose-based gait recognition solutions. Our proposed\nmethod is reliable in training gait recognition methods in unconstrained\nenvironments, especially in settings with scarce amounts of annotated data.",
          "link": "http://arxiv.org/abs/2105.05528",
          "publishedOn": "2021-07-20T02:04:46.705Z",
          "wordCount": null,
          "title": "WildGait: Learning Gait Representations from Raw Surveillance Streams. (arXiv:2105.05528v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.00970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossrieder_J/0/1/0/all/0/1\">Jan Grossrieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "Recent machine learning strategies for segmentation tasks have shown great\nability when trained on large pixel-wise annotated image datasets. It remains a\nmajor challenge however to aggregate such datasets, as the time and monetary\ncost associated with collecting extensive annotations is extremely high. This\nis particularly the case for generating precise pixel-wise annotations in video\nand volumetric image data. To this end, this work presents a novel framework to\nproduce pixel-wise segmentations using minimal supervision. Our method relies\non 2D point supervision, whereby a single 2D location within an object of\ninterest is provided on each image of the data. Our method then estimates the\nobject appearance in a semi-supervised fashion by learning\nobject-image-specific features and by using these in a semi-supervised learning\nframework. Our object model is then used in a graph-based optimization problem\nthat takes into account all provided locations and the image data in order to\ninfer the complete pixel-wise segmentation. In practice, we solve this\noptimally as a tracking problem using a K-shortest path approach. Both the\nobject model and segmentation are then refined iteratively to further improve\nthe final segmentation. We show that by collecting 2D locations using a gaze\ntracker, our approach can provide state-of-the-art segmentations on a range of\nobjects and image modalities (video and 3D volumes), and that these can then be\nused to train supervised machine learning classifiers.",
          "link": "http://arxiv.org/abs/1809.00970",
          "publishedOn": "2021-07-20T02:04:46.694Z",
          "wordCount": null,
          "title": "Iterative multi-path tracking for video and volume segmentation with sparse point supervision. (arXiv:1809.00970v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:46.693Z",
          "wordCount": null,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuehua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>",
          "description": "With the identity information in face data more closely related to personal\ncredit and property security, people pay increasing attention to the protection\nof face data privacy. In different tasks, people have various requirements for\nface de-identification (De-ID), so we propose a systematical solution\ncompatible for these De-ID operations. Firstly, an attribute disentanglement\nand generative network is constructed to encode two parts of the face, which\nare the identity (facial features like mouth, nose and eyes) and expression\n(including expression, pose and illumination). Through face swapping, we can\nremove the original ID completely. Secondly, we add an adversarial vector\nmapping network to perturb the latent code of the face image, different from\nprevious traditional adversarial methods. Through this, we can construct\nunrestricted adversarial image to decrease ID similarity recognized by model.\nOur method can flexibly de-identify the face data in various ways and the\nprocessed images have high image quality.",
          "link": "http://arxiv.org/abs/2107.08581",
          "publishedOn": "2021-07-20T02:04:46.688Z",
          "wordCount": null,
          "title": "A Systematical Solution for Face De-identification. (arXiv:2107.08581v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12434",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiahang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">Victor M. Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+RaviKumar_N/0/1/0/all/0/1\">Nishant RaviKumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yashu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1\">Gongning Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jingkun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1\">Buntheng Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sermesant_M/0/1/0/all/0/1\">Maxime Sermesant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiexiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.",
          "link": "http://arxiv.org/abs/2006.12434",
          "publishedOn": "2021-07-20T02:04:46.616Z",
          "wordCount": null,
          "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge. (arXiv:2006.12434v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "Cell detection is the task of detecting the approximate positions of cell\ncentroids from microscopy images. Recently, convolutional neural network-based\napproaches have achieved promising performance. However, these methods require\na certain amount of annotation for each imaging condition. This annotation is a\ntime-consuming and labor-intensive task. To overcome this problem, we propose a\nsemi-supervised cell-detection method that effectively uses a time-lapse\nsequence with one labeled image and the other images unlabeled. First, we train\na cell-detection network with a one-labeled image and estimate the unlabeled\nimages with the trained network. We then select high-confidence positions from\nthe estimations by tracking the detected cells from the labeled frame to those\nfar from it. Next, we generate pseudo-labels from the tracking results and\ntrain the network by using pseudo-labels. We evaluated our method for seven\nconditions of public datasets, and we achieved the best results relative to\nother semi-supervised methods. Our code is available at\nhttps://github.com/naivete5656/SCDTC",
          "link": "http://arxiv.org/abs/2107.08639",
          "publishedOn": "2021-07-20T02:04:46.615Z",
          "wordCount": null,
          "title": "Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency. (arXiv:2107.08639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>",
          "description": "Video crowd localization is a crucial yet challenging task, which aims to\nestimate exact locations of human heads in the given crowded videos. To model\nspatial-temporal dependencies of human mobility, we propose a multi-focus\nGaussian neighbor attention (GNA), which can effectively exploit long-range\ncorrespondences while maintaining the spatial topological structure of the\ninput videos. In particular, our GNA can also capture the scale variation of\nhuman heads well using the equipped multi-focus mechanism. Based on the\nmulti-focus GNA, we develop a unified neural network called GNANet to\naccurately locate head centers in video clips by fully aggregating\nspatial-temporal information via a scene modeling module and a context\ncross-attention module. Moreover, to facilitate future researches in this\nfield, we introduce a large-scale crowded video benchmark named SenseCrowd,\nwhich consists of 60K+ frames captured in various surveillance scenarios and\n2M+ head annotations. Finally, we conduct extensive experiments on three\ndatasets including our SenseCrowd, and the experiment results show that the\nproposed method is capable to achieve state-of-the-art performance for both\nvideo crowd localization and counting. The code and the dataset will be\nreleased.",
          "link": "http://arxiv.org/abs/2107.08645",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "Video Crowd Localization with Multi-focus Gaussian Neighbor Attention and a Large-Scale Benchmark. (arXiv:2107.08645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.10141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1\">Rami Ben-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpigel_M/0/1/0/all/0/1\">Mor Shpigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azulai_O/0/1/0/all/0/1\">Ophir Azulai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzelay_U/0/1/0/all/0/1\">Udi Barzelay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_D/0/1/0/all/0/1\">Daniel Rotman</a>",
          "description": "Classification of new class entities requires collecting and annotating\nhundreds or thousands of samples that is often prohibitively costly. Few-shot\nlearning suggests learning to classify new classes using just a few examples.\nOnly a small number of studies address the challenge of few-shot learning on\nspatio-temporal patterns such as videos. In this paper, we present the Temporal\nAware Embedding Network (TAEN) for few-shot action recognition, that learns to\nrepresent actions, in a metric space as a trajectory, conveying both short term\nsemantics and longer term connectivity between action parts. We demonstrate the\neffectiveness of TAEN on two few shot tasks, video classification and temporal\naction detection and evaluate our method on the Kinetics-400 and on ActivityNet\n1.2 few-shot benchmarks. With training of just a few fully connected layers we\nreach comparable results to prior art on both few shot video classification and\ntemporal detection tasks, while reaching state-of-the-art in certain scenarios.",
          "link": "http://arxiv.org/abs/2004.10141",
          "publishedOn": "2021-07-20T02:04:46.614Z",
          "wordCount": null,
          "title": "TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition. (arXiv:2004.10141v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Congram_B/0/1/0/all/0/1\">Benjamin Congram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Timothy D. Barfoot</a>",
          "description": "Visual Teach and Repeat has shown relative navigation is a robust and\nefficient solution for autonomous vision-based path following in difficult\nenvironments. Adding additional absolute sensors such as Global Navigation\nSatellite Systems (GNSS) has the potential to expand the domain of Visual Teach\nand Repeat to environments where the ability to visually localize is not\nguaranteed. Our method of lazy mapping and delaying estimation until a\npath-tracking error is needed avoids the need to estimate absolute states. As a\nresult, map optimization is not required and paths can be driven immediately\nafter being taught. We validate our approach on a real robot through an\nexperiment in a joint indoor-outdoor environment comprising 3.5km of autonomous\nroute repeating across a variety of lighting conditions. We achieve smooth\nerror signals throughout the runs despite large sections of dropout for each\nsensor.",
          "link": "http://arxiv.org/abs/2101.05107",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. (arXiv:2101.05107v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>",
          "description": "Semantic segmentation using fine-resolution remotely sensed images plays a\ncritical role in many practical applications, such as urban planning,\nenvironmental protection, natural and anthropogenic landscape monitoring, etc.\nHowever, the automation of semantic segmentation, i.e., automatic\ncategorization/labeling and segmentation is still a challenging task,\nparticularly for fine-resolution images with huge spatial and spectral\ncomplexity. Addressing such a problem represents an exciting research field,\nwhich paves the way for scene-level landscape pattern analysis and decision\nmaking. In this paper, we propose an approach for automatic land segmentation\nbased on the Feature Pyramid Network (FPN). As a classic architecture, FPN can\nbuild a feature pyramid with high-level semantics throughout. However,\nintrinsic defects in feature extraction and fusion hinder FPN from further\naggregating more discriminative features. Hence, we propose an Attention\nAggregation Module (AAM) to enhance multi-scale feature learning through\nattention-guided feature aggregation. Based on FPN and AAM, a novel framework\nnamed Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for\nsemantic segmentation of fine-resolution remotely sensed images. Extensive\nexperiments conducted on three datasets demonstrate the effectiveness of our A2\n-FPN in segmentation accuracy. Code is available at\nhttps://github.com/lironui/A2-FPN.",
          "link": "http://arxiv.org/abs/2102.07997",
          "publishedOn": "2021-07-20T02:04:46.613Z",
          "wordCount": null,
          "title": "A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed Images. (arXiv:2102.07997v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.612Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.",
          "link": "http://arxiv.org/abs/2107.08585",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Non-binary deep transfer learning for imageclassification. (arXiv:2107.08585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:46.611Z",
          "wordCount": null,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bozhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Facial attributes in StyleGAN generated images are entangled in the latent\nspace which makes it very difficult to independently control a specific\nattribute without affecting the others. Supervised attribute editing requires\nannotated training data which is difficult to obtain and limits the editable\nattributes to those with labels. Therefore, unsupervised attribute editing in\nan disentangled latent space is key to performing neat and versatile semantic\nface editing. In this paper, we present a new technique termed\nStructure-Texture Independent Architecture with Weight Decomposition and\nOrthogonal Regularization (STIA-WO) to disentangle the latent space for\nunsupervised semantic face editing. By applying STIA-WO to GAN, we have\ndeveloped a StyleGAN termed STGAN-WO which performs weight decomposition\nthrough utilizing the style vector to construct a fully controllable weight\nmatrix to regulate image synthesis, and employs orthogonal regularization to\nensure each entry of the style vector only controls one independent feature\nmatrix. To further disentangle the facial attributes, STGAN-WO introduces a\nstructure-texture independent architecture which utilizes two independently and\nidentically distributed (i.i.d.) latent vectors to control the synthesis of the\ntexture and structure components in a disentangled way. Unsupervised semantic\nediting is achieved by moving the latent code in the coarse layers along its\northogonal directions to change texture related attributes or changing the\nlatent code in the fine layers to manipulate structure related ones. We present\nexperimental results which show that our new STGAN-WO can achieve better\nattribute editing than state of the art methods.",
          "link": "http://arxiv.org/abs/2011.02638",
          "publishedOn": "2021-07-20T02:04:46.610Z",
          "wordCount": null,
          "title": "Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. (arXiv:2011.02638v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>",
          "description": "In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.",
          "link": "http://arxiv.org/abs/2107.08621",
          "publishedOn": "2021-07-20T02:04:46.609Z",
          "wordCount": null,
          "title": "Face.evoLVe: A High-Performance Face Recognition Library. (arXiv:2107.08621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.09465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Chi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyagi_N/0/1/0/all/0/1\">Neelam Tyagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nancy Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berry_S/0/1/0/all/0/1\">Sean Berry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "We developed a new joint probabilistic segmentation and image distribution\nmatching generative adversarial network (PSIGAN) for unsupervised domain\nadaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI)\nimages. Our UDA approach models the co-dependency between images and their\nsegmentation as a joint probability distribution using a new structure\ndiscriminator. The structure discriminator computes structure of interest\nfocused adversarial loss by combining the generated pseudo MRI with\nprobabilistic segmentations produced by a simultaneously trained segmentation\nsub-network. The segmentation sub-network is trained using the pseudo MRI\nproduced by the generator sub-network. This leads to a cyclical optimization of\nboth the generator and segmentation sub-networks that are jointly trained as\npart of an end-to-end network. Extensive experiments and comparisons against\nmultiple state-of-the-art methods were done on four different MRI sequences\ntotalling 257 scans for generating multi-organ and tumor segmentation. The\nexperiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20\nT2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right\nkidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for\nparotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our\nmethod achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the\nabdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for\nlung tumors.",
          "link": "http://arxiv.org/abs/2007.09465",
          "publishedOn": "2021-07-20T02:04:46.608Z",
          "wordCount": null,
          "title": "PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation. (arXiv:2007.09465v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>",
          "description": "Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.",
          "link": "http://arxiv.org/abs/2107.05140",
          "publishedOn": "2021-07-20T02:04:46.587Z",
          "wordCount": null,
          "title": "Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction. (arXiv:2107.05140v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:46.585Z",
          "wordCount": null,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:46.582Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>",
          "description": "Affective Analysis is not a single task, and the valence-arousal value,\nexpression class, and action unit can be predicted at the same time. Previous\nresearches did not pay enough attention to the entanglement and hierarchical\nrelation of these three facial attributes. We propose a novel model named\nfeature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply a teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperforms other models.\nThis is a submission to The 2nd Workshop and Competition on Affective Behavior\nAnalysis in the wild (ABAW). The code and model are available for research\npurposes at https://github.com/ryanhe312/ABAW2-FPNMAA.",
          "link": "http://arxiv.org/abs/2107.03670",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Feature Pyramid Network for Multi-task Affective Analysis. (arXiv:2107.03670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:46.580Z",
          "wordCount": null,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1\">Iuliia Kotseruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1\">Manos Papagelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>",
          "description": "This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.",
          "link": "http://arxiv.org/abs/2107.04902",
          "publishedOn": "2021-07-20T02:04:46.579Z",
          "wordCount": null,
          "title": "Industry and Academic Research in Computer Vision. (arXiv:2107.04902v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:46.578Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02739",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Han_S/0/1/0/all/0/1\">Sukjin Han</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Schulman_E/0/1/0/all/0/1\">Eric H. Schulman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Ramakrishnan</a>",
          "description": "Many differentiated products have key attributes that are unstructured and\nthus high-dimensional (e.g., design, text). Instead of treating unstructured\nattributes as unobservables in economic models, quantifying them can be\nimportant to answer interesting economic questions. To propose an analytical\nframework for this type of products, this paper considers one of the simplest\ndesign products -- fonts -- and investigates merger and product differentiation\nusing an original dataset from the world's largest online marketplace for\nfonts. We quantify font shapes by constructing embeddings from a deep\nconvolutional neural network. Each embedding maps a font's shape onto a\nlow-dimensional vector. In the resulting product space, designers are assumed\nto engage in Hotelling-type spatial competition. From the image embeddings, we\nconstruct two alternative measures that capture the degree of design\ndifferentiation. We then study the causal effects of a merger on the merging\nfirm's creative decisions using the constructed measures in a synthetic control\nmethod. We find that the merger causes the merging firm to increase the visual\nvariety of font design. Notably, such effects are not captured when using\ntraditional measures for product offerings (e.g., specifications and the number\nof products) constructed from structured data.",
          "link": "http://arxiv.org/abs/2107.02739",
          "publishedOn": "2021-07-20T02:04:46.577Z",
          "wordCount": null,
          "title": "Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts. (arXiv:2107.02739v1 [econ.EM] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
          "link": "http://arxiv.org/abs/2107.01579",
          "publishedOn": "2021-07-20T02:04:46.576Z",
          "wordCount": null,
          "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>",
          "description": "We present a bottom-up differentiable relaxation of the process of drawing\npoints, lines and curves into a pixel raster. Our approach arises from the\nobservation that rasterising a pixel in an image given parameters of a\nprimitive can be reformulated in terms of the primitive's distance transform,\nand then relaxed to allow the primitive's parameters to be learned. This\nrelaxation allows end-to-end differentiable programs and deep networks to be\nlearned and optimised and provides several building blocks that allow control\nover how a compositional drawing process is modelled. We emphasise the\nbottom-up nature of our proposed approach, which allows for drawing operations\nto be composed in ways that can mimic the physical reality of drawing rather\nthan being tied to, for example, approaches in modern computer graphics. With\nthe proposed approach we demonstrate how sketches can be generated by directly\noptimising against photographs and how auto-encoders can be built to transform\nrasterised handwritten digits into vectors without supervision. Extensive\nexperimental results highlight the power of this approach under different\nmodelling assumptions for drawing tasks.",
          "link": "http://arxiv.org/abs/2103.16194",
          "publishedOn": "2021-07-20T02:04:46.572Z",
          "wordCount": null,
          "title": "Differentiable Drawing and Sketching. (arXiv:2103.16194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.561Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1\">Baptiste Chopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_A/0/1/0/all/0/1\">Angela Bartolo</a>",
          "description": "Human motion prediction aims to forecast future human poses given a prior\npose sequence. The discontinuity of the predicted motion and the performance\ndeterioration in long-term horizons are still the main challenges encountered\nin current literature. In this work, we tackle these issues by using a compact\nmanifold-valued representation of human motion. Specifically, we model the\ntemporal evolution of the 3D human poses as trajectory, what allows us to map\nhuman motions to single points on a sphere manifold. To learn these\nnon-Euclidean representations, we build a manifold-aware Wasserstein generative\nadversarial model that captures the temporal and spatial dependencies of human\nmotion through different losses. Extensive experiments show that our approach\noutperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our\nqualitative results show the smoothness of the predicted motions.",
          "link": "http://arxiv.org/abs/2105.08715",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Human Motion Prediction Using Manifold-Aware Wasserstein GAN. (arXiv:2105.08715v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeonwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1\">Kazuya Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuhide Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1\">Ryoma Bise</a>",
          "description": "The domain shift problem is an important issue in automatic cell detection. A\ndetection network trained with training data under a specific condition (source\ndomain) may not work well in data under other conditions (target domain). We\npropose an unsupervised domain adaptation method for cell detection using the\npseudo-cell-position heatmap, where a cell centroid becomes a peak with a\nGaussian distribution in the map. In the prediction result for the target\ndomain, even if a peak location is correct, the signal distribution around the\npeak often has anon-Gaussian shape. The pseudo-cell-position heatmap is\nre-generated using the peak positions in the predicted heatmap to have a clear\nGaussian shape. Our method selects confident pseudo-cell-position heatmaps\nusing a Bayesian network and adds them to the training data in the next\niteration. The method can incrementally extend the domain from the source\ndomain to the target domain in a semi-supervised manner. In the experiments\nusing 8 combinations of domains, the proposed method outperformed the existing\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2107.08653",
          "publishedOn": "2021-07-20T02:04:46.488Z",
          "wordCount": null,
          "title": "Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position Heatmap. (arXiv:2107.08653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yifei Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "In a class of piecewise-constant image segmentation models, we propose to\nincorporate a weighted difference of anisotropic and isotropic total variation\n(AITV) to regularize the partition boundaries in an image. In particular, we\nreplace the total variation regularization in the Chan-Vese segmentation model\nand a fuzzy region competition model by the proposed AITV. To deal with the\nnonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in\nwhich the subproblems can be minimized by the primal-dual hybrid gradient\nmethod with linesearch. The convergence of the DCA scheme is analyzed. In\naddition, a generalization to color image segmentation is discussed. In the\nnumerical experiments, we compare the proposed models with the classic convex\napproaches and the two-stage segmentation methods (smoothing and then\nthresholding) on various images, showing that our models are effective in image\nsegmentation and robust with respect to impulsive noises.",
          "link": "http://arxiv.org/abs/2005.04401",
          "publishedOn": "2021-07-20T02:04:46.483Z",
          "wordCount": null,
          "title": "A Weighted Difference of Anisotropic and Isotropic Total Variation for Relaxed Mumford-Shah Color and Multiphase Image Segmentation. (arXiv:2005.04401v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Mushfiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedin_T/0/1/0/all/0/1\">Thasin Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prottoy_K/0/1/0/all/0/1\">Khondokar S. S. Prottoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshruba_A/0/1/0/all/0/1\">Ayana Moshruba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_F/0/1/0/all/0/1\">Fazlul Hasan Siddiqui</a>",
          "description": "Video captioning, i.e. the task of generating captions from video sequences\ncreates a bridge between the Natural Language Processing and Computer Vision\ndomains of computer science. The task of generating a semantically accurate\ndescription of a video is quite complex. Considering the complexity, of the\nproblem, the results obtained in recent research works are praiseworthy.\nHowever, there is plenty of scope for further investigation. This paper\naddresses this scope and proposes a novel solution. Most video captioning\nmodels comprise two sequential/recurrent layers - one as a video-to-context\nencoder and the other as a context-to-caption decoder. This paper proposes a\nnovel architecture, namely Semantically Sensible Video Captioning (SSVC) which\nmodifies the context generation mechanism by using two novel approaches -\n\"stacked attention\" and \"spatial hard pull\". As there are no exclusive metrics\nfor evaluating video captioning models, we emphasize both quantitative and\nqualitative analysis of our model. Hence, we have used the BLEU scoring metric\nfor quantitative analysis and have proposed a human evaluation metric for\nqualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS\nScore overcomes the shortcomings of common automated scoring metrics. This\npaper reports that the use of the aforementioned novelties improves the\nperformance of state-of-the-art architectures.",
          "link": "http://arxiv.org/abs/2009.07335",
          "publishedOn": "2021-07-20T02:04:46.482Z",
          "wordCount": null,
          "title": "Video captioning with stacked attention and semantic hard pull. (arXiv:2009.07335v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Di Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1\">Lorenzo Garattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>",
          "description": "Action recognition based on skeleton data has recently witnessed increasing\nattention and progress. State-of-the-art approaches adopting Graph\nConvolutional networks (GCNs) can effectively extract features on human\nskeletons relying on the pre-defined human topology. Despite associated\nprogress, GCN-based methods have difficulties to generalize across domains,\nespecially with different human topological structures. In this context, we\nintroduce UNIK, a novel skeleton-based action recognition method that is not\nonly effective to learn spatio-temporal features on human skeleton sequences\nbut also able to generalize across datasets. This is achieved by learning an\noptimal dependency matrix from the uniform distribution based on a multi-head\nattention mechanism. Subsequently, to study the cross-domain generalizability\nof skeleton-based action recognition in real-world videos, we re-evaluate\nstate-of-the-art approaches as well as the proposed UNIK in light of a novel\nPosetics dataset. This dataset is created from Kinetics-400 videos by\nestimating, refining and filtering poses. We provide an analysis on how much\nperformance improves on smaller benchmark datasets after pre-training on\nPosetics for the action classification task. Experimental results show that the\nproposed UNIK, with pre-training on Posetics, generalizes well and outperforms\nstate-of-the-art when transferred onto four target action classification\ndatasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.",
          "link": "http://arxiv.org/abs/2107.08580",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition. (arXiv:2107.08580v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yiyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yunxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "Place recognition is indispensable for a drift-free localization system. Due\nto the variations of the environment, place recognition using single-modality\nhas limitations. In this paper, we propose a bi-modal place recognition method,\nwhich can extract a compound global descriptor from the two modalities, vision\nand LiDAR. Specifically, we first build the elevation image generated from 3D\npoints as a structural representation. Then, we derive the correspondences\nbetween 3D points and image pixels that are further used in merging the\npixel-wise visual features into the elevation map grids. In this way, we fuse\nthe structural features and visual features in the consistent bird-eye view\nframe, yielding a semantic representation, namely CORAL. And the whole network\nis called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD\nhas superior performance against other state-of-the-art methods. We also\ndemonstrate that our network can be generalized to other scenes and sensor\nconfigurations on cross-city datasets.",
          "link": "http://arxiv.org/abs/2011.10934",
          "publishedOn": "2021-07-20T02:04:46.481Z",
          "wordCount": null,
          "title": "CORAL: Colored structural representation for bi-modal place recognition. (arXiv:2011.10934v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingrong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinwei He</a>",
          "description": "Medical image segmentation plays an essential role in developing\ncomputer-assisted diagnosis and therapy systems, yet still faces many\nchallenges. In the past few years, the popular encoder-decoder architectures\nbased on CNNs (e.g., U-Net) have been successfully applied in the task of\nmedical image segmentation. However, due to the locality of convolution\noperations, they demonstrate limitations in learning global context and\nlong-range spatial relations. Recently, several researchers try to introduce\ntransformers to both the encoder and decoder components with promising results,\nbut the efficiency requires further improvement due to the high computational\ncomplexity of transformers. In this paper, we propose LeViT-UNet, which\nintegrates a LeViT Transformer module into the U-Net architecture, for fast and\naccurate medical image segmentation. Specifically, we use LeViT as the encoder\nof the LeViT-UNet, which better trades off the accuracy and efficiency of the\nTransformer block. Moreover, multi-scale feature maps from transformer blocks\nand convolutional blocks of LeViT are passed into the decoder via\nskip-connection, which can effectively reuse the spatial information of the\nfeature maps. Our experiments indicate that the proposed LeViT-UNet achieves\nbetter performance comparing to various competing methods on several\nchallenging medical image segmentation benchmarks including Synapse and ACDC.\nCode and models will be publicly available at\nhttps://github.com/apple1986/LeViT_UNet.",
          "link": "http://arxiv.org/abs/2107.08623",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation. (arXiv:2107.08623v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuanhan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiachen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Shunxing Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A.Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Catie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Unsupervised learning algorithms (e.g., self-supervised learning,\nauto-encoder, contrastive learning) allow deep learning models to learn\neffective image representations from large-scale unlabeled data. In medical\nimage analysis, even unannotated data can be difficult to obtain for individual\nlabs. Fortunately, national-level efforts have been made to provide efficient\naccess to obtain biomedical image data from previous scientific publications.\nFor instance, NIH has launched the Open-i search engine that provides a\nlarge-scale image database with free access. However, the images in scientific\npublications consist of a considerable amount of compound figures with\nsubplots. To extract and curate individual subplots, many different compound\nfigure separation approaches have been developed, especially with the recent\nadvances in deep learning. However, previous approaches typically required\nresource extensive bounding box annotation to train detection models. In this\npaper, we propose a simple compound figure separation (SimCFS) framework that\nuses weak classification annotations from individual images. Our technical\ncontribution is three-fold: (1) we introduce a new side loss that is designed\nfor compound figure separation; (2) we introduce an intra-class image\naugmentation method to simulate hard cases; (3) the proposed framework enables\nan efficient deployment to new classes of images, without requiring resource\nextensive bounding box annotations. From the results, the SimCFS achieved a new\nstate-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation\nDatabase. The source code of SimCFS is made publicly available at\nhttps://github.com/hrlblab/ImageSeperation.",
          "link": "http://arxiv.org/abs/2107.08650",
          "publishedOn": "2021-07-20T02:04:46.480Z",
          "wordCount": null,
          "title": "Compound Figure Separation of Biomedical Images with Side Loss. (arXiv:2107.08650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yan Bin Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>",
          "description": "We present a new architecture for human action forecasting from videos. A\ntemporal recurrent encoder captures temporal information of input videos while\na self-attention model is used to attend on relevant feature dimensions of the\ninput space. To handle temporal variations in observed video data, a feature\nmasking techniques is employed. We classify observed actions accurately using\nan auxiliary classifier which helps to understand what has happened so far.\nThen the decoder generates actions for the future based on the output of the\nrecurrent encoder and the self-attention model. Experimentally, we validate\neach component of our architecture where we see that the impact of\nself-attention to identify relevant feature dimensions, temporal masking, and\nobserved auxiliary classifier. We evaluate our method on two standard action\nforecasting benchmarks and obtain state-of-the-art results.",
          "link": "http://arxiv.org/abs/2107.08579",
          "publishedOn": "2021-07-20T02:04:46.479Z",
          "wordCount": null,
          "title": "Action Forecasting with Feature-wise Self-Attention. (arXiv:2107.08579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:46.478Z",
          "wordCount": null,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_A/0/1/0/all/0/1\">Avichai Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Generating images from scene graphs is a challenging task that attracted\nsubstantial interest recently. Prior works have approached this task by\ngenerating an intermediate layout description of the target image. However, the\nrepresentation of each object in the layout was generated independently, which\nresulted in high overlap, low coverage, and an overall blurry layout. We\npropose a novel method that alleviates these issues by generating the entire\nlayout description gradually to improve inter-object dependency. We empirically\nshow on the COCO-STUFF dataset that our approach improves the quality of both\nthe intermediate layout and the final image. Our approach improves the layout\ncoverage by almost 20 points and drops object overlap to negligible amounts.",
          "link": "http://arxiv.org/abs/2009.10939",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Scene Graph to Image Generation with Contextualized Object Layout Refinement. (arXiv:2009.10939v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n\nWith these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-07-20T02:04:46.096Z",
          "wordCount": null,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.003Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belmonte_R/0/1/0/all/0/1\">Romain Belmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaert_B/0/1/0/all/0/1\">Benjamin Allaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1\">Pierre Tirilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1\">Ioan Marius Bilasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>",
          "description": "Although facial landmark localization (FLL) approaches are becoming\nincreasingly accurate for characterizing facial regions, one question remains\nunanswered: what is the impact of these approaches on subsequent related tasks?\nIn this paper, the focus is put on facial expression recognition (FER), where\nfacial landmarks are used for face registration, which is a common usage. Since\nthe most used datasets for facial landmark localization do not allow for a\nproper measurement of performance according to the different difficulties\n(e.g., pose, expression, illumination, occlusion, motion blur), we also\nquantify the performance of recent approaches in the presence of head pose\nvariations and facial expressions. Finally, a study of the impact of these\napproaches on FER is conducted. We show that the landmark accuracy achieved so\nfar optimizing the conventional Euclidean distance does not necessarily\nguarantee a gain in performance for FER. To deal with this issue, we propose a\nnew evaluation metric for FLL adapted to FER.",
          "link": "http://arxiv.org/abs/1905.10784",
          "publishedOn": "2021-07-20T02:04:46.002Z",
          "wordCount": null,
          "title": "Impact of facial landmark localization on facial expression recognition. (arXiv:1905.10784v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.001Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:45.991Z",
          "wordCount": null,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1912.08393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Changqun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingcan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "By the aid of attention mechanisms to weight the image features adaptively,\nrecent advanced deep learning-based models encourage the predicted results to\napproximate the ground-truth masks with as large predictable areas as possible,\nthus achieving the state-of-the-art performance. However, these methods do not\npay enough attention to small areas prone to misprediction. In this way, it is\nstill tough to accurately locate salient objects due to the existence of\nregions with indistinguishable foreground and background and regions with\ncomplex or fine structures. To address these problems, we propose a novel\nconvolutional neural network with purificatory mechanism and structural\nsimilarity loss. Specifically, in order to better locate preliminary salient\nobjects, we first introduce the promotion attention, which is based on spatial\nand channel attention mechanisms to promote attention to salient regions.\nSubsequently, for the purpose of restoring the indistinguishable regions that\ncan be regarded as error-prone regions of one model, we propose the\nrectification attention, which is learned from the areas of wrong prediction\nand guide the network to focus on error-prone regions thus rectifying errors.\nThrough these two attentions, we use the Purificatory Mechanism to impose\nstrict weights with different regions of the whole salient objects and purify\nresults from hard-to-distinguish regions, thus accurately predicting the\nlocations and details of salient objects. In addition to paying different\nattention to these hard-to-distinguish regions, we also consider the structural\nconstraints on complex regions and propose the Structural Similarity Loss. In\nexperiments, the proposed approach outperforms 19 state-of-the-art methods on\nsix datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.",
          "link": "http://arxiv.org/abs/1912.08393",
          "publishedOn": "2021-07-20T02:04:45.988Z",
          "wordCount": null,
          "title": "Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss. (arXiv:1912.08393v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yingchao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1\">Wenhui Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>",
          "description": "The balance between high accuracy and high speed has always been a\nchallenging task in semantic image segmentation. Compact segmentation networks\nare more widely used in the case of limited resources, while their performances\nare constrained. In this paper, motivated by the residual learning and global\naggregation, we propose a simple yet general and effective knowledge\ndistillation framework called double similarity distillation (DSD) to improve\nthe classification accuracy of all existing compact networks by capturing the\nsimilarity knowledge in pixel and category dimensions, respectively.\nSpecifically, we propose a pixel-wise similarity distillation (PSD) module that\nutilizes residual attention maps to capture more detailed spatial dependencies\nacross multiple layers. Compared with exiting methods, the PSD module greatly\nreduces the amount of calculation and is easy to expand. Furthermore,\nconsidering the differences in characteristics between semantic segmentation\ntask and other computer vision tasks, we propose a category-wise similarity\ndistillation (CSD) module, which can help the compact segmentation network\nstrengthen the global category correlation by constructing the correlation\nmatrix. Combining these two modules, DSD framework has no extra parameters and\nonly a minimal increase in FLOPs. Extensive experiments on four challenging\ndatasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that\nDSD outperforms current state-of-the-art methods, proving its effectiveness and\ngenerality. The code and models will be publicly available.",
          "link": "http://arxiv.org/abs/2107.08591",
          "publishedOn": "2021-07-20T02:04:45.987Z",
          "wordCount": null,
          "title": "Double Similarity Distillation for Semantic Image Segmentation. (arXiv:2107.08591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:45.985Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>",
          "description": "Few-shot class-incremental learning is to recognize the new classes given few\nsamples and not forget the old classes. It is a challenging task since\nrepresentation optimization and prototype reorganization can only be achieved\nunder little supervision. To address this problem, we propose a novel\nincremental prototype learning scheme. Our scheme consists of a random episode\nselection strategy that adapts the feature representation to various generated\nincremental episodes to enhance the corresponding extensibility, and a\nself-promoted prototype refinement mechanism which strengthens the expression\nability of the new classes by explicitly considering the dependencies among\ndifferent classes. Particularly, a dynamic relation projection module is\nproposed to calculate the relation matrix in a shared embedding space and\nleverage it as the factor for bootstrapping the update of prototypes. Extensive\nexperiments on three benchmark datasets demonstrate the above-par incremental\nperformance, outperforming state-of-the-art methods by a margin of 13%, 17% and\n11%, respectively.",
          "link": "http://arxiv.org/abs/2107.08918",
          "publishedOn": "2021-07-20T02:04:45.984Z",
          "wordCount": null,
          "title": "Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning. (arXiv:2107.08918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:45.661Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1\">Boris Shirokih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>",
          "description": "Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.",
          "link": "http://arxiv.org/abs/2107.08543",
          "publishedOn": "2021-07-20T02:04:45.229Z",
          "wordCount": 664,
          "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shutai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>",
          "description": "Small target detection is known to be a challenging problem. Inspired by the\nstructural characteristics and physiological mechanism of eagle-eye, a\nminiature vision system is designed for small target detection in this paper.\nFirst, a hardware platform is established, which consists of a pan-tilt, a\nshort-focus camera and a long-focus camera. Then, based on the visual attention\nmechanism of eagle-eye, the cameras with different focal lengths are controlled\ncooperatively to achieve small target detection. Experimental results show that\nthe designed biological eagle-eye vision system can accurately detect small\ntargets, which has a strong adaptive ability.",
          "link": "http://arxiv.org/abs/2107.08406",
          "publishedOn": "2021-07-20T02:04:45.201Z",
          "wordCount": 552,
          "title": "A Miniature Biological Eagle-Eye Vision System for Small Target Detection. (arXiv:2107.08406v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:45.170Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Dongze Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for the\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features communication. By axially shifting\nchannels of the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, just like designing\nthose of convolution kernels. With the proposed AS-MLP architecture, our model\nobtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Code is available at https://github.com/svip-lab/AS-MLP.",
          "link": "http://arxiv.org/abs/2107.08391",
          "publishedOn": "2021-07-20T02:04:45.154Z",
          "wordCount": 658,
          "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision. (arXiv:2107.08391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:42.595Z",
          "wordCount": 572,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiahuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Wu</a>",
          "description": "Existing popular unsupervised embedding learning methods focus on enhancing\nthe instance-level local discrimination of the given unlabeled images by\nexploring various negative data. However, the existed sample outliers which\nexhibit large intra-class divergences or small inter-class variations severely\nlimit their learning performance. We justify that the performance limitation is\ncaused by the gradient vanishing on these sample outliers. Moreover, the\nshortage of positive data and disregard for global discrimination consideration\nalso pose critical issues for unsupervised learning but are always ignored by\nexisting methods. To handle these issues, we propose a novel solution to\nexplicitly model and directly explore the uncertainty of the given unlabeled\nlearning samples. Instead of learning a deterministic feature point for each\nsample in the embedding space, we propose to represent a sample by a stochastic\nGaussian with the mean vector depicting its space localization and covariance\nvector representing the sample uncertainty. We leverage such uncertainty\nmodeling as momentum to the learning which is helpful to tackle the outliers.\nFurthermore, abundant positive candidates can be readily drawn from the learned\ninstance-specific distributions which are further adopted to mitigate the\naforementioned issues. Thorough rationale analyses and extensive experiments\nare presented to verify our superiority.",
          "link": "http://arxiv.org/abs/2107.08892",
          "publishedOn": "2021-07-20T02:04:42.575Z",
          "wordCount": 635,
          "title": "Unsupervised Embedding Learning from Uncertainty Momentum Modeling. (arXiv:2107.08892v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:42.557Z",
          "wordCount": 612,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:42.480Z",
          "wordCount": 767,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08767",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "As interpretability has been pointed out as the obstacle to the adoption of\nDeep Neural Networks (DNNs), there is an increasing interest in solving a\ntransparency issue to guarantee the impressive performance. In this paper, we\ndemonstrate the efficiency of recent attribution techniques to explain the\ndiagnostic decision by visualizing the significant factors in the input image.\nBy utilizing the characteristics of objectness that DNNs have learned, fully\ndecomposing the network prediction visualizes clear localization of target\nlesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis\nwith publicly accessible datasets. As an intuitive assessment metric for\nexplanations, we report the performance of intersection of Union between visual\nexplanation and bounding box of lesions. Experiment results show that recently\nproposed attribution methods visualize the more accurate localization for the\ndiagnostic decision compared to the traditionally used CAM. Furthermore, we\nanalyze the inconsistency of intentions between humans and DNNs, which is\neasily obscured by high performance. By visualizing the relevant factors, it is\npossible to confirm that the criterion for decision is in line with the\nlearning strategy. Our analysis of unmasking machine intelligence represents\nthe necessity of explainability in the medical diagnostic decision.",
          "link": "http://arxiv.org/abs/2107.08767",
          "publishedOn": "2021-07-20T02:04:42.431Z",
          "wordCount": 654,
          "title": "Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units. (arXiv:2107.08767v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Myeong-Seok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong-Ju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Aerial image registration or matching is a geometric process of aligning two\naerial images captured in different environments. Estimating the precise\ntransformation parameters is hindered by various environments such as time,\nweather, and viewpoints. The characteristics of the aerial images are mainly\ncomposed of a straight line owing to building and road. Therefore, the straight\nlines are distorted when estimating homography parameters directly between two\nimages. In this paper, we propose a deep homography alignment network to\nprecisely match two aerial images by progressively estimating the various\ntransformation parameters. The proposed network is possible to train the\nmatching network with a higher degree of freedom by progressively analyzing the\ntransformation parameters. The precision matching performances have been\nincreased by applying homography transformation. In addition, we introduce a\nmethod that can effectively learn the difficult-to-learn homography estimation\nnetwork. Since there is no published learning data for aerial image\nregistration, in this paper, a pair of images to which random homography\ntransformation is applied within a certain range is used for learning. Hence,\nwe could confirm that the deep homography alignment network shows high\nprecision matching performance compared with conventional works.",
          "link": "http://arxiv.org/abs/2107.08768",
          "publishedOn": "2021-07-20T02:04:42.402Z",
          "wordCount": 624,
          "title": "Precise Aerial Image Matching based on Deep Homography Estimation. (arXiv:2107.08768v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Man Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yuanzhu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_X/0/1/0/all/0/1\">Xinchao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Muqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jing Song</a>",
          "description": "The 3D visual perception for vehicles with the surround-view fisheye camera\nsystem is a critical and challenging task for low-cost urban autonomous\ndriving. While existing monocular 3D object detection methods perform not well\nenough on the fisheye images for mass production, partly due to the lack of 3D\ndatasets of such images. In this paper, we manage to overcome and avoid the\ndifficulty of acquiring the large scale of accurate 3D labeled truth data, by\nbreaking down the 3D object detection task into some sub-tasks, such as\nvehicle's contact point detection, type classification, re-identification and\nunit assembling, etc. Particularly, we propose the concept of Multidimensional\nVector to include the utilizable information generated in different dimensions\nand stages, instead of the descriptive approach for the bird's eye view (BEV)\nor a cube of eight points. The experiments of real fisheye images demonstrate\nthat our solution achieves state-of-the-art accuracy while being real-time in\npractice.",
          "link": "http://arxiv.org/abs/2107.08862",
          "publishedOn": "2021-07-20T02:04:42.276Z",
          "wordCount": 617,
          "title": "Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras. (arXiv:2107.08862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:42.239Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "By considering the spatial correspondence, dense self-supervised\nrepresentation learning has achieved superior performance on various dense\nprediction tasks. However, the pixel-level correspondence tends to be noisy\nbecause of many similar misleading pixels, e.g., backgrounds. To address this\nissue, in this paper, we propose to explore \\textbf{set} \\textbf{sim}ilarity\n(SetSim) for dense self-supervised representation learning. We generalize\npixel-wise similarity learning to set-wise one to improve the robustness\nbecause sets contain more semantic and structure information. Specifically, by\nresorting to attentional features of views, we establish corresponding sets,\nthus filtering out noisy backgrounds that may cause incorrect correspondences.\nMeanwhile, these attentional features can keep the coherence of the same image\nacross different views to alleviate semantic inconsistency. We further search\nthe cross-view nearest neighbours of sets and employ the structured\nneighbourhood information to enhance the robustness. Empirical evaluations\ndemonstrate that SetSim is superior to state-of-the-art methods on object\ndetection, keypoint detection, instance segmentation, and semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.08712",
          "publishedOn": "2021-07-20T02:04:42.135Z",
          "wordCount": 603,
          "title": "Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1\">Buyue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Universal lesion detection in computed tomography (CT) images is an important\nyet challenging task due to the large variations in lesion type, size, shape,\nand appearance. Considering that data in clinical routine (such as the\nDeepLesion dataset) are usually annotated with a long and a short diameter\naccording to the standard of Response Evaluation Criteria in Solid Tumors\n(RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection\nin which the four extreme points and center point of the RECIST diameters are\ndetected. By detecting a lesion as keypoints, we provide a more conceptually\nstraightforward formulation for detection, and overcome several drawbacks\n(e.g., requiring extensive effort in designing data-appropriate anchors and\nlosing shape information) of existing bounding-box-based methods while\nexploring a single-task, one-stage approach compared to other RECIST-based\napproaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49%\nat four false positives per image, outperforming other recent methods including\nthose using multi-task learning.",
          "link": "http://arxiv.org/abs/2107.08715",
          "publishedOn": "2021-07-20T02:04:42.115Z",
          "wordCount": 609,
          "title": "RECIST-Net: Lesion detection via grouping keypoints on RECIST-based annotation. (arXiv:2107.08715v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young J. Kim</a>",
          "description": "We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.",
          "link": "http://arxiv.org/abs/2107.08737",
          "publishedOn": "2021-07-20T02:04:42.096Z",
          "wordCount": 584,
          "title": "Synthesizing Human Faces using Latent Space Factorization and Local Weights (Extended Version). (arXiv:2107.08737v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>",
          "description": "Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.",
          "link": "http://arxiv.org/abs/2107.08565",
          "publishedOn": "2021-07-20T02:04:41.858Z",
          "wordCount": 550,
          "title": "Learning point embedding for 3D data processing. (arXiv:2107.08565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>",
          "description": "In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.",
          "link": "http://arxiv.org/abs/2107.08430",
          "publishedOn": "2021-07-20T02:04:41.791Z",
          "wordCount": 625,
          "title": "YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>",
          "description": "We propose an approach to instance segmentation from 3D point clouds based on\ndynamic convolution. This enables it to adapt, at inference, to varying feature\nand object scales. Doing so avoids some pitfalls of bottom up approaches,\nincluding a dependence on hyper-parameter tuning and heuristic post-processing\npipelines to compensate for the inevitable variability in object sizes, even\nwithin a single scene. The representation capability of the network is greatly\nimproved by gathering homogeneous points that have identical semantic\ncategories and close votes for the geometric centroids. Instances are then\ndecoded via several simple convolution layers, where the parameters are\ngenerated conditioned on the input. The proposed approach is proposal-free, and\ninstead exploits a convolution process that adapts to the spatial and semantic\ncharacteristics of each instance. A light-weight transformer, built on the\nbottleneck layer, allows the model to capture long-range dependencies, with\nlimited computational overhead. The result is a simple, efficient, and robust\napproach that yields strong performance on various datasets: ScanNetV2, S3DIS,\nand PartNet. The consistent improvements on both voxel- and point-based\narchitectures imply the effectiveness of the proposed method. Code is available\nat: https://git.io/DyCo3D",
          "link": "http://arxiv.org/abs/2107.08392",
          "publishedOn": "2021-07-20T02:04:41.770Z",
          "wordCount": 628,
          "title": "Dynamic Convolution for 3D Point Cloud Instance Segmentation. (arXiv:2107.08392v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yongxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaolin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuncong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>",
          "description": "As one of the prevalent components, Feature Pyramid Network (FPN) is widely\nused in the current object detection models to improve the performance of\nmulti-scale detection. However, its interaction is still in a local and lossy\nmanner, thus limiting the representation power. In this paper, to simulate a\nglobal view of human vision in object detection and address the inherent\ndefects of interaction mode in FPN, we construct a novel architecture termed\nContent-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN,\nwhich fuses features within a local receptive field, CA-FPN can adaptively\naggregate similar features from a global view. It is equipped with a global\ncontent extraction module and light linear spatial transformers. The former\nallows to extract multi-scale context information and the latter can deeply\ncombine the global content extraction module with the vanilla FPN using the\nlinearized attention function, which is designed to reduce model complexity.\nFurthermore, CA-FPN can be readily plugged into existing FPN-based models.\nExtensive experiments on the challenging COCO and PASCAL VOC object detection\ndatasets demonstrated that our CA-FPN significantly outperforms competitive\nFPN-based detectors without bells and whistles. When plugging CA-FPN into\nCascade R-CNN framework built upon a standard ResNet-50 backbone, our method\ncan achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous\nstate-of-the-art by 1.5 AP, demonstrating the potentiality of application.",
          "link": "http://arxiv.org/abs/2105.09464",
          "publishedOn": "2021-07-20T02:04:41.740Z",
          "wordCount": 703,
          "title": "Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection. (arXiv:2105.09464v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>",
          "description": "Guided depth super-resolution is a practical task where a low-resolution and\nnoisy input depth map is restored to a high-resolution version, with the help\nof a high-resolution RGB guide image. Existing methods usually view this task\nas a generalized guided filtering problem that relies on designing explicit\nfilters and objective functions, or a dense regression problem that directly\npredicts the target image via deep neural networks. These methods suffer from\neither model capability or interpretability. Inspired by the recent progress in\nimplicit neural representation, we propose to formulate the guided\nsuper-resolution as a neural implicit image interpolation problem, where we\ntake the form of a general image interpolation but use a novel Joint Implicit\nImage Function (JIIF) representation to learn both the interpolation weights\nand values. JIIF represents the target image domain with spatially distributed\nlocal latent codes extracted from the input image and the guide image, and uses\na graph attention mechanism to learn the interpolation weights at the same time\nin one unified deep implicit function. We demonstrate the effectiveness of our\nJIIF representation on guided depth super-resolution task, significantly\noutperforming state-of-the-art methods on three public benchmarks. Code can be\nfound at \\url{https://git.io/JC2sU}.",
          "link": "http://arxiv.org/abs/2107.08717",
          "publishedOn": "2021-07-20T02:04:41.695Z",
          "wordCount": 639,
          "title": "Joint Implicit Image Function for Guided Depth Super-Resolution. (arXiv:2107.08717v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:41.676Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08111",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>",
          "description": "Building robust deep learning-based models requires diverse training data,\nideally from several sources. However, these datasets cannot be combined easily\nbecause of patient privacy concerns or regulatory hurdles, especially if\nmedical data is involved. Federated learning (FL) is a way to train machine\nlearning models without the need for centralized datasets. Each FL client\ntrains on their local data while only sharing model parameters with a global\nserver that aggregates the parameters from all clients. At the same time, each\nclient's data can exhibit differences and inconsistencies due to the local\nvariation in the patient population, imaging equipment, and acquisition\nprotocols. Hence, the federated learned models should be able to adapt to the\nlocal particularities of a client's data. In this work, we combine FL with an\nAutoML technique based on local neural architecture search by training a\n\"supernet\". Furthermore, we propose an adaptation scheme to allow for\npersonalized model architectures at each FL client's site. The proposed method\nis evaluated on four different datasets from 3D prostate MRI and shown to\nimprove the local models' performance after adaptation through selecting an\noptimal path through the AutoML supernet.",
          "link": "http://arxiv.org/abs/2107.08111",
          "publishedOn": "2021-07-20T02:04:41.619Z",
          "wordCount": 650,
          "title": "Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures. (arXiv:2107.08111v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:41.599Z",
          "wordCount": 616,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08330",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Konwer_A/0/1/0/all/0/1\">Aishik Konwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "COVID-19 image analysis has mostly focused on diagnostic tasks using single\ntimepoint scans acquired upon disease presentation or admission. We present a\ndeep learning-based approach to predict lung infiltrate progression from serial\nchest radiographs (CXRs) of COVID-19 patients. Our method first utilizes\nconvolutional neural networks (CNNs) for feature extraction from patches within\nthe concerned lung zone, and also from neighboring and remote boundary regions.\nThe framework further incorporates a multi-scale Gated Recurrent Unit (GRU)\nwith a correlation module for effective predictions. The GRU accepts CNN\nfeature vectors from three different areas as input and generates a fused\nrepresentation. The correlation module attempts to minimize the correlation\nloss between hidden representations of concerned and neighboring area feature\nvectors, while maximizing the loss between the same from concerned and remote\nregions. Further, we employ an attention module over the output hidden states\nof each encoder timepoint to generate a context vector. This vector is used as\nan input to a decoder module to predict patch severity grades at a future\ntimepoint. Finally, we ensemble the patch classification scores to calculate\npatient-wise grades. Specifically, our framework predicts zone-wise disease\nseverity for a patient on a given day by learning representations from the\nprevious temporal CXRs. Our novel multi-institutional dataset comprises\nsequential CXR scans from N=93 patients. Our approach outperforms transfer\nlearning and radiomic feature-based baseline approaches on this dataset.",
          "link": "http://arxiv.org/abs/2107.08330",
          "publishedOn": "2021-07-20T02:04:41.508Z",
          "wordCount": 742,
          "title": "Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction. (arXiv:2107.08330v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08355",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Liupeng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Lingli Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiangqiang Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinghua Li</a>",
          "description": "The data fusion technology aims to aggregate the characteristics of different\ndata and obtain products with multiple data advantages. To solves the problem\nof reduced resolution of PolSAR images due to system limitations, we propose a\nfully polarimetric synthetic aperture radar (PolSAR) images and\nsingle-polarization synthetic aperture radar SAR (SinSAR) images fusion network\nto generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the\npolarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the\nspatial information of the high-resolution single-polarization SAR (HR-SinSAR)\nimage, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR\nimage and design a cross-attention mechanism to extract features from the joint\ninput data. Besides, based on the physical imaging mechanism, we designed the\nPolSAR polarimetric loss function for constrained network training. The\nexperimental results confirm the superiority of fusion network over traditional\nalgorithms. The average PSNR is increased by more than 3.6db, and the average\nMAE is reduced to less than 0.07. Experiments on polarimetric decomposition and\npolarimetric signature show that it maintains polarimetric information well.",
          "link": "http://arxiv.org/abs/2107.08355",
          "publishedOn": "2021-07-20T02:04:41.490Z",
          "wordCount": 621,
          "title": "Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network. (arXiv:2107.08355v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>",
          "description": "Manually annotating medical images is extremely expensive, especially for\nlarge-scale datasets. Self-supervised contrastive learning has been explored to\nlearn feature representations from unlabeled images. However, unlike natural\nimages, the application of contrastive learning to medical images is relatively\nlimited. In this work, we propose a self-supervised framework, namely\nlesion-based contrastive learning for automated diabetic retinopathy (DR)\ngrading. Instead of taking entire images as the input in the common contrastive\nlearning scheme, lesion patches are employed to encourage the feature extractor\nto learn representations that are highly discriminative for DR grading. We also\ninvestigate different data augmentation operations in defining our contrastive\nprediction task. Extensive experiments are conducted on the publicly-accessible\ndataset EyePACS, demonstrating that our proposed framework performs\noutstandingly on DR grading in terms of both linear evaluation and transfer\ncapacity evaluation.",
          "link": "http://arxiv.org/abs/2107.08274",
          "publishedOn": "2021-07-20T02:04:41.472Z",
          "wordCount": 588,
          "title": "Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from Fundus Images. (arXiv:2107.08274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.",
          "link": "http://arxiv.org/abs/2107.08323",
          "publishedOn": "2021-07-20T02:04:41.454Z",
          "wordCount": 613,
          "title": "Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zare_S/0/1/0/all/0/1\">Samira Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hien Van Nguyen</a>",
          "description": "Set-input deep networks have recently drawn much interest in computer vision\nand machine learning. This is in part due to the increasing number of important\ntasks such as meta-learning, clustering, and anomaly detection that are defined\non set inputs. These networks must take an arbitrary number of input samples\nand produce the output invariant to the input set permutation. Several\nalgorithms have been recently developed to address this urgent need. Our paper\nanalyzes these algorithms using both synthetic and real-world datasets, and\nshows that they are not effective in dealing with common data variations such\nas image translation or viewpoint change. To address this limitation, we\npropose a permutation-invariant cascaded attentional set operator (PICASO). The\ngist of PICASO is a cascade of multihead attention blocks with dynamic\ntemplates. The proposed operator is a stand-alone module that can be adapted\nand extended to serve different machine learning tasks. We demonstrate the\nutilities of PICASO in four diverse scenarios: (i) clustering, (ii) image\nclassification under novel viewpoints, (iii) image anomaly detection, and (iv)\nstate prediction. PICASO increases the SmallNORB image classification accuracy\nwith novel viewpoints by about 10% points. For set anomaly detection on CelebA\ndataset, our model improves the areas under ROC and PR curves dataset by about\n22% and 10%, respectively. For the state prediction on CLEVR dataset, it\nimproves the AP by about 40%.",
          "link": "http://arxiv.org/abs/2107.08305",
          "publishedOn": "2021-07-20T02:04:41.436Z",
          "wordCount": 654,
          "title": "PICASO: Permutation-Invariant Cascaded Attentional Set Operator. (arXiv:2107.08305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "We present the WoodScape fisheye semantic segmentation challenge for\nautonomous driving which was held as part of the CVPR 2021 Workshop on\nOmnidirectional Computer Vision (OmniCV). This challenge is one of the first\nopportunities for the research community to evaluate the semantic segmentation\ntechniques targeted for fisheye camera perception. Due to strong radial\ndistortion standard models don't generalize well to fisheye images and hence\nthe deformations in the visual appearance of objects and entities needs to be\nencoded implicitly or as explicit knowledge. This challenge served as a medium\nto investigate the challenges and new methodologies to handle the complexities\nwith perception on fisheye images. The challenge was hosted on CodaLab and used\nthe recently released WoodScape dataset comprising of 10k samples. In this\npaper, we provide a summary of the competition which attracted the\nparticipation of 71 global teams and a total of 395 submissions. The top teams\nrecorded significantly improved mean IoU and accuracy scores over the baseline\nPSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms\nand analyze the failure cases. We conclude by providing future directions for\nthe research.",
          "link": "http://arxiv.org/abs/2107.08246",
          "publishedOn": "2021-07-20T02:04:41.417Z",
          "wordCount": 659,
          "title": "Woodscape Fisheye Semantic Segmentation for Autonomous Driving -- CVPR 2021 OmniCV Workshop Challenge. (arXiv:2107.08246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:41.399Z",
          "wordCount": 673,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lisha Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle\nacross different cameras. Two key challenges lie in the subtle inter-instance\ndiscrepancy caused by near-duplicate identities and the large intra-instance\nvariance caused by different views. Since the holistic appearance suffers from\nviewpoint variation and distortion, part-level feature learning has been\nintroduced to enhance vehicle description. However, existing approaches to\nlocalize and amplify significant parts often fail to handle spatial\nmisalignment as well as occlusion and require expensive annotations. In this\npaper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)\ncomposed of a Part Attention Network (PANet) for vehicle part localization with\nself-attention and a Part-Mentored Network (PMNet) for mentoring the global and\nlocal feature aggregation. Firstly, PANet is introduced to predict a foreground\nmask and pinpoint $K$ prominent vehicle parts only with weak identity\nsupervision. Secondly, we propose a PMNet to learn global and part-level\nfeatures with multi-scale attention and aggregate them in $K$ main-partial\ntasks via part transfer. Like humans who first differentiate objects with\ngeneral information and then observe salient parts for more detailed clues,\nPANet and PMNet construct a two-stage attention structure to perform a\ncoarse-to-fine search among identities. Finally, we address this Re-ID issue as\na multi-task problem, including global feature learning, identity\nclassification, and part transfer. We adopt Homoscedastic Uncertainty to learn\nthe optimal weighing of different losses. Comprehensive experiments are\nconducted on two benchmark datasets. Our approach outperforms recent\nstate-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in\nmAP on VeRi776. Results on occluded test sets also demonstrate the\ngeneralization ability of PMANet.",
          "link": "http://arxiv.org/abs/2107.08228",
          "publishedOn": "2021-07-20T02:04:41.369Z",
          "wordCount": 733,
          "title": "Looking Twice for Partial Clues: Weakly-supervised Part-Mentored Attention Network for Vehicle Re-Identification. (arXiv:2107.08228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haiwen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>",
          "description": "In fine-grained image recognition (FGIR), the localization and amplification\nof region attention is an important factor, which has been explored a lot by\nconvolutional neural networks (CNNs) based approaches. The recently developed\nvision transformer (ViT) has achieved promising results on computer vision\ntasks. Compared with CNNs, Image sequentialization is a brand new manner.\nHowever, ViT is limited in its receptive field size and thus lacks local\nattention like CNNs due to the fixed size of its patches, and is unable to\ngenerate multi-scale features to learn discriminative region attention. To\nfacilitate the learning of discriminative region attention without box/part\nannotations, we use the strength of the attention weights to measure the\nimportance of the patch tokens corresponding to the raw images. We propose the\nrecurrent attention multi-scale transformer (RAMS-Trans), which uses the\ntransformer's self-attention to recursively learn discriminative region\nattention in a multi-scale manner. Specifically, at the core of our approach\nlies the dynamic patch proposal module (DPPM) guided region amplification to\ncomplete the integration of multi-scale image patches. The DPPM starts with the\nfull-size image patches and iteratively scales up the region attention to\ngenerate new patches from global to local by the intensity of the attention\nweights generated at each scale as an indicator. Our approach requires only the\nattention weights that come with ViT itself and can be easily trained\nend-to-end. Extensive experiments demonstrate that RAMS-Trans performs better\nthan concurrent works, in addition to efficient CNN models, achieving\nstate-of-the-art results on three benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.08192",
          "publishedOn": "2021-07-20T02:04:41.304Z",
          "wordCount": 689,
          "title": "RAMS-Trans: Recurrent Attention Multi-scale Transformer forFine-grained Image Recognition. (arXiv:2107.08192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Convolutional neural network (CNN)-based stereo matching approaches generally\nrequire a dense cost volume (DCV) for disparity estimation. However, generating\nsuch cost volumes is computationally-intensive and memory-consuming, hindering\nCNN training and inference efficiency. To address this problem, we propose\nSCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching\nfrom sparse cost volume (SCV) representations. Our inspiration is derived from\nthe fact that DCV representations are somewhat redundant and can be replaced\nwith SCV representations. Benefiting from these SCV representations, our\nSCV-Stereo can update disparity estimations in an iterative fashion for\naccurate and efficient stereo matching. Extensive experiments carried out on\nthe KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly\nminimize the trade-off between accuracy and efficiency for stereo matching. Our\nproject page is https://sites.google.com/view/scv-stereo.",
          "link": "http://arxiv.org/abs/2107.08187",
          "publishedOn": "2021-07-20T02:04:41.252Z",
          "wordCount": 582,
          "title": "SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume. (arXiv:2107.08187v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Stereo matching is a key component of autonomous driving perception. Recent\nunsupervised stereo matching approaches have received adequate attention due to\ntheir advantage of not requiring disparity ground truth. These approaches,\nhowever, perform poorly near occlusions. To overcome this drawback, in this\npaper, we propose CoT-Stereo, a novel unsupervised stereo matching approach.\nSpecifically, we adopt a co-teaching framework where two networks interactively\nteach each other about the occlusions in an unsupervised fashion, which greatly\nimproves the robustness of unsupervised stereo matching. Extensive experiments\non the KITTI Stereo benchmarks demonstrate the superior performance of\nCoT-Stereo over all other state-of-the-art unsupervised stereo matching\napproaches in terms of both accuracy and speed. Our project webpage is\nhttps://sites.google.com/view/cot-stereo.",
          "link": "http://arxiv.org/abs/2107.08186",
          "publishedOn": "2021-07-20T02:04:41.214Z",
          "wordCount": 566,
          "title": "Co-Teaching: An Ark to Unsupervised Stereo Matching. (arXiv:2107.08186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08120",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>",
          "description": "Magnetic resonance Fingerprinting (MRF) is a relatively new multi-parametric\nquantitative imaging method that involves a two-step process: (i)\nreconstructing a series of time frames from highly-undersampled non-Cartesian\nspiral k-space data and (ii) pattern matching using the time frames to infer\ntissue properties (e.g., T1 and T2 relaxation times). In this paper, we\nintroduce a novel end-to-end deep learning framework to seamlessly map the\ntissue properties directly from spiral k-space MRF data, thereby avoiding\ntime-consuming processing such as the nonuniform fast Fourier transform (NUFFT)\nand the dictionary-based Fingerprint matching. Our method directly consumes the\nnon-Cartesian k- space data, performs adaptive density compensation, and\npredicts multiple tissue property maps in one forward pass. Experiments on both\n2D and 3D MRF data demonstrate that quantification accuracy comparable to\nstate-of-the-art methods can be accomplished within 0.5 second, which is 1100\nto 7700 times faster than the original MRF framework. The proposed method is\nthus promising for facilitating the adoption of MRF in clinical settings.",
          "link": "http://arxiv.org/abs/2107.08120",
          "publishedOn": "2021-07-20T02:04:40.658Z",
          "wordCount": 610,
          "title": "Real-Time Mapping of Tissue Properties for Magnetic Resonance Fingerprinting. (arXiv:2107.08120v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:40.602Z",
          "wordCount": 589,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruojin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>",
          "description": "We present a technique for estimating the relative 3D rotation of an RGB\nimage pair in an extreme setting, where the images have little or no overlap.\nWe observe that, even when images do not overlap, there may be rich hidden cues\nas to their geometric relationship, such as light source directions, vanishing\npoints, and symmetries present in the scene. We propose a network design that\ncan automatically learn such implicit cues by comparing all pairs of points\nbetween the two input images. Our method therefore constructs dense feature\ncorrelation volumes and processes these to predict relative 3D rotations. Our\npredictions are formed over a fine-grained discretization of rotations,\nbypassing difficulties associated with regressing 3D rotations. We demonstrate\nour approach on a large variety of extreme RGB image pairs, including indoor\nand outdoor images captured under different lighting conditions and geographic\nlocations. Our evaluation shows that our model can successfully estimate\nrelative rotations among non-overlapping images without compromising\nperformance over overlapping image pairs.",
          "link": "http://arxiv.org/abs/2104.13530",
          "publishedOn": "2021-07-20T02:04:40.583Z",
          "wordCount": 635,
          "title": "Extreme Rotation Estimation using Dense Correlation Volumes. (arXiv:2104.13530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:40.526Z",
          "wordCount": 596,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.14595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_J/0/1/0/all/0/1\">Jennifer Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_C/0/1/0/all/0/1\">Christian Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroger_C/0/1/0/all/0/1\">Christine Kr&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_J/0/1/0/all/0/1\">J&#xf6;rn Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "In many research areas, scientific progress is accelerated by\nmultidisciplinary access to image data and their interdisciplinary annotation.\nHowever, keeping track of these annotations to ensure a high-quality\nmulti-purpose data set is a challenging and labour intensive task. We developed\nthe open-source online platform EXACT (EXpert Algorithm Collaboration Tool)\nthat enables the collaborative interdisciplinary analysis of images from\ndifferent domains online and offline. EXACT supports multi-gigapixel medical\nwhole slide images as well as image series with thousands of images. The\nsoftware utilises a flexible plugin system that can be adapted to diverse\napplications such as counting mitotic figures with a screening mode, finding\nfalse annotations on a novel validation view, or using the latest deep learning\nimage analysis technologies. This is combined with a version control system\nwhich makes it possible to keep track of changes in the data sets and, for\nexample, to link the results of deep learning experiments to specific data set\nversions. EXACT is freely available and has already been successfully applied\nto a broad range of annotation tasks, including highly diverse applications\nlike deep learning supported cytology scoring, interdisciplinary multi-centre\nwhole slide image tumour annotation, and highly specialised whale sound\nspectroscopy clustering.",
          "link": "http://arxiv.org/abs/2004.14595",
          "publishedOn": "2021-07-20T02:04:40.502Z",
          "wordCount": 712,
          "title": "EXACT: A collaboration toolset for algorithm-aided annotation of images with annotation version control. (arXiv:2004.14595v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_L/0/1/0/all/0/1\">Laurent Lejeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1\">Raphael Sznitman</a>",
          "description": "The ability to quickly annotate medical imaging data plays a critical role in\ntraining deep learning frameworks for segmentation. Doing so for image volumes\nor video sequences is even more pressing as annotating these is particularly\nburdensome. To alleviate this problem, this work proposes a new method to\nefficiently segment medical imaging volumes or videos using point-wise\nannotations only. This allows annotations to be collected extremely quickly and\nremains applicable to numerous segmentation tasks. Our approach trains a deep\nlearning model using an appropriate Positive/Unlabeled objective function using\nsparse point-wise annotations. While most methods of this kind assume that the\nproportion of positive samples in the data is known a-priori, we introduce a\nnovel self-supervised method to estimate this prior efficiently by combining a\nBayesian estimation framework and new stopping criteria. Our method iteratively\nestimates appropriate class priors and yields high segmentation quality for a\nvariety of object types and imaging modalities. In addition, by leveraging a\nspatio-temporal tracking framework, we regularize our predictions by leveraging\nthe complete data volume. We show experimentally that our approach outperforms\nstate-of-the-art methods tailored to the same problem.",
          "link": "http://arxiv.org/abs/2107.08394",
          "publishedOn": "2021-07-20T02:04:40.480Z",
          "wordCount": 623,
          "title": "A Positive/Unlabeled Approach for the Segmentation of Medical Sequences using Point-Wise Supervision. (arXiv:2107.08394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-19T01:59:51.012Z",
          "wordCount": 727,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-19T01:59:50.979Z",
          "wordCount": 673,
          "title": "Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>",
          "description": "Graph convolutional networks have significantly improved 3D human pose\nestimation by representing the human skeleton as an undirected graph. However,\nthis representation fails to reflect the articulated characteristic of human\nskeletons as the hierarchical orders among the joints are not explicitly\npresented. In this paper, we propose to represent the human skeleton as a\ndirected graph with the joints as nodes and bones as edges that are directed\nfrom parent joints to child joints. By so doing, the directions of edges can\nexplicitly reflect the hierarchical relationships among the nodes. Based on\nthis representation, we adopt the spatial-temporal directed graph convolution\n(ST-DGConv) to extract features from 2D poses represented in a temporal\nsequence of directed graphs. We further propose a spatial-temporal conditional\ndirected graph convolution (ST-CondDGConv) to leverage varying non-local\ndependence for different poses by conditioning the graph topology on input\nposes. Altogether, we form a U-shaped network with ST-DGConv and ST-CondDGConv\nlayers, named U-shaped Conditional Directed Graph Convolutional Network\n(U-CondDGCN), for 3D human pose estimation from monocular videos. To evaluate\nthe effectiveness of our U-CondDGCN, we conducted extensive experiments on two\nchallenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both\nquantitative and qualitative results show that our method achieves top\nperformance. Also, ablation studies show that directed graphs can better\nexploit the hierarchy of articulated human skeletons than undirected graphs,\nand the conditional connections can yield adaptive graph topologies for\ndifferent kinds of poses.",
          "link": "http://arxiv.org/abs/2107.07797",
          "publishedOn": "2021-07-19T00:49:08.141Z",
          "wordCount": 683,
          "title": "Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>",
          "description": "Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.",
          "link": "http://arxiv.org/abs/2107.07933",
          "publishedOn": "2021-07-19T00:49:08.120Z",
          "wordCount": 605,
          "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:08.097Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_Y/0/1/0/all/0/1\">Yugo Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_D/0/1/0/all/0/1\">Delong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_Y/0/1/0/all/0/1\">Yukinobu Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinami_R/0/1/0/all/0/1\">Ryota Hinami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishiwatari_S/0/1/0/all/0/1\">Shonosuke Ishiwatari</a>",
          "description": "Japanese comics (called manga) are traditionally created in monochrome\nformat. In recent years, in addition to monochrome comics, full color comics, a\nmore attractive medium, have appeared. Unfortunately, color comics require\nmanual colorization, which incurs high labor costs. Although automatic\ncolorization methods have been recently proposed, most of them are designed for\nillustrations, not for comics. Unlike illustrations, since comics are composed\nof many consecutive images, the painting style must be consistent. To realize\nconsistent colorization, we propose here a semi-automatic colorization method\nbased on generative adversarial networks (GAN); the method learns the painting\nstyle of a specific comic from small amount of training data. The proposed\nmethod takes a pair of a screen tone image and a flat colored image as input,\nand outputs a colorized image. Experiments show that the proposed method\nachieves better performance than the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.07943",
          "publishedOn": "2021-07-19T00:49:08.092Z",
          "wordCount": 595,
          "title": "Painting Style-Aware Manga Colorization Based on Generative Adversarial Networks. (arXiv:2107.07943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>",
          "description": "This paper presents a novel unsupervised approach to reconstruct human shape\nand pose from noisy point cloud. Traditional approaches search for\ncorrespondences and conduct model fitting iteratively where a good\ninitialization is critical. Relying on large amount of dataset with\nground-truth annotations, recent learning-based approaches predict\ncorrespondences for every vertice on the point cloud; Chamfer distance is\nusually used to minimize the distance between a deformed template model and the\ninput point cloud. However, Chamfer distance is quite sensitive to noise and\noutliers, thus could be unreliable to assign correspondences. To address these\nissues, we model the probability distribution of the input point cloud as\ngenerated from a parametric human model under a Gaussian Mixture Model. Instead\nof explicitly aligning correspondences, we treat the process of correspondence\nsearch as an implicit probabilistic association by updating the posterior\nprobability of the template model given the input. A novel unsupervised loss is\nfurther derived that penalizes the discrepancy between the deformed template\nand the input point cloud conditioned on the posterior probability. Our\napproach is very flexible, which works with both complete point cloud and\nincomplete ones including even a single depth image as input. Our network is\ntrained from scratch with no need to warm-up the network with supervised data.\nCompared to previous unsupervised methods, our method shows the capability to\ndeal with substantial noise and outliers. Extensive experiments conducted on\nvarious public synthetic datasets as well as a very noisy real dataset (i.e.\nCMU Panoptic) demonstrate the superior performance of our approach over the\nstate-of-the-art methods. Code can be found\n\\url{https://github.com/wangsen1312/unsupervised3dhuman.git}",
          "link": "http://arxiv.org/abs/2107.07539",
          "publishedOn": "2021-07-19T00:49:07.772Z",
          "wordCount": 701,
          "title": "Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds. (arXiv:2107.07539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1\">Ramprasaath R. Selvaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>",
          "description": "Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.",
          "link": "http://arxiv.org/abs/2107.07651",
          "publishedOn": "2021-07-19T00:49:07.747Z",
          "wordCount": 662,
          "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. (arXiv:2107.07651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "In recent years, deep learning has made brilliant achievements in image\nclassification. However, image classification of small datasets is still not\nobtained good research results. This article first briefly explains the\napplication and characteristics of convolutional neural networks and visual\ntransformers. Meanwhile, the influence of small data set on classification and\nthe solution are introduced. Then a series of experiments are carried out on\nthe small datasets by using various models, and the problems of some models in\nthe experiments are discussed. Through the comparison of experimental results,\nthe recommended deep learning model is given according to the model application\nenvironment. Finally, we give directions for future work.",
          "link": "http://arxiv.org/abs/2107.07699",
          "publishedOn": "2021-07-19T00:49:07.742Z",
          "wordCount": 571,
          "title": "A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Converlutional Neural Networks to Visual Transformers. (arXiv:2107.07699v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.530Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_D/0/1/0/all/0/1\">Dong Yue</a>",
          "description": "In recent years, how to strike a good trade-off between accuracy and\ninference speed has become the core issue for real-time semantic segmentation\napplications, which plays a vital role in real-world scenarios such as\nautonomous driving systems and drones. In this study, we devise a novel\nlightweight network using a multi-scale context fusion (MSCFNet) scheme, which\nexplores an asymmetric encoder-decoder architecture to dispose this problem.\nMore specifically, the encoder adopts some developed efficient asymmetric\nresidual (EAR) modules, which are composed of factorization depth-wise\nconvolution and dilation convolution. Meanwhile, instead of complicated\ncomputation, simple deconvolution is applied in the decoder to further reduce\nthe amount of parameters while still maintaining high segmentation accuracy.\nAlso, MSCFNet has branches with efficient attention modules from different\nstages of the network to well capture multi-scale contextual information. Then\nwe combine them before the final classification to enhance the expression of\nthe features and improve the segmentation efficiency. Comprehensive experiments\non challenging datasets have demonstrated that the proposed MSCFNet, which\ncontains only 1.15M parameters, achieves 71.9\\% Mean IoU on the Cityscapes\ntesting dataset and can run at over 50 FPS on a single Titan XP GPU\nconfiguration.",
          "link": "http://arxiv.org/abs/2103.13044",
          "publishedOn": "2021-07-19T00:49:07.523Z",
          "wordCount": 679,
          "title": "MSCFNet: A Lightweight Network With Multi-Scale Context Fusion for Real-Time Semantic Segmentation. (arXiv:2103.13044v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-19T00:49:07.510Z",
          "wordCount": 581,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.433Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse%7F_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price%7F_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer%7F_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiginton_C/0/1/0/all/0/1\">Curtis Wiginton</a>",
          "description": "We address the problem of form understanding: finding text entities and the\nrelationships/links between them in form images. The proposed FUDGE model\nformulates this problem on a graph of text elements (the vertices) and uses a\nGraph Convolutional Network to predict changes to the graph. The initial\nvertices are detected text lines and do not necessarily correspond to the final\ntext entities, which can span multiple lines. Also, initial edges contain many\nfalse-positive relationships. FUDGE edits the graph structure by combining text\nsegments (graph vertices) and pruning edges in an iterative fashion to obtain\nthe final text entities and relationships. While recent work in this area has\nfocused on leveraging large-scale pre-trained Language Models (LM), FUDGE\nachieves almost the same level of entity linking performance on the FUNSD\ndataset by learning only visual features from the (small) provided training\nset. FUDGE can be applied on forms where text recognition is difficult (e.g.\ndegraded or historical forms) and on forms in resource-poor languages where\npre-training such LMs is challenging. FUDGE is state-of-the-art on the\nhistorical NAF dataset.",
          "link": "http://arxiv.org/abs/2105.08194",
          "publishedOn": "2021-07-19T00:49:06.730Z",
          "wordCount": 652,
          "title": "Visual FUDGE: Form Understanding via Dynamic Graph Editing. (arXiv:2105.08194v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1\">Katrina Ask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Orthopedic disorders are a common cause for euthanasia among horses, which\noften could have been avoided with earlier detection. These conditions often\ncreate varying degrees of subtle but long-term pain. It is challenging to train\na visual pain recognition method with video data depicting such pain, since the\nresulting pain behavior also is subtle, sparsely appearing, and varying, making\nit challenging for even an expert human labeler to provide accurate\nground-truth for the data. We show that transferring features from a dataset of\nhorses with acute nociceptive pain (where labeling is less ambiguous) can aid\nthe learning to recognize more complex orthopedic pain. Moreover, we present a\nhuman expert baseline for the problem, as well as an extensive empirical study\nof various domain transfer methods and of what is detected by the pain\nrecognition method trained on acute pain in the orthopedic dataset. Finally,\nthis is accompanied with a discussion around the challenges posed by real-world\nanimal behavior datasets and how best practices can be established for similar\nfine-grained action recognition tasks. Our code is available at\nhttps://github.com/sofiabroome/painface-recognition.",
          "link": "http://arxiv.org/abs/2105.10313",
          "publishedOn": "2021-07-19T00:49:06.704Z",
          "wordCount": 665,
          "title": "Sharing Pain: Using Domain Transfer Between Pain Types for Recognition of Sparse Pain Expressions in Horses. (arXiv:2105.10313v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-19T00:49:06.626Z",
          "wordCount": 620,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:06.597Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Despite remarkable progress achieved, most neural architecture search (NAS)\nmethods focus on searching for one single accurate and robust architecture. To\nfurther build models with better generalization capability and performance,\nmodel ensemble is usually adopted and performs better than stand-alone models.\nInspired by the merits of model ensemble, we propose to search for multiple\ndiverse models simultaneously as an alternative way to find powerful models.\nSearching for ensembles is non-trivial and has two key challenges: enlarged\nsearch space and potentially more complexity for the searched model. In this\npaper, we propose a one-shot neural ensemble architecture search (NEAS)\nsolution that addresses the two challenges. For the first challenge, we\nintroduce a novel diversity-based metric to guide search space shrinking,\nconsidering both the potentiality and diversity of candidate operators. For the\nsecond challenge, we enable a new search dimension to learn layer sharing among\ndifferent models for efficiency purposes. The experiments on ImageNet clearly\ndemonstrate that our solution can improve the supernet's capacity of ranking\nensemble architectures, and further lead to better search results. The\ndiscovered architectures achieve superior performance compared with\nstate-of-the-arts such as MobileNetV3 and EfficientNet families under aligned\nsettings. Moreover, we evaluate the generalization ability and robustness of\nour searched architecture on the COCO detection benchmark and achieve a 3.1%\nimprovement on AP compared with MobileNetV3. Codes and models are available at\nhttps://github.com/researchmm/NEAS.",
          "link": "http://arxiv.org/abs/2104.00597",
          "publishedOn": "2021-07-19T00:49:06.565Z",
          "wordCount": 700,
          "title": "One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking. (arXiv:2104.00597v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:06.547Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junwei Liang</a>",
          "description": "With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.",
          "link": "http://arxiv.org/abs/2011.10670",
          "publishedOn": "2021-07-19T00:49:06.497Z",
          "wordCount": 626,
          "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:06.477Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.11150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jihun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "In this work, we attempt to explain the prediction of any black-box\nclassifier from an information-theoretic perspective. For each input feature,\nwe compare the classifier outputs with and without that feature using two\ninformation-theoretic metrics. Accordingly, we obtain two attribution maps--an\ninformation gain (IG) map and a point-wise mutual information (PMI) map. IG map\nprovides a class-independent answer to \"How informative is each pixel?\", and\nPMI map offers a class-specific explanation of \"How much does each pixel\nsupport a specific class?\" Compared to existing methods, our method improves\nthe correctness of the attribution maps in terms of a quantitative metric. We\nalso provide a detailed analysis of an ImageNet classifier using the proposed\nmethod, and the code is available online.",
          "link": "http://arxiv.org/abs/2009.11150",
          "publishedOn": "2021-07-19T00:49:06.470Z",
          "wordCount": 580,
          "title": "Information-Theoretic Visual Explanation for Black-Box Classifiers. (arXiv:2009.11150v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardenas_B/0/1/0/all/0/1\">Bryan G. Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Devanshu Arya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>",
          "description": "Recent developments related to generative models have made it possible to\ngenerate diverse high-fidelity images. In particular, layout-to-image\ngeneration models have gained significant attention due to their capability to\ngenerate realistic complex images containing distinct objects. These models are\ngenerally conditioned on either semantic layouts or textual descriptions.\nHowever, unlike natural images, providing auxiliary information can be\nextremely hard in domains such as biomedical imaging and remote sensing. In\nthis work, we propose a multi-object generation framework that can synthesize\nimages with multiple objects without explicitly requiring their contextual\ninformation during the generation process. Based on a vector-quantized\nvariational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial\ncoherency within an image as well as semantic coherency between the objects and\nthe background through two powerful autoregressive priors: PixelSNAIL and\nLayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent\nencodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the\nsemantic distribution of the objects. An implicit advantage of our approach is\nthat the generated samples are accompanied by object-level annotations. We\ndemonstrate how coherency and fidelity are preserved with our method through\nexperiments on the Multi-MNIST and CLEVR datasets; thereby outperforming\nstate-of-the-art multi-object generative methods. The efficacy of our approach\nis demonstrated through application on medical imaging datasets, where we show\nthat augmenting the training set with generated samples using our approach\nimproves the performance of existing models.",
          "link": "http://arxiv.org/abs/2006.12150",
          "publishedOn": "2021-07-19T00:49:06.464Z",
          "wordCount": 711,
          "title": "Generating Annotated High-Fidelity Images Containing Multiple Coherent Objects. (arXiv:2006.12150v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:06.449Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">Joseph P. Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Ming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Kinship is a soft biometric detectable in media with an abundance of\npractical applications. Despite the difficulty of detecting kinship, annual\ndata challenges using still-images have consistently improved performances and\nattracted new researchers. Now, systems reach performance levels unforeseeable\na decade ago, closing in on performances acceptable to deploy in practice.\nSimilar to other biometric tasks, we expect systems can benefit from additional\nmodalities. We hypothesize that adding modalities to FIW, which contains only\nstill-images, will improve performance. Thus, to narrow the gap between\nresearch and reality and enhance the power of kinship recognition systems, we\nextend FIW with multimedia (MM) data (i.e., video, audio, and text captions).\nSpecifically, we introduce the first publicly available multi-task MM kinship\ndataset. To build FIW MM, we developed machinery to automatically collect,\nannotate, and prepare the data, requiring minimal human input and no financial\ncost. The proposed MM corpus allows the problem statements to be more realistic\ntemplate-based protocols. We show significant improvements in all benchmarks\nwith the added modalities. The results highlight edge cases to inspire future\nresearch with different areas of improvement. FIW MM provides the data required\nto increase the potential of automated systems to detect kinship in MM. It also\nallows experts from diverse fields to collaborate in novel ways.",
          "link": "http://arxiv.org/abs/2007.14509",
          "publishedOn": "2021-07-19T00:49:06.444Z",
          "wordCount": 706,
          "title": "Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xudong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Generative Adversarial Networks (GANs) have been widely-used in image\ntranslation, but their high computational and storage costs impede the\ndeployment on mobile devices. Prevalent methods for CNN compression cannot be\ndirectly applied to GANs due to the complicated generator architecture and the\nunstable adversarial training. To solve these, in this paper, we introduce a\nnovel GAN compression method, termed DMAD, by proposing a Differentiable Mask\nand a co-Attention Distillation. The former searches for a light-weight\ngenerator architecture in a training-adaptive manner. To overcome channel\ninconsistency when pruning the residual connections, an adaptive cross-block\ngroup sparsity is further incorporated. The latter simultaneously distills\ninformative attention maps from both the generator and discriminator of a\npre-trained model to the searched generator, effectively stabilizing the\nadversarial training of our light-weight model. Experiments show that DMAD can\nreduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13$\\times$ and\nthat of Pix2Pix by 4$\\times$ while retaining a comparable performance against\nthe full model. Our code can be available at https://github.com/SJLeo/DMAD.",
          "link": "http://arxiv.org/abs/2011.08382",
          "publishedOn": "2021-07-19T00:49:06.438Z",
          "wordCount": 663,
          "title": "Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:06.431Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:06.425Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1\">Guillaume Le Moing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "This presentation introduces a self-supervised learning approach to the\nsynthesis of new video clips from old ones, with several new key elements for\nimproved spatial resolution and realism: It conditions the synthesis process on\ncontextual information for temporal continuity and ancillary information for\nfine control. The prediction model is doubly autoregressive, in the latent\nspace of an autoencoder for forecasting, and in image space for updating\ncontextual information, which is also used to enforce spatio-temporal\nconsistency through a learnable optical flow module. Adversarial training of\nthe autoencoder in the appearance and temporal domains is used to further\nimprove the realism of its output. A quantizer inserted between the encoder and\nthe transformer in charge of forecasting future frames in latent space (and its\ninverse inserted between the transformer and the decoder) adds even more\nflexibility by affording simple mechanisms for handling multimodal ancillary\ninformation for controlling the synthesis process (eg, a few sample frames, an\naudio track, a trajectory in image space) and taking into account the\nintrinsically uncertain nature of the future by allowing multiple predictions.\nExperiments with an implementation of the proposed approach give very good\nqualitative and quantitative results on multiple tasks and standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08037",
          "publishedOn": "2021-07-19T00:49:06.418Z",
          "wordCount": 628,
          "title": "CCVS: Context-aware Controllable Video Synthesis. (arXiv:2107.08037v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Flow-based generative models have recently become one of the most efficient\napproaches to model data generation. Indeed, they are constructed with a\nsequence of invertible and tractable transformations. Glow first introduced a\nsimple type of generative flow using an invertible $1 \\times 1$ convolution.\nHowever, the $1 \\times 1$ convolution suffers from limited flexibility compared\nto the standard convolutions. In this paper, we propose a novel invertible $n\n\\times n$ convolution approach that overcomes the limitations of the invertible\n$1 \\times 1$ convolution. In addition, our proposed network is not only\ntractable and invertible but also uses fewer parameters than standard\nconvolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have\nshown that our invertible $n \\times n$ convolution helps to improve the\nperformance of generative models significantly.",
          "link": "http://arxiv.org/abs/1905.10170",
          "publishedOn": "2021-07-19T00:49:06.400Z",
          "wordCount": 594,
          "title": "Generative Flow via Invertible nxn Convolution. (arXiv:1905.10170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chull Hwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hye Joo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>",
          "description": "We address representation learning for large-scale instance-level image\nretrieval. Apart from backbone, training pipelines and loss functions, popular\napproaches have focused on different spatial pooling and attention mechanisms,\nwhich are at the core of learning a powerful global image representation. There\nare different forms of attention according to the interaction of elements of\nthe feature tensor (local and global) and the dimensions where it is applied\n(spatial and channel). Unfortunately, each study addresses only one or two\nforms of attention and applies it to different problems like classification,\ndetection or retrieval.\n\nWe present global-local attention module (GLAM), which is attached at the end\nof a backbone network and incorporates all four forms of attention: local and\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\npooling, we learn a powerful embedding for image retrieval. Focusing on global\ndescriptors, we provide empirical evidence of the interaction of all forms of\nattention and improve the state of the art on standard benchmarks.",
          "link": "http://arxiv.org/abs/2107.08000",
          "publishedOn": "2021-07-19T00:49:06.393Z",
          "wordCount": 606,
          "title": "All the attention you need: Global-local, spatial-channel attention for image retrieval. (arXiv:2107.08000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_B/0/1/0/all/0/1\">Brandon Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "The self-supervised loss formulation for jointly training depth and egomotion\nneural networks with monocular images is well studied and has demonstrated\nstate-of-the-art accuracy. One of the main limitations of this approach,\nhowever, is that the depth and egomotion estimates are only determined up to an\nunknown scale. In this paper, we present a novel scale recovery loss that\nenforces consistency between a known camera height and the estimated camera\nheight, generating metric (scaled) depth and egomotion predictions. We show\nthat our proposed method is competitive with other scale recovery techniques\nthat require more information. Further, we demonstrate that our method\nfacilitates network retraining within new environments, whereas other\nscale-resolving approaches are incapable of doing so. Notably, our egomotion\nnetwork is able to produce more accurate estimates than a similar method which\nrecovers scale at test time only.",
          "link": "http://arxiv.org/abs/2009.03787",
          "publishedOn": "2021-07-19T00:49:06.379Z",
          "wordCount": 630,
          "title": "Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07907",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kanglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Gaofeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1\">Jiang Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>",
          "description": "Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2107.07907",
          "publishedOn": "2021-07-19T00:49:06.350Z",
          "wordCount": 683,
          "title": "Lightness Modulated Deep Inverse Tone Mapping. (arXiv:2107.07907v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Josse_E/0/1/0/all/0/1\">Elias Josse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nerborg_A/0/1/0/all/0/1\">Amanda Nerborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>",
          "description": "The world is expecting an aging population and shortage of healthcare\nprofessionals. This poses the problem of providing a safe and dignified life\nfor the elderly. Technological solutions involving cameras can contribute to\nsafety, comfort and efficient emergency responses, but they are invasive of\nprivacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a\nlow-resolution infrared thermopile array sensor, which offers more privacy.\nMounted over a bed, it can determine if the user is on the bed or not without\nhuman interaction. For this purpose, two datasets were captured, one (480\nimages) under constant conditions, and a second one (200 images) under\ndifferent variations such as use of a duvet, sleeping with a pet, or increased\nroom temperature. We test three machine learning algorithms: Support Vector\nMachines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With\n10-fold cross validation, the highest accuracy in the main dataset is for both\nSVM and k-NN (99%). The results with variable data show a lower reliability\nunder certain circumstances, highlighting the need of extra work to meet the\nchallenge of variations in the environment.",
          "link": "http://arxiv.org/abs/2107.07986",
          "publishedOn": "2021-07-19T00:49:06.343Z",
          "wordCount": 628,
          "title": "In-Bed Person Monitoring Using Thermal Infrared Sensors. (arXiv:2107.07986v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizhi Lu</a>",
          "description": "Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.",
          "link": "http://arxiv.org/abs/2107.07987",
          "publishedOn": "2021-07-19T00:49:06.308Z",
          "wordCount": 532,
          "title": "Deep Learning to Ternary Hash Codes by Continuation. (arXiv:2107.07987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fabbrizzi_S/0/1/0/all/0/1\">Simone Fabbrizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>",
          "description": "Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in major discrimination if not dealt\nwith proper care. CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, both the problems of\nunderstanding and discovering biases are of utmost importance. Yet, to date\nthere is no comprehensive survey on bias in visual datasets. To this end, this\nwork aims to: i) describe the biases that can affect visual datasets; ii)\nreview the literature on methods for bias discovery and quantification in\nvisual datasets; iii) discuss existing attempts to collect bias-aware visual\ndatasets. A key conclusion of our study is that the problem of bias discovery\nand quantification in visual datasets is still open and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed; moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist that can be used to\nspot different types of bias during visual dataset collection.",
          "link": "http://arxiv.org/abs/2107.07919",
          "publishedOn": "2021-07-19T00:49:06.301Z",
          "wordCount": 632,
          "title": "A Survey on Bias in Visual Datasets. (arXiv:2107.07919v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "We study the problem of inferring an object-centric scene representation from\na single image, aiming to derive a representation that explains the image\nformation process, captures the scene's 3D nature, and is learned without\nsupervision. Most existing methods on scene decomposition lack one or more of\nthese characteristics, due to the fundamental challenge in integrating the\ncomplex 3D-to-2D image formation process into powerful inference schemes like\ndeep networks. In this paper, we propose unsupervised discovery of Object\nRadiance Fields (uORF), integrating recent progresses in neural 3D scene\nrepresentations and rendering with deep inference networks for unsupervised 3D\nscene decomposition. Trained on multi-view RGB images without annotations, uORF\nlearns to decompose complex scenes with diverse, textured background from a\nsingle image. We show that uORF performs well on unsupervised 3D scene\nsegmentation, novel view synthesis, and scene editing on three datasets.",
          "link": "http://arxiv.org/abs/2107.07905",
          "publishedOn": "2021-07-19T00:49:06.288Z",
          "wordCount": 580,
          "title": "Unsupervised Discovery of Object Radiance Fields. (arXiv:2107.07905v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engel_N/0/1/0/all/0/1\">Nico Engel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>",
          "description": "We present a vehicle self-localization method using point-based deep neural\nnetworks. Our approach processes measurements and point features, i.e.\nlandmarks, from a high-definition digital map to infer the vehicle's pose. To\nlearn the best association and incorporate local information between the point\nsets, we propose an attention mechanism that matches the measurements to the\ncorresponding landmarks. Finally, we use this representation for the\npoint-cloud registration and the subsequent pose regression task. Furthermore,\nwe introduce a training simulation framework that artificially generates\nmeasurements and landmarks to facilitate the deployment process and reduce the\ncost of creating extensive datasets from real-world data. We evaluate our\nmethod on our dataset, as well as an adapted version of the Kitti odometry\ndataset, where we achieve superior performance compared to related approaches;\nand additionally show dominant generalization capabilities.",
          "link": "http://arxiv.org/abs/2107.07787",
          "publishedOn": "2021-07-19T00:49:06.279Z",
          "wordCount": 583,
          "title": "Attention-based Vehicle Self-Localization with HD Feature Maps. (arXiv:2107.07787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1\">Mathieu Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>",
          "description": "This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.",
          "link": "http://arxiv.org/abs/2107.07789",
          "publishedOn": "2021-07-19T00:49:06.272Z",
          "wordCount": 664,
          "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees. (arXiv:2107.07789v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07826",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blekos_K/0/1/0/all/0/1\">Kostas Blekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S Lalos</a>",
          "description": "Delineation approaches provide significant benefits to various domains,\nincluding agriculture, environmental and natural disasters monitoring. Most of\nthe work in the literature utilize traditional segmentation methods that\nrequire a large amount of computational and storage resources. Deep learning\nhas transformed computer vision and dramatically improved machine translation,\nthough it requires massive dataset for training and significant resources for\ninference. More importantly, energy-efficient embedded vision hardware\ndelivering real-time and robust performance is crucial in the aforementioned\napplication. In this work, we propose a U-Net based tree delineation method,\nwhich is effectively trained using multi-spectral imagery but can then\ndelineate single-spectrum images. The deep architecture that also performs\nlocalization, i.e., a class label corresponds to each pixel, has been\nsuccessfully used to allow training with a small set of segmented images. The\nground truth data were generated using traditional image denoising and\nsegmentation approaches. To be able to execute the proposed DNN efficiently in\nembedded platforms designed for deep learning approaches, we employ traditional\nmodel compression and acceleration methods. Extensive evaluation studies using\ndata collected from UAVs equipped with multi-spectral cameras demonstrate the\neffectiveness of the proposed methods in terms of delineation accuracy and\nexecution efficiency.",
          "link": "http://arxiv.org/abs/2107.07826",
          "publishedOn": "2021-07-19T00:49:06.255Z",
          "wordCount": 657,
          "title": "Efficient automated U-Net based tree crown delineation using UAV multi-spectral imagery on embedded devices. (arXiv:2107.07826v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runde Li</a>",
          "description": "To solve the issue of video dehazing, there are two main tasks to attain: how\nto align adjacent frames to the reference frame; how to restore the reference\nframe. Some papers adopt explicit approaches (e.g., the Markov random field,\noptical flow, deformable convolution, 3D convolution) to align neighboring\nframes with the reference frame in feature space or image space, they then use\nvarious restoration methods to achieve the final dehazing results. In this\npaper, we propose a progressive alignment and restoration method for video\ndehazing. The alignment process aligns consecutive neighboring frames stage by\nstage without using the optical flow estimation. The restoration process is not\nonly implemented under the alignment process but also uses a refinement network\nto improve the dehazing performance of the whole network. The proposed networks\ninclude four fusion networks and one refinement network. To decrease the\nparameters of networks, three fusion networks in the first fusion stage share\nthe same parameters. Extensive experiments demonstrate that the proposed video\ndehazing method achieves outstanding performance against the-state-of-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.07837",
          "publishedOn": "2021-07-19T00:49:06.241Z",
          "wordCount": 600,
          "title": "Progressive Deep Video Dehazing without Explicit Alignment Estimation. (arXiv:2107.07837v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07975",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Savioli_N/0/1/0/all/0/1\">Nicolo Savioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1\">Antonio de Marvao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_S/0/1/0/all/0/1\">Stuart A. Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chin_C/0/1/0/all/0/1\">Calvin W.L. Chin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1\">Declan P. O&#x27;Regan</a>",
          "description": "Optimising the analysis of cardiac structure and function requires accurate\n3D representations of shape and motion. However, techniques such as cardiac\nmagnetic resonance imaging are conventionally limited to acquiring contiguous\ncross-sectional slices with low through-plane resolution and potential\ninter-slice spatial misalignment. Super-resolution in medical imaging aims to\nincrease the resolution of images but is conventionally trained on features\nfrom low resolution datasets and does not super-resolve corresponding\nsegmentations. Here we propose a semi-supervised multi-task generative\nadversarial network (Gemini-GAN) that performs joint super-resolution of the\nimages and their labels using a ground truth of high resolution 3D cines and\nsegmentations, while an unsupervised variational adversarial mixture\nautoencoder (V-AMA) is used for continuous domain adaptation. Our proposed\napproach is extensively evaluated on two transnational multi-ethnic populations\nof 1,331 and 205 adults respectively, delivering an improvement on state of the\nart methods in terms of Dice index, peak signal to noise ratio, and structural\nsimilarity index measure. This framework also exceeds the performance of state\nof the art generative domain adaptation models on external validation (Dice\nindex 0.81 vs 0.74 for the left ventricle). This demonstrates how joint\nsuper-resolution and segmentation, trained on 3D ground-truth data with\ncross-domain generalization, enables robust precision phenotyping in diverse\npopulations.",
          "link": "http://arxiv.org/abs/2107.07975",
          "publishedOn": "2021-07-19T00:49:06.224Z",
          "wordCount": 667,
          "title": "Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed Adversarial Gaussian Domain Adaptation. (arXiv:2107.07975v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kannan_N/0/1/0/all/0/1\">Nagajothi Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_D/0/1/0/all/0/1\">Daya Sagar B S</a>",
          "description": "The study of water bodies such as rivers is an important problem in the\nremote sensing community. A meaningful set of quantitative features reflecting\nthe geophysical properties help us better understand the formation and\nevolution of rivers. Typically, river sub-basins are analysed using Cartosat\nDigital Elevation Models (DEMs), obtained at regular time epochs. One of the\nuseful geophysical features of a river sub-basin is that of a roughness measure\non DEMs. However, to the best of our knowledge, there is not much literature\navailable on theoretical analysis of roughness measures. In this article, we\nrevisit the roughness measure on DEM data adapted from multiscale\ngranulometries in mathematical morphology, namely multiscale directional\ngranulometric index (MDGI). This measure was classically used to obtain\nshape-size analysis in greyscale images. In earlier works, MDGIs were\nintroduced to capture the characteristic surficial roughness of a river\nsub-basin along specific directions. Also, MDGIs can be efficiently computed\nand are known to be useful features for classification of river sub-basins. In\nthis article, we provide a theoretical analysis of a MDGI. In particular, we\ncharacterize non-trivial sufficient conditions on the structure of DEMs under\nwhich MDGIs are invariant. These properties are illustrated with some\nfictitious DEMs. We also provide connections to a discrete derivative of volume\nof a DEM. Based on these connections, we provide intuition as to why a MDGI is\nconsidered a roughness measure. Further, we experimentally illustrate on\nLower-Indus, Wardha, and Barmer river sub-basins that the proposed features\ncapture the characteristics of the river sub-basin.",
          "link": "http://arxiv.org/abs/2107.07827",
          "publishedOn": "2021-07-19T00:49:06.208Z",
          "wordCount": 708,
          "title": "A Theoretical Analysis of Granulometry-based Roughness Measures on Cartosat DEMs. (arXiv:2107.07827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07985",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rimner_A/0/1/0/all/0/1\">Andreas Rimner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deasy_J/0/1/0/all/0/1\">Joseph O. Deasy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>",
          "description": "Accurate and robust segmentation of lung cancers from CTs is needed to more\naccurately plan and deliver radiotherapy and to measure treatment response.\nThis is particularly difficult for tumors located close to mediastium, due to\nlow soft-tissue contrast. Therefore, we developed a new cross-modality educed\ndistillation (CMEDL) approach, using unpaired CT and MRI scans, whereby a\nteacher MRI network guides a student CT network to extract features that signal\nthe difference between foreground and background. Our contribution eliminates\ntwo requirements of distillation methods: (i) paired image sets by using an\nimage to image (I2I) translation and (ii) pre-training of the teacher network\nwith a large training set by using concurrent training of all networks. Our\nframework uses an end-to-end trained unpaired I2I translation, teacher, and\nstudent segmentation networks. Our framework can be combined with any I2I and\nsegmentation network. We demonstrate our framework's feasibility using 3\nsegmentation and 2 I2I methods. All networks were trained with 377 CT and 82\nT2w MRI from different sets of patients. Ablation tests and different\nstrategies for incorporating MRI information into CT were performed. Accuracy\nwas measured using Dice similarity (DSC), surface Dice (sDSC), and Hausdorff\ndistance at the 95$^{th}$ percentile (HD95). The CMEDL approach was\nsignificantly (p $<$ 0.001) more accurate than non-CMEDL methods,\nquantitatively and visually. It produced the highest segmentation accuracy\n(sDSC of 0.83 $\\pm$ 0.16 and HD95 of 5.20 $\\pm$ 6.86mm). CMEDL was also more\naccurate than using either pMRI's or the combination of CT's with pMRI's for\nsegmentation.",
          "link": "http://arxiv.org/abs/2107.07985",
          "publishedOn": "2021-07-19T00:49:06.202Z",
          "wordCount": 715,
          "title": "Unpaired cross-modality educed distillation (CMEDL) applied to CT lung tumor segmentation. (arXiv:2107.07985v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shatri_E/0/1/0/all/0/1\">Elona Shatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gy&#xf6;rgy Fazekas</a>",
          "description": "The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.",
          "link": "http://arxiv.org/abs/2107.07786",
          "publishedOn": "2021-07-19T00:49:06.177Z",
          "wordCount": 651,
          "title": "DoReMi: First glance at a universal OMR dataset. (arXiv:2107.07786v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzammul_M/0/1/0/all/0/1\">Muhammed Muzammul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "This survey paper specially analyzed computer vision-based object detection\nchallenges and solutions by different techniques. We mainly highlighted object\ndetection by three different trending strategies, i.e., 1) domain adaptive deep\nlearning-based approaches (discrepancy-based, Adversarial-based,\nReconstruction-based, Hybrid). We examined general as well as tiny object\ndetection-related challenges and offered solutions by historical and\ncomparative analysis. In part 2) we mainly focused on tiny object detection\ntechniques (multi-scale feature learning, Data augmentation, Training strategy\n(TS), Context-based detection, GAN-based detection). In part 3), To obtain\nknowledge-able findings, we discussed different object detection methods, i.e.,\nconvolutions and convolutional neural networks (CNN), pooling operations with\ntrending types. Furthermore, we explained results with the help of some object\ndetection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD,\nwhich are generally considered the base bone of CV, CNN, and OD. We performed\ncomparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12,\nand ImageNet to analyze results and present findings. At the end, we showed\nfuture directions with existing challenges of the field. In the future, OD\nmethods and models can be analyzed for real-time object detection, tracking\nstrategies.",
          "link": "http://arxiv.org/abs/2107.07927",
          "publishedOn": "2021-07-19T00:49:06.171Z",
          "wordCount": 627,
          "title": "A Survey on Deep Domain Adaptation and Tiny Object Detection Challenges, Techniques and Datasets. (arXiv:2107.07927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Few-Shot image classification aims to utilize pretrained knowledge learned\nfrom a large-scale dataset to tackle a series of downstream classification\ntasks. Typically, each task involves only few training examples from brand-new\ncategories. This requires the pretraining models to focus on well-generalizable\nknowledge, but ignore domain-specific information. In this paper, we observe\nthat image background serves as a source of domain-specific knowledge, which is\na shortcut for models to learn in the source dataset, but is harmful when\nadapting to brand-new classes. To prevent the model from learning this shortcut\nknowledge, we propose COSOC, a novel Few-Shot Learning framework, to\nautomatically figure out foreground objects at both pretraining and evaluation\nstage. COSOC is a two-stage algorithm motivated by the observation that\nforeground objects from different images within the same class share more\nsimilar patterns than backgrounds. At the pretraining stage, for each class, we\ncluster contrastive-pretrained features of randomly cropped image patches, such\nthat crops containing only foreground objects can be identified by a single\ncluster. We then force the pretraining model to focus on found foreground\nobjects by a fusion sampling strategy; at the evaluation stage, among images in\neach training class of any few-shot task, we seek for shared contents and\nfilter out background. The recognized foreground objects of each class are used\nto match foreground of testing images. Extensive experiments tailored to\ninductive FSL tasks on two benchmarks demonstrate the state-of-the-art\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.07746",
          "publishedOn": "2021-07-19T00:49:06.158Z",
          "wordCount": 694,
          "title": "Rectifying the Shortcut Learning of Background: Shared Object Concentration for Few-Shot Image Recognition. (arXiv:2107.07746v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hubner_P/0/1/0/all/0/1\">Patrick H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Martin Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wursthorn_S/0/1/0/all/0/1\">Sven Wursthorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1\">Stefan Hinz</a>",
          "description": "In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.",
          "link": "http://arxiv.org/abs/2107.07778",
          "publishedOn": "2021-07-19T00:49:06.151Z",
          "wordCount": 590,
          "title": "Pose Normalization of Indoor Mapping Datasets Partially Compliant to the Manhattan World Assumption. (arXiv:2107.07778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:06.129Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qaiser_T/0/1/0/all/0/1\">Talha Qaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barfoot_T/0/1/0/all/0/1\">Theodore Barfoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barwick_T/0/1/0/all/0/1\">Tara Barwick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doran_S/0/1/0/all/0/1\">Simon J. Doran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Martin F. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedlake_L/0/1/0/all/0/1\">Linda Wedlake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunariu_N/0/1/0/all/0/1\">Nina Tunariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_D/0/1/0/all/0/1\">Dow-Mu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messiou_C/0/1/0/all/0/1\">Christina Messiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockall_A/0/1/0/all/0/1\">Andrea Rockall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Whole body magnetic resonance imaging (WB-MRI) is the recommended modality\nfor diagnosis of multiple myeloma (MM). WB-MRI is used to detect sites of\ndisease across the entire skeletal system, but it requires significant\nexpertise and is time-consuming to report due to the great number of images. To\naid radiological reading, we propose an auxiliary task-based multiple instance\nlearning approach (ATMIL) for MM classification with the ability to localize\nsites of disease. This approach is appealing as it only requires patient-level\nannotations where an attention mechanism is used to identify local regions with\nactive disease. We borrow ideas from multi-task learning and define an\nauxiliary task with adaptive reweighting to support and improve learning\nefficiency in the presence of data scarcity. We validate our approach on both\nsynthetic and real multi-center clinical data. We show that the MIL attention\nmodule provides a mechanism to localize bone regions while the adaptive\nreweighting of the auxiliary task considerably improves the performance.",
          "link": "http://arxiv.org/abs/2107.07805",
          "publishedOn": "2021-07-19T00:49:06.122Z",
          "wordCount": 623,
          "title": "Multiple Instance Learning with Auxiliary Task Weighting for Multiple Myeloma Classification. (arXiv:2107.07805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.107Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07761",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mascolini_A/0/1/0/all/0/1\">Alessio Mascolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardamone_D/0/1/0/all/0/1\">Dario Cardamone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponzio_F/0/1/0/all/0/1\">Francesco Ponzio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cataldo_S/0/1/0/all/0/1\">Santa Di Cataldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ficarra_E/0/1/0/all/0/1\">Elisa Ficarra</a>",
          "description": "Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.",
          "link": "http://arxiv.org/abs/2107.07761",
          "publishedOn": "2021-07-19T00:49:06.096Z",
          "wordCount": 733,
          "title": "Exploiting generative self-supervised learning for the assessment of biological images with lack of annotations: a COVID-19 case-study. (arXiv:2107.07761v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07714",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lavrik_E/0/1/0/all/0/1\">E. Lavrik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shiroya_M/0/1/0/all/0/1\">M. Shiroya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schmidt_H/0/1/0/all/0/1\">H.R. Schmidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Toia_A/0/1/0/all/0/1\">A. Toia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heuser_J/0/1/0/all/0/1\">J.M. Heuser</a>",
          "description": "Optical inspection of 1191 silicon micro-strip sensors was performed using a\ncustom made optical inspection setup, employing a machine-learning based\napproach for the defect analysis and subsequent quality assurance. Furthermore,\nmetrological control of the sensor's surface was performed. In this manuscript,\nwe present the analysis of various sensor surface defects. Among these are\nimplant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,\nsurface scratches, double metallization layer defects, passivation layer\ndefects, bias resistor defects as well as dust particle identification. The\ndefect detection was done using the application of Convolutional Deep Neural\nNetworks (CDNNs). From this, defective strips and defect clusters were\nidentified, as well as a 2D map of the defects using their geometrical\npositions on the sensor was performed. Based on the total number of defects\nfound on the sensor's surface, a method for the estimation of sensor's overall\nquality grade and quality score was proposed.",
          "link": "http://arxiv.org/abs/2107.07714",
          "publishedOn": "2021-07-19T00:49:06.076Z",
          "wordCount": 610,
          "title": "Optical Inspection of the Silicon Micro-strip Sensors for the CBM Experiment employing Artificial Intelligence. (arXiv:2107.07714v1 [physics.ins-det])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:06.068Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>",
          "description": "Recent supervised deep learning methods have shown that heart rate can be\nmeasured remotely using facial videos. However, the performance of these\nsupervised method are dependent on the availability of large-scale labelled\ndata and they have been limited to 2D deep learning architectures that do not\nfully exploit the 3D spatiotemporal information. To solve this problem, we\npresent a novel 3D self-supervised spatiotemporal learning framework for remote\nHR estimation on facial videos. Concretely, we propose a landmark-based spatial\naugmentation which splits the face into several informative parts based on the\nShafer's dichromatic reflection model and a novel sparsity-based temporal\naugmentation exploiting Nyquist-Shannon sampling theorem to enhance the signal\nmodelling ability. We evaluated our method on 3 public datasets and\noutperformed other self-supervised methods and achieved competitive accuracy\nwith the state-of-the-art supervised methods.",
          "link": "http://arxiv.org/abs/2107.07695",
          "publishedOn": "2021-07-19T00:49:06.044Z",
          "wordCount": 572,
          "title": "Self-Supervised Learning Framework for Remote Heart Rate Estimation Using Spatiotemporal Augmentation. (arXiv:2107.07695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Probabilistic state-estimation approaches offer a principled foundation for\ndesigning localization systems, because they naturally integrate sequences of\nimperfect motion and exteroceptive sensor data. Recently, probabilistic\nlocalization systems utilizing appearance-invariant visual place recognition\n(VPR) methods as the primary exteroceptive sensor have demonstrated\nstate-of-the-art performance in the presence of substantial appearance change.\nHowever, existing systems 1) do not fully utilize odometry data within the\nmotion models, and 2) are unable to handle route deviations, due to the\nassumption that query traverses exactly repeat the mapping traverse. To address\nthese shortcomings, we present a new probabilistic topometric localization\nsystem which incorporates full 3-dof odometry into the motion model and\nfurthermore, adds an \"off-map\" state within the state-estimation framework,\nallowing query traverses which feature significant route detours from the\nreference map to be successfully localized. We perform extensive evaluation on\nmultiple query traverses from the Oxford RobotCar dataset exhibiting both\nsignificant appearance change and deviations from routes previously traversed.\nIn particular, we evaluate performance on two practically relevant localization\ntasks: loop closure detection and global localization. Our approach achieves\nmajor performance improvements over both existing and improved state-of-the-art\nsystems.",
          "link": "http://arxiv.org/abs/2107.07707",
          "publishedOn": "2021-07-19T00:49:06.038Z",
          "wordCount": 634,
          "title": "Probabilistic Appearance-Invariant Topometric Localization with New Place Awareness. (arXiv:2107.07707v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:06.031Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Mann Patel</a>",
          "description": "Violence rates however have been brought down about 57% during the span of\nthe past 4 decades yet it doesn't change the way that the demonstration of\nviolence actually happens, unseen by the law. Violence can be mass controlled\nsometimes by higher authorities, however, to hold everything in line one must\n\"Microgovern\" over each movement occurring in every road of each square. To\naddress the butterfly effects impact in our setting, I made a unique model and\na theorized system to handle the issue utilizing deep learning. The model takes\nthe input of the CCTV video feeds and after drawing inference, recognizes if a\nviolent movement is going on. And hypothesized architecture aims towards\nprobability-driven computation of video feeds and reduces overhead from naively\ncomputing for every CCTV video feeds.",
          "link": "http://arxiv.org/abs/2107.07578",
          "publishedOn": "2021-07-19T00:49:06.004Z",
          "wordCount": 561,
          "title": "Real-Time Violence Detection Using CNN-LSTM. (arXiv:2107.07578v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07596",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandewalle_P/0/1/0/all/0/1\">Patrick Vandewalle</a>",
          "description": "We integrate sparse radar data into a monocular depth estimation model and\nintroduce a novel preprocessing method for reducing the sparseness and limited\nfield of view provided by radar. We explore the intrinsic error of different\nradar modalities and show our proposed method results in more data points with\nreduced error. We further propose a novel method for estimating dense depth\nmaps from monocular 2D images and sparse radar measurements using deep learning\nbased on the deep ordinal regression network by Fu et al. Radar data are\nintegrated by first converting the sparse 2D points to a height-extended 3D\nmeasurement and then including it into the network using a late fusion\napproach. Experiments are conducted on the nuScenes dataset. Our experiments\ndemonstrate state-of-the-art performance in both day and night scenes.",
          "link": "http://arxiv.org/abs/2107.07596",
          "publishedOn": "2021-07-19T00:49:05.988Z",
          "wordCount": 593,
          "title": "Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network. (arXiv:2107.07596v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>",
          "description": "OdoViz is a reactive web-based tool for 3D visualization and processing of\nautonomous vehicle datasets designed to support common tasks in visual place\nrecognition research. The system includes functionality for loading,\ninspecting, visualizing, and processing GPS/INS poses, point clouds and camera\nimages. It supports a number of commonly used driving datasets and can be\nadapted to load custom datasets with minimal effort. OdoViz's design consists\nof a slim server to serve the datasets coupled with a rich client frontend.\nThis design supports multiple deployment configurations including single user\nstand-alone installations, research group installations serving datasets\ninternally across a lab, or publicly accessible web-frontends for providing\nonline interfaces for exploring and interacting with datasets. The tool allows\nviewing complete vehicle trajectories traversed at multiple different time\nperiods simultaneously, facilitating tasks such as sub-sampling, comparing and\nfinding pose correspondences both across and within sequences. This\nsignificantly reduces the effort required in creating subsets of data from\nexisting datasets for machine learning tasks. Further to the above, the system\nalso supports adding custom extensions and plugins to extend the capabilities\nof the software for other potential data management, visualization and\nprocessing tasks. The platform has been open-sourced to promote its use and\nencourage further contributions from the research community.",
          "link": "http://arxiv.org/abs/2107.07557",
          "publishedOn": "2021-07-19T00:49:05.971Z",
          "wordCount": 644,
          "title": "OdoViz: A 3D Odometry Visualization and Processing Tool. (arXiv:2107.07557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zida Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>",
          "description": "3D hand-object pose estimation is an important issue to understand the\ninteraction between human and environment. Current hand-object pose estimation\nmethods require detailed 3D labels, which are expensive and labor-intensive. To\ntackle the problem of data collection, we propose a semi-supervised 3D\nhand-object pose estimation method with two key techniques: pose dictionary\nlearning and an object-oriented coordinate system. The proposed pose dictionary\nlearning module can distinguish infeasible poses by reconstruction error,\nenabling unlabeled data to provide supervision signals. The proposed\nobject-oriented coordinate system can make 3D estimations equivariant to the\ncamera perspective. Experiments are conducted on FPHA and HO-3D datasets. Our\nmethod reduces estimation error by 19.5% / 24.9% for hands/objects compared to\nstraightforward use of labeled data on FPHA and outperforms several baseline\nmethods. Extensive experiments also validate the robustness of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.07676",
          "publishedOn": "2021-07-19T00:49:05.963Z",
          "wordCount": 571,
          "title": "Semi-supervised 3D Hand-Object Pose Estimation via Pose Dictionary Learning. (arXiv:2107.07676v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:05.921Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:05.915Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>",
          "description": "Contrastive learning is a discriminative approach that aims at grouping\nsimilar samples closer and diverse samples far from each other. It it an\nefficient technique to train an encoder generating distinguishable and\ninformative representations, and it may even increase the encoder's\ntransferability. Most current applications of contrastive learning benefit only\na single representation from the last layer of an encoder.In this paper, we\npropose a multi-level contrasitive learning approach which applies contrastive\nlosses at different layers of an encoder to learn multiple representations from\nthe encoder. Afterward, an ensemble can be constructed to take advantage of the\nmultiple representations for the downstream tasks. We evaluated the proposed\nmethod on few-shot learning problems and conducted experiments using the\nmini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new\nstate-of-the-art results for both datasets, comparing to previous regular,\nensemble, and contrastive learing (single-level) based approaches.",
          "link": "http://arxiv.org/abs/2107.07608",
          "publishedOn": "2021-07-19T00:49:05.614Z",
          "wordCount": 572,
          "title": "Multi-Level Contrastive Learning for Few-Shot Problems. (arXiv:2107.07608v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.226Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yinong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haonan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jingwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Lixiang Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongruixuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In order to mitigate the spread of COVID-19, Wuhan was the first city to\nimplement strict lockdown policy in 2020. Even though numerous researches have\ndiscussed the travel restriction between cities and provinces, few studies\nfocus on the effect of transportation control inside the city due to the lack\nof the measurement and available data in Wuhan. Since the public transports\nhave been shut down in the beginning of city lockdown, the change of traffic\ndensity is a good indicator to reflect the intracity population flow.\nTherefore, in this paper, we collected time-series high-resolution remote\nsensing images with the resolution of 1m acquired before, during and after\nWuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and\ncounted for the statistics of traffic density to reflect the changes of human\ntransmissions in the whole period of Wuhan lockdown. Open Street Map was used\nto obtain observation road surfaces, and a vehicle detection method combing\nmorphology filter and deep learning was utilized to extract vehicles with the\naccuracy of 62.56%. According to the experimental results, the traffic density\nof Wuhan dropped with the percentage higher than 80%, and even higher than 90%\non main roads during city lockdown; after lockdown lift, the traffic density\nrecovered to the normal rate. Traffic density distributions also show the\nobvious reduction and increase throughout the whole study area. The significant\nreduction and recovery of traffic density indicates that the lockdown policy in\nWuhan show effectiveness in controlling human transmission inside the city, and\nthe city returned to normal after lockdown lift.",
          "link": "http://arxiv.org/abs/2006.16098",
          "publishedOn": "2021-07-16T00:48:25.974Z",
          "wordCount": 813,
          "title": "An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images. (arXiv:2006.16098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peter Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meghanath_G/0/1/0/all/0/1\">Ganga Meghanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Subrata Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>",
          "description": "Videos take a lot of time to transport over the network, hence running\nanalytics on the live video on embedded or mobile devices has become an\nimportant system driver. Considering that such devices, e.g., surveillance\ncameras or AR/VR gadgets, are resource constrained, creating lightweight deep\nneural networks (DNNs) for embedded devices is crucial. None of the current\napproximation techniques for object classification DNNs can adapt to changing\nruntime conditions, e.g., changes in resource availability on the device, the\ncontent characteristics, or requirements from the user. In this paper, we\nintroduce ApproxNet, a video object classification system for embedded or\nmobile clients. It enables novel dynamic approximation techniques to achieve\ndesired inference latency and accuracy trade-off under changing runtime\nconditions. It achieves this by enabling two approximation knobs within a\nsingle DNN model, rather than creating and maintaining an ensemble of models\n(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at\nruntime to these changes, provides low and stable latency for the image and\nvideo frame classification problems, and show the improvement in accuracy and\nlatency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],\nNestDNN [MobiCom-18], and MSDNet [ICLR-18].",
          "link": "http://arxiv.org/abs/1909.02068",
          "publishedOn": "2021-07-16T00:48:25.719Z",
          "wordCount": 717,
          "title": "ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients. (arXiv:1909.02068v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1\">Mouath Aouayeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1\">Catherine Soladie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1\">Kidiyo Kpalma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1\">Renaud Seguier</a>",
          "description": "As various databases of facial expressions have been made accessible over the\nlast few decades, the Facial Expression Recognition (FER) task has gotten a lot\nof interest. The multiple sources of the available databases raised several\nchallenges for facial recognition task. These challenges are usually addressed\nby Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to\naddress vision tasks. One of the major issue with Transformers is the need of a\nlarge data for training, while most FER databases are limited compared to other\nvision applications. Therefore, we propose in this paper to learn a vision\nTransformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases\nincluding CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model\noutperforms state-of-the-art methods on CK+ and SFEW and achieves competitive\nresults on JAFFE and RAF-DB.",
          "link": "http://arxiv.org/abs/2107.03107",
          "publishedOn": "2021-07-16T00:48:25.308Z",
          "wordCount": 625,
          "title": "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mier_J/0/1/0/all/0/1\">Juan Carlos Mier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Eddie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>",
          "description": "Lossy Image compression is necessary for efficient storage and transfer of\ndata. Typically the trade-off between bit-rate and quality determines the\noptimal compression level. This makes the image quality metric an integral part\nof any imaging system. While the existing full-reference metrics such as PSNR\nand SSIM may be less sensitive to perceptual quality, the recently introduced\nlearning methods may fail to generalize to unseen data. In this paper we\npropose the largest image compression quality dataset to date with human\nperceptual preferences, enabling the use of deep learning, and we develop a\nfull reference perceptual quality assessment metric for lossy image compression\nthat outperforms the existing state-of-the-art methods. We show that the\nproposed model can effectively learn from thousands of examples available in\nthe new dataset, and consequently it generalizes better to other unseen\ndatasets of human perceptual preference.",
          "link": "http://arxiv.org/abs/2103.01114",
          "publishedOn": "2021-07-16T00:48:24.289Z",
          "wordCount": 611,
          "title": "Deep Perceptual Image Quality Assessment for Compression. (arXiv:2103.01114v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>",
          "description": "Markerless motion capture and understanding of professional non-daily human\nmovements is an important yet unsolved task, which suffers from complex motion\npatterns and severe self-occlusion, especially for the monocular setting. In\nthis paper, we propose SportsCap -- the first approach for simultaneously\ncapturing 3D human motions and understanding fine-grained actions from\nmonocular challenging sports video input. Our approach utilizes the semantic\nand temporally structured sub-motion prior in the embedding space for motion\ncapture and understanding in a data-driven multi-task manner. To enable robust\ncapture under complex motion patterns, we propose an effective motion embedding\nmodule to recover both the implicit motion embedding and explicit 3D motion\ndetails via a corresponding mapping function as well as a sub-motion\nclassifier. Based on such hybrid motion information, we introduce a\nmulti-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict\nthe fine-grained semantic action attributes, and adopt a semantic attribute\nmapping block to assemble various correlated action attributes into a\nhigh-level action label for the overall detailed understanding of the whole\nsequence, so as to enable various applications like action assessment or motion\nscoring. Comprehensive experiments on both public and our proposed datasets\nshow that with a challenging monocular sports video input, our novel approach\nnot only significantly improves the accuracy of 3D human motion capture, but\nalso recovers accurate fine-grained semantic action attributes.",
          "link": "http://arxiv.org/abs/2104.11452",
          "publishedOn": "2021-07-16T00:48:24.247Z",
          "wordCount": 713,
          "title": "SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos. (arXiv:2104.11452v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1\">Jayender Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1\">Sarah Frisken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Kai-yu Tong</a>",
          "description": "In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, researchers use regularization to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (1) The regularization\nstrength of a specific image pair should be associated with the content of the\nimages, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially\nregularizing the transformation (but overlooking the temporal consistency of\ndifferent estimations) may not be the best strategy to cope with the\nill-posedness. In this study, we propose a mean-teacher based registration\nframework. This framework incorporates an additional \\textit{temporal\nregularization} term by encouraging the teacher model's temporal ensemble\nprediction to be consistent with that of the student model. At each training\nstep, it also automatically adjusts the weights of the \\textit{spatial\nregularization} and the \\textit{temporal regularization} by taking account of\nthe transformation uncertainty and appearance uncertainty derived from the\nperturbed teacher model. We perform experiments on multi- and uni-modal\nregistration tasks, and the results show that our strategy outperforms the\ntraditional and learning-based benchmark methods.",
          "link": "http://arxiv.org/abs/2107.02433",
          "publishedOn": "2021-07-16T00:48:24.188Z",
          "wordCount": 675,
          "title": "Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:24.167Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:24.161Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08757",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenjian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>",
          "description": "We present an Expectation-Maximization (EM) Regularized Deep Learning\n(EMReDL) model for weakly supervised tumor segmentation. The proposed framework\nis tailored to glioblastoma, a type of malignant tumor characterized by its\ndiffuse infiltration into the surrounding brain tissue, which poses significant\nchallenge to treatment target and tumor burden estimation using conventional\nstructural MRI. Although physiological MRI provides more specific information\nregarding tumor infiltration, the relatively low resolution hinders a precise\nfull annotation. This has motivated us to develop a weakly supervised deep\nlearning solution that exploits the partial labelled tumor regions.\n\nEMReDL contains two components: a physiological prior prediction model and\nEM-regularized segmentation model. The physiological prior prediction model\nexploits the physiological MRI by training a classifier to generate a\nphysiological prior map. This map is passed to the segmentation model for\nregularization using the EM algorithm. We evaluated the model on a glioblastoma\ndataset with the pre-operative multiparametric and recurrence MRI available.\nEMReDL showed to effectively segment the infiltrated tumor from the partially\nlabelled region of potential infiltration. The segmented core tumor and\ninfiltrated tumor demonstrated high consistency with the tumor burden labelled\nby experts. The performance comparisons showed that EMReDL achieved higher\naccuracy than published state-of-the-art models. On MR spectroscopy, the\nsegmented region displayed more aggressive features than other partial labelled\nregion. The proposed model can be generalized to other segmentation tasks that\nrely on partial labels, with the CNN architecture flexible in the framework.",
          "link": "http://arxiv.org/abs/2101.08757",
          "publishedOn": "2021-07-16T00:48:24.154Z",
          "wordCount": 731,
          "title": "Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma. (arXiv:2101.08757v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1\">Udit Singh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1\">Aniket Gujarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1\">Satyajit Tourani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>",
          "description": "The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.",
          "link": "http://arxiv.org/abs/2103.08573",
          "publishedOn": "2021-07-16T00:48:24.116Z",
          "wordCount": 677,
          "title": "RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.06562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Anqi Joyce Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1\">Ioan Andrei B&#xe2;rsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenlong Wang</a>",
          "description": "Existing multi-camera SLAM systems assume synchronized shutters for all\ncameras, which is often not the case in practice. In this work, we propose a\ngeneralized multi-camera SLAM formulation which accounts for asynchronous\nsensor observations. Our framework integrates a continuous-time motion model to\nrelate information across asynchronous multi-frames during tracking, local\nmapping, and loop closing. For evaluation, we collected AMV-Bench, a\nchallenging new SLAM dataset covering 482 km of driving recorded using our\nasynchronous multi-camera robotic platform. AMV-Bench is over an order of\nmagnitude larger than previous multi-view HD outdoor SLAM datasets, and covers\ndiverse and challenging motions and environments. Our experiments emphasize the\nnecessity of asynchronous sensor modeling, and show that the use of multiple\ncameras is critical towards robust and accurate SLAM in challenging outdoor\nscenes. For additional information, please see the project website at:\nhttps://www.cs.toronto.edu/~ajyang/amv-slam",
          "link": "http://arxiv.org/abs/2101.06562",
          "publishedOn": "2021-07-16T00:48:24.104Z",
          "wordCount": 620,
          "title": "Asynchronous Multi-View SLAM. (arXiv:2101.06562v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yunlong_Z/0/1/0/all/0/1\">Zhang Yunlong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenxin_L/0/1/0/all/0/1\">Li Chenxin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1\">Lin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyan_S/0/1/0/all/0/1\">Sun Liyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yihong_Z/0/1/0/all/0/1\">Zhuang Yihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Huang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinghao_D/0/1/0/all/0/1\">Ding Xinghao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqing_L/0/1/0/all/0/1\">Liu Xiaoqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yizhou_Y/0/1/0/all/0/1\">Yu Yizhou</a>",
          "description": "This paper investigates the problem of pseudo-healthy synthesis that is\ndefined as synthesizing a subject-specific pathology-free image from a\npathological one. Recent approaches based on Generative Adversarial Network\n(GAN) have been developed for this task. However, these methods will inevitably\nfall into the trade-off between preserving the subject-specific identity and\ngenerating healthy-like appearances. To overcome this challenge, we propose a\nnovel adversarial training regime, Generator versus Segmentor (GVS), to\nalleviate this trade-off by a divide-and-conquer strategy. We further consider\nthe deteriorating generalization performance of the segmentor throughout the\ntraining and develop a pixel-wise weighted loss by muting the well-transformed\npixels to promote it. Moreover, we propose a new metric to measure how healthy\nthe synthetic images look. The qualitative and quantitative experiments on the\npublic dataset BraTS demonstrate that the proposed method outperforms the\nexisting methods. Besides, we also certify the effectiveness of our method on\ndatasets LiTS. Our implementation and pre-trained networks are publicly\navailable at https://github.com/Au3C2/Generator-Versus-Segmentor.",
          "link": "http://arxiv.org/abs/2009.05722",
          "publishedOn": "2021-07-16T00:48:24.080Z",
          "wordCount": 644,
          "title": "Generator Versus Segmentor: Pseudo-healthy Synthesis. (arXiv:2009.05722v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arun_N/0/1/0/all/0/1\">Nishanth Arun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1\">Nathan Gaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Mehak Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bryan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1\">Jay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidwani_M/0/1/0/all/0/1\">Mishka Gidwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1\">Julius Adebayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Matthew D. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.",
          "link": "http://arxiv.org/abs/2008.02766",
          "publishedOn": "2021-07-16T00:48:24.064Z",
          "wordCount": 735,
          "title": "Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging. (arXiv:2008.02766v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kafri_O/0/1/0/all/0/1\">Omer Kafri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1\">Or Patashnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "We present StyleFusion, a new mapping architecture for StyleGAN, which takes\nas input a number of latent codes and fuses them into a single style code.\nInserting the resulting style code into a pre-trained StyleGAN generator\nresults in a single harmonized image in which each semantic region is\ncontrolled by one of the input latent codes. Effectively, StyleFusion yields a\ndisentangled representation of the image, providing fine-grained control over\neach region of the generated image. Moreover, to help facilitate global control\nover the generated image, a special input latent code is incorporated into the\nfused representation. StyleFusion operates in a hierarchical manner, where each\nlevel is tasked with learning to disentangle a pair of image regions (e.g., the\ncar body and wheels). The resulting learned disentanglement allows one to\nmodify both local, fine-grained semantics (e.g., facial features) as well as\nmore global features (e.g., pose and background), providing improved\nflexibility in the synthesis process. As a natural extension, StyleFusion\nenables one to perform semantically-aware cross-image mixing of regions that\nare not necessarily aligned. Finally, we demonstrate how StyleFusion can be\npaired with existing editing techniques to more faithfully constrain the edit\nto the user's region of interest.",
          "link": "http://arxiv.org/abs/2107.07437",
          "publishedOn": "2021-07-16T00:48:24.050Z",
          "wordCount": 641,
          "title": "StyleFusion: A Generative Model for Disentangling Spatial Segments. (arXiv:2107.07437v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.013Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>",
          "description": "We present VILENS (Visual Inertial Lidar Legged Navigation System), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term which is estimated online. This bias is only\nobservable because of the tight fusion of this preintegrated velocity factor\nwith vision, lidar, and IMU factors. Extensive experimental validation on the\nANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km\ntraveled. The experiments involved dynamic locomotion over loose rocks, slopes,\nand mud; these included perceptual challenges, such as dark and dusty\nunderground caverns or open, feature-deprived areas, as well as mobility\nchallenges such as slipping and terrain deformation. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.",
          "link": "http://arxiv.org/abs/2107.07243",
          "publishedOn": "2021-07-16T00:48:23.997Z",
          "wordCount": 623,
          "title": "VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:23.969Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:23.960Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1\">Gabriele Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1\">Halvar Kelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1\">Matthias Karck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1\">Raffaele De Simone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>",
          "description": "The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.",
          "link": "http://arxiv.org/abs/2107.06941",
          "publishedOn": "2021-07-16T00:48:23.952Z",
          "wordCount": 720,
          "title": "Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:23.934Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1\">Sudhanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.",
          "link": "http://arxiv.org/abs/2107.07500",
          "publishedOn": "2021-07-16T00:48:23.918Z",
          "wordCount": 640,
          "title": "Recommending best course of treatment based on similarities of prognostic markers\\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Migdal_P/0/1/0/all/0/1\">Piotr Migda&#x142;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olechno_B/0/1/0/all/0/1\">Bart&#x142;omiej Olechno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podgorski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Podg&#xf3;rski</a>",
          "description": "We present practical approaches of using deep learning to create and enhance\nlevel maps and textures for video games -- desktop, mobile, and web. We aim to\npresent new possibilities for game developers and level artists. The task of\ndesigning levels and filling them with details is challenging. It is both\ntime-consuming and takes effort to make levels rich, complex, and with a\nfeeling of being natural. Fortunately, recent progress in deep learning\nprovides new tools to accompany level designers and visual artists. Moreover,\nthey offer a way to generate infinite worlds for game replayability and adjust\neducational games to players' needs. We present seven approaches to create\nlevel maps, each using statistical methods, machine learning, or deep learning.\nIn particular, we include:\n\n- Generative Adversarial Networks for creating new images from existing\nexamples (e.g. ProGAN).\n\n- Super-resolution techniques for upscaling images while preserving crisp\ndetail (e.g. ESRGAN).\n\n- Neural style transfer for changing visual themes.\n\n- Image translation - turning semantic maps into images (e.g. GauGAN).\n\n- Semantic segmentation for turning images into semantic masks (e.g. U-Net).\n\n- Unsupervised semantic segmentation for extracting semantic features (e.g.\nTile2Vec).\n\n- Texture synthesis - creating large patterns based on a smaller sample (e.g.\nInGAN).",
          "link": "http://arxiv.org/abs/2107.07397",
          "publishedOn": "2021-07-16T00:48:23.875Z",
          "wordCount": 666,
          "title": "Level generation and style enhancement -- deep learning for game development overview. (arXiv:2107.07397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:23.866Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miah_M/0/1/0/all/0/1\">Mehdi Miah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We propose a method for multi-object tracking and segmentation (MOTS) that\ndoes not require fine-tuning or per benchmark hyperparameter selection. The\nproposed method addresses particularly the data association problem. Indeed,\nthe recently introduced HOTA metric, that has a better alignment with the human\nvisual assessment by evenly balancing detections and associations quality, has\nshown that improvements are still needed for data association. After creating\ntracklets using instance segmentation and optical flow, the proposed method\nrelies on a space-time memory network (STM) developed for one-shot video object\nsegmentation to improve the association of tracklets with temporal gaps. To the\nbest of our knowledge, our method, named MeNToS, is the first to use the STM\nnetwork to track object masks for MOTS. We took the 4th place in the RobMOTS\nchallenge. The project page is https://mehdimiah.com/mentos.html.",
          "link": "http://arxiv.org/abs/2107.07067",
          "publishedOn": "2021-07-16T00:48:23.860Z",
          "wordCount": 585,
          "title": "MeNToS: Tracklets Association with a Space-Time Memory Network. (arXiv:2107.07067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_M/0/1/0/all/0/1\">Manuel Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1\">Belen Masia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>",
          "description": "We present a single-image data-driven method to automatically relight images\nwith full-body humans in them. Our framework is based on a realistic scene\ndecomposition leveraging precomputed radiance transfer (PRT) and spherical\nharmonics (SH) lighting. In contrast to previous work, we lift the assumptions\non Lambertian materials and explicitly model diffuse and specular reflectance\nin our data. Moreover, we introduce an additional light-dependent residual term\nthat accounts for errors in the PRT-based image reconstruction. We propose a\nnew deep learning architecture, tailored to the decomposition performed in PRT,\nthat is trained using a combination of L1, logarithmic, and rendering losses.\nOur model outperforms the state of the art for full-body human relighting both\nwith synthetic images and photographs.",
          "link": "http://arxiv.org/abs/2107.07259",
          "publishedOn": "2021-07-16T00:48:23.853Z",
          "wordCount": 570,
          "title": "Single-image Full-body Human Relighting. (arXiv:2107.07259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:23.836Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>",
          "description": "Identifying relations between objects is central to understanding the scene.\nWhile several works have been proposed for relation modeling in the image\ndomain, there have been many constraints in the video domain due to challenging\ndynamics of spatio-temporal interactions (e.g., Between which objects are there\nan interaction? When do relations occur and end?). To date, two representative\nmethods have been proposed to tackle Video Visual Relation Detection (VidVRD):\nsegment-based and window-based. We first point out the limitations these two\nmethods have and propose Temporal Span Proposal Network (TSPN), a novel method\nwith two advantages in terms of efficiency and effectiveness. 1) TSPN tells\nwhat to look: it sparsifies relation search space by scoring relationness\n(i.e., confidence score for the existence of a relation between pair of\nobjects) of object pair. 2) TSPN tells when to look: it leverages the full\nvideo context to simultaneously predict the temporal span and categories of the\nentire relations. TSPN demonstrates its effectiveness by achieving new\nstate-of-the-art by a significant margin on two VidVRD benchmarks\n(ImageNet-VidVDR and VidOR) while also showing lower time complexity than\nexisting methods - in particular, twice as efficient as a popular segment-based\napproach.",
          "link": "http://arxiv.org/abs/2107.07154",
          "publishedOn": "2021-07-16T00:48:23.829Z",
          "wordCount": 669,
          "title": "What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection. (arXiv:2107.07154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nmechanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., ``turn left'') in instructions. Each module\npredicts navigation action separately and their weighted sum is used for\npredicting the final action. Extensive experimental results demonstrate the\neffectiveness of the proposed method on the R2R and R4R benchmarks against\nseveral state-of-the-art navigators, and NvEM even beats some pre-training\nones. Our code is available at https://github.com/MarSaKi/NvEM.",
          "link": "http://arxiv.org/abs/2107.07201",
          "publishedOn": "2021-07-16T00:48:23.822Z",
          "wordCount": 643,
          "title": "Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xunli Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>",
          "description": "Amodal segmentation is a new direction of instance segmentation while\nconsidering the segmentation of the visible and occluded parts of the instance.\nThe existing state-of-the-art method uses multi-task branches to predict the\namodal part and the visible part separately and subtract the visible part from\nthe amodal part to obtain the occluded part. However, the amodal part contains\nvisible information. Therefore, the separated prediction method will generate\nduplicate information. Different from this method, we propose a method of\namodal segmentation based on the idea of the jigsaw. The method uses multi-task\nbranches to predict the two naturally decoupled parts of visible and occluded,\nwhich is like getting two matching jigsaw pieces. Then put the two jigsaw\npieces together to get the amodal part. This makes each branch focus on the\nmodeling of the object. And we believe that there are certain rules in the\nocclusion relationship in the real world. This is a kind of occlusion context\ninformation. This jigsaw method can better model the occlusion relationship and\nuse the occlusion context information, which is important for amodal\nsegmentation. Experiments on two widely used amodally annotated datasets prove\nthat our method exceeds existing state-of-the-art methods. The source code of\nthis work will be made public soon.",
          "link": "http://arxiv.org/abs/2107.07464",
          "publishedOn": "2021-07-16T00:48:23.816Z",
          "wordCount": 636,
          "title": "Amodal segmentation just like doing a jigsaw. (arXiv:2107.07464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:23.808Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1\">Ibrahim Sobh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1\">Ahmed Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>",
          "description": "Deep neural networks (DNNs) have accomplished impressive success in various\napplications, including autonomous driving perception tasks, in recent years.\nOn the other hand, current deep neural networks are easily fooled by\nadversarial attacks. This vulnerability raises significant concerns,\nparticularly in safety-critical applications. As a result, research into\nattacking and defending DNNs has gained much coverage. In this work, detailed\nadversarial attacks are applied on a diverse multi-task visual perception deep\nnetwork across distance estimation, semantic segmentation, motion detection,\nand object detection. The experiments consider both white and black box attacks\nfor targeted and un-targeted cases, while attacking a task and inspecting the\neffect on all the others, in addition to inspecting the effect of applying a\nsimple defense method. We conclude this paper by comparing and discussing the\nexperimental results, proposing insights and future work. The visualizations of\nthe attacks are available at https://youtu.be/R3JUV41aiPY.",
          "link": "http://arxiv.org/abs/2107.07449",
          "publishedOn": "2021-07-16T00:48:23.791Z",
          "wordCount": 585,
          "title": "Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (arXiv:2107.07449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1\">Puneet Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1\">Shivam Chandhok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>",
          "description": "Recent progress towards designing models that can generalize to unseen\ndomains (i.e domain generalization) or unseen classes (i.e zero-shot learning)\nhas embarked interest towards building models that can tackle both domain-shift\nand semantic shift simultaneously (i.e zero-shot domain generalization). For\nmodels to generalize to unseen classes in unseen domains, it is crucial to\nlearn feature representation that preserves class-level (domain-invariant) as\nwell as domain-specific information. Motivated from the success of generative\nzero-shot approaches, we propose a feature generative framework integrated with\na COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly\nintegrate class-level semantic and domain-specific information. The generated\nvisual features better capture the underlying data distribution enabling us to\ngeneralize to unseen classes and domains at test-time. We thoroughly evaluate\nand analyse our approach on established large-scale benchmark - DomainNet and\ndemonstrate promising performance over baselines and state-of-art methods.",
          "link": "http://arxiv.org/abs/2107.07497",
          "publishedOn": "2021-07-16T00:48:23.785Z",
          "wordCount": 577,
          "title": "Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains. (arXiv:2107.07497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07468",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1\">Jo&#xe3;o P C Bertoldo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1\">Etienne Decenci&#xe8;re</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1\">David Ryckelynck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1\">Henry Proudhon</a>",
          "description": "X-ray Computed Tomography (XCT) techniques have evolved to a point that\nhigh-resolution data can be acquired so fast that classic segmentation methods\nare prohibitively cumbersome, demanding automated data pipelines capable of\ndealing with non-trivial 3D images. Deep learning has demonstrated success in\nmany image processing tasks, including material science applications, showing a\npromising alternative for a humanfree segmentation pipeline. In this paper a\nmodular interpretation of UNet (Modular U-Net) is proposed and trained to\nsegment 3D tomography images of a three-phased glass fiber-reinforced Polyamide\n66. We compare 2D and 3D versions of our model, finding that the former is\nslightly better than the latter. We observe that human-comparable results can\nbe achievied even with only 10 annotated layers and using a shallow U-Net\nyields better results than a deeper one. As a consequence, Neural Network (NN)\nshow indeed a promising venue to automate XCT data processing pipelines needing\nno human, adhoc intervention.",
          "link": "http://arxiv.org/abs/2107.07468",
          "publishedOn": "2021-07-16T00:48:23.779Z",
          "wordCount": 630,
          "title": "A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>",
          "description": "Identifying potential threats concealed within the baggage is of prime\nconcern for the security staff. Many researchers have developed frameworks that\ncan detect baggage threats from X-ray scans. However, to the best of our\nknowledge, all of these frameworks require extensive training on large-scale\nand well-annotated datasets, which are hard to procure in the real world. This\npaper presents a novel unsupervised anomaly instance segmentation framework\nthat recognizes baggage threats, in X-ray scans, as anomalies without requiring\nany ground truth labels. Furthermore, thanks to its stylization capacity, the\nframework is trained only once, and at the inference stage, it detects and\nextracts contraband items regardless of their scanner specifications. Our\none-staged approach initially learns to reconstruct normal baggage content via\nan encoder-decoder network utilizing a proposed stylization loss function. The\nmodel subsequently identifies the abnormal regions by analyzing the disparities\nwithin the original and the reconstructed scans. The anomalous regions are then\nclustered and post-processed to fit a bounding box for their localization. In\naddition, an optional classifier can also be appended with the proposed\nframework to recognize the categories of these extracted anomalies. A thorough\nevaluation of the proposed system on four public baggage X-ray datasets,\nwithout any re-training, demonstrates that it achieves competitive performance\nas compared to the conventional fully supervised methods (i.e., the mean\naverage precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on\nOPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art\nsemi-supervised and unsupervised baggage threat detection frameworks by 67.37%,\n32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,\nand COMPASS-XP datasets, respectively.",
          "link": "http://arxiv.org/abs/2107.07333",
          "publishedOn": "2021-07-16T00:48:23.766Z",
          "wordCount": 708,
          "title": "Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition. (arXiv:2107.07333v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:23.748Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">D. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">J. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">K. Lee</a>",
          "description": "In the process of intelligently segmenting foods in images using deep neural\nnetworks for diet management, data collection and labeling for network training\nare very important but labor-intensive tasks. In order to solve the\ndifficulties of data collection and annotations, this paper proposes a food\nsegmentation method applicable to real-world through synthetic data. To perform\nfood segmentation on healthcare robot systems, such as meal assistance robot\narm, we generate synthetic data using the open-source 3D graphics software\nBlender placing multiple objects on meal plate and train Mask R-CNN for\ninstance segmentation. Also, we build a data collection system and verify our\nsegmentation model on real-world food data. As a result, on our real-world\ndataset, the model trained only synthetic data is available to segment food\ninstances that are not trained with 52.2% mask AP@all, and improve performance\nby +6.4%p after fine-tuning comparing to the model trained from scratch. In\naddition, we also confirm the possibility and performance improvement on the\npublic dataset for fair analysis. Our code and pre-trained weights are\navaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation",
          "link": "http://arxiv.org/abs/2107.07191",
          "publishedOn": "2021-07-16T00:48:23.735Z",
          "wordCount": 621,
          "title": "Deep Learning based Food Instance Segmentation using Synthetic Data. (arXiv:2107.07191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:23.725Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1\">Gereon Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1\">Mohamed Elgharib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Generative adversarial models (GANs) continue to produce advances in terms of\nthe visual quality of still images, as well as the learning of temporal\ncorrelations. However, few works manage to combine these two interesting\ncapabilities for the synthesis of video content: Most methods require an\nextensive training dataset in order to learn temporal correlations, while being\nrather limited in the resolution and visual quality of their output frames. In\nthis paper, we present a novel approach to the video synthesis problem that\nhelps to greatly improve visual quality and drastically reduce the amount of\ntraining data and resources necessary for generating video content. Our\nformulation separates the spatial domain, in which individual frames are\nsynthesized, from the temporal domain, in which motion is generated. For the\nspatial domain we make use of a pre-trained StyleGAN network, the latent space\nof which allows control over the appearance of the objects it was trained for.\nThe expressive power of this model allows us to embed our training videos in\nthe StyleGAN latent space. Our temporal architecture is then trained not on\nsequences of RGB frames, but on sequences of StyleGAN latent codes. The\nadvantageous properties of the StyleGAN space simplify the discovery of\ntemporal correlations. We demonstrate that it suffices to train our temporal\narchitecture on only 10 minutes of footage of 1 subject for about 6 hours.\nAfter training, our model can not only generate new portrait videos for the\ntraining subject, but also for any random subject which can be embedded in the\nStyleGAN space.",
          "link": "http://arxiv.org/abs/2107.07224",
          "publishedOn": "2021-07-16T00:48:23.715Z",
          "wordCount": 693,
          "title": "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN. (arXiv:2107.07224v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:23.709Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deane_J/0/1/0/all/0/1\">Jake Deane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearney_S/0/1/0/all/0/1\">Sinead Kearney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1\">Darren Cosker</a>",
          "description": "Synthetic data is becoming increasingly common for training computer vision\nmodels for a variety of tasks. Notably, such data has been applied in tasks\nrelated to humans such as 3D pose estimation where data is either difficult to\ncreate or obtain in realistic settings. Comparatively, there has been less work\ninto synthetic animal data and it's uses for training models. Consequently, we\nintroduce a parametric canine model, DynaDog+T, for generating synthetic canine\nimages and data which we use for a common computer vision task, binary\nsegmentation, which would otherwise be difficult due to the lack of available\ndata.",
          "link": "http://arxiv.org/abs/2107.07330",
          "publishedOn": "2021-07-16T00:48:23.691Z",
          "wordCount": 544,
          "title": "DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation. (arXiv:2107.07330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:23.675Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:23.649Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jizhizi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Automatic image matting (AIM) refers to estimating the soft foreground from\nan arbitrary natural image without any auxiliary input like trimap, which is\nuseful for image editing. Prior methods try to learn semantic features to aid\nthe matting process while being limited to images with salient opaque\nforegrounds such as humans and animals. In this paper, we investigate the\ndifficulties when extending them to natural images with salient\ntransparent/meticulous foregrounds or non-salient foregrounds. To address the\nproblem, a novel end-to-end matting network is proposed, which can predict a\ngeneralized trimap for any image of the above types as a unified semantic\nrepresentation. Simultaneously, the learned semantic features guide the matting\nnetwork to focus on the transition areas via an attention mechanism. We also\nconstruct a test set AIM-500 that contains 500 diverse natural images covering\nall types along with manually labeled alpha mattes, making it feasible to\nbenchmark the generalization ability of AIM models. Results of the experiments\ndemonstrate that our network trained on available composite matting datasets\noutperforms existing methods both objectively and subjectively. The source code\nand dataset are available at https://github.com/JizhiziLi/AIM.",
          "link": "http://arxiv.org/abs/2107.07235",
          "publishedOn": "2021-07-16T00:48:23.587Z",
          "wordCount": 630,
          "title": "Deep Automatic Natural Image Matting. (arXiv:2107.07235v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1\">Oriol Corcoll</a>",
          "description": "Automatic image cropping techniques are commonly used to enhance the\naesthetic quality of an image; they do it by detecting the most beautiful or\nthe most salient parts of the image and removing the unwanted content to have a\nsmaller image that is more visually pleasing. In this thesis, I introduce an\nadditional dimension to the problem of cropping, semantics. I argue that image\ncropping can also enhance the image's relevancy for a given entity by using the\nsemantic information contained in the image. I call this problem, Semantic\nImage Cropping. To support my argument, I provide a new dataset containing 100\nimages with at least two different entities per image and four ground truth\ncroppings collected using Amazon Mechanical Turk. I use this dataset to show\nthat state-of-the-art cropping algorithms that only take into account\naesthetics do not perform well in the problem of semantic image cropping.\nAdditionally, I provide a new deep learning system that takes not just\naesthetics but also semantics into account to generate image croppings, and I\nevaluate its performance using my new semantic cropping dataset, showing that\nusing the semantic information of an image can help to produce better\ncroppings.",
          "link": "http://arxiv.org/abs/2107.07153",
          "publishedOn": "2021-07-16T00:48:23.563Z",
          "wordCount": 615,
          "title": "Semantic Image Cropping. (arXiv:2107.07153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yakun Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1\">Muwei Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shaoxiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "The goal of photometric stereo is to measure the precise surface normal of a\n3D object from observations with various shading cues. However, non-Lambertian\nsurfaces influence the measurement accuracy due to irregular shading cues.\nDespite deep neural networks have been employed to simulate the performance of\nnon-Lambertian surfaces, the error in specularities, shadows, and crinkle\nregions is hard to be reduced. In order to address this challenge, we here\npropose a photometric stereo network that incorporates Lambertian priors to\nbetter measure the surface normal. In this paper, we use the initial normal\nunder the Lambertian assumption as the prior information to refine the normal\nmeasurement, instead of solely applying the observed shading cues to deriving\nthe surface normal. Our method utilizes the Lambertian information to\nreparameterize the network weights and the powerful fitting ability of deep\nneural networks to correct these errors caused by general reflectance\nproperties. Our explorations include: the Lambertian priors (1) reduce the\nlearning hypothesis space, making our method learn the mapping in the same\nsurface normal space and improving the accuracy of learning, and (2) provides\nthe differential features learning, improving the surfaces reconstruction of\ndetails. Extensive experiments verify the effectiveness of the proposed\nLambertian prior photometric stereo network in accurate surface normal\nmeasurement, on the challenging benchmark dataset.",
          "link": "http://arxiv.org/abs/2107.07192",
          "publishedOn": "2021-07-16T00:48:23.554Z",
          "wordCount": 653,
          "title": "Incorporating Lambertian Priors into Surface Normals Measurement. (arXiv:2107.07192v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kazuki Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>",
          "description": "Multi-pedestrian trajectory prediction is an indispensable safety element of\nautonomous systems that interact with crowds in unstructured environments. Many\nrecent efforts have developed trajectory prediction algorithms with focus on\nunderstanding social norms behind pedestrian motions. Yet we observe these\nworks usually hold two assumptions that prevent them from being smoothly\napplied to robot applications: positions of all pedestrians are consistently\ntracked; the target agent pays attention to all pedestrians in the scene. The\nfirst assumption leads to biased interaction modeling with incomplete\npedestrian data, and the second assumption introduces unnecessary disturbances\nand leads to the freezing robot problem. Thus, we propose Gumbel Social\nTransformer, in which an Edge Gumbel Selector samples a sparse interaction\ngraph of partially observed pedestrians at each time step. A Node Transformer\nEncoder and a Masked LSTM encode the pedestrian features with the sampled\nsparse graphs to predict trajectories. We demonstrate that our model overcomes\nthe potential problems caused by the assumptions, and our approach outperforms\nthe related works in benchmark evaluation.",
          "link": "http://arxiv.org/abs/2107.07056",
          "publishedOn": "2021-07-16T00:48:23.530Z",
          "wordCount": 612,
          "title": "Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction. (arXiv:2107.07056v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:23.523Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1\">Velat Kilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1\">Deepti Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Brinton Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1\">Mark A. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Lidar-based object detectors are critical parts of the 3D perception pipeline\nin autonomous navigation systems such as self-driving cars. However, they are\nknown to be sensitive to adverse weather conditions such as rain, snow and fog\ndue to reduced signal-to-noise ratio (SNR) and signal-to-background ratio\n(SBR). As a result, lidar-based object detectors trained on data captured in\nnormal weather tend to perform poorly in such scenarios. However, collecting\nand labelling sufficient training data in a diverse range of adverse weather\nconditions is laborious and prohibitively expensive. To address this issue, we\npropose a physics-based approach to simulate lidar point clouds of scenes in\nadverse weather conditions. These augmented datasets can then be used to train\nlidar-based detectors to improve their all-weather reliability. Specifically,\nwe introduce a hybrid Monte-Carlo based approach that treats (i) the effects of\nlarge particles by placing them randomly and comparing their back reflected\npower against the target, and (ii) attenuation effects on average through\ncalculation of scattering efficiencies from the Mie theory and particle size\ndistributions. Retraining networks with this augmented data improves mean\naverage precision evaluated on real world rainy scenes and we observe greater\nimprovement in performance with our model relative to existing models from the\nliterature. Furthermore, we evaluate recent state-of-the-art detectors on the\nsimulated weather conditions and present an in-depth analysis of their\nperformance.",
          "link": "http://arxiv.org/abs/2107.07004",
          "publishedOn": "2021-07-16T00:48:23.517Z",
          "wordCount": 683,
          "title": "Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of Adverse Weather Conditions for 3D Object Detection. (arXiv:2107.07004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaomeng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leake_D/0/1/0/all/0/1\">David Leake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>",
          "description": "The case difference heuristic (CDH) approach is a knowledge-light method for\nlearning case adaptation knowledge from the case base of a case-based reasoning\nsystem. Given a pair of cases, the CDH approach attributes the difference in\ntheir solutions to the difference in the problems they solve, and generates\nadaptation rules to adjust solutions accordingly when a retrieved case and new\nquery have similar problem differences. As an alternative to learning\nadaptation rules, several researchers have applied neural networks to learn to\npredict solution differences from problem differences. Previous work on such\napproaches has assumed that the feature set describing problems is predefined.\nThis paper investigates a two-phase process combining deep learning for feature\nextraction and neural network based adaptation learning from extracted\nfeatures. Its performance is demonstrated in a regression task on an image\ndata: predicting age given the image of a face. Results show that the combined\nprocess can successfully learn adaptation knowledge applicable to nonsymbolic\ndifferences in cases. The CBR system achieves slightly lower performance\noverall than a baseline deep network regressor, but better performance than the\nbaseline on novel queries.",
          "link": "http://arxiv.org/abs/2107.07095",
          "publishedOn": "2021-07-16T00:48:23.509Z",
          "wordCount": 654,
          "title": "Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features. (arXiv:2107.07095v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_D/0/1/0/all/0/1\">Di You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jingfen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>",
          "description": "Recent deep network-based compressive sensing (CS) methods have achieved\ngreat success. However, most of them regard different sampling matrices as\ndifferent independent tasks and need to train a specific model for each target\nsampling matrix. Such practices give rise to inefficiency in computing and\nsuffer from poor generalization ability. In this paper, we propose a novel\nCOntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of\narbitrary-sampling matrices (including unseen sampling matrices) with one\nsingle model. Under the optimization-inspired deep unfolding framework, our\nCOAST exhibits good interpretability. In COAST, a random projection\naugmentation (RPA) strategy is proposed to promote the training diversity in\nthe sampling space to enable arbitrary sampling, and a controllable proximal\nmapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are\nfurther developed to dynamically modulate the network features and effectively\neliminate the blocking artifacts, respectively. Extensive experiments on widely\nused benchmark datasets demonstrate that our proposed COAST is not only able to\nhandle arbitrary sampling matrices with one single model but also to achieve\nstate-of-the-art performance with fast speed. The source code is available on\nhttps://github.com/jianzhangcs/COAST.",
          "link": "http://arxiv.org/abs/2107.07225",
          "publishedOn": "2021-07-16T00:48:23.418Z",
          "wordCount": 647,
          "title": "COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing. (arXiv:2107.07225v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_S/0/1/0/all/0/1\">Shivran Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "The cognitive system for human action and behavior has evolved into a deep\nlearning regime, and especially the advent of Graph Convolution Networks has\ntransformed the field in recent years. However, previous works have mainly\nfocused on over-parameterized and complex models based on dense graph\nconvolution networks, resulting in low efficiency in training and inference.\nMeanwhile, the Transformer architecture-based model has not yet been well\nexplored for cognitive application in human action and behavior estimation.\nThis work proposes a novel skeleton-based human action recognition model with\nsparse attention on the spatial dimension and segmented linear attention on the\ntemporal dimension of data. Our model can also process the variable length of\nvideo clips grouped as a single batch. Experiments show that our model can\nachieve comparable performance while utilizing much less trainable parameters\nand achieve high speed in training and inference. Experiments show that our\nmodel achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline\nmodels at competitive accuracy.",
          "link": "http://arxiv.org/abs/2107.07089",
          "publishedOn": "2021-07-16T00:48:23.377Z",
          "wordCount": 605,
          "title": "STAR: Sparse Transformer-based Action Recognition. (arXiv:2107.07089v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:23.370Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi-Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chung-Yen Su</a>",
          "description": "Nowadays, due to the rapid population expansion, food shortage has become a\ncritical issue. In order to stabilizing the food source production, preventing\ncrops from being attacked by pests is very important. In generally, farmers use\npesticides to kill pests, however, improperly using pesticides will also kill\nsome insects which is beneficial to crops, such as bees. If the number of bees\nis too few, the supplement of food in the world will be in short. Besides,\nexcessive pesticides will seriously pollute the environment. Accordingly,\nfarmers need a machine which can automatically recognize the pests. Recently,\ndeep learning is popular because its effectiveness in the field of image\nclassification. In this paper, we propose a small and efficient model called\nExquisiteNet to complete the task of recognizing the pests and we expect to\napply our model on mobile devices. ExquisiteNet mainly consists of two blocks.\nOne is double fusion with squeeze-and-excitation-bottleneck block (DFSEB\nblock), and the other is max feature expansion block (ME block). ExquisiteNet\nonly has 0.98M parameters and its computing speed is very fast almost the same\nas SqueezeNet. In order to evaluate our model's performance, we test our model\non a benchmark pest dataset called IP102. Compared to many state-of-the-art\nmodels, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet\netc., our model achieves higher accuracy, that is, 52.32% on the test set of\nIP102 without any data augmentation.",
          "link": "http://arxiv.org/abs/2107.07167",
          "publishedOn": "2021-07-16T00:48:23.363Z",
          "wordCount": 675,
          "title": "An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet. (arXiv:2107.07167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ufer_N/0/1/0/all/0/1\">Nikolai Ufer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Sabine Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.",
          "link": "http://arxiv.org/abs/2107.06935",
          "publishedOn": "2021-07-16T00:48:23.356Z",
          "wordCount": 647,
          "title": "Object Retrieval and Localization in Large Art Collections using Deep Multi-Style Feature Fusion and Iterative Voting. (arXiv:2107.06935v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:23.350Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiyu Song</a>",
          "description": "Up-to-date High-Definition (HD) maps are essential for self-driving cars. To\nachieve constantly updated HD maps, we present a deep neural network (DNN),\nDiff-Net, to detect changes in them. Compared to traditional methods based on\nobject detectors, the essential design in our work is a parallel feature\ndifference calculation structure that infers map changes by comparing features\nextracted from the camera and rasterized images. To generate these rasterized\nimages, we project map elements onto images in the camera view, yielding\nmeaningful map representations that can be consumed by a DNN accordingly. As we\nformulate the change detection task as an object detection problem, we leverage\nthe anchor-based structure that predicts bounding boxes with different change\nstatus categories. Furthermore, rather than relying on single frame input, we\nintroduce a spatio-temporal fusion module that fuses features from history\nframes into the current, thus improving the overall performance. Finally, we\ncomprehensively validate our method's effectiveness using freshly collected\ndatasets. Results demonstrate that our Diff-Net achieves better performance\nthan the baseline methods and is ready to be integrated into a map production\npipeline maintaining an up-to-date HD map.",
          "link": "http://arxiv.org/abs/2107.07030",
          "publishedOn": "2021-07-16T00:48:23.333Z",
          "wordCount": 631,
          "title": "Diff-Net: Image Feature Difference based High-Definition Map Change Detection. (arXiv:2107.07030v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:23.325Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:23.318Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jian Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Jun Zhang</a>",
          "description": "Automatic surgical instruction generation is a prerequisite towards\nintra-operative context-aware surgical assistance. However, generating\ninstructions from surgical scenes is challenging, as it requires jointly\nunderstanding the surgical activity of current view and modelling relationships\nbetween visual information and textual description. Inspired by the neural\nmachine translation and imaging captioning tasks in open domain, we introduce a\ntransformer-backboned encoder-decoder network with self-critical reinforcement\nlearning to generate instructions from surgical images. We evaluate the\neffectiveness of our method on DAISI dataset, which includes 290 procedures\nfrom various medical disciplines. Our approach outperforms the existing\nbaseline over all caption evaluation metrics. The results demonstrate the\nbenefits of the encoder-decoder structure backboned by transformer in handling\nmultimodal context.",
          "link": "http://arxiv.org/abs/2107.06964",
          "publishedOn": "2021-07-16T00:48:23.301Z",
          "wordCount": 550,
          "title": "Surgical Instruction Generation with Transformers. (arXiv:2107.06964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:23.284Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "The mainstream approach for filter pruning is usually either to force a\nhard-coded importance estimation upon a computation-heavy pretrained model to\nselect \"important\" filters, or to impose a hyperparameter-sensitive sparse\nconstraint on the loss objective to regularize the network training. In this\npaper, we present a novel filter pruning method, dubbed dynamic-coded filter\nfusion (DCFF), to derive compact CNNs in a computation-economical and\nregularization-free manner for efficient image classification. Each filter in\nour DCFF is firstly given an inter-similarity distribution with a temperature\nparameter as a filter proxy, on top of which, a fresh Kullback-Leibler\ndivergence based dynamic-coded criterion is proposed to evaluate the filter\nimportance. In contrast to simply keeping high-score filters in other methods,\nwe propose the concept of filter fusion, i.e., the weighted averages using the\nassigned proxies, as our preserved filters. We obtain a one-hot\ninter-similarity distribution as the temperature parameter approaches infinity.\nThus, the relative importance of each filter can vary along with the training\nof the compact CNN, leading to dynamically changeable fused filters without\nboth the dependency on the pretrained model and the introduction of sparse\nconstraints. Extensive experiments on classification benchmarks demonstrate the\nsuperiority of our DCFF over the compared counterparts. For example, our DCFF\nderives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while\nreaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained\nwith 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1\naccuracy on ILSVRC-2012. Our code, narrower models and training logs are\navailable at https://github.com/lmbxmu/DCFF.",
          "link": "http://arxiv.org/abs/2107.06916",
          "publishedOn": "2021-07-16T00:48:23.278Z",
          "wordCount": 705,
          "title": "Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion. (arXiv:2107.06916v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1\">Matteo Stefanini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>",
          "description": "Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.",
          "link": "http://arxiv.org/abs/2107.06912",
          "publishedOn": "2021-07-16T00:48:23.264Z",
          "wordCount": 664,
          "title": "From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1\">Thomas A. Langlois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">H. Charles Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>",
          "description": "Developments in machine learning interpretability techniques over the past\ndecade have provided new tools to observe the image regions that are most\ninformative for classification and localization in artificial neural networks\n(ANNs). Are the same regions similarly informative to human observers? Using\ndata from 78 new experiments and 6,610 participants, we show that passive\nattention techniques reveal a significant overlap with human visual selectivity\nestimates derived from 6 distinct behavioral tasks including visual\ndiscrimination, spatial localization, recognizability, free-viewing,\ncued-object search, and saliency search fixations. We find that input\nvisualizations derived from relatively simple ANN architectures probed using\nguided backpropagation methods are the best predictors of a shared component in\nthe joint variability of the human measures. We validate these correlational\nresults with causal manipulations using recognition experiments. We show that\nimages masked with ANN attention maps were easier for humans to classify than\ncontrol masks in a speeded recognition experiment. Similarly, we find that\nrecognition performance in the same ANN models was likewise influenced by\nmasking input images using human visual selectivity maps. This work contributes\na new approach to evaluating the biological and psychological validity of\nleading ANNs as models of human vision: by examining their similarities and\ndifferences in terms of their visual selectivity to the information contained\nin images.",
          "link": "http://arxiv.org/abs/2107.07013",
          "publishedOn": "2021-07-16T00:48:23.237Z",
          "wordCount": 658,
          "title": "Passive attention in artificial neural networks predicts human visual selectivity. (arXiv:2107.07013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kakaletsis_E/0/1/0/all/0/1\">Efstratios Kakaletsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">Nikos Nikolaidis</a>",
          "description": "In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)\npotential landing site detection using terrain information through\nidentification of flat areas, is presented. The algorithm utilizes digital\nelevation models (DEM) that represent the height distribution of an area. Flat\nareas which constitute appropriate landing zones for UAVs in normal or\nemergency situations result by thresholding the image gradient magnitude of the\ndigital surface model (DSM). The proposed technique also uses connected\ncomponents evaluation on the thresholded gradient image in order to discover\nconnected regions of sufficient size for landing. Moreover, man-made structures\nand vegetation areas are detected and excluded from the potential landing\nsites. Quantitative performance evaluation of the proposed landing site\ndetection algorithm in a number of areas on real world and synthetic datasets,\naccompanied by a comparison with a state-of-the-art algorithm, proves its\nefficiency and superiority.",
          "link": "http://arxiv.org/abs/2107.06921",
          "publishedOn": "2021-07-16T00:48:23.157Z",
          "wordCount": 604,
          "title": "Potential UAV Landing Sites Detection through Digital Elevation Models Analysis. (arXiv:2107.06921v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2011.04419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vikas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "Despite recent success, most contrastive self-supervised learning methods are\ndomain-specific, relying heavily on data augmentation techniques that require\nknowledge about a particular domain, such as image cropping and rotation. To\novercome such limitation, we propose a novel domain-agnostic approach to\ncontrastive learning, named DACL, that is applicable to domains where\ninvariances, and thus, data augmentation techniques, are not readily available.\nKey to our approach is the use of Mixup noise to create similar and dissimilar\nexamples by mixing data samples differently either at the input or hidden-state\nlevels. To demonstrate the effectiveness of DACL, we conduct experiments across\nvarious domains such as tabular data, images, and graphs. Our results show that\nDACL not only outperforms other domain-agnostic noising methods, such as\nGaussian-noise, but also combines well with domain-specific methods, such as\nSimCLR, to improve self-supervised visual representation learning. Finally, we\ntheoretically analyze our method and show advantages over the Gaussian-noise\nbased contrastive learning approach.",
          "link": "http://arxiv.org/abs/2011.04419",
          "publishedOn": "2021-07-21T02:01:37.638Z",
          "wordCount": 624,
          "title": "Towards Domain-Agnostic Contrastive Learning. (arXiv:2011.04419v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09519",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gaetan_F/0/1/0/all/0/1\">Frusque Gaetan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gabriel_M/0/1/0/all/0/1\">Michau Gabriel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olga_F/0/1/0/all/0/1\">Fink Olga</a>",
          "description": "Acoustic monitoring for machine fault detection is a recent and expanding\nresearch path that has already provided promising results for industries.\nHowever, it is impossible to collect enough data to learn all types of faults\nfrom a machine. Thus, new algorithms, trained using data from healthy\nconditions only, were developed to perform unsupervised anomaly detection. A\nkey issue in the development of these algorithms is the noise in the signals,\nas it impacts the anomaly detection performance. In this work, we propose a\npowerful data-driven and quasi non-parametric denoising strategy for spectral\ndata based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)\ndecomposition. This method is particularly adapted for machine emitting\nstationary sound. We demonstrate in a case study, the Malfunctioning Industrial\nMachine Investigation and Inspection (MIMII) baseline, how the use of our\ndenoising strategy leads to a sensible improvement of the unsupervised anomaly\ndetection. Such approaches are capable to make sound-based monitoring of\nindustrial processes more reliable.",
          "link": "http://arxiv.org/abs/2107.09519",
          "publishedOn": "2021-07-21T02:01:37.625Z",
          "wordCount": 635,
          "title": "Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection. (arXiv:2107.09519v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-07-21T02:01:37.619Z",
          "wordCount": 674,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05041",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1\">Gia-Lac Tran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1\">Dimitrios Milios</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1\">Pietro Michiardi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1\">Maurizio Filippone</a>",
          "description": "Approximations to Gaussian processes based on inducing variables, combined\nwith variational inference techniques, enable state-of-the-art sparse\napproaches to infer GPs at scale through mini batch-based learning. In this\nwork, we address one limitation of sparse GPs, which is due to the challenge in\ndealing with a large number of inducing variables without imposing a special\nstructure on the inducing inputs. In particular, we introduce a novel\nhierarchical prior, which imposes sparsity on the set of inducing variables. We\ntreat our model variationally, and we experimentally show considerable\ncomputational gains compared to standard sparse GPs when sparsity on the\ninducing variables is realized considering the nearest inducing inputs of a\nrandom mini-batch of the data. We perform an extensive experimental validation\nthat demonstrates the effectiveness of our approach compared to the\nstate-of-the-art. Our approach enables the possibility to use sparse GPs using\na large number of inducing points without incurring a prohibitive computational\ncost.",
          "link": "http://arxiv.org/abs/2011.05041",
          "publishedOn": "2021-07-21T02:01:37.612Z",
          "wordCount": 615,
          "title": "Sparse within Sparse Gaussian Processes using Neighbor Information. (arXiv:2011.05041v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-07-21T02:01:37.283Z",
          "wordCount": 773,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yue Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "The heterogeneity across devices usually hinders the optimization convergence\nand generalization performance of federated learning (FL) when the aggregation\nof devices' knowledge occurs in the gradient space. For example, devices may\ndiffer in terms of data distribution, network latency, input/output space,\nand/or model architecture, which can easily lead to the misalignment of their\nlocal gradients. To improve the tolerance to heterogeneity, we propose a novel\nfederated prototype learning (FedProto) framework in which the devices and\nserver communicate the class prototypes instead of the gradients. FedProto\naggregates the local prototypes collected from different devices, and then\nsends the global prototypes back to all devices to regularize the training of\nlocal models. The training on each device aims to minimize the classification\nerror on the local data while keeping the resulting local prototypes\nsufficiently close to the corresponding global ones. Through experiments, we\npropose a benchmark setting tailored for heterogeneous FL, with FedProto\noutperforming several recent FL approaches on multiple datasets.",
          "link": "http://arxiv.org/abs/2105.00243",
          "publishedOn": "2021-07-21T02:01:37.276Z",
          "wordCount": 629,
          "title": "FedProto: Federated Prototype Learning over Heterogeneous Devices. (arXiv:2105.00243v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawks_B/0/1/0/all/0/1\">Benjamin Hawks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_N/0/1/0/all/0/1\">Nicholas J. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1\">Alessandro Pappalardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umuroglu_Y/0/1/0/all/0/1\">Yaman Umuroglu</a>",
          "description": "Efficient machine learning implementations optimized for inference in\nhardware have wide-ranging benefits, depending on the application, from lower\ninference latency to higher data throughput and reduced energy consumption. Two\npopular techniques for reducing computation in neural networks are pruning,\nremoving insignificant synapses, and quantization, reducing the precision of\nthe calculations. In this work, we explore the interplay between pruning and\nquantization during the training of neural networks for ultra low latency\napplications targeting high energy physics use cases. Techniques developed for\nthis study have potential applications across many other domains. We study\nvarious configurations of pruning during quantization-aware training, which we\nterm quantization-aware pruning, and the effect of techniques like\nregularization, batch normalization, and different pruning schemes on\nperformance, computational complexity, and information content metrics. We find\nthat quantization-aware pruning yields more computationally efficient models\nthan either pruning or quantization alone for our task. Further,\nquantization-aware pruning typically performs similar to or better in terms of\ncomputational efficiency compared to other neural architecture search\ntechniques like Bayesian optimization. Surprisingly, while networks with\ndifferent training configurations can have similar performance for the\nbenchmark application, the information content in the network can vary\nsignificantly, affecting its generalizability.",
          "link": "http://arxiv.org/abs/2102.11289",
          "publishedOn": "2021-07-21T02:01:37.257Z",
          "wordCount": 709,
          "title": "Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference. (arXiv:2102.11289v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>",
          "description": "Learning effective representations in image-based environments is crucial for\nsample efficient Reinforcement Learning (RL). Unfortunately, in RL,\nrepresentation learning is confounded with the exploratory experience of the\nagent -- learning a useful representation requires diverse data, while\neffective exploration is only possible with coherent representations.\nFurthermore, we would like to learn representations that not only generalize\nacross tasks but also accelerate downstream exploration for efficient\ntask-specific training. To address these challenges we propose Proto-RL, a\nself-supervised framework that ties representation learning with exploration\nthrough prototypical representations. These prototypes simultaneously serve as\na summarization of the exploratory experience of an agent as well as a basis\nfor representing observations. We pre-train these task-agnostic representations\nand prototypes on environments without downstream task information. This\nenables state-of-the-art downstream policy learning on a set of difficult\ncontinuous control tasks.",
          "link": "http://arxiv.org/abs/2102.11271",
          "publishedOn": "2021-07-21T02:01:37.250Z",
          "wordCount": 596,
          "title": "Reinforcement Learning with Prototypical Representations. (arXiv:2102.11271v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "We create classical (non-quantum) dynamic data structures supporting queries\nfor recommender systems and least-squares regression that are comparable to\ntheir quantum analogues. De-quantizing such algorithms has received a flurry of\nattention in recent years; we obtain sharper bounds for these problems. More\nsignificantly, we achieve these improvements by arguing that the previous\nquantum-inspired algorithms for these problems are doing leverage or\nridge-leverage score sampling in disguise; these are powerful and standard\ntechniques in randomized numerical linear algebra. With this recognition, we\nare able to employ the large body of work in numerical linear algebra to obtain\nalgorithms for these problems that are simpler or faster (or both) than\nexisting approaches.",
          "link": "http://arxiv.org/abs/2011.04125",
          "publishedOn": "2021-07-21T02:01:37.244Z",
          "wordCount": 639,
          "title": "Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra. (arXiv:2011.04125v5 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caballero_M/0/1/0/all/0/1\">Michael Caballero</a>",
          "description": "One major sub-domain in the subject of polling public opinion with social\nmedia data is electoral prediction. Electoral prediction utilizing social media\ndata potentially would significantly affect campaign strategies, complementing\ntraditional polling methods and providing cheaper polling in real-time. First,\nthis paper explores past successful methods from research for analysis and\nprediction of the 2020 US Presidential Election using Twitter data. Then, this\nresearch proposes a new method for electoral prediction which combines\nsentiment, from NLP on the text of tweets, and structural data with aggregate\npolling, a time series analysis, and a special focus on Twitter users critical\nto the election. Though this method performed worse than its baseline of\npolling predictions, it is inconclusive whether this is an accurate method for\npredicting elections due to scarcity of data. More research and more data are\nneeded to accurately measure this method's overall effectiveness.",
          "link": "http://arxiv.org/abs/2107.09640",
          "publishedOn": "2021-07-21T02:01:37.237Z",
          "wordCount": 592,
          "title": "Predicting the 2020 US Presidential Election with Twitter. (arXiv:2107.09640v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hexu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.",
          "link": "http://arxiv.org/abs/2105.14686",
          "publishedOn": "2021-07-21T02:01:37.231Z",
          "wordCount": 624,
          "title": "Fully Hyperbolic Neural Networks. (arXiv:2105.14686v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">John Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kshitiz Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hongyuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1\">Ashkan Yousefpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1\">Mani Malek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1\">Dzmitry Huba</a>",
          "description": "Federated Learning (FL) trains a shared model across distributed devices\nwhile keeping the training data on the devices. Most FL schemes are\nsynchronous: they perform a synchronized aggregation of model updates from\nindividual devices. Synchronous training can be slow because of late-arriving\ndevices (stragglers). On the other hand, completely asynchronous training makes\nFL less private because of incompatibility with secure aggregation. In this\nwork, we propose a model aggregation scheme, FedBuff, that combines the best\nproperties of synchronous and asynchronous FL. Similar to synchronous FL,\nFedBuff is compatible with secure aggregation. Similar to asynchronous FL,\nFedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and\nsend updates to the server. The server aggregates client updates in a private\nbuffer until updates have been received, at which point a server model update\nis immediately performed. We provide theoretical convergence guarantees for\nFedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x\nfaster than previous proposals for synchronous FL (e.g., FedAvgM), and up to\n2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We\nshow that FedBuff is robust to different staleness distributions and is more\nscalable than synchronous FL techniques.",
          "link": "http://arxiv.org/abs/2106.06639",
          "publishedOn": "2021-07-21T02:01:37.215Z",
          "wordCount": 653,
          "title": "Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-07-21T02:01:37.209Z",
          "wordCount": 614,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1\">Mikhail Sarafanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1\">Nikolay O. Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>",
          "description": "In the paper, we propose an adaptive data-driven model-based approach for\nfilling the gaps in time series. The approach is based on the automated\nevolutionary identification of the optimal structure for a composite\ndata-driven model. It allows adapting the model for the effective gap-filling\nin a specific dataset without the involvement of the data scientist. As a case\nstudy, both synthetic and real datasets from different fields (environmental,\neconomic, etc) are used. The experiments confirm that the proposed approach\nallows achieving the higher quality of the gap restoration and improve the\neffectiveness of forecasting models.",
          "link": "http://arxiv.org/abs/2103.01124",
          "publishedOn": "2021-07-21T02:01:37.203Z",
          "wordCount": 584,
          "title": "Automated data-driven approach for gap filling in the time series using evolutionary learning. (arXiv:2103.01124v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reyes_J/0/1/0/all/0/1\">Jonatan Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorio_L/0/1/0/all/0/1\">Lisa Di Jorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_Kam_C/0/1/0/all/0/1\">Cecile Low-Kam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersten_Oertel_M/0/1/0/all/0/1\">Marta Kersten-Oertel</a>",
          "description": "Federated Learning using the Federated Averaging algorithm has shown great\nadvantages for large-scale applications that rely on collaborative learning,\nespecially when the training data is either unbalanced or inaccessible due to\nprivacy constraints. We hypothesize that Federated Averaging underestimates the\nfull extent of heterogeneity of data when the aggregation is performed. We\npropose Precision-weighted Federated Learning a novel algorithm that takes into\naccount the variance of the stochastic gradients when computing the weighted\naverage of the parameters of models trained in a Federated Learning setting.\nWith Precision-weighted Federated Learning, we provide an alternate averaging\nscheme that leverages the heterogeneity of the data when it has a large\ndiversity of features in its composition. Our method was evaluated using\nstandard image classification datasets with two different data partitioning\nstrategies (IID/non-IID) to measure the performance and speed of our method in\nresource-constrained environments, such as mobile and IoT devices. We obtained\na good balance between computational efficiency and convergence rates with\nPrecision-weighted Federated Learning. Our performance evaluations show 9%\nbetter predictions with MNIST, 18% with Fashion-MNIST, and 5% with CIFAR-10 in\nthe non-IID setting. Further reliability evaluations ratify the stability in\nour method by reaching a 99% reliability index with IID partitions and 96% with\nnon-IID partitions. In addition, we obtained a 20x speedup on Fashion-MNIST\nwith only 10 clients and up to 37x with 100 clients participating in the\naggregation concurrently per communication round. The results indicate that\nPrecision-weighted Federated Learning is an effective and faster alternative\napproach for aggregating private data, especially in domains where data is\nhighly heterogeneous.",
          "link": "http://arxiv.org/abs/2107.09627",
          "publishedOn": "2021-07-21T02:01:37.196Z",
          "wordCount": 688,
          "title": "Precision-Weighted Federated Learning. (arXiv:2107.09627v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00304",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Unke_O/0/1/0/all/0/1\">Oliver T. Unke</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chmiela_S/0/1/0/all/0/1\">Stefan Chmiela</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gastegger_M/0/1/0/all/0/1\">Michael Gastegger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schutt_K/0/1/0/all/0/1\">Kristof T. Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sauceda_H/0/1/0/all/0/1\">Huziel E. Sauceda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>",
          "description": "Machine-learned force fields (ML-FFs) combine the accuracy of ab initio\nmethods with the efficiency of conventional force fields. However, current\nML-FFs typically ignore electronic degrees of freedom, such as the total charge\nor spin state, and assume chemical locality, which is problematic when\nmolecules have inconsistent electronic states, or when nonlocal effects play a\nsignificant role. This work introduces SpookyNet, a deep neural network for\nconstructing ML-FFs with explicit treatment of electronic degrees of freedom\nand quantum nonlocality. Chemically meaningful inductive biases and analytical\ncorrections built into the network architecture allow it to properly model\nphysical limits. SpookyNet improves upon the current state-of-the-art (or\nachieves similar performance) on popular quantum chemistry data sets. Notably,\nit is able to generalize across chemical and conformational space and can\nleverage the learned chemical insights, e.g. by predicting unknown spin states,\nthus helping to close a further important remaining gap for today's machine\nlearning models in quantum chemistry.",
          "link": "http://arxiv.org/abs/2105.00304",
          "publishedOn": "2021-07-21T02:01:37.189Z",
          "wordCount": 621,
          "title": "SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects. (arXiv:2105.00304v2 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liangxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) allows multiple clients to collaboratively learn a\nglobally shared model through cycles of model aggregation and local model\ntraining, without the need to share data. Most existing FL methods train local\nmodels separately on different clients, and then simply average their\nparameters to obtain a centralized model on the server side. However, these\napproaches generally suffer from large aggregation errors and severe local\nforgetting, which are particularly bad in heterogeneous data settings. To\ntackle these issues, in this paper, we propose a novel FL framework that uses\nonline Laplace approximation to approximate posteriors on both the client and\nserver side. On the server side, a multivariate Gaussian product mechanism is\nemployed to construct and maximize a global posterior, largely reducing the\naggregation errors induced by large discrepancies between local models. On the\nclient side, a prior loss that uses the global posterior probabilistic\nparameters delivered from the server is designed to guide the local training.\nBinding such learning constraints from other clients enables our method to\nmitigate local forgetting. Finally, we achieve state-of-the-art results on\nseveral benchmarks, clearly demonstrating the advantages of the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2102.01936",
          "publishedOn": "2021-07-21T02:01:37.170Z",
          "wordCount": 660,
          "title": "A Bayesian Federated Learning Framework with Online Laplace Approximation. (arXiv:2102.01936v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riek_L/0/1/0/all/0/1\">Laurel D. Riek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "In many real-world applications, multiple agents seek to learn how to perform\nhighly related yet slightly different tasks in an online bandit learning\nprotocol. We formulate this problem as the $\\epsilon$-multi-player multi-armed\nbandit problem, in which a set of players concurrently interact with a set of\narms, and for each arm, the reward distributions for all players are similar\nbut not necessarily identical. We develop an upper confidence bound-based\nalgorithm, RobustAgg$(\\epsilon)$, that adaptively aggregates rewards collected\nby different players. In the setting where an upper bound on the pairwise\nsimilarities of reward distributions between players is known, we achieve\ninstance-dependent regret guarantees that depend on the amenability of\ninformation sharing across players. We complement these upper bounds with\nnearly matching lower bounds. In the setting where pairwise similarities are\nunknown, we provide a lower bound, as well as an algorithm that trades off\nminimax regret guarantees for adaptivity to unknown similarity structure.",
          "link": "http://arxiv.org/abs/2010.15390",
          "publishedOn": "2021-07-21T02:01:37.164Z",
          "wordCount": 634,
          "title": "Multitask Bandit Learning Through Heterogeneous Feedback Aggregation. (arXiv:2010.15390v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-07-21T02:01:37.157Z",
          "wordCount": 584,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shohei Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_M/0/1/0/all/0/1\">Michio Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>",
          "description": "Discovering causal structures among latent factors from observed data is a\nparticularly challenging problem. Despite some efforts for this problem,\nexisting methods focus on the single-domain data only. In this paper, we\npropose Multi-Domain Linear Non-Gaussian Acyclic Models for Latent Factors\n(MD-LiNA), where the causal structure among latent factors of interest is\nshared for all domains, and we provide its identification results. The model\nenriches the causal representation for multi-domain data. We propose an\nintegrated two-phase algorithm to estimate the model. In particular, we first\nlocate the latent factors and estimate the factor loading matrix. Then to\nuncover the causal structure among shared latent factors of interest, we derive\na score function based on the characterization of independence relations\nbetween external influences and the dependence relations between multi-domain\nlatent factors and latent factors of interest. We show that the proposed method\nprovides locally consistent estimators. Experimental results on both synthetic\nand real-world data demonstrate the efficacy and robustness of our approach.",
          "link": "http://arxiv.org/abs/2009.09176",
          "publishedOn": "2021-07-21T02:01:37.150Z",
          "wordCount": 635,
          "title": "Causal Discovery with Multi-Domain LiNGAM for Latent Factors. (arXiv:2009.09176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_Y/0/1/0/all/0/1\">Yamin Arefeen</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Beker_O/0/1/0/all/0/1\">Onur Beker</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1\">Jaejin Cho</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Heng Yu</a> (4), <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a> (1 and 5 and 6), <a href=\"http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1\">Berkin Bilgic</a> (3 and 5 and 7) ((1) Massachusetts Institute of Technology, (2) &#xc9;cole Polytechnique F&#xe9;d&#xe9;rale de Lausanne, (3) Athinoula A. Martinos Center for Biomedical Imaging (4) Tsinghua University, (5) Harvard-MIT Health Sciences and Technology, (6) Institute for Medical Engineering and Science, (7) Harvard Medical School)",
          "description": "Purpose: To develop a scan-specific model that estimates and corrects k-space\nerrors made when reconstructing accelerated Magnetic Resonance Imaging (MRI)\ndata.\n\nMethods: Scan-Specific Artifact Reduction in k-space (SPARK) trains a\nconvolutional-neural-network to estimate and correct k-space errors made by an\ninput reconstruction technique by back-propagating from the mean-squared-error\nloss between an auto-calibration signal (ACS) and the input technique's\nreconstructed ACS. First, SPARK is applied to GRAPPA and demonstrates improved\nrobustness over other scan-specific models, such as RAKI and residual-RAKI.\nSubsequent experiments demonstrate that SPARK synergizes with residual-RAKI to\nimprove reconstruction performance. SPARK also improves reconstruction quality\nwhen applied to advanced acquisition and reconstruction techniques like 2D\nvirtual coil (VC-) GRAPPA, 2D LORAKS, 3D GRAPPA without an integrated ACS\nregion, and 2D/3D wave-encoded images.\n\nResults: SPARK yields 1.5x - 2x RMSE reduction when applied to GRAPPA and\nimproves robustness to ACS size for various acceleration rates in comparison to\nother scan-specific techniques. When applied to advanced reconstruction\ntechniques such as residual-RAKI, 2D VC-GRAPPA and LORAKS, SPARK achieves up to\n20% RMSE improvement. SPARK with 3D GRAPPA also improves performance by ~2x and\nperceived image quality without a fully sampled ACS region. Finally, SPARK\nsynergizes with non-cartesian 2D and 3D wave-encoding imaging by reducing RMSE\nbetween 20-25% and providing qualitative improvements.\n\nConclusion: SPARK synergizes with physics-based acquisition and\nreconstruction techniques to improve accelerated MRI by training scan-specific\nmodels to estimate and correct reconstruction errors in k-space.",
          "link": "http://arxiv.org/abs/2104.01188",
          "publishedOn": "2021-07-21T02:01:37.134Z",
          "wordCount": 766,
          "title": "Scan Specific Artifact Reduction in K-space (SPARK) Neural Networks Synergize with Physics-based Reconstruction to Accelerate MRI. (arXiv:2104.01188v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04656",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya K. Ramdas</a>",
          "description": "We prove calibration guarantees for the popular histogram binning (also\ncalled uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram\nbinning has displayed strong practical performance, but theoretical guarantees\nhave only been shown for sample split versions that avoid 'double dipping' the\ndata. We demonstrate that the statistical cost of sample splitting is\npractically significant on a credit default dataset. We then prove calibration\nguarantees for the original method that double dips the data, using a certain\nMarkov property of order statistics. Based on our results, we make practical\nrecommendations for choosing the number of bins in histogram binning. In our\nillustrative simulations, we propose a new tool for assessing calibration --\nvalidity plots -- which provide more information than an ECE estimate. Code for\nthis work will be made publicly available at\nhttps://github.com/aigen/df-posthoc-calibration.",
          "link": "http://arxiv.org/abs/2105.04656",
          "publishedOn": "2021-07-21T02:01:37.126Z",
          "wordCount": 594,
          "title": "Distribution-free calibration guarantees for histogram binning without sample splitting. (arXiv:2105.04656v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Although the optimization objectives for learning neural networks are highly\nnon-convex, gradient-based methods have been wildly successful at learning\nneural networks in practice. This juxtaposition has led to a number of recent\nstudies on provable guarantees for neural networks trained by gradient descent.\nUnfortunately, the techniques in these works are often highly specific to the\nproblem studied in each setting, relying on different assumptions on the\ndistribution, optimization parameters, and network architectures, making it\ndifficult to generalize across different settings. In this work, we propose a\nunified non-convex optimization framework for the analysis of neural network\ntraining. We introduce the notions of proxy convexity and proxy\nPolyak-Lojasiewicz (PL) inequalities, which are satisfied if the original\nobjective function induces a proxy objective function that is implicitly\nminimized when using gradient methods. We show that stochastic gradient descent\n(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads\nto efficient guarantees for proxy objective functions. We further show that\nmany existing guarantees for neural networks trained by gradient descent can be\nunified through proxy convexity and proxy PL inequalities.",
          "link": "http://arxiv.org/abs/2106.13792",
          "publishedOn": "2021-07-21T02:01:37.120Z",
          "wordCount": 659,
          "title": "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1\">Nicolas Tempelmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerhake_U/0/1/0/all/0/1\">Udo Feuerhake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wage_O/0/1/0/all/0/1\">Oskar Wage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>",
          "description": "The discovery of spatio-temporal dependencies within urban road networks that\ncause Recurrent Congestion (RC) patterns is crucial for numerous real-world\napplications, including urban planning and scheduling of public transportation\nservices. While most existing studies investigate temporal patterns of RC\nphenomena, the influence of the road network topology on RC is often\noverlooked. This article proposes the ST-Discovery algorithm, a novel\nunsupervised spatio-temporal data mining algorithm that facilitates the\neffective data-driven discovery of RC dependencies induced by the road network\ntopology using real-world traffic data. We factor out regularly reoccurring\ntraffic phenomena, such as rush hours, mainly induced by the daytime, by\nmodelling and systematically exploiting temporal traffic load outliers. We\npresent an algorithm that first constructs connected subgraphs of the road\nnetwork based on the traffic speed outliers. Second, the algorithm identifies\npairs of subgraphs that indicate spatio-temporal correlations in their traffic\nload behaviour to identify topological dependencies within the road network.\nFinally, we rank the identified subgraph pairs based on the dependency score\ndetermined by our algorithm. Our experimental results demonstrate that\nST-Discovery can effectively reveal topological dependencies in urban road\nnetworks.",
          "link": "http://arxiv.org/abs/2107.09554",
          "publishedOn": "2021-07-21T02:01:37.112Z",
          "wordCount": 630,
          "title": "Mining Topological Dependencies of Recurrent Congestion in Road Networks. (arXiv:2107.09554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04754",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Aizenbud_Y/0/1/0/all/0/1\">Yariv Aizenbud</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>",
          "description": "A common observation in data-driven applications is that high dimensional\ndata has a low intrinsic dimension, at least locally. In this work, we consider\nthe problem of estimating a $d$ dimensional sub-manifold of $\\mathbb{R}^D$ from\na finite set of noisy samples. Assuming that the data was sampled uniformly\nfrom a tubular neighborhood of $\\mathcal{M}\\in \\mathcal{C}^k$, a compact\nmanifold without boundary, we present an algorithm that takes a point $r$ from\nthe tubular neighborhood and outputs $\\hat p_n\\in \\mathbb{R}^D$, and\n$\\widehat{T_{\\hat p_n}\\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.\nWe prove that as the number of samples $n\\to\\infty$ the point $\\hat p_n$\nconverges to $p\\in \\mathcal{M}$ and $\\widehat{T_{\\hat p_n}\\mathcal{M}}$\nconverges to $T_p\\mathcal{M}$ (the tangent space at that point) with high\nprobability. Furthermore, we show that the estimation yields asymptotic rates\nof convergence of $n^{-\\frac{k}{2k + d}}$ for the point estimation and\n$n^{-\\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates\nare known to be optimal for the case of function estimation.",
          "link": "http://arxiv.org/abs/2105.04754",
          "publishedOn": "2021-07-21T02:01:37.106Z",
          "wordCount": 610,
          "title": "Non-Parametric Estimation of Manifolds from Noisy Data. (arXiv:2105.04754v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1\">V.B. Surya Prasath</a>",
          "description": "The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.",
          "link": "http://arxiv.org/abs/2107.09602",
          "publishedOn": "2021-07-21T02:01:37.098Z",
          "wordCount": 729,
          "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review. (arXiv:2107.09602v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00222",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1\">Ali Unlu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.",
          "link": "http://arxiv.org/abs/2103.00222",
          "publishedOn": "2021-07-21T02:01:37.091Z",
          "wordCount": 583,
          "title": "Variational Laplace for Bayesian neural networks. (arXiv:2103.00222v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1\">Uiwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dahuin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyemi Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Generative adversarial networks (GANs) with clustered latent spaces can\nperform conditional generation in a completely unsupervised manner. However,\nthe salient attributes of unlabeled data in the real-world are mostly\nimbalanced. Existing unsupervised conditional GANs cannot properly cluster the\nattributes in their latent spaces because they assume uniform distributions of\nthe attributes. To address this problem, we theoretically derive Stein latent\noptimization that provides reparameterizable gradient estimations of the latent\ndistribution parameters assuming a Gaussian mixture prior in a continuous\nlatent space. Structurally, we introduce an encoder network and a novel\ncontrastive loss to help generated data from a single mixture component to\nrepresent a single attribute. We confirm that the proposed method, named Stein\nLatent Optimization for GANs (SLOGAN), successfully learns the balanced or\nimbalanced attributes and performs unsupervised tasks such as unsupervised\nconditional generation, unconditional generation, and cluster assignment even\nin the absence of information of the attributes (e.g. the imbalance ratio).\nMoreover, we demonstrate that the attributes to be learned can be manipulated\nusing a small amount of probe data.",
          "link": "http://arxiv.org/abs/2106.05319",
          "publishedOn": "2021-07-21T02:01:37.085Z",
          "wordCount": 634,
          "title": "Stein Latent Optimization for GANs. (arXiv:2106.05319v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Marzyeh Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Deep Metric Learning (DML) aims to find representations suitable for\nzero-shot transfer to a priori unknown test distributions. However, common\nevaluation protocols only test a single, fixed data split in which train and\ntest classes are assigned randomly. More realistic evaluations should consider\na broad spectrum of distribution shifts with potentially varying degree and\ndifficulty. In this work, we systematically construct train-test splits of\nincreasing difficulty and present the ooDML benchmark to characterize\ngeneralization under out-of-distribution shifts in DML. ooDML is designed to\nprobe the generalization performance on much more challenging, diverse\ntrain-to-test distribution shifts. Based on our new benchmark, we conduct a\nthorough empirical analysis of state-of-the-art DML methods. We find that while\ngeneralization tends to consistently degrade with difficulty, some methods are\nbetter at retaining performance as the distribution shift increases. Finally,\nwe propose few-shot DML as an efficient way to consistently improve\ngeneralization in response to unknown test shifts presented in ooDML. Code\navailable here:\nhttps://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.",
          "link": "http://arxiv.org/abs/2107.09562",
          "publishedOn": "2021-07-21T02:01:37.052Z",
          "wordCount": 604,
          "title": "Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning. (arXiv:2107.09562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>",
          "description": "Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.",
          "link": "http://arxiv.org/abs/2107.04570",
          "publishedOn": "2021-07-21T02:01:37.040Z",
          "wordCount": 687,
          "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernau_D/0/1/0/all/0/1\">Daniel Bernau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eibl_G/0/1/0/all/0/1\">G&#xfc;nther Eibl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1\">Philip W. Grassal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1\">Hannah Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1\">Florian Kerschbaum</a>",
          "description": "Differential privacy allows bounding the influence that training data records\nhave on a machine learning model. To use differential privacy in machine\nlearning, data scientists must choose privacy parameters $(\\epsilon,\\delta)$.\nChoosing meaningful privacy parameters is key, since models trained with weak\nprivacy parameters might result in excessive privacy leakage, while strong\nprivacy parameters might overly degrade model utility. However, privacy\nparameter values are difficult to choose for two main reasons. First, the\ntheoretical upper bound on privacy loss $(\\epsilon,\\delta)$ might be loose,\ndepending on the chosen sensitivity and data distribution of practical\ndatasets. Second, legal requirements and societal norms for anonymization often\nrefer to individual identifiability, to which $(\\epsilon,\\delta)$ are only\nindirectly related.\n\nWe transform $(\\epsilon,\\delta)$ to a bound on the Bayesian posterior belief\nof the adversary assumed by differential privacy concerning the presence of any\nrecord in the training dataset. The bound holds for multidimensional queries\nunder composition, and we show that it can be tight in practice. Furthermore,\nwe derive an identifiability bound, which relates the adversary assumed in\ndifferential privacy to previous work on membership inference adversaries. We\nformulate an implementation of this differential privacy adversary that allows\ndata scientists to audit model training and compute empirical identifiability\nscores and empirical $(\\epsilon,\\delta)$.",
          "link": "http://arxiv.org/abs/2103.02913",
          "publishedOn": "2021-07-21T02:01:37.032Z",
          "wordCount": 690,
          "title": "Quantifying identifiability to choose and audit $\\epsilon$ in differentially private deep learning. (arXiv:2103.02913v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>",
          "description": "One of the central problems in machine learning is domain adaptation. Unlike\npast theoretical work, we consider a new model for subpopulation shift in the\ninput or representation space. In this work, we propose a provably effective\nframework for domain adaptation based on label propagation. In our analysis, we\nuse a simple but realistic expansion assumption, proposed in\n\\citet{wei2021theoretical}. Using a teacher classifier trained on the source\ndomain, our algorithm not only propagates to the target domain but also\nimproves upon the teacher. By leveraging existing generalization bounds, we\nalso obtain end-to-end finite-sample guarantees on the entire algorithm. In\naddition, we extend our theoretical framework to a more general setting of\nsource-to-target transfer based on a third unlabeled dataset, which can be\neasily applied in various learning scenarios. Inspired by our theory, we adapt\nconsistency-based semi-supervised learning methods to domain adaptation\nsettings and gain significant improvements.",
          "link": "http://arxiv.org/abs/2102.11203",
          "publishedOn": "2021-07-21T02:01:37.021Z",
          "wordCount": 628,
          "title": "A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Devos_A/0/1/0/all/0/1\">Arnout Devos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>",
          "description": "In this paper, we propose a learning algorithm that enables a model to\nquickly exploit commonalities among related tasks from an unseen task\ndistribution, before quickly adapting to specific tasks from that same\ndistribution. We investigate how learning with different task distributions can\nfirst improve adaptability by meta-finetuning on related tasks before improving\ngoal task generalization with finetuning. Synthetic regression experiments\nvalidate the intuition that learning to meta-learn improves adaptability and\nconsecutively generalization. Experiments on more complex image classification,\ncontinual regression, and reinforcement learning tasks demonstrate that\nlearning to meta-learn generally improves task-specific adaptation. The\nmethodology, setup, and hypotheses in this proposal were positively evaluated\nby peer review before conclusive experiments were carried out.",
          "link": "http://arxiv.org/abs/2012.02684",
          "publishedOn": "2021-07-21T02:01:37.004Z",
          "wordCount": 577,
          "title": "Model-Agnostic Learning to Meta-Learn. (arXiv:2012.02684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1\">Ryan Goldhahn</a>",
          "description": "To tackle the susceptibility of deep neural networks to examples, the\nadversarial training has been proposed which provides a notion of robust\nthrough an inner maximization problem presenting the first-order embedded\nwithin the outer minimization of the training loss. To generalize the\nadversarial robustness over different perturbation types, the adversarial\ntraining method has been augmented with the improved inner maximization\npresenting a union of multiple perturbations e.g., various $\\ell_p$\nnorm-bounded perturbations.",
          "link": "http://arxiv.org/abs/2104.10586",
          "publishedOn": "2021-07-21T02:01:36.979Z",
          "wordCount": 591,
          "title": "Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations. (arXiv:2104.10586v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13677",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ke_Z/0/1/0/all/0/1\">Ziwen Ke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhuo-Xu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jing Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhilang Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_L/0/1/0/all/0/1\">Leslie Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjie Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>",
          "description": "In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)\ndecomposition, or robust principal component analysis (PCA), has achieved\nstunning performance. However, the selection of the parameters of L+S is\nempirical, and the acceleration rate is limited, which are common failings of\niterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many\ndeep learning approaches have been proposed to address these issues, but few of\nthem use a low-rank prior. In this paper, a model-based low-rank plus sparse\nnetwork, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In\nparticular, we use an alternating linearized minimization method to solve the\noptimization problem with low-rank and sparse regularization. Learned soft\nsingular value thresholding is introduced to ensure the clear separation of the\nL component and S component. Then, the iterative steps are unrolled into a\nnetwork in which the regularization parameters are learnable. We prove that the\nproposed L+S-Net achieves global convergence under two standard assumptions.\nExperiments on retrospective and prospective cardiac cine datasets show that\nthe proposed model outperforms state-of-the-art CS and existing deep learning\nmethods and has great potential for extremely high acceleration factors (up to\n24x).",
          "link": "http://arxiv.org/abs/2010.13677",
          "publishedOn": "2021-07-21T02:01:36.971Z",
          "wordCount": 677,
          "title": "Deep Low-rank plus Sparse Network for Dynamic MR Imaging. (arXiv:2010.13677v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.",
          "link": "http://arxiv.org/abs/2103.05108",
          "publishedOn": "2021-07-21T02:01:36.929Z",
          "wordCount": 587,
          "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations. (arXiv:2103.05108v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>",
          "description": "We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for\nvisual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic\napproach that uses data augmentation to learn directly from pixels. We\nintroduce several improvements that yield state-of-the-art results on the\nDeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid\nlocomotion tasks directly from pixel observations, previously unattained by\nmodel-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides\nsignificantly better computational footprint compared to prior work, with the\nmajority of tasks taking just 8 hours to train on a single GPU. Finally, we\npublicly release DrQ-v2's implementation to provide RL practitioners with a\nstrong and computationally efficient baseline.",
          "link": "http://arxiv.org/abs/2107.09645",
          "publishedOn": "2021-07-21T02:01:36.923Z",
          "wordCount": 545,
          "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning. (arXiv:2107.09645v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1\">Adam Katona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1\">Daniel W. Franks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>",
          "description": "One of the most important lessons from the success of deep learning is that\nlearned representations tend to perform much better at any task compared to\nrepresentations we design by hand. Yet evolution of evolvability algorithms,\nwhich aim to automatically learn good genetic representations, have received\nrelatively little attention, perhaps because of the large amount of\ncomputational power they require. The recent method Evolvability ES allows\ndirect selection for evolvability with little computation. However, it can only\nbe used to solve problems where evolvability and task performance are aligned.\nWe propose Quality Evolvability ES, a method that simultaneously optimizes for\ntask performance and evolvability and without this restriction. Our proposed\napproach Quality Evolvability has similar motivation to Quality Diversity\nalgorithms, but with some important differences. While Quality Diversity aims\nto find an archive of diverse and well-performing, but potentially genetically\ndistant individuals, Quality Evolvability aims to find a single individual with\na diverse and well-performing distribution of offspring. By doing so Quality\nEvolvability is forced to discover more evolvable representations. We\ndemonstrate on robotic locomotion control tasks that Quality Evolvability ES,\nsimilarly to Quality Diversity methods, can learn faster than objective-based\nmethods and can handle deceptive problems.",
          "link": "http://arxiv.org/abs/2103.10790",
          "publishedOn": "2021-07-21T02:01:36.890Z",
          "wordCount": 687,
          "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring. (arXiv:2103.10790v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach to conformal prediction when the target task has\nlimited data available for training. Conformal prediction identifies a small\nset of promising output candidates in place of a single prediction, with\nguarantees that the set contains the correct answer with high probability. When\ntraining data is limited, however, the predicted set can easily become unusably\nlarge. In this work, we obtain substantially tighter prediction sets while\nmaintaining desirable marginal guarantees by casting conformal prediction as a\nmeta-learning paradigm over exchangeable collections of auxiliary tasks. Our\nconformalization algorithm is simple, fast, and agnostic to the choice of\nunderlying model, learning algorithm, or dataset. We demonstrate the\neffectiveness of this approach across a number of few-shot classification and\nregression tasks in natural language processing, computer vision, and\ncomputational chemistry for drug discovery.",
          "link": "http://arxiv.org/abs/2102.08898",
          "publishedOn": "2021-07-21T02:01:36.863Z",
          "wordCount": 603,
          "title": "Few-shot Conformal Prediction with Auxiliary Tasks. (arXiv:2102.08898v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turian_J/0/1/0/all/0/1\">Joseph Turian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shier_J/0/1/0/all/0/1\">Jordie Shier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzanetakis_G/0/1/0/all/0/1\">George Tzanetakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNally_K/0/1/0/all/0/1\">Kirk McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_M/0/1/0/all/0/1\">Max Henry</a>",
          "description": "We release synth1B1, a multi-modal audio corpus consisting of 1 billion\n4-second synthesized sounds, paired with the synthesis parameters used to\ngenerate them. The dataset is 100x larger than any audio dataset in the\nliterature. We also introduce torchsynth, an open source modular synthesizer\nthat generates the synth1B1 samples on-the-fly at 16200x faster than real-time\n(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth\ntimbre and subtractive synth pitch. Using these datasets, we demonstrate new\nrank-based evaluation criteria for existing audio representations. Finally, we\npropose a novel approach to synthesizer hyperparameter optimization.",
          "link": "http://arxiv.org/abs/2104.12922",
          "publishedOn": "2021-07-21T02:01:36.832Z",
          "wordCount": 571,
          "title": "One Billion Audio Sounds from GPU-enabled Modular Synthesis. (arXiv:2104.12922v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.09067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieber_D/0/1/0/all/0/1\">David Bieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>",
          "description": "Sampling is a fundamental technique, and sampling without replacement is\noften desirable when duplicate samples are not beneficial. Within machine\nlearning, sampling is useful for generating diverse outputs from a trained\nmodel. We present an elegant procedure for sampling without replacement from a\nbroad class of randomized programs, including generative neural models that\nconstruct outputs sequentially. Our procedure is efficient even for\nexponentially-large output spaces. Unlike prior work, our approach is\nincremental, i.e., samples can be drawn one at a time, allowing for increased\nflexibility. We also present a new estimator for computing expectations from\nsamples drawn without replacement. We show that incremental sampling without\nreplacement is applicable to many domains, e.g., program synthesis and\ncombinatorial optimization.",
          "link": "http://arxiv.org/abs/2002.09067",
          "publishedOn": "2021-07-21T02:01:36.825Z",
          "wordCount": 582,
          "title": "Incremental Sampling Without Replacement for Sequence Models. (arXiv:2002.09067v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hayakawa_S/0/1/0/all/0/1\">Satoshi Hayakawa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>",
          "description": "We study kernel quadrature rules with positive weights for probability\nmeasures on general domains. Our theoretical analysis combines the spectral\nproperties of the kernel with random sampling of points. This results in\neffective algorithms to construct kernel quadrature rules with positive weights\nand small worst-case error. Besides additional robustness, our numerical\nexperiments indicate that this can achieve fast convergence rates that compete\nwith the optimal bounds in well-known examples.",
          "link": "http://arxiv.org/abs/2107.09597",
          "publishedOn": "2021-07-21T02:01:36.719Z",
          "wordCount": 506,
          "title": "Positively Weighted Kernel Quadrature via Subsampling. (arXiv:2107.09597v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuting Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangxiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Changzhi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangrong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>",
          "description": "The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses\n(HCoVs), which spreads rapidly around the world. Compared with new drug\ndevelopment, drug repurposing may be the best shortcut for treating COVID-19.\nTherefore, we constructed a comprehensive heterogeneous network based on the\nHCoVs-related target proteins and use the previously proposed deepDTnet, to\ndiscover potential drug candidates for COVID-19. We obtain high performance in\npredicting the possible drugs effective for COVID-19 related proteins. In\nsummary, this work utilizes a powerful heterogeneous network-based deep\nlearning method, which may be beneficial to quickly identify candidate\nrepurposable drugs toward future clinical trials for COVID-19. The code and\ndata are available at https://github.com/stjin-XMU/HnDR-COVID.",
          "link": "http://arxiv.org/abs/2107.09217",
          "publishedOn": "2021-07-21T02:01:36.701Z",
          "wordCount": 601,
          "title": "Heterogeneous network-based drug repurposing for COVID-19. (arXiv:2107.09217v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wanguang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>",
          "description": "Linear discriminant analysis (LDA) is a widely used algorithm in machine\nlearning to extract a low-dimensional representation of high-dimensional data,\nit features to find the orthogonal discriminant projection subspace by using\nthe Fisher discriminant criterion. However, the traditional Euclidean-based\nmethods for solving LDA are easily convergent to spurious local minima and\nhardly obtain an optimal solution. To address such a problem, in this paper, we\npropose a novel algorithm namely Riemannian-based discriminant analysis (RDA)\nfor subspace learning. In order to obtain an explicit solution, we transform\nthe traditional Euclidean-based methods to the Riemannian manifold space and\nuse the trust-region method to learn the discriminant projection subspace. We\ncompare the proposed algorithm to existing variants of LDA, as well as the\nunsupervised tensor decomposition methods on image classification tasks. The\nnumerical results suggest that RDA achieves state-of-the-art performance in\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2101.08032",
          "publishedOn": "2021-07-21T02:01:36.638Z",
          "wordCount": 618,
          "title": "Riemannian Manifold Optimization for Discriminant Subspace Learning. (arXiv:2101.08032v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shubhanshu Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fields_G/0/1/0/all/0/1\">Greg Fields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>",
          "description": "Machine learning models trained on uncurated datasets can often end up\nadversely affecting inputs belonging to underrepresented groups. To address\nthis issue, we consider the problem of adaptively constructing training sets\nwhich allow us to learn classifiers that are fair in a minimax sense. We first\npropose an adaptive sampling algorithm based on the principle of optimism, and\nderive theoretical bounds on its performance. We also propose heuristic\nextensions of this algorithm suitable for application to large scale, practical\nproblems. Next, by deriving algorithm independent lower-bounds for a specific\nclass of problems, we show that the performance achieved by our adaptive scheme\ncannot be improved in general. We then validate the benefits of adaptively\nconstructing training sets via experiments on synthetic tasks with logistic\nregression classifiers, as well as on several real-world tasks using\nconvolutional neural networks (CNNs).",
          "link": "http://arxiv.org/abs/2103.00755",
          "publishedOn": "2021-07-21T02:01:36.628Z",
          "wordCount": 596,
          "title": "Adaptive Sampling for Minimax Fair Classification. (arXiv:2103.00755v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_E/0/1/0/all/0/1\">Eike Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potdevin_Y/0/1/0/all/0/1\">Yannik Potdevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Esfandiar Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zidowitz_S/0/1/0/all/0/1\">Stephan Zidowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breyer_S/0/1/0/all/0/1\">Sabrina Breyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1\">Sandra Henn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechmann_L/0/1/0/all/0/1\">Ludwig Pechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leucker_M/0/1/0/all/0/1\">Martin Leucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostalski_P/0/1/0/all/0/1\">Philipp Rostalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_C/0/1/0/all/0/1\">Christian Herzog</a>",
          "description": "Machine learning is expected to fuel significant improvements in medical\ncare. To ensure that fundamental principles such as beneficence, respect for\nhuman autonomy, prevention of harm, justice, privacy, and transparency are\nrespected, medical machine learning applications must be developed responsibly.\nIn this paper, we survey the technical challenges involved in creating medical\nmachine learning systems responsibly and in conformity with existing\nregulations, as well as possible solutions to address these challenges. We\nbegin by providing a brief overview of existing regulations affecting medical\nmachine learning, showing that properties such as safety, robustness,\nreliability, privacy, security, transparency, explainability, and\nnondiscrimination are all demanded already by existing law and regulations -\nalbeit, in many cases, to an uncertain degree. Next, we discuss the underlying\ntechnical challenges, possible ways for addressing them, and their respective\nmerits and drawbacks. We notice that distribution shift, spurious correlations,\nmodel underspecification, and data scarcity represent severe challenges in the\nmedical context (and others) that are very difficult to solve with classical\nblack-box deep neural networks. Important measures that may help to address\nthese challenges include the use of large and representative datasets and\nfederated learning as a means to that end, the careful exploitation of domain\nknowledge wherever feasible, the use of inherently transparent models,\ncomprehensive model testing and verification, as well as stakeholder inclusion.",
          "link": "http://arxiv.org/abs/2107.09546",
          "publishedOn": "2021-07-21T02:01:36.619Z",
          "wordCount": 693,
          "title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions. (arXiv:2107.09546v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.13972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1\">Rory Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_G/0/1/0/all/0/1\">Geoffrey Holmes</a>",
          "description": "SHAP (SHapley Additive exPlanation) values provide a game theoretic\ninterpretation of the predictions of machine learning models based on Shapley\nvalues. While exact calculation of SHAP values is computationally intractable\nin general, a recursive polynomial-time algorithm called TreeShap is available\nfor decision tree models. However, despite its polynomial time complexity,\nTreeShap can become a significant bottleneck in practical machine learning\npipelines when applied to large decision tree ensembles. We present\nGPUTreeShap, a modified TreeShap algorithm suitable for massively parallel\ncomputation on graphics processing units. Our approach first preprocesses each\ndecision tree to isolate variable sized sub-problems from the original\nrecursive algorithm, then solves a bin packing problem, and finally maps\nsub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel\nexecution with specialised hardware instructions. With a single NVIDIA Tesla\nV100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of\nup to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU\nimplementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also\nexperiment with multi-GPU computing using eight V100 GPUs, demonstrating\nthroughput of 1.2M rows per second -- equivalent CPU-based performance is\nestimated to require 6850 CPU cores.",
          "link": "http://arxiv.org/abs/2010.13972",
          "publishedOn": "2021-07-21T02:01:36.593Z",
          "wordCount": 663,
          "title": "GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles. (arXiv:2010.13972v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.05675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinhai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>",
          "description": "Few-shot learning aims at rapidly adapting to novel categories with only a\nhandful of samples at test time, which has been predominantly tackled with the\nidea of meta-learning. However, meta-learning approaches essentially learn\nacross a variety of few-shot tasks and thus still require large-scale training\ndata with fine-grained supervision to derive a generalized model, thereby\ninvolving prohibitive annotation cost. In this paper, we advance the few-shot\nclassification paradigm towards a more challenging scenario, i.e.,\ncross-granularity few-shot classification, where the model observes only coarse\nlabels during training while is expected to perform fine-grained classification\nduring testing. This task largely relieves the annotation cost since\nfine-grained labeling usually requires strong domain-specific expertise. To\nbridge the cross-granularity gap, we approximate the fine-grained data\ndistribution by greedy clustering of each coarse-class into pseudo-fine-classes\naccording to the similarity of image embeddings. We then propose a\nmeta-embedder that jointly optimizes the visual- and semantic-discrimination,\nin both instance-wise and coarse class-wise, to obtain a good feature space for\nthis coarse-to-fine pseudo-labeling process. Extensive experiments and ablation\nstudies are conducted to demonstrate the effectiveness and robustness of our\napproach on three representative datasets.",
          "link": "http://arxiv.org/abs/2007.05675",
          "publishedOn": "2021-07-21T02:01:36.578Z",
          "wordCount": 674,
          "title": "Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding. (arXiv:2007.05675v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05206",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dongxuan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hancun Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaocheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sheng Chen</a>",
          "description": "Huge overhead of beam training imposes a significant challenge in\nmillimeter-wave (mmWave) wireless communications. To address this issue, in\nthis paper, we propose a wide beam based training approach to calibrate the\nnarrow beam direction according to the channel power leakage. To handle the\ncomplex nonlinear properties of the channel power leakage, deep learning is\nutilized to predict the optimal narrow beam directly. Specifically, three deep\nlearning assisted calibrated beam training schemes are proposed. The first\nscheme adopts convolution neural network to implement the prediction based on\nthe instantaneous received signals of wide beam training. We also perform the\nadditional narrow beam training based on the predicted probabilities for\nfurther beam direction calibrations. However, the first scheme only depends on\none wide beam training, which lacks the robustness to noise. To tackle this\nproblem, the second scheme adopts long-short term memory (LSTM) network for\ntracking the movement of users and calibrating the beam direction according to\nthe received signals of prior beam training, in order to enhance the robustness\nto noise. To further reduce the overhead of wide beam training, our third\nscheme, an adaptive beam training strategy, selects partial wide beams to be\ntrained based on the prior received signals. Two criteria, namely, optimal\nneighboring criterion and maximum probability criterion, are designed for the\nselection. Furthermore, to handle mobile scenarios, auxiliary LSTM is\nintroduced to calibrate the directions of the selected wide beams more\nprecisely. Simulation results demonstrate that our proposed schemes achieve\nsignificantly higher beamforming gain with smaller beam training overhead\ncompared with the conventional and existing deep-learning based counterparts.",
          "link": "http://arxiv.org/abs/2101.05206",
          "publishedOn": "2021-07-21T02:01:36.571Z",
          "wordCount": 740,
          "title": "Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave Communication Systems. (arXiv:2101.05206v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Langevin_A/0/1/0/all/0/1\">Antoine Langevin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Carbonneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheriet_M/0/1/0/all/0/1\">Mohamed Cheriet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagnon_G/0/1/0/all/0/1\">Ghyslain Gagnon</a>",
          "description": "Non-intrusive load monitoring (NILM) is a technique that uses a single sensor\nto measure the total power consumption of a building. Using an energy\ndisaggregation method, the consumption of individual appliances can be\nestimated from the aggregate measurement. Recent disaggregation algorithms have\nsignificantly improved the performance of NILM systems. However, the\ngeneralization capability of these methods to different houses as well as the\ndisaggregation of multi-state appliances are still major challenges. In this\npaper we address these issues and propose an energy disaggregation approach\nbased on the variational autoencoders framework. The probabilistic encoder\nmakes this approach an efficient model for encoding information relevant to the\nreconstruction of the target appliance consumption. In particular, the proposed\nmodel accurately generates more complex load profiles, thus improving the power\nsignal reconstruction of multi-state appliances. Moreover, its regularized\nlatent space improves the generalization capabilities of the model across\ndifferent houses. The proposed model is compared to state-of-the-art NILM\napproaches on the UK-DALE and REFIT datasets, and yields competitive results.\nThe mean absolute error reduces by 18% on average across all appliances\ncompared to the state-of-the-art. The F1-Score increases by more than 11%,\nshowing improvements for the detection of the target appliance in the aggregate\nmeasurement.",
          "link": "http://arxiv.org/abs/2103.12177",
          "publishedOn": "2021-07-21T02:01:36.564Z",
          "wordCount": 669,
          "title": "Energy Disaggregation using Variational Autoencoders. (arXiv:2103.12177v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hatua_A/0/1/0/all/0/1\">Amartya Hatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rakesh M. Verma</a>",
          "description": "This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.",
          "link": "http://arxiv.org/abs/2103.08001",
          "publishedOn": "2021-07-21T02:01:36.542Z",
          "wordCount": 598,
          "title": "Claim Verification using a Multi-GAN based Model. (arXiv:2103.08001v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1\">Tim Franzmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>",
          "description": "Can artificial agents learn to assist others in achieving their goals without\nknowing what those goals are? Generic reinforcement learning agents could be\ntrained to behave altruistically towards others by rewarding them for\naltruistic behaviour, i.e., rewarding them for benefiting other agents in a\ngiven situation. Such an approach assumes that other agents' goals are known so\nthat the altruistic agent can cooperate in achieving those goals. However,\nexplicit knowledge of other agents' goals is often difficult to acquire. Even\nassuming such knowledge to be given, training of altruistic agents would\nrequire manually-tuned external rewards for each new environment. Thus, it is\nbeneficial to develop agents that do not depend on external supervision and can\nlearn altruistic behaviour in a task-agnostic manner. Assuming that other\nagents rationally pursue their goals, we hypothesize that giving them more\nchoices will allow them to pursue those goals better. Some concrete examples\ninclude opening a door for others or safeguarding them to pursue their\nobjectives without interference. We formalize this concept and propose an\naltruistic agent that learns to increase the choices another agent has by\nmaximizing the number of states that the other agent can reach in its future.\nWe evaluate our approach on three different multi-agent environments where\nanother agent's success depends on the altruistic agent's behaviour. Finally,\nwe show that our unsupervised agents can perform comparably to agents\nexplicitly trained to work cooperatively. In some cases, our agents can even\noutperform the supervised ones.",
          "link": "http://arxiv.org/abs/2107.09598",
          "publishedOn": "2021-07-21T02:01:36.535Z",
          "wordCount": 681,
          "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards. (arXiv:2107.09598v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1\">David Brandfonbrener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Most prior approaches to offline reinforcement learning (RL) have taken an\niterative actor-critic approach involving off-policy evaluation. In this paper\nwe show that simply doing one step of constrained/regularized policy\nimprovement using an on-policy Q estimate of the behavior policy performs\nsurprisingly well. This one-step algorithm beats the previously reported\nresults of iterative algorithms on a large portion of the D4RL benchmark. The\nsimple one-step baseline achieves this strong performance without many of the\ntricks used by previously proposed iterative algorithms and is more robust to\nhyperparameters. We argue that the relatively poor performance of iterative\napproaches is a result of the high variance inherent in doing off-policy\nevaluation and magnified by the repeated optimization of policies against those\nhigh-variance estimates. In addition, we hypothesize that the strong\nperformance of the one-step algorithm is due to a combination of favorable\nstructure in the environment and behavior policy.",
          "link": "http://arxiv.org/abs/2106.08909",
          "publishedOn": "2021-07-21T02:01:36.527Z",
          "wordCount": 607,
          "title": "Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09543",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1\">Lidia Garrucho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1\">Akis Linardos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>",
          "description": "Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.",
          "link": "http://arxiv.org/abs/2107.09543",
          "publishedOn": "2021-07-21T02:01:36.520Z",
          "wordCount": 691,
          "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_J/0/1/0/all/0/1\">Jana Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_J/0/1/0/all/0/1\">Johannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muriedas_J/0/1/0/all/0/1\">Juan Pedro Guti&#xe9;rrez H. Muriedas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettin_U/0/1/0/all/0/1\">Uwe Mettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampe_A/0/1/0/all/0/1\">Alexander Lampe</a>",
          "description": "In recent years, reinforcement learning (RL) has gained increasing attention\nin control engineering. Especially, policy gradient methods are widely used. In\nthis work, we improve the tracking performance of proximal policy optimization\n(PPO) for arbitrary reference signals by incorporating information about future\nreference values. Two variants of extending the argument of the actor and the\ncritic taking future reference values into account are presented. In the first\nvariant, global future reference values are added to the argument. For the\nsecond variant, a novel kind of residual space with future reference values\napplicable to model-free reinforcement learning is introduced. Our approach is\nevaluated against a PI controller on a simple drive train model. We expect our\nmethod to generalize to arbitrary references better than previous approaches,\npointing towards the applicability of RL to control real systems.",
          "link": "http://arxiv.org/abs/2107.09647",
          "publishedOn": "2021-07-21T02:01:36.513Z",
          "wordCount": 584,
          "title": "Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information. (arXiv:2107.09647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Modern machine learning models with high accuracy are often miscalibrated --\nthe predicted top probability does not reflect the actual accuracy, and tends\nto be over-confident. It is commonly believed that such over-confidence is\nmainly due to over-parametrization, in particular when the model is large\nenough to memorize the training data and maximize the confidence.\n\nIn this paper, we show theoretically that over-parametrization is not the\nonly reason for over-confidence. We prove that logistic regression is\ninherently over-confident, in the realizable, under-parametrized setting where\nthe data is generated from the logistic model, and the sample size is much\nlarger than the number of parameters. Further, this over-confidence happens for\ngeneral well-specified binary classification problems as long as the activation\nis symmetric and concave on the positive part. Perhaps surprisingly, we also\nshow that over-confidence is not always the case -- there exists another\nactivation function (and a suitable loss function) under which the learned\nclassifier is under-confident at some probability values. Overall, our theory\nprovides a precise characterization of calibration in realizable binary\nclassification, which we verify on simulations and real data experiments.",
          "link": "http://arxiv.org/abs/2102.07856",
          "publishedOn": "2021-07-21T02:01:36.497Z",
          "wordCount": 668,
          "title": "Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification. (arXiv:2102.07856v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13416",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Burton_C/0/1/0/all/0/1\">Charles Burton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stubbs_S/0/1/0/all/0/1\">Spencer Stubbs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Onyisi_P/0/1/0/all/0/1\">Peter Onyisi</a>",
          "description": "Mixture Density Networks (MDNs) can be used to generate probability density\nfunctions of model parameters $\\boldsymbol{\\theta}$ given a set of observables\n$\\mathbf{x}$. In some applications, training data are available only for\ndiscrete values of a continuous parameter $\\boldsymbol{\\theta}$. In such\nsituations a number of performance-limiting issues arise which can result in\nbiased estimates. We demonstrate the usage of MDNs for parameter estimation,\ndiscuss the origins of the biases, and propose a corrective method for each\nissue.",
          "link": "http://arxiv.org/abs/2103.13416",
          "publishedOn": "2021-07-21T02:01:36.490Z",
          "wordCount": 549,
          "title": "Mixture Density Network Estimation of Continuous Variable Maximum Likelihood Using Discrete Training Samples. (arXiv:2103.13416v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternating\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-07-21T02:01:36.482Z",
          "wordCount": 753,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-07-21T02:01:36.474Z",
          "wordCount": 576,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tony Xiao Han</a>",
          "description": "Integrated sensing and communication (ISAC) is a promising technology to\nimprove the band-utilization efficiency via spectrum sharing or hardware\nsharing between radar and communication systems. Since a common radio resource\nbudget is shared by both functionalities, there exists a tradeoff between the\nsensing and communication performance. However, this tradeoff curve is\ncurrently unknown in ISAC systems with human motion recognition tasks based on\ndeep learning. To fill this gap, this paper formulates and solves a\nmulti-objective optimization problem which simultaneously maximizes the\nrecognition accuracy and the communication data rate. The key ingredient of\nthis new formulation is a nonlinear recognition accuracy model with respect to\nthe wireless resources, where the model is derived from power function\nregression of the system performance of the deep spectrogram network. To avoid\ncost-expensive data collection procedures, a primitive-based autoregressive\nhybrid (PBAH) channel model is developed, which facilitates efficient training\nand testing dataset generation for human motion recognition in a virtual\nenvironment. Extensive results demonstrate that the proposed wireless\nrecognition accuracy and PBAH channel models match the actual experimental data\nvery well. Moreover, it is found that the accuracy-rate region consists of a\ncommunication saturation zone, a sensing saturation zone, and a\ncommunication-sensing adversarial zone, of which the third zone achieves the\ndesirable balanced performance for ISAC systems.",
          "link": "http://arxiv.org/abs/2107.09621",
          "publishedOn": "2021-07-21T02:01:36.468Z",
          "wordCount": 682,
          "title": "Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate. (arXiv:2107.09621v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.03974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr H. Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Ashvin Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Laura Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Catherine Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Meta-reinforcement learning (RL) can meta-train policies that adapt to new\ntasks with orders of magnitude less data than standard RL, but meta-training\nitself is costly and time-consuming. If we can meta-train on offline data, then\nwe can reuse the same static dataset, labeled once with rewards for different\ntasks, to meta-train policies that adapt to a variety of new tasks at meta-test\ntime. Although this capability would make meta-RL a practical tool for\nreal-world use, offline meta-RL presents additional challenges beyond online\nmeta-RL or standard offline RL settings. Meta-RL learns an exploration strategy\nthat collects data for adapting, and also meta-trains a policy that quickly\nadapts to data from a new task. Since this policy was meta-trained on a fixed,\noffline dataset, it might behave unpredictably when adapting to data collected\nby the learned exploration strategy, which differs systematically from the\noffline data and thus induces distributional shift. We do not want to remove\nthis distributional shift by simply adopting a conservative exploration\nstrategy, because learning an exploration strategy enables an agent to collect\nbetter data for faster adaptation. Instead, we propose a hybrid offline meta-RL\nalgorithm, which uses offline data with rewards to meta-train an adaptive\npolicy, and then collects additional unsupervised online data, without any\nreward labels to bridge this distribution shift. By not requiring reward labels\nfor online collection, this data can be much cheaper to collect. We compare our\nmethod to prior work on offline meta-RL on simulated robot locomotion and\nmanipulation tasks and find that using additional unsupervised online data\ncollection leads to a dramatic improvement in the adaptive capabilities of the\nmeta-trained policies, matching the performance of fully online meta-RL on a\nrange of challenging domains that require generalization to new tasks.",
          "link": "http://arxiv.org/abs/2107.03974",
          "publishedOn": "2021-07-21T02:01:36.449Z",
          "wordCount": 747,
          "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision. (arXiv:2107.03974v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03814",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birrell_J/0/1/0/all/0/1\">Jeremiah Birrell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dupuis_P/0/1/0/all/0/1\">Paul Dupuis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1\">Luc Rey-Bellet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>",
          "description": "We derive a new variational formula for the R\\'enyi family of divergences,\n$R_\\alpha(Q\\|P)$, between probability measures $Q$ and $P$. Our result\ngeneralizes the classical Donsker-Varadhan variational formula for the\nKullback-Leibler divergence. We further show that this R\\'enyi variational\nformula holds over a range of function spaces; this leads to a formula for the\noptimizer under very weak assumptions and is also key in our development of a\nconsistency theory for R\\'enyi divergence estimators. By applying this theory\nto neural-network estimators, we show that if a neural network family satisfies\none of several strengthened versions of the universal approximation property\nthen the corresponding R\\'enyi divergence estimator is consistent. In contrast\nto density-estimator based methods, our estimators involve only expectations\nunder $Q$ and $P$ and hence are more effective in high dimensional systems. We\nillustrate this via several numerical examples of neural network estimation in\nsystems of up to 5000 dimensions.",
          "link": "http://arxiv.org/abs/2007.03814",
          "publishedOn": "2021-07-21T02:01:36.442Z",
          "wordCount": 643,
          "title": "Variational Representations and Neural Network Estimation of R\\'enyi Divergences. (arXiv:2007.03814v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erez_L/0/1/0/all/0/1\">Liad Erez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>",
          "description": "We study the online learning with feedback graphs framework introduced by\nMannor and Shamir (2011), in which the feedback received by the online learner\nis specified by a graph $G$ over the available actions. We develop an algorithm\nthat simultaneously achieves regret bounds of the form:\n$\\smash{\\mathcal{O}(\\sqrt{\\theta(G) T})}$ with adversarial losses;\n$\\mathcal{O}(\\theta(G)\\operatorname{polylog}{T})$ with stochastic losses; and\n$\\mathcal{O}(\\theta(G)\\operatorname{polylog}{T} + \\smash{\\sqrt{\\theta(G) C})}$\nwith stochastic losses subject to $C$ adversarial corruptions. Here,\n$\\theta(G)$ is the clique covering number of the graph $G$. Our algorithm is an\ninstantiation of Follow-the-Regularized-Leader with a novel regularization that\ncan be seen as a product of a Tsallis entropy component (inspired by Zimmert\nand Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted\nstochastic case by Amir et al. (2020)), thus subtly interpolating between the\ntwo forms of entropies. One of our key technical contributions is in\nestablishing the convexity of this regularizer and controlling its inverse\nHessian, despite its complex product structure.",
          "link": "http://arxiv.org/abs/2107.09572",
          "publishedOn": "2021-07-21T02:01:36.436Z",
          "wordCount": 583,
          "title": "Best-of-All-Worlds Bounds for Online Learning with Feedback Graphs. (arXiv:2107.09572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haotian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianxiao Li</a>",
          "description": "We study the approximation properties of convolutional architectures applied\nto time series modelling, which can be formulated mathematically as a\nfunctional approximation problem. In the recurrent setting, recent results\nreveal an intricate connection between approximation efficiency and memory\nstructures in the data generation process. In this paper, we derive parallel\nresults for convolutional architectures, with WaveNet being a prime example.\nOur results reveal that in this new setting, approximation efficiency is not\nonly characterised by memory, but also additional fine structures in the target\nrelationship. This leads to a novel definition of spectrum-based regularity\nthat measures the complexity of temporal relationships under the convolutional\napproximation scheme. These analyses provide a foundation to understand the\ndifferences between architectural choices for time series modelling and can\ngive theoretically grounded guidance for practical applications.",
          "link": "http://arxiv.org/abs/2107.09355",
          "publishedOn": "2021-07-21T02:01:36.429Z",
          "wordCount": 578,
          "title": "Approximation Theory of Convolutional Architectures for Time Series Modelling. (arXiv:2107.09355v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>",
          "description": "This open problem asks whether there exists an online learning algorithm for\nbinary classification that guarantees, for all target concepts, to make a\nsublinear number of mistakes, under only the assumption that the (possibly\nrandom) sequence of points X allows that such a learning algorithm can exist\nfor that sequence. As a secondary problem, it also asks whether a specific\nconcise condition completely determines whether a given (possibly random)\nsequence of points X admits the existence of online learning algorithms\nguaranteeing a sublinear number of mistakes for all target concepts.",
          "link": "http://arxiv.org/abs/2107.09542",
          "publishedOn": "2021-07-21T02:01:36.422Z",
          "wordCount": 547,
          "title": "Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?. (arXiv:2107.09542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1\">Shubham Dhayarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1\">V.K. Viekash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>",
          "description": "Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.",
          "link": "http://arxiv.org/abs/2107.06212",
          "publishedOn": "2021-07-21T02:01:36.416Z",
          "wordCount": 713,
          "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_S/0/1/0/all/0/1\">Shanel Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1\">Benjamin Th&#xe9;rien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsene_Racicot_L/0/1/0/all/0/1\">Laurent Als&#xe8;ne-Racicot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1\">Michael Eickenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "The wavelet scattering transform creates geometric invariants and deformation\nstability from an initial structured signal. In multiple signal domains it has\nbeen shown to yield more discriminative representations compared to other\nnon-learned representations, and to outperform learned representations in\ncertain tasks, particularly on limited labeled data and highly structured\nsignals. The wavelet filters used in the scattering transform are typically\nselected to create a tight frame via a parameterized mother wavelet. Focusing\non Morlet wavelets, we propose to instead adapt the scales, orientations, and\nslants of the filters to produce problem-specific parametrizations of the\nscattering transform. We show that our learned versions of the scattering\ntransform yield significant performance gains over the standard scattering\ntransform in the small sample classification settings, and our empirical\nresults suggest that tight frames may not always be necessary for scattering\ntransforms to extract effective representations.",
          "link": "http://arxiv.org/abs/2107.09539",
          "publishedOn": "2021-07-21T02:01:36.391Z",
          "wordCount": 583,
          "title": "Parametric Scattering Networks. (arXiv:2107.09539v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tung T. Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Hien Quoc Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzetta_T/0/1/0/all/0/1\">Thomas L. Marzetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthaiou_M/0/1/0/all/0/1\">Michail Matthaiou</a>",
          "description": "Federated learning (FL) has been considered as a promising learning framework\nfor future machine learning systems due to its privacy preservation and\ncommunication efficiency. In beyond-5G/6G systems, it is likely to have\nmultiple FL groups with different learning purposes. This scenario leads to a\nquestion: How does a wireless network support multiple FL groups? As an answer,\nwe first propose to use a cell-free massive multiple-input multiple-output\n(MIMO) network to guarantee the stable operation of multiple FL processes by\nletting the iterations of these FL processes be executed together within a\nlarge-scale coherence time. We then develop a novel scheme that asynchronously\nexecutes the iterations of FL processes under multicasting downlink and\nconventional uplink transmission protocols. Finally, we propose a\nsimple/low-complexity resource allocation algorithm which optimally chooses the\npower and computation resources to minimize the execution time of each\niteration of each FL process.",
          "link": "http://arxiv.org/abs/2107.09577",
          "publishedOn": "2021-07-21T02:01:36.385Z",
          "wordCount": 611,
          "title": "How Does Cell-Free Massive MIMO Support Multiple Federated Learning Groups?. (arXiv:2107.09577v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiabao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>",
          "description": "Deep neural networks (DNNs) are under threat from adversarial example\nattacks. The adversary can easily change the outputs of DNNs by adding small\nwell-designed perturbations to inputs. Adversarial example detection is a\nfundamental work for robust DNNs-based service. Adversarial examples show the\ndifference between humans and DNNs in image recognition. From a human-centric\nperspective, image features could be divided into dominant features that are\ncomprehensible to humans, and recessive features that are incomprehensible to\nhumans, yet are exploited by DNNs. In this paper, we reveal that imperceptible\nadversarial examples are the product of recessive features misleading neural\nnetworks, and an adversarial attack is essentially a kind of method to enrich\nthese recessive features in the image. The imperceptibility of the adversarial\nexamples indicates that the perturbations enrich recessive features, yet hardly\naffect dominant features. Therefore, adversarial examples are sensitive to\nfiltering off recessive features, while benign examples are immune to such\noperation. Inspired by this idea, we propose a label-only adversarial detection\napproach that is referred to as feature-filter. Feature-filter utilizes\ndiscrete cosine transform to approximately separate recessive features from\ndominant features, and gets a mutant image that is filtered off recessive\nfeatures. By only comparing DNN's prediction labels on the input and its\nmutant, feature-filter can real-time detect imperceptible adversarial examples\nat high accuracy and few false positives.",
          "link": "http://arxiv.org/abs/2107.09502",
          "publishedOn": "2021-07-21T02:01:36.368Z",
          "wordCount": 653,
          "title": "Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features. (arXiv:2107.09502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Emma Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Andy Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rayan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.",
          "link": "http://arxiv.org/abs/2103.09957",
          "publishedOn": "2021-07-21T02:01:36.362Z",
          "wordCount": 697,
          "title": "CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays. (arXiv:2103.09957v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09362",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "Since production-level trained deep neural networks (DNNs) are of a great\nbusiness value, protecting such DNN models against copyright infringement and\nunauthorized access is in a rising demand. However, conventional model\nprotection methods focused only the image classification task, and these\nprotection methods were never applied to semantic segmentation although it has\nan increasing number of applications. In this paper, we propose to protect\nsemantic segmentation models from unauthorized access by utilizing block-wise\ntransformation with a secret key for the first time. Protected models are\ntrained by using transformed images. Experiment results show that the proposed\nprotection method allows rightful users with the correct key to access the\nmodel to full capacity and deteriorate the performance for unauthorized users.\nHowever, protected models slightly drop the segmentation performance compared\nto non-protected models.",
          "link": "http://arxiv.org/abs/2107.09362",
          "publishedOn": "2021-07-21T02:01:36.354Z",
          "wordCount": 606,
          "title": "Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access. (arXiv:2107.09362v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasaei_R/0/1/0/all/0/1\">Rozhin Yasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shih-Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeini_E/0/1/0/all/0/1\">Emad Kasaeyan Naeini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Aggressive time-to-market constraints and enormous hardware design and\nfabrication costs have pushed the semiconductor industry toward hardware\nIntellectual Properties (IP) core design. However, the globalization of the\nintegrated circuits (IC) supply chain exposes IP providers to theft and illegal\nredistribution of IPs. Watermarking and fingerprinting are proposed to detect\nIP piracy. Nevertheless, they come with additional hardware overhead and cannot\nguarantee IP security as advanced attacks are reported to remove the watermark,\nforge, or bypass it. In this work, we propose a novel methodology, GNN4IP, to\nassess similarities between circuits and detect IP piracy. We model the\nhardware design as a graph and construct a graph neural network model to learn\nits behavior using the comprehensive dataset of register transfer level codes\nand gate-level netlists that we have gathered. GNN4IP detects IP piracy with\n96% accuracy in our dataset and recognizes the original IP in its obfuscated\nversion with 100% accuracy.",
          "link": "http://arxiv.org/abs/2107.09130",
          "publishedOn": "2021-07-21T02:01:36.345Z",
          "wordCount": 594,
          "title": "GNN4IP: Graph Neural Network for Hardware Intellectual Property Piracy Detection. (arXiv:2107.09130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09510",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaessen_T/0/1/0/all/0/1\">Thomas Vaessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myin_Germeys_I/0/1/0/all/0/1\">Inez Myin-Germeys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "Multimodal wearable physiological data in daily life settings have been used\nto estimate self-reported stress labels.However, missing data modalities in\ndata collection make it challenging to leverage all the collected samples.\nBesides, heterogeneous sensor data and labels among individuals add challenges\nin building robust stress detection models. In this paper, we proposed a\nmodality fusion network (MFN) to train models and infer self-reported binary\nstress labels under both complete and incomplete modality condition. In\naddition, we applied a personalized attention (PA) strategy to leverage\npersonalized representation along with the generalized one-size-fits-all model.\nWe evaluated our methods on a multimodal wearable sensor dataset (N=41)\nincluding galvanic skin response (GSR) and electrocardiogram (ECG). Compared to\nthe baseline method using the samples with complete modalities, the performance\nof the MFN improved by 1.6\\% in f1-scores. On the other hand, the proposed PA\nstrategy showed a 2.3\\% higher stress detection f1-score and approximately up\nto 70\\% reduction in personalized model parameter size (9.1 MB) compared to the\nprevious state-of-the-art transfer learning strategy (29.3 MB).",
          "link": "http://arxiv.org/abs/2107.09510",
          "publishedOn": "2021-07-21T02:01:36.339Z",
          "wordCount": 618,
          "title": "Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild. (arXiv:2107.09510v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1\">Alexander Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehring_L/0/1/0/all/0/1\">Lukas Gehring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1\">Tanja Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "The problem of selecting an algorithm that appears most suitable for a\nspecific instance of an algorithmic problem class, such as the Boolean\nsatisfiability problem, is called instance-specific algorithm selection. Over\nthe past decade, the problem has received considerable attention, resulting in\na number of different methods for algorithm selection. Although most of these\nmethods are based on machine learning, surprisingly little work has been done\non meta learning, that is, on taking advantage of the complementarity of\nexisting algorithm selection methods in order to combine them into a single\nsuperior algorithm selector. In this paper, we introduce the problem of meta\nalgorithm selection, which essentially asks for the best way to combine a given\nset of algorithm selectors. We present a general methodological framework for\nmeta algorithm selection as well as several concrete learning methods as\ninstantiations of this framework, essentially combining ideas of meta learning\nand ensemble learning. In an extensive experimental evaluation, we demonstrate\nthat ensembles of algorithm selectors can significantly outperform single\nalgorithm selectors and have the potential to form the new state of the art in\nalgorithm selection.",
          "link": "http://arxiv.org/abs/2107.09414",
          "publishedOn": "2021-07-21T02:01:36.271Z",
          "wordCount": 624,
          "title": "Algorithm Selection on a Meta Level. (arXiv:2107.09414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_H/0/1/0/all/0/1\">Hongbin Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>",
          "description": "In this paper we describe a speaker diarization system that enables\nlocalization and identification of all speakers present in a conversation or\nmeeting. We propose a novel systematic approach to tackle several long-standing\nchallenges in speaker diarization tasks: (1) to segment and separate\noverlapping speech from two speakers; (2) to estimate the number of speakers\nwhen participants may enter or leave the conversation at any time; (3) to\nprovide accurate speaker identification on short text-independent utterances;\n(4) to track down speakers movement during the conversation; (5) to detect\nspeaker change incidence real-time. First, a differential directional\nmicrophone array-based approach is exploited to capture the target speakers'\nvoice in far-field adverse environment. Second, an online speaker-location\njoint clustering approach is proposed to keep track of speaker location. Third,\nan instant speaker number detector is developed to trigger the mechanism that\nseparates overlapped speech. The results suggest that our system effectively\nincorporates spatial information and achieves significant gains.",
          "link": "http://arxiv.org/abs/2107.09321",
          "publishedOn": "2021-07-21T02:01:36.264Z",
          "wordCount": 619,
          "title": "A Real-time Speaker Diarization System Based on Spatial Spectrum. (arXiv:2107.09321v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>",
          "description": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.",
          "link": "http://arxiv.org/abs/2107.09099",
          "publishedOn": "2021-07-21T02:01:36.257Z",
          "wordCount": 554,
          "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09145",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1\">Wooseok Ha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1\">Francois Lanusse</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_E/0/1/0/all/0/1\">Eli Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dang_S/0/1/0/all/0/1\">Song Dang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+He_K/0/1/0/all/0/1\">Kangmin He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Upadhyayula_S/0/1/0/all/0/1\">Srigokul Upadhyayula</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>",
          "description": "Recent deep-learning models have achieved impressive prediction performance,\nbut often sacrifice interpretability and computational efficiency.\nInterpretability is crucial in many disciplines, such as science and medicine,\nwhere models must be carefully vetted or where interpretation is the goal\nitself. Moreover, interpretable models are concise and often yield\ncomputational efficiency. Here, we propose adaptive wavelet distillation (AWD),\na method which aims to distill information from a trained neural network into a\nwavelet transform. Specifically, AWD penalizes feature attributions of a neural\nnetwork in the wavelet domain to learn an effective multi-resolution wavelet\ntransform. The resulting model is highly predictive, concise, computationally\nefficient, and has properties (such as a multi-scale structure) which make it\neasy to interpret. In close collaboration with domain experts, we showcase how\nAWD addresses challenges in two real-world settings: cosmological parameter\ninference and molecular-partner prediction. In both cases, AWD yields a\nscientifically interpretable and concise model which gives predictive\nperformance better than state-of-the-art neural networks. Moreover, AWD\nidentifies predictive features that are scientifically meaningful in the\ncontext of respective domains. All code and models are released in a\nfull-fledged package available on Github\n(https://github.com/Yu-Group/adaptive-wavelets).",
          "link": "http://arxiv.org/abs/2107.09145",
          "publishedOn": "2021-07-21T02:01:36.204Z",
          "wordCount": 630,
          "title": "Adaptive wavelet distillation from neural networks through interpretations. (arXiv:2107.09145v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Deep learning and especially the use of Deep Neural Networks (DNNs) provides\nimpressive results in various regression and classification tasks. However, to\nachieve these results, there is a high demand for computing and storing\nresources. This becomes problematic when, for instance, real-time, mobile\napplications are considered, in which the involved (embedded) devices have\nlimited resources. A common way of addressing this problem is to transform the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Within the MCA framework, we\npropose a clustering-based approach that is able to increase the number of\nemployed centroids/representatives, while at the same time, have an\nacceleration gain compared to conventional, $k$-means based approaches. This is\nachieved by imposing a special structure to the employed representatives, which\nis enabled by the particularities of the problem at hand. Moreover, the\ntheoretical acceleration gains are presented and the key system\nhyper-parameters that affect that gain, are identified. Extensive evaluation\nstudies carried out using various state-of-the-art DNN models trained in image\nclassification, validate the superiority of the proposed method as compared for\nits use in MCA tasks.",
          "link": "http://arxiv.org/abs/2107.09095",
          "publishedOn": "2021-07-21T02:01:36.197Z",
          "wordCount": 623,
          "title": "A New Clustering-Based Technique for the Acceleration of Deep Convolutional Networks. (arXiv:2107.09095v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_J/0/1/0/all/0/1\">Jackie Ayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Na Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">X. Jessie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>",
          "description": "It is extremely important to ensure a safe takeover transition in\nconditionally automated driving. One of the critical factors that quantifies\nthe safe takeover transition is takeover time. Previous studies identified the\neffects of many factors on takeover time, such as takeover lead time,\nnon-driving tasks, modalities of the takeover requests (TORs), and scenario\nurgency. However, there is a lack of research to predict takeover time by\nconsidering these factors all at the same time. Toward this end, we used\neXtreme Gradient Boosting (XGBoost) to predict the takeover time using a\ndataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley\nAdditive exPlanation) to analyze and explain the effects of the predictors on\ntakeover time. We identified seven most critical predictors that resulted in\nthe best prediction performance. Their main effects and interaction effects on\ntakeover time were examined. The results showed that the proposed approach\nprovided both good performance and explainability. Our findings have\nimplications on the design of in-vehicle monitoring and alert systems to\nfacilitate the interaction between the drivers and the automated vehicle.",
          "link": "http://arxiv.org/abs/2107.09545",
          "publishedOn": "2021-07-21T02:01:36.176Z",
          "wordCount": 615,
          "title": "Predicting Driver Takeover Time in Conditionally Automated Driving. (arXiv:2107.09545v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilmet_V/0/1/0/all/0/1\">Vincent Wilmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sauraj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redl_T/0/1/0/all/0/1\">Tabea Redl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandaker_H/0/1/0/all/0/1\">H&#xe5;kon Sandaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenning Li</a>",
          "description": "Anomaly detection in images plays a significant role for many applications\nacross all industries, such as disease diagnosis in healthcare or quality\nassurance in manufacturing. Manual inspection of images, when extended over a\nmonotonously repetitive period of time is very time consuming and can lead to\nanomalies being overlooked.Artificial neural networks have proven themselves\nvery successful on simple, repetitive tasks, in some cases even outperforming\nhumans. Therefore, in this paper we investigate different methods of deep\nlearning, including supervised and unsupervised learning, for anomaly detection\napplied to a quality assurance use case. We utilize the MVTec anomaly dataset\nand develop three different models, a CNN for supervised anomaly detection,\nKD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly\ndetection and a DCGAN for generating reconstructed images. By experiments, we\nfound that KD-CAE performs better on the anomaly datasets compared to CNN and\nNI-CAE, with NI-CAE performing the best on the Transistor dataset. We also\nimplemented a DCGAN for the creation of new training data but due to\ncomputational limitation and lack of extrapolating the mechanics of AnoGAN, we\nrestricted ourselves just to the generation of GAN based images. We conclude\nthat unsupervised methods are more powerful for anomaly detection in images,\nespecially in a setting where only a small amount of anomalous data is\navailable, or the data is unlabeled.",
          "link": "http://arxiv.org/abs/2107.09204",
          "publishedOn": "2021-07-21T02:01:36.145Z",
          "wordCount": 686,
          "title": "A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images. (arXiv:2107.09204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09507",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1\">Jian Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yisi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1\">Zirui Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sourina_O/0/1/0/all/0/1\">Olga Sourina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_Wittig_W/0/1/0/all/0/1\">Wolfgang M&#xfc;ller-Wittig</a>",
          "description": "In the context of electroencephalogram (EEG)-based driver drowsiness\nrecognition, it is still a challenging task to design a calibration-free\nsystem, since there exists a significant variability of EEG signals among\ndifferent subjects and recording sessions. As deep learning has received much\nresearch attention in recent years, many efforts have been made to use deep\nlearning methods for EEG signal recognition. However, existing works mostly\ntreat deep learning models as blackbox classifiers, while what have been\nlearned by the models and to which extent they are affected by the noise from\nEEG data are still underexplored. In this paper, we develop a novel\nconvolutional neural network that can explain its decision by highlighting the\nlocal areas of the input sample that contain important information for the\nclassification. The network has a compact structure for ease of interpretation\nand takes advantage of separable convolutions to process the EEG signals in a\nspatial-temporal sequence. Results show that the model achieves an average\naccuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness\nrecognition, which is higher than the conventional baseline methods of\n53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%.\nVisualization results show that the model has learned to recognize biologically\nexplainable features from EEG signals, e.g., Alpha spindles, as strong\nindicators of drowsiness across different subjects. In addition, we also\nexplore reasons behind some wrongly classified samples and how the model is\naffected by artifacts and noise in the data. Our work illustrates a promising\ndirection on using interpretable deep learning models to discover meaning\npatterns related to different mental states from complex EEG signals.",
          "link": "http://arxiv.org/abs/2107.09507",
          "publishedOn": "2021-07-21T02:01:36.039Z",
          "wordCount": 713,
          "title": "EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN. (arXiv:2107.09507v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "Federated learning (FL) has recently emerged as a promising technology to\nenable artificial intelligence (AI) at the network edge, where distributed\nmobile devices collaboratively train a shared AI model under the coordination\nof an edge server. To significantly improve the communication efficiency of FL,\nover-the-air computation allows a large number of mobile devices to\nconcurrently upload their local models by exploiting the superposition property\nof wireless multi-access channels. Due to wireless channel fading, the model\naggregation error at the edge server is dominated by the weakest channel among\nall devices, causing severe straggler issues. In this paper, we propose a\nrelay-assisted cooperative FL scheme to effectively address the straggler\nissue. In particular, we deploy multiple half-duplex relays to cooperatively\nassist the devices in uploading the local model updates to the edge server. The\nnature of the over-the-air computation poses system objectives and constraints\nthat are distinct from those in traditional relay communication systems.\nMoreover, the strong coupling between the design variables renders the\noptimization of such a system challenging. To tackle the issue, we propose an\nalternating-optimization-based algorithm to optimize the transceiver and relay\noperation with low complexity. Then, we analyze the model aggregation error in\na single-relay case and show that our relay-assisted scheme achieves a smaller\nerror than the one without relays provided that the relay transmit power and\nthe relay channel gains are sufficiently large. The analysis provides critical\ninsights on relay deployment in the implementation of cooperative FL. Extensive\nnumerical results show that our design achieves faster convergence compared\nwith state-of-the-art schemes.",
          "link": "http://arxiv.org/abs/2107.09518",
          "publishedOn": "2021-07-21T02:01:36.023Z",
          "wordCount": 721,
          "title": "Relay-Assisted Cooperative Federated Learning. (arXiv:2107.09518v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09509",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nath_R/0/1/0/all/0/1\">Rajdeep Kumar Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thapliyal_H/0/1/0/all/0/1\">Himanshu Thapliyal</a>",
          "description": "The advent of IoT has enabled the design of connected and integrated smart\nhealth monitoring systems. These smart health monitoring systems could be\nrealized in a smart home context to render long-term care to the elderly\npopulation. In this paper, we present the design of a wearable health\nmonitoring system suitable for older adults in a smart home context. The\nproposed system offers solutions to monitor the stress, blood pressure, and\nlocation of an individual within a smart home environment. The stress detection\nmodel proposed in this work uses Electrodermal Activity (EDA),\nPhotoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart\nwristband for detecting physiological stress. The stress detection model is\ntrained and tested using stress labels obtained from salivary cortisol which is\na clinically established biomarker for physiological stress. A voice-based\nprototype is also implemented and the feasibility of the proposed system for\nintegration in a smart home environment is analyzed by simulating a data\nacquisition and streaming scenario. We have also proposed a blood pressure\nestimation model using PPG signal and advanced regression techniques for\nintegration with the stress detection model in the wearable health monitoring\nsystem. Finally, the design of a voice-assisted indoor location system is\nproposed for integration with the proposed system within a smart home\nenvironment. The proposed wearable health monitoring system is an important\ndirection to realize a smart home environment with extensive diagnostic\ncapabilities so that such a system could be useful for rendering long-term and\npersonalized care to the aging population in the comfort of their home.",
          "link": "http://arxiv.org/abs/2107.09509",
          "publishedOn": "2021-07-21T02:01:36.014Z",
          "wordCount": 719,
          "title": "Wearable Health Monitoring System for Older Adults in a Smart Home Environment. (arXiv:2107.09509v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardolph_M/0/1/0/all/0/1\">Megan D. Bardolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coulson_S/0/1/0/all/0/1\">Seana Coulson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>",
          "description": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.",
          "link": "http://arxiv.org/abs/2107.09648",
          "publishedOn": "2021-07-21T02:01:35.987Z",
          "wordCount": 554,
          "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?. (arXiv:2107.09648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merchant_A/0/1/0/all/0/1\">Amil Merchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1\">Luke Metz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Sam Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin Dogus Cubuk</a>",
          "description": "Optimization of non-convex loss surfaces containing many local minima remains\na critical problem in a variety of domains, including operations research,\ninformatics, and material design. Yet, current techniques either require\nextremely high iteration counts or a large number of random restarts for good\nperformance. In this work, we propose adapting recent developments in\nmeta-learning to these many-minima problems by learning the optimization\nalgorithm for various loss landscapes. We focus on problems from atomic\nstructural optimization--finding low energy configurations of many-atom\nsystems--including widely studied models such as bimetallic clusters and\ndisordered silicon. We find that our optimizer learns a 'hopping' behavior\nwhich enables efficient exploration and improves the rate of low energy minima\ndiscovery. Finally, our learned optimizers show promising generalization with\nefficiency gains on never before seen tasks (e.g. new elements or\ncompositions). Code will be made available shortly.",
          "link": "http://arxiv.org/abs/2107.09661",
          "publishedOn": "2021-07-21T02:01:35.981Z",
          "wordCount": 574,
          "title": "Learn2Hop: Learned Optimization on Rough Landscapes. (arXiv:2107.09661v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most of methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduced a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. Moreover, to boost the\nperformance, we argue that weak augmentations matter to represent a more\nreliable relation, and leverage momentum strategy for practical efficiency.\nExperimental results show that our proposed ReSSL significantly outperforms the\nprevious state-of-the-art algorithms in terms of both performance and training\nefficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.",
          "link": "http://arxiv.org/abs/2107.09282",
          "publishedOn": "2021-07-21T02:01:35.973Z",
          "wordCount": 615,
          "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation. (arXiv:2107.09282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Ling Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Choy Heng Lai</a>",
          "description": "The success of deep neural networks in real-world problems has prompted many\nattempts to explain their training dynamics and generalization performance, but\nmore guiding principles for the training of neural networks are still needed.\nMotivated by the edge of chaos principle behind the optimal performance of\nneural networks, we study the role of various hyperparameters in modern neural\nnetwork training algorithms in terms of the order-chaos phase diagram. In\nparticular, we study a fully analytical feedforward neural network trained on\nthe widely adopted Fashion-MNIST dataset, and study the dynamics associated\nwith the hyperparameters in back-propagation during the training process. We\nfind that for the basic algorithm of stochastic gradient descent with momentum,\nin the range around the commonly used hyperparameter values, clear scaling\nrelations are present with respect to the training time during the ordered\nphase in the phase diagram, and the model's optimal generalization power at the\nedge of chaos is similar across different training parameter combinations. In\nthe chaotic phase, the same scaling no longer exists. The scaling allows us to\nchoose the training parameters to achieve faster training without sacrificing\nperformance. In addition, we find that the commonly used model regularization\nmethod - weight decay - effectively pushes the model towards the ordered phase\nto achieve better performance. Leveraging on this fact and the scaling\nrelations in the other hyperparameters, we derived a principled guideline for\nhyperparameter determination, such that the model can achieve optimal\nperformance by saturating it at the edge of chaos. Demonstrated on this simple\nneural network model and training algorithm, our work improves the\nunderstanding of neural network training dynamics, and can potentially be\nextended to guiding principles of more complex model architectures and\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.09437",
          "publishedOn": "2021-07-21T02:01:35.966Z",
          "wordCount": 740,
          "title": "Edge of chaos as a guiding principle for modern neural network training. (arXiv:2107.09437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaesik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.",
          "link": "http://arxiv.org/abs/2107.09240",
          "publishedOn": "2021-07-21T02:01:35.947Z",
          "wordCount": 618,
          "title": "Generative Video Transformer: Can Objects be the Words?. (arXiv:2107.09240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09224",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiuyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zheng Wen</a>",
          "description": "A fundamental challenge for any intelligent system is prediction: given some\ninputs $X_1,..,X_\\tau$ can you predict outcomes $Y_1,.., Y_\\tau$. The KL\ndivergence $\\mathbf{d}_{\\mathrm{KL}}$ provides a natural measure of prediction\nquality, but the majority of deep learning research looks only at the marginal\npredictions per input $X_t$. In this technical report we propose a scoring rule\n$\\mathbf{d}_{\\mathrm{KL}}^\\tau$, parameterized by $\\tau \\in \\mathcal{N}$ that\nevaluates the joint predictions at $\\tau$ inputs simultaneously. We show that\nthe commonly-used $\\tau=1$ can be insufficient to drive good decisions in many\nsettings of interest. We also show that, as $\\tau$ grows, performing well\naccording to $\\mathbf{d}_{\\mathrm{KL}}^\\tau$ recovers universal guarantees for\nany possible decision. Finally, we provide problem-dependent guidance on the\nscale of $\\tau$ for which our score provides sufficient guarantees for good\nperformance.",
          "link": "http://arxiv.org/abs/2107.09224",
          "publishedOn": "2021-07-21T02:01:35.941Z",
          "wordCount": 566,
          "title": "Evaluating Probabilistic Inference in Deep Learning: Beyond Marginal Predictions. (arXiv:2107.09224v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09402",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Safiuddin_M/0/1/0/all/0/1\">Mohammad Safiuddin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Reddy_C/0/1/0/all/0/1\">CH Likith Reddy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Vasantada_G/0/1/0/all/0/1\">Ganesh Vasantada</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Harsha_C/0/1/0/all/0/1\">CHJNS Harsha</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gangolu_S/0/1/0/all/0/1\">Srinu Gangolu</a>",
          "description": "The microstructure of material strongly influences its mechanical properties\nand the microstructure itself is influenced by the processing conditions. Thus,\nestablishing a Process-Structure-Property relationship is a crucial task in\nmaterial design and is of interest in many engineering applications. We develop\na GAN (Generative Adversarial Network) to synthesize microstructures based on\ngiven processing conditions. This approach is devoid of feature engineering,\nneeds little domain awareness, and can be applied to a wide variety of material\nsystems. Results show that our GAN model can produce high-fidelity multi-phase\nmicrostructures which have a good correlation with the given processing\nconditions.",
          "link": "http://arxiv.org/abs/2107.09402",
          "publishedOn": "2021-07-21T02:01:35.934Z",
          "wordCount": 537,
          "title": "Establishing process-structure linkages using Generative Adversarial Networks. (arXiv:2107.09402v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09301",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mourdoukoutas_N/0/1/0/all/0/1\">Nikolaos Mourdoukoutas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Federici_M/0/1/0/all/0/1\">Marco Federici</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pantalos_G/0/1/0/all/0/1\">Georges Pantalos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1\">Mark van der Wilk</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "We propose a novel Bayesian neural network architecture that can learn\ninvariances from data alone by inferring a posterior distribution over\ndifferent weight-sharing schemes. We show that our model outperforms other\nnon-invariant architectures, when trained on datasets that contain specific\ninvariances. The same holds true when no data augmentation is performed.",
          "link": "http://arxiv.org/abs/2107.09301",
          "publishedOn": "2021-07-21T02:01:35.929Z",
          "wordCount": 501,
          "title": "A Bayesian Approach to Invariant Deep Neural Networks. (arXiv:2107.09301v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Cheng-Hung Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Neural evaluation metrics derived for numerous speech generation tasks have\nrecently attracted great attention. In this paper, we propose SVSNet, the first\nend-to-end neural network model to assess the speaker voice similarity between\nnatural speech and synthesized speech. Unlike most neural evaluation metrics\nthat use hand-crafted features, SVSNet directly takes the raw waveform as input\nto more completely utilize speech information for prediction. SVSNet consists\nof encoder, co-attention, distance calculation, and prediction modules and is\ntrained in an end-to-end manner. The experimental results on the Voice\nConversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that\nSVSNet notably outperforms well-known baseline systems in the assessment of\nspeaker similarity at the utterance and system levels.",
          "link": "http://arxiv.org/abs/2107.09392",
          "publishedOn": "2021-07-21T02:01:35.922Z",
          "wordCount": 589,
          "title": "SVSNet: An End-to-end Speaker Voice Similarity Assessment Model. (arXiv:2107.09392v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "Due to the high communication cost in distributed and federated learning,\nmethods relying on compressed communication are becoming increasingly popular.\nBesides, the best theoretically and practically performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of communications (faster convergence), e.g., Nesterov's accelerated\ngradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to\ncombine the benefits of communication compression and convergence acceleration,\nwe propose a \\emph{compressed and accelerated} gradient method for distributed\noptimization, which we call CANITA. Our CANITA achieves the \\emph{first\naccelerated rate}\n$O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} +\n\\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon\nthe state-of-the-art non-accelerated rate\n$O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} +\n\\frac{\\omega^2+n}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA (Khaled et al.,\n2020b) for distributed general convex problems, where $\\epsilon$ is the target\nerror, $L$ is the smooth parameter of the objective, $n$ is the number of\nmachines/devices, and $\\omega$ is the compression parameter (larger $\\omega$\nmeans more compression can be applied, and no compression implies $\\omega=0$).\nOur results show that as long as the number of devices $n$ is large (often true\nin distributed/federated learning), or the compression $\\omega$ is not very\nhigh, CANITA achieves the faster convergence rate\n$O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication\nrounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs.\n$O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result,\nCANITA enjoys the advantages of both compression (compressed communication in\neach round) and acceleration (much fewer communication rounds).",
          "link": "http://arxiv.org/abs/2107.09461",
          "publishedOn": "2021-07-21T02:01:35.903Z",
          "wordCount": 676,
          "title": "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression. (arXiv:2107.09461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riera_M/0/1/0/all/0/1\">Marc Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnau_J/0/1/0/all/0/1\">Jose-Maria Arnau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Antonio Gonzalez</a>",
          "description": "Deep Neural Networks (DNNs) have achieved tremendous success for cognitive\napplications. The core operation in a DNN is the dot product between quantized\ninputs and weights. Prior works exploit the weight/input repetition that arises\ndue to quantization to avoid redundant computations in Convolutional Neural\nNetworks (CNNs). However, in this paper we show that their effectiveness is\nseverely limited when applied to Fully-Connected (FC) layers, which are\ncommonly used in state-of-the-art DNNs, as it is the case of modern Recurrent\nNeural Networks (RNNs) and Transformer models.\n\nTo improve energy-efficiency of FC computation we present CREW, a hardware\naccelerator that implements Computation Reuse and an Efficient Weight Storage\nmechanism to exploit the large number of repeated weights in FC layers. CREW\nfirst performs the multiplications of the unique weights by their respective\ninputs and stores the results in an on-chip buffer. The storage requirements\nare modest due to the small number of unique weights and the relatively small\nsize of the input compared to convolutional layers. Next, CREW computes each\noutput by fetching and adding its required products. To this end, each weight\nis replaced offline by an index in the buffer of unique products. Indices are\ntypically smaller than the quantized weights, since the number of unique\nweights for each input tends to be much lower than the range of quantized\nweights, which reduces storage and memory bandwidth requirements.\n\nOverall, CREW greatly reduces the number of multiplications and provides\nsignificant savings in model memory footprint and memory bandwidth usage. We\nevaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x\nspeedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,\na state-of-art computation reuse technique, CREW achieves 2.10x speedup and\n2.08x energy savings on average.",
          "link": "http://arxiv.org/abs/2107.09408",
          "publishedOn": "2021-07-21T02:01:35.897Z",
          "wordCount": 730,
          "title": "CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs. (arXiv:2107.09408v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ostwald_D/0/1/0/all/0/1\">Dirk Ostwald</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Usee_F/0/1/0/all/0/1\">Franziska Us&#xe9;e</a>",
          "description": "Backpropagation (BP) is a core component of the contemporary deep learning\nincarnation of neural networks. Briefly, BP is an algorithm that exploits the\ncomputational architecture of neural networks to efficiently evaluate the\ngradient of a cost function during neural network parameter optimization. The\nvalidity of BP rests on the application of a multivariate chain rule to the\ncomputational architecture of neural networks and their associated objective\nfunctions. Introductions to deep learning theory commonly present the\ncomputational architecture of neural networks in matrix form, but eschew a\nparallel formulation and justification of BP in the framework of matrix\ndifferential calculus. This entails several drawbacks for the theory and\ndidactics of deep learning. In this work, we overcome these limitations by\nproviding a full induction proof of the BP algorithm in matrix notation.\nSpecifically, we situate the BP algorithm in the framework of matrix\ndifferential calculus, encompass affine-linear potential functions, prove the\nvalidity of the BP algorithm in inductive form, and exemplify the\nimplementation of the matrix form BP algorithm in computer code.",
          "link": "http://arxiv.org/abs/2107.09384",
          "publishedOn": "2021-07-21T02:01:35.890Z",
          "wordCount": 616,
          "title": "An induction proof of the backpropagation algorithm in matrix notation. (arXiv:2107.09384v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaomin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>",
          "description": "Computational intelligence-based ocean characteristics forecasting\napplications, such as Significant Wave Height (SWH) prediction, are crucial for\navoiding social and economic loss in coastal cities. Compared to the\ntraditional empirical-based or numerical-based forecasting models, \"soft\ncomputing\" approaches, including machine learning and deep learning models,\nhave shown numerous success in recent years. In this paper, we focus on\nenabling the deep learning model to learn both short-term and long-term\nspatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural\nNetwork (WGNN) approach is proposed to integrate the advantages of wavelet\ntransform and graph neural network. Several parallel graph neural networks are\nseparately trained on wavelet decomposed data, and the reconstruction of each\nmodel's prediction forms the final SWH prediction. Experimental results show\nthat the proposed WGNN approach outperforms other models, including the\nnumerical models, the machine learning models, and several deep learning\nmodels.",
          "link": "http://arxiv.org/abs/2107.09483",
          "publishedOn": "2021-07-21T02:01:35.883Z",
          "wordCount": 596,
          "title": "Significant Wave Height Prediction based on Wavelet Graph Neural Network. (arXiv:2107.09483v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tetali_H/0/1/0/all/0/1\">Harsha Vardhan Tetali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_J/0/1/0/all/0/1\">Joel B. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>",
          "description": "With the recent success of representation learning methods, which includes\ndeep learning as a special case, there has been considerable interest in\ndeveloping representation learning techniques that can incorporate known\nphysical constraints into the learned representation. As one example, in many\napplications that involve a signal propagating through physical media (e.g.,\noptics, acoustics, fluid dynamics, etc), it is known that the dynamics of the\nsignal must satisfy constraints imposed by the wave equation. Here we propose a\nmatrix factorization technique that decomposes such signals into a sum of\ncomponents, where each component is regularized to ensure that it satisfies\nwave equation constraints. Although our proposed formulation is non-convex, we\nprove that our model can be efficiently solved to global optimality in\npolynomial time. We demonstrate the benefits of our work by applications in\nstructural health monitoring, where prior work has attempted to solve this\nproblem using sparse dictionary learning approaches that do not come with any\ntheoretical guarantees regarding convergence to global optimality and employ\nheuristics to capture desired physical constraints.",
          "link": "http://arxiv.org/abs/2107.09144",
          "publishedOn": "2021-07-21T02:01:35.877Z",
          "wordCount": 599,
          "title": "Wave-Informed Matrix Factorization withGlobal Optimality Guarantees. (arXiv:2107.09144v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuxuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Deep neural networks often have a huge number of parameters, which posts\nchallenges in deployment in application scenarios with limited memory and\ncomputation capacity. Knowledge distillation is one approach to derive compact\nmodels from bigger ones. However, it has been observed that a converged heavy\nteacher model is strongly constrained for learning a compact student network\nand could make the optimization subject to poor local optima. In this paper, we\npropose ProKT, a new model-agnostic method by projecting the supervision\nsignals of a teacher model into the student's parameter space. Such projection\nis implemented by decomposing the training objective into local intermediate\ntargets with an approximate mirror descent technique. The proposed method could\nbe less sensitive with the quirks during optimization which could result in a\nbetter local optimum. Experiments on both image and text datasets show that our\nproposed ProKT consistently achieves superior performance compared to other\nexisting knowledge distillation methods.",
          "link": "http://arxiv.org/abs/2107.09305",
          "publishedOn": "2021-07-21T02:01:35.858Z",
          "wordCount": 597,
          "title": "Follow Your Path: a Progressive Method for Knowledge Distillation. (arXiv:2107.09305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09200",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zlokapa_A/0/1/0/all/0/1\">Alexander Zlokapa</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Neven_H/0/1/0/all/0/1\">Hartmut Neven</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1\">Seth Lloyd</a>",
          "description": "Given the success of deep learning in classical machine learning, quantum\nalgorithms for traditional neural network architectures may provide one of the\nmost promising settings for quantum machine learning. Considering a\nfully-connected feedforward neural network, we show that conditions amenable to\nclassical trainability via gradient descent coincide with those necessary for\nefficiently solving quantum linear systems. We propose a quantum algorithm to\napproximately train a wide and deep neural network up to $O(1/n)$ error for a\ntraining set of size $n$ by performing sparse matrix inversion in $O(\\log n)$\ntime. To achieve an end-to-end exponential speedup over gradient descent, the\ndata distribution must permit efficient state preparation and readout. We\nnumerically demonstrate that the MNIST image dataset satisfies such conditions;\nmoreover, the quantum algorithm matches the accuracy of the fully-connected\nnetwork. Beyond the proven architecture, we provide empirical evidence for\n$O(\\log n)$ training of a convolutional neural network with pooling.",
          "link": "http://arxiv.org/abs/2107.09200",
          "publishedOn": "2021-07-21T02:01:35.851Z",
          "wordCount": 602,
          "title": "A quantum algorithm for training wide and deep classical neural networks. (arXiv:2107.09200v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09194",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1\">William T. Stephenson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Frangella_Z/0/1/0/all/0/1\">Zachary Frangella</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1\">Tamara Broderick</a>",
          "description": "Models like LASSO and ridge regression are extensively used in practice due\nto their interpretability, ease of use, and strong theoretical guarantees.\nCross-validation (CV) is widely used for hyperparameter tuning in these models,\nbut do practical optimization methods minimize the true out-of-sample loss? A\nrecent line of research promises to show that the optimum of the CV loss\nmatches the optimum of the out-of-sample loss (possibly after simple\ncorrections). It remains to show how tractable it is to minimize the CV loss.\nIn the present paper, we show that, in the case of ridge regression, the CV\nloss may fail to be quasiconvex and thus may have multiple local optima. We can\nguarantee that the CV loss is quasiconvex in at least one case: when the\nspectrum of the covariate matrix is nearly flat and the noise in the observed\nresponses is not too high. More generally, we show that quasiconvexity status\nis independent of many properties of the observed data (response norm,\ncovariate-matrix right singular vectors and singular-value scaling) and has a\ncomplex dependence on the few that remain. We empirically confirm our theory\nusing simulated experiments.",
          "link": "http://arxiv.org/abs/2107.09194",
          "publishedOn": "2021-07-21T02:01:35.844Z",
          "wordCount": 635,
          "title": "Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression. (arXiv:2107.09194v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siblini_W/0/1/0/all/0/1\">Wissam Siblini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coter_G/0/1/0/all/0/1\">Guillaume Coter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabry_R/0/1/0/all/0/1\">R&#xe9;my Fabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Guelton_L/0/1/0/all/0/1\">Liyun He-Guelton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oble_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Obl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebichot_B/0/1/0/all/0/1\">Bertrand Lebichot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgne_Y/0/1/0/all/0/1\">Yann-A&#xeb;l Le Borgne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1\">Gianluca Bontempi</a>",
          "description": "The dark face of digital commerce generalization is the increase of fraud\nattempts. To prevent any type of attacks, state of the art fraud detection\nsystems are now embedding Machine Learning (ML) modules. The conception of such\nmodules is only communicated at the level of research and papers mostly focus\non results for isolated benchmark datasets and metrics. But research is only a\npart of the journey, preceded by the right formulation of the business problem\nand collection of data, and followed by a practical integration. In this paper,\nwe give a wider vision of the process, on a case study of transfer learning for\nfraud detection, from business to research, and back to business.",
          "link": "http://arxiv.org/abs/2107.09323",
          "publishedOn": "2021-07-21T02:01:35.837Z",
          "wordCount": 568,
          "title": "Transfer Learning for Credit Card Fraud Detection: A Journey from Research to Production. (arXiv:2107.09323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09088",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Strupl_M/0/1/0/all/0/1\">Miroslav &#x160;trupl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Faccio_F/0/1/0/all/0/1\">Francesco Faccio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srivastava_R/0/1/0/all/0/1\">Rupesh Kumar Srivastava</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "Reward-Weighted Regression (RWR) belongs to a family of widely known\niterative Reinforcement Learning algorithms based on the\nExpectation-Maximization framework. In this family, learning at each iteration\nconsists of sampling a batch of trajectories using the current policy and\nfitting a new policy to maximize a return-weighted log-likelihood of actions.\nAlthough RWR is known to yield monotonic improvement of the policy under\ncertain circumstances, whether and under which conditions RWR converges to the\noptimal policy have remained open questions. In this paper, we provide for the\nfirst time a proof that RWR converges to a global optimum when no function\napproximation is used.",
          "link": "http://arxiv.org/abs/2107.09088",
          "publishedOn": "2021-07-21T02:01:35.829Z",
          "wordCount": 575,
          "title": "Reward-Weighted Regression Converges to a Global Optimum. (arXiv:2107.09088v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1\">Mila Soares de Oliveira de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_P/0/1/0/all/0/1\">Pedro Nuno de Souza Moura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1\">Jean-Pierre Briot</a>",
          "description": "This paper presents a comparative analysis on two artificial neural networks\n(with different architectures) for the task of tempo estimation. For this\npurpose, it also proposes the modeling, training and evaluation of a B-RNN\n(Bidirectional Recurrent Neural Network) model capable of estimating tempo in\nbpm (beats per minutes) of musical pieces, without using external auxiliary\nmodules. An extensive database (12,550 pieces in total) was curated to conduct\na quantitative and qualitative analysis over the experiment. Percussion-only\ntracks were also included in the dataset. The performance of the B-RNN is\ncompared to that of state-of-the-art models. For further comparison, a\nstate-of-the-art CNN was also retrained with the same datasets used for the\nB-RNN training. Evaluation results for each model and datasets are presented\nand discussed, as well as observations and ideas for future research. Tempo\nestimation was more accurate for the percussion only dataset, suggesting that\nthe estimation can be more accurate for percussion-only tracks, although\nfurther experiments (with more of such datasets) should be made to gather\nstronger evidence.",
          "link": "http://arxiv.org/abs/2107.09208",
          "publishedOn": "2021-07-21T02:01:35.810Z",
          "wordCount": 621,
          "title": "Music Tempo Estimation via Neural Networks -- A Comparative Analysis. (arXiv:2107.09208v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1\">Arya Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Soumyabrata Pal</a>",
          "description": "One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition\nmethod that has been widely studied in the past decade. In 1bCS, linear samples\nof a high dimensional signal are quantized to only one bit per sample (sign of\nthe measurement). Assuming the original signal vector to be sparse, existing\nresults either aim to find the support of the vector, or approximate the signal\nwithin an $\\epsilon$-ball. The focus of this paper is support recovery, which\noften also computationally facilitates approximate signal recovery. A universal\nmeasurement matrix for 1bCS refers to one set of measurements that work for all\nsparse signals. With universality, it is known that $\\tilde{\\Theta}(k^2)$ 1bCS\nmeasurements are necessary and sufficient for support recovery (where $k$\ndenotes the sparsity). In this work, we show that it is possible to universally\nrecover the support with a small number of false positives with\n$\\tilde{O}(k^{3/2})$ measurements. If the dynamic range of the signal vector is\nknown, then with a different technique, this result can be improved to only\n$\\tilde{O}(k)$ measurements. Further results on support recovery are also\nprovided.",
          "link": "http://arxiv.org/abs/2107.09091",
          "publishedOn": "2021-07-21T02:01:35.804Z",
          "wordCount": 620,
          "title": "Support Recovery in Universal One-bit Compressed Sensing. (arXiv:2107.09091v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>",
          "description": "Communication between agents in collaborative multi-agent settings is in\ngeneral implicit or a direct data stream. This paper considers text-based\nnatural language as a novel form of communication between multiple agents\ntrained with reinforcement learning. This could be considered first steps\ntoward a truly autonomous communication without the need to define a limited\nset of instructions, and natural collaboration between humans and robots.\nInspired by the game of Blind Leads, we propose an environment where one agent\nuses natural language instructions to guide another through a maze. We test the\nability of reinforcement learning agents to effectively communicate through\ndiscrete word-level symbols and show that the agents are able to sufficiently\ncommunicate through natural language with a limited vocabulary. Although the\ncommunication is not always perfect English, the agents are still able to\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\n0.61 over randomly generated sequences while maintaining a 100% maze completion\nrate. This is a 3.5 times the performance of the random baseline using our\nreference set.",
          "link": "http://arxiv.org/abs/2107.09356",
          "publishedOn": "2021-07-21T02:01:35.797Z",
          "wordCount": 665,
          "title": "Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language. (arXiv:2107.09356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spantidi_O/0/1/0/all/0/1\">Ourania Spantidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zervakis_G/0/1/0/all/0/1\">Georgios Zervakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_I/0/1/0/all/0/1\">Iraklis Anagnostopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amrouch_H/0/1/0/all/0/1\">Hussam Amrouch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1\">J&#xf6;rg Henkel</a>",
          "description": "Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy\nlevels on many AI tasks. Several applications rely more and more on DNNs to\ndeliver sophisticated services and DNN accelerators are becoming integral\ncomponents of modern systems-on-chips. DNNs perform millions of arithmetic\noperations per inference and DNN accelerators integrate thousands of\nmultiply-accumulate units leading to increased energy requirements. Approximate\ncomputing principles are employed to significantly lower the energy consumption\nof DNN accelerators at the cost of some accuracy loss. Nevertheless, recent\nresearch demonstrated that complex DNNs are increasingly sensitive to\napproximation. Hence, the obtained energy savings are often limited when\ntargeting tight accuracy constraints. In this work, we present a dynamically\nconfigurable approximate multiplier that supports three operation modes, i.e.,\nexact, positive error, and negative error. In addition, we propose a\nfilter-oriented approximation method to map the weights to the appropriate\nmodes of the approximate multiplier. Our mapping algorithm balances the\npositive with the negative errors due to the approximate multiplications,\naiming at maximizing the energy reduction while minimizing the overall\nconvolution error. We evaluate our approach on multiple DNNs and datasets\nagainst state-of-the-art approaches, where our method achieves 18.33% energy\ngains on average across 7 NNs on 4 different datasets for a maximum accuracy\ndrop of only 1%.",
          "link": "http://arxiv.org/abs/2107.09366",
          "publishedOn": "2021-07-21T02:01:35.789Z",
          "wordCount": 656,
          "title": "Positive/Negative Approximate Multipliers for DNN Accelerators. (arXiv:2107.09366v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Sanchita Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevost_J/0/1/0/all/0/1\">John J. Prevost</a>",
          "description": "Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.",
          "link": "http://arxiv.org/abs/2107.09262",
          "publishedOn": "2021-07-21T02:01:35.782Z",
          "wordCount": 610,
          "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. (arXiv:2107.09262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Addanki_R/0/1/0/all/0/1\">Ravichandra Addanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter W. Battaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deac_A/0/1/0/all/0/1\">Andreea Deac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1\">Jonathan Godwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keck_T/0/1/0/all/0/1\">Thomas Keck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wai Lok Sibon Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_J/0/1/0/all/0/1\">Jacklynn Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakoor_S/0/1/0/all/0/1\">Shantanu Thakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>",
          "description": "Effectively and efficiently deploying graph neural networks (GNNs) at scale\nremains one of the most challenging aspects of graph representation learning.\nMany powerful solutions have only ever been validated on comparatively small\ndatasets, often with counter-intuitive outcomes -- a barrier which has been\nbroken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered\nthe OGB-LSC with two large-scale GNNs: a deep transductive node classifier\npowered by bootstrapping, and a very deep (up to 50-layer) inductive graph\nregressor regularised by denoising objectives. Our models achieved an\naward-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In\ndoing so, we demonstrate evidence of scalable self-supervised graph\nrepresentation learning, and utility of very deep GNNs -- both very important\nopen issues. Our code is publicly available at:\nhttps://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.",
          "link": "http://arxiv.org/abs/2107.09422",
          "publishedOn": "2021-07-21T02:01:35.764Z",
          "wordCount": 610,
          "title": "Large-scale graph representation learning with very deep GNNs and self-supervision. (arXiv:2107.09422v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1\">Brenden K. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1\">Claudio P. Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1\">Mikel Landajuela Larma</a>",
          "description": "Many AutoML problems involve optimizing discrete objects under a black-box\nreward. Neural-guided search provides a flexible means of searching these\ncombinatorial spaces using an autoregressive recurrent neural network. A major\nbenefit of this approach is that builds up objects sequentially--this provides\nan opportunity to incorporate domain knowledge into the search by directly\nmodifying the logits emitted during sampling. In this work, we formalize a\nframework for incorporating such in situ priors and constraints into\nneural-guided search, and provide sufficient conditions for enforcing\nconstraints. We integrate several priors and constraints from existing works\ninto this framework, propose several new ones, and demonstrate their efficacy\nin informing the task of symbolic regression.",
          "link": "http://arxiv.org/abs/2107.09182",
          "publishedOn": "2021-07-21T02:01:35.757Z",
          "wordCount": 537,
          "title": "Incorporating domain knowledge into neural-guided search. (arXiv:2107.09182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09428",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuya Fujita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Non-autoregressive (NAR) modeling has gained more and more attention in\nspeech processing. With recent state-of-the-art attention-based automatic\nspeech recognition (ASR) structure, NAR can realize promising real-time factor\n(RTF) improvement with only small degradation of accuracy compared to the\nautoregressive (AR) models. However, the recognition inference needs to wait\nfor the completion of a full speech utterance, which limits their applications\non low latency scenarios. To address this issue, we propose a novel end-to-end\nstreaming NAR speech recognition system by combining blockwise-attention and\nconnectionist temporal classification with mask-predict (Mask-CTC) NAR. During\ninference, the input audio is separated into small blocks and then processed in\na blockwise streaming way. To address the insertion and deletion error at the\nedge of the output of each block, we apply an overlapping decoding strategy\nwith a dynamic mapping trick that can produce more coherent sentences.\nExperimental results show that the proposed method improves online ASR\nrecognition in low latency conditions compared to vanilla Mask-CTC. Moreover,\nit can achieve a much faster inference speed compared to the AR attention-based\nmodels. All of our codes will be publicly available at\nhttps://github.com/espnet/espnet.",
          "link": "http://arxiv.org/abs/2107.09428",
          "publishedOn": "2021-07-21T02:01:35.750Z",
          "wordCount": 638,
          "title": "Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models. (arXiv:2107.09428v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Daniel Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>",
          "description": "We study how an offline dataset of prior (possibly random) experience can be\nused to address two challenges that autonomous systems face when they endeavor\nto learn from, adapt to, and collaborate with humans : (1) identifying the\nhuman's intent and (2) safely optimizing the autonomous system's behavior to\nachieve this inferred intent. First, we use the offline dataset to efficiently\ninfer the human's reward function via pool-based active preference learning.\nSecond, given this learned reward function, we perform offline reinforcement\nlearning to optimize a policy based on the inferred human intent. Crucially,\nour proposed approach does not require actual physical rollouts or an accurate\nsimulator for either the reward learning or policy optimization steps, enabling\nboth safe and efficient apprenticeship learning. We identify and evaluate our\napproach on a subset of existing offline RL benchmarks that are well suited for\noffline reward learning and also evaluate extensions of these benchmarks which\nallow more open-ended behaviors. Our experiments show that offline\npreference-based reward learning followed by offline reinforcement learning\nenables efficient and high-performing policies, while only requiring small\nnumbers of preference queries. Videos available at\nhttps://sites.google.com/view/offline-prefs.",
          "link": "http://arxiv.org/abs/2107.09251",
          "publishedOn": "2021-07-21T02:01:35.743Z",
          "wordCount": 619,
          "title": "OPAL: Offline Preference-Based Apprenticeship Learning. (arXiv:2107.09251v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1\">Mohanad Odema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berken Utku Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>",
          "description": "Edge-Cloud hierarchical systems employing intelligence through Deep Neural\nNetworks (DNNs) endure the dilemma of workload distribution within them.\nPrevious solutions proposed to distribute workloads at runtime according to the\nstate of the surroundings, like the wireless conditions. However, such\nconditions are usually overlooked at design time. This paper addresses this\nissue for DNN architectural design by presenting a novel methodology, LENS,\nwhich administers multi-objective Neural Architecture Search (NAS) for\ntwo-tiered systems, where the performance objectives are refashioned to\nconsider the wireless communication parameters. From our experimental search\nspace, we demonstrate that LENS improves upon the traditional solution's Pareto\nset by 76.47% and 75% with respect to the energy and latency metrics,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.09309",
          "publishedOn": "2021-07-21T02:01:35.736Z",
          "wordCount": 579,
          "title": "LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies. (arXiv:2107.09309v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1\">Pierre Stock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a>",
          "description": "Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are\ndescribed by a vector of parameters $\\theta$, and realized as a piecewise\nlinear continuous function $R_{\\theta}: x \\in \\mathbb R^{d} \\mapsto\nR_{\\theta}(x) \\in \\mathbb R^{k}$. Natural scalings and permutations operations\non the parameters $\\theta$ leave the realization unchanged, leading to\nequivalence classes of parameters that yield the same realization. These\nconsiderations in turn lead to the notion of identifiability -- the ability to\nrecover (the equivalence class of) $\\theta$ from the sole knowledge of its\nrealization $R_{\\theta}$. The overall objective of this paper is to introduce\nan embedding for ReLU neural networks of any depth, $\\Phi(\\theta)$, that is\ninvariant to scalings and that provides a locally linear parameterization of\nthe realization of the network. Leveraging these two key properties, we derive\nsome conditions under which a deep ReLU network is indeed locally identifiable\nfrom the knowledge of the realization on a finite set of samples $x_{i} \\in\n\\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary\nand sufficient conditions for the network to be identifiable from a bounded\nsubset $\\mathcal X \\subseteq \\mathbb R^{d}$.",
          "link": "http://arxiv.org/abs/2107.09370",
          "publishedOn": "2021-07-21T02:01:35.729Z",
          "wordCount": 618,
          "title": "An Embedding of ReLU Networks and an Analysis of their Identifiability. (arXiv:2107.09370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehead_S/0/1/0/all/0/1\">Spencer Whitehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.",
          "link": "http://arxiv.org/abs/2107.09106",
          "publishedOn": "2021-07-21T02:01:35.711Z",
          "wordCount": 624,
          "title": "Separating Skills and Concepts for Novel Visual Question Answering. (arXiv:2107.09106v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>",
          "description": "Saliency methods -- techniques to identify the importance of input features\non a model's output -- are a common first step in understanding neural network\nbehavior. However, interpreting saliency requires tedious manual inspection to\nidentify and aggregate patterns in model behavior, resulting in ad hoc or\ncherry-picked analysis. To address these concerns, we present Shared Interest:\na set of metrics for comparing saliency with human annotated ground truths. By\nproviding quantitative descriptors, Shared Interest allows ranking, sorting,\nand aggregation of inputs thereby facilitating large-scale systematic analysis\nof model behavior. We use Shared Interest to identify eight recurring patterns\nin model behavior including focusing on a sufficient subset of ground truth\nfeatures or being distracted by contextual features. Working with\nrepresentative real-world users, we show how Shared Interest can be used to\nrapidly develop or lose trust in a model's reliability, uncover issues that are\nmissed in manual analyses, and enable interactive probing of model behavior.",
          "link": "http://arxiv.org/abs/2107.09234",
          "publishedOn": "2021-07-21T02:01:35.704Z",
          "wordCount": 605,
          "title": "Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment. (arXiv:2107.09234v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Promberger_A/0/1/0/all/0/1\">Andreas Promberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1\">Falk Nickel</a>",
          "description": "Friction systems are mechanical systems wherein friction is used for force\ntransmission (e.g. mechanical braking systems or automatic gearboxes). For\nfinding optimal and safe design parameters, engineers have to predict friction\nsystem performance. This is especially difficult in real-world applications,\nbecause it is affected by many parameters. We have used symbolic regression and\ngenetic programming for finding accurate and trustworthy prediction models for\nthis task. However, it is not straight-forward how nominal variables can be\nincluded. In particular, a one-hot-encoding is unsatisfactory because genetic\nprogramming tends to remove such indicator variables. We have therefore used\nso-called factor variables for representing nominal variables in symbolic\nregression models. Our results show that GP is able to produce symbolic\nregression models for predicting friction performance with predictive accuracy\nthat is comparable to artificial neural networks. The symbolic regression\nmodels with factor variables are less complex than models using a one-hot\nencoding.",
          "link": "http://arxiv.org/abs/2107.09484",
          "publishedOn": "2021-07-21T02:01:35.697Z",
          "wordCount": 628,
          "title": "Predicting Friction System Performance with Symbolic Regression and Genetic Programming with Factor Variables. (arXiv:2107.09484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1\">Jo&#xe3;o Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tateo_D/0/1/0/all/0/1\">Davide Tateo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muratore_F/0/1/0/all/0/1\">Fabio Muratore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>",
          "description": "Reinforcement learning methods for robotics are increasingly successful due\nto the constant development of better policy gradient techniques. A precise\n(low variance) and accurate (low bias) gradient estimator is crucial to face\nincreasingly complex tasks. Traditional policy gradient algorithms use the\nlikelihood-ratio trick, which is known to produce unbiased but high variance\nestimates. More modern approaches exploit the reparametrization trick, which\ngives lower variance gradient estimates but requires differentiable value\nfunction approximators. In this work, we study a different type of stochastic\ngradient estimator: the Measure-Valued Derivative. This estimator is unbiased,\nhas low variance, and can be used with differentiable and non-differentiable\nfunction approximators. We empirically evaluate this estimator in the\nactor-critic policy gradient setting and show that it can reach comparable\nperformance with methods based on the likelihood-ratio or reparametrization\ntricks, both in low and high-dimensional action spaces.",
          "link": "http://arxiv.org/abs/2107.09359",
          "publishedOn": "2021-07-21T02:01:35.690Z",
          "wordCount": 575,
          "title": "An Empirical Analysis of Measure-Valued Derivatives for Policy Gradients. (arXiv:2107.09359v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion and Gaussian perturbations improves, while even improving the\nperformance on clean images slightly without performing any data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-07-21T02:01:35.682Z",
          "wordCount": 556,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lirong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Recent studies show that advanced priors play a major role in deep generative\nmodels. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has\nachieved impressive results. However, due to the nature of model design, an\nexemplar-based model usually requires vast amounts of data to participate in\ntraining, which leads to huge computational complexity. To address this issue,\nwe propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of\nVAE with a prior based on Bayesian pseudocoreset. The proposed prior is\nconditioned on a small-scale pseudocoreset rather than the whole dataset for\nreducing the computational cost and avoiding overfitting. Simultaneously, we\nobtain the optimal pseudocoreset via a stochastic optimization algorithm during\nVAE training aiming to minimize the Kullback-Leibler divergence between the\nprior based on the pseudocoreset and that based on the whole dataset.\nExperimental results show that ByPE-VAE can achieve competitive improvements\nover the state-of-the-art VAEs in the tasks of density estimation,\nrepresentation learning, and generative data augmentation. Particularly, on a\nbasic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE\nwhile almost holding the performance. Code is available at our supplementary\nmaterials.",
          "link": "http://arxiv.org/abs/2107.09286",
          "publishedOn": "2021-07-21T02:01:35.665Z",
          "wordCount": 612,
          "title": "ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE. (arXiv:2107.09286v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amato_D/0/1/0/all/0/1\">Domenico Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancarlo_R/0/1/0/all/0/1\">Raffaele Giancarlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosco_G/0/1/0/all/0/1\">Giosu&#xe8; Lo Bosco</a>",
          "description": "Sorted Table Search Procedures are the quintessential query-answering tool,\nstill very useful, e.g, Search Engines (Google Chrome). Speeding them up, in\nsmall additional space with respect to the table being searched into, is still\na quite significant achievement. Static Learned Indexes have been very\nsuccessful in achieving such a speed-up, but leave open a major question: To\nwhat extent one can enjoy the speed-up of Learned Indexes while using constant\nor nearly constant additional space. By generalizing the experimental\nmethodology of a recent benchmarking study on Learned Indexes, we shed light on\nthis question, by considering two scenarios. The first, quite elementary, i.e.,\ntextbook code, and the second using advanced Learned Indexing algorithms and\nthe supporting sophisticated software platforms. Although in both cases one\nwould expect a positive answer, its achievement is not as simple as it seems.\nIndeed, our extensive set of experiments reveal a complex relationship between\nquery time and model space. The findings regarding this relationship and the\ncorresponding quantitative estimates, across memory levels, can be of interest\nto algorithm designers and of use to practitioners as well. As an essential\npart of our research, we introduce two new models that are of interest in their\nown right. The first is a constant space model that can be seen as a\ngeneralization of $k$-ary search, while the second is a synoptic {\\bf RMI}, in\nwhich we can control model space usage.",
          "link": "http://arxiv.org/abs/2107.09480",
          "publishedOn": "2021-07-21T02:01:35.658Z",
          "wordCount": 696,
          "title": "Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study. (arXiv:2107.09480v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "Graph neural networks (GNNs) are naturally distributed architectures for\nlearning representations from network data. This renders them suitable\ncandidates for decentralized tasks. In these scenarios, the underlying graph\noften changes with time due to link failures or topology variations, creating a\nmismatch between the graphs on which GNNs were trained and the ones on which\nthey are tested. Online learning can be leveraged to retrain GNNs at testing\ntime to overcome this issue. However, most online algorithms are centralized\nand usually offer guarantees only on convex problems, which GNNs rarely lead\nto. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture\nthat can be updated with distributed online learning mechanisms. The WD-GNN\nconsists of two components: the wide part is a linear graph filter and the deep\npart is a nonlinear GNN. At training time, the joint wide and deep architecture\nlearns nonlinear representations from data. At testing time, the wide, linear\npart is retrained, while the deep, nonlinear one remains fixed. This often\nleads to a convex formulation. We further propose a distributed online learning\nalgorithm that can be implemented in a decentralized setting. We also show the\nstability of the WD-GNN to changes of the underlying graph and analyze the\nconvergence of the proposed online learning procedure. Experiments on movie\nrecommendation, source localization and robot swarm control corroborate\ntheoretical findings and show the potential of the WD-GNN for distributed\nonline learning.",
          "link": "http://arxiv.org/abs/2107.09203",
          "publishedOn": "2021-07-21T02:01:35.651Z",
          "wordCount": 681,
          "title": "Wide and Deep Graph Neural Network with Distributed Online Learning. (arXiv:2107.09203v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uy_W/0/1/0/all/0/1\">Wayne Isaac Tan Uy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuepeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxiao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>",
          "description": "Noise poses a challenge for learning dynamical-system models because already\nsmall variations can distort the dynamics described by trajectory data. This\nwork builds on operator inference from scientific machine learning to infer\nlow-dimensional models from high-dimensional state trajectories polluted with\nnoise. The presented analysis shows that, under certain conditions, the\ninferred operators are unbiased estimators of the well-studied projection-based\nreduced operators from traditional model reduction. Furthermore, the connection\nbetween operator inference and projection-based model reduction enables\nbounding the mean-squared errors of predictions made with the learned models\nwith respect to traditional reduced models. The analysis also motivates an\nactive operator inference approach that judiciously samples high-dimensional\ntrajectories with the aim of achieving a low mean-squared error by reducing the\neffect of noise. Numerical experiments with high-dimensional linear and\nnonlinear state dynamics demonstrate that predictions obtained with active\noperator inference have orders of magnitude lower mean-squared errors than\noperator inference with traditional, equidistantly sampled trajectory data.",
          "link": "http://arxiv.org/abs/2107.09256",
          "publishedOn": "2021-07-21T02:01:35.603Z",
          "wordCount": 601,
          "title": "Active operator inference for learning low-dimensional dynamical-system models from noisy data. (arXiv:2107.09256v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kunin_D/0/1/0/all/0/1\">Daniel Kunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagastuy_Brena_J/0/1/0/all/0/1\">Javier Sagastuy-Brena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1\">Lauren Gillespie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margalit_E/0/1/0/all/0/1\">Eshed Margalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L. K. Yamins</a>",
          "description": "In this work we explore the limiting dynamics of deep neural networks trained\nwith stochastic gradient descent (SGD). We find empirically that long after\nperformance has converged, networks continue to move through parameter space by\na process of anomalous diffusion in which distance travelled grows as a power\nlaw in the number of gradient updates with a nontrivial exponent. We reveal an\nintricate interaction between the hyperparameters of optimization, the\nstructure in the gradient noise, and the Hessian matrix at the end of training\nthat explains this anomalous diffusion. To build this understanding, we first\nderive a continuous-time model for SGD with finite learning rates and batch\nsizes as an underdamped Langevin equation. We study this equation in the\nsetting of linear regression, where we can derive exact, analytic expressions\nfor the phase space dynamics of the parameters and their instantaneous\nvelocities from initialization to stationarity. Using the Fokker-Planck\nequation, we show that the key ingredient driving these dynamics is not the\noriginal training loss, but rather the combination of a modified loss, which\nimplicitly regularizes the velocity, and probability currents, which cause\noscillations in phase space. We identify qualitative and quantitative\npredictions of this theory in the dynamics of a ResNet-18 model trained on\nImageNet. Through the lens of statistical physics, we uncover a mechanistic\norigin for the anomalous limiting dynamics of deep neural networks trained with\nSGD.",
          "link": "http://arxiv.org/abs/2107.09133",
          "publishedOn": "2021-07-21T02:01:35.596Z",
          "wordCount": 700,
          "title": "Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion. (arXiv:2107.09133v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utimula_K/0/1/0/all/0/1\">Keishu Utimula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayaschi_K/0/1/0/all/0/1\">Ken-taro Hayaschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakano_K/0/1/0/all/0/1\">Kousuke Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongo_K/0/1/0/all/0/1\">Kenta Hongo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maezono_R/0/1/0/all/0/1\">Ryo Maezono</a>",
          "description": "When agents are swarmed to carry out a mission, there is often a sudden\nfailure of some of the agents observed from the command base. It is generally\ndifficult to distinguish whether the failure is caused by actuators\n(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication\nbetween the command base and the concerning agent. By making a collision to the\nagent by another, we would be able to distinguish which hypothesis is likely:\nFor $h_a$, we expect to detect corresponding displacements while for $h_a$ we\ndo not. Such swarm strategies to grasp the situation are preferably to be\ngenerated autonomously by artificial intelligence (AI). Preferable actions\n($e.g.$, the collision) for the distinction would be those maximizing the\ndifference between the expected behaviors for each hypothesis, as a value\nfunction. Such actions exist, however, only very sparsely in the whole\npossibilities, for which the conventional search based on gradient methods does\nnot make sense. Instead, we have successfully applied the reinforcement\nlearning technique, achieving the maximization of such a sparse value function.\nThe machine learning actually concluded autonomously the colliding action to\ndistinguish the hypothesises. Getting recognized an agent with actuator error\nby the action, the agents behave as if other ones want to assist the\nmalfunctioning one to achieve a given mission.",
          "link": "http://arxiv.org/abs/2107.09232",
          "publishedOn": "2021-07-21T02:01:35.577Z",
          "wordCount": 674,
          "title": "Reinforcement learning autonomously identifying the source of errors for agents in a group mission. (arXiv:2107.09232v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varga_B/0/1/0/all/0/1\">Bal&#xe1;zs Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulcsar_B/0/1/0/all/0/1\">Bal&#xe1;zs Kulcs&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1\">Morteza Haghir Chehreghani</a>",
          "description": "This paper presents a constrained policy gradient algorithm. We introduce\nconstraints for safe learning with the following steps. First, learning is\nslowed down (lazy learning) so that the episodic policy change can be computed\nwith the help of the policy gradient theorem and the neural tangent kernel.\nThen, this enables us the evaluation of the policy at arbitrary states too. In\nthe same spirit, learning can be guided, ensuring safety via augmenting episode\nbatches with states where the desired action probabilities are prescribed.\nFinally, exogenous discounted sum of future rewards (returns) can be computed\nat these specific state-action pairs such that the policy network satisfies\nconstraints. Computing the returns is based on solving a system of linear\nequations (equality constraints) or a constrained quadratic program (inequality\nconstraints). Simulation results suggest that adding constraints (external\ninformation) to the learning can improve learning in terms of speed and safety\nreasonably if constraints are appropriately selected. The efficiency of the\nconstrained learning was demonstrated with a shallow and wide ReLU network in\nthe Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the\npaper is giving a practical use of the neural tangent kernel in reinforcement\nlearning.",
          "link": "http://arxiv.org/abs/2107.09139",
          "publishedOn": "2021-07-21T02:01:35.570Z",
          "wordCount": 641,
          "title": "Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach. (arXiv:2107.09139v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1\">Curtis Hawthorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1\">Ian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swavely_R/0/1/0/all/0/1\">Rigel Swavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1\">Jesse Engel</a>",
          "description": "Automatic Music Transcription has seen significant progress in recent years\nby training custom deep neural networks on large datasets. However, these\nmodels have required extensive domain-specific design of network architectures,\ninput/output representations, and complex decoding schemes. In this work, we\nshow that equivalent performance can be achieved using a generic\nencoder-decoder Transformer with standard decoding methods. We demonstrate that\nthe model can learn to translate spectrogram inputs directly to MIDI-like\noutput events for several transcription tasks. This sequence-to-sequence\napproach simplifies transcription by jointly modeling audio features and\nlanguage-like output dependencies, thus removing the need for task-specific\narchitectures. These results point toward possibilities for creating new Music\nInformation Retrieval models by focusing on dataset creation and labeling\nrather than custom model design.",
          "link": "http://arxiv.org/abs/2107.09142",
          "publishedOn": "2021-07-21T02:01:35.563Z",
          "wordCount": 555,
          "title": "Sequence-to-Sequence Piano Transcription with Transformers. (arXiv:2107.09142v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09207",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hou_T/0/1/0/all/0/1\">Thomas Y. Hou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhenzhen Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyun Zhang</a>",
          "description": "We show that the Riemannian gradient descent algorithm on the low-rank matrix\nmanifold almost surely escapes some spurious critical points on the boundary of\nthe manifold. Given that the low-rank matrix manifold is an incomplete set,\nthis result is the first to overcome this difficulty and partially justify the\nglobal use of the Riemannian gradient descent on the manifold. The spurious\ncritical points are some rank-deficient matrices that capture only part of the\nSVD components of the ground truth. They exhibit very singular behavior and\nevade the classical analysis of strict saddle points. We show that using the\ndynamical low-rank approximation and a rescaled gradient flow, some of the\nspurious critical points can be converted to classical strict saddle points,\nwhich leads to the desired result. Numerical experiments are provided to\nsupport our theoretical findings.",
          "link": "http://arxiv.org/abs/2107.09207",
          "publishedOn": "2021-07-21T02:01:35.534Z",
          "wordCount": 586,
          "title": "Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold. (arXiv:2107.09207v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingzhong Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>",
          "description": "Stein variational gradient descent (SVGD) and its variants have shown\npromising successes in approximate inference for complex distributions.\nHowever, their empirical performance depends crucially on the choice of optimal\nkernel. Unfortunately, RBF kernel with median heuristics is a common choice in\nprevious approaches which has been proved sub-optimal. Inspired by the paradigm\nof multiple kernel learning, our solution to this issue is using a combination\nof multiple kernels to approximate the optimal kernel instead of a single one\nwhich may limit the performance and flexibility. To do so, we extend Kernelized\nStein Discrepancy (KSD) to its multiple kernel view called Multiple Kernelized\nStein Discrepancy (MKSD). Further, we leverage MKSD to construct a general\nalgorithm based on SVGD, which be called Multiple Kernel SVGD (MK-SVGD).\nBesides, we automatically assign a weight to each kernel without any other\nparameters. The proposed method not only gets rid of optimal kernel dependence\nbut also maintains computational effectiveness. Experiments on various tasks\nand models show the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2107.09338",
          "publishedOn": "2021-07-21T02:01:35.518Z",
          "wordCount": 596,
          "title": "Kernel Selection for Stein Variational Gradient Descent. (arXiv:2107.09338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1\">Daniel Severo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1\">James Townsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1\">Ashish Khisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1\">Alireza Makhzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>",
          "description": "Current methods that optimally compress multisets are not suitable for\nhigh-dimensional symbols, as their compute time scales linearly with alphabet\nsize. Compressing a multiset as an ordered sequence with off-the-shelf codecs\nis computationally more efficient, but has a sub-optimal compression rate, as\nbits are wasted encoding the order between symbols. We present a method that\ncan recover those bits, assuming symbols are i.i.d., at the cost of an\nadditional $\\mathcal{O}(|\\mathcal{M}|\\log M)$ in average time complexity, where\n$|\\mathcal{M}|$ and $M$ are the total and unique number of symbols in the\nmultiset. Our method is compatible with any prefix-free code. Experiments show\nthat, when paired with efficient coders, our method can efficiently compress\nhigh-dimensional sources such as multisets of images and collections of JSON\nfiles.",
          "link": "http://arxiv.org/abs/2107.09202",
          "publishedOn": "2021-07-21T02:01:35.511Z",
          "wordCount": 560,
          "title": "Compressing Multisets with Large Alphabets. (arXiv:2107.09202v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09060",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jiazhen Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1\">Haikun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_G/0/1/0/all/0/1\">Gastao Cruz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gilliam_C/0/1/0/all/0/1\">Christopher Gilliam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1\">Thierry Blu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botnar_R/0/1/0/all/0/1\">Ren&#xe9; Botnar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>",
          "description": "Physiological motion, such as cardiac and respiratory motion, during Magnetic\nResonance (MR) image acquisition can cause image artifacts. Motion correction\ntechniques have been proposed to compensate for these types of motion during\nthoracic scans, relying on accurate motion estimation from undersampled\nmotion-resolved reconstruction. A particular interest and challenge lie in the\nderivation of reliable non-rigid motion fields from the undersampled\nmotion-resolved data. Motion estimation is usually formulated in image space\nvia diffusion, parametric-spline, or optical flow methods. However, image-based\nregistration can be impaired by remaining aliasing artifacts due to the\nundersampled motion-resolved reconstruction. In this work, we describe a\nformalism to perform non-rigid registration directly in the sampled Fourier\nspace, i.e. k-space. We propose a deep-learning based approach to perform fast\nand accurate non-rigid registration from the undersampled k-space data. The\nbasic working principle originates from the Local All-Pass (LAP) technique, a\nrecently introduced optical flow-based registration. The proposed LAPNet is\ncompared against traditional and deep learning image-based registrations and\ntested on fully-sampled and highly-accelerated (with two undersampling\nstrategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients\nwith suspected liver or lung metastases and 25 healthy subjects. The proposed\nLAPNet provided consistent and superior performance to image-based approaches\nthroughout different sampling trajectories and acceleration factors.",
          "link": "http://arxiv.org/abs/2107.09060",
          "publishedOn": "2021-07-21T02:01:35.437Z",
          "wordCount": 677,
          "title": "LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging. (arXiv:2107.09060v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pikoulis_E/0/1/0/all/0/1\">Erion-Vasilis Pikoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrokefalidis_C/0/1/0/all/0/1\">Christos Mavrokefalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalos_A/0/1/0/all/0/1\">Aris S. Lalos</a>",
          "description": "Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount\nof interest in the past few decades, while one of the most critical operations\nin these systems is the perception of the environment. Deep learning and,\nespecially, the use of Deep Neural Networks (DNNs) provides impressive results\nin analyzing and understanding complex and dynamic scenes from visual data. The\nprediction horizons for those perception systems are very short and inference\nmust often be performed in real time, stressing the need of transforming the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Our goal in this work is to\ninvestigate best practices for appropriately applying novel weight sharing\ntechniques, optimizing the available variables and the training procedures\ntowards the significant acceleration of widely adopted DNNs. Extensive\nevaluation studies carried out using various state-of-the-art DNN models in\nobject detection and tracking experiments, provide details about the type of\nerrors that manifest after the application of weight sharing techniques,\nresulting in significant acceleration gains with negligible accuracy losses.",
          "link": "http://arxiv.org/abs/2107.09101",
          "publishedOn": "2021-07-21T02:01:35.427Z",
          "wordCount": 620,
          "title": "Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems. (arXiv:2107.09101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicente_J/0/1/0/all/0/1\">Juan Pablo de Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>",
          "description": "Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.",
          "link": "http://arxiv.org/abs/2107.09170",
          "publishedOn": "2021-07-21T02:01:35.400Z",
          "wordCount": 624,
          "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors. (arXiv:2107.09170v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09078",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Cai_H/0/1/0/all/0/1\">Haoyuan Cai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>",
          "description": "Quantum computers hold unprecedented potentials for machine learning\napplications. Here, we prove that physical quantum circuits are PAC (probably\napproximately correct) learnable on a quantum computer via empirical risk\nminimization: to learn a quantum circuit with at most $n^c$ gates and each gate\nacting on a constant number of qubits, the sample complexity is bounded by\n$\\tilde{O}(n^{c+1})$. In particular, we explicitly construct a family of\nvariational quantum circuits with $O(n^{c+1})$ elementary gates arranged in a\nfixed pattern, which can represent all physical quantum circuits consisting of\nat most $n^c$ elementary gates. Our results provide a valuable guide for\nquantum machine learning in both theory and experiment.",
          "link": "http://arxiv.org/abs/2107.09078",
          "publishedOn": "2021-07-21T02:01:35.387Z",
          "wordCount": 544,
          "title": "Sample Complexity of Learning Quantum Circuits. (arXiv:2107.09078v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Abhinav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriharsha_R/0/1/0/all/0/1\">Ram Sriharsha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sichen Zhong</a>",
          "description": "Decomposing a complex time series into trend, seasonality, and remainder\ncomponents is an important primitive that facilitates time series anomaly\ndetection, change point detection and forecasting. Although numerous batch\nalgorithms are known for time series decomposition, none operate well in an\nonline scalable setting where high throughput and real-time response are\nparamount. In this paper, we propose OnlineSTL, a novel online algorithm for\ntime series decomposition which solves the scalability problem and is deployed\nfor real-time metrics monitoring on high resolution, high ingest rate data.\nExperiments on different synthetic and real world time series datasets\ndemonstrate that OnlineSTL achieves orders of magnitude speedups while\nmaintaining quality of decomposition.",
          "link": "http://arxiv.org/abs/2107.09110",
          "publishedOn": "2021-07-21T02:01:35.365Z",
          "wordCount": 550,
          "title": "OnlineSTL: Scaling Time Series Decomposition by 100x. (arXiv:2107.09110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larma_M/0/1/0/all/0/1\">Mikel Landajuela Larma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1\">Brenden K. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo K. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santiago_C/0/1/0/all/0/1\">Claudio P. Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatt_R/0/1/0/all/0/1\">Ruben Glatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1\">T. Nathan Mundhenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettit_J/0/1/0/all/0/1\">Jacob F. Pettit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faissol_D/0/1/0/all/0/1\">Daniel M. Faissol</a>",
          "description": "Many machine learning strategies designed to automate mathematical tasks\nleverage neural networks to search large combinatorial spaces of mathematical\nsymbols. In contrast to traditional evolutionary approaches, using a neural\nnetwork at the core of the search allows learning higher-level symbolic\npatterns, providing an informed direction to guide the search. When no labeled\ndata is available, such networks can still be trained using reinforcement\nlearning. However, we demonstrate that this approach can suffer from an early\ncommitment phenomenon and from initialization bias, both of which limit\nexploration. We present two exploration methods to tackle these issues,\nbuilding upon ideas of entropy regularization and distribution initialization.\nWe show that these techniques can improve the performance, increase sample\nefficiency, and lower the complexity of solutions for the task of symbolic\nregression.",
          "link": "http://arxiv.org/abs/2107.09158",
          "publishedOn": "2021-07-21T02:01:35.334Z",
          "wordCount": 610,
          "title": "Improving exploration in policy gradient search: Application to symbolic optimization. (arXiv:2107.09158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09051",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>",
          "description": "AI in finance broadly refers to the applications of AI techniques in\nfinancial businesses. This area has been lasting for decades with both classic\nand modern AI techniques applied to increasingly broader areas of finance,\neconomy and society. In contrast to either discussing the problems, aspects and\nopportunities of finance that have benefited from specific AI techniques and in\nparticular some new-generation AI and data science (AIDS) areas or reviewing\nthe progress of applying specific techniques to resolving certain financial\nproblems, this review offers a comprehensive and dense roadmap of the\noverwhelming challenges, techniques and opportunities of AI research in finance\nover the past decades. The landscapes and challenges of financial businesses\nand data are firstly outlined, followed by a comprehensive categorization and a\ndense overview of the decades of AI research in finance. We then structure and\nillustrate the data-driven analytics and learning of financial businesses and\ndata. The comparison, criticism and discussion of classic vs. modern AI\ntechniques for finance are followed. Lastly, open issues and opportunities\naddress future AI-empowered finance and finance-motivated AI research.",
          "link": "http://arxiv.org/abs/2107.09051",
          "publishedOn": "2021-07-21T02:01:35.316Z",
          "wordCount": 627,
          "title": "AI in Finance: Challenges, Techniques and Opportunities. (arXiv:2107.09051v1 [q-fin.CP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanmay Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avaneesh/0/1/0/all/0/1\">Avaneesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rohit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shorey_R/0/1/0/all/0/1\">Rajeev Shorey</a>",
          "description": "With the increasing reliance of users on smart devices, bringing essential\ncomputation at the edge has become a crucial requirement for any type of\nbusiness. Many such computations utilize Convolution Neural Networks (CNNs) to\nperform AI tasks, having high resource and computation requirements, that are\ninfeasible for edge devices. Splitting the CNN architecture to perform part of\nthe computation on edge and remaining on the cloud is an area of research that\nhas seen increasing interest in the field. In this paper, we assert that\nrunning CNNs between an edge device and the cloud is synonymous to solving a\nresource-constrained optimization problem that minimizes the latency and\nmaximizes resource utilization at the edge. We formulate a multi-objective\noptimization problem and propose the LMOS algorithm to achieve a Pareto\nefficient solution. Experiments done on real-world edge devices show that, LMOS\nensures feasible execution of different CNN models at the edge and also\nimproves upon existing state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2107.09123",
          "publishedOn": "2021-07-21T02:01:35.299Z",
          "wordCount": 598,
          "title": "Latency-Memory Optimized Splitting of Convolution Neural Networks for Resource Constrained Edge Devices. (arXiv:2107.09123v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09118",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1\">Donya Khaledyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tajally_A/0/1/0/all/0/1\">AmirReza Tajally</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkhosh_R/0/1/0/all/0/1\">Reza Sarkhosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamsi_A/0/1/0/all/0/1\">Afshar Shamsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asgharnezhad_H/0/1/0/all/0/1\">Hamzeh Asgharnezhad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "Deep learning (DL) models have received particular attention in medical\nimaging due to their promising pattern recognition capabilities. However, Deep\nNeural Networks (DNNs) require a huge amount of data, and because of the lack\nof sufficient data in this field, transfer learning can be a great solution.\nDNNs used for disease diagnosis meticulously concentrate on improving the\naccuracy of predictions without providing a figure about their confidence of\npredictions. Knowing how much a DNN model is confident in a computer-aided\ndiagnosis model is necessary for gaining clinicians' confidence and trust in\nDL-based solutions. To address this issue, this work presents three different\nmethods for quantifying uncertainties for skin cancer detection from images. It\nalso comprehensively evaluates and compares performance of these DNNs using\nnovel uncertainty-related metrics. The obtained results reveal that the\npredictive uncertainty estimation methods are capable of flagging risky and\nerroneous predictions with a high uncertainty estimate. We also demonstrate\nthat ensemble approaches are more reliable in capturing uncertainties through\ninference.",
          "link": "http://arxiv.org/abs/2107.09118",
          "publishedOn": "2021-07-21T02:01:35.272Z",
          "wordCount": 625,
          "title": "Confidence Aware Neural Networks for Skin Cancer Detection. (arXiv:2107.09118v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09070",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+List_F/0/1/0/all/0/1\">Florian List</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rodd_N/0/1/0/all/0/1\">Nicholas L. Rodd</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lewis_G/0/1/0/all/0/1\">Geraint F. Lewis</a>",
          "description": "The two leading hypotheses for the Galactic Center Excess (GCE) in the\n$\\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars\n(MSPs) and dark-matter (DM) annihilation. The dichotomy between these\nexplanations is typically reflected by modeling them as two separate emission\ncomponents. However, point-sources (PSs) such as MSPs become statistically\ndegenerate with smooth Poisson emission in the ultra-faint limit (formally\nwhere each source is expected to contribute much less than one photon on\naverage), leading to an ambiguity that can render questions such as whether the\nemission is PS-like or Poissonian in nature ill-defined. We present a\nconceptually new approach that describes the PS and Poisson emission in a\nunified manner and only afterwards derives constraints on the Poissonian\ncomponent from the so obtained results. For the implementation of this\napproach, we leverage deep learning techniques, centered around a neural\nnetwork-based method for histogram regression that expresses uncertainties in\nterms of quantiles. We demonstrate that our method is robust against a number\nof systematics that have plagued previous approaches, in particular DM / PS\nmisattribution. In the $\\textit{Fermi}$ data, we find a faint GCE described by\na median source-count distribution (SCD) peaked at a flux of $\\sim4 \\times\n10^{-11} \\ \\text{counts} \\ \\text{cm}^{-2} \\ \\text{s}^{-1}$ (corresponding to\n$\\sim3 - 4$ expected counts per PS), which would require $N \\sim\n\\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N =\n\\text{29,300}$ across the sky). Although faint, this SCD allows us to derive\nthe constraint $\\eta_P \\leq 66\\%$ for the Poissonian fraction of the GCE flux\n$\\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE\nflux is due to PSs.",
          "link": "http://arxiv.org/abs/2107.09070",
          "publishedOn": "2021-07-21T02:01:35.252Z",
          "wordCount": 759,
          "title": "Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets. (arXiv:2107.09070v1 [astro-ph.HE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09082",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Veiga_M/0/1/0/all/0/1\">Maria Han Veiga</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Meng_X/0/1/0/all/0/1\">Xi Meng</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gnedin_O/0/1/0/all/0/1\">Oleg Y. Gnedin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gnedin_N/0/1/0/all/0/1\">Nickolay Y. Gnedin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Huan_X/0/1/0/all/0/1\">Xun Huan</a>",
          "description": "We describe a novel end-to-end approach using Machine Learning to reconstruct\nthe power spectrum of cosmological density perturbations at high redshift from\nobserved quasar spectra. State-of-the-art cosmological simulations of structure\nformation are used to generate a large synthetic dataset of line-of-sight\nabsorption spectra paired with 1-dimensional fluid quantities along the same\nline-of-sight, such as the total density of matter and the density of neutral\natomic hydrogen. With this dataset, we build a series of data-driven models to\npredict the power spectrum of total matter density. We are able to produce\nmodels which yield reconstruction to accuracy of about 1% for wavelengths $k\n\\leq 2 h Mpc^{-1}$, while the error increases at larger $k$. We show the size\nof data sample required to reach a particular error rate, giving a sense of how\nmuch data is necessary to reach a desired accuracy. This work provides a\nfoundation for developing methods to analyse very large upcoming datasets with\nthe next-generation observational facilities.",
          "link": "http://arxiv.org/abs/2107.09082",
          "publishedOn": "2021-07-21T02:01:35.213Z",
          "wordCount": 625,
          "title": "Reconstruction of the Density Power Spectrum from Quasar Spectra using Machine Learning. (arXiv:2107.09082v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09086",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Auddy_S/0/1/0/all/0/1\">Sayantan Auddy</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dey_R/0/1/0/all/0/1\">Ramit Dey</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lin_M/0/1/0/all/0/1\">Min-Kai Lin</a> (ASIAA, NCTS Physics Division), <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hall_C/0/1/0/all/0/1\">Cassandra Hall</a>",
          "description": "The observed sub-structures, like annular gaps, in dust emissions from\nprotoplanetary disk, are often interpreted as signatures of embedded planets.\nFitting a model of planetary gaps to these observed features using customized\nsimulations or empirical relations can reveal the characteristics of the hidden\nplanets. However, customized fitting is often impractical owing to the\nincreasing sample size and the complexity of disk-planet interaction. In this\npaper we introduce the architecture of DPNNet-2.0, second in the series after\nDPNNet \\citep{aud20}, designed using a Convolutional Neural Network ( CNN, here\nspecifically ResNet50) for predicting exoplanet masses directly from simulated\nimages of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally\nconsists of a multi-input framework that uses both a CNN and multi-layer\nperceptron (a class of artificial neural network) for processing image and disk\nparameters simultaneously. This enables DPNNet-2.0 to be trained using images\ndirectly, with the added option of considering disk parameters (disk\nviscosities, disk temperatures, disk surface density profiles, dust abundances,\nand particle Stokes numbers) generated from disk-planet hydrodynamic\nsimulations as inputs. This work provides the required framework and is the\nfirst step towards the use of computer vision (implementing CNN) to directly\nextract mass of an exoplanet from planetary gaps observed in dust-surface\ndensity maps by telescopes such as the Atacama Large (sub-)Millimeter Array.",
          "link": "http://arxiv.org/abs/2107.09086",
          "publishedOn": "2021-07-21T02:01:35.180Z",
          "wordCount": 681,
          "title": "DPNNet-2.0 Part I: Finding hidden planets from simulated images of protoplanetary disk gaps. (arXiv:2107.09086v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09055",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sonkiya_P/0/1/0/all/0/1\">Priyank Sonkiya</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_V/0/1/0/all/0/1\">Vikas Bajpai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bansal_A/0/1/0/all/0/1\">Anukriti Bansal</a>",
          "description": "The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.",
          "link": "http://arxiv.org/abs/2107.09055",
          "publishedOn": "2021-07-21T02:01:35.125Z",
          "wordCount": 699,
          "title": "Stock price prediction using BERT and GAN. (arXiv:2107.09055v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00088",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1\">Sean Hooten</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1\">Raymond G. Beausoleil</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1\">Thomas Van Vaerenbergh</a>",
          "description": "We present a proof-of-concept technique for the inverse design of\nelectromagnetic devices motivated by the policy gradient method in\nreinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE\nCriteria for Enhanced Design). This technique uses a probabilistic generative\nneural network interfaced with an electromagnetic solver to assist in the\ndesign of photonic devices, such as grating couplers. We show that PHORCED\nobtains better performing grating coupler designs than local gradient-based\ninverse design via the adjoint method, while potentially providing faster\nconvergence over competing state-of-the-art generative methods. Furthermore, we\nimplement transfer learning with PHORCED, demonstrating that a neural network\ntrained to optimize 8$^\\circ$ grating couplers can then be re-trained on\ngrating couplers with alternate scattering angles while requiring >$10\\times$\nfewer simulations than control cases.",
          "link": "http://arxiv.org/abs/2107.00088",
          "publishedOn": "2021-07-20T02:04:49.165Z",
          "wordCount": 591,
          "title": "Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03920",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1\">Niccol&#xf2; Dalmasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1\">David Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1\">Ann B. Lee</a>",
          "description": "Many areas of science make extensive use of computer simulators that\nimplicitly encode likelihood functions of complex systems. Classical\nstatistical methods are poorly suited for these so-called likelihood-free\ninference (LFI) settings, outside the asymptotic and low-dimensional regimes.\nAlthough new machine learning methods, such as normalizing flows, have\nrevolutionized the sample efficiency and capacity of LFI methods, it remains an\nopen question whether they produce reliable measures of uncertainty. This paper\npresents a statistical framework for LFI that unifies classical statistics with\nmodern machine learning to: (1) efficiently construct frequentist confidence\nsets and hypothesis tests with finite-sample guarantees of nominal coverage\n(type I error control) and power; (2) provide practical diagnostics for\nassessing empirical coverage over the entire parameter space. We refer to our\nframework as likelihood-free frequentist inference (LF2I). Any method that\nestimates a test statistic, like the likelihood ratio, can be plugged into our\nframework to create valid confidence sets and compute diagnostics, without\ncostly Monte Carlo samples at fixed parameter settings. In this work, we\nspecifically study the power of two test statistics (ACORE and BFF), which,\nrespectively, maximize versus integrate an odds function over the parameter\nspace. Our study offers multifaceted perspectives on the challenges in LF2I.",
          "link": "http://arxiv.org/abs/2107.03920",
          "publishedOn": "2021-07-20T02:04:49.148Z",
          "wordCount": 674,
          "title": "Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification. (arXiv:2107.03920v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1\">Nolan Wagener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>",
          "description": "Many sequential decision problems involve finding a policy that maximizes\ntotal reward while obeying safety constraints. Although much recent research\nhas focused on the development of safe reinforcement learning (RL) algorithms\nthat produce a safe policy after training, ensuring safety during training as\nwell remains an open problem. A fundamental challenge is performing exploration\nwhile still satisfying constraints in an unknown Markov decision process (MDP).\nIn this work, we address this problem for the chance-constrained setting. We\npropose a new algorithm, SAILR, that uses an intervention mechanism based on\nadvantage functions to keep the agent safe throughout training and optimizes\nthe agent's policy using off-the-shelf RL algorithms designed for unconstrained\nMDPs. Our method comes with strong guarantees on safety during both training\nand deployment (i.e., after training and without the intervention mechanism)\nand policy performance compared to the optimal safety-constrained policy. In\nour experiments, we show that SAILR violates constraints far less during\ntraining than standard safe RL and constrained MDP approaches and converges to\na well-performing policy that can be deployed safely without intervention. Our\ncode is available at https://github.com/nolanwagener/safe_rl.",
          "link": "http://arxiv.org/abs/2106.09110",
          "publishedOn": "2021-07-20T02:04:49.130Z",
          "wordCount": 653,
          "title": "Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sivaraman Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We derive bounds on the path length $\\zeta$ of gradient descent (GD) and\ngradient flow (GF) curves for various classes of smooth convex and nonconvex\nfunctions. Among other results, we prove that: (a) if the iterates are linearly\nconvergent with factor $(1-c)$, then $\\zeta$ is at most $\\mathcal{O}(1/c)$; (b)\nunder the Polyak-Kurdyka-Lojasiewicz (PKL) condition, $\\zeta$ is at most\n$\\mathcal{O}(\\sqrt{\\kappa})$, where $\\kappa$ is the condition number, and at\nleast $\\widetilde\\Omega(\\sqrt{d} \\wedge \\kappa^{1/4})$; (c) for quadratics,\n$\\zeta$ is $\\Theta(\\min\\{\\sqrt{d},\\sqrt{\\log \\kappa}\\})$ and in some cases can\nbe independent of $\\kappa$; (d) assuming just convexity, $\\zeta$ can be at most\n$2^{4d\\log d}$; (e) for separable quasiconvex functions, $\\zeta$ is\n${\\Theta}(\\sqrt{d})$. Thus, we advance current understanding of the properties\nof GD and GF curves beyond rates of convergence. We expect our techniques to\nfacilitate future studies for other algorithms.",
          "link": "http://arxiv.org/abs/1908.01089",
          "publishedOn": "2021-07-20T02:04:49.113Z",
          "wordCount": 637,
          "title": "Path Length Bounds for Gradient Descent and Flow. (arXiv:1908.01089v4 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1\">Mohit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lingyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zi Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1\">Peter Cho-Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>",
          "description": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications\ngenerates a strong demand for explanations that are robust to noise and align\nwell with human intuition. Most existing methods generate explanations by\nidentifying a subgraph of an input graph that has a strong correlation with the\nprediction. These explanations are not robust to noise because independently\noptimizing the correlation for a single input can easily overfit noise.\nMoreover, they do not align well with human intuition because removing an\nidentified subgraph from an input graph does not necessarily change the\nprediction result. In this paper, we propose a novel method to generate robust\ncounterfactual explanations on GNNs by explicitly modelling the common decision\nlogic of GNNs on similar input graphs. Our explanations are naturally robust to\nnoise because they are produced from the common decision boundaries of a GNN\nthat govern the predictions of many similar input graphs. The explanations also\nalign well with human intuition because removing the set of edges identified by\nan explanation from the input graph changes the prediction significantly.\nExhaustive experiments on many public datasets demonstrate the superior\nperformance of our method.",
          "link": "http://arxiv.org/abs/2107.04086",
          "publishedOn": "2021-07-20T02:04:49.068Z",
          "wordCount": 645,
          "title": "Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04631",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Fangcao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cervone_G/0/1/0/all/0/1\">Guido Cervone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salvador_M/0/1/0/all/0/1\">Mark Salvador</a>",
          "description": "Atmospheric correction is a fundamental task in remote sensing because\nobservations are taken either of the atmosphere or looking through the\natmosphere. Atmospheric correction errors can significantly alter the spectral\nsignature of the observations, and lead to invalid classifications or target\ndetection. This is even more crucial when working with hyperspectral data,\nwhere a precise measurement of spectral properties is required.\nState-of-the-art physics-based atmospheric correction approaches require\nextensive prior knowledge about sensor characteristics, collection geometry,\nand environmental characteristics of the scene being collected. These\napproaches are computationally expensive, prone to inaccuracy due to lack of\nsufficient environmental and collection information, and often impossible for\nreal-time applications. In this paper, a geometry-dependent hybrid neural\nnetwork is proposed for automatic atmospheric correction using multi-scan\nhyperspectral data collected from different geometries. The proposed network\ncan characterize the atmosphere without any additional meteorological data. A\ngrid-search method is also proposed to solve the temperature emissivity\nseparation problem. Results show that the proposed network has the capacity to\naccurately characterize the atmosphere and estimate target emissivity spectra\nwith a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This\nsolution can lead to accurate atmospheric correction to improve target\ndetection for real time applications.",
          "link": "http://arxiv.org/abs/2107.04631",
          "publishedOn": "2021-07-20T02:04:49.049Z",
          "wordCount": 670,
          "title": "Ill-posed Surface Emissivity Retrieval from Multi-Geometry Hyperspectral Images using a Hybrid Deep Neural Network. (arXiv:2107.04631v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>",
          "description": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n\nResults reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "link": "http://arxiv.org/abs/2105.05318",
          "publishedOn": "2021-07-20T02:04:49.029Z",
          "wordCount": 674,
          "title": "GANs for Medical Image Synthesis: An Empirical Study. (arXiv:2105.05318v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-20T02:04:49.013Z",
          "wordCount": 624,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10564",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1\">Aleksandr Podkopaev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "We study three notions of uncertainty quantification -- calibration,\nconfidence intervals and prediction sets -- for binary classification in the\ndistribution-free setting, that is without making any distributional\nassumptions on the data. With a focus towards calibration, we establish a\n'tripod' of theorems that connect these three notions for score-based\nclassifiers. A direct implication is that distribution-free calibration is only\npossible, even asymptotically, using a scoring function whose level sets\npartition the feature space into at most countably many sets. Parametric\ncalibration schemes such as variants of Platt scaling do not satisfy this\nrequirement, while nonparametric schemes based on binning do. To close the\nloop, we derive distribution-free confidence intervals for binned probabilities\nfor both fixed-width and uniform-mass binning. As a consequence of our 'tripod'\ntheorems, these confidence intervals for binned probabilities lead to\ndistribution-free calibration. We also derive extensions to settings with\nstreaming data and covariate shift.",
          "link": "http://arxiv.org/abs/2006.10564",
          "publishedOn": "2021-07-20T02:04:48.995Z",
          "wordCount": 637,
          "title": "Distribution-free binary classification: prediction sets, confidence intervals and calibration. (arXiv:2006.10564v3 [stat.ML] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1\">Erin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>",
          "description": "Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.",
          "link": "http://arxiv.org/abs/2107.03227",
          "publishedOn": "2021-07-20T02:04:48.978Z",
          "wordCount": 618,
          "title": "Scalable Data Balancing for Unlabeled Satellite Imagery. (arXiv:2107.03227v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burkhalter_L/0/1/0/all/0/1\">Lukas Burkhalter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lycklama_H/0/1/0/all/0/1\">Hidde Lycklama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viand_A/0/1/0/all/0/1\">Alexander Viand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchler_N/0/1/0/all/0/1\">Nicolas K&#xfc;chler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hithnawi_A/0/1/0/all/0/1\">Anwar Hithnawi</a>",
          "description": "Federated Learning is an emerging decentralized machine learning paradigm\nthat allows a large number of clients to train a joint model without the need\nto share their private data. Participants instead only share ephemeral updates\nnecessary to train the model. To ensure the confidentiality of the client\nupdates, Federated Learning systems employ secure aggregation; clients encrypt\ntheir gradient updates, and only the aggregated model is revealed to the\nserver. Achieving this level of data protection, however, presents new\nchallenges to the robustness of Federated Learning, i.e., the ability to\ntolerate failures and attacks. Unfortunately, in this setting, a malicious\nclient can now easily exert influence on the model behavior without being\ndetected. As Federated Learning is being deployed in practice in a range of\nsensitive applications, its robustness is growing in importance. In this paper,\nwe take a step towards understanding and improving the robustness of secure\nFederated Learning. We start this paper with a systematic study that evaluates\nand analyzes existing attack vectors and discusses potential defenses and\nassesses their effectiveness. We then present RoFL, a secure Federated Learning\nsystem that improves robustness against malicious clients through input checks\non the encrypted model updates. RoFL extends Federated Learning's secure\naggregation protocol to allow expressing a variety of properties and\nconstraints on model updates using zero-knowledge proofs. To enable RoFL to\nscale to typical Federated Learning settings, we introduce several ML and\ncryptographic optimizations specific to Federated Learning. We implement and\nevaluate a prototype of RoFL and show that realistic ML models can be trained\nin a reasonable time while improving robustness.",
          "link": "http://arxiv.org/abs/2107.03311",
          "publishedOn": "2021-07-20T02:04:48.935Z",
          "wordCount": 731,
          "title": "RoFL: Attestable Robustness for Secure Federated Learning. (arXiv:2107.03311v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12627",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Yuan Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1\">Richard Kueng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1\">Giacomo Torlai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1\">Victor V. Albert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1\">John Preskill</a>",
          "description": "Classical machine learning (ML) provides a potentially powerful approach to\nsolving challenging quantum many-body problems in physics and chemistry.\nHowever, the advantages of ML over more traditional methods have not been\nfirmly established. In this work, we prove that classical ML algorithms can\nefficiently predict ground state properties of gapped Hamiltonians in finite\nspatial dimensions, after learning from data obtained by measuring other\nHamiltonians in the same quantum phase of matter. In contrast, under widely\naccepted complexity theory assumptions, classical algorithms that do not learn\nfrom data cannot achieve the same guarantee. We also prove that classical ML\nalgorithms can efficiently classify a wide range of quantum phases of matter.\nOur arguments are based on the concept of a classical shadow, a succinct\nclassical description of a many-body quantum state that can be constructed in\nfeasible quantum experiments and be used to predict many properties of the\nstate. Extensive numerical experiments corroborate our theoretical results in a\nvariety of scenarios, including Rydberg atom systems, 2D random Heisenberg\nmodels, symmetry-protected topological phases, and topologically ordered\nphases.",
          "link": "http://arxiv.org/abs/2106.12627",
          "publishedOn": "2021-07-20T02:04:48.915Z",
          "wordCount": 642,
          "title": "Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1\">Christopher M. Poskitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.",
          "link": "http://arxiv.org/abs/2106.07851",
          "publishedOn": "2021-07-20T02:04:48.896Z",
          "wordCount": 750,
          "title": "Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1\">Sara Hajj Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Mohamed Nassar</a>",
          "description": "Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, some deep learning models do not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.",
          "link": "http://arxiv.org/abs/2107.04764",
          "publishedOn": "2021-07-20T02:04:48.877Z",
          "wordCount": 661,
          "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1\">Eran Yahav</a>",
          "description": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
          "link": "http://arxiv.org/abs/2106.06981",
          "publishedOn": "2021-07-20T02:04:48.833Z",
          "wordCount": 651,
          "title": "Thinking Like Transformers. (arXiv:2106.06981v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dongsheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1\">Na Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xianfeng Gu</a>",
          "description": "Optimal transport (OT) plays an essential role in various areas like machine\nlearning and deep learning. However, computing discrete optimal transport plan\nfor large scale problems with adequate accuracy and efficiency is still highly\nchallenging. Recently, methods based on the Sinkhorn algorithm add an entropy\nregularizer to the prime problem and get a trade off between efficiency and\naccuracy. In this paper, we propose a novel algorithm to further improve the\nefficiency and accuracy based on Nesterov's smoothing technique. Basically, the\nnon-smooth c-transform of the Kantorovich potential is approximated by the\nsmooth Log-Sum-Exp function, which finally smooths the original non-smooth\nKantorovich dual functional (energy). The smooth Kantorovich functional can be\noptimized by the fast proximal gradient algorithm (FISTA) efficiently.\nTheoretically, the computational complexity of the proposed method is given by\n$O(n^{\\frac{5}{2}} \\sqrt{\\log n} /\\epsilon)$, which is lower than that of the\nSinkhorn algorithm. Empirically, compared with the Sinkhorn algorithm, our\nexperimental results demonstrate that the proposed method achieves faster\nconvergence and better accuracy with the same parameter.",
          "link": "http://arxiv.org/abs/2104.05802",
          "publishedOn": "2021-07-20T02:04:48.814Z",
          "wordCount": 634,
          "title": "Efficient Optimal Transport Algorithm by Accelerated Gradient descent. (arXiv:2104.05802v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1\">Mrigank Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.",
          "link": "http://arxiv.org/abs/2103.01134",
          "publishedOn": "2021-07-20T02:04:48.795Z",
          "wordCount": 678,
          "title": "Domain Generalization via Inference-time Label-Preserving Target Projections. (arXiv:2103.01134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless. Code available at\n$\\href{https://github.com/dlmacedo/entropic-out-of-distribution-detection}{\\text{entropic\nout-of-distribution detection}}$.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-07-20T02:04:48.643Z",
          "wordCount": 638,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.",
          "link": "http://arxiv.org/abs/2107.08821",
          "publishedOn": "2021-07-20T02:04:48.623Z",
          "wordCount": 567,
          "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. (arXiv:2107.08821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhenhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>",
          "description": "Text to speech (TTS) is a crucial task for user interaction, but TTS model\ntraining relies on a sizable set of high-quality original datasets. Due to\nprivacy and security issues, the original datasets are usually unavailable\ndirectly. Recently, federated learning proposes a popular distributed machine\nlearning paradigm with an enhanced privacy protection mechanism. It offers a\npractical and secure framework for data owners to collaborate with others, thus\nobtaining a better global model trained on the larger dataset. However, due to\nthe high complexity of transformer models, the convergence process becomes slow\nand unstable in the federated learning setting. Besides, the transformer model\ntrained in federated learning is costly communication and limited computational\nspeed on clients, impeding its popularity. To deal with these challenges, we\npropose the federated dynamic transformer. On the one hand, the performance is\ngreatly improved comparing with the federated transformer, approaching\ncentralize-trained Transformer-TTS when increasing clients number. On the other\nhand, it achieves faster and more stable convergence in the training phase and\nsignificantly reduces communication time. Experiments on the LJSpeech dataset\nalso strongly prove our method's advantage.",
          "link": "http://arxiv.org/abs/2107.08795",
          "publishedOn": "2021-07-20T02:04:48.580Z",
          "wordCount": 624,
          "title": "Federated Learning with Dynamic Transformer for Text to Speech. (arXiv:2107.08795v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiuqi/0/1/0/all/0/1\">Jiuqi</a> (Elise) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulet_B/0/1/0/all/0/1\">Benoit Boulet</a>",
          "description": "With the rapid increase in the integration of renewable energy generation and\nthe wide adoption of various electric appliances, power grids are now faced\nwith more and more challenges. One prominent challenge is to implement\nefficient anomaly detection for different types of anomalous behaviors within\npower grids. These anomalous behaviors might be induced by unusual consumption\npatterns of the users, faulty grid infrastructures, outages, external\ncyberattacks, or energy fraud. Identifying such anomalies is of critical\nimportance for the reliable and efficient operation of modern power grids.\nVarious methods have been proposed for anomaly detection on power grid\ntime-series data. This paper presents a short survey of the recent advances in\nanomaly detection for power grid time-series data. Specifically, we first\noutline current research challenges in the power grid anomaly detection domain\nand further review the major anomaly detection approaches. Finally, we conclude\nthe survey by identifying the potential directions for future research.",
          "link": "http://arxiv.org/abs/2107.08835",
          "publishedOn": "2021-07-20T02:04:48.542Z",
          "wordCount": 602,
          "title": "Time Series Anomaly Detection for Smart Grids: A Survey. (arXiv:2107.08835v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "End-to-end AutoML has attracted intensive interests from both academia and\nindustry, which automatically searches for ML pipelines in a space induced by\nfeature engineering, algorithm/model selection, and hyper-parameter tuning.\nExisting AutoML systems, however, suffer from scalability issues when applying\nto application domains with large, high-dimensional search spaces. We present\nVolcanoML, a scalable and extensible framework that facilitates systematic\nexploration of large AutoML search spaces. VolcanoML introduces and implements\nbasic building blocks that decompose a large search space into smaller ones,\nand allows users to utilize these building blocks to compose an execution plan\nfor the AutoML problem at hand. VolcanoML further supports a Volcano-style\nexecution model - akin to the one supported by modern database systems - to\nexecute the plan constructed. Our evaluation demonstrates that, not only does\nVolcanoML raise the level of expressiveness for search space decomposition in\nAutoML, it also leads to actual findings of decomposition strategies that are\nsignificantly more efficient than the ones employed by state-of-the-art AutoML\nsystems such as auto-sklearn.",
          "link": "http://arxiv.org/abs/2107.08861",
          "publishedOn": "2021-07-20T02:04:48.525Z",
          "wordCount": 616,
          "title": "VolcanoML: Speeding up End-to-End AutoML via Scalable Search Space Decomposition. (arXiv:2107.08861v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Mon-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojun Li</a>",
          "description": "Maintaining long-term exploration ability remains one of the challenges of\ndeep reinforcement learning (DRL). In practice, the reward shaping-based\napproaches are leveraged to provide intrinsic rewards for the agent to\nincentivize motivation. However, most existing IRS modules rely on attendant\nmodels or additional memory to record and analyze learning procedures, which\nleads to high computational complexity and low robustness. Moreover, they\noveremphasize the influence of a single state on exploration, which cannot\nevaluate the exploration performance from a global perspective. To tackle the\nproblem, state entropy-based methods are proposed to encourage the agent to\nvisit the state space more equitably. However, the estimation error and sample\ncomplexity are prohibitive when handling environments with high-dimensional\nobservation. In this paper, we introduce a novel metric entitled Jain's\nfairness index (JFI) to replace the entropy regularizer, which requires no\nadditional models or memory. In particular, JFI overcomes the vanishing\nintrinsic rewards problem and can be generalized into arbitrary tasks.\nFurthermore, we use a variational auto-encoder (VAE) model to capture the\nlife-long novelty of states. Finally, the global JFI score and local state\nnovelty are combined to form a multimodal intrinsic reward, controlling the\nexploration extent more precisely. Finally, extensive simulation results\ndemonstrate that our multimodal reward shaping (MMRS) method can achieve higher\nperformance in contrast to other benchmark schemes.",
          "link": "http://arxiv.org/abs/2107.08888",
          "publishedOn": "2021-07-20T02:04:48.508Z",
          "wordCount": 658,
          "title": "Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koutini_K/0/1/0/all/0/1\">Khaled Koutini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_F/0/1/0/all/0/1\">Florian Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_J/0/1/0/all/0/1\">Jan Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>",
          "description": "Convolutional Neural Networks (CNNs) have been dominating classification\ntasks in various domains, such as machine vision, machine listening, and\nnatural language processing. In machine listening, while generally exhibiting\nvery good generalization capabilities, CNNs are sensitive to the specific audio\nrecording device used, which has been recognized as a substantial problem in\nthe acoustic scene classification (DCASE) community. In this study, we\ninvestigate the relationship between over-parameterization of acoustic scene\nclassification models, and their resulting generalization abilities.\nSpecifically, we test scaling CNNs in width and depth, under different\nconditions. Our results indicate that increasing width improves generalization\nto unseen devices, even without an increase in the number of parameters.",
          "link": "http://arxiv.org/abs/2107.08933",
          "publishedOn": "2021-07-20T02:04:48.490Z",
          "wordCount": 562,
          "title": "Over-Parameterization and Generalization in Audio Classification. (arXiv:2107.08933v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_M/0/1/0/all/0/1\">Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_M/0/1/0/all/0/1\">Morteza Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "We introduce the \\textit{epistemic neural network} (ENN) as an interface for\nuncertainty modeling in deep learning. All existing approaches to uncertainty\nmodeling can be expressed as ENNs, and any ENN can be identified with a\nBayesian neural network. However, this new perspective provides several\npromising directions for future research. Where prior work has developed\nprobabilistic inference tools for neural networks; we ask instead, `which\nneural networks are suitable as tools for probabilistic inference?'. We propose\na clear and simple metric for progress in ENNs: the KL-divergence with respect\nto a target distribution. We develop a computational testbed based on inference\nin a neural network Gaussian process and release our code as a benchmark at\n\\url{https://github.com/deepmind/enn}. We evaluate several canonical approaches\nto uncertainty modeling in deep learning, and find they vary greatly in their\nperformance. We provide insight to the sensitivity of these results and show\nthat our metric is highly correlated with performance in sequential decision\nproblems. Finally, we provide indications that new ENN architectures can\nimprove performance in both the statistical quality and computational cost.",
          "link": "http://arxiv.org/abs/2107.08924",
          "publishedOn": "2021-07-20T02:04:48.445Z",
          "wordCount": 612,
          "title": "Epistemic Neural Networks. (arXiv:2107.08924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ntakouris_T/0/1/0/all/0/1\">Theodoros Ntakouris</a>",
          "description": "In this document, a neural network is employed in order to estimate the\nsolution of the initial value problem in the context of non linear\ntrajectories. Such trajectories can be subject to gravity, thrust, drag,\ncentrifugal force, temperature, ambient air density and pressure. First, we\ngenerate a grid of trajectory points given a specified uniform density as a\ndesign parameter and then we investigate the performance of a neural network in\na compression and inverse problem task: the network is trained to predict the\ninitial conditions of the dynamics model we used in the simulation, given a\ntarget point in space. We investigate this as a regression task, with error\npropagation in consideration. For target points, up to a radius of 2\nkilometers, the model is able to accurately predict the initial conditions of\nthe trajectories, with sub-meter deviation. This simulation-based training\nprocess and novel real-world evaluation method is capable of computing\ntrajectories of arbitrary dimensions.",
          "link": "http://arxiv.org/abs/2107.08849",
          "publishedOn": "2021-07-20T02:04:48.411Z",
          "wordCount": 604,
          "title": "Exploring the efficacy of neural networks for trajectory compression and the inverse problem. (arXiv:2107.08849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_B/0/1/0/all/0/1\">Bhumika Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Anuj Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum/0/1/0/all/0/1\">Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katarya_R/0/1/0/all/0/1\">Rahul Katarya</a>",
          "description": "Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity,\ncyberbullying amongst children and other vulnerable groups are only growing\nover online classes, and increased access to social platforms, especially post\nCOVID-19. It is paramount to detect and ensure minors' safety across social\nplatforms so that any violence or hate-crime is automatically detected and\nstrict action is taken against it. In our work, we explore binary\nclassification by using a combination of datasets from various social media\nplatforms that cover a wide range of cyberbullying such as sexism, racism,\nabusive, and hate-speech. We experiment through multiple models such as\nBi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique\npreprocessing technique by introducing a slang-abusive corpus, achieving a\nhigher precision in comparison to models without slang preprocessing.",
          "link": "http://arxiv.org/abs/2107.08902",
          "publishedOn": "2021-07-20T02:04:48.393Z",
          "wordCount": 626,
          "title": "Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media. (arXiv:2107.08902v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_K/0/1/0/all/0/1\">Ke Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tierui Gong</a>",
          "description": "Federated learning is a widely used distributed deep learning framework that\nprotects the privacy of each client by exchanging model parameters rather than\nraw data. However, federated learning suffers from high communication costs, as\na considerable number of model parameters need to be transmitted many times\nduring the training process, making the approach inefficient, especially when\nthe communication network bandwidth is limited. This article proposes RingFed,\na novel framework to reduce communication overhead during the training process\nof federated learning. Rather than transmitting parameters between the center\nserver and each client, as in original federated learning, in the proposed\nRingFed, the updated parameters are transmitted between each client in turn,\nand only the final result is transmitted to the central server, thereby\nreducing the communication overhead substantially. After several local updates,\nclients first send their parameters to another proximal client, not to the\ncenter server directly, to preaggregate. Experiments on two different public\ndatasets show that RingFed has fast convergence, high model accuracy, and low\ncommunication cost.",
          "link": "http://arxiv.org/abs/2107.08873",
          "publishedOn": "2021-07-20T02:04:48.329Z",
          "wordCount": 609,
          "title": "RingFed: Reducing Communication Costs in Federated Learning on Non-IID Data. (arXiv:2107.08873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafferty_A/0/1/0/all/0/1\">Anna N. Rafferty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1\">Goran Radanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil T. Heffernan</a>",
          "description": "This survey article has grown out of the RL4ED workshop organized by the\nauthors at the Educational Data Mining (EDM) 2021 conference. We organized this\nworkshop as part of a community-building effort to bring together researchers\nand practitioners interested in the broad areas of reinforcement learning (RL)\nand education (ED). This article aims to provide an overview of the workshop\nactivities and summarize the main research directions in the area of RL for ED.",
          "link": "http://arxiv.org/abs/2107.08828",
          "publishedOn": "2021-07-20T02:04:48.308Z",
          "wordCount": 509,
          "title": "Reinforcement Learning for Education: Opportunities and Challenges. (arXiv:2107.08828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08787",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yichen Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fridlyand_J/0/1/0/all/0/1\">Jane Fridlyand</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_T/0/1/0/all/0/1\">Tiffany Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qi_T/0/1/0/all/0/1\">Ting Qi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simon_N/0/1/0/all/0/1\">Noah Simon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Leng_N/0/1/0/all/0/1\">Ning Leng</a>",
          "description": "Finding translational biomarkers stands center stage of the future of\npersonalized medicine in healthcare. We observed notable challenges in\nidentifying robust biomarkers as some with great performance in one scenario\noften fail to perform well in new trials (e.g. different population,\nindications). With rapid development in the clinical trial world (e.g. assay,\ndisease definition), new trials very likely differ from legacy ones in many\nperspectives and in development of biomarkers this heterogeneity should be\nconsidered. In response, we recommend considering building in the heterogeneity\nwhen evaluating biomarkers. In this paper, we present one evaluation strategy\nby using leave-one-study-out (LOSO) in place of conventional cross-validation\n(cv) methods to account for the potential heterogeneity across trials used for\nbuilding and testing the biomarkers. To demonstrate the performance of K-fold\nvs LOSO cv in estimating the effect size of biomarkers, we leveraged data from\nclinical trials and simulation studies. In our assessment, LOSO cv provided a\nmore objective estimate of the future performance. This conclusion remained\ntrue across different evaluation metrics and different statistical methods.",
          "link": "http://arxiv.org/abs/2107.08787",
          "publishedOn": "2021-07-20T02:04:48.291Z",
          "wordCount": 633,
          "title": "The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker. (arXiv:2107.08787v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood, we study the associated computational cost and we evaluate\nits performance using simulated data. We find that, when data comprise multiple\nrelated data sets, BHD outperforms the Bayesian Dirichlet equivalent uniform\n(BDeu) score in terms of reconstruction accuracy as measured by the Structural\nHamming distance, and that it is as accurate as BDeu when data are homogeneous.\nThis improvement is particularly clear when either the number of variables in\nthe network or the number of observations is large. Moreover, the estimated\nnetworks are sparser and therefore more interpretable than those obtained with\nBDeu thanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-20T02:04:47.806Z",
          "wordCount": 682,
          "title": "A Bayesian Hierarchical Score for Structure Learning from Related Data Sets. (arXiv:2008.01683v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_R/0/1/0/all/0/1\">Ryo Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onishi_M/0/1/0/all/0/1\">Masaki Onishi</a>",
          "description": "Crowd movement guidance has been a fascinating problem in various fields,\nsuch as easing traffic congestion in unusual events and evacuating people from\nan emergency-affected area. To grab the reins of crowds, there has been\nconsiderable demand for a decision support system that can answer a typical\nquestion: ``what will be the outcomes of each of the possible options in the\ncurrent situation. In this paper, we consider the problem of estimating the\neffects of crowd movement guidance from past data. To cope with limited amount\nof available data biased by past decision-makers, we leverage two recent\ntechniques in deep representation learning for spatial data analysis and causal\ninference. We use a spatial convolutional operator to extract effective spatial\nfeatures of crowds from a small amount of data and use balanced representation\nlearning based on the integral probability metrics to mitigate the selection\nbias and missing counterfactual outcomes. To evaluate the performance on\nestimating the treatment effects of possible guidance, we use a multi-agent\nsimulator to generate realistic data on evacuation scenarios in a crowded\ntheater, since there are no available datasets recording outcomes of all\npossible crowd movement guidance. The results of three experiments demonstrate\nthat our proposed method reduces the estimation error by at most 56% from\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.03980",
          "publishedOn": "2021-07-20T02:04:47.788Z",
          "wordCount": 685,
          "title": "Grab the Reins of Crowds: Estimating the Effects of Crowd Movement Guidance Using Causal Inference. (arXiv:2102.03980v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1\">Eran Malach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "Several recent works have shown separation results between deep neural\nnetworks, and hypothesis classes with inferior approximation capacity such as\nshallow networks or kernel classes. On the other hand, the fact that deep\nnetworks can efficiently express a target function does not mean that this\ntarget function can be learned efficiently by deep neural networks. In this\nwork we study the intricate connection between learnability and approximation\ncapacity. We show that learnability with deep networks of a target function\ndepends on the ability of simpler classes to approximate the target.\nSpecifically, we show that a necessary condition for a function to be learnable\nby gradient descent on deep neural networks is to be able to approximate the\nfunction, at least in a weak sense, with shallow neural networks. We also show\nthat a class of functions can be learned by an efficient statistical query\nalgorithm if and only if it can be approximated in a weak sense by some kernel\nclass. We give several examples of functions which demonstrate depth\nseparation, and conclude that they cannot be efficiently learned, even by a\nhypothesis class that can efficiently approximate them.",
          "link": "http://arxiv.org/abs/2102.00434",
          "publishedOn": "2021-07-20T02:04:47.732Z",
          "wordCount": 672,
          "title": "The Connection Between Approximation, Depth Separation and Learnability in Neural Networks. (arXiv:2102.00434v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16336",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1\">Emily M. Goren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.",
          "link": "http://arxiv.org/abs/2103.16336",
          "publishedOn": "2021-07-20T02:04:47.479Z",
          "wordCount": null,
          "title": "Model-based clustering of partial records. (arXiv:2103.16336v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04046",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ahfock_D/0/1/0/all/0/1\">Daniel Ahfock</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McLachlan_G/0/1/0/all/0/1\">Geoffrey J. McLachlan</a>",
          "description": "There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data for a classifier consists of a limited number of classified\nobservations but a much larger number of unclassified observations. This is\nbecause the procurement of classified data can be quite costly due to high\nacquisition costs and subsequent financial, time, and ethical issues that can\narise in attempts to provide the true class labels for the unclassified data\nthat have been acquired. We provide here a review of statistical SSL approaches\nto this problem, focussing on the recent result that a classifier formed from a\npartially classified sample can actually have smaller expected error rate than\nthat if the sample were completely classified.",
          "link": "http://arxiv.org/abs/2104.04046",
          "publishedOn": "2021-07-20T02:04:47.476Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning of Classifiers from a Statistical Perspective: A Brief Review. (arXiv:2104.04046v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets.",
          "link": "http://arxiv.org/abs/2106.07868",
          "publishedOn": "2021-07-20T02:04:47.463Z",
          "wordCount": null,
          "title": "Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1\">Guang Tan</a>",
          "description": "We consider graph representation learning in a self-supervised manner. Graph\nneural networks (GNNs) use neighborhood aggregation as a core component that\nresults in feature smoothing among nodes in proximity. While successful in\nvarious prediction tasks, such a paradigm falls short of capturing nodes'\nsimilarities over a long distance, which proves to be important for\nhigh-quality learning. To tackle this problem, we strengthen the graph with two\nadditional graph views, in which nodes are directly linked to those with the\nmost similar features or local structures. Not restricted by connectivity in\nthe original graph, the generated views allow the model to enhance its\nexpressive power with new and complementary perspectives from which to look at\nthe relationship between nodes. Following a contrastive learning approach, we\npropose a method that aims to maximize the agreement between representations\nacross generated views and the original graph. We also propose a channel-level\ncontrast approach that greatly reduces computation cost, compared to the\ncommonly used node level contrast, which requires computation cost quadratic in\nthe number of nodes. Extensive experiments on seven assortative graphs and four\ndisassortative graphs demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.03723",
          "publishedOn": "2021-07-20T02:04:47.461Z",
          "wordCount": null,
          "title": "Self-Supervised Graph Learning with Proximity-based Views and Channel Contrast. (arXiv:2106.03723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">T&#xfa;lio Marcondes Moreira</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1\">Jackson Geraldo de Faria Jr</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1\">Pedro O.S. Vaz de Melo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1\">Luiz Chaimowicz</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1\">Gilberto Medeiros-Ribeiro</a> (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)",
          "description": "Tidal range structures have been considered for large scale electricity\ngeneration for their potential ability to produce reasonable predictable energy\nwithout the emission of greenhouse gases. Once the main forcing components for\ndriving the tides have deterministic dynamics, the available energy in a given\ntidal power plant has been estimated, through analytical and numerical\noptimisation routines, as a mostly predictable event. This constraint imposes\nstate-of-art flexible operation methods to rely on tidal predictions\n(concurrent with measured data and up to a multiple of half-tidal cycles into\nthe future) to infer best operational strategies for tidal lagoons, with the\nadditional cost of requiring to run optimisation routines for every new tide.\nIn this paper, we propose a novel optimised operation of tidal lagoons with\nproximal policy optimisation through Unity ML-Agents. We compare this technique\nwith 6 different operation optimisation approaches (baselines) devised from the\nliterature, utilising the Swansea Bay Tidal Lagoon as a case study. We show\nthat our approach is successful in maximising energy generation through an\noptimised operational policy of turbines and sluices, yielding competitive\nresults with state-of-the-art methods of optimisation, regardless of test data\nused, requiring training once and performing real-time flexible control with\nmeasured ocean data only.",
          "link": "http://arxiv.org/abs/2106.10360",
          "publishedOn": "2021-07-20T02:04:47.421Z",
          "wordCount": null,
          "title": "Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_Kolahi_H/0/1/0/all/0/1\">Hassan Hafez-Kolahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniri_B/0/1/0/all/0/1\">Behrad Moniri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghshah_M/0/1/0/all/0/1\">Mahdieh Soleymani Baghshah</a>",
          "description": "In parametric Bayesian learning, a prior is assumed on the parameter $W$\nwhich determines the distribution of samples. In this setting, Minimum Excess\nRisk (MER) is defined as the difference between the minimum expected loss\nachievable when learning from data and the minimum expected loss that could be\nachieved if $W$ was observed. In this paper, we build upon and extend the\nrecent results of (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning\nand derive information-theoretic bounds on it. We formulate the problem as a\n(constrained) rate-distortion optimization and show how the solution can be\nbounded above and below by two other rate-distortion functions that are easier\nto study. The lower bound represents the minimum possible excess risk\nachievable by any process using $R$ bits of information from the parameter $W$.\nFor the upper bound, the optimization is further constrained to use $R$ bits\nfrom the training set, a setting which relates MER to information-theoretic\nbounds on the generalization gap in frequentist learning. We derive\ninformation-theoretic bounds on the difference between these upper and lower\nbounds and show that they can provide order-wise tight rates for MER under\ncertain conditions. This analysis gives more insight into the\ninformation-theoretic nature of Bayesian learning as well as providing novel\nbounds.",
          "link": "http://arxiv.org/abs/2105.04180",
          "publishedOn": "2021-07-20T02:04:47.413Z",
          "wordCount": null,
          "title": "Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning. (arXiv:2105.04180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05556",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ozcelik_R/0/1/0/all/0/1\">R&#x131;za &#xd6;z&#xe7;elik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bag_A/0/1/0/all/0/1\">Alperen Ba&#x11f;</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Atil_B/0/1/0/all/0/1\">Berk At&#x131;l</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif &#xd6;zk&#x131;r&#x131;ml&#x131;</a>",
          "description": "Motivation: Computational models that accurately identify high-affinity\nprotein-compound pairs can accelerate drug discovery pipelines. These models\naim to learn binding mechanics through drug-target interaction datasets and use\nthe learned knowledge for predicting the affinity of an input protein-compound\npair. However, the datasets they rely on bear misleading patterns that bias\nmodels towards memorizing dataset-specific biomolecule properties, instead of\nlearning binding mechanics. This results in models that struggle while\npredicting drug-target affinities (DTA), especially between de novo\nbiomolecules. Here we present DebiasedDTA, the first DTA model debiasing\napproach that avoids dataset biases in order to boost affinity prediction for\nnovel biomolecules. DebiasedDTA uses ensemble learning and sample weight\nadaptation for bias identification and avoidance and is applicable to almost\nall existing DTA prediction models. Results: The results show that DebiasedDTA\ncan boost models while predicting the interactions between novel biomolecules.\nKnown biomolecules also benefit from the performance improvement, especially\nwhen the test biomolecules are dissimilar to the training set. The experiments\nalso show that DebiasedDTA can augment DTA prediction models of different input\nand model structures and is able to avoid biases of different sources.\nAvailability and Implementation: The source code, the models, and the datasets\nare freely available for download at\nhttps://github.com/boun-tabi/debiaseddta-reproduce, implementation in Python3,\nand supported for Linux, MacOS and MS Windows. Contact:\narzucan.ozgur@boun.edu.tr, elif.ozkirimli@roche.com",
          "link": "http://arxiv.org/abs/2107.05556",
          "publishedOn": "2021-07-20T02:04:47.411Z",
          "wordCount": null,
          "title": "DebiasedDTA: Model Debiasing to Boost Drug-Target Affinity Prediction. (arXiv:2107.05556v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A non-redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-07-20T02:04:47.406Z",
          "wordCount": null,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06419",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1\">Peiqing Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haiying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>",
          "description": "Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n\nMethods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n\nResults: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n\nConclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.",
          "link": "http://arxiv.org/abs/2103.06419",
          "publishedOn": "2021-07-20T02:04:47.391Z",
          "wordCount": null,
          "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09022",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1\">Geon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yong Keun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.",
          "link": "http://arxiv.org/abs/2103.09022",
          "publishedOn": "2021-07-20T02:04:47.260Z",
          "wordCount": null,
          "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning in Projection Domain. (arXiv:2103.09022v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In deep learning with differential privacy (DP), the neural network achieves\nthe privacy usually at the cost of slower convergence (and thus lower\nperformance) than its non-private counterpart. This work gives the first\nconvergence analysis of the DP deep learning, through the lens of training\ndynamics and the neural tangent kernel (NTK). Our convergence theory\nsuccessfully characterizes the effects of two key components in the DP\ntraining: the per-sample clipping (flat or layerwise) and the noise addition.\nOur analysis not only initiates a general principled framework to understand\nthe DP deep learning with any network architecture and loss function, but also\nmotivates a new clipping method -- the global clipping, that significantly\nimproves the convergence while preserving the same privacy guarantee as the\nexisting local clipping.\n\nIn terms of theoretical results, we establish the precise connection between\nthe per-sample clipping and NTK matrix. We show that in the gradient flow,\ni.e., with infinitesimal learning rate, the noise level of DP optimizers does\nnot affect the convergence. We prove that DP gradient descent (GD) with global\nclipping guarantees the monotone convergence to zero loss, which can be\nviolated by the existing DP-GD with local clipping. Notably, our analysis\nframework easily extends to other optimizers, e.g., DP-Adam. Empirically\nspeaking, DP optimizers equipped with global clipping perform strongly on a\nwide range of classification and regression tasks. In particular, our global\nclipping is surprisingly effective at learning calibrated classifiers, in\ncontrast to the existing DP classifiers which are oftentimes over-confident and\nunreliable. Implementation-wise, the new clipping can be realized by adding one\nline of code into the Opacus library.",
          "link": "http://arxiv.org/abs/2106.07830",
          "publishedOn": "2021-07-20T02:04:47.255Z",
          "wordCount": null,
          "title": "On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zejin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongji Wang</a>",
          "description": "The increasing concerns about data privacy and security drive an emerging\nfield of studying privacy-preserving machine learning from isolated data\nsources, i.e., federated learning. A class of federated learning, vertical\nfederated learning, where different parties hold different features for common\nusers, has a great potential of driving a more variety of business cooperation\namong enterprises in many fields. In machine learning, decision tree ensembles\nsuch as gradient boosting decision tree (GBDT) and random forest are widely\napplied powerful models with high interpretability and modeling efficiency.\nHowever, the interpretability is compromised in state-of-the-art vertical\nfederated learning frameworks such as SecureBoost with anonymous features to\navoid possible data breaches. To address this issue in the inference process,\nin this paper, we propose Fed-EINI to protect data privacy and allow the\ndisclosure of feature meaning by concealing decision paths with a\ncommunication-efficient secure computation method for inference outputs. The\nadvantages of Fed-EINI will be demonstrated through both theoretical analysis\nand extensive numerical results.",
          "link": "http://arxiv.org/abs/2105.09540",
          "publishedOn": "2021-07-20T02:04:47.253Z",
          "wordCount": null,
          "title": "Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11793",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wanjun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Minghua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_S/0/1/0/all/0/1\">Steven H. Low</a>",
          "description": "AC optimal power flow (AC-OPF) problems need to be solved more frequently in\nthe future to maintain stable and economic power system operation. To tackle\nthis challenge, a deep neural network-based voltage-constrained approach\n(DeepOPF-V) is proposed to solve AC-OPF problems with high computational\nefficiency. Its unique design predicts voltages of all buses and then uses them\nto reconstruct the remaining variables without solving non-linear AC power flow\nequations. A fast post-processing process is developed to enforce the box\nconstraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE\n118/300-bus systems and a 2000-bus test system. Compared with existing studies,\nDeepOPF-V achieves decent computation speedup up to four orders of magnitude\nand comparable performance in optimality gap and preserving the feasibility of\nthe solution.",
          "link": "http://arxiv.org/abs/2103.11793",
          "publishedOn": "2021-07-20T02:04:47.241Z",
          "wordCount": null,
          "title": "DeepOPF-V: Solving AC-OPF Problems Efficiently. (arXiv:2103.11793v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Meng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuo-Jun Shen</a>",
          "description": "In this work, we propose a deep reinforcement learning (DRL) model for\nfinding a feasible solution for (mixed) integer programming (MIP) problems.\nFinding a feasible solution for MIP problems is critical because many\nsuccessful heuristics rely on a known initial feasible solution. However, it is\nin general NP-hard. Inspired by the feasibility pump (FP), a well-known\nheuristic for searching feasible MIP solutions, we develop a smart feasibility\npump (SFP) method using DRL. In addition to multi-layer perception (MLP), we\npropose a novel convolution neural network (CNN) structure for the policy\nnetwork to capture the hidden information of the constraint matrix of the MIP\nproblem. Numerical experiments on various problem instances show that SFP\nsignificantly outperforms the classic FP in terms of the number of steps\nrequired to reach the first feasible solution. Moreover, the CNN structure\nworks without the projection of the current solution as the input, which saves\nthe computational effort at each step of the FP algorithms to find projections.\nThis highlights the representational power of the CNN structure.",
          "link": "http://arxiv.org/abs/2102.09663",
          "publishedOn": "2021-07-20T02:04:47.225Z",
          "wordCount": null,
          "title": "Smart Feasibility Pump: Reinforcement Learning for (Mixed) Integer Programming. (arXiv:2102.09663v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron M. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>",
          "description": "We present a novel sensor-based learning navigation algorithm to compute a\ncollision-free trajectory for a robot in dense and dynamic environments with\nmoving obstacles or targets. Our approach uses deep reinforcement\nlearning-based expert policy that is trained using a sim2real paradigm. In\norder to increase the reliability and handle the failure cases of the expert\npolicy, we combine with a policy extraction technique to transform the\nresulting policy into a decision tree format. The resulting decision tree has\nproperties which we use to analyze and modify the policy and improve\nperformance on navigation metrics including smoothness, frequency of\noscillation, frequency of immobilization, and obstruction of target. We are\nable to modify the policy to address these imperfections without retraining,\ncombining the learning power of deep learning with the control of\ndomain-specific algorithms. We highlight the benefits of our algorithm in\nsimulated environments and navigating a Clearpath Jackal robot among moving\npedestrians. (Videos at this url:\nhttps://gamma.umd.edu/researchdirections/xrl/navviper)",
          "link": "http://arxiv.org/abs/2104.10818",
          "publishedOn": "2021-07-20T02:04:47.223Z",
          "wordCount": null,
          "title": "XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision Trees. (arXiv:2104.10818v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xueying Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.",
          "link": "http://arxiv.org/abs/2103.04075",
          "publishedOn": "2021-07-20T02:04:47.219Z",
          "wordCount": null,
          "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised Kinematic-Visual Data Alignment. (arXiv:2103.04075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Michelle M. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>",
          "description": "Spatial context is central to understanding health and disease. Yet reference\nprotein interaction networks lack such contextualization, thereby limiting the\nstudy of where protein interactions likely occur in the human body and how they\nmay be altered in disease. Contextualized protein interactions could better\ncharacterize genes with disease-specific interactions and elucidate diseases'\nmanifestation in specific cell types. Here, we introduce AWARE, a graph neural\nmessage passing approach to inject cellular and tissue context into protein\nembeddings. AWARE optimizes for a multi-scale embedding space, whose structure\nreflects network topology at a single-cell resolution. We construct a\nmulti-scale network of the Human Cell Atlas and apply AWARE to learn protein,\ncell type, and tissue embeddings that uphold cell type and tissue hierarchies.\nWe demonstrate AWARE's utility on the novel task of predicting whether a\nprotein is altered in disease and where that association most likely manifests\nin the human body. To this end, AWARE outperforms generic embeddings without\ncontextual information by at least 12.5%, showing AWARE's potential to reveal\ncontext-dependent roles of proteins in disease.",
          "link": "http://arxiv.org/abs/2106.02246",
          "publishedOn": "2021-07-20T02:04:47.216Z",
          "wordCount": null,
          "title": "Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>",
          "description": "Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.",
          "link": "http://arxiv.org/abs/2104.04184",
          "publishedOn": "2021-07-20T02:04:47.190Z",
          "wordCount": null,
          "title": "Robust Training of Social Media Image Classification Models for Rapid Disaster Response. (arXiv:2104.04184v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper concentrates on the approximation power of deep feed-forward\nneural networks in terms of width and depth. It is proved by construction that\nReLU networks with width $\\mathcal{O}\\big(\\max\\{d\\lfloor N^{1/d}\\rfloor,\\,\nN+2\\}\\big)$ and depth $\\mathcal{O}(L)$ can approximate a H\\\"older continuous\nfunction on $[0,1]^d$ with an approximation rate\n$\\mathcal{O}\\big(\\lambda\\sqrt{d} (N^2L^2\\ln N)^{-\\alpha/d}\\big)$, where\n$\\alpha\\in (0,1]$ and $\\lambda>0$ are H\\\"older order and constant,\nrespectively. Such a rate is optimal up to a constant in terms of width and\ndepth separately, while existing results are only nearly optimal without the\nlogarithmic factor in the approximation rate. More generally, for an arbitrary\ncontinuous function $f$ on $[0,1]^d$, the approximation rate becomes\n$\\mathcal{O}\\big(\\,\\sqrt{d}\\,\\omega_f\\big( (N^2L^2\\ln N)^{-1/d}\\big)\\,\\big)$,\nwhere $\\omega_f(\\cdot)$ is the modulus of continuity. We also extend our\nanalysis to any continuous function $f$ on a bounded set. Particularly, if ReLU\nnetworks with depth $31$ and width $\\mathcal{O}(N)$ are used to approximate\none-dimensional Lipschitz continuous functions on $[0,1]$ with a Lipschitz\nconstant $\\lambda>0$, the approximation rate in terms of the total number of\nparameters, $W=\\mathcal{O}(N^2)$, becomes $\\mathcal{O}(\\tfrac{\\lambda}{W\\ln\nW})$, which has not been discovered in the literature for fixed-depth ReLU\nnetworks.",
          "link": "http://arxiv.org/abs/2103.00502",
          "publishedOn": "2021-07-20T02:04:47.189Z",
          "wordCount": null,
          "title": "Optimal Approximation Rate of ReLU Networks in terms of Width and Depth. (arXiv:2103.00502v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12534",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Joonho Kim</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Oz_Y/0/1/0/all/0/1\">Yaron Oz</a>",
          "description": "We consider information spreading measures in randomly initialized\nvariational quantum circuits and introduce entanglement diagnostics for\nefficient variational quantum/classical computations. We establish a robust\nconnection between entanglement measures and optimization accuracy by solving\ntwo eigensolver problems for Ising Hamiltonians with nearest-neighbor and\nlong-range spin interactions. As the circuit depth affects the average\nentanglement of random circuit states, the entanglement diagnostics can\nidentify a high-performing depth range for optimization tasks encoded in local\nHamiltonians. We argue, based on an eigensolver problem for the\nSachdev-Ye-Kitaev model, that entanglement alone is insufficient as a\ndiagnostic to the approximation of volume-law entangled target states and that\na large number of circuit parameters is needed for such an optimization task.",
          "link": "http://arxiv.org/abs/2102.12534",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Entanglement Diagnostics for Efficient Quantum Computation. (arXiv:2102.12534v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.",
          "link": "http://arxiv.org/abs/2103.02503",
          "publishedOn": "2021-07-20T02:04:47.181Z",
          "wordCount": null,
          "title": "Domain Generalization in Vision: A Survey. (arXiv:2103.02503v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.05461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1\">Rodrigo Fernandes de Mello</a>",
          "description": "The Statistical Learning Theory (SLT) provides the foundation to ensure that\na supervised algorithm generalizes the mapping $f: \\mathcal{X} \\to \\mathcal{Y}$\ngiven $f$ is selected from its search space bias $\\mathcal{F}$. SLT depends on\nthe Shattering coefficient function $\\mathcal{N}(\\mathcal{F},n)$ to upper bound\nthe empirical risk minimization principle, from which one can estimate the\nnecessary training sample size to ensure the probabilistic learning convergence\nand, most importantly, the characterization of the capacity of $\\mathcal{F}$,\nincluding its underfitting and overfitting abilities while addressing specific\ntarget problems. However, the analytical solution of the Shattering coefficient\nis still an open problem since the first studies by Vapnik and Chervonenkis in\n$1962$, which we address on specific datasets, in this paper, by employing\nequivalence relations from Topology, data separability results by Har-Peled and\nJones, and combinatorics. Our approach computes the Shattering coefficient for\nboth binary and multi-class datasets, leading to the following additional\ncontributions: (i) the estimation of the required number of hyperplanes in the\nworst and best-case classification scenarios and the respective $\\Omega$ and\n$O$ complexities; (ii) the estimation of the training sample sizes required to\nensure supervised learning; and (iii) the comparison of dataset embeddings,\nonce they (re)organize samples into some new space configuration. All results\nintroduced and discussed along this paper are supported by the R package\nshattering (https://cran.r-project.org/web/packages/shattering).",
          "link": "http://arxiv.org/abs/1911.05461",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "On the Complexity of Labeled Datasets. (arXiv:1911.05461v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>",
          "description": "Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.",
          "link": "http://arxiv.org/abs/2010.15728",
          "publishedOn": "2021-07-20T02:04:47.162Z",
          "wordCount": null,
          "title": "Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation. (arXiv:2010.15728v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wadia_N/0/1/0/all/0/1\">Neha S. Wadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1\">Daniel Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Samuel S. Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>",
          "description": "Machine learning is predicated on the concept of generalization: a model\nachieving low error on a sufficiently large training set should also perform\nwell on novel samples from the same distribution. We show that both data\nwhitening and second order optimization can harm or entirely prevent\ngeneralization. In general, model training harnesses information contained in\nthe sample-sample second moment matrix of a dataset. For a general class of\nmodels, namely models with a fully connected first layer, we prove that the\ninformation contained in this matrix is the only information which can be used\nto generalize. Models trained using whitened data, or with certain second order\noptimization schemes, have less access to this information, resulting in\nreduced or nonexistent generalization ability. We experimentally verify these\npredictions for several architectures, and further demonstrate that\ngeneralization continues to be harmed even when theoretical requirements are\nrelaxed. However, we also show experimentally that regularized second order\noptimization can provide a practical tradeoff, where training is accelerated\nbut less information is lost, and generalization can in some circumstances even\nimprove.",
          "link": "http://arxiv.org/abs/2008.07545",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization. (arXiv:2008.07545v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1\">Olivier Sigaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building autonomous machines that can explore open-ended environments,\ndiscover possible interactions and autonomously build repertoires of skills is\na general objective of artificial intelligence. Developmental approaches argue\nthat this can only be achieved by autonomous and intrinsically motivated\nlearning agents that can generate, select and learn to solve their own\nproblems. In recent years, we have seen a convergence of developmental\napproaches, and developmental robotics in particular, with deep reinforcement\nlearning (RL) methods, forming the new domain of developmental machine\nlearning. Within this new domain, we review here a set of methods where deep RL\nalgorithms are trained to tackle the developmental robotics problem of the\nautonomous acquisition of open-ended repertoires of skills. Intrinsically\nmotivated goal-conditioned RL algorithms train agents to learn to represent,\ngenerate and pursue their own goals. The self-generation of goals requires the\nlearning of compact goal encodings as well as their associated goal-achievement\nfunctions, which results in new challenges compared to traditional RL\nalgorithms designed to tackle pre-defined sets of goals using external reward\nsignals. This paper proposes a typology of these methods at the intersection of\ndeep RL and developmental approaches, surveys recent approaches and discusses\nfuture avenues.",
          "link": "http://arxiv.org/abs/2012.09830",
          "publishedOn": "2021-07-20T02:04:47.161Z",
          "wordCount": null,
          "title": "Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey. (arXiv:2012.09830v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1\">Junior Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1\">Eftychios Sifakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>",
          "description": "We present a differentiable soft-body physics simulator that can be composed\nwith neural networks as a differentiable layer. In contrast to other\ndifferentiable physics approaches that use explicit forward models to define\nstate transitions, we focus on implicit state transitions defined via function\nminimization. Implicit state transitions appear in implicit numerical\nintegration methods, which offer the benefits of large time steps and excellent\nnumerical stability, but require a special treatment to achieve\ndifferentiability due to the absence of an explicit differentiable forward\npass. In contrast to other implicit differentiation approaches that require\nexplicit formulas for the force function and the force Jacobian matrix, we\npresent an energy-based approach that allows us to compute these derivatives\nautomatically and in a matrix-free fashion via reverse-mode automatic\ndifferentiation. This allows for more flexibility and productivity when\ndefining physical models and is particularly important in the context of neural\nnetwork training, which often relies on reverse-mode automatic differentiation\n(backpropagation). We demonstrate the effectiveness of our differentiable\nsimulator in policy optimization for locomotion tasks and show that it achieves\nbetter sample efficiency than model-free reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.05791",
          "publishedOn": "2021-07-20T02:04:47.160Z",
          "wordCount": null,
          "title": "Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08925",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Martin_C/0/1/0/all/0/1\">Christoph Martin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sharafi_N/0/1/0/all/0/1\">Nahal Sharafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hallerberg_S/0/1/0/all/0/1\">Sarah Hallerberg</a>",
          "description": "Covariant Lyapunov vectors (CLVs) characterize the directions along which\nperturbations in dynamical systems grow. They have also been studied as\npotential predictors of critical transitions and extreme events. For many\napplications, it is, however, necessary to estimate the vectors from data since\nmodel equations are unknown for many interesting phenomena. We propose a novel\nmethod for estimating CLVs based on data records without knowing the underlying\nequations of the system which is suitable also for high-dimensional data and\ncomputationally inexpensive. We demonstrate that this purely data-driven\napproach can accurately estimate CLVs from data records generated by chaotic\ndynamical systems of dimension 128 and multiple lower-dimensional systems and\nthus provides the foundation for numerous future applications in data-analysis\nand data-based predictions.",
          "link": "http://arxiv.org/abs/2107.08925",
          "publishedOn": "2021-07-20T02:04:47.152Z",
          "wordCount": null,
          "title": "Estimating covariant Lyapunov vectors from data. (arXiv:2107.08925v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyunghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Suk-Ju Kang</a>",
          "description": "Conditional generative adversarial networks (cGANs) have demonstrated\nremarkable success due to their class-wise controllability and superior quality\nfor complex generation tasks. Typical cGANs solve the joint distribution\nmatching problem by decomposing two easier sub-problems: marginal matching and\nconditional matching. From our toy experiments, we found that it is the best to\napply only conditional matching to certain samples due to the content-aware\noptimization of the discriminator. This paper proposes a simple (a few lines of\ncode) but effective training methodology, selective focusing learning, which\nenforces the discriminator and generator to learn easy samples of each class\nrapidly while maintaining diversity. Our key idea is to selectively apply\nconditional and joint matching for the data in each mini-batch. We conducted\nexperiments on recent cGAN variants in ImageNet (64x64 and 128x128), CIFAR-10,\nand CIFAR-100 datasets, and improved the performance significantly (up to\n35.18% in terms of FID) without sacrificing diversity.",
          "link": "http://arxiv.org/abs/2107.08792",
          "publishedOn": "2021-07-20T02:04:47.039Z",
          "wordCount": null,
          "title": "Selective Focusing Learning in Conditional GANs. (arXiv:2107.08792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miura_T/0/1/0/all/0/1\">Takayuki Miura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1\">Satoshi Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibahara_T/0/1/0/all/0/1\">Toshiki Shibahara</a>",
          "description": "The advance of explainable artificial intelligence, which provides reasons\nfor its predictions, is expected to accelerate the use of deep neural networks\nin the real world like Machine Learning as a Service (MLaaS) that returns\npredictions on queried data with the trained model. Deep neural networks\ndeployed in MLaaS face the threat of model extraction attacks. A model\nextraction attack is an attack to violate intellectual property and privacy in\nwhich an adversary steals trained models in a cloud using only their\npredictions. In particular, a data-free model extraction attack has been\nproposed recently and is more critical. In this attack, an adversary uses a\ngenerative model instead of preparing input data. The feasibility of this\nattack, however, needs to be studied since it requires more queries than that\nwith surrogate datasets. In this paper, we propose MEGEX, a data-free model\nextraction attack against a gradient-based explainable AI. In this method, an\nadversary uses the explanations to train the generative model and reduces the\nnumber of queries to steal the model. Our experiments show that our proposed\nmethod reconstructs high-accuracy models -- 0.97$\\times$ and 0.98$\\times$ the\nvictim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,\nrespectively. This implies that there is a trade-off between the\ninterpretability of models and the difficulty of stealing them.",
          "link": "http://arxiv.org/abs/2107.08909",
          "publishedOn": "2021-07-20T02:04:47.019Z",
          "wordCount": null,
          "title": "MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI. (arXiv:2107.08909v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1\">Matko Bo&#x161;njak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1\">Alexander Lerchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>",
          "description": "Neural networks leverage robust internal representations in order to\ngeneralise. Learning them is difficult, and often requires a large training set\nthat covers the data distribution densely. We study a common setting where our\ntask is not purely opaque. Indeed, very often we may have access to information\nabout the underlying system (e.g. that observations must obey certain laws of\nphysics) that any \"tabula rasa\" neural network would need to re-learn from\nscratch, penalising data efficiency. We incorporate this information into a\npre-trained reasoning module, and investigate its role in shaping the\ndiscovered representations in diverse self-supervised learning settings from\npixels. Our approach paves the way for a new class of data-efficient\nrepresentation learning.",
          "link": "http://arxiv.org/abs/2107.08881",
          "publishedOn": "2021-07-20T02:04:47.018Z",
          "wordCount": null,
          "title": "Reasoning-Modulated Representations. (arXiv:2107.08881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eiben_A/0/1/0/all/0/1\">Agoston E. Eiben</a>",
          "description": "When controllers (brains) and morphologies (bodies) of robots simultaneously\nevolve, this can lead to a problem, namely the brain & body mismatch problem.\nIn this research, we propose a solution of lifetime learning. We set up a\nsystem where modular robots can create offspring that inherit the bodies of\nparents by recombination and mutation. With regards to the brains of the\noffspring, we use two methods to create them. The first one entails solely\nevolution which means the brain of a robot child is inherited from its parents.\nThe second approach is evolution plus learning which means the brain of a child\nis inherited as well, but additionally is developed by a learning algorithm -\nRevDEknn. We compare these two methods by running experiments in a simulator\ncalled Revolve and use efficiency, efficacy, and the morphology intelligence of\nthe robots for the comparison. The experiments show that the evolution plus\nlearning method does not only lead to a higher fitness level, but also to more\nmorphologically evolving robots. This constitutes a quantitative demonstration\nthat changes in the brain can induce changes in the body, leading to the\nconcept of morphological intelligence, which is quantified by the learning\ndelta, meaning the ability of a morphology to facilitate learning.",
          "link": "http://arxiv.org/abs/2107.08249",
          "publishedOn": "2021-07-20T02:04:46.996Z",
          "wordCount": null,
          "title": "The Effects of Learning in Morphologically Evolving Robot Systems. (arXiv:2107.08249v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>",
          "description": "Although deep learning has demonstrated astonishing performance in many\napplications, there are still concerns on their dependability. One desirable\nproperty of deep learning applications with societal impact is fairness (i.e.,\nnon-discrimination). Unfortunately, discrimination might be intrinsically\nembedded into the models due to discrimination in the training data. As a\ncountermeasure, fairness testing systemically identifies discriminative\nsamples, which can be used to retrain the model and improve its fairness.\nExisting fairness testing approaches however have two major limitations. First,\nthey only work well on traditional machine learning models and have poor\nperformance (e.g., effectiveness and efficiency) on deep learning models.\nSecond, they only work on simple tabular data and are not applicable for\ndomains such as text. In this work, we bridge the gap by proposing a scalable\nand effective approach for systematically searching for discriminative samples\nwhile extending fairness testing to address a challenging domain, i.e., text\nclassification. Compared with state-of-the-art methods, our approach only\nemploys lightweight procedures like gradient computation and clustering, which\nmakes it significantly more scalable. Experimental results show that on\naverage, our approach explores the search space more effectively (9.62 and 2.38\ntimes more than the state-of-art methods respectively on tabular and text\ndatasets) and generates much more individual discriminatory instances (24.95\nand 2.68 times) within reasonable time. The retrained models reduce\ndiscrimination by 57.2% and 60.2% respectively on average.",
          "link": "http://arxiv.org/abs/2107.08176",
          "publishedOn": "2021-07-20T02:04:46.989Z",
          "wordCount": null,
          "title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling. (arXiv:2107.08176v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.",
          "link": "http://arxiv.org/abs/2107.08221",
          "publishedOn": "2021-07-20T02:04:46.986Z",
          "wordCount": null,
          "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08225",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Muehlebach_M/0/1/0/all/0/1\">Michael Muehlebach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We introduce a class of first-order methods for smooth constrained\noptimization that are based on an analogy to non-smooth dynamical systems. Two\ndistinctive features of our approach are that (i) projections or optimizations\nover the entire feasible set are avoided, in stark contrast to projected\ngradient methods or the Frank-Wolfe method, and (ii) iterates are allowed to\nbecome infeasible, which differs from active set or feasible direction methods,\nwhere the descent motion stops as soon as a new constraint is encountered. The\nresulting algorithmic procedure is simple to implement even when constraints\nare nonlinear, and is suitable for large-scale constrained optimization\nproblems in which the feasible set fails to have a simple structure. The key\nunderlying idea is that constraints are expressed in terms of velocities\ninstead of positions, which has the algorithmic consequence that optimizations\nover feasible sets at each iteration are replaced with optimizations over\nlocal, sparse convex approximations. The result is a simplified suite of\nalgorithms and an expanded range of possible applications in machine learning.",
          "link": "http://arxiv.org/abs/2107.08225",
          "publishedOn": "2021-07-20T02:04:46.985Z",
          "wordCount": null,
          "title": "On Constraints in First-Order Optimization: A View from Non-Smooth Dynamical Systems. (arXiv:2107.08225v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Geun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Carole-Jean Wu</a>",
          "description": "Federated learning enables a cluster of decentralized mobile devices at the\nedge to collaboratively train a shared machine learning model, while keeping\nall the raw training samples on device. This decentralized training approach is\ndemonstrated as a practical solution to mitigate the risk of privacy leakage.\nHowever, enabling efficient FL deployment at the edge is challenging because of\nnon-IID training data distribution, wide system heterogeneity and\nstochastic-varying runtime effects in the field. This paper jointly optimizes\ntime-to-convergence and energy efficiency of state-of-the-art FL use cases by\ntaking into account the stochastic nature of edge execution. We propose AutoFL\nby tailor-designing a reinforcement learning algorithm that learns and\ndetermines which K participant devices and per-device execution targets for\neach FL model aggregation round in the presence of stochastic runtime variance,\nsystem and data heterogeneity. By considering the unique characteristics of FL\nedge deployment judiciously, AutoFL achieves 3.6 times faster model convergence\ntime and 4.7 and 5.2 times higher energy efficiency for local clients and\nglobally over the cluster of K participants, respectively.",
          "link": "http://arxiv.org/abs/2107.08147",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. (arXiv:2107.08147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avati_A/0/1/0/all/0/1\">Anand Avati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_M/0/1/0/all/0/1\">Martin Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_E/0/1/0/all/0/1\">Emily Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>",
          "description": "Machine learning has recently demonstrated impressive progress in predictive\naccuracy across a wide array of tasks. Most ML approaches focus on\ngeneralization performance on unseen data that are similar to the training data\n(In-Distribution, or IND). However, real world applications and deployments of\nML rarely enjoy the comfort of encountering examples that are always IND. In\nsuch situations, most ML models commonly display erratic behavior on\nOut-of-Distribution (OOD) examples, such as assigning high confidence to wrong\npredictions, or vice-versa. Implications of such unusual model behavior are\nfurther exacerbated in the healthcare setting, where patient health can\npotentially be put at risk. It is crucial to study the behavior and robustness\nproperties of models under distributional shift, understand common failure\nmodes, and take mitigation steps before the model is deployed. Having a\nbenchmark that shines light upon these aspects of a model is a first and\nnecessary step in addressing the issue. Recent work and interest in increasing\nmodel robustness in OOD settings have focused more on image modality, while the\nElectronic Health Record (EHR) modality is still largely under-explored. We aim\nto bridge this gap by releasing BEDS-Bench, a benchmark for quantifying the\nbehavior of ML models over EHR data under OOD settings. We use two open access,\nde-identified EHR datasets to construct several OOD data settings to run tests\non, and measure relevant metrics that characterize crucial aspects of a model's\nOOD behavior. We evaluate several learning algorithms under BEDS-Bench and find\nthat all of them show poor generalization performance under distributional\nshift in general. Our results highlight the need and the potential to improve\nrobustness of EHR models under distributional shift, and BEDS-Bench provides\none way to measure progress towards that goal.",
          "link": "http://arxiv.org/abs/2107.08189",
          "publishedOn": "2021-07-20T02:04:46.984Z",
          "wordCount": null,
          "title": "BEDS-Bench: Behavior of EHR-models under Distributional Shift--A Benchmark. (arXiv:2107.08189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korb_K/0/1/0/all/0/1\">Kevin B Korb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allison_L/0/1/0/all/0/1\">Lloyd Allison</a>",
          "description": "Causal discovery automates the learning of causal Bayesian networks from data\nand has been of active interest from their beginning. With the sourcing of\nlarge data sets off the internet, interest in scaling up to very large data\nsets has grown. One approach to this is to parallelize search using Markov\nBlanket (MB) discovery as a first step, followed by a process of combining MBs\nin a global causal model. We develop and explore three new methods of MB\ndiscovery using Minimum Message Length (MML) and compare them empirically to\nthe best existing methods, whether developed specifically as MB discovery or as\nfeature selection. Our best MML method is consistently competitive and has some\nadvantageous features.",
          "link": "http://arxiv.org/abs/2107.08140",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Markov Blanket Discovery using Minimum Message Length. (arXiv:2107.08140v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petrenko_A/0/1/0/all/0/1\">Aleksei Petrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shacklett_B/0/1/0/all/0/1\">Brennan Shacklett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "We present Megaverse, a new 3D simulation platform for reinforcement learning\nand embodied AI research. The efficient design of our engine enables\nphysics-based simulation with high-dimensional egocentric observations at more\nthan 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to\n70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive\nobjects. We achieve this high simulation performance by leveraging batched\nsimulation, thereby taking full advantage of the massive parallelism of modern\nGPUs. We use Megaverse to build a new benchmark that consists of several\nsingle-agent and multi-agent tasks covering a variety of cognitive challenges.\nWe evaluate model-free RL on this benchmark to provide baselines and facilitate\nfuture research. The source code is available at https://www.megaverse.info",
          "link": "http://arxiv.org/abs/2107.08170",
          "publishedOn": "2021-07-20T02:04:46.983Z",
          "wordCount": null,
          "title": "Megaverse: Simulating Embodied Agents at One Million Experiences per Second. (arXiv:2107.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molino_P/0/1/0/all/0/1\">Piero Molino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "In the last years machine learning (ML) has moved from a academic endeavor to\na pervasive technology adopted in almost every aspect of computing. ML-powered\nproducts are now embedded in our digital lives: from recommendations of what to\nwatch, to divining our search intent, to powering virtual assistants in\nconsumer and enterprise settings. Recent successes in applying ML in natural\nsciences revealed that ML can be used to tackle some of the hardest real-world\nproblems humanity faces today. For these reasons ML has become central in the\nstrategy of tech companies and has gathered even more attention from academia\nthan ever before. Despite these successes, what we have witnessed so far is\njust the beginning. Right now the people training and using ML models are\nexpert developers working within large organizations, but we believe the next\nwave of ML systems will allow a larger amount of people, potentially without\ncoding skills, to perform the same tasks. These new ML systems will not require\nusers to fully understand all the details of how models are trained and\nutilized for obtaining predictions. Declarative interfaces are well suited for\nthis goal, by hiding complexity and favouring separation of interests, and can\nlead to increased productivity. We worked on such abstract interfaces by\ndeveloping two declarative ML systems, Overton and Ludwig, that require users\nto declare only their data schema (names and types of inputs) and tasks rather\nthen writing low level ML code. In this article we will describe how ML systems\nare currently structured, highlight important factors for their success and\nadoption, what are the issues current ML systems are facing and how the systems\nwe developed addressed them. Finally we will talk about learnings from the\ndevelopment of ML systems throughout the years and how we believe the next\ngeneration of ML systems will look like.",
          "link": "http://arxiv.org/abs/2107.08148",
          "publishedOn": "2021-07-20T02:04:46.981Z",
          "wordCount": null,
          "title": "Declarative Machine Learning Systems. (arXiv:2107.08148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "We study privacy in a distributed learning framework, where clients\ncollaboratively build a learning model iteratively through interactions with a\nserver from whom we need privacy. Motivated by stochastic optimization and the\nfederated learning (FL) paradigm, we focus on the case where a small fraction\nof data samples are randomly sub-sampled in each round to participate in the\nlearning process, which also enables privacy amplification. To obtain even\nstronger local privacy guarantees, we study this in the shuffle privacy model,\nwhere each client randomizes its response using a local differentially private\n(LDP) mechanism and the server only receives a random permutation (shuffle) of\nthe clients' responses without their association to each client. The principal\nresult of this paper is a privacy-optimization performance trade-off for\ndiscrete randomization mechanisms in this sub-sampled shuffle privacy model.\nThis is enabled through a new theoretical technique to analyze the Renyi\nDifferential Privacy (RDP) of the sub-sampled shuffle model. We numerically\ndemonstrate that, for important regimes, with composition our bound yields\nsignificant improvement in privacy guarantee over the state-of-the-art\napproximate Differential Privacy (DP) guarantee (with strong composition) for\nsub-sampled shuffled models. We also demonstrate numerically significant\nimprovement in privacy-learning performance operating point using real data\nsets.",
          "link": "http://arxiv.org/abs/2107.08763",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Renyi Differential Privacy of the Subsampled Shuffle Model in Distributed Learning. (arXiv:2107.08763v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_A/0/1/0/all/0/1\">Athresh Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1\">Sriraam Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "While probabilistic models are an important tool for studying causality,\ndoing so suffers from the intractability of inference. As a step towards\ntractable causal models, we consider the problem of learning interventional\ndistributions using sum-product networks (SPNs) that are over-parameterized by\ngate functions, e.g., neural networks. Providing an arbitrarily intervened\ncausal graph as input, effectively subsuming Pearl's do-operator, the gate\nfunction predicts the parameters of the SPN. The resulting interventional SPNs\nare motivated and illustrated by a structural causal model themed around\npersonal health. Our empirical evaluation on three benchmark data sets as well\nas a synthetic health data set clearly demonstrates that interventional SPNs\nindeed are both expressive in modelling and flexible in adapting to the\ninterventions.",
          "link": "http://arxiv.org/abs/2102.10440",
          "publishedOn": "2021-07-20T02:04:46.980Z",
          "wordCount": null,
          "title": "Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models. (arXiv:2102.10440v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Iker Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalski_P/0/1/0/all/0/1\">Piotr Skalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barns_Graham_A/0/1/0/all/0/1\">Alec Barns-Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jason Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_D/0/1/0/all/0/1\">David Sutton</a>",
          "description": "Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.",
          "link": "http://arxiv.org/abs/2107.08756",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "Path Integrals for the Attribution of Model Uncertainties. (arXiv:2107.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1\">Katherine Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.",
          "link": "http://arxiv.org/abs/2011.05601",
          "publishedOn": "2021-07-20T02:04:46.979Z",
          "wordCount": null,
          "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>",
          "description": "Treatment effect estimation, which refers to the estimation of causal effects\nand aims to measure the strength of the causal relationship, is of great\nimportance in many fields but is a challenging problem in practice. As present,\ndata-driven causal effect estimation faces two main challenges, i.e., selection\nbias and the missing of counterfactual. To address these two issues, most of\nthe existing approaches tend to reduce the selection bias by learning a\nbalanced representation, and then to estimate the counterfactual through the\nrepresentation. However, they heavily rely on the finely hand-crafted metric\nfunctions when learning balanced representations, which generally doesn't work\nwell for the situations where the original distribution is complicated. In this\npaper, we propose a CETransformer model for casual effect estimation via\ntransformer based representation learning. To learn the representation of\ncovariates(features) robustly, a self-supervised transformer is proposed, by\nwhich the correlation between covariates can be well exploited through\nself-attention mechanism. In addition, an adversarial network is adopted to\nbalance the distribution of the treated and control groups in the\nrepresentation space. Experimental results on three real-world datasets\ndemonstrate the advantages of the proposed CETransformer, compared with the\nstate-of-the-art treatment effect estimation methods.",
          "link": "http://arxiv.org/abs/2107.08714",
          "publishedOn": "2021-07-20T02:04:46.978Z",
          "wordCount": null,
          "title": "CETransformer: Casual Effect Estimation via Transformer Based Representation Learning. (arXiv:2107.08714v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xueting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenhuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>",
          "description": "Graph neural networks (GNNs) is widely used to learn a powerful\nrepresentation of graph-structured data. Recent work demonstrates that\ntransferring knowledge from self-supervised tasks to downstream tasks could\nfurther improve graph representation. However, there is an inherent gap between\nself-supervised tasks and downstream tasks in terms of optimization objective\nand training data. Conventional pre-training methods may be not effective\nenough on knowledge transfer since they do not make any adaptation for\ndownstream tasks. To solve such problems, we propose a new transfer learning\nparadigm on GNNs which could effectively leverage self-supervised tasks as\nauxiliary tasks to help the target task. Our methods would adaptively select\nand combine different auxiliary tasks with the target task in the fine-tuning\nstage. We design an adaptive auxiliary loss weighting model to learn the\nweights of auxiliary tasks by quantifying the consistency between auxiliary\ntasks and the target task. In addition, we learn the weighting model through\nmeta-learning. Our methods can be applied to various transfer learning\napproaches, it performs well not only in multi-task learning but also in\npre-training and fine-tuning. Comprehensive experiments on multiple downstream\ntasks demonstrate that the proposed methods can effectively combine auxiliary\ntasks with the target task and significantly improve the performance compared\nto state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08765",
          "publishedOn": "2021-07-20T02:04:46.977Z",
          "wordCount": null,
          "title": "Adaptive Transfer Learning on Graph Neural Networks. (arXiv:2107.08765v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08721",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>",
          "description": "News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.",
          "link": "http://arxiv.org/abs/2107.08721",
          "publishedOn": "2021-07-20T02:04:46.976Z",
          "wordCount": null,
          "title": "Stock Movement Prediction with Financial News using Contextualized Embedding from BERT. (arXiv:2107.08721v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuangjia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Ying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>",
          "description": "Constructing appropriate representations of molecules lies at the core of\nnumerous tasks such as material science, chemistry and drug designs. Recent\nresearches abstract molecules as attributed graphs and employ graph neural\nnetworks (GNN) for molecular representation learning, which have made\nremarkable achievements in molecular graph modeling. Albeit powerful, current\nmodels either are based on local aggregation operations and thus miss\nhigher-order graph properties or focus on only node information without fully\nusing the edge information. For this sake, we propose a Communicative Message\nPassing Transformer (CoMPT) neural network to improve the molecular graph\nrepresentation by reinforcing message interactions between nodes and edges\nbased on the Transformer architecture. Unlike the previous transformer-style\nGNNs that treat molecules as fully connected graphs, we introduce a message\ndiffusion mechanism to leverage the graph connectivity inductive bias and\nreduce the message enrichment explosion. Extensive experiments demonstrated\nthat the proposed model obtained superior performances (around 4$\\%$ on\naverage) against state-of-the-art baselines on seven chemical property datasets\n(graph-level tasks) and two chemical shift datasets (node-level tasks). Further\nvisualization studies also indicated a better representation capacity achieved\nby our model.",
          "link": "http://arxiv.org/abs/2107.08773",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "Learning Attributed Graph Representations with Communicative Message Passing Transformer. (arXiv:2107.08773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shehabi_S/0/1/0/all/0/1\">Shadi Al Shehabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baba_A/0/1/0/all/0/1\">Abdullatif Baba</a>",
          "description": "Association rules are useful to discover relationships, which are mostly\nhidden, between the different items in large datasets. Symbolic models are the\nprincipal tools to extract association rules. This basic technique is\ntime-consuming, and it generates a big number of associated rules. To overcome\nthis drawback, we suggest a new method, called MARC, to extract the more\nimportant association rules of two important levels: Type I, and Type II. This\napproach relies on a multi-topographic unsupervised neural network model as\nwell as clustering quality measures that evaluate the success of a given\nnumerical classification model to behave as a natural symbolic model.",
          "link": "http://arxiv.org/abs/2107.08814",
          "publishedOn": "2021-07-20T02:04:46.975Z",
          "wordCount": null,
          "title": "MARC: Mining Association Rules from datasets by using Clustering models. (arXiv:2107.08814v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1\">Ha Young Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Recently, semiconductors' demand has exploded in virtual reality,\nsmartphones, wearable devices, the internet of things, robotics, and\nautomobiles. Semiconductor manufacturers want to make semiconductors with high\nyields. To do this, manufacturers conduct many quality assurance activities.\nWafer map pattern classification is a typical way of quality assurance. The\ndefect pattern on the wafer map can tell us which process has a problem. Most\nof the existing wafer map classification methods are based on supervised\nmethods. The supervised methods tend to have high performance, but they require\nextensive labor and expert knowledge to produce labeled datasets with a\nbalanced distribution in mind. In the semiconductor manufacturing process, it\nis challenging to get defect data with balanced distribution. In this paper, we\npropose a one-class classification method using an Adversarial Autoencoder\n(AAE) with Deep Support Vector Data Description (DSVDD) prior, which generates\nrandom vectors within the hypersphere of DSVDD. We use the WM-811k dataset,\nwhich consists of a real-world wafer map. We compare the F1 score performance\nof our model with DSVDD and AAE.",
          "link": "http://arxiv.org/abs/2107.08823",
          "publishedOn": "2021-07-20T02:04:46.974Z",
          "wordCount": null,
          "title": "One-Class Classification for Wafer Map using Adversarial Autoencoder with DSVDD Prior. (arXiv:2107.08823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Reward function specification, which requires considerable human effort and\niteration, remains a major impediment for learning behaviors through deep\nreinforcement learning. In contrast, providing visual demonstrations of desired\nbehaviors often presents an easier and more natural way to teach agents. We\nconsider a setting where an agent is provided a fixed dataset of visual\ndemonstrations illustrating how to perform a task, and must learn to solve the\ntask using the provided demonstrations and unsupervised environment\ninteractions. This setting presents a number of challenges including\nrepresentation learning for visual observations, sample complexity due to high\ndimensional spaces, and learning instability due to the lack of a fixed reward\nor learning signal. Towards addressing these challenges, we develop a\nvariational model-based adversarial imitation learning (V-MAIL) algorithm. The\nmodel-based approach provides a strong signal for representation learning,\nenables sample efficiency, and improves the stability of adversarial training\nby enabling on-policy learning. Through experiments involving several\nvision-based locomotion and manipulation tasks, we find that V-MAIL learns\nsuccessful visuomotor policies in a sample-efficient manner, has better\nstability compared to prior work, and also achieves higher asymptotic\nperformance. We further find that by transferring the learned models, V-MAIL\ncan learn new tasks from visual demonstrations without any additional\nenvironment interactions. All results including videos can be found online at\n\\url{https://sites.google.com/view/variational-mail}.",
          "link": "http://arxiv.org/abs/2107.08829",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "Visual Adversarial Imitation Learning using Variational Models. (arXiv:2107.08829v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaolong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanegas_F/0/1/0/all/0/1\">Fernando Vanegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Felipe Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1\">Conrad Sanderson</a>",
          "description": "The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue\nas well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,\nhave limited endurance. The range of UAV applications can be widened if teams\nof multiple UAVs are used. We propose a framework for a team of UAVs to\ncooperatively explore and find a target in complex GPS-denied environments with\nobstacles. The team of UAVs autonomously navigates, explores, detects, and\nfinds the target in a cluttered environment with a known map. Examples of such\nenvironments include indoor scenarios, urban or natural canyons, caves, and\ntunnels, where the GPS signal is limited or blocked. The framework is based on\na probabilistic decentralised Partially Observable Markov Decision Process\nwhich accounts for the uncertainties in sensing and the environment. The team\ncan cooperate efficiently, with each UAV sharing only limited processed\nobservations and their locations during the mission. The system is simulated\nusing the Robotic Operating System and Gazebo. Performance of the system with\nan increasing number of UAVs in several indoor scenarios with obstacles is\ntested. Results indicate that the proposed multi-UAV system has improvements in\nterms of time-cost, the proportion of search area surveyed, as well as\nsuccessful rates for search and rescue missions.",
          "link": "http://arxiv.org/abs/2107.08834",
          "publishedOn": "2021-07-20T02:04:46.973Z",
          "wordCount": null,
          "title": "A Multi-UAV System for Exploration and Target Finding in Cluttered and GPS-Denied Environments. (arXiv:2107.08834v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1\">Takeshi D. Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubo_T/0/1/0/all/0/1\">Takatomi Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_K/0/1/0/all/0/1\">Kazushi Ikeda</a>",
          "description": "Graph neural networks (GNNs) have been widely used to learn vector\nrepresentation of graph-structured data and achieved better task performance\nthan conventional methods. The foundation of GNNs is the message passing\nprocedure, which propagates the information in a node to its neighbors. Since\nthis procedure proceeds one step per layer, the range of the information\npropagation among nodes is small in the lower layers, and it expands toward the\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\nstructural information in a graph. On the other hand, it is known that deep GNN\nmodels suffer from performance degradation because they lose nodes' local\ninformation, which would be essential for good model performance, through many\nmessage passing steps. In this study, we propose a multi-level attention\npooling (MLAP) for graph-level classification tasks, which can adapt to both\nlocal and global structural information in a graph. It has an attention pooling\nlayer for each message passing step and computes the final graph representation\nby unifying the layer-wise graph representations. The MLAP architecture allows\nmodels to utilize the structural information of graphs with multiple levels of\nlocalities because it preserves layer-wise information before losing them due\nto oversmoothing. Results of our experiments show that the MLAP architecture\nimproves deeper models' performance in graph classification tasks compared to\nthe baseline architectures. In addition, analyses on the layer-wise graph\nrepresentations suggest that aggregating information from multiple levels of\nlocalities indeed has the potential to improve the discriminability of learned\ngraph representations.",
          "link": "http://arxiv.org/abs/2103.01488",
          "publishedOn": "2021-07-20T02:04:46.972Z",
          "wordCount": null,
          "title": "Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities. (arXiv:2103.01488v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaoyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai L&#xfc;</a>",
          "description": "Generative Adversarial Networks (GAN) is an adversarial model, and it has\nbeen demonstrated to be effective for various generative tasks. However, GAN\nand its variants also suffer from many training problems, such as mode collapse\nand gradient vanish. In this paper, we firstly propose a general crossover\noperator, which can be widely applied to GANs using evolutionary strategies.\nThen we design an evolutionary GAN framework C-GAN based on it. And we combine\nthe crossover operator with evolutionary generative adversarial networks (EGAN)\nto implement the evolutionary generative adversarial networks with crossover\n(CE-GAN). Under the premise that a variety of loss functions are used as\nmutation operators to generate mutation individuals, we evaluate the generated\nsamples and allow the mutation individuals to learn experiences from the output\nin a knowledge distillation manner, imitating the best output outcome,\nresulting in better offspring. Then, we greedily selected the best offspring as\nparents for subsequent training using discriminator as evaluator. Experiments\non real datasets demonstrate the effectiveness of CE-GAN and show that our\nmethod is competitive in terms of generated images quality and time efficiency.",
          "link": "http://arxiv.org/abs/2101.11186",
          "publishedOn": "2021-07-20T02:04:46.971Z",
          "wordCount": null,
          "title": "Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation. (arXiv:2101.11186v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosters_W/0/1/0/all/0/1\">Walter Kosters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>",
          "description": "Deep reinforcement learning has shown remarkable success in the past few\nyears. Highly complex sequential decision making problems from game playing and\nrobotics have been solved with deep model-free methods. Unfortunately, the\nsample complexity of model-free methods is often high. To reduce the number of\nenvironment samples, model-based reinforcement learning creates an explicit\nmodel of the environment dynamics. Achieving high model accuracy is a challenge\nin high-dimensional problems. In recent years, a diverse landscape of\nmodel-based methods has been introduced to improve model accuracy, using\nmethods such as uncertainty modeling, model-predictive control, latent models,\nand end-to-end learning and planning. Some of these methods succeed in\nachieving high accuracy at low sample complexity, most do so either in a\nrobotics or in a games context. In this paper, we survey these methods; we\nexplain in detail how they work and what their strengths and weaknesses are. We\nconclude with a research agenda for future work to make the methods more robust\nand more widely applicable to other applications.",
          "link": "http://arxiv.org/abs/2107.08241",
          "publishedOn": "2021-07-20T02:04:46.880Z",
          "wordCount": null,
          "title": "High-Accuracy Model-Based Reinforcement Learning, a Survey. (arXiv:2107.08241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08265",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Srijith_P/0/1/0/all/0/1\">P. K. Srijith</a> (1), <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a> (2) ((1) Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India, (2) RIKEN Center for AI Project, Tokyo, Japan)",
          "description": "Deep Gaussian Processes (DGPs) are multi-layer, flexible extensions of\nGaussian processes but their training remains challenging. Sparse\napproximations simplify the training but often require optimization over a\nlarge number of inducing inputs and their locations across layers. In this\npaper, we simplify the training by setting the locations to a fixed subset of\ndata and sampling the inducing inputs from a variational distribution. This\nreduces the trainable parameters and computation cost without significant\nperformance degradations, as demonstrated by our empirical results on\nregression problems. Our modifications simplify and stabilize DGP training\nwhile making it amenable to sampling schemes for setting the inducing inputs.",
          "link": "http://arxiv.org/abs/2107.08265",
          "publishedOn": "2021-07-20T02:04:46.879Z",
          "wordCount": null,
          "title": "Subset-of-Data Variational Inference for Deep Gaussian-Processes Regression. (arXiv:2107.08265v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milli_S/0/1/0/all/0/1\">Smitha Milli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belli_L/0/1/0/all/0/1\">Luca Belli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>",
          "description": "Most recommendation engines today are based on predicting user engagement,\ne.g. predicting whether a user will click on an item or not. However, there is\npotentially a large gap between engagement signals and a desired notion of\n\"value\" that is worth optimizing for. We use the framework of measurement\ntheory to (a) confront the designer with a normative question about what the\ndesigner values, (b) provide a general latent variable model approach that can\nbe used to operationalize the target construct and directly optimize for it,\nand (c) guide the designer in evaluating and revising their operationalization.\nWe implement our approach on the Twitter platform on millions of users. In line\nwith established approaches to assessing the validity of measurements, we\nperform a qualitative evaluation of how well our model captures a desired\nnotion of \"value\".",
          "link": "http://arxiv.org/abs/2008.12623",
          "publishedOn": "2021-07-20T02:04:46.878Z",
          "wordCount": null,
          "title": "From Optimizing Engagement to Measuring Value. (arXiv:2008.12623v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1\">Hishan Parry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1\">Lei Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1\">Amin Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jia Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1\">Geoff V. Merrett</a>",
          "description": "The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.",
          "link": "http://arxiv.org/abs/2107.08199",
          "publishedOn": "2021-07-20T02:04:46.868Z",
          "wordCount": null,
          "title": "Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>",
          "description": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "link": "http://arxiv.org/abs/2006.10483",
          "publishedOn": "2021-07-20T02:04:46.867Z",
          "wordCount": null,
          "title": "Algorithmic Decision Making with Conditional Fairness. (arXiv:2006.10483v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1705.07164",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hong_J/0/1/0/all/0/1\">Johnny Hong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>",
          "description": "Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class\nof models, which have attracted great attention in various applications.\nHowever, this framework has two main drawbacks: (i) Wasserstein-1 (or\nEarth-Mover) distance is restrictive such that WGANs cannot always fit data\ngeometry well; (ii) It is difficult to achieve fast training of WGANs. In this\npaper, we propose a new class of \\textit{Relaxed Wasserstein} (RW) distances by\ngeneralizing Wasserstein-1 distance with Bregman cost functions. We show that\nRW distances achieve nice statistical properties while not sacrificing the\ncomputational tractability. Combined with the GANs framework, we develop\nRelaxed WGANs (RWGANs) which are not only statistically flexible but can be\napproximated efficiently using heuristic approaches. Experiments on real images\ndemonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms\nother competing approaches, e.g., WGANs, even with gradient penalty.",
          "link": "http://arxiv.org/abs/1705.07164",
          "publishedOn": "2021-07-20T02:04:46.866Z",
          "wordCount": null,
          "title": "Relaxed Wasserstein with Applications to GANs. (arXiv:1705.07164v8 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>",
          "description": "Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub.",
          "link": "http://arxiv.org/abs/2107.08369",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Chuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chang Chen</a>",
          "description": "This work investigates the framework and performance issues of the composite\nneural network, which is composed of a collection of pre-trained and\nnon-instantiated neural network models connected as a rooted directed acyclic\ngraph for solving complicated applications. A pre-trained neural network model\nis generally well trained, targeted to approximate a specific function. Despite\na general belief that a composite neural network may perform better than a\nsingle component, the overall performance characteristics are not clear. In\nthis work, we construct the framework of a composite network, and prove that a\ncomposite neural network performs better than any of its pre-trained components\nwith a high probability bound. In addition, if an extra pre-trained component\nis added to a composite network, with high probability, the overall performance\nwill not be degraded. In the study, we explore a complicated application --\nPM2.5 prediction -- to illustrate the correctness of the proposed composite\nnetwork theory. In the empirical evaluations of PM2.5 prediction, the\nconstructed composite neural network models support the proposed theory and\nperform better than other machine learning models, demonstrate the advantages\nof the proposed framework.",
          "link": "http://arxiv.org/abs/1910.09739",
          "publishedOn": "2021-07-20T02:04:46.865Z",
          "wordCount": null,
          "title": "Composite Neural Network: Theory and Application to PM2.5 Prediction. (arXiv:1910.09739v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandekar_A/0/1/0/all/0/1\">Abhishek Dandekar</a>",
          "description": "Machine learning (ML) techniques are being increasingly used in mobile\nnetworks for network planning, operation, management, optimisation and much\nmore. These techniques are realised using a set of logical nodes known as ML\npipeline. A single network operator might have thousands of such ML pipelines\ndistributed across its network. These pipelines need to be managed and\norchestrated across network domains. Thus it is essential to have autonomic\nmulti-domain orchestration of ML pipelines in mobile networks. International\nTelecommunications Union (ITU) has provided an architectural framework for\nmanagement and orchestration of ML pipelines in future networks. We extend this\nframework to enable autonomic orchestration of ML pipelines across multiple\nnetwork domains. We present our system architecture and describe its\napplication using a smart factory use case. Our work allows autonomic\norchestration of multi-domain ML pipelines in a standardised, technology\nagnostic, privacy preserving fashion.",
          "link": "http://arxiv.org/abs/2107.08194",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Towards autonomic orchestration of machine learning pipelines in future networks. (arXiv:2107.08194v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.06470",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lyzhov_A/0/1/0/all/0/1\">Alexander Lyzhov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1\">Dmitry Molchanov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty\nestimation is one of the main benchmarks for assessment of ensembling\nperformance. At the same time, deep learning ensembles have provided\nstate-of-the-art results in uncertainty estimation. In this work, we focus on\nin-domain uncertainty for image classification. We explore the standards for\nits quantification and point out pitfalls of existing metrics. Avoiding these\npitfalls, we perform a broad study of different ensembling techniques. To\nprovide more insight in this study, we introduce the deep ensemble equivalent\nscore (DEE) and show that many sophisticated ensembling techniques are\nequivalent to an ensemble of only few independently trained networks in terms\nof test performance.",
          "link": "http://arxiv.org/abs/2002.06470",
          "publishedOn": "2021-07-20T02:04:46.864Z",
          "wordCount": null,
          "title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning. (arXiv:2002.06470v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongjie Zhang</a>",
          "description": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks\nand achieves fast adaptation to new tasks. Despite recent progress, efficient\nexploration in meta-RL remains a key challenge in sparse-reward tasks, as it\nrequires quickly finding informative task-relevant experiences in both\nmeta-training and adaptation. To address this challenge, we explicitly model an\nexploration policy learning problem for meta-RL, which is separated from\nexploitation policy learning, and introduce a novel empowerment-driven\nexploration objective, which aims to maximize information gain for task\nidentification. We derive a corresponding intrinsic reward and develop a new\noff-policy meta-RL framework, which efficiently learns separate context-aware\nexploration and exploitation policies by sharing the knowledge of task\ninference. Experimental evaluation shows that our meta-RL method significantly\noutperforms state-of-the-art baselines on various sparse-reward MuJoCo\nlocomotion tasks and more complex sparse-reward Meta-World tasks.",
          "link": "http://arxiv.org/abs/2006.08170",
          "publishedOn": "2021-07-20T02:04:46.863Z",
          "wordCount": null,
          "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration. (arXiv:2006.08170v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>",
          "description": "Previous studies have shown that initializing neural machine translation\n(NMT) models with the pre-trained language models (LM) can speed up the model\ntraining and boost the model performance. In this work, we identify a critical\nside-effect of pre-training for NMT, which is due to the discrepancy between\nthe training objectives of LM-based pre-training and NMT. Since the LM\nobjective learns to reconstruct a few source tokens and copy most of them, the\npre-training initialization would affect the copying behaviors of NMT models.\nWe provide a quantitative analysis of copying behaviors by introducing a metric\ncalled copying ratio, which empirically shows that pre-training based NMT\nmodels have a larger copying ratio than the standard one. In response to this\nproblem, we propose a simple and effective method named copying penalty to\ncontrol the copying behaviors in decoding. Extensive experiments on both\nin-domain and out-of-domain benchmarks show that the copying penalty method\nconsistently improves translation performance by controlling copying behaviors\nfor pre-training based NMT models. Source code is freely available at\nhttps://github.com/SunbowLiu/CopyingPenalty.",
          "link": "http://arxiv.org/abs/2107.08212",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation. (arXiv:2107.08212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11860",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garg_A/0/1/0/all/0/1\">Aksh Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_S/0/1/0/all/0/1\">Sana Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rocca_M/0/1/0/all/0/1\">Marianna La Rocca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garner_R/0/1/0/all/0/1\">Rachael Garner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duncan_D/0/1/0/all/0/1\">Dominique Duncan</a>",
          "description": "With COVID-19 cases rising rapidly, deep learning has emerged as a promising\ndiagnosis technique. However, identifying the most accurate models to\ncharacterize COVID-19 patients is challenging because comparing results\nobtained with different types of data and acquisition processes is non-trivial.\nIn this paper we designed, evaluated, and compared the performance of 20\nconvolutional neutral networks in classifying patients as COVID-19 positive,\nhealthy, or suffering from other pulmonary lung infections based on Chest CT\nscans, serving as the first to consider the EfficientNet family for COVID-19\ndiagnosis and employ intermediate activation maps for visualizing model\nperformance. All models are trained and evaluated in Python using 4173 Chest CT\nimages from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with\n2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or\nsuffering from other pulmonary infections, respectively. EfficientNet-B5 was\nidentified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of\n0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of\n0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class\ndataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of\n0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of\n0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation\nmaps and Gradient-weighted Class Activation Mappings offered\nhuman-interpretable evidence of the model's perception of ground-class\nopacities and consolidations, hinting towards a promising use-case of\nartificial intelligence-assisted radiology tools. With a prediction speed of\nunder 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a\nrapid, scalable, and accurate diagnostic for COVID-19.",
          "link": "http://arxiv.org/abs/2012.11860",
          "publishedOn": "2021-07-20T02:04:46.862Z",
          "wordCount": null,
          "title": "Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT. (arXiv:2012.11860v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08135",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1\">Ikko Yamane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1\">Junya Honda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Ordinary supervised learning is useful when we have paired training data of\ninput $X$ and output $Y$. However, such paired data can be difficult to collect\nin practice. In this paper, we consider the task of predicting $Y$ from $X$\nwhen we have no paired data of them, but we have two separate, independent\ndatasets of $X$ and $Y$ each observed with some mediating variable $U$, that\nis, we have two datasets $S_X = \\{(X_i, U_i)\\}$ and $S_Y = \\{(U'_j, Y'_j)\\}$. A\nnaive approach is to predict $U$ from $X$ using $S_X$ and then $Y$ from $U$\nusing $S_Y$, but we show that this is not statistically consistent. Moreover,\npredicting $U$ can be more difficult than predicting $Y$ in practice, e.g.,\nwhen $U$ has higher dimensionality. To circumvent the difficulty, we propose a\nnew method that avoids predicting $U$ but directly learns $Y = f(X)$ by\ntraining $f(X)$ with $S_{X}$ to predict $h(U)$ which is trained with $S_{Y}$ to\napproximate $Y$. We prove statistical consistency and error bounds of our\nmethod and experimentally confirm its practical usefulness.",
          "link": "http://arxiv.org/abs/2107.08135",
          "publishedOn": "2021-07-20T02:04:46.861Z",
          "wordCount": null,
          "title": "Mediated Uncoupled Learning: Learning Functions without Direct Input-output Correspondences. (arXiv:2107.08135v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">JoonSung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>",
          "description": "Recently Autoencoder(AE) based models are widely used in the field of anomaly\ndetection. A model trained with normal data generates a larger restoration\nerror for abnormal data. Whether or not abnormal data is determined by\nobserving the restoration error. It takes a lot of cost and time to obtain\nabnormal data in the industrial field. Therefore the model trains only normal\ndata and detects abnormal data in the inference phase. However, the restoration\narea for the input data of AE is limited in the latent space. To solve this\nproblem, we propose Multiple-hypothesis Autoencoder(MH-AE) model composed of\nseveral decoders. MH-AE model increases the restoration area through contention\nbetween decoders. The proposed method shows that the anomaly detection\nperformance is improved compared to the traditional AE for various input\ndatasets.",
          "link": "http://arxiv.org/abs/2107.08790",
          "publishedOn": "2021-07-20T02:04:46.860Z",
          "wordCount": null,
          "title": "Anomaly Detection Based on Multiple-Hypothesis Autoencoder. (arXiv:2107.08790v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiahua Luo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vong_C/0/1/0/all/0/1\">Chi-Man Vong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jie Du</a> (2) ((1) Department of Computer and Information Science, University of Macau, Macao SAR, China, (2) School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China)",
          "description": "Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic\nmodel with very competitive generalization. However, SBL needs to invert a big\ncovariance matrix with complexity O(M^3 ) (M: feature size) for updating the\nregularization priors, making it difficult for practical use. There are three\nissues in SBL: 1) Inverting the covariance matrix may obtain singular solutions\nin some cases, which hinders SBL from convergence; 2) Poor scalability to\nproblems with high dimensional feature space or large data size; 3) SBL easily\nsuffers from memory overflow for large-scale data. This paper addresses these\nissues with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called\nDQN-SBL where the inversion of big covariance matrix is ignored so that the\ncomplexity and memory storage are reduced to O(M). The DQN-SBL is thoroughly\nevaluated on non-linear classifiers and linear feature selection using various\nbenchmark datasets of different sizes. Experimental results verify that DQN-SBL\nreceives competitive generalization with a very sparse model and scales well to\nlarge-scale problems.",
          "link": "http://arxiv.org/abs/2107.08195",
          "publishedOn": "2021-07-20T02:04:46.829Z",
          "wordCount": null,
          "title": "Sparse Bayesian Learning with Diagonal Quasi-Newton Method For Large Scale Classification. (arXiv:2107.08195v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.12804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work demonstrates that, contrary to a common belief, using the objective\nwith independence assumption for modelling the span probability $P(a_s,a_e) =\nP(a_s)P(a_e)$ of span starting at position $a_s$ and ending at position $a_e$\nhas adverse effects. Therefore we propose multiple approaches to modelling\njoint probability $P(a_s,a_e)$ directly. Among those, we propose a compound\nobjective, composed from the joint probability while still keeping the\nobjective with independence assumption as an auxiliary objective. We find that\nthe compound objective is consistently superior or equal to other assumptions\nin exact match. Additionally, we identified common errors caused by the\nassumption of independence and manually checked the counterpart predictions,\ndemonstrating the impact of the compound objective on the real examples. Our\nfindings are supported via experiments with three extractive QA models (BIDAF,\nBERT, ALBERT) over six datasets and our code, individual results and manual\nanalysis are available online.",
          "link": "http://arxiv.org/abs/2008.12804",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chi-Heng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azabou_M/0/1/0/all/0/1\">Mehdi Azabou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Eva L. Dyer</a>",
          "description": "Optimal transport (OT) is a widely used technique for distribution alignment,\nwith applications throughout the machine learning, graphics, and vision\ncommunities. Without any additional structural assumptions on trans-port,\nhowever, OT can be fragile to outliers or noise, especially in high dimensions.\nHere, we introduce a new form of structured OT that simultaneously learns\nlow-dimensional structure in data while leveraging this structure to solve the\nalignment task. Compared with OT, the resulting transport plan has better\nstructural interpretability, highlighting the connections between individual\ndata points and local geometry, and is more robust to noise and sampling. We\napply the method to synthetic as well as real datasets, where we show that our\nmethod can facilitate alignment in noisy settings and can be used to both\ncorrect and interpret domain shift.",
          "link": "http://arxiv.org/abs/2012.11589",
          "publishedOn": "2021-07-20T02:04:46.806Z",
          "wordCount": null,
          "title": "Making transport more robust and interpretable by moving data through a small number of anchor points. (arXiv:2012.11589v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafarani_F/0/1/0/all/0/1\">Farzad Zafarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_C/0/1/0/all/0/1\">Chris Clifton</a>",
          "description": "With the increasing collection of users' data, protecting individual privacy\nhas gained more interest. Differential Privacy is a strong concept of\nprotecting individuals. Naive Bayes is one of the popular machine learning\nalgorithm, used as a baseline for many tasks. In this work, we have provided a\ndifferentially private Naive Bayes classifier that adds noise proportional to\nthe Smooth Sensitivity of its parameters. We have compared our result to\nVaidya, Shafiq, Basu, and Hong in which they have scaled the noise to the\nglobal sensitivity of the parameters. Our experiment results on the real-world\ndatasets show that the accuracy of our method has improved significantly while\nstill preserving $\\varepsilon$-differential privacy.",
          "link": "http://arxiv.org/abs/2003.13955",
          "publishedOn": "2021-07-20T02:04:46.805Z",
          "wordCount": null,
          "title": "Differentially Private Naive Bayes Classifier using Smooth Sensitivity. (arXiv:2003.13955v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>",
          "description": "Policy optimization is a widely-used method in reinforcement learning. Due to\nits local-search nature, however, theoretical guarantees on global optimality\noften rely on extra assumptions on the Markov Decision Processes (MDPs) that\nbypass the challenge of global exploration. To eliminate the need of such\nassumptions, in this work, we develop a general solution that adds dilated\nbonuses to the policy update to facilitate global exploration. To showcase the\npower and generality of this technique, we apply it to several episodic MDP\nsettings with adversarial losses and bandit feedback, improving and\ngeneralizing the state-of-the-art. Specifically, in the tabular case, we obtain\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret where $T$ is the number of episodes,\nimproving the $\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.\n(2020). When the number of states is infinite, under the assumption that the\nstate-action values are linear in some low-dimensional features, we obtain\n$\\widetilde{\\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,\nmatching the result of Neu and Olkhovskaya (2020) while importantly removing\nthe need of an exploratory policy that their algorithm requires. When a\nsimulator is unavailable, we further consider a linear MDP setting and obtain\n$\\widetilde{\\mathcal{O}}({T}^{14/15})$ regret, which is the first result for\nlinear MDPs with adversarial losses and bandit feedback.",
          "link": "http://arxiv.org/abs/2107.08346",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses. (arXiv:2107.08346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meiyazhagan_J/0/1/0/all/0/1\">J.Meiyazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudharsan_S/0/1/0/all/0/1\">S. Sudharsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senthilvelan_M/0/1/0/all/0/1\">M. Senthilvelan</a>",
          "description": "We predict the emergence of extreme events in a parametrically driven\nnonlinear dynamical system using three Deep Learning models, namely Multi-Layer\nPerceptron, Convolutional Neural Network and Long Short-Term Memory. The Deep\nLearning models are trained using the training set and are allowed to predict\nthe test set data. After prediction, the time series of the actual and the\npredicted values are plotted one over the other in order to visualize the\nperformance of the models. Upon evaluating the Root Mean Square Error value\nbetween predicted and the actual values of all three models, we find that the\nLong Short-Term Memory model can serve as the best model to forecast the\nchaotic time series and to predict the emergence of extreme events for the\nconsidered system.",
          "link": "http://arxiv.org/abs/2107.08819",
          "publishedOn": "2021-07-20T02:04:46.804Z",
          "wordCount": null,
          "title": "Model-free prediction of emergence of extreme events in a parametrically driven nonlinear dynamical system by Deep Learning. (arXiv:2107.08819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.09478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parunandi_K/0/1/0/all/0/1\">Karthikeya S. Parunandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aayushman Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1\">Raman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravorty_S/0/1/0/all/0/1\">Suman Chakravorty</a>",
          "description": "The problem of Reinforcement Learning (RL) in an unknown nonlinear dynamical\nsystem is equivalent to the search for an optimal feedback law utilizing the\nsimulations/ rollouts of the unknown dynamical system. Most RL techniques\nsearch over a complex global nonlinear feedback parametrization making them\nsuffer from high training times as well as variance. Instead, we advocate\nsearching over a local feedback representation consisting of an open-loop\nsequence, and an associated optimal linear feedback law completely determined\nby the open-loop. We show that this alternate approach results in highly\nefficient training, the answers obtained are repeatable and hence reliable, and\nthe resulting closed performance is superior to global state-of-the-art RL\ntechniques. Finally, if we replan, whenever required, which is feasible due to\nthe fast and reliable local solution, allows us to recover global optimality of\nthe resulting feedback law.",
          "link": "http://arxiv.org/abs/2002.09478",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "On the Search for Feedback in Reinforcement Learning. (arXiv:2002.09478v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mounir_R/0/1/0/all/0/1\">Ramy Mounir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gula_R/0/1/0/all/0/1\">Roman Gula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theuerkauf_J/0/1/0/all/0/1\">J&#xf6;rn Theuerkauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeep Sarkar</a>",
          "description": "Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.",
          "link": "http://arxiv.org/abs/2005.02463",
          "publishedOn": "2021-07-20T02:04:46.803Z",
          "wordCount": null,
          "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife Extended Videos. (arXiv:2005.02463v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Projection robust Wasserstein (PRW) distance, or Wasserstein projection\npursuit (WPP), is a robust variant of the Wasserstein distance. Recent work\nsuggests that this quantity is more robust than the standard Wasserstein\ndistance, in particular when comparing probability measures in high-dimensions.\nHowever, it is ruled out for practical application because the optimization\nmodel is essentially non-convex and non-smooth which makes the computation\nintractable. Our contribution in this paper is to revisit the original\nmotivation behind WPP/PRW, but take the hard route of showing that, despite its\nnon-convexity and lack of nonsmoothness, and even despite some hardness results\nproved by~\\citet{Niles-2019-Estimation} in a minimax sense, the original\nformulation for PRW/WPP \\textit{can} be efficiently computed in practice using\nRiemannian optimization, yielding in relevant cases better behavior than its\nconvex relaxation. More specifically, we provide three simple algorithms with\nsolid theoretical guarantee on their complexity bound (one in the appendix),\nand demonstrate their effectiveness and efficiency by conducing extensive\nexperiments on synthetic and real data. This paper provides a first step into a\ncomputational theory of the PRW distance and provides the links between optimal\ntransport and Riemannian optimization.",
          "link": "http://arxiv.org/abs/2006.07458",
          "publishedOn": "2021-07-20T02:04:46.802Z",
          "wordCount": null,
          "title": "Projection Robust Wasserstein Distance and Riemannian Optimization. (arXiv:2006.07458v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.00038",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>",
          "description": "We present an information-based uncertainty quantification method for general\nMarkov Random Fields. Markov Random Fields (MRF) are structured, probabilistic\ngraphical models over undirected graphs, and provide a fundamental unifying\nmodeling tool for statistical mechanics, probabilistic machine learning, and\nartificial intelligence. Typically MRFs are complex and high-dimensional with\nnodes and edges (connections) built in a modular fashion from simpler,\nlow-dimensional probabilistic models and their local connections; in turn, this\nmodularity allows to incorporate available data to MRFs and efficiently\nsimulate them by leveraging their graph-theoretic structure. Learning graphical\nmodels from data and/or constructing them from physical modeling and\nconstraints necessarily involves uncertainties inherited from data, modeling\nchoices, or numerical approximations. These uncertainties in the MRF can be\nmanifested either in the graph structure or the probability distribution\nfunctions, and necessarily will propagate in predictions for quantities of\ninterest. Here we quantify such uncertainties using tight, information based\nbounds on the predictions of quantities of interest; these bounds take\nadvantage of the graphical structure of MRFs and are capable of handling the\ninherent high-dimensionality of such graphical models. We demonstrate our\nmethods in MRFs for medical diagnostics and statistical mechanics models. In\nthe latter, we develop uncertainty quantification bounds for finite size\neffects and phase diagrams, which constitute two of the typical predictions\ngoals of statistical mechanics modeling.",
          "link": "http://arxiv.org/abs/2009.00038",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Uncertainty quantification for Markov Random Fields. (arXiv:2009.00038v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_G/0/1/0/all/0/1\">Gautam Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnahan_M/0/1/0/all/0/1\">Mason Carnahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamapant_S/0/1/0/all/0/1\">Shilpa Shamapant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surendranath_Y/0/1/0/all/0/1\">Yashitha Surendranath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saumya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arundhati Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Co Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millan_J/0/1/0/all/0/1\">Jose del R Millan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed H Tewfik</a>",
          "description": "In this paper, we propose a deep learning-based algorithm to improve the\nperformance of automatic speech recognition (ASR) systems for aphasia, apraxia,\nand dysarthria speech by utilizing electroencephalography (EEG) features\nrecorded synchronously with aphasia, apraxia, and dysarthria speech. We\ndemonstrate a significant decoding performance improvement by more than 50\\%\nduring test time for isolated speech recognition task and we also provide\npreliminary results indicating performance improvement for the more challenging\ncontinuous speech recognition task by utilizing EEG features. The results\npresented in this paper show the first step towards demonstrating the\npossibility of utilizing non-invasive neural signals to design a real-time\nrobust speech prosthetic for stroke survivors recovering from aphasia, apraxia,\nand dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will\nbe released to the public to help further advance this interesting and crucial\nresearch.",
          "link": "http://arxiv.org/abs/2103.00383",
          "publishedOn": "2021-07-20T02:04:46.801Z",
          "wordCount": null,
          "title": "Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition. (arXiv:2103.00383v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We introduce ParaBLEU, a paraphrase representation learning model and\nevaluation metric for text generation. Unlike previous approaches, ParaBLEU\nlearns to understand paraphrasis using generative conditioning as a pretraining\nobjective. ParaBLEU correlates more strongly with human judgements than\nexisting metrics, obtaining new state-of-the-art results on the 2017 WMT\nMetrics Shared Task. We show that our model is robust to data scarcity,\nexceeding previous state-of-the-art performance using only $50\\%$ of the\navailable training data and surpassing BLEU, ROUGE and METEOR with only $40$\nlabelled examples. Finally, we demonstrate that ParaBLEU can be used to\nconditionally generate novel paraphrases from a single demonstration, which we\nuse to confirm our hypothesis that it learns abstract, generalized paraphrase\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.08251",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Generative Pretraining for Paraphrase Evaluation. (arXiv:2107.08251v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>",
          "description": "Fairness is crucial for neural networks which are used in applications with\nimportant societal implication. Recently, there have been multiple attempts on\nimproving fairness of neural networks, with a focus on fairness testing (e.g.,\ngenerating individual discriminatory instances) and fairness training (e.g.,\nenhancing fairness through augmented training). In this work, we propose an\napproach to formally verify neural networks against fairness, with a focus on\nindependence-based fairness such as group fairness. Our method is built upon an\napproach for learning Markov Chains from a user-provided neural network (i.e.,\na feed-forward neural network or a recurrent neural network) which is\nguaranteed to facilitate sound analysis. The learned Markov Chain not only\nallows us to verify (with Probably Approximate Correctness guarantee) whether\nthe neural network is fair or not, but also facilities sensitivity analysis\nwhich helps to understand why fairness is violated. We demonstrate that with\nour analysis results, the neural weights can be optimized to improve fairness.\nOur approach has been evaluated with multiple models trained on benchmark\ndatasets and the experiment results show that our approach is effective and\nefficient.",
          "link": "http://arxiv.org/abs/2107.08362",
          "publishedOn": "2021-07-20T02:04:46.799Z",
          "wordCount": null,
          "title": "Probabilistic Verification of Neural Networks Against Group Fairness. (arXiv:2107.08362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.",
          "link": "http://arxiv.org/abs/2102.11068",
          "publishedOn": "2021-07-20T02:04:46.798Z",
          "wordCount": null,
          "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?. (arXiv:2102.11068v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08850",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1\">Jonathan Ganz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirsch_T/0/1/0/all/0/1\">Tobias Kirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_L/0/1/0/all/0/1\">Lucas Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_C/0/1/0/all/0/1\">Christoph Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumcke_I/0/1/0/all/0/1\">Ingmar Bl&#xfc;mcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jabari_S/0/1/0/all/0/1\">Samir Jabari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>",
          "description": "Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.",
          "link": "http://arxiv.org/abs/2107.08850",
          "publishedOn": "2021-07-20T02:04:46.797Z",
          "wordCount": null,
          "title": "Automatic and explainable grading of meningiomas from histopathology images. (arXiv:2107.08850v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jack Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenain_R/0/1/0/all/0/1\">Raphael Lenain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meepegama_U/0/1/0/all/0/1\">Udeepa Meepegama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fristed_E/0/1/0/all/0/1\">Emil Fristed</a>",
          "description": "We propose a method for learning de-identified prosody representations from\nraw audio using a contrastive self-supervised signal. Whereas prior work has\nrelied on conditioning models on bottlenecks, we introduce a set of inductive\nbiases that exploit the natural structure of prosody to minimize timbral\ninformation and decouple prosody from speaker representations. Despite\naggressive downsampling of the input and having no access to linguistic\ninformation, our model performs comparably to state-of-the-art speech\nrepresentations on DAMMP, a new benchmark we introduce for spoken language\nunderstanding. We use minimum description length probing to show that our\nrepresentations have selectively learned the subcomponents of non-timbral\nprosody, and that the product quantizer naturally disentangles them without\nusing bottlenecks. We derive an information-theoretic definition of speech\nde-identifiability and use it to demonstrate that our prosody representations\nare less identifiable than other speech representations.",
          "link": "http://arxiv.org/abs/2107.08248",
          "publishedOn": "2021-07-20T02:04:46.796Z",
          "wordCount": null,
          "title": "Learning De-identified Representations of Prosody from Raw Audio. (arXiv:2107.08248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Alan Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1\">Hugo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungsu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1\">Tadashi Kozuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">A. Rupam Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Martha White</a>",
          "description": "Approximate Policy Iteration (API) algorithms alternate between (approximate)\npolicy evaluation and (approximate) greedification. Many different approaches\nhave been explored for approximate policy evaluation, but less is understood\nabout approximate greedification and what choices guarantee policy improvement.\nIn this work, we investigate approximate greedification when reducing the KL\ndivergence between the parameterized policy and the Boltzmann distribution over\naction values. In particular, we investigate the difference between the forward\nand reverse KL divergences, with varying degrees of entropy regularization. We\nshow that the reverse KL has stronger policy improvement guarantees, but that\nreducing the forward KL can result in a worse policy. We also demonstrate,\nhowever, that a large enough reduction of the forward KL can induce improvement\nunder additional assumptions. Empirically, we show on simple continuous-action\nenvironments that the forward KL can induce more exploration, but at the cost\nof a more suboptimal policy. No significant differences were observed in the\ndiscrete-action setting or on a suite of benchmark problems. Throughout, we\nhighlight that many policy gradient methods can be seen as an instance of API,\nwith either the forward or reverse KL for the policy update, and discuss next\nsteps for understanding and improving our policy optimization algorithms.",
          "link": "http://arxiv.org/abs/2107.08285",
          "publishedOn": "2021-07-20T02:04:46.795Z",
          "wordCount": null,
          "title": "Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences. (arXiv:2107.08285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiandong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feiwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>",
          "description": "Recently, neural network compression schemes like channel pruning have been\nwidely used to reduce the model size and computational complexity of deep\nneural network (DNN) for applications in power-constrained scenarios such as\nembedded systems. Reinforcement learning (RL)-based auto-pruning has been\nfurther proposed to automate the DNN pruning process to avoid expensive\nhand-crafted work. However, the RL-based pruner involves a time-consuming\ntraining process and the high expense of each sample further exacerbates this\nproblem. These impediments have greatly restricted the real-world application\nof RL-based auto-pruning. Thus, in this paper, we propose an efficient\nauto-pruning framework which solves this problem by taking advantage of the\nhistorical data from the previous auto-pruning process. In our framework, we\nfirst boost the convergence of the RL-pruner by transfer learning. Then, an\naugmented transfer learning scheme is proposed to further speed up the training\nprocess by improving the transferability. Finally, an assistant learning\nprocess is proposed to improve the sample efficiency of the RL agent. The\nexperiments have shown that our framework can accelerate the auto-pruning\nprocess by 1.5-2.5 times for ResNet20, and 1.81-2.375 times for other neural\nnetworks like ResNet56, ResNet18, and MobileNet v1.",
          "link": "http://arxiv.org/abs/2107.08815",
          "publishedOn": "2021-07-20T02:04:46.794Z",
          "wordCount": null,
          "title": "Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data. (arXiv:2107.08815v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1\">Evangelia Gergatsouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouleakis_T/0/1/0/all/0/1\">Themis Gouleakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patris_N/0/1/0/all/0/1\">Nikolas Patris</a>",
          "description": "Following the research agenda initiated by Munoz & Vassilvitskii [1] and\nLykouris & Vassilvitskii [2] on learning-augmented online algorithms for\nclassical online optimization problems, in this work, we consider the Online\nFacility Location problem under this framework. In Online Facility Location\n(OFL), demands arrive one-by-one in a metric space and must be (irrevocably)\nassigned to an open facility upon arrival, without any knowledge about future\ndemands.\n\nWe present an online algorithm for OFL that exploits potentially imperfect\npredictions on the locations of the optimal facilities. We prove that the\ncompetitive ratio decreases smoothly from sublogarithmic in the number of\ndemands to constant, as the error, i.e., the total distance of the predicted\nlocations to the optimal facility locations, decreases towards zero. We\ncomplement our analysis with a matching lower bound establishing that the\ndependence of the algorithm's competitive ratio on the error is optimal, up to\nconstant factors. Finally, we evaluate our algorithm on real world data and\ncompare our learning augmented approach with the current best online algorithm\nfor the problem.",
          "link": "http://arxiv.org/abs/2107.08277",
          "publishedOn": "2021-07-20T02:04:46.793Z",
          "wordCount": null,
          "title": "Learning Augmented Online Facility Location. (arXiv:2107.08277v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Software Vulnerabilities (SVs) are increasing in complexity and scale, posing\ngreat security risks to many software systems. Given the limited resources in\npractice, SV assessment and prioritization help practitioners devise optimal SV\nmitigation plans based on various SV characteristics. The surge in SV data\nsources and data-driven techniques such as Machine Learning and Deep Learning\nhave taken SV assessment and prioritization to the next level. Our survey\nprovides a taxonomy of the past research efforts and highlights the best\npractices for data-driven SV assessment and prioritization. We also discuss the\ncurrent limitations and propose potential solutions to address such issues.",
          "link": "http://arxiv.org/abs/2107.08364",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "A Survey on Data-driven Software Vulnerability Assessment and Prioritization. (arXiv:2107.08364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sushant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1\">Shahin Jabbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a post\nhoc manner. In this work, we analyze two popular post hoc interpretation\ntechniques: SmoothGrad which is a gradient based method, and a variant of LIME\nwhich is a perturbation based method. More specifically, we derive explicit\nclosed form expressions for the explanations output by these two methods and\nshow that they both converge to the same explanation in expectation, i.e., when\nthe number of perturbed samples used by these methods is large. We then\nleverage this connection to establish other desirable properties, such as\nrobustness, for these techniques. We also derive finite sample complexity\nbounds for the number of perturbations required for these methods to converge\nto their expected explanation. Finally, we empirically validate our theory\nusing extensive experimentation on both synthetic and real world datasets.",
          "link": "http://arxiv.org/abs/2102.10618",
          "publishedOn": "2021-07-20T02:04:46.792Z",
          "wordCount": null,
          "title": "Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_A/0/1/0/all/0/1\">Amine Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "We consider an Intelligent Reflecting Surface (IRS)-aided multiple-input\nsingle-output (MISO) system for downlink transmission. We compare the\nperformance of Deep Reinforcement Learning (DRL) and conventional optimization\nmethods in finding optimal phase shifts of the IRS elements to maximize the\nuser signal-to-noise (SNR) ratio. Furthermore, we evaluate the robustness of\nthese methods to channel impairments and changes in the system. We demonstrate\nnumerically that DRL solutions show more robustness to noisy channels and user\nmobility.",
          "link": "http://arxiv.org/abs/2107.08293",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "On the Robustness of Deep Reinforcement Learning in IRS-Aided Wireless Communications Systems. (arXiv:2107.08293v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Karishma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>",
          "description": "Identifying and characterizing disinformation in political discourse on\nsocial media is critical to ensure the integrity of elections and democratic\nprocesses around the world. Persistent manipulation of social media has\nresulted in increased concerns regarding the 2020 U.S. Presidential Election,\ndue to its potential to influence individual opinions and social dynamics. In\nthis work, we focus on the identification of distorted facts, in the form of\nunreliable and conspiratorial narratives in election-related tweets, to\ncharacterize discourse manipulation prior to the election. We apply a detection\nmodel to separate factual from unreliable (or conspiratorial) claims analyzing\na dataset of 242 million election-related tweets. The identified claims are\nused to investigate targeted topics of disinformation, and conspiracy groups,\nmost notably the far-right QAnon conspiracy group. Further, we characterize\naccount engagements with unreliable and conspiracy tweets, and with the QAnon\nconspiracy group, by political leaning and tweet types. Finally, using a\nregression discontinuity design, we investigate whether Twitter's actions to\ncurb QAnon activity on the platform were effective, and how QAnon accounts\nadapt to Twitter's restrictions.",
          "link": "http://arxiv.org/abs/2107.08319",
          "publishedOn": "2021-07-20T02:04:46.791Z",
          "wordCount": null,
          "title": "Characterizing Online Engagement with Disinformation and Conspiracies in the 2020 U.S. Presidential Election. (arXiv:2107.08319v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1\">Benjamin Paa&#xdf;en</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1\">Alexander Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1\">Terrence C. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Differentiable neural computers extend artificial neural networks with an\nexplicit memory without interference, thus enabling the model to perform\nclassic computation tasks such as graph traversal. However, such models are\ndifficult to train, requiring long training times and large datasets. In this\nwork, we achieve some of the computational capabilities of differentiable\nneural computers with a model that can be trained very efficiently, namely an\necho state network with an explicit memory without interference. This extension\nenables echo state networks to recognize all regular languages, including those\nthat contractive echo state networks provably can not recognize. Further, we\ndemonstrate experimentally that our model performs comparably to its\nfully-trained deep version on several typical benchmark tasks for\ndifferentiable neural computers.",
          "link": "http://arxiv.org/abs/2009.06342",
          "publishedOn": "2021-07-20T02:04:46.774Z",
          "wordCount": null,
          "title": "Reservoir Memory Machines as Neural Computers. (arXiv:2009.06342v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06070",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laval_J/0/1/0/all/0/1\">Jorge Laval</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_A/0/1/0/all/0/1\">Anye Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenchao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qing_Z/0/1/0/all/0/1\">Zhu Qing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peeta_S/0/1/0/all/0/1\">Srinivas Peeta</a>",
          "description": "Self-driving technology companies and the research community are accelerating\ntheir pace to use machine learning longitudinal motion planning (mMP) for\nautonomous vehicles (AVs). This paper reviews the current state of the art in\nmMP, with an exclusive focus on its impact on traffic congestion. We identify\nthe availability of congestion scenarios in current datasets, and summarize the\nrequired features for training mMP. For learning methods, we survey the major\nmethods in both imitation learning and non-imitation learning. We also\nhighlight the emerging technologies adopted by some leading AV companies, e.g.\nTesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly\nfocusing on the long tail problem related to safety and overlooked the impact\non traffic congestion, ii) the current public self-driving datasets have not\nincluded enough congestion scenarios, and mostly lack the necessary input\nfeatures/output labels to train mMP, and iii) albeit reinforcement learning\n(RL) approach can integrate congestion mitigation into the learning goal, the\nmajor mMP method adopted by industry is still behavior cloning (BC), whose\ncapability to learn a congestion-mitigating mMP remains to be seen. Based on\nthe review, the study identifies the research gaps in current mMP development.\nSome suggestions towards congestion mitigation for future mMP studies are\nproposed: i) enrich data collection to facilitate the congestion learning, ii)\nincorporate non-imitation learning methods to combine traffic efficiency into a\nsafety-oriented technical route, and iii) integrate domain knowledge from the\ntraditional car following (CF) theory to improve the string stability of mMP.",
          "link": "http://arxiv.org/abs/1910.06070",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion. (arXiv:1910.06070v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Katsunari Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ejima_T/0/1/0/all/0/1\">Takuya Ejima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokumaru_Y/0/1/0/all/0/1\">Yuki Tokumaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuki_T/0/1/0/all/0/1\">Toshitaka Matsuki</a>",
          "description": "Here, we introduce a fully local index named \"sensitivity\" for each neuron to\ncontrol chaoticity or gradient globally in a neural network (NN). We also\npropose a learning method to adjust it named \"sensitivity adjustment learning\n(SAL)\". The index is the gradient magnitude of its output with respect to its\ninputs. By adjusting its time average to 1.0 in each neuron, information\ntransmission in the neuron changes to be moderate without shrinking or\nexpanding for both forward and backward computations. That results in moderate\ninformation transmission through a layer of neurons when the weights and inputs\nare random. Therefore, SAL can control the chaoticity of the network dynamics\nin a recurrent NN (RNN). It can also solve the vanishing gradient problem in\nerror backpropagation (BP) learning in a deep feedforward NN or an RNN. We\ndemonstrate that when applying SAL to an RNN with small and random initial\nweights, log-sensitivity, which is the logarithm of RMS (root mean square)\nsensitivity over all the neurons, is equivalent to the maximum Lyapunov\nexponent until it reaches 0.0. We also show that SAL works with BP or BPTT (BP\nthrough time) to avoid the vanishing gradient problem in a 300-layer NN or an\nRNN that learns a problem with a lag of 300 steps between the first input and\nthe output. Compared with manually fine-tuning the spectral radius of the\nweight matrix before learning, SAL's continuous nonlinear learning nature\nprevents loss of sensitivities during learning, resulting in a significant\nimprovement in learning performance.",
          "link": "http://arxiv.org/abs/2012.13134",
          "publishedOn": "2021-07-20T02:04:46.773Z",
          "wordCount": null,
          "title": "Sensitivity -- Local Index to Control Chaoticity or Gradient Globally. (arXiv:2012.13134v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunfan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "This paper presents Particle-based Object Manipulation (Prompt), a new\napproach to robot manipulation of novel objects ab initio, without prior object\nmodels or pre-training on a large object data set. The key element of Prompt is\na particle-based object representation, in which each particle represents a\npoint in the object, the local geometric, physical, and other features of the\npoint, and also its relation with other particles. Like the model-based\nanalytic approaches to manipulation, the particle representation enables the\nrobot to reason about the object's geometry and dynamics in order to choose\nsuitable manipulation actions. Like the data-driven approaches, the particle\nrepresentation is learned online in real-time from visual sensor input,\nspecifically, multi-view RGB images. The particle representation thus connects\nvisual perception with robot control. Prompt combines the benefits of both\nmodel-based reasoning and data-driven learning. We show empirically that Prompt\nsuccessfully handles a variety of everyday objects, some of which are\ntransparent. It handles various manipulation tasks, including grasping,\npushing, etc,. Our experiments also show that Prompt outperforms a\nstate-of-the-art data-driven grasping method on the daily objects, even though\nit does not use any offline training data.",
          "link": "http://arxiv.org/abs/2107.08865",
          "publishedOn": "2021-07-20T02:04:46.772Z",
          "wordCount": null,
          "title": "Ab Initio Particle-based Object Manipulation. (arXiv:2107.08865v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1\">Divya Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finck_M/0/1/0/all/0/1\">Mich&#xe8;le Finck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1\">Asia Biega</a>",
          "description": "Data minimization is a legal obligation defined in the European Union's\nGeneral Data Protection Regulation (GDPR) as the responsibility to process an\nadequate, relevant, and limited amount of personal data in relation to a\nprocessing purpose. However, unlike fairness or transparency, the principle has\nnot seen wide adoption for machine learning systems due to a lack of\ncomputational interpretation. In this paper, we build on literature in machine\nlearning and law to propose the first learning framework for limiting data\ncollection based on an interpretation that ties the data collection purpose to\nsystem performance. We formalize a data minimization criterion based on\nperformance curve derivatives and provide an effective and interpretable\npiecewise power law technique that models distinct stages of an algorithm's\nperformance throughout data collection. Results from our empirical\ninvestigation offer deeper insights into the relevant considerations when\ndesigning a data minimization framework, including the choice of feature\nacquisition algorithm, initialization conditions, as well as impacts on\nindividuals that hint at tensions between data minimization and fairness.",
          "link": "http://arxiv.org/abs/2107.08096",
          "publishedOn": "2021-07-20T02:04:46.699Z",
          "wordCount": null,
          "title": "Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice. (arXiv:2107.08096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elflein_S/0/1/0/all/0/1\">Sven Elflein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1\">Bertrand Charpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1\">Daniel Z&#xfc;gner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Several density estimation methods have shown to fail to detect\nout-of-distribution (OOD) samples by assigning higher likelihoods to anomalous\ndata. Energy-based models (EBMs) are flexible, unnormalized density models\nwhich seem to be able to improve upon this failure mode. In this work, we\nprovide an extensive study investigating OOD detection with EBMs trained with\ndifferent approaches on tabular and image data and find that EBMs do not\nprovide consistent advantages. We hypothesize that EBMs do not learn semantic\nfeatures despite their discriminative structure similar to Normalizing Flows.\nTo verify this hypotheses, we show that supervision and architectural\nrestrictions improve the OOD detection of EBMs independent of the training\napproach.",
          "link": "http://arxiv.org/abs/2107.08785",
          "publishedOn": "2021-07-20T02:04:46.698Z",
          "wordCount": null,
          "title": "On Out-of-distribution Detection with Energy-based Models. (arXiv:2107.08785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacham_P/0/1/0/all/0/1\">Praneeth Kacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "Currently, in the numerical linear algebra community, it is thought that to\nobtain nearly-optimal bounds for various problems such as rank computation,\nfinding a maximal linearly independent subset of columns, regression, low rank\napproximation, maximum matching on general graphs and linear matroid union, one\nwould need to resolve the main open question of Nelson and Nguyen (FOCS, 2013)\nregarding the logarithmic factors in the sketching dimension for existing\nconstant factor approximation oblivious subspace embeddings. We show how to\nbypass this question using a refined sketching technique, and obtain optimal or\nnearly optimal bounds for these problems. A key technique we use is an explicit\nmapping of Indyk based on uncertainty principles and extractors, which after\nfirst applying known oblivious subspace embeddings, allows us to quickly spread\nout the mass of the vector so that sampling is now effective, and we avoid a\nlogarithmic factor that is standard in the sketching dimension resulting from\nmatrix Chernoff bounds. For the fundamental problems of rank computation and\nfinding a linearly independent subset of columns, our algorithms improve\nCheung, Kwok, and Lau (JACM, 2013) and are optimal to within a constant factor\nand a $\\log\\log(n)$-factor, respectively. Further, for constant factor\nregression and low rank approximation we give the first optimal algorithms, for\nthe current matrix multiplication exponent.",
          "link": "http://arxiv.org/abs/2107.08090",
          "publishedOn": "2021-07-20T02:04:46.697Z",
          "wordCount": null,
          "title": "Near-Optimal Algorithms for Linear Algebra in the Current Matrix Multiplication Time. (arXiv:2107.08090v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">JaeYoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1\">Junyu Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Christy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1\">Farookh Hussain</a>",
          "description": "The high-dimensional or sparse reward task of a reinforcement learning (RL)\nenvironment requires a superior potential controller such as hierarchical\nreinforcement learning (HRL) rather than an atomic RL because it absorbs the\ncomplexity of commands to achieve the purpose of the task in its hierarchical\nstructure. One of the HRL issues is how to train each level policy with the\noptimal data collection from its experience. That is to say, how to synchronize\nadjacent level policies optimally. Our research finds that a HRL model through\nthe off-policy correction technique of HRL, which trains a higher-level policy\nwith the goal of reflecting a lower-level policy which is newly trained using\nthe off-policy method, takes the critical role of synchronizing both level\npolicies at all times while they are being trained. We propose a novel HRL\nmodel supporting the optimal level synchronization using the off-policy\ncorrection technique with a deep generative model. This uses the advantage of\nthe inverse operation of a flow-based deep generative model (FDGM) to achieve\nthe goal corresponding to the current state of the lower-level policy. The\nproposed model also considers the freedom of the goal dimension between HRL\npolicies which makes it the generalized inverse model of the model-free RL in\nHRL with the optimal synchronization method. The comparative experiment results\nshow the performance of our proposed model.",
          "link": "http://arxiv.org/abs/2107.08183",
          "publishedOn": "2021-07-20T02:04:46.690Z",
          "wordCount": null,
          "title": "Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model. (arXiv:2107.08183v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_K/0/1/0/all/0/1\">Kry Yik Chau Lui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Leal_P/0/1/0/all/0/1\">Pablo Hernandez-Leal</a>",
          "description": "Trading markets represent a real-world financial application to deploy\nreinforcement learning agents, however, they carry hard fundamental challenges\nsuch as high variance and costly exploration. Moreover, markets are inherently\na multiagent domain composed of many actors taking actions and changing the\nenvironment. To tackle these type of scenarios agents need to exhibit certain\ncharacteristics such as risk-awareness, robustness to perturbations and low\nlearning variance. We take those as building blocks and propose a family of\nfour algorithms. First, we contribute with two algorithms that use risk-averse\nobjective functions and variance reduction techniques. Then, we augment the\nframework to multi-agent learning and assume an adversary which can take over\nand perturb the learning process. Our third and fourth algorithms perform well\nunder this setting and balance theoretical guarantees with practical use.\nAdditionally, we consider the multi-agent nature of the environment and our\nwork is the first one extending empirical game theory analysis for multi-agent\nlearning by considering risk-sensitive payoffs.",
          "link": "http://arxiv.org/abs/2107.08083",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets. (arXiv:2107.08083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.03024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihao Zhu</a>",
          "description": "We introduce algorithms that achieve state-of-the-art \\emph{dynamic regret}\nbounds for non-stationary linear stochastic bandit setting. It captures natural\napplications such as dynamic pricing and ads allocation in a changing\nenvironment. We show how the difficulty posed by the non-stationarity can be\novercome by a novel marriage between stochastic and adversarial bandits\nlearning algorithms. Defining $d,B_T,$ and $T$ as the problem dimension, the\n\\emph{variation budget}, and the total time horizon, respectively, our main\ncontributions are the tuned Sliding Window UCB (\\texttt{SW-UCB}) algorithm with\noptimal $\\widetilde{O}(d^{2/3}(B_T+1)^{1/3}T^{2/3})$ dynamic regret, and the\ntuning free bandit-over-bandit (\\texttt{BOB}) framework built on top of the\n\\texttt{SW-UCB} algorithm with best\n$\\widetilde{O}(d^{2/3}(B_T+1)^{1/4}T^{3/4})$ dynamic regret.",
          "link": "http://arxiv.org/abs/1810.03024",
          "publishedOn": "2021-07-20T02:04:46.689Z",
          "wordCount": null,
          "title": "Learning to Optimize under Non-Stationarity. (arXiv:1810.03024v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Anderson da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "This works proposes a methodology to searching for automatically Artificial\nNeural Networks (ANN) by using Cellular Genetic Algorithm (CGA). The goal of\nthis methodology is to find compact networks whit good performance for\nclassification problems. The main reason for developing this work is centered\nat the difficulties of configuring compact ANNs with good performance rating.\nThe use of CGAs aims at seeking the components of the RNA in the same way that\na common Genetic Algorithm (GA), but it has the differential of incorporating a\nCellular Automaton (CA) to give location for the GA individuals. The location\nimposed by the CA aims to control the spread of solutions in the populations to\nmaintain the genetic diversity for longer time. This genetic diversity is\nimportant for obtain good results with the GAs.",
          "link": "http://arxiv.org/abs/2107.08326",
          "publishedOn": "2021-07-20T02:04:46.687Z",
          "wordCount": null,
          "title": "Otimizacao de Redes Neurais atraves de Algoritmos Geneticos Celulares. (arXiv:2107.08326v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peiris_V/0/1/0/all/0/1\">Vinesha Peiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhorukova_N/0/1/0/all/0/1\">Nadezda Sukhorukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshchina_V/0/1/0/all/0/1\">Vera Roshchina</a>",
          "description": "We explore the potential for using a nonsmooth loss function based on the\nmax-norm in the training of an artificial neural network. We hypothesise that\nthis may lead to superior classification results in some special cases where\nthe training data is either very small or unbalanced.\n\nOur numerical experiments performed on a simple artificial neural network\nwith no hidden layers (a setting immediately amenable to standard nonsmooth\noptimisation techniques) appear to confirm our hypothesis that uniform\napproximation based approaches may be more suitable for the datasets with\nreliable training data that either is limited size or biased in terms of\nrelative cluster sizes.",
          "link": "http://arxiv.org/abs/2107.08800",
          "publishedOn": "2021-07-20T02:04:46.686Z",
          "wordCount": null,
          "title": "Deep Learning with Nonsmooth Objectives. (arXiv:2107.08800v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rong Pan</a>",
          "description": "Recurrence data arise from multi-disciplinary domains spanning reliability,\ncyber security, healthcare, online retailing, etc. This paper investigates an\nadditive-tree-based approach, known as Boost-R (Boosting for Recurrence Data),\nfor recurrent event data with both static and dynamic features. Boost-R\nconstructs an ensemble of gradient boosted additive trees to estimate the\ncumulative intensity function of the recurrent event process, where a new tree\nis added to the ensemble by minimizing the regularized L2 distance between the\nobserved and predicted cumulative intensity. Unlike conventional regression\ntrees, a time-dependent function is constructed by Boost-R on each tree leaf.\nThe sum of these functions, from multiple trees, yields the ensemble estimator\nof the cumulative intensity. The divide-and-conquer nature of tree-based\nmethods is appealing when hidden sub-populations exist within a heterogeneous\npopulation. The non-parametric nature of regression trees helps to avoid\nparametric assumptions on the complex interactions between event processes and\nfeatures. Critical insights and advantages of Boost-R are investigated through\ncomprehensive numerical examples. Datasets and computer code of Boost-R are\nmade available on GitHub. To our best knowledge, Boost-R is the first gradient\nboosted additive-tree-based approach for modeling large-scale recurrent event\ndata with both static and dynamic feature information.",
          "link": "http://arxiv.org/abs/2107.08784",
          "publishedOn": "2021-07-20T02:04:46.685Z",
          "wordCount": null,
          "title": "Boost-R: Gradient Boosted Trees for Recurrence Data. (arXiv:2107.08784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_C/0/1/0/all/0/1\">Carlos Mougan Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanellos_G/0/1/0/all/0/1\">Georgios Kanellos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottron_T/0/1/0/all/0/1\">Thomas Gottron</a>",
          "description": "Explainable AI constitutes a fundamental step towards establishing fairness\nand addressing bias in algorithmic decision-making. Despite the large body of\nwork on the topic, the benefit of solutions is mostly evaluated from a\nconceptual or theoretical point of view and the usefulness for real-world use\ncases remains uncertain. In this work, we aim to state clear user-centric\ndesiderata for explainable AI reflecting common explainability needs\nexperienced in statistical production systems of the European Central Bank. We\nlink the desiderata to archetypical user roles and give examples of techniques\nand methods which can be used to address the user's needs. To this end, we\nprovide two concrete use cases from the domain of statistical data production\nin central banks: the detection of outliers in the Centralised Securities\nDatabase and the data-driven identification of data quality checks for the\nSupervisory Banking data system.",
          "link": "http://arxiv.org/abs/2107.08045",
          "publishedOn": "2021-07-20T02:04:46.586Z",
          "wordCount": null,
          "title": "Desiderata for Explainable AI in statistical production systems of the European Central Bank. (arXiv:2107.08045v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriani_A/0/1/0/all/0/1\">Amal Feriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>",
          "description": "Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of\nReinforcement Learning due to the non-stationarity of the environments and the\nlarge dimensionality of the combined action space. Deep MARL algorithms have\nbeen applied to solve different task offloading problems. However, in\nreal-world applications, information required by the agents (i.e. rewards and\nstates) are subject to noise and alterations. The stability and the robustness\nof deep MARL to practical challenges is still an open research problem. In this\nwork, we apply state-of-the art MARL algorithms to solve task offloading with\nreward uncertainty. We show that perturbations in the reward signal can induce\ndecrease in the performance compared to learning with perfect rewards. We\nexpect this paper to stimulate more research in studying and addressing the\npractical challenges of deploying deep MARL solutions in wireless\ncommunications systems.",
          "link": "http://arxiv.org/abs/2107.08114",
          "publishedOn": "2021-07-20T02:04:46.584Z",
          "wordCount": null,
          "title": "Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. (arXiv:2107.08114v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guevara_J/0/1/0/all/0/1\">Jorge Guevara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_D/0/1/0/all/0/1\">Dario Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1\">Campbell Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1\">Bianca Zadrozny</a>",
          "description": "Future climate change scenarios are usually hypothesized using simulations\nfrom weather generators. However, there only a few works comparing and\nevaluating promising deep learning models for weather generation against\nclassical approaches. This study shows preliminary results making such\nevaluations for the multisite precipitation synthesis task. We compared two\nopen-source weather generators: IBMWeathergen (an extension of the Weathergen\nlibrary) and RGeneratePrec, and two deep generative models: GAN and VAE, on a\nvariety of metrics. Our preliminary results can serve as a guide for improving\nthe design of deep learning architectures and algorithms for the multisite\nprecipitation synthesis task.",
          "link": "http://arxiv.org/abs/2107.08074",
          "publishedOn": "2021-07-20T02:04:46.581Z",
          "wordCount": null,
          "title": "A comparative study of stochastic and deep generative models for multisite precipitation synthesis. (arXiv:2107.08074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_E/0/1/0/all/0/1\">Eva Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1\">Martin Zaefferer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mersmann_O/0/1/0/all/0/1\">Olaf Mersmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "Machine learning algorithms such as random forests or xgboost are gaining\nmore importance and are increasingly incorporated into production processes in\norder to enable comprehensive digitization and, if possible, automation of\nprocesses. Hyperparameters of these algorithms used have to be set\nappropriately, which can be referred to as hyperparameter tuning or\noptimization. Based on the concept of tunability, this article presents an\noverview of theoretical and practical results for popular machine learning\nalgorithms. This overview is accompanied by an experimental analysis of 30\nhyperparameters from six relevant machine learning algorithms. In particular,\nit provides (i) a survey of important hyperparameters, (ii) two parameter\ntuning studies, and (iii) one extensive global parameter tuning study, as well\nas (iv) a new way, based on consensus ranking, to analyze results from multiple\nalgorithms. The R package mlr is used as a uniform interface to the machine\nlearning models. The R package SPOT is used to perform the actual tuning\n(optimization). All additional code is provided together with this paper.",
          "link": "http://arxiv.org/abs/2107.08761",
          "publishedOn": "2021-07-20T02:04:46.575Z",
          "wordCount": null,
          "title": "Experimental Investigation and Evaluation of Model-based Hyperparameter Optimization. (arXiv:2107.08761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08751",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.",
          "link": "http://arxiv.org/abs/2107.08751",
          "publishedOn": "2021-07-20T02:04:46.574Z",
          "wordCount": null,
          "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation. (arXiv:2107.08751v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Soumyadeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_J/0/1/0/all/0/1\">Janu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Awanish Kumar</a>",
          "description": "In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.",
          "link": "http://arxiv.org/abs/2107.08211",
          "publishedOn": "2021-07-20T02:04:46.557Z",
          "wordCount": null,
          "title": "Self Training with Ensemble of Teacher Models. (arXiv:2107.08211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yao Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haipeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.",
          "link": "http://arxiv.org/abs/2107.08356",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "DeHumor: Visual Analytics for Decomposing Humor. (arXiv:2107.08356v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1\">Michael Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>",
          "description": "In its most elementary form, compressed sensing studies the design of\ndecoding algorithms to recover a sufficiently sparse vector or code from a\nlower dimensional linear measurement vector. Typically it is assumed that the\ndecoder has access to the encoder matrix, which in the combinatorial case is\nsparse and binary. In this paper we consider the problem of designing a decoder\nto recover a set of sparse codes from their linear measurements alone, that is\nwithout access to encoder matrix. To this end we study the matrix factorisation\ntask of recovering both the encoder and sparse coding matrices from the\nassociated linear measurement matrix. The contribution of this paper is a\ncomputationally efficient decoding algorithm, Decoder-Expander Based\nFactorisation, with strong performance guarantees. In particular, under mild\nassumptions on the sparse coding matrix and by deploying a novel random encoder\nmatrix, we prove that Decoder-Expander Based Factorisation recovers both the\nencoder and sparse coding matrix at the optimal measurement rate with high\nprobability and from a near optimal number of measurement vectors. In addition,\nour experiments demonstrate the efficacy and computational efficiency of our\nalgorithm in practice. Beyond compressed sensing our results may be of interest\nfor researchers working in areas such as linear sketching, coding theory and\nmatrix compression.",
          "link": "http://arxiv.org/abs/2004.05094",
          "publishedOn": "2021-07-20T02:04:46.339Z",
          "wordCount": null,
          "title": "Encoder blind combinatorial compressed sensing. (arXiv:2004.05094v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chirag Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya K. Ramdas</a>",
          "description": "We study the problem of post-hoc calibration for multiclass classification,\nwith an emphasis on histogram binning. Multiple works have focused on\ncalibration with respect to the confidence of just the predicted class (or\n'top-label'). We find that the popular notion of confidence calibration [Guo et\nal., 2017] is not sufficiently strong -- there exist predictors that are not\ncalibrated in any meaningful way but are perfectly confidence calibrated. We\npropose a closely related (but subtly different) notion, top-label calibration,\nthat accurately captures the intuition and simplicity of confidence\ncalibration, but addresses its drawbacks. We formalize a histogram binning (HB)\nalgorithm that reduces top-label multiclass calibration to the binary case,\nprove that it has clean theoretical guarantees without distributional\nassumptions, and perform a methodical study of its practical performance. Some\nprediction tasks require stricter notions of multiclass calibration such as\nclass-wise or canonical calibration. We formalize appropriate HB algorithms\ncorresponding to each of these goals. In experiments with deep neural nets, we\nfind that our principled versions of HB are often better than temperature\nscaling, for both top-label and class-wise calibration. Code for this work will\nbe made publicly available at https://github.com/aigen/df-posthoc-calibration.",
          "link": "http://arxiv.org/abs/2107.08353",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Top-label calibration. (arXiv:2107.08353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11830",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Richter_L/0/1/0/all/0/1\">Lorenz Richter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sallandt_L/0/1/0/all/0/1\">Leon Sallandt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nusken_N/0/1/0/all/0/1\">Nikolas N&#xfc;sken</a>",
          "description": "High-dimensional partial differential equations (PDEs) are ubiquitous in\neconomics, science and engineering. However, their numerical treatment poses\nformidable challenges since traditional grid-based methods tend to be\nfrustrated by the curse of dimensionality. In this paper, we argue that tensor\ntrains provide an appealing approximation framework for parabolic PDEs: the\ncombination of reformulations in terms of backward stochastic differential\nequations and regression-type methods in the tensor format holds the promise of\nleveraging latent low-rank structures enabling both compression and efficient\ncomputation. Following this paradigm, we develop novel iterative schemes,\ninvolving either explicit and fast or implicit and accurate updates. We\ndemonstrate in a number of examples that our methods achieve a favorable\ntrade-off between accuracy and computational efficiency in comparison with\nstate-of-the-art neural network based approaches.",
          "link": "http://arxiv.org/abs/2102.11830",
          "publishedOn": "2021-07-20T02:04:46.338Z",
          "wordCount": null,
          "title": "Solving high-dimensional parabolic PDEs using the tensor train format. (arXiv:2102.11830v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12056",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1\">Changfa Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiancheng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Da Cheng</a>",
          "description": "Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2102.12056",
          "publishedOn": "2021-07-20T02:04:46.337Z",
          "wordCount": null,
          "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1\">William Blanzeisky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1\">P&#xe1;draig Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1\">Kenneth Kennedy</a>",
          "description": "A significant impediment to progress in research on bias in machine learning\n(ML) is the availability of relevant datasets. This situation is unlikely to\nchange much given the sensitivity of such data. For this reason, there is a\nrole for synthetic data in this research. In this short paper, we present one\nsuch family of synthetic data sets. We provide an overview of the data,\ndescribe how the level of bias can be varied, and present a simple example of\nan experiment on the data.",
          "link": "http://arxiv.org/abs/2107.08928",
          "publishedOn": "2021-07-20T02:04:46.336Z",
          "wordCount": null,
          "title": "Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hengguan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>",
          "description": "Perception of time from sequentially acquired sensory inputs is rooted in\neveryday behaviors of individual organisms. Yet, most algorithms for\ntime-series modeling fail to learn dynamics of random event timings directly\nfrom visual or audio inputs, requiring timing annotations during training that\nare usually unavailable for real-world applications. For instance, neuroscience\nperspectives on postdiction imply that there exist variable temporal ranges\nwithin which the incoming sensory inputs can affect the earlier perception, but\nsuch temporal ranges are mostly unannotated for real applications such as\nautomatic speech recognition (ASR). In this paper, we present a probabilistic\nordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),\nthat learns both the timings and the dynamics of time series data without\nrequiring any timing annotations during training. STRODE allows the usage of\ndifferential equations to sample from the posterior point processes,\nefficiently and analytically. We further provide theoretical guarantees on the\nlearning of STRODE. Our empirical results show that our approach successfully\ninfers event timings of time series data. Our method achieves competitive or\nsuperior performances compared to existing state-of-the-art methods for both\nsynthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2107.08273",
          "publishedOn": "2021-07-20T02:04:46.247Z",
          "wordCount": null,
          "title": "STRODE: Stochastic Boundary Ordinary Differential Equation. (arXiv:2107.08273v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.",
          "link": "http://arxiv.org/abs/2107.08264",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhe Yu</a>",
          "description": "This paper aims to improve machine learning fairness on multiple protected\nat-tributes. Machine learning fairness has attracted increasing attention since\nmachine learning models are increasingly used for high-stakes and high-risk\ndecisions. Most existing solutions for machine learning fairness only target\none protected attribute(e.g. sex) at a time. These solutions cannot generate a\nmachine learning model which is fair against every protected attribute (e.g.\nboth sex and race) at the same time. To solve this problem, we propose\nFairBalance in this paper to balance the distribution of training data across\nevery protected attribute before training the machine learning models. Our\nresults show that, under the assumption of unbiased ground truth labels,\nFairBalance can significantly reduce bias metrics (AOD, EOD, and SPD) on every\nknown protected attribute without much, if not any damage to the prediction\nperformance.",
          "link": "http://arxiv.org/abs/2107.08310",
          "publishedOn": "2021-07-20T02:04:46.210Z",
          "wordCount": null,
          "title": "Fair Balance: Mitigating Machine Learning Bias Against Multiple Protected Attributes With Data Balancing. (arXiv:2107.08310v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haibo Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>",
          "description": "Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.",
          "link": "http://arxiv.org/abs/2101.11296",
          "publishedOn": "2021-07-20T02:04:46.209Z",
          "wordCount": null,
          "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity. (arXiv:2101.11296v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1\">Takeshi Teshima</a>",
          "description": "Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.",
          "link": "http://arxiv.org/abs/2006.06979",
          "publishedOn": "2021-07-20T02:04:46.206Z",
          "wordCount": null,
          "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation. (arXiv:2006.06979v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1\">Jary Pomponi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1\">Aurelio Uncini</a>",
          "description": "Catastrophic forgetting (CF) happens whenever a neural network overwrites\npast knowledge while being trained on new tasks. Common techniques to handle CF\ninclude regularization of the weights (using, e.g., their importance on past\ntasks), and rehearsal strategies, where the network is constantly re-trained on\npast data. Generative models have also been applied for the latter, in order to\nhave endless sources of data. In this paper, we propose a novel method that\ncombines the strengths of regularization and generative-based rehearsal\napproaches. Our generative model consists of a normalizing flow (NF), a\nprobabilistic and invertible neural network, trained on the internal embeddings\nof the network. By keeping a single NF conditioned on the task, we show that\nour memory overhead remains constant. In addition, exploiting the invertibility\nof the NF, we propose a simple approach to regularize the network's embeddings\nwith respect to past tasks. We show that our method performs favorably with\nrespect to state-of-the-art approaches in the literature, with bounded\ncomputational power and memory overheads.",
          "link": "http://arxiv.org/abs/2007.02443",
          "publishedOn": "2021-07-20T02:04:46.205Z",
          "wordCount": null,
          "title": "Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tasche_D/0/1/0/all/0/1\">Dirk Tasche</a>",
          "description": "For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.",
          "link": "http://arxiv.org/abs/2107.08209",
          "publishedOn": "2021-07-20T02:04:46.127Z",
          "wordCount": null,
          "title": "Minimising quantifier variance under prior probability shift. (arXiv:2107.08209v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xianghao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rumeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xiaoqing Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">You Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>",
          "description": "Electronic nose has been proven to be effective in alternative herbal\nmedicine classification, but due to the nature of supervised learning, previous\nresearch heavily relies on the labelled training data, which are time-costly\nand labor-intensive to collect. To alleviate the critical dependency on the\ntraining data in real-world applications, this study aims to improve\nclassification accuracy via data augmentation strategies. The effectiveness of\nfive data augmentation strategies under different training data inadequacy are\ninvestigated in two scenarios: the noise-free scenario where different\navailabilities of unlabelled data were considered, and the noisy scenario where\ndifferent levels of Gaussian noises and translational shifts were added to\nrepresent sensor drifts. The five augmentation strategies, namely noise-adding\ndata augmentation, semi-supervised learning, classifier-based online learning,\nInductive Conformal Prediction (ICP) online learning and our novel ensemble ICP\nonline learning proposed in this study, are experimented and compared against\nsupervised learning baseline, with Linear Discriminant Analysis (LDA) and\nSupport Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble\nICP online learning, outperforms the others by showing non-decreasing\nclassification accuracy on all tasks and a significant improvement on most\nsimulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a\nsystematic analysis of different augmentation strategies. It shows at least one\nstrategy significantly improved the classification accuracy with LDA (p<=0.05)\nand non-decreasing classification accuracy with SVM in each task. In\nparticular, our proposed strategy demonstrated both effectiveness and\nrobustness in boosting the classification model generalizability, which can be\nemployed in other machine learning applications.",
          "link": "http://arxiv.org/abs/2102.03088",
          "publishedOn": "2021-07-20T02:04:46.115Z",
          "wordCount": null,
          "title": "Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose. (arXiv:2102.03088v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12301",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1\">Tianyi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zheng_Z/0/1/0/all/0/1\">Zeyu Zheng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1\">Elynn Y. Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Optimal transport (OT) distances are increasingly used as loss functions for\nstatistical inference, notably in the learning of generative models or\nsupervised learning. Yet, the behavior of minimum Wasserstein estimators is\npoorly understood, notably in high-dimensional regimes or under model\nmisspecification. In this work we adopt the viewpoint of projection robust (PR)\nOT, which seeks to maximize the OT cost between two measures by choosing a\n$k$-dimensional subspace onto which they can be projected. Our first\ncontribution is to establish several fundamental statistical properties of PR\nWasserstein distances, complementing and improving previous literature that has\nbeen restricted to one-dimensional and well-specified cases. Next, we propose\nthe integral PR Wasserstein (IPRW) distance as an alternative to the PRW\ndistance, by averaging rather than optimizing on subspaces. Our complexity\nbounds can help explain why both PRW and IPRW distances outperform Wasserstein\ndistances empirically in high-dimensional inference tasks. Finally, we consider\nparametric inference using the PRW distance. We provide an asymptotic guarantee\nof two types of minimum PRW estimators and formulate a central limit theorem\nfor max-sliced Wasserstein estimator under model misspecification. To enable\nour analysis on PRW with projection dimension larger than one, we devise a\nnovel combination of variational analysis and statistical theory.",
          "link": "http://arxiv.org/abs/2006.12301",
          "publishedOn": "2021-07-20T02:04:46.114Z",
          "wordCount": null,
          "title": "On Projection Robust Optimal Transport: Sample Complexity and Model Misspecification. (arXiv:2006.12301v5 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>",
          "description": "Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.",
          "link": "http://arxiv.org/abs/2107.08142",
          "publishedOn": "2021-07-20T02:04:46.113Z",
          "wordCount": null,
          "title": "Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08179",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Birmpa_P/0/1/0/all/0/1\">Panagiota Birmpa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1\">Jinchao Feng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katsoulakis_M/0/1/0/all/0/1\">Markos A. Katsoulakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_Bellet_L/0/1/0/all/0/1\">Luc Rey-Bellet</a>",
          "description": "Probabilistic graphical models are a fundamental tool in probabilistic\nmodeling, machine learning and artificial intelligence. They allow us to\nintegrate in a natural way expert knowledge, physical modeling, heterogeneous\nand correlated data and quantities of interest. For exactly this reason,\nmultiple sources of model uncertainty are inherent within the modular structure\nof the graphical model. In this paper we develop information-theoretic, robust\nuncertainty quantification methods and non-parametric stress tests for directed\ngraphical models to assess the effect and the propagation through the graph of\nmulti-sourced model uncertainties to quantities of interest. These methods\nallow us to rank the different sources of uncertainty and correct the graphical\nmodel by targeting its most impactful components with respect to the quantities\nof interest. Thus, from a machine learning perspective, we provide a\nmathematically rigorous approach to correctability that guarantees a systematic\nselection for improvement of components of a graphical model while controlling\npotential new errors created in the process in other parts of the model. We\ndemonstrate our methods in two physico-chemical examples, namely quantum\nscale-informed chemical kinetics and materials screening to improve the\nefficiency of fuel cells.",
          "link": "http://arxiv.org/abs/2107.08179",
          "publishedOn": "2021-07-20T02:04:46.106Z",
          "wordCount": null,
          "title": "Model Uncertainty and Correctability for Directed Graphical Models. (arXiv:2107.08179v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samo_Y/0/1/0/all/0/1\">Yves-Laurent Kom Samo</a>",
          "description": "We introduce the first application of the lean methodology to machine\nlearning projects. Similar to lean startups and lean manufacturing, we argue\nthat lean machine learning (LeanML) can drastically slash avoidable wastes in\ncommercial machine learning projects, reduce the business risk in investing in\nmachine learning capabilities and, in so doing, further democratize access to\nmachine learning. The lean design pattern we propose in this paper is based on\ntwo realizations. First, it is possible to estimate the best performance one\nmay achieve when predicting an outcome $y \\in \\mathcal{Y}$ using a given set of\nexplanatory variables $x \\in \\mathcal{X}$, for a wide range of performance\nmetrics, and without training any predictive model. Second, doing so is\nconsiderably easier, faster, and cheaper than learning the best predictive\nmodel. We derive formulae expressing the best $R^2$, MSE, classification\naccuracy, and log-likelihood per observation achievable when using $x$ to\npredict $y$ as a function of the mutual information $I\\left(y; x\\right)$, and\npossibly a measure of the variability of $y$ (e.g. its Shannon entropy in the\ncase of classification accuracy, and its variance in the case regression MSE).\nWe illustrate the efficacy of the LeanML design pattern on a wide range of\nregression and classification problems, synthetic and real-life.",
          "link": "http://arxiv.org/abs/2107.08066",
          "publishedOn": "2021-07-20T02:04:46.095Z",
          "wordCount": null,
          "title": "LeanML: A Design Pattern To Slash Avoidable Wastes in Machine Learning Projects. (arXiv:2107.08066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">J. G. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gluzman_M/0/1/0/all/0/1\">Mark Gluzman</a>",
          "description": "The policy improvement bound on the difference of the discounted returns\nplays a crucial role in the theoretical justification of the trust-region\npolicy optimization (TRPO) algorithm. The existing bound leads to a degenerate\nbound when the discount factor approaches one, making the applicability of TRPO\nand related algorithms questionable when the discount factor is close to one.\nWe refine the results in \\cite{Schulman2015, Achiam2017} and propose a novel\nbound that is \"continuous\" in the discount factor. In particular, our bound is\napplicable for MDPs with the long-run average rewards as well.",
          "link": "http://arxiv.org/abs/2107.08068",
          "publishedOn": "2021-07-20T02:04:46.094Z",
          "wordCount": null,
          "title": "Refined Policy Improvement Bounds for MDPs. (arXiv:2107.08068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holderrieth_P/0/1/0/all/0/1\">Peter Holderrieth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1\">Michael Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Motivated by objects such as electric fields or fluid streams, we study the\nproblem of learning stochastic fields, i.e. stochastic processes whose samples\nare fields like those occurring in physics and engineering. Considering general\ntransformations such as rotations and reflections, we show that spatial\ninvariance of stochastic fields requires an inference model to be equivariant.\nLeveraging recent advances from the equivariance literature, we study\nequivariance in two classes of models. Firstly, we fully characterise\nequivariant Gaussian processes. Secondly, we introduce Steerable Conditional\nNeural Processes (SteerCNPs), a new, fully equivariant member of the Neural\nProcess family. In experiments with Gaussian process vector fields, images, and\nreal-world weather data, we observe that SteerCNPs significantly improve the\nperformance of previous models and equivariance leads to improvements in\ntransfer learning tasks.",
          "link": "http://arxiv.org/abs/2011.12916",
          "publishedOn": "2021-07-20T02:04:45.372Z",
          "wordCount": null,
          "title": "Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes. (arXiv:2011.12916v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tengyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingbin Liang</a>",
          "description": "Designing off-policy reinforcement learning algorithms is typically a very\nchallenging task, because a desirable iteration update often involves an\nexpectation over an on-policy distribution. Prior off-policy actor-critic (AC)\nalgorithms have introduced a new critic that uses the density ratio for\nadjusting the distribution mismatch in order to stabilize the convergence, but\nat the cost of potentially introducing high biases due to the estimation errors\nof both the density ratio and value function. In this paper, we develop a\ndoubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take\nadvantage of learned nuisance functions to reduce estimation errors. Moreover,\nDR-Off-PAC adopts a single timescale structure, in which both actor and critics\nare updated simultaneously with constant stepsize, and is thus more sample\nefficient than prior algorithms that adopt either two timescale or nested-loop\nstructure. We study the finite-time convergence rate and characterize the\nsample complexity for DR-Off-PAC to attain an $\\epsilon$-accurate optimal\npolicy. We also show that the overall convergence of DR-Off-PAC is doubly\nrobust to the approximation errors that depend only on the expressive power of\napproximation functions. To the best of our knowledge, our study establishes\nthe first overall sample complexity analysis for a single time-scale off-policy\nAC algorithm.",
          "link": "http://arxiv.org/abs/2102.11866",
          "publishedOn": "2021-07-20T02:04:45.244Z",
          "wordCount": null,
          "title": "Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality. (arXiv:2102.11866v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1\">Maksim E. Eren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solovyev_N/0/1/0/all/0/1\">Nick Solovyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamer_C/0/1/0/all/0/1\">Chris Hamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Renee McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1\">Boian S. Alexandrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>",
          "description": "The unprecedented outbreak of Severe Acute Respiratory Syndrome Coronavirus-2\n(SARS-CoV-2), or COVID-19, continues to be a significant worldwide problem. As\na result, a surge of new COVID-19 related research has followed suit. The\ngrowing number of publications requires document organization methods to\nidentify relevant information. In this paper, we expand upon our previous work\nwith clustering the CORD-19 dataset by applying multi-dimensional analysis\nmethods. Tensor factorization is a powerful unsupervised learning method\ncapable of discovering hidden patterns in a document corpus. We show that a\nhigher-order representation of the corpus allows for the simultaneous grouping\nof similar articles, relevant journals, authors with similar research\ninterests, and topic keywords. These groupings are identified within and among\nthe latent components extracted via tensor decomposition. We further\ndemonstrate the application of this method with a publicly available\ninteractive visualization of the dataset.",
          "link": "http://arxiv.org/abs/2107.08190",
          "publishedOn": "2021-07-20T02:04:44.973Z",
          "wordCount": null,
          "title": "COVID-19 Multidimensional Kaggle Literature Organization. (arXiv:2107.08190v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chrostoforidis_A/0/1/0/all/0/1\">Aristeidis Chrostoforidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriakides_G/0/1/0/all/0/1\">George Kyriakides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margaritis_K/0/1/0/all/0/1\">Konstantinos Margaritis</a>",
          "description": "In this work, we propose a novel evolutionary algorithm for neural\narchitecture search, applicable to global search spaces. The algorithm's\narchitectural representation organizes the topology in multiple hierarchical\nmodules, while the design process exploits this representation, in order to\nexplore the search space. We also employ a curation system, which promotes the\nutilization of well performing sub-structures to subsequent generations. We\napply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of\n$93.2\\%$ and $94.8\\%$ respectively in a relatively small number of generations.",
          "link": "http://arxiv.org/abs/2107.08484",
          "publishedOn": "2021-07-20T02:04:44.663Z",
          "wordCount": 519,
          "title": "A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08649",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lim_D/0/1/0/all/0/1\">Dong-Young Lim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sabanis_S/0/1/0/all/0/1\">Sotirios Sabanis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>",
          "description": "We consider non-convex stochastic optimization problems where the objective\nfunctions have super-linearly growing and discontinuous stochastic gradients.\nIn such a setting, we provide a non-asymptotic analysis for the tamed\nunadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al.\n(2021). In particular, we establish non-asymptotic error bounds for the TUSLA\nalgorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result\nenables us to further derive non-asymptotic estimates for the expected excess\nrisk. To illustrate the applicability of the main results, we consider an\nexample from transfer learning with ReLU neural networks, which represents a\nkey paradigm in machine learning. Numerical experiments are presented for the\naforementioned example which supports our theoretical findings. Hence, in this\nsetting, we demonstrate both theoretically and numerically that the TUSLA\nalgorithm can solve the optimization problem involving neural networks with\nReLU activation function. Besides, we provide simulation results for synthetic\nexamples where popular algorithms, e.g. ADAM, AMSGrad, RMSProp, and (vanilla)\nSGD, may fail to find the minimizer of the objective functions due to the\nsuper-linear growth and the discontinuity of the corresponding stochastic\ngradient, while the TUSLA algorithm converges rapidly to the optimal solution.",
          "link": "http://arxiv.org/abs/2107.08649",
          "publishedOn": "2021-07-20T02:04:44.607Z",
          "wordCount": 655,
          "title": "Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huafeng Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chonggang Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhimin Hu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaodong Yuan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingshu Zhang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanquan Liu</a> (3) ((1) School of Information, North China University of Technology,(2) Department of Neurology, Kailuan General Hospital, Tangshan,(3) School of Intelligent Systems Engineering, Sun Yat-sen University)",
          "description": "Sleep staging assumes an important role in the diagnosis of sleep disorders.\nIn general, experts classify sleep stages manually based on polysomnography\n(PSG), which is quite time-consuming. Meanwhile, the acquisition of multiple\nsignals is complex, which can affect the subject's sleep. Therefore, the use of\nsingle-channel electroencephalogram (EEG) for automatic sleep staging has\nbecome mainstream. In the literature, a large number of sleep staging methods\nbased on single-channel EEG have been proposed with good results and realize\nthe preliminary automation of sleep staging. However, the performance for most\nof these methods in the N1 stage is generally not high. In this paper, we\npropose a deep learning model SDAN based on raw EEG. The method utilises a\none-dimensional convolutional neural network (CNN) to automatically extract\nfeatures from raw EEG. It serially combines the channel attention and spatial\nattention mechanisms to filter and highlight key information and then uses soft\nthreshold to eliminate redundant information. Additionally, we introduce a\nresidual network to avoid degradation problems caused by network deepening.\nExperiments were conducted using two datasets with 5-fold cross-validation and\nhold-out validation method. The final average accuracy, overall accuracy, macro\nF1 score and Cohen's Kappa coefficient of the model reach 96.74%, 91.86%,\n82.64% and 0.8742 on the Sleep-EDF dataset, and 95.98%, 89.96%, 79.08% and\n0.8216 on the Sleep-EDFx dataset. Significantly, our model performed superiorly\nin the N1 stage, with F1 scores of 54.08% and 52.49% on the two datasets\nrespectively. The results show the superiority of our network over the best\nexisting methods, reaching a new state-of-the-art. In particular, the present\nmethod achieves excellent results in the N1 sleep stage compared to other\nmethods.",
          "link": "http://arxiv.org/abs/2107.08442",
          "publishedOn": "2021-07-20T02:04:44.589Z",
          "wordCount": 744,
          "title": "Sleep Staging Based on Serialized Dual Attention Network. (arXiv:2107.08442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mrabah_N/0/1/0/all/0/1\">Nairouz Mrabah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touati_M/0/1/0/all/0/1\">Mohamed Fawzi Touati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ksantini_R/0/1/0/all/0/1\">Riadh Ksantini</a>",
          "description": "Most recent graph clustering methods have resorted to Graph Auto-Encoders\n(GAEs) to perform joint clustering and embedding learning. However, two\ncritical issues have been overlooked. First, the accumulative error, inflicted\nby learning with noisy clustering assignments, degrades the effectiveness and\nrobustness of the clustering model. This problem is called Feature Randomness.\nSecond, reconstructing the adjacency matrix sets the model to learn irrelevant\nsimilarities for the clustering task. This problem is called Feature Drift.\nInterestingly, the theoretical relation between the aforementioned problems has\nnot yet been investigated. We study these issues from two aspects: (1) the\nexistence of a trade-off between Feature Randomness and Feature Drift when\nclustering and reconstruction are performed at the same level, and (2) the\nproblem of Feature Drift is more pronounced for GAE models, compared with\nvanilla auto-encoder models, due to the graph convolutional operation and the\ngraph decoding design. Motivated by these findings, we reformulate the\nGAE-based clustering methodology. Our solution is two-fold. First, we propose a\nsampling operator $\\Xi$ that triggers a protection mechanism against the noisy\nclustering assignments. Second, we propose an operator $\\Upsilon$ that triggers\na correction mechanism against Feature Drift by gradually transforming the\nreconstructed graph into a clustering-oriented one. As principal advantages,\nour solution grants a considerable improvement in clustering effectiveness and\nrobustness and can be easily tailored to existing GAE models.",
          "link": "http://arxiv.org/abs/2107.08562",
          "publishedOn": "2021-07-20T02:04:44.571Z",
          "wordCount": 653,
          "title": "Rethinking Graph Autoencoder Models for Attributed Graph Clustering. (arXiv:2107.08562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08593",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1\">Yiran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>",
          "description": "In this work, we use an explainable convolutional neural network (NLS-Net) to\nsolve an inverse problem of the nonlinear Schr\\\"odinger equation, which is\nwidely used in fiber-optic communications. The landscape and minimizers of the\nnon-convex loss function of the learning problem are studied empirically. It\nprovides a guidance for choosing hyper-parameters of the method. The estimation\nerror of the optimal solution is discussed in terms of expressive power of the\nNLS-Net and data. Besides, we compare the performance of several training\nalgorithms that are popular in deep learning. It is shown that one can obtain a\nrelatively accurate estimate of the considered parameters using the proposed\nmethod. The study provides a natural framework of solving inverse problems of\nnonlinear partial differential equations with deep learning.",
          "link": "http://arxiv.org/abs/2107.08593",
          "publishedOn": "2021-07-20T02:04:44.554Z",
          "wordCount": 566,
          "title": "Inverse Problem of Nonlinear Schr\\\"odinger Equation as Learning of Convolutional Neural Network. (arXiv:2107.08593v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiabao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuemin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Libin Zheng</a>",
          "description": "With the rapid development of smart mobile devices, the car-hailing platforms\n(e.g., Uber or Lyft) have attracted much attention from both the academia and\nthe industry. In this paper, we consider an important dynamic car-hailing\nproblem, namely \\textit{maximum revenue vehicle dispatching} (MRVD), in which\nrider requests dynamically arrive and drivers need to serve as many riders as\npossible such that the entire revenue of the platform is maximized. We prove\nthat the MRVD problem is NP-hard and intractable. In addition, the dynamic\ncar-hailing platforms have no information of the future riders, which makes the\nproblem even harder. To handle the MRVD problem, we propose a queueing-based\nvehicle dispatching framework, which first uses existing machine learning\nalgorithms to predict the future vehicle demand of each region, then estimates\nthe idle time periods of drivers through a queueing model for each region. With\nthe information of the predicted vehicle demands and estimated idle time\nperiods of drivers, we propose two batch-based vehicle dispatching algorithms\nto efficiently assign suitable drivers to riders such that the expected overall\nrevenue of the platform is maximized during each batch processing. Through\nextensive experiments, we demonstrate the efficiency and effectiveness of our\nproposed approaches over both real and synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.08662",
          "publishedOn": "2021-07-20T02:04:44.536Z",
          "wordCount": 649,
          "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. (arXiv:2107.08662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momeni_A/0/1/0/all/0/1\">Ali Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleury_R/0/1/0/all/0/1\">Romain Fleury</a>",
          "description": "Wave-based analog signal processing holds the promise of extremely fast,\non-the-fly, power-efficient data processing, occurring as a wave propagates\nthrough an artificially engineered medium. Yet, due to the fundamentally weak\nnon-linearities of traditional wave materials, such analog processors have been\nso far largely confined to simple linear projections such as image edge\ndetection or matrix multiplications. Complex neuromorphic computing tasks,\nwhich inherently require strong non-linearities, have so far remained\nout-of-reach of wave-based solutions, with a few attempts that implemented\nnon-linearities on the digital front, or used weak and inflexible non-linear\nsensors, restraining the learning performance. Here, we tackle this issue by\ndemonstrating the relevance of Time-Floquet physics to induce a strong\nnon-linear entanglement between signal inputs at different frequencies,\nenabling a power-efficient and versatile wave platform for analog extreme deep\nlearning involving a single, uniformly modulated dielectric layer and a\nscattering medium. We prove the efficiency of the method for extreme learning\nmachines and reservoir computing to solve a range of challenging learning\ntasks, from forecasting chaotic time series to the simultaneous classification\nof distinct datasets. Our results open the way for wave-based machine learning\nwith high energy efficiency, speed, and scalability.",
          "link": "http://arxiv.org/abs/2107.08564",
          "publishedOn": "2021-07-20T02:04:44.473Z",
          "wordCount": 643,
          "title": "Wave-based extreme deep learning based on non-linear time-Floquet entanglement. (arXiv:2107.08564v1 [cs.ET])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podda_M/0/1/0/all/0/1\">Marco Podda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "The problem of labeled graph generation is gaining attention in the Deep\nLearning community. The task is challenging due to the sparse and discrete\nnature of graph spaces. Several approaches have been proposed in the\nliterature, most of which require to transform the graphs into sequences that\nencode their structure and labels and to learn the distribution of such\nsequences through an auto-regressive generative model. Among this family of\napproaches, we focus on the GraphGen model. The preprocessing phase of GraphGen\ntransforms graphs into unique edge sequences called Depth-First Search (DFS)\ncodes, such that two isomorphic graphs are assigned the same DFS code. Each\nelement of a DFS code is associated with a graph edge: specifically, it is a\nquintuple comprising one node identifier for each of the two endpoints, their\nnode labels, and the edge label. GraphGen learns to generate such sequences\nauto-regressively and models the probability of each component of the quintuple\nindependently. While effective, the independence assumption made by the model\nis too loose to capture the complex label dependencies of real-world graphs\nprecisely. By introducing a novel graph preprocessing approach, we are able to\nprocess the labeling information of both nodes and edges jointly. The\ncorresponding model, which we term GraphGen-Redux, improves upon the generative\nperformances of GraphGen in a wide range of datasets of chemical and social\ngraphs. In addition, it uses approximately 78% fewer parameters than the\nvanilla variant and requires 50% fewer epochs of training on average.",
          "link": "http://arxiv.org/abs/2107.08396",
          "publishedOn": "2021-07-20T02:04:44.455Z",
          "wordCount": 677,
          "title": "GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation. (arXiv:2107.08396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alon_N/0/1/0/all/0/1\">Noga Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzman_R/0/1/0/all/0/1\">Ron Holzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We extend the theory of PAC learning in a way which allows to model a rich\nvariety of learning tasks where the data satisfy special properties that ease\nthe learning process. For example, tasks where the distance of the data from\nthe decision boundary is bounded away from zero. The basic and simple idea is\nto consider partial concepts: these are functions that can be undefined on\ncertain parts of the space. When learning a partial concept, we assume that the\nsource distribution is supported only on points where the partial concept is\ndefined.\n\nThis way, one can naturally express assumptions on the data such as lying on\na lower dimensional surface or margin conditions. In contrast, it is not at all\nclear that such assumptions can be expressed by the traditional PAC theory. In\nfact we exhibit easy-to-learn partial concept classes which provably cannot be\ncaptured by the traditional PAC theory. This also resolves a question posed by\nAttias, Kontorovich, and Mansour 2019.\n\nWe characterize PAC learnability of partial concept classes and reveal an\nalgorithmic landscape which is fundamentally different than the classical one.\nFor example, in the classical PAC model, learning boils down to Empirical Risk\nMinimization (ERM). In stark contrast, we show that the ERM principle fails in\nexplaining learnability of partial concept classes. In fact, we demonstrate\nclasses that are incredibly easy to learn, but such that any algorithm that\nlearns them must use an hypothesis space with unbounded VC dimension. We also\nfind that the sample compression conjecture fails in this setting.\n\nThus, this theory features problems that cannot be represented nor solved in\nthe traditional way. We view this as evidence that it might provide insights on\nthe nature of learnability in realistic scenarios which the classical theory\nfails to explain.",
          "link": "http://arxiv.org/abs/2107.08444",
          "publishedOn": "2021-07-20T02:04:44.435Z",
          "wordCount": 746,
          "title": "A Theory of PAC Learnability of Partial Concept Classes. (arXiv:2107.08444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jacek Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_J/0/1/0/all/0/1\">Jakub Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraskiewicz_W/0/1/0/all/0/1\">Witold Kraskiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topolewski_M/0/1/0/all/0/1\">Mateusz Topolewski</a>",
          "description": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT data set.",
          "link": "http://arxiv.org/abs/2107.08687",
          "publishedOn": "2021-07-20T02:04:44.416Z",
          "wordCount": 478,
          "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention. (arXiv:2107.08687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08595",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tuo_R/0/1/0/all/0/1\">Rui Tuo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowei Zhang</a>",
          "description": "High-dimensional simulation optimization is notoriously challenging. We\npropose a new sampling algorithm that converges to a global optimal solution\nand suffers minimally from the curse of dimensionality. The algorithm consists\nof two stages. First, we take samples following a sparse grid experimental\ndesign and approximate the response surface via kernel ridge regression with a\nBrownian field kernel. Second, we follow the expected improvement strategy --\nwith critical modifications that boost the algorithm's sample efficiency -- to\niteratively sample from the next level of the sparse grid. Under mild\nconditions on the smoothness of the response surface and the simulation noise,\nwe establish upper bounds on the convergence rate for both noise-free and noisy\nsimulation samples. These upper rates deteriorate only slightly in the\ndimension of the feasible set, and they can be improved if the objective\nfunction is known be of a higher-order smoothness. Extensive numerical\nexperiments demonstrate that the proposed algorithm dramatically outperforms\ntypical alternatives in practice.",
          "link": "http://arxiv.org/abs/2107.08595",
          "publishedOn": "2021-07-20T02:04:44.400Z",
          "wordCount": 615,
          "title": "High-Dimensional Simulation Optimization via Brownian Fields and Sparse Grids. (arXiv:2107.08595v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peri_S/0/1/0/all/0/1\">Skand Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "Object-centric world models provide structured representation of the scene\nand can be an important backbone in reinforcement learning and planning.\nHowever, existing approaches suffer in partially-observable environments due to\nthe lack of belief states. In this paper, we propose Structured World Belief, a\nmodel for learning and inference of object-centric belief states. Inferred by\nSequential Monte Carlo (SMC), our belief states provide multiple object-centric\nscene hypotheses. To synergize the benefits of SMC particles with object\nrepresentations, we also propose a new object-centric dynamics model that\nconsiders the inductive bias of object permanence. This enables tracking of\nobject states even when they are invisible for a long time. To further\nfacilitate object tracking in this regime, we allow our model to attend\nflexibly to any spatial location in the image which was restricted in previous\nmodels. In experiments, we show that object-centric belief provides a more\naccurate and robust performance for filtering and generation. Furthermore, we\nshow the efficacy of structured world belief in improving the performance of\nreinforcement learning, planning and supervised reasoning.",
          "link": "http://arxiv.org/abs/2107.08577",
          "publishedOn": "2021-07-20T02:04:44.344Z",
          "wordCount": 614,
          "title": "Structured World Belief for Reinforcement Learning in POMDP. (arXiv:2107.08577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lonkar_S/0/1/0/all/0/1\">Subodh Lonkar</a>",
          "description": "Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.",
          "link": "http://arxiv.org/abs/2107.08640",
          "publishedOn": "2021-07-20T02:04:44.326Z",
          "wordCount": 592,
          "title": "Facial Expressions Recognition with Convolutional Neural Networks. (arXiv:2107.08640v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Generalization performance of stochastic optimization stands a central place\nin machine learning. In this paper, we investigate the excess risk performance\nand towards improved learning rates for two popular approaches of stochastic\noptimization: empirical risk minimization (ERM) and stochastic gradient descent\n(SGD). Although there exists plentiful generalization analysis of ERM and SGD\nfor supervised learning, current theoretical understandings of ERM and SGD are\neither have stronger assumptions in convex learning, e.g., strong convexity\ncondition, or show slow rates and less studied in nonconvex learning. Motivated\nby these problems, we aim to provide improved rates under milder assumptions in\nconvex learning and derive faster rates in nonconvex learning. It is notable\nthat our analysis span two popular theoretical viewpoints: stability and\nuniform convergence. To be specific, in stability regime, we present high\nprobability rates of order $\\mathcal{O} (1/n)$ w.r.t. the sample size $n$ for\nERM and SGD with milder assumptions in convex learning and similar high\nprobability rates of order $\\mathcal{O} (1/n)$ in nonconvex learning, rather\nthan in expectation. Furthermore, this type of learning rate is improved to\nfaster order $\\mathcal{O} (1/n^2)$ in uniform convergence regime. To the best\nof our knowledge, for ERM and SGD, the learning rates presented in this paper\nare all state-of-the-art.",
          "link": "http://arxiv.org/abs/2107.08686",
          "publishedOn": "2021-07-20T02:04:44.309Z",
          "wordCount": 645,
          "title": "Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints. (arXiv:2107.08686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>",
          "description": "We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, and unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.",
          "link": "http://arxiv.org/abs/2107.08661",
          "publishedOn": "2021-07-20T02:04:44.288Z",
          "wordCount": 614,
          "title": "Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maran_D/0/1/0/all/0/1\">D. Maran</a>",
          "description": "We improve a theoretical result of the article \"On Exploiting Spectral\nProperties for Solving MDP with Large State Space\" showing that their\nalgorithm, which was proved to converge under some unrealistic assumptions, is\nactually guaranteed to converge always.",
          "link": "http://arxiv.org/abs/2107.08488",
          "publishedOn": "2021-07-20T02:04:44.270Z",
          "wordCount": 484,
          "title": "A note on the article \"On Exploiting Spectral Properties for Solving MDP with Large State Space\". (arXiv:2107.08488v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibeling_D/0/1/0/all/0/1\">Duligur Ibeling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>",
          "description": "This paper presents a topological learning-theoretic perspective on causal\ninference by introducing a series of topologies defined on general spaces of\nstructural causal models (SCMs). As an illustration of the framework we prove a\ntopological causal hierarchy theorem, showing that substantive assumption-free\ncausal inference is possible only in a meager set of SCMs. Thanks to a known\ncorrespondence between open sets in the weak topology and statistically\nverifiable hypotheses, our results show that inductive assumptions sufficient\nto license valid causal inferences are statistically unverifiable in principle.\nSimilar to no-free-lunch theorems for statistical inference, the present\nresults clarify the inevitability of substantial assumptions for causal\ninference. An additional benefit of our topological approach is that it easily\naccommodates SCMs with infinitely many variables. We finally suggest that the\nframework may be helpful for the positive project of exploring and assessing\nalternative causal-inductive assumptions.",
          "link": "http://arxiv.org/abs/2107.08558",
          "publishedOn": "2021-07-20T02:04:44.206Z",
          "wordCount": 578,
          "title": "A Topological Perspective on Causal Inference. (arXiv:2107.08558v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>",
          "description": "We study multi-task reinforcement learning (RL) in tabular episodic Markov\ndecision processes (MDPs). We formulate a heterogeneous multi-player RL\nproblem, in which a group of players concurrently face similar but not\nnecessarily identical MDPs, with a goal of improving their collective\nperformance through inter-player information sharing. We design and analyze an\nalgorithm based on the idea of model transfer, and provide gap-dependent and\ngap-independent upper and lower bounds that characterize the intrinsic\ncomplexity of the problem.",
          "link": "http://arxiv.org/abs/2107.08622",
          "publishedOn": "2021-07-20T02:04:44.188Z",
          "wordCount": 502,
          "title": "Provably Efficient Multi-Task Reinforcement Learning with Model Transfer. (arXiv:2107.08622v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelhack_M/0/1/0/all/0/1\">Mohamed Abdelhack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Sandhya Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_B/0/1/0/all/0/1\">Bradley Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_M/0/1/0/all/0/1\">Michael Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_C/0/1/0/all/0/1\">Christopher King</a>",
          "description": "Data quality is a common problem in machine learning, especially in\nhigh-stakes settings such as healthcare. Missing data affects accuracy,\ncalibration, and feature attribution in complex patterns. Developers often\ntrain models on carefully curated datasets to minimize missing data bias;\nhowever, this reduces the usability of such models in production environments,\nsuch as real-time healthcare records. Making machine learning models robust to\nmissing data is therefore crucial for practical application. While some\nclassifiers naturally handle missing data, others, such as deep neural\nnetworks, are not designed for unknown values. We propose a novel neural\nnetwork modification to mitigate the impacts of missing data. The approach is\ninspired by neuromodulation that is performed by biological neural networks.\nOur proposal replaces the fixed weights of a fully-connected layer with a\nfunction of an additional input (reliability score) at each input, mimicking\nthe ability of cortex to up- and down-weight inputs based on the presence of\nother data. The modulation function is jointly learned with the main task using\na multi-layer perceptron. We tested our modulating fully connected layer on\nmultiple classification, regression, and imputation problems, and it either\nimproved performance or generated comparable performance to conventional neural\nnetwork architectures concatenating reliability to the inputs. Models with\nmodulating layers were more robust against degradation of data quality by\nintroducing additional missingness at evaluation time. These results suggest\nthat explicitly accounting for reduced information quality with a modulating\nfully connected layer can enable the deployment of artificial intelligence\nsystems in real-time settings.",
          "link": "http://arxiv.org/abs/2107.08574",
          "publishedOn": "2021-07-20T02:04:44.170Z",
          "wordCount": 695,
          "title": "A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sisejkovic_D/0/1/0/all/0/1\">Dominik Sisejkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_F/0/1/0/all/0/1\">Farhad Merchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimann_L/0/1/0/all/0/1\">Lennart M. Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leupers_R/0/1/0/all/0/1\">Rainer Leupers</a>",
          "description": "Logic locking has emerged as a prominent key-driven technique to protect the\nintegrity of integrated circuits. However, novel machine-learning-based attacks\nhave recently been introduced to challenge the security foundations of locking\nschemes. These attacks are able to recover a significant percentage of the key\nwithout having access to an activated circuit. This paper address this issue\nthrough two focal points. First, we present a theoretical model to test locking\nschemes for key-related structural leakage that can be exploited by machine\nlearning. Second, based on the theoretical model, we introduce D-MUX: a\ndeceptive multiplexer-based logic-locking scheme that is resilient against\nstructure-exploiting machine learning attacks. Through the design of D-MUX, we\nuncover a major fallacy in existing multiplexer-based locking schemes in the\nform of a structural-analysis attack. Finally, an extensive cost evaluation of\nD-MUX is presented. To the best of our knowledge, D-MUX is the first\nmachine-learning-resilient locking scheme capable of protecting against all\nknown learning-based attacks. Hereby, the presented work offers a starting\npoint for the design and evaluation of future-generation logic locking in the\nera of machine learning.",
          "link": "http://arxiv.org/abs/2107.08695",
          "publishedOn": "2021-07-20T02:04:44.143Z",
          "wordCount": 626,
          "title": "Deceptive Logic Locking for Hardware Integrity Protection against Machine Learning Attacks. (arXiv:2107.08695v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinke Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chonghe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guanding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongning Guo</a>",
          "description": "Generative adversarial networks (GANs) are emerging machine learning models\nfor generating synthesized data similar to real data by jointly training a\ngenerator and a discriminator. In many applications, data and computational\nresources are distributed over many devices, so centralized computation with\nall data in one location is infeasible due to privacy and/or communication\nconstraints. This paper proposes a new framework for training GANs in a\ndistributed fashion: Each device computes a local discriminator using local\ndata; a single server aggregates their results and computes a global GAN.\nSpecifically, in each iteration, the server sends the global GAN to the\ndevices, which then update their local discriminators; the devices send their\nresults to the server, which then computes their average as the global\ndiscriminator and updates the global generator accordingly. Two different\nupdate schedules are designed with different levels of parallelism between the\ndevices and the server. Numerical results obtained using three popular datasets\ndemonstrate that the proposed framework can outperform a state-of-the-art\nframework in terms of convergence speed.",
          "link": "http://arxiv.org/abs/2107.08681",
          "publishedOn": "2021-07-20T02:04:44.124Z",
          "wordCount": 627,
          "title": "A New Distributed Method for Training Generative Adversarial Networks. (arXiv:2107.08681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimpley_A/0/1/0/all/0/1\">Anish Pimpley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Anubha Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1\">Alekh Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hiren Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_R/0/1/0/all/0/1\">Rathijit Sen</a>",
          "description": "Optimizing resource allocation for analytical workloads is vital for reducing\ncosts of cloud-data services. At the same time, it is incredibly hard for users\nto allocate resources per query in serverless processing systems, and they\nfrequently misallocate by orders of magnitude. Unfortunately, prior work\nfocused on predicting peak allocation while ignoring aggressive trade-offs\nbetween resource allocation and run-time. Additionally, these methods fail to\npredict allocation for queries that have not been observed in the past. In this\npaper, we tackle both these problems. We introduce a system for optimal\nresource allocation that can predict performance with aggressive trade-offs,\nfor both new and past observed queries. We introduce the notion of a\nperformance characteristic curve (PCC) as a parameterized representation that\ncan compactly capture the relationship between resources and performance. To\ntackle training data sparsity, we introduce a novel data augmentation technique\nto efficiently synthesize the entire PCC using a single run of the query.\nLastly, we demonstrate the advantages of a constrained loss function coupled\nwith GNNs, over traditional ML methods, for capturing the domain specific\nbehavior through an extensive experimental evaluation over SCOPE big data\nworkloads at Microsoft.",
          "link": "http://arxiv.org/abs/2107.08594",
          "publishedOn": "2021-07-20T02:04:44.093Z",
          "wordCount": 628,
          "title": "Optimal Resource Allocation for Serverless Queries. (arXiv:2107.08594v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Susan B. Davidson</a>",
          "description": "High-quality labels are expensive to obtain for many machine learning tasks,\nsuch as medical image classification tasks. Therefore, probabilistic (weak)\nlabels produced by weak supervision tools are used to seed a process in which\ninfluential samples with weak labels are identified and cleaned by several\nhuman annotators to improve the model performance. To lower the overall cost\nand computational overhead of this process, we propose a solution called\nChef(CHEap and Fast label cleaning), which consists of the following three\ncomponents. First, to reduce the cost of human annotators, we use Infl, which\nprioritizes the most influential training samples for cleaning and provides\ncleaned labels to save the cost of one human annotator. Second, to accelerate\nthe sample selector phase and the model constructor phase, we use Increm-Infl\nto incrementally produce influential samples, and DeltaGrad-L to incrementally\nupdate the model. Third, we redesign the typical label cleaning pipeline so\nthat human annotators iteratively clean smaller batch of samples rather than\none big batch of samples. This yields better over all model performance and\nenables possible early termination when the expected model performance has been\nachieved. Extensive experiments show that our approach gives good model\nprediction performance while achieving significant speed-ups.",
          "link": "http://arxiv.org/abs/2107.08588",
          "publishedOn": "2021-07-20T02:04:44.023Z",
          "wordCount": 636,
          "title": "Chef: a cheap and fast pipeline for iteratively cleaning label uncertainties. (arXiv:2107.08588v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08673",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Massalimova_A/0/1/0/all/0/1\">Aidana Massalimova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.",
          "link": "http://arxiv.org/abs/2107.08673",
          "publishedOn": "2021-07-20T02:04:44.004Z",
          "wordCount": 581,
          "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images. (arXiv:2107.08673v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hanif Heidari</a>",
          "description": "Measuring the predictability and complexity of time series is an essential\ntool in designing and controlling the nonlinear system. There exist different\nentropy measures in the literature to analyze the predictability and complexity\nof time series. However, these measures have some drawbacks especially in short\ntime series. To overcome the difficulties, this paper proposes a new method for\nestimating the entropy of a time series using the LogNNet 784:25:10 neural\nnetwork model. The LogNNet reservoir matrix consists of 19625 elements which is\nfilled with the time series elements. After that, the network is trained on\nMNIST-10 dataset and the classification accuracy is calculated. The accuracy is\nconsidered as the entropy measure and denoted by NNetEn. A more complex\ntransformation of the input information by the time series in the reservoir\nleads to higher NNetEn values. Many practical time series data have less than\n19625 elements. Some duplicating or stretching methods are investigated to\novercome this difficulty and the most successful method is identified for\npractical applications. The epochs number in the training process of LogNNet is\nconsidered as the input parameter. A new time series characteristic called time\nseries learning inertia is introduced to investigate the effect of epochs\nnumber in the efficiency of neural network. To show the robustness and\nefficiency of the proposed method, it is applied on some chaotic, periodic,\nrandom, binary and constant time series. The NNetEn is compared with some\nexisting entropy measures. The results show that the proposed method is more\nrobust and accurate than existing methods.",
          "link": "http://arxiv.org/abs/2107.08399",
          "publishedOn": "2021-07-20T02:04:43.986Z",
          "wordCount": 712,
          "title": "A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tong Lin</a>",
          "description": "Adaptive gradient methods, especially Adam-type methods (such as Adam,\nAMSGrad, and AdaBound), have been proposed to speed up the training process\nwith an element-wise scaling term on learning rates. However, they often\ngeneralize poorly compared with stochastic gradient descent (SGD) and its\naccelerated schemes such as SGD with momentum (SGDM). In this paper, we propose\na new adaptive method called DecGD, which simultaneously achieves good\ngeneralization like SGDM and obtain rapid convergence like Adam-type methods.\nIn particular, DecGD decomposes the current gradient into the product of two\nterms including a surrogate gradient and a loss based vector. Our method\nadjusts the learning rates adaptively according to the current loss based\nvector instead of the squared gradients used in Adam-type methods. The\nintuition for adaptive learning rates of DecGD is that a good optimizer, in\ngeneral cases, needs to decrease the learning rates as the loss decreases,\nwhich is similar to the learning rates decay scheduling technique. Therefore,\nDecGD gets a rapid convergence in the early phases of training and controls the\neffective learning rates according to the loss based vectors which help lead to\na better generalization. Convergence analysis is discussed in both convex and\nnon-convex situations. Finally, empirical results on widely-used tasks and\nmodels demonstrate that DecGD shows better generalization performance than SGDM\nand rapid convergence like Adam-type methods.",
          "link": "http://arxiv.org/abs/2107.08377",
          "publishedOn": "2021-07-20T02:04:43.967Z",
          "wordCount": 652,
          "title": "A New Adaptive Gradient Method with Gradient Decomposition. (arXiv:2107.08377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08710",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Higham_C/0/1/0/all/0/1\">Catherine F. Higham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bedford_A/0/1/0/all/0/1\">Adrian Bedford</a>",
          "description": "We demonstrate the feasibility of framing a classically learned deep neural\nnetwork as an energy based model that can be processed on a one-step quantum\nannealer in order to exploit fast sampling times. We propose approaches to\novercome two hurdles for high resolution image classification on a quantum\nprocessing unit (QPU): the required number and binary nature of the model\nstates. With this novel method we successfully transfer a convolutional neural\nnetwork to the QPU and show the potential for classification speedup of at\nleast one order of magnitude.",
          "link": "http://arxiv.org/abs/2107.08710",
          "publishedOn": "2021-07-20T02:04:43.913Z",
          "wordCount": 522,
          "title": "Quantum Deep Learning: Sampling Neural Nets with a Quantum Annealer. (arXiv:2107.08710v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08596",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Katsman_I/0/1/0/all/0/1\">Isay Katsman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lou_A/0/1/0/all/0/1\">Aaron Lou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_D/0/1/0/all/0/1\">Derek Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingxuan Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>",
          "description": "Tractably modelling distributions over manifolds has long been an important\ngoal in the natural sciences. Recent work has focused on developing general\nmachine learning models to learn such distributions. However, for many\napplications these distributions must respect manifold symmetries -- a trait\nwhich most previous models disregard. In this paper, we lay the theoretical\nfoundations for learning symmetry-invariant distributions on arbitrary\nmanifolds via equivariant manifold flows. We demonstrate the utility of our\napproach by using it to learn gauge invariant densities over $SU(n)$ in the\ncontext of quantum field theory.",
          "link": "http://arxiv.org/abs/2107.08596",
          "publishedOn": "2021-07-20T02:04:43.826Z",
          "wordCount": 526,
          "title": "Equivariant Manifold Flows. (arXiv:2107.08596v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Jos&#xe9; Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creus_R/0/1/0/all/0/1\">Roger Creus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>",
          "description": "Pre-training Reinforcement Learning agents in a task-agnostic manner has\nshown promising results. However, previous works still struggle in learning and\ndiscovering meaningful skills in high-dimensional state-spaces, such as\npixel-spaces. We approach the problem by leveraging unsupervised skill\ndiscovery and self-supervised learning of state representations. In our work,\nwe learn a compact latent representation by making use of variational and\ncontrastive techniques. We demonstrate that both enable RL agents to learn a\nset of basic navigation skills by maximizing an information theoretic\nobjective. We assess our method in Minecraft 3D pixel maps with different\ncomplexities. Our results show that representations and conditioned policies\nlearned from pixels are enough for toy examples, but do not scale to realistic\nand complex maps. To overcome these limitations, we explore alternative input\nobservations such as the relative position of the agent along with the raw\npixels.",
          "link": "http://arxiv.org/abs/2107.08398",
          "publishedOn": "2021-07-20T02:04:43.808Z",
          "wordCount": 579,
          "title": "Unsupervised Skill-Discovery and Skill-Learning in Minecraft. (arXiv:2107.08398v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08429",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Naik_S/0/1/0/all/0/1\">Shibabrat Naik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Krajnak_V/0/1/0/all/0/1\">Vladim&#xed;r Kraj&#x148;&#xe1;k</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wiggins_S/0/1/0/all/0/1\">Stephen Wiggins</a>",
          "description": "We develop a machine learning framework that can be applied to data sets\nderived from the trajectories of Hamilton's equations. The goal is to learn the\nphase space structures that play the governing role for phase space transport\nrelevant to particular applications. Our focus is on learning reactive islands\nin two degrees-of-freedom Hamiltonian systems. Reactive islands are constructed\nfrom the stable and unstable manifolds of unstable periodic orbits and play the\nrole of quantifying transition dynamics. We show that support vector machines\n(SVM) is an appropriate machine learning framework for this purpose as it\nprovides an approach for finding the boundaries between qualitatively distinct\ndynamical behaviors, which is in the spirit of the phase space transport\nframework. We show how our method allows us to find reactive islands directly\nin the sense that we do not have to first compute unstable periodic orbits and\ntheir stable and unstable manifolds. We apply our approach to the\nH\\'enon-Heiles Hamiltonian system, which is a benchmark system in the dynamical\nsystems community. We discuss different sampling and learning approaches and\ntheir advantages and disadvantages.",
          "link": "http://arxiv.org/abs/2107.08429",
          "publishedOn": "2021-07-20T02:04:43.788Z",
          "wordCount": 625,
          "title": "Support vector machines for learning reactive islands. (arXiv:2107.08429v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkawi_A/0/1/0/all/0/1\">Ali Malkawi</a>",
          "description": "Current performance-driven building design methods are not widely adopted\noutside the research field for several reasons that make them difficult to\nintegrate into a typical design process. In the early design phase, in\nparticular, the time-intensity and the cognitive load associated with\noptimization and form parametrization are incompatible with design exploration,\nwhich requires quick iteration. This research introduces a novel method for\nperformance-driven geometry generation that can afford interaction directly in\nthe 3d modeling environment, eliminating the need for explicit parametrization,\nand is multiple orders faster than the equivalent form optimization. The method\nuses Machine Learning techniques to train a generative model offline. The\ngenerative model learns a distribution of optimal performing geometries and\ntheir simulation contexts based on a dataset that addresses the performance(s)\nof interest. By navigating the generative model's latent space, geometries with\nthe desired characteristics can be quickly generated. A case study is\npresented, demonstrating the generation of a synthetic dataset and the use of a\nVariational Autoencoder (VAE) as a generative model for geometries with optimal\nsolar gain. The results show that the VAE-generated geometries perform on\naverage at least as well as the optimized ones, suggesting that the introduced\nmethod shows a feasible path towards more intuitive and interactive early-phase\nperformance-driven design assistance.",
          "link": "http://arxiv.org/abs/2107.08572",
          "publishedOn": "2021-07-20T02:04:43.769Z",
          "wordCount": 637,
          "title": "Early-Phase Performance-Driven Design using Generative Models. (arXiv:2107.08572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08470",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>",
          "description": "This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.",
          "link": "http://arxiv.org/abs/2107.08470",
          "publishedOn": "2021-07-20T02:04:43.752Z",
          "wordCount": 602,
          "title": "ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_H/0/1/0/all/0/1\">Hsu-Hsun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsay_R/0/1/0/all/0/1\">Ren-Song Tsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hsin-I Wu</a>",
          "description": "Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.",
          "link": "http://arxiv.org/abs/2107.08382",
          "publishedOn": "2021-07-20T02:04:43.694Z",
          "wordCount": 609,
          "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications. (arXiv:2107.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chihcheng Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1\">Catarina Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chun Ouyang</a>",
          "description": "Predictive process analytics often apply machine learning to predict the\nfuture states of a running business process. However, the internal mechanisms\nof many existing predictive algorithms are opaque and a human decision-maker is\nunable to understand \\emph{why} a certain activity was predicted. Recently,\ncounterfactuals have been proposed in the literature to derive\nhuman-understandable explanations from predictive models. Current\ncounterfactual approaches consist of finding the minimum feature change that\ncan make a certain prediction flip its outcome. Although many algorithms have\nbeen proposed, their application to the sequence and multi-dimensional data\nlike event logs has not been explored in the literature.\n\nIn this paper, we explore the use of a recent, popular model-agnostic\ncounterfactual algorithm, DiCE, in the context of predictive process analytics.\nThe analysis reveals that the algorithm is limited when being applied to derive\nexplanations of process predictions, due to (1) process domain knowledge not\nbeing taken into account, (2) long traces that often tend to be less\nunderstandable, and (3) difficulties in optimising the counterfactual search\nwith categorical variables. We design an extension of DiCE that can generate\ncounterfactuals for process predictions, and propose an approach that supports\nderiving milestone-aware counterfactuals at different stages of a trace to\npromote interpretability. We apply our approach to BPIC2012 event log and the\nanalysis results demonstrate the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2107.08697",
          "publishedOn": "2021-07-20T02:04:43.676Z",
          "wordCount": 652,
          "title": "Interpreting Process Predictions using a Milestone-Aware Counterfactual Approach. (arXiv:2107.08697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Assayag_S/0/1/0/all/0/1\">Shai Ben-Assayag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1\">Ran El-Yaniv</a>",
          "description": "Playing board games is considered a major challenge for both humans and AI\nresearchers. Because some complicated board games are quite hard to learn,\nhumans usually begin with playing on smaller boards and incrementally advance\nto master larger board strategies. Most neural network frameworks that are\ncurrently tasked with playing board games neither perform such incremental\nlearning nor possess capabilities to automatically scale up. In this work, we\nlook at the board as a graph and combine a graph neural network architecture\ninside the AlphaZero framework, along with some other innovative improvements.\nOur ScalableAlphaZero is capable of learning to play incrementally on small\nboards, and advancing to play on large ones. Our model can be trained quickly\nto play different challenging board games on multiple board sizes, without\nusing any domain knowledge. We demonstrate the effectiveness of\nScalableAlphaZero and show, for example, that by training it for only three\ndays on small Othello boards, it can defeat the AlphaZero model on a large\nboard, which was trained to play the large board for $30$ days.",
          "link": "http://arxiv.org/abs/2107.08387",
          "publishedOn": "2021-07-20T02:04:43.657Z",
          "wordCount": 616,
          "title": "Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN. (arXiv:2107.08387v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dan Shen</a>",
          "description": "Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.",
          "link": "http://arxiv.org/abs/2107.08598",
          "publishedOn": "2021-07-20T02:04:43.636Z",
          "wordCount": 670,
          "title": "Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Onoszko_N/0/1/0/all/0/1\">Noa Onoszko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_G/0/1/0/all/0/1\">Gustav Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mogren_O/0/1/0/all/0/1\">Olof Mogren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zec_E/0/1/0/all/0/1\">Edvin Listo Zec</a>",
          "description": "We tackle the non-convex problem of learning a personalized deep learning\nmodel in a decentralized setting. More specifically, we study decentralized\nfederated learning, a peer-to-peer setting where data is distributed among many\nclients and where there is no central server to orchestrate the training. In\nreal world scenarios, the data distributions are often heterogeneous between\nclients. Therefore, in this work we study the problem of how to efficiently\nlearn a model in a peer-to-peer system with non-iid client data. We propose a\nmethod named Performance-Based Neighbor Selection (PENS) where clients with\nsimilar data distributions detect each other and cooperate by evaluating their\ntraining losses on each other's data to learn a model suitable for the local\ndata distribution. Our experiments on benchmark datasets show that our proposed\nmethod is able to achieve higher accuracies as compared to strong baselines.",
          "link": "http://arxiv.org/abs/2107.08517",
          "publishedOn": "2021-07-20T02:04:43.618Z",
          "wordCount": 579,
          "title": "Decentralized federated learning of deep neural networks on non-iid data. (arXiv:2107.08517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>",
          "description": "Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.",
          "link": "http://arxiv.org/abs/2107.08471",
          "publishedOn": "2021-07-20T02:04:43.555Z",
          "wordCount": 556,
          "title": "A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdolshah_M/0/1/0/all/0/1\">Majid Abdolshah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thommen Karimpanal George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "Transfer in reinforcement learning is usually achieved through generalisation\nacross tasks. Whilst many studies have investigated transferring knowledge when\nthe reward function changes, they have assumed that the dynamics of the\nenvironments remain consistent. Many real-world RL problems require transfer\namong environments with different dynamics. To address this problem, we propose\nan approach based on successor features in which we model successor feature\nfunctions with Gaussian Processes permitting the source successor features to\nbe treated as noisy measurements of the target successor feature function. Our\ntheoretical analysis proves the convergence of this approach as well as the\nbounded error on modelling successor feature functions with Gaussian Processes\nin environments with both different dynamics and rewards. We demonstrate our\nmethod on benchmark datasets and show that it outperforms current baselines.",
          "link": "http://arxiv.org/abs/2107.08426",
          "publishedOn": "2021-07-20T02:04:43.537Z",
          "wordCount": 569,
          "title": "A New Representation of Successor Features for Transfer across Dissimilar Environments. (arXiv:2107.08426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feiyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanrong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1\">Ao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "The exploration/exploitation (E&E) dilemma lies at the core of interactive\nsystems such as online advertising, for which contextual bandit algorithms have\nbeen proposed. Bayesian approaches provide guided exploration with principled\nuncertainty estimation, but the applicability is often limited due to\nover-simplified assumptions. Non-Bayesian bootstrap methods, on the other hand,\ncan apply to complex problems by using deep reward models, but lacks clear\nguidance to the exploration behavior. It still remains largely unsolved to\ndevelop a practical method for complex deep contextual bandits.\n\nIn this paper, we introduce Guided Bootstrap (GuideBoot for short), combining\nthe best of both worlds. GuideBoot provides explicit guidance to the\nexploration behavior by training multiple models over both real samples and\nnoisy samples with fake labels, where the noise is added according to the\npredictive uncertainty. The proposed method is efficient as it can make\ndecisions on-the-fly by utilizing only one randomly chosen model, but is also\neffective as we show that it can be viewed as a non-Bayesian approximation of\nThompson sampling. Moreover, we extend it to an online version that can learn\nsolely from streaming data, which is favored in real applications. Extensive\nexperiments on both synthetic task and large-scale advertising environments\nshow that GuideBoot achieves significant improvements against previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.08383",
          "publishedOn": "2021-07-20T02:04:43.515Z",
          "wordCount": 649,
          "title": "GuideBoot: Guided Bootstrap for Deep Contextual Bandits. (arXiv:2107.08383v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L Rubin</a>",
          "description": "Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.",
          "link": "http://arxiv.org/abs/2107.08371",
          "publishedOn": "2021-07-20T02:04:43.497Z",
          "wordCount": 583,
          "title": "An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging. (arXiv:2107.08371v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruenbacher_S/0/1/0/all/0/1\">Sophie Gruenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolka_S/0/1/0/all/0/1\">Scott Smolka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>",
          "description": "We introduce a new stochastic verification algorithm that formally quantifies\nthe behavioral robustness of any time-continuous process formulated as a\ncontinuous-depth model. The algorithm solves a set of global optimization (Go)\nproblems over a given time horizon to construct a tight enclosure (Tube) of the\nset of all process executions starting from a ball of initial states. We call\nour algorithm GoTube. Through its construction, GoTube ensures that the\nbounding tube is conservative up to a desired probability. GoTube is\nimplemented in JAX and optimized to scale to complex continuous-depth models.\nCompared to advanced reachability analysis tools for time-continuous neural\nnetworks, GoTube provably does not accumulate over-approximation errors between\ntime steps and avoids the infamous wrapping effect inherent in symbolic\ntechniques. We show that GoTube substantially outperforms state-of-the-art\nverification tools in terms of the size of the initial ball, speed,\ntime-horizon, task completion, and scalability, on a large set of experiments.\nGoTube is stable and sets the state-of-the-art for its ability to scale up to\ntime horizons well beyond what has been possible before.",
          "link": "http://arxiv.org/abs/2107.08467",
          "publishedOn": "2021-07-20T02:04:43.457Z",
          "wordCount": 631,
          "title": "GoTube: Scalable Stochastic Verification of Continuous-Depth Models. (arXiv:2107.08467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ampanavos_S/0/1/0/all/0/1\">Spyridon Ampanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourbakhsh_M/0/1/0/all/0/1\">Mehdi Nourbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>",
          "description": "Structural engineering knowledge can be of significant importance to the\narchitectural design team during the early design phase. However, architects\nand engineers do not typically work together during the conceptual phase; in\nfact, structural engineers are often called late into the process. As a result,\nupdates in the design are more difficult and time-consuming to complete. At the\nsame time, there is a lost opportunity for better design exploration guided by\nstructural feedback. In general, the earlier in the design process the\niteration happens, the greater the benefits in cost efficiency and informed\nde-sign exploration, which can lead to higher-quality creative results. In\norder to facilitate an informed exploration in the early design stage, we\nsuggest the automation of fundamental structural engineering tasks and\nintroduce ApproxiFramer, a Machine Learning-based system for the automatic\ngeneration of structural layouts from building plan sketches in real-time. The\nsystem aims to assist architects by presenting them with feasible structural\nsolutions during the conceptual phase so that they proceed with their design\nwith adequate knowledge of its structural implications. In this paper, we\ndescribe the system and evaluate the performance of a proof-of-concept\nimplementation in the domain of orthogonal, metal, rigid structures. We trained\na Convolutional Neural Net to iteratively generate structural design solutions\nfor sketch-level building plans using a synthetic dataset and achieved an\naverage error of 2.2% in the predicted positions of the columns.",
          "link": "http://arxiv.org/abs/2107.08567",
          "publishedOn": "2021-07-20T02:04:43.390Z",
          "wordCount": 671,
          "title": "Structural Design Recommendations in the Early Design Phase using Machine Learning. (arXiv:2107.08567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>",
          "description": "Bayesian neural network (BNN) allows for uncertainty quantification in\nprediction, offering an advantage over regular neural networks that has not\nbeen explored in the differential privacy (DP) framework. We fill this\nimportant gap by leveraging recent development in Bayesian deep learning and\nprivacy accounting to offer a more precise analysis of the trade-off between\nprivacy and accuracy in BNN. We propose three DP-BNNs that characterize the\nweight uncertainty for the same network architecture in distinct ways, namely\nDP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of\ninterest) and DP-MC Dropout (via the model architecture). Interestingly, we\nshow a new equivalence between DP-SGD and DP-SGLD, implying that some\nnon-Bayesian DP training naturally allows for uncertainty quantification.\nHowever, the hyperparameters such as learning rate and batch size, can have\ndifferent or even opposite effects in DP-SGD and DP-SGLD.\n\nExtensive experiments are conducted to compare DP-BNNs, in terms of privacy\nguarantee, prediction accuracy, uncertainty quantification, calibration,\ncomputation speed, and generalizability to network architecture. As a result,\nwe observe a new tradeoff between the privacy and the reliability. When\ncompared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate\nunder strong privacy guarantee, demonstrating the great potential of DP-BNN in\nreal-world tasks.",
          "link": "http://arxiv.org/abs/2107.08461",
          "publishedOn": "2021-07-20T02:04:43.317Z",
          "wordCount": 645,
          "title": "Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability. (arXiv:2107.08461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08514",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokate_P/0/1/0/all/0/1\">Pranali Kokate</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pancholi_S/0/1/0/all/0/1\">Sidharth Pancholi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1\">Amit M. Joshi</a>",
          "description": "The Brain-Computer Interface system is a profoundly developing area of\nexperimentation for Motor activities which plays vital role in decoding\ncognitive activities. Classification of Cognitive-Motor Imagery activities from\nEEG signals is a critical task. Hence proposed a unique algorithm for\nclassifying left/right-hand movements by utilizing Multi-layer Perceptron\nNeural Network. Handcrafted statistical Time domain and Power spectral density\nfrequency domain features were extracted and obtained a combined accuracy of\n96.02%. Results were compared with the deep learning framework. In addition to\naccuracy, Precision, F1-Score, and recall was considered as the performance\nmetrics. The intervention of unwanted signals contaminates the EEG signals\nwhich influence the performance of the algorithm. Therefore, a novel approach\nwas approached to remove the artifacts using Independent Components Analysis\nwhich boosted the performance. Following the selection of appropriate feature\nvectors that provided acceptable accuracy. The same method was used on all nine\nsubjects. As a result, intra-subject accuracy was obtained for 9 subjects\n94.72%. The results show that the proposed approach would be useful to classify\nthe upper limb movements accurately.",
          "link": "http://arxiv.org/abs/2107.08514",
          "publishedOn": "2021-07-20T02:04:43.244Z",
          "wordCount": 639,
          "title": "Classification of Upper Arm Movements from EEG signals using Machine Learning with ICA Analysis. (arXiv:2107.08514v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>",
          "description": "Federated learning is a prominent framework that enables clients (e.g.,\nmobile devices or organizations) to train a collaboratively global model under\na central server's orchestration while keeping local training datasets'\nprivacy. However, the aggregation step in federated learning is vulnerable to\nadversarial attacks as the central server cannot manage clients' behavior.\nTherefore, the global model's performance and convergence of the training\nprocess will be affected under such attacks.To mitigate this vulnerability\nissue, we propose a novel robust aggregation algorithm inspired by the truth\ninference methods in crowdsourcing via incorporating the worker's reliability\ninto aggregation. We evaluate our solution on three real-world datasets with a\nvariety of machine learning models. Experimental results show that our solution\nensures robust federated learning and is resilient to various types of attacks,\nincluding noisy data attacks, Byzantine attacks, and label flipping attacks.",
          "link": "http://arxiv.org/abs/2107.08402",
          "publishedOn": "2021-07-20T02:04:43.220Z",
          "wordCount": 582,
          "title": "RobustFed: A Truth Inference Approach for Robust Federated Learning. (arXiv:2107.08402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>",
          "description": "Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.",
          "link": "http://arxiv.org/abs/2106.10507",
          "publishedOn": "2021-07-19T01:59:51.065Z",
          "wordCount": 727,
          "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1\">Jessica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1\">In Hwa Um</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1\">Ognjen Arandjelovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">David J Harrison</a>",
          "description": "Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.",
          "link": "http://arxiv.org/abs/2107.04388",
          "publishedOn": "2021-07-19T01:59:51.035Z",
          "wordCount": 673,
          "title": "Hoechst Is All You Need: Lymphocyte Classification with Deep Learning. (arXiv:2107.04388v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1\">Sobhan Miryoosefi</a>",
          "description": "Finding the minimal structural assumptions that empower sample-efficient\nlearning is one of the most important research directions in Reinforcement\nLearning (RL). This paper advances our understanding of this fundamental\nquestion by introducing a new complexity measure -- Bellman Eluder (BE)\ndimension. We show that the family of RL problems of low BE dimension is\nremarkably rich, which subsumes a vast majority of existing tractable RL\nproblems including but not limited to tabular MDPs, linear MDPs, reactive\nPOMDPs, low Bellman rank problems as well as low Eluder dimension problems.\nThis paper further designs a new optimization-based algorithm -- GOLF, and\nreanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang\net al., 2017). We prove that both algorithms learn the near-optimal policies of\nlow BE dimension problems in a number of samples that is polynomial in all\nrelevant parameters, but independent of the size of state-action space. Our\nregret and sample complexity results match or improve the best existing results\nfor several well-known subclasses of low BE dimension problems.",
          "link": "http://arxiv.org/abs/2102.00815",
          "publishedOn": "2021-07-19T00:49:08.148Z",
          "wordCount": 659,
          "title": "Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03686",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>",
          "description": "We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.",
          "link": "http://arxiv.org/abs/2106.03686",
          "publishedOn": "2021-07-19T00:49:08.127Z",
          "wordCount": 681,
          "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1\">David J. Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1\">Ramakrishna Vedantam</a>",
          "description": "We address the question of characterizing and finding optimal representations\nfor supervised learning. Traditionally, this question has been tackled using\nthe Information Bottleneck, which compresses the inputs while retaining\ninformation about the targets, in a decoder-agnostic fashion. In machine\nlearning, however, our goal is not compression but rather generalization, which\nis intimately linked to the predictive family or decoder of interest (e.g.\nlinear classifier). We propose the Decodable Information Bottleneck (DIB) that\nconsiders information retention and compression from the perspective of the\ndesired predictive family. As a result, DIB gives rise to representations that\nare optimal in terms of expected test performance and can be estimated with\nguarantees. Empirically, we show that the framework can be used to enforce a\nsmall generalization gap on downstream classifiers and to predict the\ngeneralization ability of neural networks.",
          "link": "http://arxiv.org/abs/2009.12789",
          "publishedOn": "2021-07-19T00:49:08.103Z",
          "wordCount": 611,
          "title": "Learning Optimal Representations with the Decodable Information Bottleneck. (arXiv:2009.12789v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "Catastrophic forgetting means that a trained neural network model gradually\nforgets the previously learned tasks when being retrained on new tasks.\nOvercoming the forgetting problem is a major problem in machine learning.\nNumerous continual learning algorithms are very successful in incremental\nlearning of classification tasks, where new samples with their labels appear\nfrequently. However, there is currently no research that addresses the\ncatastrophic forgetting problem in regression tasks as far as we know. This\nproblem has emerged as one of the primary constraints in some applications,\nsuch as renewable energy forecasts. This article clarifies problem-related\ndefinitions and proposes a new methodological framework that can forecast\ntargets and update itself by means of continual learning. The framework\nconsists of forecasting neural networks and buffers, which store newly\ncollected data from a non-stationary data stream in an application. The changed\nprobability distribution of the data stream, which the framework has\nidentified, will be learned sequentially. The framework is called CLeaR\n(Continual Learning for Regression Tasks), where components can be flexibly\ncustomized for a specific application scenario. We design two sets of\nexperiments to evaluate the CLeaR framework concerning fitting error\n(training), prediction error (test), and forgetting ratio. The first one is\nbased on an artificial time series to explore how hyperparameters affect the\nCLeaR framework. The second one is designed with data collected from European\nwind farms to evaluate the CLeaR framework's performance in a real-world\napplication. The experimental results demonstrate that the CLeaR framework can\ncontinually acquire knowledge in the data stream and improve the prediction\naccuracy. The article concludes with further research issues arising from\nrequirements to extend the framework.",
          "link": "http://arxiv.org/abs/2101.00926",
          "publishedOn": "2021-07-19T00:49:08.077Z",
          "wordCount": 751,
          "title": "CLeaR: An Adaptive Continual Learning Framework for Regression Tasks. (arXiv:2101.00926v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1\">Yaniv Shulman</a>",
          "description": "Quantization based model compression serves as high performing and fast\napproach for inference that yields models which are highly compressed when\ncompared to their full-precision floating point counterparts. The most extreme\nquantization is a 1-bit representation of parameters such that they have only\ntwo possible values, typically -1(0) or +1, enabling efficient implementation\nof the ubiquitous dot product using only additions. The main contribution of\nthis work is the introduction of a method to smooth the combinatorial problem\nof determining a binary vector of weights to minimize the expected loss for a\ngiven objective by means of empirical risk minimization with backpropagation.\nThis is achieved by approximating a multivariate binary state over the weights\nutilizing a deterministic and differentiable transformation of real-valued,\ncontinuous parameters. The proposed method adds little overhead in training,\ncan be readily applied without any substantial modifications to the original\narchitecture, does not introduce additional saturating nonlinearities or\nauxiliary losses, and does not prohibit applying other methods for binarizing\nthe activations. Contrary to common assertions made in the literature, it is\ndemonstrated that binary weighted networks can train well with the same\nstandard optimization techniques and similar hyperparameter settings as their\nfull-precision counterparts, specifically momentum SGD with large learning\nrates and $L_2$ regularization. To conclude experiments demonstrate the method\nperforms remarkably well across a number of inductive image classification\ntasks with various architectures compared to their full-precision counterparts.\nThe source code is publicly available at\nhttps://bitbucket.org/YanivShu/binary_weighted_networks_public.",
          "link": "http://arxiv.org/abs/2107.01400",
          "publishedOn": "2021-07-19T00:49:08.072Z",
          "wordCount": 691,
          "title": "Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1\">Shruthi Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1\">Oshani Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyigoz_E/0/1/0/all/0/1\">Elif K. Eyigoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1\">Daniel M. Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saiz_F/0/1/0/all/0/1\">Fernando Suarez Saiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_P/0/1/0/all/0/1\">Pablo Meyer Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1\">Deborah L. McGuinness</a>",
          "description": "Academic advances of AI models in high-precision domains, like healthcare,\nneed to be made explainable in order to enhance real-world adoption. Our past\nstudies and ongoing interactions indicate that medical experts can use AI\nsystems with greater trust if there are ways to connect the model inferences\nabout patients to explanations that are tied back to the context of use.\nSpecifically, risk prediction is a complex problem of diagnostic and\ninterventional importance to clinicians wherein they consult different sources\nto make decisions. To enable the adoption of the ever improving AI risk\nprediction models in practice, we have begun to explore techniques to\ncontextualize such models along three dimensions of interest: the patients'\nclinical state, AI predictions about their risk of complications, and\nalgorithmic explanations supporting the predictions. We validate the importance\nof these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes\n(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a\ncommon T2DM comorbidity. Within the POC, we include risk prediction models for\nCKD, post-hoc explainers of the predictions, and other natural-language modules\nwhich operationalize domain knowledge and CPGs to provide context. With primary\ncare physicians (PCP) as our end-users, we present our initial results and\nclinician feedback in this paper. Our POC approach covers multiple knowledge\nsources and clinical scenarios, blends knowledge to explain data and\npredictions to PCPs, and received an enthusiastic response from our medical\nexpert.",
          "link": "http://arxiv.org/abs/2107.02359",
          "publishedOn": "2021-07-19T00:49:07.963Z",
          "wordCount": 746,
          "title": "Leveraging Clinical Context for User-Centered Explainability: A Diabetes Use Case. (arXiv:2107.02359v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14172",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kileel_J/0/1/0/all/0/1\">Joe Kileel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscovich_A/0/1/0/all/0/1\">Amit Moscovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelesko_N/0/1/0/all/0/1\">Nathan Zelesko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>",
          "description": "Manifold learning methods play a prominent role in nonlinear dimensionality\nreduction and other tasks involving high-dimensional data sets with low\nintrinsic dimensionality. Many of these methods are graph-based: they associate\na vertex with each data point and a weighted edge with each pair. Existing\ntheory shows that the Laplacian matrix of the graph converges to the\nLaplace-Beltrami operator of the data manifold, under the assumption that the\npairwise affinities are based on the Euclidean norm. In this paper, we\ndetermine the limiting differential operator for graph Laplacians constructed\nusing $\\textit{any}$ norm. Our proof involves an interplay between the second\nfundamental form of the manifold and the convex geometry of the given norm's\nunit ball. To demonstrate the potential benefits of non-Euclidean norms in\nmanifold learning, we consider the task of mapping the motion of large\nmolecules with continuous variability. In a numerical simulation we show that a\nmodified Laplacian eigenmaps algorithm, based on the Earthmover's distance,\noutperforms the classic Euclidean Laplacian eigenmaps, both in terms of\ncomputational cost and the sample size needed to recover the intrinsic\ngeometry.",
          "link": "http://arxiv.org/abs/2012.14172",
          "publishedOn": "2021-07-19T00:49:07.957Z",
          "wordCount": 651,
          "title": "Manifold learning with arbitrary norms. (arXiv:2012.14172v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhizhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).",
          "link": "http://arxiv.org/abs/2107.08039",
          "publishedOn": "2021-07-19T00:49:07.951Z",
          "wordCount": 568,
          "title": "Representation Consolidation for Training Expert Students. (arXiv:2107.08039v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1\">Zsolt Zombori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">Miroslav Ol&#x161;&#xe1;k</a>",
          "description": "In this work we study how to learn good algorithms for selecting reasoning\nsteps in theorem proving. We explore this in the connection tableau calculus\nimplemented by leanCoP where the partial tableau provides a clean and compact\nnotion of a state to which a limited number of inferences can be applied. We\nstart by incorporating a state-of-the-art learning algorithm -- a graph neural\nnetwork (GNN) -- into the plCoP theorem prover. Then we use it to observe the\nsystem's behaviour in a reinforcement learning setting, i.e., when learning\ninference guidance from successful Monte-Carlo tree searches on many problems.\nDespite its better pattern matching capability, the GNN initially performs\nworse than a simpler previously used learning algorithm. We observe that the\nsimpler algorithm is less confident, i.e., its recommendations have higher\nentropy. This leads us to explore how the entropy of the inference selection\nimplemented via the neural network influences the proof search. This is related\nto research in human decision-making under uncertainty, and in particular the\nprobability matching theory. Our main result shows that a proper entropy\nregularisation, i.e., training the GNN not to be overconfident, greatly\nimproves plCoP's performance on a large mathematical corpus.",
          "link": "http://arxiv.org/abs/2105.14706",
          "publishedOn": "2021-07-19T00:49:07.934Z",
          "wordCount": 664,
          "title": "The Role of Entropy in Guiding a Connection Prover. (arXiv:2105.14706v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1\">Trevor Ablett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yifan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>",
          "description": "Learned visuomotor policies have shown considerable success as an alternative\nto traditional, hand-crafted frameworks for robotic manipulation. Surprisingly,\nan extension of these methods to the multiview domain is relatively unexplored.\nA successful multiview policy could be deployed on a mobile manipulation\nplatform, allowing the robot to complete a task regardless of its view of the\nscene. In this work, we demonstrate that a multiview policy can be found\nthrough imitation learning by collecting data from a variety of viewpoints. We\nillustrate the general applicability of the method by learning to complete\nseveral challenging multi-stage and contact-rich tasks, from numerous\nviewpoints, both in a simulated environment and on a real mobile manipulation\nplatform. Furthermore, we analyze our policies to determine the benefits of\nlearning from multiview data compared to learning with data collected from a\nfixed perspective. We show that learning from multiview data results in little,\nif any, penalty to performance for a fixed-view task compared to learning with\nan equivalent amount of fixed-view data. Finally, we examine the visual\nfeatures learned by the multiview and fixed-view policies. Our results indicate\nthat multiview policies implicitly learn to identify spatially correlated\nfeatures.",
          "link": "http://arxiv.org/abs/2104.13907",
          "publishedOn": "2021-07-19T00:49:07.928Z",
          "wordCount": 673,
          "title": "Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations. (arXiv:2104.13907v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "As machine learning (ML) continue to be integrated into healthcare systems\nthat affect clinical decision making, new strategies will need to be\nincorporated in order to effectively detect and evaluate subgroup disparities\nto ensure accountability and generalizability in clinical workflows. In this\npaper, we explore how epistemic uncertainty can be used to evaluate disparity\nin patient demographics (race) and data acquisition (scanner) subgroups for\nbreast density assessment on a dataset of 108,190 mammograms collected from 33\nclinical sites. Our results show that even if aggregate performance is\ncomparable, the choice of uncertainty quantification metric can significantly\nthe subgroup level. We hope this analysis can promote further work on how\nuncertainty can be leveraged to increase transparency of machine learning\napplications for clinical deployment.",
          "link": "http://arxiv.org/abs/2107.02716",
          "publishedOn": "2021-07-19T00:49:07.921Z",
          "wordCount": 591,
          "title": "Evaluating subgroup disparity using epistemic uncertainty in mammography. (arXiv:2107.02716v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09048",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Rolle_A/0/1/0/all/0/1\">Alexander Rolle</a>, <a href=\"http://arxiv.org/find/math/1/au:+Scoccola_L/0/1/0/all/0/1\">Luis Scoccola</a>",
          "description": "We present a multiscale, consistent approach to density-based clustering that\nsatisfies stability theorems -- in both the input data and in the parameters --\nwhich hold without distributional assumptions. The stability in the input data\nis with respect to the Gromov--Hausdorff--Prokhorov distance on metric\nprobability spaces and interleaving distances between (multi-parameter)\nhierarchical clusterings we introduce. We prove stability results for standard\nsimplification procedures for hierarchical clusterings, which can be combined\nwith our approach to yield a stable flat clustering algorithm. We illustrate\nthe stability of the approach with computational examples. Our framework is\nbased on the concepts of persistence and interleaving distance from Topological\nData Analysis.",
          "link": "http://arxiv.org/abs/2005.09048",
          "publishedOn": "2021-07-19T00:49:07.914Z",
          "wordCount": 564,
          "title": "Stable and consistent density-based clustering. (arXiv:2005.09048v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02522",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1\">Xueyuan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1\">Shangzhe Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1\">Jichang Zhao</a>",
          "description": "Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.",
          "link": "http://arxiv.org/abs/2106.02522",
          "publishedOn": "2021-07-19T00:49:07.896Z",
          "wordCount": 671,
          "title": "Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v3 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.",
          "link": "http://arxiv.org/abs/2107.05445",
          "publishedOn": "2021-07-19T00:49:07.890Z",
          "wordCount": 591,
          "title": "Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We consider a binary classification problem when the data comes from a\nmixture of two isotropic distributions satisfying concentration and\nanti-concentration properties enjoyed by log-concave distributions among\nothers. We show that there exists a universal constant $C_{\\mathrm{err}}>0$\nsuch that if a pseudolabeler $\\boldsymbol{\\beta}_{\\mathrm{pl}}$ can achieve\nclassification error at most $C_{\\mathrm{err}}$, then for any $\\varepsilon>0$,\nan iterative self-training algorithm initialized at $\\boldsymbol{\\beta}_0 :=\n\\boldsymbol{\\beta}_{\\mathrm{pl}}$ using pseudolabels $\\hat y =\n\\mathrm{sgn}(\\langle \\boldsymbol{\\beta}_t, \\mathbf{x}\\rangle)$ and using at\nmost $\\tilde O(d/\\varepsilon^2)$ unlabeled examples suffices to learn the\nBayes-optimal classifier up to $\\varepsilon$ error, where $d$ is the ambient\ndimension. That is, self-training converts weak learners to strong learners\nusing only unlabeled examples. We additionally show that by running gradient\ndescent on the logistic loss one can obtain a pseudolabeler\n$\\boldsymbol{\\beta}_{\\mathrm{pl}}$ with classification error $C_{\\mathrm{err}}$\nusing only $O(d)$ labeled examples (i.e., independent of $\\varepsilon$).\nTogether our results imply that mixture models can be learned to within\n$\\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled\nexamples and $\\tilde O(d/\\varepsilon^2)$ unlabeled examples by way of a\nsemi-supervised self-training algorithm.",
          "link": "http://arxiv.org/abs/2106.13805",
          "publishedOn": "2021-07-19T00:49:07.884Z",
          "wordCount": 650,
          "title": "Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guanya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honig_W/0/1/0/all/0/1\">Wolfgang H&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xichen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Soon-Jo Chung</a>",
          "description": "We present Neural-Swarm2, a learning-based method for motion planning and\ncontrol that allows heterogeneous multirotors in a swarm to safely fly in close\nproximity. Such operation for drones is challenging due to complex aerodynamic\ninteraction forces, such as downwash generated by nearby drones and ground\neffect. Conventional planning and control methods neglect capturing these\ninteraction forces, resulting in sparse swarm configuration during flight. Our\napproach combines a physics-based nominal dynamics model with learned Deep\nNeural Networks (DNNs) with strong Lipschitz properties. We make use of two\ntechniques to accurately predict the aerodynamic interactions between\nheterogeneous multirotors: i) spectral normalization for stability and\ngeneralization guarantees of unseen data and ii) heterogeneous deep sets for\nsupporting any number of heterogeneous neighbors in a permutation-invariant\nmanner without reducing expressiveness. The learned residual dynamics benefit\nboth the proposed interaction-aware multi-robot motion planning and the\nnonlinear tracking control design because the learned interaction forces reduce\nthe modelling errors. Experimental results demonstrate that Neural-Swarm2 is\nable to generalize to larger swarms beyond training cases and significantly\noutperforms a baseline nonlinear tracking controller with up to three times\nreduction in worst-case tracking errors.",
          "link": "http://arxiv.org/abs/2012.05457",
          "publishedOn": "2021-07-19T00:49:07.877Z",
          "wordCount": 683,
          "title": "Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions. (arXiv:2012.05457v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yizhen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>",
          "description": "Graph representation learning plays a vital role in processing\ngraph-structured data. However, prior arts on graph representation learning\nheavily rely on labeling information. To overcome this problem, inspired by the\nrecent success of graph contrastive learning and Siamese networks in visual\nrepresentation learning, we propose a novel self-supervised approach in this\npaper to learn node representations by enhancing Siamese self-distillation with\nmulti-scale contrastive learning. Specifically, we first generate two augmented\nviews from the input graph based on local and global perspectives. Then, we\nemploy two objectives called cross-view and cross-network contrastiveness to\nmaximize the agreement between node representations across different views and\nnetworks. To demonstrate the effectiveness of our approach, we perform\nempirical experiments on five real-world datasets. Our method not only achieves\nnew state-of-the-art results but also surpasses some semi-supervised\ncounterparts by large margins. Code is made available at\nhttps://github.com/GRAND-Lab/MERIT",
          "link": "http://arxiv.org/abs/2105.05682",
          "publishedOn": "2021-07-19T00:49:07.866Z",
          "wordCount": 632,
          "title": "Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. (arXiv:2105.05682v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muniz_I/0/1/0/all/0/1\">I. Muniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camargo_F/0/1/0/all/0/1\">F. H. F. Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_A/0/1/0/all/0/1\">A. Marques</a>",
          "description": "With the constant advancements of genetic engineering, a common concern is to\nbe able to identify the lab-of-origin of genetically engineered DNA sequences.\nFor that reason, AltLabs has hosted the genetic Engineering Attribution\nChallenge to gather many teams to propose new tools to solve this problem. Here\nwe show our proposed method to rank the most likely labs-of-origin and generate\nembeddings for DNA sequences and labs. These embeddings can also perform\nvarious other tasks, like clustering both DNA sequences and labs and using them\nas features for Machine Learning models applied to solve other problems. This\nwork demonstrates that our method outperforms the classic training method for\nthis task while generating other helpful information.",
          "link": "http://arxiv.org/abs/2107.07878",
          "publishedOn": "2021-07-19T00:49:07.860Z",
          "wordCount": 567,
          "title": "Ranking labs-of-origin for genetically engineered DNA using Metric Learning. (arXiv:2107.07878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-07-19T00:49:07.855Z",
          "wordCount": 611,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.04785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Neural architecture search (NAS) with an accuracy predictor that predicts the\naccuracy of candidate architectures has drawn increasing attention due to its\nsimplicity and effectiveness. Previous works usually employ neural\nnetwork-based predictors which require more delicate design and are easy to\noverfit. Considering that most architectures are represented as sequences of\ndiscrete symbols which are more like tabular data and preferred by non-neural\npredictors, in this paper, we study an alternative approach which uses\nnon-neural model for accuracy prediction. Specifically, as decision tree based\nmodels can better handle tabular data, we leverage gradient boosting decision\ntree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor\ncan achieve comparable (if not better) prediction accuracy than neural network\nbased predictors. Moreover, considering that a compact search space can ease\nthe search process, we propose to prune the search space gradually according to\nimportant features derived from GBDT. In this way, NAS can be performed by\nfirst pruning the search space and then searching a neural architecture, which\nis more efficient and effective. Experiments on NASBench-101 and ImageNet\ndemonstrate the effectiveness of using GBDT as predictor for NAS: (1) On\nNASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,\nregularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global\noptimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further\nachieves 23.4% top-1 error rate on ImageNet when enhanced with search space\npruning. Code is provided in the supplementary materials.",
          "link": "http://arxiv.org/abs/2007.04785",
          "publishedOn": "2021-07-19T00:49:07.849Z",
          "wordCount": 738,
          "title": "Accuracy Prediction with Non-neural Model for Neural Architecture Search. (arXiv:2007.04785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1\">Parikshit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Prathamesh Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>",
          "description": "We present DeepMVI, a deep learning method for missing value imputation in\nmultidimensional time-series datasets. Missing values are commonplace in\ndecision support platforms that aggregate data over long time stretches from\ndisparate sources, and reliable data analytics calls for careful handling of\nmissing data. One strategy is imputing the missing values, and a wide variety\nof algorithms exist spanning simple interpolation, matrix factorization methods\nlike SVD, statistical models like Kalman filters, and recent deep learning\nmethods. We show that often these provide worse results on aggregate analytics\ncompared to just excluding the missing data. DeepMVI uses a neural network to\ncombine fine-grained and coarse-grained patterns along a time series, and\ntrends from related series across categorical dimensions. After failing with\noff-the-shelf neural architectures, we design our own network that includes a\ntemporal transformer with a novel convolutional window feature, and kernel\nregression with learned embeddings. The parameters and their training are\ndesigned carefully to generalize across different placements of missing blocks\nand data characteristics. Experiments across nine real datasets, four different\nmissing scenarios, comparing seven existing methods show that DeepMVI is\nsignificantly more accurate, reducing error by more than 50% in more than half\nthe cases, compared to the best existing method. Although slower than simpler\nmatrix factorization methods, we justify the increased time overheads by\nshowing that DeepMVI is the only option that provided overall more accurate\nanalytics than dropping missing values.",
          "link": "http://arxiv.org/abs/2103.01600",
          "publishedOn": "2021-07-19T00:49:07.816Z",
          "wordCount": 694,
          "title": "Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1\">Jakob Schoeffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_N/0/1/0/all/0/1\">Niklas Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1\">Isabel Valera</a>",
          "description": "Algorithmic decision systems are increasingly used in areas such as hiring,\nschool admission, or loan approval. Typically, these systems rely on labeled\ndata for training a classification model. However, in many scenarios,\nground-truth labels are unavailable, and instead we have only access to\nimperfect labels as the result of (potentially biased) human-made decisions.\nDespite being imperfect, historical decisions often contain some useful\ninformation on the unobserved true labels. In this paper, we focus on scenarios\nwhere only imperfect labels are available and propose a new fair ranking-based\ndecision system based on monotonic relationships between legitimate features\nand the outcome. Our approach is both intuitive and easy to implement, and thus\nparticularly suitable for adoption in real-world settings. More in detail, we\nintroduce a distance-based decision criterion, which incorporates useful\ninformation from historical decisions and accounts for unwanted correlation\nbetween protected and legitimate features. Through extensive experiments on\nsynthetic and real-world data, we show that our method is fair in the sense\nthat a) it assigns the desirable outcome to the most qualified individuals, and\nb) it removes the effect of stereotypes in decision-making, thereby\noutperforming traditional classification algorithms. Additionally, we are able\nto show theoretically that our method is consistent with a prominent concept of\nindividual fairness which states that \"similar individuals should be treated\nsimilarly.\"",
          "link": "http://arxiv.org/abs/2102.04565",
          "publishedOn": "2021-07-19T00:49:07.808Z",
          "wordCount": 688,
          "title": "A Ranking Approach to Fair Classification. (arXiv:2102.04565v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jia-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_T/0/1/0/all/0/1\">Tao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1\">Bin Tong</a>",
          "description": "Conversion rate (CVR) prediction is one of the most critical tasks for\ndigital display advertising. Commercial systems often require to update models\nin an online learning manner to catch up with the evolving data distribution.\nHowever, conversions usually do not happen immediately after a user click. This\nmay result in inaccurate labeling, which is called delayed feedback problem. In\nprevious studies, delayed feedback problem is handled either by waiting\npositive label for a long period of time, or by consuming the negative sample\non its arrival and then insert a positive duplicate when a conversion happens\nlater. Indeed, there is a trade-off between waiting for more accurate labels\nand utilizing fresh data, which is not considered in existing works. To strike\na balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback\nModel (ES-DFM), which models the relationship between the observed conversion\ndistribution and the true conversion distribution. Then we optimize the\nexpectation of true conversion distribution via importance sampling under the\nelapsed-time sampling distribution. We further estimate the importance weight\nfor each instance, which is used as the weight of loss function in CVR\nprediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive\nexperiments on a public data and a private industrial dataset. Experimental\nresults confirm that our method consistently outperforms the previous\nstate-of-the-art results.",
          "link": "http://arxiv.org/abs/2012.03245",
          "publishedOn": "2021-07-19T00:49:07.797Z",
          "wordCount": 713,
          "title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling. (arXiv:2012.03245v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galindo_Torres_S/0/1/0/all/0/1\">S.A. Galindo-Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "A better understanding of dispersion in natural streams requires knowledge of\nlongitudinal dispersion coefficient(LDC). Various methods have been proposed\nfor predictions of LDC. Those studies can be grouped into three types:\nanalytical, statistical and ML-driven researches(Implicit and explicit).\nHowever, a comprehensive evaluation of them is still lacking. In this paper, we\nfirst present an in-depth analysis of those methods and find out their defects.\nThis is carried out on an extensive database composed of 660 samples of\nhydraulic and channel properties worldwide. The reliability and\nrepresentativeness of utilized data are enhanced through the deployment of the\nSubset Selection of Maximum Dissimilarity(SSMD) for testing set selection and\nthe Inter Quartile Range(IQR) for removal of the outlier. The evaluation\nreveals the rank of those methods as: ML-driven method > the statistical method\n> the analytical method. Whereas implicit ML-driven methods are black-boxes in\nnature, explicit ML-driven methods have more potential in prediction of LDC.\nBesides, overfitting is a universal problem in existing models. Those models\nalso suffer from a fixed parameter combination. To establish an interpretable\nmodel for LDC prediction with higher performance, we then design a novel\nsymbolic regression method called evolutionary symbolic regression\nnetwork(ESRN). It is a combination of genetic algorithms and neural networks.\nStrategies are introduced to avoid overfitting and explore more parameter\ncombinations. Results show that the ESRN model has superiorities over other\nexisting symbolic models in performance. The proposed model is suitable for\npractical engineering problems due to its advantage in low requirement of\nparameters (only w and U* are required). It can provide convincing solutions\nfor situations where the field test cannot be carried out or limited field\ninformation can be obtained.",
          "link": "http://arxiv.org/abs/2106.11026",
          "publishedOn": "2021-07-19T00:49:07.788Z",
          "wordCount": 788,
          "title": "A data-based comparative review and AI-driven symbolic model for longitudinal dispersion coefficient in natural streams. (arXiv:2106.11026v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07863",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1\">Ushnish Sengupta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kontogiannis_A/0/1/0/all/0/1\">Alexandros Kontogiannis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1\">Matthew P. Juniper</a>",
          "description": "Magnetic resonance velocimetry (MRV) is a non-invasive experimental technique\nwidely used in medicine and engineering to measure the velocity field of a\nfluid. These measurements are dense but have a low signal-to-noise ratio (SNR).\nThe measurements can be de-noised by imposing physical constraints on the flow,\nwhich are encapsulated in governing equations for mass and momentum. Previous\nstudies have required the shape of the boundary (for example, a blood vessel)\nto be known a priori. This, however, requires a set of additional measurements,\nwhich can be expensive to obtain. In this paper, we present a physics-informed\nneural network that instead uses the noisy MRV data alone to simultaneously\ninfer the most likely boundary shape and de-noised velocity field. We achieve\nthis by training an auxiliary neural network that takes the value 1.0 within\nthe inferred domain of the governing PDE and 0.0 outside. This network is used\nto weight the PDE residual term in the loss function accordingly and implicitly\nlearns the geometry of the system. We test our algorithm by assimilating both\nsynthetic and real MRV measurements for flows that can be well modeled by the\nPoisson and Stokes equations. We find that we are able to reconstruct very\nnoisy (SNR = 2.5) MRV signals and recover the ground truth with low\nreconstruction errors of 3.7 - 7.5%. The simplicity and flexibility of our\nphysics-informed neural network approach can readily scale to assimilating MRV\ndata with complex 3D geometries, time-varying 4D data, or unknown parameters in\nthe physical model.",
          "link": "http://arxiv.org/abs/2107.07863",
          "publishedOn": "2021-07-19T00:49:07.736Z",
          "wordCount": 698,
          "title": "Simultaneous boundary shape estimation and velocity field de-noising in Magnetic Resonance Velocimetry using Physics-informed Neural Networks. (arXiv:2107.07863v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chaochao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jo&#x15b;e Miguel Hern&#xe1;ndez-Lobato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Due to spurious correlations, machine learning systems often fail to\ngeneralize to environments whose distributions differ from the ones used at\ntraining time. Prior work addressing this, either explicitly or implicitly,\nattempted to find a data representation that has an invariant relationship with\nthe target. This is done by leveraging a diverse set of training environments\nto reduce the effect of spurious features and build an invariant predictor.\nHowever, these methods have generalization guarantees only when both data\nrepresentation and classifiers come from a linear model class. We propose\ninvariant Causal Representation Learning (iCaRL), an approach that enables\nout-of-distribution (OOD) generalization in the nonlinear setting (i.e.,\nnonlinear representations and nonlinear classifiers). It builds upon a\npractical and general assumption: the prior over the data representation (i.e.,\na set of latent variables encoding the data) given the target and the\nenvironment belongs to general exponential family distributions. Based on this,\nwe show that it is possible to identify the data representation up to simple\ntransformations. We also prove that all direct causes of the target can be\nfully discovered, which further enables us to obtain generalization guarantees\nin the nonlinear setting. Extensive experiments on both synthetic and\nreal-world datasets show that our approach outperforms a variety of baseline\nmethods. Finally, in the discussion, we further explore the aforementioned\nassumption and propose a more general hypothesis, called the Agnostic\nHypothesis: there exist a set of hidden causal factors affecting both inputs\nand outcomes. The Agnostic Hypothesis can provide a unifying view of machine\nlearning. More importantly, it can inspire a new direction to explore a general\ntheory for identifying hidden causal factors, which is key to enabling the OOD\ngeneralization guarantees.",
          "link": "http://arxiv.org/abs/2102.12353",
          "publishedOn": "2021-07-19T00:49:07.724Z",
          "wordCount": 752,
          "title": "Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "Collecting and aggregating information from several probability measures or\nhistograms is a fundamental task in machine learning. One of the popular\nsolution methods for this task is to compute the barycenter of the probability\nmeasures under the Wasserstein metric. However, approximating the Wasserstein\nbarycenter is numerically challenging because of the curse of dimensionality.\nThis paper proposes the projection robust Wasserstein barycenter (PRWB) that\nhas the potential to mitigate the curse of dimensionality. Since PRWB is\nnumerically very challenging to solve, we further propose a relaxed PRWB\n(RPRWB) model, which is more tractable. The RPRWB projects the probability\nmeasures onto a lower-dimensional subspace that maximizes the Wasserstein\nbarycenter objective. The resulting problem is a max-min problem over the\nStiefel manifold. By combining the iterative Bregman projection algorithm and\nRiemannian optimization, we propose two new algorithms for computing the RPRWB.\nThe complexity of arithmetic operations of the proposed algorithms for\nobtaining an $\\epsilon$-stationary solution is analyzed. We incorporate the\nRPRWB into a discrete distribution clustering algorithm, and the numerical\nresults on real text datasets confirm that our RPRWB model helps improve the\nclustering performance significantly.",
          "link": "http://arxiv.org/abs/2102.03390",
          "publishedOn": "2021-07-19T00:49:07.718Z",
          "wordCount": 644,
          "title": "Projection Robust Wasserstein Barycenters. (arXiv:2102.03390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lifeng Lai</a>",
          "description": "The Wasserstein distance has become increasingly important in machine\nlearning and deep learning. Despite its popularity, the Wasserstein distance is\nhard to approximate because of the curse of dimensionality. A recently proposed\napproach to alleviate the curse of dimensionality is to project the sampled\ndata from the high dimensional probability distribution onto a\nlower-dimensional subspace, and then compute the Wasserstein distance between\nthe projected data. However, this approach requires to solve a max-min problem\nover the Stiefel manifold, which is very challenging in practice. The only\nexisting work that solves this problem directly is the RGAS (Riemannian\nGradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an\nentropy-regularized optimal transport problem in each iteration, and thus can\nbe costly for large-scale problems. In this paper, we propose a Riemannian\nblock coordinate descent (RBCD) method to solve this problem, which is based on\na novel reformulation of the regularized max-min problem over the Stiefel\nmanifold. We show that the complexity of arithmetic operations for RBCD to\nobtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$. This significantly\nimproves the corresponding complexity of RGAS, which is $O(\\epsilon^{-12})$.\nMoreover, our RBCD has very low per-iteration complexity, and hence is suitable\nfor large-scale problems. Numerical results on both synthetic and real datasets\ndemonstrate that our method is more efficient than existing methods, especially\nwhen the number of sampled data is very large.",
          "link": "http://arxiv.org/abs/2012.05199",
          "publishedOn": "2021-07-19T00:49:07.702Z",
          "wordCount": 720,
          "title": "A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance. (arXiv:2012.05199v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1\">Panagiotis A. Traganitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1\">Georgios B. Giannakis</a>",
          "description": "Crowdsourcing has emerged as a powerful paradigm for efficiently labeling\nlarge datasets and performing various learning tasks, by leveraging crowds of\nhuman annotators. When additional information is available about the data,\nsemi-supervised crowdsourcing approaches that enhance the aggregation of labels\nfrom human annotators are well motivated. This work deals with semi-supervised\ncrowdsourced classification, under two regimes of semi-supervision: a) label\nconstraints, that provide ground-truth labels for a subset of data; and b)\npotentially easier to obtain instance-level constraints, that indicate\nrelationships between pairs of data. Bayesian algorithms based on variational\ninference are developed for each regime, and their quantifiably improved\nperformance, compared to unsupervised crowdsourcing, is analytically and\nempirically validated on several crowdsourcing datasets.",
          "link": "http://arxiv.org/abs/2012.11048",
          "publishedOn": "2021-07-19T00:49:07.695Z",
          "wordCount": 574,
          "title": "Bayesian Crowdsourcing with Constraints. (arXiv:2012.11048v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marklund_H/0/1/0/all/0/1\">Henrik Marklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marvin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1\">Akshay Balsubramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_R/0/1/0/all/0/1\">Richard Lanas Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Etienne David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1\">Ian Stavness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1\">Berton A. Earnshaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1\">Imran S. Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundaje_A/0/1/0/all/0/1\">Anshul Kundaje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emma Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "Distribution shifts -- where the training distribution differs from the test\ndistribution -- can substantially degrade the accuracy of machine learning (ML)\nsystems deployed in the wild. Despite their ubiquity in the real-world\ndeployments, these distribution shifts are under-represented in the datasets\nwidely used in the ML community today. To address this gap, we present WILDS, a\ncurated benchmark of 10 datasets reflecting a diverse range of distribution\nshifts that naturally arise in real-world applications, such as shifts across\nhospitals for tumor identification; across camera traps for wildlife\nmonitoring; and across time and location in satellite imaging and poverty\nmapping. On each dataset, we show that standard training yields substantially\nlower out-of-distribution than in-distribution performance. This gap remains\neven with models trained by existing methods for tackling distribution shifts,\nunderscoring the need for new methods for training models that are more robust\nto the types of distribution shifts that arise in practice. To facilitate\nmethod development, we provide an open-source package that automates dataset\nloading, contains default model architectures and hyperparameters, and\nstandardizes evaluations. Code and leaderboards are available at\nhttps://wilds.stanford.edu.",
          "link": "http://arxiv.org/abs/2012.07421",
          "publishedOn": "2021-07-19T00:49:07.662Z",
          "wordCount": 696,
          "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts. (arXiv:2012.07421v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>",
          "description": "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.",
          "link": "http://arxiv.org/abs/2010.11524",
          "publishedOn": "2021-07-19T00:49:07.642Z",
          "wordCount": 654,
          "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arpita Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_D/0/1/0/all/0/1\">Debasis Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarma_M/0/1/0/all/0/1\">Monalisa Sarma</a>",
          "description": "User intention which often changes dynamically is considered to be an\nimportant factor for modeling users in the design of recommendation systems.\nRecent studies are starting to focus on predicting user intention (what users\nwant) beyond user preference (what users like). In this work, a user intention\nmodel is proposed based on deep sequential topic analysis. The model predicts a\nuser's intention in terms of the topic of interest. The Hybrid Topic Model\n(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to\nderive the topic of interest of users and the history of preferences. HTM finds\nthe true topics of papers estimating word-topic distribution which includes\nsyntactic and semantic correlations among words. Next, to model user intention,\na Long Short Term Memory (LSTM) based sequential deep learning model is\nproposed. This model takes into account temporal context, namely the time\ndifference between clicks of two consecutive papers seen by a user. Extensive\nexperiments with the real-world research paper dataset indicate that the\nproposed approach significantly outperforms the state-of-the-art methods.\nFurther, the proposed approach introduces a new road map to model a user\nactivity suitable for the design of a research paper recommendation system.",
          "link": "http://arxiv.org/abs/2107.07831",
          "publishedOn": "2021-07-19T00:49:07.625Z",
          "wordCount": 630,
          "title": "Modeling User Behaviour in Research Paper Recommendation System. (arXiv:2107.07831v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1\">Eli Meirom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>",
          "description": "Graph neural networks (GNNs) can process graphs of different sizes, but their\nability to generalize across sizes, specifically from small to large graphs, is\nstill not well understood. In this paper, we identify an important type of data\nwhere generalization from small to large graphs is challenging: graph\ndistributions for which the local structure depends on the graph size. This\neffect occurs in multiple important graph learning domains, including social\nand biological networks. We first prove that when there is a difference between\nthe local structures, GNNs are not guaranteed to generalize across sizes: there\nare \"bad\" global minima that do well on small graphs but fail on large graphs.\nWe then study the size-generalization problem empirically and demonstrate that\nwhen there is a discrepancy in local structure, GNNs tend to converge to\nnon-generalizing solutions. Finally, we suggest two approaches for improving\nsize generalization, motivated by our findings. Notably, we propose a novel\nSelf-Supervised Learning (SSL) task aimed at learning meaningful\nrepresentations of local structures that appear in large graphs. Our SSL task\nimproves classification accuracy on several popular datasets.",
          "link": "http://arxiv.org/abs/2010.08853",
          "publishedOn": "2021-07-19T00:49:07.610Z",
          "wordCount": 674,
          "title": "From Local Structures to Size Generalization in Graph Neural Networks. (arXiv:2010.08853v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Recent advances show that neural networks embedded with physics-informed\npriors significantly outperform vanilla neural networks in learning and\npredicting the long term dynamics of complex physical systems from noisy data.\nDespite this success, there has only been a limited study on how to optimally\ncombine physics priors to improve predictive performance. To tackle this\nproblem we unpack and generalize recent innovations into individual inductive\nbias segments. As such, we are able to systematically investigate all possible\ncombinations of inductive biases of which existing methods are a natural\nsubset. Using this framework we introduce Variational Integrator Graph Networks\n- a novel method that unifies the strengths of existing approaches by combining\nan energy constraint, high-order symplectic variational integrators, and graph\nneural networks. We demonstrate, across an extensive ablation, that the\nproposed unifying framework outperforms existing methods, for data-efficient\nlearning and in predictive accuracy, across both single and many-body problems\nstudied in recent literature. We empirically show that the improvements arise\nbecause high order variational integrators combined with a potential energy\nconstraint induce coupled learning of generalized position and momentum updates\nwhich can be formalized via the Partitioned Runge-Kutta method.",
          "link": "http://arxiv.org/abs/2004.13688",
          "publishedOn": "2021-07-19T00:49:07.603Z",
          "wordCount": 669,
          "title": "Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems. (arXiv:2004.13688v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07757",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Musso_D/0/1/0/all/0/1\">Daniele Musso</a>",
          "description": "Local entropic loss functions provide a versatile framework to define\narchitecture-aware regularization procedures. Besides the possibility of being\nanisotropic in the synaptic space, the local entropic smoothening of the loss\nfunction can vary during training, thus yielding a tunable model complexity. A\nscoping protocol where the regularization is strong in the early-stage of the\ntraining and then fades progressively away constitutes an alternative to\nstandard initialization procedures for deep convolutional neural networks,\nnonetheless, it has wider applicability. We analyze anisotropic, local entropic\nsmoothenings in the language of statistical physics and information theory,\nproviding insight into both their interpretation and workings. We comment some\naspects related to the physics of renormalization and the spacetime structure\nof convolutional networks.",
          "link": "http://arxiv.org/abs/2107.07757",
          "publishedOn": "2021-07-19T00:49:07.595Z",
          "wordCount": 563,
          "title": "Entropic alternatives to initialization. (arXiv:2107.07757v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>",
          "description": "It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.",
          "link": "http://arxiv.org/abs/2107.07684",
          "publishedOn": "2021-07-19T00:49:07.581Z",
          "wordCount": 561,
          "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation. (arXiv:2107.07684v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_L/0/1/0/all/0/1\">Long Kiu Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Adam Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knowles_D/0/1/0/all/0/1\">Derek Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kousik_S/0/1/0/all/0/1\">Shreyas Kousik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Grace X. Gao</a>",
          "description": "Neural networks have recently become popular for a wide variety of uses, but\nhave seen limited application in safety-critical domains such as robotics near\nand around humans. This is because it remains an open challenge to train a\nneural network to obey safety constraints. Most existing safety-related methods\nonly seek to verify that already-trained networks obey constraints, requiring\nalternating training and verification. Instead, this work proposes a\nconstrained method to simultaneously train and verify a feedforward neural\nnetwork with rectified linear unit (ReLU) nonlinearities. Constraints are\nenforced by computing the network's output-space reachable set and ensuring\nthat it does not intersect with unsafe sets; training is achieved by\nformulating a novel collision-check loss function between the reachable set and\nunsafe portions of the output space. The reachable and unsafe sets are\nrepresented by constrained zonotopes, a convex polytope representation that\nenables differentiable collision checking. The proposed method is demonstrated\nsuccessfully on a network with one nonlinearity layer and approximately 50\nparameters.",
          "link": "http://arxiv.org/abs/2107.07696",
          "publishedOn": "2021-07-19T00:49:07.574Z",
          "wordCount": 608,
          "title": "Constrained Feedforward Neural Network Training via Reachability Analysis. (arXiv:2107.07696v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00628",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Frasch_M/0/1/0/all/0/1\">Martin G. Frasch</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Strong_S/0/1/0/all/0/1\">Shadrian B. Strong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nilosek_D/0/1/0/all/0/1\">David Nilosek</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Leaverton_J/0/1/0/all/0/1\">Joshua Leaverton</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schifrin_B/0/1/0/all/0/1\">Barry S. Schifrin</a>",
          "description": "Despite broad application during labor and delivery, there remains\nconsiderable debate about the value of electronic fetal monitoring (EFM). EFM\nincludes the surveillance of the fetal heart rate (FHR) patterns in conjunction\nwith the maternal uterine contractions providing a wealth of data about fetal\nbehavior and the threat of diminished oxygenation and perfusion. Adverse\noutcomes universally associate a fetal injury with the failure to timely\nrespond to FHR pattern information. Historically, the EFM data, stored\ndigitally, are available only as rasterized pdf images for contemporary or\nhistorical discussion and examination. In reality, however, they are rarely\nreviewed systematically. Using a unique archive of EFM collected over 50 years\nof practice in conjunction with adverse outcomes, we present a deep learning\nframework for training and detection of incipient or past fetal injury. We\nreport 94% accuracy in identifying early, preventable fetal injury intrapartum.\nThis framework is suited for automating an early warning and decision support\nsystem for maintaining fetal well-being during the stresses of labor.\nUltimately, such a system could enable a physician to timely respond during\nlabor and prevent adverse outcomes. When adverse outcomes cannot be avoided,\nthey can provide guidance to the early neuroprotective treatment of the\nnewborn.",
          "link": "http://arxiv.org/abs/2106.00628",
          "publishedOn": "2021-07-19T00:49:07.517Z",
          "wordCount": 666,
          "title": "Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning. (arXiv:2106.00628v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silver_T/0/1/0/all/0/1\">Tom Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>",
          "description": "Robotic planning problems in hybrid state and action spaces can be solved by\nintegrated task and motion planners (TAMP) that handle the complex interaction\nbetween motion-level decisions and task-level plan feasibility. TAMP approaches\nrely on domain-specific symbolic operators to guide the task-level search,\nmaking planning efficient. In this work, we formalize and study the problem of\noperator learning for TAMP. Central to this study is the view that operators\ndefine a lossy abstraction of the transition model of a domain. We then propose\na bottom-up relational learning method for operator learning and show how the\nlearned operators can be used for planning in a TAMP system. Experimentally, we\nprovide results in three domains, including long-horizon robotic planning\ntasks. We find our approach to substantially outperform several baselines,\nincluding three graph neural network-based model-free approaches from the\nrecent literature. Video: https://youtu.be/iVfpX9BpBRo Code:\nhttps://git.io/JCT0g",
          "link": "http://arxiv.org/abs/2103.00589",
          "publishedOn": "2021-07-19T00:49:07.500Z",
          "wordCount": 619,
          "title": "Learning Symbolic Operators for Task and Motion Planning. (arXiv:2103.00589v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>",
          "description": "The focus of disentanglement approaches has been on identifying independent\nfactors of variation in data. However, the causal variables underlying\nreal-world observations are often not statistically independent. In this work,\nwe bridge the gap to real-world scenarios by analyzing the behavior of the most\nprominent disentanglement approaches on correlated data in a large-scale\nempirical study (including 4260 models). We show and quantify that\nsystematically induced correlations in the dataset are being learned and\nreflected in the latent representations, which has implications for downstream\napplications of disentanglement such as fairness. We also demonstrate how to\nresolve these latent correlations, either using weak supervision during\ntraining or by post-hoc correcting a pre-trained model with a small number of\nlabels.",
          "link": "http://arxiv.org/abs/2006.07886",
          "publishedOn": "2021-07-19T00:49:07.484Z",
          "wordCount": 610,
          "title": "On Disentangled Representations Learned From Correlated Data. (arXiv:2006.07886v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07634",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>",
          "description": "Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\noutperformed the conventional multi-task learning with split branches and a\nbi-directional long short-team memory decoder by 12% on average.",
          "link": "http://arxiv.org/abs/2107.07634",
          "publishedOn": "2021-07-19T00:49:07.478Z",
          "wordCount": 652,
          "title": "Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05073",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mikuni_V/0/1/0/all/0/1\">Vinicius Mikuni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Canelli_F/0/1/0/all/0/1\">Florencia Canelli</a>",
          "description": "Methods for processing point cloud information have seen a great success in\ncollider physics applications. One recent breakthrough in machine learning is\nthe usage of Transformer networks to learn semantic relationships between\nsequences in language processing. In this work, we apply a modified Transformer\nnetwork called Point Cloud Transformer as a method to incorporate the\nadvantages of the Transformer architecture to an unordered set of particles\nresulting from collision events. To compare the performance with other\nstrategies, we study jet-tagging applications for highly-boosted particles.",
          "link": "http://arxiv.org/abs/2102.05073",
          "publishedOn": "2021-07-19T00:49:07.471Z",
          "wordCount": 559,
          "title": "Point Cloud Transformers applied to Collider Physics. (arXiv:2102.05073v2 [physics.data-an] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Junghyo Jo</a>",
          "description": "The outstanding performance of deep learning in various fields has been a\nfundamental query, which can be potentially examined using information theory\nthat interprets the learning process as the transmission and compression of\ninformation. Information plane analyses of the mutual information between the\ninput-hidden-output layers demonstrated two distinct learning phases of fitting\nand compression. It is debatable if the compression phase is necessary to\ngeneralize the input-output relations extracted from training data. In this\nstudy, we investigated this through experiments with various species of\nautoencoders and evaluated their information processing phase with an accurate\nkernel-based estimator of mutual information. Given sufficient training data,\nvanilla autoencoders demonstrated the compression phase, which was amplified\nafter imposing sparsity regularization for hidden activities. However, we found\nthat the compression phase is not universally observed in different species of\nautoencoders, including variational autoencoders, that have special constraints\non network weights or manifold of hidden space. These types of autoencoders\nexhibited perfect generalization ability for test data without requiring the\ncompression phase. Thus, we conclude that the compression phase is not\nnecessary for generalization in representation learning.",
          "link": "http://arxiv.org/abs/2102.07402",
          "publishedOn": "2021-07-19T00:49:07.466Z",
          "wordCount": 633,
          "title": "Information flows of diverse autoencoders. (arXiv:2102.07402v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).",
          "link": "http://arxiv.org/abs/2107.07706",
          "publishedOn": "2021-07-19T00:49:07.450Z",
          "wordCount": 687,
          "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference. (arXiv:2107.07706v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berg_J/0/1/0/all/0/1\">Jan Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>",
          "description": "Automated audio captioning (AAC) is the task of automatically creating\ntextual descriptions (i.e. captions) for the contents of a general audio\nsignal. Most AAC methods are using existing datasets to optimize and/or\nevaluate upon. Given the limited information held by the AAC datasets, it is\nvery likely that AAC methods learn only the information contained in the\nutilized datasets. In this paper we present a first approach for continuously\nadapting an AAC method to new information, using a continual learning method.\nIn our scenario, a pre-optimized AAC method is used for some unseen general\naudio signals and can update its parameters in order to adapt to the new\ninformation, given a new reference caption. We evaluate our method using a\nfreely available, pre-optimized AAC method and two freely available AAC\ndatasets. We compare our proposed method with three scenarios, two of training\non one of the datasets and evaluating on the other and a third of training on\none dataset and fine-tuning on the other. Obtained results show that our method\nachieves a good balance between distilling new knowledge and not forgetting the\nprevious one.",
          "link": "http://arxiv.org/abs/2107.08028",
          "publishedOn": "2021-07-19T00:49:07.440Z",
          "wordCount": 628,
          "title": "Continual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach. (arXiv:2107.08028v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1\">Meng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1\">Louis-Pascal Xhonneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "This paper studies learning logic rules for reasoning on knowledge graphs.\nLogic rules provide interpretable explanations when used for prediction as well\nas being able to generalize to other tasks, and hence are critical to learn.\nExisting methods either suffer from the problem of searching in a large search\nspace (e.g., neural logic programming) or ineffective optimization due to\nsparse rewards (e.g., techniques based on reinforcement learning). To address\nthese limitations, this paper proposes a probabilistic model called RNNLogic.\nRNNLogic treats logic rules as a latent variable, and simultaneously trains a\nrule generator as well as a reasoning predictor with logic rules. We develop an\nEM-based algorithm for optimization. In each iteration, the reasoning predictor\nis first updated to explore some generated logic rules for reasoning. Then in\nthe E-step, we select a set of high-quality rules from all generated rules with\nboth the rule generator and reasoning predictor via posterior inference; and in\nthe M-step, the rule generator is updated with the rules selected in the\nE-step. Experiments on four datasets prove the effectiveness of RNNLogic.",
          "link": "http://arxiv.org/abs/2010.04029",
          "publishedOn": "2021-07-19T00:49:07.391Z",
          "wordCount": 647,
          "title": "RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs. (arXiv:2010.04029v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.09910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1\">Johan A.K. Suykens</a>",
          "description": "This paper generalizes regularized regression problems in a hyper-reproducing\nkernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning\nand out-of-sample extensions, and proves asymptotic convergence results for the\nintroduced regression models in an approximation theory view. Algorithmically,\nwe consider two regularized regression models with bivariate forms in this\nspace, including kernel ridge regression (KRR) and support vector regression\n(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with\nNystr\\\"{o}m approximation for scalability in large sample cases. This framework\nis general: the underlying kernel is learned from a broad class, and can be\npositive definite or not, which adapts to various requirements in kernel\nlearning. Theoretically, we study the convergence behavior of regularized\nregression algorithms in hyper-RKHS and derive the learning rates, which goes\nbeyond the classical analysis on RKHS due to the non-trivial independence of\npairwise samples and the characterisation of hyper-RKHS. Experimentally,\nresults on several benchmarks suggest that the employed framework is able to\nlearn a general kernel function form an arbitrary similarity matrix, and thus\nachieves a satisfactory performance on classification tasks.",
          "link": "http://arxiv.org/abs/1809.09910",
          "publishedOn": "2021-07-19T00:49:07.386Z",
          "wordCount": 656,
          "title": "Generalization Properties of hyper-RKHS and its Applications. (arXiv:1809.09910v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achaji_L/0/1/0/all/0/1\">Lina Achaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouqueray_T/0/1/0/all/0/1\">Thibault Fouqueray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aioun_F/0/1/0/all/0/1\">Francois Aioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1\">Francois Charpillet</a>",
          "description": "The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.",
          "link": "http://arxiv.org/abs/2107.08031",
          "publishedOn": "2021-07-19T00:49:07.373Z",
          "wordCount": 656,
          "title": "Is attention to bounding boxes all you need for pedestrian action prediction?. (arXiv:2107.08031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoxian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>",
          "description": "We introduce a new class of graph neural networks (GNNs), by combining\nseveral concepts that were so far studied independently - graph kernels,\nattention-based networks with structural priors and more recently, efficient\nTransformers architectures applying small memory footprint implicit attention\nmethods via low rank decomposition techniques. The goal of the paper is\ntwofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much\nmore expressive than SOTA GNNs as capable of modeling longer-range dependencies\nwithin a single layer. Consequently, they can use more shallow architecture\ndesign. Furthermore, GKAT attention layers scale linearly rather than\nquadratically in the number of nodes of the input graphs, even when those\ngraphs are dense, requiring less compute than their regular graph attention\ncounterparts. They achieve it by applying new classes of graph kernels\nadmitting random feature map decomposition via random walks on graphs. As a\nbyproduct of the introduced techniques, we obtain a new class of learnable\ngraph sketches, called graphots, compactly encoding topological graph\nproperties as well as nodes' features. We conducted exhaustive empirical\ncomparison of our method with nine different GNN classes on tasks ranging from\nmotif detection through social network classification to bioinformatics\nchallenges, showing consistent gains coming from GKATs.",
          "link": "http://arxiv.org/abs/2107.07999",
          "publishedOn": "2021-07-19T00:49:07.330Z",
          "wordCount": 630,
          "title": "Graph Kernel Attention Transformers. (arXiv:2107.07999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tanveer Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalas_A/0/1/0/all/0/1\">Antonis Michalas</a>",
          "description": "Social networking and micro-blogging services, such as Twitter, play an\nimportant role in sharing digital information. Despite the popularity and\nusefulness of social media, there have been many instances where corrupted\nusers found ways to abuse it, as for instance, through raising or lowering\nuser's credibility. As a result, while social media facilitates an\nunprecedented ease of access to information, it also introduces a new challenge\n- that of ascertaining the credibility of shared information. Currently, there\nis no automated way of determining which news or users are credible and which\nare not. Hence, establishing a system that can measure the social media user's\ncredibility has become an issue of great importance. Assigning a credibility\nscore to a user has piqued the interest of not only the research community but\nalso most of the big players on both sides - such as Facebook, on the side of\nindustry, and political parties on the societal one. In this work, we created a\nmodel which, we hope, will ultimately facilitate and support the increase of\ntrust in the social network communities. Our model collected data and analysed\nthe behaviour of~50,000 politicians on Twitter. Influence score, based on\nseveral chosen features, was assigned to each evaluated user. Further, we\nclassified the political Twitter users as either trusted or untrusted using\nrandom forest, multilayer perceptron, and support vector machine. An active\nlearning model was used to classify any unlabelled ambiguous records from our\ndataset. Finally, to measure the performance of the proposed model, we used\nprecision, recall, F1 score, and accuracy as the main evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.08027",
          "publishedOn": "2021-07-19T00:49:07.323Z",
          "wordCount": 704,
          "title": "SOK: Seeing and Believing: Evaluating the Trustworthiness of Twitter Users. (arXiv:2107.08027v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1\">Matthew Mattina</a>",
          "description": "Exploiting sparsity is a key technique in accelerating quantized\nconvolutional neural network (CNN) inference on mobile devices. Prior sparse\nCNN accelerators largely exploit un-structured sparsity and achieve significant\nspeedups. Due to the unbounded, largely unpredictable sparsity patterns,\nhowever, exploiting unstructured sparsity requires complicated hardware design\nwith significant energy and area overhead, which is particularly detrimental to\nmobile/IoT inference scenarios where energy and area efficiency are crucial. We\npropose to exploit structured sparsity, more specifically, Density Bound Block\n(DBB) sparsity for both weights and activations. DBB block tensors bound the\nmaximum number of non-zeros per block. DBB thus exposes statically predictable\nsparsity patterns that enable lean sparsity-exploiting hardware. We propose new\nhardware primitives to implement DBB sparsity for (static) weights and\n(dynamic) activations, respectively, with very low overheads. Building on top\nof the primitives, we describe S2TA, a systolic array-based CNN accelerator\nthat exploits joint weight and activation DBB sparsity and new dimensions of\ndata reuse unavailable on the traditional systolic array. S2TA in 16nm achieves\nmore than 2x speedup and energy reduction compared to a strong baseline of a\nsystolic array with zero-value clock gating, over five popular CNN benchmarks.\nCompared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and\nSparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per\ninference, respectively.",
          "link": "http://arxiv.org/abs/2107.07983",
          "publishedOn": "2021-07-19T00:49:07.318Z",
          "wordCount": 652,
          "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration. (arXiv:2107.07983v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.02653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sigrist_F/0/1/0/all/0/1\">Fabio Sigrist</a>",
          "description": "We introduce a novel way to combine boosting with Gaussian process and mixed\neffects models. This allows for relaxing, first, the linearity assumption for\nthe mean function in Gaussian process and grouped random effects models in a\nflexible non-parametric way and, second, the independence assumption made in\nmost boosting algorithms. The former is advantageous for predictive accuracy\nand for avoiding model misspecifications. The latter is important for more\nefficient learning of the mean function and for obtaining probabilistic\npredictions. In addition, we present an extension that scales to large data\nusing a Vecchia approximation for the Gaussian process model relying on novel\nresults for covariance parameter inference. We obtain increased predictive\naccuracy compared to existing approaches on several simulated and real-world\ndata sets.",
          "link": "http://arxiv.org/abs/2004.02653",
          "publishedOn": "2021-07-19T00:49:07.312Z",
          "wordCount": 583,
          "title": "Gaussian Process Boosting. (arXiv:2004.02653v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08013",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Miles_C/0/1/0/all/0/1\">Cole Miles</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Carbone_M/0/1/0/all/0/1\">Matthew R. Carbone</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sturm_E/0/1/0/all/0/1\">Erica J. Sturm</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lu_D/0/1/0/all/0/1\">Deyu Lu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Weichselbaum_A/0/1/0/all/0/1\">Andreas Weichselbaum</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barros_K/0/1/0/all/0/1\">Kipton Barros</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Konik_R/0/1/0/all/0/1\">Robert M. Konik</a>",
          "description": "We employ variational autoencoders to extract physical insight from a dataset\nof one-particle Anderson impurity model spectral functions. Autoencoders are\ntrained to find a low-dimensional, latent space representation that faithfully\ncharacterizes each element of the training set, as measured by a reconstruction\nerror. Variational autoencoders, a probabilistic generalization of standard\nautoencoders, further condition the learned latent space to promote highly\ninterpretable features. In our study, we find that the learned latent space\ncomponents strongly correlate with well known, but nontrivial, parameters that\ncharacterize emergent behaviors in the Anderson impurity model. In particular,\none latent space component correlates with particle-hole asymmetry, while\nanother is in near one-to-one correspondence with the Kondo temperature, a\ndynamically generated low-energy scale in the impurity model. With symbolic\nregression, we model this component as a function of bare physical input\nparameters and \"rediscover\" the non-perturbative formula for the Kondo\ntemperature. The machine learning pipeline we develop opens opportunities to\ndiscover new domain knowledge in other physical systems.",
          "link": "http://arxiv.org/abs/2107.08013",
          "publishedOn": "2021-07-19T00:49:07.294Z",
          "wordCount": 623,
          "title": "Machine-learning Kondo physics using variational autoencoders. (arXiv:2107.08013v1 [cond-mat.str-el])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lulan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.",
          "link": "http://arxiv.org/abs/2107.07988",
          "publishedOn": "2021-07-19T00:49:07.288Z",
          "wordCount": 621,
          "title": "Controlled AutoEncoders to Generate Faces from Voices. (arXiv:2107.07988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08001",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gabrie_M/0/1/0/all/0/1\">Marylou Gabri&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rotskoff_G/0/1/0/all/0/1\">Grant M. Rotskoff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1\">Eric Vanden-Eijnden</a>",
          "description": "Normalizing flows can generate complex target distributions and thus show\npromise in many applications in Bayesian statistics as an alternative or\ncomplement to MCMC for sampling posteriors. Since no data set from the target\nposterior distribution is available beforehand, the flow is typically trained\nusing the reverse Kullback-Leibler (KL) divergence that only requires samples\nfrom a base distribution. This strategy may perform poorly when the posterior\nis complicated and hard to sample with an untrained normalizing flow. Here we\nexplore a distinct training strategy, using the direct KL divergence as loss,\nin which samples from the posterior are generated by (i) assisting a local MCMC\nalgorithm on the posterior with a normalizing flow to accelerate its mixing\nrate and (ii) using the data generated this way to train the flow. The method\nonly requires a limited amount of \\textit{a~priori} input about the posterior,\nand can be used to estimate the evidence required for model validation, as we\nillustrate on examples.",
          "link": "http://arxiv.org/abs/2107.08001",
          "publishedOn": "2021-07-19T00:49:07.282Z",
          "wordCount": 613,
          "title": "Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods. (arXiv:2107.08001v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07886",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_H/0/1/0/all/0/1\">Haodi Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jing_J/0/1/0/all/0/1\">Ju Jing</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jiasheng Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_J/0/1/0/all/0/1\">Jason T. L. Wang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Wang_H/0/1/0/all/0/1\">Haimin Wang</a>",
          "description": "We present a new deep learning method, dubbed FibrilNet, for tracing\nchromospheric fibrils in Halpha images of solar observations. Our method\nconsists of a data pre-processing component that prepares training data from a\nthreshold-based tool, a deep learning model implemented as a Bayesian\nconvolutional neural network for probabilistic image segmentation with\nuncertainty quantification to predict fibrils, and a post-processing component\ncontaining a fibril-fitting algorithm to determine fibril orientations. The\nFibrilNet tool is applied to high-resolution Halpha images from an active\nregion (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped\nwith high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We\nquantitatively assess the FibrilNet tool, comparing its image segmentation\nalgorithm and fibril-fitting algorithm with those employed by the\nthreshold-based tool. Our experimental results and major findings are\nsummarized as follows. First, the image segmentation results (i.e., detected\nfibrils) of the two tools are quite similar, demonstrating the good learning\ncapability of FibrilNet. Second, FibrilNet finds more accurate and smoother\nfibril orientation angles than the threshold-based tool. Third, FibrilNet is\nfaster than the threshold-based tool and the uncertainty maps produced by\nFibrilNet not only provide a quantitative way to measure the confidence on each\ndetected fibril, but also help identify fibril structures that are not detected\nby the threshold-based tool but are inferred through machine learning. Finally,\nwe apply FibrilNet to full-disk Halpha images from other solar observatories\nand additional high-resolution Halpha images collected by BBSO/GST,\ndemonstrating the tool's usability in diverse datasets.",
          "link": "http://arxiv.org/abs/2107.07886",
          "publishedOn": "2021-07-19T00:49:07.276Z",
          "wordCount": 700,
          "title": "Tracing Halpha Fibrils through Bayesian Deep Learning. (arXiv:2107.07886v1 [astro-ph.SR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hyung-Kwon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaemin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwook Seo</a>",
          "description": "We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.",
          "link": "http://arxiv.org/abs/2107.07859",
          "publishedOn": "2021-07-19T00:49:07.270Z",
          "wordCount": 686,
          "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections. (arXiv:2107.07859v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Molecular property prediction plays a fundamental role in drug discovery to\ndiscover candidate molecules with target properties. However, molecular\nproperty prediction is essentially a few-shot problem which makes it hard to\nobtain regular models. In this paper, we propose a property-aware adaptive\nrelation networks (PAR) for the few-shot molecular property prediction problem.\nIn comparison to existing works, we leverage the facts that both substructures\nand relationships among molecules are different considering various molecular\nproperties. Our PAR is compatible with existing graph-based molecular encoders,\nand are further equipped with the ability to obtain property-aware molecular\nembedding and model molecular relation graph adaptively. The resultant relation\ngraph also facilitates effective label propagation within each task. Extensive\nexperiments on benchmark molecular property prediction datasets show that our\nmethod consistently outperforms state-of-the-art methods and is able to obtain\nproperty-aware molecular embedding and model molecular relation graph properly.",
          "link": "http://arxiv.org/abs/2107.07994",
          "publishedOn": "2021-07-19T00:49:07.265Z",
          "wordCount": 579,
          "title": "Property-aware Adaptive Relation Networks for Molecular Property Prediction. (arXiv:2107.07994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1\">Tim Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1\">Jan Ernsting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1\">Nils R. Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holstein_V/0/1/0/all/0/1\">Vincent Holstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1\">Ramona Leenings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beisemann_M/0/1/0/all/0/1\">Marie Beisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_L/0/1/0/all/0/1\">Lukas Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1\">Kelvin Sarink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1\">Daniel Emden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opel_N/0/1/0/all/0/1\">Nils Opel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redlich_R/0/1/0/all/0/1\">Ronny Redlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repple_J/0/1/0/all/0/1\">Jonathan Repple</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grotegerd_D/0/1/0/all/0/1\">Dominik Grotegerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinert_S/0/1/0/all/0/1\">Susanne Meinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_J/0/1/0/all/0/1\">Jochen G. Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niendorf_T/0/1/0/all/0/1\">Thoralf Niendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endemann_B/0/1/0/all/0/1\">Beate Endemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1\">Fabian Bamberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroncke_T/0/1/0/all/0/1\">Thomas Kr&#xf6;ncke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulow_R/0/1/0/all/0/1\">Robin B&#xfc;low</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volzke_H/0/1/0/all/0/1\">Henry V&#xf6;lzke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stackelberg_O/0/1/0/all/0/1\">Oyunbileg von Stackelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowade_R/0/1/0/all/0/1\">Ramona Felizitas Sowade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umutlu_L/0/1/0/all/0/1\">Lale Umutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1\">B&#xf6;rge Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caspers_S/0/1/0/all/0/1\">Svenja Caspers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Consortium_G/0/1/0/all/0/1\">German National Cohort Study Center Consortium</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugel_H/0/1/0/all/0/1\">Harald Kugel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircher_T/0/1/0/all/0/1\">Tilo Kircher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1\">Benjamin Risse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaser_C/0/1/0/all/0/1\">Christian Gaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">James H. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1\">Udo Dannlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_K/0/1/0/all/0/1\">Klaus Berger</a>",
          "description": "The deviation between chronological age and age predicted from neuroimaging\ndata has been identified as a sensitive risk-marker of cross-disorder brain\nchanges, growing into a cornerstone of biological age-research. However,\nMachine Learning models underlying the field do not consider uncertainty,\nthereby confounding results with training data density and variability. Also,\nexisting models are commonly based on homogeneous training sets, often not\nindependently validated, and cannot be shared due to data protection issues.\nHere, we introduce an uncertainty-aware, shareable, and transparent Monte-Carlo\nDropout Composite-Quantile-Regression (MCCQR) Neural Network trained on\nN=10,691 datasets from the German National Cohort. The MCCQR model provides\nrobust, distribution-free uncertainty quantification in high-dimensional\nneuroimaging data, achieving lower error rates compared to existing models\nacross ten recruitment centers and in three independent validation samples\n(N=4,004). In two examples, we demonstrate that it prevents spurious\nassociations and increases power to detect accelerated brain-aging. We make the\npre-trained model publicly available.",
          "link": "http://arxiv.org/abs/2107.07977",
          "publishedOn": "2021-07-19T00:49:07.248Z",
          "wordCount": 661,
          "title": "An Uncertainty-Aware, Shareable and Transparent Neural Network Architecture for Brain-Age Modeling. (arXiv:2107.07977v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiye Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bigot_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Bigot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maabout_S/0/1/0/all/0/1\">Sofian Maabout</a>",
          "description": "This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations), recording, over time, observations of multiple\nmeasurements. From such data, we propose to learn, in an online fashion, a\ngraph that captures two aspects of dependency: one describing the sparse\nspatial relationship between sensors, and the other characterizing the\nmeasurement relationship. To this purpose, we introduce a novel multivariate\nautoregressive model to infer the graph topology encoded in the coefficient\nmatrix which captures the sparse Granger causality dependency structure present\nin such matrix-valued time series. We decompose the graph by imposing a\nKronecker sum structure on the coefficient matrix. We develop two online\napproaches to learn the graph in a recursive way. The first one uses Wald test\nfor the projected OLS estimation, where we derive the asymptotic distribution\nfor the estimator. For the second one, we formalize a Lasso-type optimization\nproblem. We rely on homotopy algorithms to derive updating rules for estimating\nthe coefficient matrix. Furthermore, we provide an adaptive tuning procedure\nfor the regularization parameter. Numerical experiments using both synthetic\nand real data, are performed to support the effectiveness of the proposed\nlearning approaches.",
          "link": "http://arxiv.org/abs/2107.08020",
          "publishedOn": "2021-07-19T00:49:07.242Z",
          "wordCount": 635,
          "title": "Online Graph Topology Learning from Matrix-valued Time Series. (arXiv:2107.08020v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavazza_F/0/1/0/all/0/1\">Francesca Tavazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cost_B/0/1/0/all/0/1\">Brian De Cost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_K/0/1/0/all/0/1\">Kamal Choudhary</a>",
          "description": "Uncertainty quantification in Artificial Intelligence (AI)-based predictions\nof material properties is of immense importance for the success and reliability\nof AI applications in material science. While confidence intervals are commonly\nreported for machine learning (ML) models, prediction intervals, i.e., the\nevaluation of the uncertainty on each prediction, are seldomly available. In\nthis work we compare 3 different approaches to obtain such individual\nuncertainty, testing them on 12 ML-physical properties. Specifically, we\ninvestigated using the Quantile loss function, machine learning the prediction\nintervals directly and using Gaussian Processes. We identify each approachs\nadvantages and disadvantages and end up slightly favoring the modeling of the\nindividual uncertainties directly, as it is the easiest to fit and, in most\ncases, minimizes over-and under-estimation of the predicted errors. All data\nfor training and testing were taken from the publicly available JARVIS-DFT\ndatabase, and the codes developed for computing the prediction intervals are\navailable through JARVIS-Tools.",
          "link": "http://arxiv.org/abs/2107.07997",
          "publishedOn": "2021-07-19T00:49:07.236Z",
          "wordCount": 585,
          "title": "Uncertainty Prediction for Machine Learning Models of Material Properties. (arXiv:2107.07997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sondak_D/0/1/0/all/0/1\">David Sondak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Accurately learning the temporal behavior of dynamical systems requires\nmodels with well-chosen learning biases. Recent innovations embed the\nHamiltonian and Lagrangian formalisms into neural networks and demonstrate a\nsignificant improvement over other approaches in predicting trajectories of\nphysical systems. These methods generally tackle autonomous systems that depend\nimplicitly on time or systems for which a control signal is known apriori.\nDespite this success, many real world dynamical systems are non-autonomous,\ndriven by time-dependent forces and experience energy dissipation. In this\nstudy, we address the challenge of learning from such non-autonomous systems by\nembedding the port-Hamiltonian formalism into neural networks, a versatile\nframework that can capture energy dissipation and time-dependent control\nforces. We show that the proposed \\emph{port-Hamiltonian neural network} can\nefficiently learn the dynamics of nonlinear physical systems of practical\ninterest and accurately recover the underlying stationary Hamiltonian,\ntime-dependent force, and dissipative coefficient. A promising outcome of our\nnetwork is its ability to learn and predict chaotic systems such as the Duffing\nequation, for which the trajectories are typically hard to learn.",
          "link": "http://arxiv.org/abs/2107.08024",
          "publishedOn": "2021-07-19T00:49:07.219Z",
          "wordCount": 619,
          "title": "Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems. (arXiv:2107.08024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Ali Raza Zaidi</a>",
          "description": "In this paper, we present an overview of Nearest neighbor (NN) methods, which\nare frequently employed for solving classification problems using supervised\nlearning. The article concisely introduces the theoretical background,\nalgorithmic, and implementation aspects along with the key applications. From\nan application standpoint, this article explores the challenges related to the\n5G and beyond wireless networks which can be solved using NN classification\ntechniques.",
          "link": "http://arxiv.org/abs/2107.07869",
          "publishedOn": "2021-07-19T00:49:07.212Z",
          "wordCount": 516,
          "title": "Nearest neighbor Methods and their Applications in Design of 5G & Beyond Wireless Networks. (arXiv:2107.07869v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07871",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Moseley_B/0/1/0/all/0/1\">Ben Moseley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nissen_Meyer_T/0/1/0/all/0/1\">Tarje Nissen-Meyer</a>",
          "description": "Recently, physics-informed neural networks (PINNs) have offered a powerful\nnew paradigm for solving problems relating to differential equations. Compared\nto classical numerical methods PINNs have several advantages, for example their\nability to provide mesh-free solutions of differential equations and their\nability to carry out forward and inverse modelling within the same optimisation\nproblem. Whilst promising, a key limitation to date is that PINNs have\nstruggled to accurately and efficiently solve problems with large domains\nand/or multi-scale solutions, which is crucial for their real-world\napplication. Multiple significant and related factors contribute to this issue,\nincluding the increasing complexity of the underlying PINN optimisation problem\nas the problem size grows and the spectral bias of neural networks. In this\nwork we propose a new, scalable approach for solving large problems relating to\ndifferential equations called Finite Basis PINNs (FBPINNs). FBPINNs are\ninspired by classical finite element methods, where the solution of the\ndifferential equation is expressed as the sum of a finite set of basis\nfunctions with compact support. In FBPINNs neural networks are used to learn\nthese basis functions, which are defined over small, overlapping subdomains.\nFBINNs are designed to address the spectral bias of neural networks by using\nseparate input normalisation over each subdomain, and reduce the complexity of\nthe underlying optimisation problem by using many smaller neural networks in a\nparallel divide-and-conquer approach. Our numerical experiments show that\nFBPINNs are effective in solving both small and larger, multi-scale problems,\noutperforming standard PINNs in both accuracy and computational resources\nrequired, potentially paving the way to the application of PINNs on large,\nreal-world problems.",
          "link": "http://arxiv.org/abs/2107.07871",
          "publishedOn": "2021-07-19T00:49:07.205Z",
          "wordCount": 721,
          "title": "Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations. (arXiv:2107.07871v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08011",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1\">Kimon Antonakopoulos</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "We propose a new family of adaptive first-order methods for a class of convex\nminimization problems that may fail to be Lipschitz continuous or smooth in the\nstandard sense. Specifically, motivated by a recent flurry of activity on\nnon-Lipschitz (NoLips) optimization, we consider problems that are continuous\nor smooth relative to a reference Bregman function - as opposed to a global,\nambient norm (Euclidean or otherwise). These conditions encompass a wide range\nof problems with singular objectives, such as Fisher markets, Poisson\ntomography, D-design, and the like. In this setting, the application of\nexisting order-optimal adaptive methods - like UnixGrad or AcceleGrad - is not\npossible, especially in the presence of randomness and uncertainty. The\nproposed method - which we call adaptive mirror descent (AdaMir) - aims to\nclose this gap by concurrently achieving min-max optimal rates in problems that\nare relatively continuous or smooth, including stochastic ones.",
          "link": "http://arxiv.org/abs/2107.08011",
          "publishedOn": "2021-07-19T00:49:07.198Z",
          "wordCount": 596,
          "title": "Adaptive first-order methods revisited: Convex optimization without Lipschitz requirements. (arXiv:2107.08011v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "In applications such as natural language processing or computer vision, one\nis given a large $n \\times d$ matrix $A = (a_{i,j})$ and would like to compute\na matrix decomposition, e.g., a low rank approximation, of a function $f(A) =\n(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the\nlikelihood function $f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right|\n+1\\right)}$. A natural way to do this would be to simply apply $f$ to each\nentry of $A$, and then compute the matrix decomposition, but this requires\nstoring all of $A$ as well as multiple passes over its entries. Recent work of\nLiang et al.\\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n\n\\times n$ matrix $A$ using only $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log\nn)$ words of memory, with overall error $10\\|f(A)-[f(A)]_k\\|_F^2 +\n\\operatorname{poly}(\\epsilon/k) \\|f(A)\\|_{1,2}^2$, where $[f(A)]_k$ is the best\nrank-$k$ approximation to $f(A)$ and $\\|f(A)\\|_{1,2}^2$ is the square of the\nsum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes\nover the entries of $A$. The authors pose the open question of obtaining an\nalgorithm with $n \\cdot \\operatorname{poly}(\\epsilon^{-1}k\\log n)$ words of\nmemory using only a single pass over the entries of $A$. In this paper we\nresolve this open question, obtaining the first single-pass algorithm for this\nproblem and for the same class of functions $f$ studied by Liang et al.\nMoreover, our error is $\\|f(A)-[f(A)]_k\\|_F^2 + \\operatorname{poly}(\\epsilon/k)\n\\|f(A)\\|_F^2$, where $\\|f(A)\\|_F^2$ is the sum of squares of Euclidean lengths\nof rows of $f(A)$. Thus our error is significantly smaller, as it removes the\nfactor of $10$ and also $\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$. We also give an\nalgorithm for regression, pointing out an error in previous work, and\nempirically validate our results.",
          "link": "http://arxiv.org/abs/2107.07889",
          "publishedOn": "2021-07-19T00:49:07.186Z",
          "wordCount": 731,
          "title": "Single Pass Entrywise-Transformed Low Rank Approximation. (arXiv:2107.07889v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kolcun_R/0/1/0/all/0/1\">Roman Kolcun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_D/0/1/0/all/0/1\">Diana Andreea Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safronov_V/0/1/0/all/0/1\">Vadim Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Poonam Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandalari_A/0/1/0/all/0/1\">Anna Maria Mandalari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortier_R/0/1/0/all/0/1\">Richard Mortier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>",
          "description": "Internet-of-Things (IoT) devices are known to be the source of many security\nproblems, and as such, they would greatly benefit from automated management.\nThis requires robustly identifying devices so that appropriate network security\npolicies can be applied. We address this challenge by exploring how to\naccurately identify IoT devices based on their network behavior, while\nleveraging approaches previously proposed by other researchers.\n\nWe compare the accuracy of four different previously proposed machine\nlearning models (tree-based and neural network-based) for identifying IoT\ndevices. We use packet trace data collected over a period of six months from a\nlarge IoT test-bed. We show that, while all models achieve high accuracy when\nevaluated on the same dataset as they were trained on, their accuracy degrades\nover time, when evaluated on data collected outside the training set. We show\nthat on average the models' accuracy degrades after a couple of weeks by up to\n40 percentage points (on average between 12 and 21 percentage points). We argue\nthat, in order to keep the models' accuracy at a high level, these need to be\ncontinuously updated.",
          "link": "http://arxiv.org/abs/2107.07818",
          "publishedOn": "2021-07-19T00:49:07.140Z",
          "wordCount": 634,
          "title": "Revisiting IoT Device Identification. (arXiv:2107.07818v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thor_M/0/1/0/all/0/1\">Mathias Thor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoonpong_P/0/1/0/all/0/1\">Poramate Manoonpong</a>",
          "description": "Legged robots have significant potential to operate in highly unstructured\nenvironments. The design of locomotion control is, however, still challenging.\nCurrently, controllers must be either manually designed for specific robots and\ntasks, or automatically designed via machine learning methods that require long\ntraining times and yield large opaque controllers. Drawing inspiration from\nanimal locomotion, we propose a simple yet versatile modular neural control\nstructure with fast learning. The key advantages of our approach are that\nbehavior-specific control modules can be added incrementally to obtain\nincreasingly complex emergent locomotion behaviors, and that neural connections\ninterfacing with existing modules can be quickly and automatically learned. In\na series of experiments, we show how eight modules can be quickly learned and\nadded to a base control module to obtain emergent adaptive behaviors allowing a\nhexapod robot to navigate in complex environments. We also show that modules\ncan be added and removed during operation without affecting the functionality\nof the remaining controller. Finally, the control approach was successfully\ndemonstrated on a physical hexapod robot. Taken together, our study reveals a\nsignificant step towards fast automatic design of versatile neural locomotion\ncontrol for complex robotic systems.",
          "link": "http://arxiv.org/abs/2107.07844",
          "publishedOn": "2021-07-19T00:49:07.134Z",
          "wordCount": 641,
          "title": "Versatile modular neural locomotion control with fast learning. (arXiv:2107.07844v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07875",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nalamada_T/0/1/0/all/0/1\">Trikay Nalamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jahja_M/0/1/0/all/0/1\">Maria Jahja</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakraborty_B/0/1/0/all/0/1\">Bibhas Chakraborty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "A dynamic treatment regimen (DTR) is a set of decision rules to personalize\ntreatments for an individual using their medical history. The Q-learning based\nQ-shared algorithm has been used to develop DTRs that involve decision rules\nshared across multiple stages of intervention. We show that the existing\nQ-shared algorithm can suffer from non-convergence due to the use of linear\nmodels in the Q-learning setup, and identify the condition in which Q-shared\nfails. Leveraging properties from expansion-constrained ordinary least-squares,\nwe give a penalized Q-shared algorithm that not only converges in settings that\nviolate the condition, but can outperform the original Q-shared algorithm even\nwhen the condition is satisfied. We give evidence for the proposed method in a\nreal-world application and several synthetic simulations.",
          "link": "http://arxiv.org/abs/2107.07875",
          "publishedOn": "2021-07-19T00:49:07.126Z",
          "wordCount": 561,
          "title": "A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens. (arXiv:2107.07875v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07853",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>",
          "description": "Algorithmic recourse explanations inform stakeholders on how to act to revert\nunfavorable predictions. However, in general ML models do not predict well in\ninterventional distributions. Thus, an action that changes the prediction in\nthe desired way may not lead to an improvement of the underlying target. Such\nrecourse is neither meaningful nor robust to model refits. Extending the work\nof Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that\nonly recommends actions that improve both prediction and target. We justify\nthis selection constraint by highlighting the differences between model audit\nand meaningful, actionable recourse explanations. Additionally, we introduce a\nrelaxation of MAR called effective algorithmic recourse (EAR), which, under\ncertain assumptions, yields meaningful recourse by only allowing interventions\non causes of the target.",
          "link": "http://arxiv.org/abs/2107.07853",
          "publishedOn": "2021-07-19T00:49:07.073Z",
          "wordCount": 570,
          "title": "A Causal Perspective on Meaningful and Robust Algorithmic Recourse. (arXiv:2107.07853v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Puck de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>",
          "description": "Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.",
          "link": "http://arxiv.org/abs/2107.07820",
          "publishedOn": "2021-07-19T00:49:07.065Z",
          "wordCount": 583,
          "title": "Contrastive Predictive Coding for Anomaly Detection. (arXiv:2107.07820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-07-19T00:49:07.046Z",
          "wordCount": 587,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07737",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dunjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhaoyan Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Mingwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "Graph classification plays a significant role in network analysis. It also\nfaces potential security threat like adversarial attacks. Some defense methods\nmay sacrifice algorithm complexity for robustness like adversarial training,\nwhile others may sacrifice the clean example performance such as\nsmoothing-based defense. Most of them are suffered from high-complexity or less\ntransferability. To address this problem, we proposed EGC$^2$, an enhanced\ngraph classification model with easy graph compression. EGC$^2$ captures the\nrelationship between features of different nodes by constructing feature graphs\nand improving aggregate node-level representation. To achieve lower complexity\ndefense applied to various graph classification models, EGC$^2$ utilizes a\ncentrality-based edge importance index to compress graphs, filtering out\ntrivial structures and even adversarial perturbations of the input graphs, thus\nimproves its robustness. Experiments on seven benchmark datasets demonstrate\nthat the proposed feature read-out and graph compression mechanisms enhance the\nrobustness of various basic models, thus achieving the state-of-the-art\nperformance of accuracy and robustness in the threat of different adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2107.07737",
          "publishedOn": "2021-07-19T00:49:07.040Z",
          "wordCount": 600,
          "title": "EGC2: Enhanced Graph Classification with Easy Graph Compression. (arXiv:2107.07737v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Shivshankar Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anand Vir Singh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Karamjit Singh</a>",
          "description": "Temporal Point Processes (TPPs) are often used to represent the sequence of\nevents ordered as per the time of occurrence. Owing to their flexible nature,\nTPPs have been used to model different scenarios and have shown applicability\nin various real-world applications. While TPPs focus on modeling the event\noccurrence, Marked Temporal Point Process (MTPP) focuses on modeling the\ncategory/class of the event as well (termed as the marker). Research in MTPP\nhas garnered substantial attention over the past few years, with an extensive\nfocus on supervised algorithms. Despite the research focus, limited attention\nhas been given to the challenging problem of developing solutions in\nsemi-supervised settings, where algorithms have access to a mix of labeled and\nunlabeled data. This research proposes a novel algorithm for Semi-supervised\nLearning for Marked Temporal Point Processes (SSL-MTPP) applicable in such\nscenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled\nand unlabeled data for learning a robust marker prediction model. The proposed\nalgorithm utilizes an RNN-based Encoder-Decoder module for learning effective\nrepresentations of the time sequence. The efficacy of the proposed algorithm\nhas been demonstrated via multiple protocols on the Retweet dataset, where the\nproposed SSL-MTPP demonstrates improved performance in comparison to the\ntraditional supervised learning approach.",
          "link": "http://arxiv.org/abs/2107.07729",
          "publishedOn": "2021-07-19T00:49:07.033Z",
          "wordCount": 637,
          "title": "Semi-supervised Learning for Marked Temporal Point Processes. (arXiv:2107.07729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_C/0/1/0/all/0/1\">Christof Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1\">Philipp Singer</a>",
          "description": "We present a robust classification approach for avian vocalization in complex\nand diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.\nWe illustrate how to make full use of pre-trained convolutional neural\nnetworks, by using an efficient modeling and training routine supplemented by\nnovel augmentation methods. Thereby, we improve the generalization of weakly\nlabeled crowd-sourced data to productive data collected by autonomous recording\nunits. As such, we illustrate how to progress towards an accurate automated\nassessment of avian population which would enable global biodiversity\nmonitoring at scale, impossible by manual annotation.",
          "link": "http://arxiv.org/abs/2107.07728",
          "publishedOn": "2021-07-19T00:49:07.026Z",
          "wordCount": 541,
          "title": "Recognizing bird species in diverse soundscapes under weak supervision. (arXiv:2107.07728v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qin Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>",
          "description": "Leveraging unlabelled data through weak or distant supervision is a\ncompelling approach to developing more effective text classification models.\nThis paper proposes a simple but effective data augmentation method, which\nleverages the idea of pseudo-labelling to select samples from noisy distant\nsupervision annotation datasets. The result shows that the proposed method\nimproves the accuracy of biased news detection models.",
          "link": "http://arxiv.org/abs/2107.07705",
          "publishedOn": "2021-07-19T00:49:07.019Z",
          "wordCount": 491,
          "title": "Pseudo-labelling Enhanced Media Bias Detection. (arXiv:2107.07705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Arnab Kumar Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asnani_H/0/1/0/all/0/1\">Himanshu Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>",
          "description": "Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and\ncomputational challenges due to their high-dimensionality and data-sparsity,\nalso known as `dropout' events. Recently, Regularized Auto-Encoder (RAE) based\ndeep neural network models have achieved remarkable success in learning robust\nlow-dimensional representations. The basic idea in RAEs is to learn a\nnon-linear mapping from the high-dimensional data space to a low-dimensional\nlatent space and vice-versa, simultaneously imposing a distributional prior on\nthe latent space, which brings in a regularization effect. This paper argues\nthat RAEs suffer from the infamous problem of bias-variance trade-off in their\nnaive formulation. While a simple AE without a latent regularization results in\ndata over-fitting, a very strong prior leads to under-representation and thus\nbad clustering. To address the above issues, we propose a modified RAE\nframework (called the scRAE) for effective clustering of the single-cell RNA\nsequencing data. scRAE consists of deterministic AE with a flexibly learnable\nprior generator network, which is jointly trained with the AE. This facilitates\nscRAE to trade-off better between the bias and variance in the latent space. We\ndemonstrate the efficacy of the proposed method through extensive\nexperimentation on several real-world single-cell Gene expression datasets.",
          "link": "http://arxiv.org/abs/2107.07709",
          "publishedOn": "2021-07-19T00:49:07.002Z",
          "wordCount": 642,
          "title": "ScRAE: Deterministic Regularized Autoencoders with Flexible Priors for Clustering Single-cell Gene Expression Data. (arXiv:2107.07709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rushil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vishal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1\">Yash Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1\">Guy Van den Broeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>",
          "description": "We focus on the task of future frame prediction in video governed by\nunderlying physical dynamics. We work with models which are object-centric,\ni.e., explicitly work with object representations, and propagate a loss in the\nlatent space. Specifically, our research builds on recent work by Kipf et al.\n\\cite{kipf&al20}, which predicts the next state via contrastive learning of\nobject interactions in a latent space using a Graph Neural Network. We argue\nthat injecting explicit inductive bias in the model, in form of general\nphysical laws, can help not only make the model more interpretable, but also\nimprove the overall prediction of model. As a natural by-product, our model can\nlearn feature maps which closely resemble actual object positions in the image,\nwithout having any explicit supervision about the object positions at the\ntraining time. In comparison with earlier works \\cite{jaques&al20}, which\nassume a complete knowledge of the dynamics governing the motion in the form of\na physics engine, we rely only on the knowledge of general physical laws, such\nas, world consists of objects, which have position and velocity. We propose an\nadditional decoder based loss in the pixel space, imposed in a curriculum\nmanner, to further refine the latent space predictions. Experiments in multiple\ndifferent settings demonstrate that while Kipf et al. model is effective at\ncapturing object interactions, our model can be significantly more effective at\nlocalising objects, resulting in improved performance in 3 out of 4 domains\nthat we experiment with. Additionally, our model can learn highly intrepretable\nfeature maps, resembling actual object positions.",
          "link": "http://arxiv.org/abs/2107.07713",
          "publishedOn": "2021-07-19T00:49:06.996Z",
          "wordCount": 709,
          "title": "Towards an Interpretable Latent Space in Structured Models for Video Prediction. (arXiv:2107.07713v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07732",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xinyi Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghai_U/0/1/0/all/0/1\">Udaya Ghai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hazan_E/0/1/0/all/0/1\">Elad Hazan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Megretski_A/0/1/0/all/0/1\">Alexandre Megretski</a>",
          "description": "We study online control of an unknown nonlinear dynamical system that is\napproximated by a time-invariant linear system with model misspecification. Our\nstudy focuses on robustness, which measures how much deviation from the assumed\nlinear approximation can be tolerated while maintaining a bounded $\\ell_2$-gain\ncompared to the optimal control in hindsight. Some models cannot be stabilized\neven with perfect knowledge of their coefficients: the robustness is limited by\nthe minimal distance between the assumed dynamics and the set of unstabilizable\ndynamics. Therefore it is necessary to assume a lower bound on this distance.\nUnder this assumption, and with full observation of the $d$ dimensional state,\nwe describe an efficient controller that attains $\\Omega(\\frac{1}{\\sqrt{d}})$\nrobustness together with an $\\ell_2$-gain whose dimension dependence is near\noptimal. We also give an inefficient algorithm that attains constant robustness\nindependent of the dimension, with a finite but sub-optimal $\\ell_2$-gain.",
          "link": "http://arxiv.org/abs/2107.07732",
          "publishedOn": "2021-07-19T00:49:06.989Z",
          "wordCount": 576,
          "title": "Robust Online Control with Model Misspecification. (arXiv:2107.07732v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07752",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cognolato_F/0/1/0/all/0/1\">Francesco Cognolato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OBrien_K/0/1/0/all/0/1\">Kieran O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_S/0/1/0/all/0/1\">Simon Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laun_F/0/1/0/all/0/1\">Frederik B. Laun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barth_M/0/1/0/all/0/1\">Markus Barth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollmann_S/0/1/0/all/0/1\">Steffen Bollmann</a>",
          "description": "Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.",
          "link": "http://arxiv.org/abs/2107.07752",
          "publishedOn": "2021-07-19T00:49:06.983Z",
          "wordCount": 613,
          "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07788",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhong-Ping Jiang</a>",
          "description": "This paper studies the optimal stationary control of continuous-time linear\nstochastic systems with both additive and multiplicative noises, using\nreinforcement learning techniques. Based on policy iteration, a novel\noff-policy reinforcement learning algorithm, named optimistic\nleast-squares-based policy iteration, is proposed which is able to iteratively\nfind near-optimal policies of the optimal stationary control problem directly\nfrom input/state data without explicitly identifying any system matrices,\nstarting from an initial admissible control policy. The solutions given by the\nproposed optimistic least-squares-based policy iteration are proved to converge\nto a small neighborhood of the optimal solution with probability one, under\nmild conditions. The application of the proposed algorithm to a triple inverted\npendulum example validates its feasibility and effectiveness.",
          "link": "http://arxiv.org/abs/2107.07788",
          "publishedOn": "2021-07-19T00:49:06.976Z",
          "wordCount": 567,
          "title": "Reinforcement Learning for Optimal Stationary Control of Linear Stochastic Systems. (arXiv:2107.07788v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07582",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Mamandipoor_B/0/1/0/all/0/1\">Behrooz Mamandipoor</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yeung_W/0/1/0/all/0/1\">Wesley Yeung</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Agha_Mir_Salim_L/0/1/0/all/0/1\">Louis Agha-Mir-Salim</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stone_D/0/1/0/all/0/1\">David J. Stone</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Osmani_V/0/1/0/all/0/1\">Venet Osmani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Celi_L/0/1/0/all/0/1\">Leo Anthony Celi</a>",
          "description": "Purpose. Elevations in initially obtained serum lactate levels are strong\npredictors of mortality in critically ill patients. Identifying patients whose\nserum lactate levels are more likely to increase can alert physicians to\nintensify care and guide them in the frequency of tending the blood test. We\ninvestigate whether machine learning models can predict subsequent serum\nlactate changes.\n\nMethods. We investigated serum lactate change prediction using the MIMIC-III\nand eICU-CRD datasets in internal as well as external validation of the eICU\ncohort on the MIMIC-III cohort. Three subgroups were defined based on the\ninitial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4\nmmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on\nincrease or decrease of serum lactate levels between the groups. We also\nperformed sensitivity analysis by defining the outcome as lactate change of\n>10% and furthermore investigated the influence of the time interval between\nsubsequent lactate measurements on predictive performance.\n\nResults. The LSTM models were able to predict deterioration of serum lactate\nvalues of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the\nnormal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI\n0.840-0.851) for the severe group, with a slightly lower performance in the\nexternal validation.\n\nConclusion. The LSTM demonstrated good discrimination of patients who had\ndeterioration in serum lactate levels. Clinical studies are needed to evaluate\nwhether utilization of a clinical decision support tool based on these results\ncould positively impact decision-making and patient outcomes.",
          "link": "http://arxiv.org/abs/2107.07582",
          "publishedOn": "2021-07-19T00:49:06.959Z",
          "wordCount": 727,
          "title": "Prediction of Blood Lactate Values in Critically Ill Patients: A Retrospective Multi-center Cohort Study. (arXiv:2107.07582v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Christopher T.H Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>",
          "description": "Deep generative models have made much progress in improving training\nstability and quality of generated data. Recently there has been increased\ninterest in the fairness of deep-generated data. Fairness is important in many\napplications, e.g. law enforcement, as biases will affect efficacy. Central to\nfair data generation are the fairness metrics for the assessment and evaluation\nof different generative models. In this paper, we first review fairness metrics\nproposed in previous works and highlight potential weaknesses. We then discuss\na performance benchmark framework along with the assessment of alternative\nmetrics.",
          "link": "http://arxiv.org/abs/2107.07754",
          "publishedOn": "2021-07-19T00:49:06.952Z",
          "wordCount": 525,
          "title": "Measuring Fairness in Generative Models. (arXiv:2107.07754v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1\">Toshinori Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takamitsu Matsubara</a>",
          "description": "The recent booming of entropy-regularized literature reveals that\nKullback-Leibler (KL) regularization brings advantages to Reinforcement\nLearning (RL) algorithms by canceling out errors under mild assumptions.\nHowever, existing analyses focus on fixed regularization with a constant\nweighting coefficient and have not considered the case where the coefficient is\nallowed to change dynamically. In this paper, we study the dynamic coefficient\nscheme and present the first asymptotic error bound. Based on the dynamic\ncoefficient error bound, we propose an effective scheme to tune the coefficient\naccording to the magnitude of error in favor of more robust learning. On top of\nthis development, we propose a novel algorithm: Geometric Value Iteration (GVI)\nthat features a dynamic error-aware KL coefficient design aiming to mitigate\nthe impact of errors on the performance. Our experiments demonstrate that GVI\ncan effectively exploit the trade-off between learning speed and robustness\nover uniform averaging of constant KL coefficient. The combination of GVI and\ndeep networks shows stable learning behavior even in the absence of a target\nnetwork where algorithms with a constant KL coefficient would greatly oscillate\nor even fail to converge.",
          "link": "http://arxiv.org/abs/2107.07659",
          "publishedOn": "2021-07-19T00:49:06.946Z",
          "wordCount": 616,
          "title": "Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning. (arXiv:2107.07659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barata_R/0/1/0/all/0/1\">Ricardo Barata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_M/0/1/0/all/0/1\">Miguel Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1\">Ricardo Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1\">Marco O. P. Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1\">Jo&#xe3;o Tiago Ascens&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Labeled data is essential in modern systems that rely on Machine Learning\n(ML) for predictive modelling. Such systems may suffer from the cold-start\nproblem: supervised models work well but, initially, there are no labels, which\nare costly or slow to obtain. This problem is even worse in imbalanced data\nscenarios. Online financial fraud detection is an example where labeling is: i)\nexpensive, or ii) it suffers from long delays, if relying on victims filing\ncomplaints. The latter may not be viable if a model has to be in place\nimmediately, so an option is to ask analysts to label events while minimizing\nthe number of annotations to control costs. We propose an Active Learning (AL)\nannotation system for datasets with orders of magnitude of class imbalance, in\na cold start streaming scenario. We present a computationally efficient\nOutlier-based Discriminative AL approach (ODAL) and design a novel 3-stage\nsequence of AL labeling policies where it is used as warm-up. Then, we perform\nempirical studies in four real world datasets, with various magnitudes of class\nimbalance. The results show that our method can more quickly reach a high\nperformance model than standard AL policies. Its observed gains over random\nsampling can reach 80% and be competitive with policies with an unlimited\nannotation budget or additional historical data (with 1/10 to 1/50 of the\nlabels).",
          "link": "http://arxiv.org/abs/2107.07724",
          "publishedOn": "2021-07-19T00:49:06.939Z",
          "wordCount": 678,
          "title": "Active learning for online training in imbalanced data streams under cold start. (arXiv:2107.07724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhunan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Cunhang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiguang He</a>",
          "description": "As an essential element for the diagnosis and rehabilitation of psychiatric\ndisorders, the electroencephalogram (EEG) based emotion recognition has\nachieved significant progress due to its high precision and reliability.\nHowever, one obstacle to practicality lies in the variability between subjects\nand sessions. Although several studies have adopted domain adaptation (DA)\napproaches to tackle this problem, most of them treat multiple EEG data from\ndifferent subjects and sessions together as a single source domain for\ntransfer, which either fails to satisfy the assumption of domain adaptation\nthat the source has a certain marginal distribution, or increases the\ndifficulty of adaptation. We therefore propose the multi-source marginal\ndistribution adaptation (MS-MDA) for EEG emotion recognition, which takes both\ndomain-invariant and domain-specific features into consideration. First, we\nassume that different EEG data share the same low-level features, then we\nconstruct independent branches for multiple EEG data source domains to adopt\none-to-one domain adaptation and extract domain-specific features. Finally, the\ninference is made by multiple branches. We evaluate our method on SEED and\nSEED-IV for recognizing three and four emotions, respectively. Experimental\nresults show that the MS-MDA outperforms the comparison methods and\nstate-of-the-art models in cross-session and cross-subject transfer scenarios\nin our settings. Codes at https://github.com/VoiceBeer/MS-MDA.",
          "link": "http://arxiv.org/abs/2107.07740",
          "publishedOn": "2021-07-19T00:49:06.933Z",
          "wordCount": 657,
          "title": "MS-MDA: Multisource Marginal Distribution Adaptation for Cross-subject and Cross-session EEG Emotion Recognition. (arXiv:2107.07740v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Niel Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yosinski_J/0/1/0/all/0/1\">Jason Yosinski</a>",
          "description": "Not all examples are created equal, but standard deep neural network training\nprotocols treat each training point uniformly. Each example is propagated\nforward and backward through the network the same amount of times, independent\nof how much the example contributes to the learning protocol. Recent work has\nproposed ways to accelerate training by deviating from this uniform treatment.\nPopular methods entail up-weighting examples that contribute more to the loss\nwith the intuition that examples with low loss have already been learned by the\nmodel, so their marginal value to the training procedure should be lower. This\nview assumes that updating the model with high loss examples will be beneficial\nto the model. However, this may not hold for noisy, real world data. In this\npaper, we theorize and then empirically demonstrate that loss-based\nacceleration methods degrade in scenarios with noisy and corrupted data. Our\nwork suggests measures of example difficulty need to correctly separate out\nnoise from other types of challenging examples.",
          "link": "http://arxiv.org/abs/2107.07741",
          "publishedOn": "2021-07-19T00:49:06.895Z",
          "wordCount": 589,
          "title": "When does loss-based prioritization fail?. (arXiv:2107.07741v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>",
          "description": "Scenario generation is a fundamental and crucial tool for decision-making in\npower systems with high-penetration renewables. Based on big historical data, a\nnovel federated deep generative learning framework, called Fed-LSGAN, is\nproposed by integrating federated learning and least square generative\nadversarial networks (LSGANs) for renewable scenario generation. Specifically,\nfederated learning learns a shared global model in a central server from\nrenewable sites at network edges, which enables the Fed-LSGAN to generate\nscenarios in a privacy-preserving manner without sacrificing the generation\nquality by transferring model parameters, rather than all data. Meanwhile, the\nLSGANs-based deep generative model generates scenarios that conform to the\ndistribution of historical data through fully capturing the spatial-temporal\ncharacteristics of renewable powers, which leverages the least squares loss\nfunction to improve the training stability and generation quality. The\nsimulation results demonstrate that the proposal manages to generate\nhigh-quality renewable scenarios and outperforms the state-of-the-art\ncentralized methods. Besides, an experiment with different federated learning\nsettings is designed and conducted to verify the robustness of our method.",
          "link": "http://arxiv.org/abs/2107.07738",
          "publishedOn": "2021-07-19T00:49:06.886Z",
          "wordCount": 626,
          "title": "Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach. (arXiv:2107.07738v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07687",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yuming Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sanz_Alonso_D/0/1/0/all/0/1\">Daniel Sanz-Alonso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>",
          "description": "Data assimilation is concerned with sequentially estimating a\ntemporally-evolving state. This task, which arises in a wide range of\nscientific and engineering applications, is particularly challenging when the\nstate is high-dimensional and the state-space dynamics are unknown. This paper\nintroduces a machine learning framework for learning dynamical systems in data\nassimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend\nensemble Kalman filters for state recovery with machine learning tools for\nlearning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble\nKalman filters to scale to high-dimensional states and the power of automatic\ndifferentiation to train high-dimensional surrogate models for the dynamics.\nNumerical results using the Lorenz-96 model show that AD-EnKFs outperform\nexisting methods that use expectation-maximization or particle filters to merge\ndata assimilation and machine learning. In addition, AD-EnKFs are easy to\nimplement and require minimal tuning.",
          "link": "http://arxiv.org/abs/2107.07687",
          "publishedOn": "2021-07-19T00:49:06.870Z",
          "wordCount": 564,
          "title": "Auto-differentiable Ensemble Kalman Filters. (arXiv:2107.07687v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Sanjoy Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navlakha_S/0/1/0/all/0/1\">Saket Navlakha</a>",
          "description": "Continual learning in computational systems is challenging due to\ncatastrophic forgetting. We discovered a two layer neural circuit in the fruit\nfly olfactory system that addresses this challenge by uniquely combining sparse\ncoding and associative learning. In the first layer, odors are encoded using\nsparse, high dimensional representations, which reduces memory interference by\nactivating non overlapping populations of neurons for different odors. In the\nsecond layer, only the synapses between odor activated neurons and the output\nneuron associated with the odor are modified during learning; the rest of the\nweights are frozen to prevent unrelated memories from being overwritten. We\nshow empirically and analytically that this simple and lightweight algorithm\nsignificantly boosts continual learning performance. The fly associative\nlearning algorithm is strikingly similar to the classic perceptron learning\nalgorithm, albeit two modifications, which we show are critical for reducing\ncatastrophic forgetting. Overall, fruit flies evolved an efficient lifelong\nlearning algorithm, and circuit mechanisms from neuroscience can be translated\nto improve machine computation.",
          "link": "http://arxiv.org/abs/2107.07617",
          "publishedOn": "2021-07-19T00:49:06.824Z",
          "wordCount": 595,
          "title": "Algorithmic insights on continual learning from fruit flies. (arXiv:2107.07617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Khondker Fariha Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sharif Amit Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakkoli_A/0/1/0/all/0/1\">Alireza Tavakkoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Daniel Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1\">Sutharshan Rajasegarar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmaker_C/0/1/0/all/0/1\">Chandan Karmaker</a>",
          "description": "Electrocardiogram (ECG) acquisition requires an automated system and analysis\npipeline for understanding specific rhythm irregularities. Deep neural networks\nhave become a popular technique for tracing ECG signals, outperforming human\nexperts. Despite this, convolutional neural networks are susceptible to\nadversarial examples that can misclassify ECG signals and decrease the model's\nprecision. Moreover, they do not generalize well on the out-of-distribution\ndataset. The GAN architecture has been employed in recent works to synthesize\nadversarial ECG signals to increase existing training data. However, they use a\ndisjointed CNN-based classification architecture to detect arrhythmia. Till\nnow, no versatile architecture has been proposed that can detect adversarial\nexamples and classify arrhythmia simultaneously. To alleviate this, we propose\na novel Conditional Generative Adversarial Network to simultaneously generate\nECG signals for different categories and detect cardiac abnormalities.\nMoreover, the model is conditioned on class-specific ECG signals to synthesize\nrealistic adversarial examples. Consequently, we compare our architecture and\nshow how it outperforms other classification models in normal/abnormal ECG\nsignal detection by benchmarking real world and adversarial signals.",
          "link": "http://arxiv.org/abs/2107.07677",
          "publishedOn": "2021-07-19T00:49:06.794Z",
          "wordCount": 618,
          "title": "ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks. (arXiv:2107.07677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferdous_R/0/1/0/all/0/1\">Refat E Ferdous</a>",
          "description": "During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.",
          "link": "http://arxiv.org/abs/2107.07576",
          "publishedOn": "2021-07-19T00:49:06.772Z",
          "wordCount": 666,
          "title": "Real-Time Face Recognition System for Remote Employee Tracking. (arXiv:2107.07576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carmona_C/0/1/0/all/0/1\">Chris U. Carmona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubet_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Aubet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flunkert_V/0/1/0/all/0/1\">Valentin Flunkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1\">Jan Gasthaus</a>",
          "description": "We introduce Neural Contextual Anomaly Detection (NCAD), a framework for\nanomaly detection on time series that scales seamlessly from the unsupervised\nto supervised setting, and is applicable to both univariate and multivariate\ntime series. This is achieved by effectively combining recent developments in\nrepresentation learning for multivariate time series, with techniques for deep\nanomaly detection originally developed for computer vision that we tailor to\nthe time series setting. Our window-based approach facilitates learning the\nboundary between normal and anomalous classes by injecting generic synthetic\nanomalies into the available data. Moreover, our method can effectively take\nadvantage of all the available information, be it as domain knowledge, or as\ntraining labels in the semi-supervised setting. We demonstrate empirically on\nstandard benchmark datasets that our approach obtains a state-of-the-art\nperformance in these settings.",
          "link": "http://arxiv.org/abs/2107.07702",
          "publishedOn": "2021-07-19T00:49:06.765Z",
          "wordCount": 570,
          "title": "Neural Contextual Anomaly Detection for Time Series. (arXiv:2107.07702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07642",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1\">Sanjaya Lohani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lukens_J/0/1/0/all/0/1\">Joseph M. Lukens</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jones_D/0/1/0/all/0/1\">Daniel E. Jones</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1\">Thomas A. Searles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1\">Ryan T. Glasser</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1\">Brian T. Kirby</a>",
          "description": "We consider the properties of a specific distribution of mixed quantum states\nof arbitrary dimension that can be biased towards a specific mean purity. In\nparticular, we analyze mixtures of Haar-random pure states with\nDirichlet-distributed coefficients. We analytically derive the concentration\nparameters required to match the mean purity of the Bures and Hilbert--Schmidt\ndistributions in any dimension. Numerical simulations suggest that this value\nrecovers the Hilbert--Schmidt distribution exactly, offering an alternative and\nintuitive physical interpretation for ensembles of Hilbert--Schmidt-distributed\nrandom quantum states. We then demonstrate how substituting these\nDirichlet-weighted Haar mixtures in place of the Bures and Hilbert--Schmidt\ndistributions results in measurable performance advantages in\nmachine-learning-based quantum state tomography systems and Bayesian quantum\nstate reconstruction. Finally, we experimentally characterize the distribution\nof quantum states generated by both a cloud-accessed IBM quantum computer and\nan in-house source of polarization-entangled photons. In each case, our method\ncan more closely match the underlying distribution than either Bures or\nHilbert--Schmidt distributed states for various experimental conditions.",
          "link": "http://arxiv.org/abs/2107.07642",
          "publishedOn": "2021-07-19T00:49:06.759Z",
          "wordCount": 611,
          "title": "Improving application performance with biased distributions of quantum states. (arXiv:2107.07642v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahremanlou_L/0/1/0/all/0/1\">Lida Ghahremanlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_S/0/1/0/all/0/1\">Shanthi Robertson</a>",
          "description": "To examine whether intersectional bias can be observed in language\ngeneration, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size\nfrom 124 million to ~2.7 billion parameters. We conduct an experiment combining\nup to three social categories - gender, religion and disability - into\nunconditional or zero-shot prompts used to generate sentences that are then\nanalysed for sentiment. Our results confirm earlier tests conducted with\nauto-regressive causal models, including the \\emph{GPT} family of models. We\nalso illustrate why bias may be resistant to techniques that target single\ncategories (e.g. gender, religion and race), as it can also manifest, in often\nsubtle ways, in texts prompted by concatenated social categories. To address\nthese difficulties, we suggest technical and community-based approaches need to\ncombine to acknowledge and address complex and intersectional language model\nbias.",
          "link": "http://arxiv.org/abs/2107.07691",
          "publishedOn": "2021-07-19T00:49:06.746Z",
          "wordCount": 568,
          "title": "Intersectional Bias in Causal Language Models. (arXiv:2107.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07603",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Scurll_J/0/1/0/all/0/1\">Joshua M. Scurll</a>",
          "description": "Clustering and visualizing high-dimensional (HD) data are important tasks in\na variety of fields. For example, in bioinformatics, they are crucial for\nanalyses of single-cell data such as mass cytometry (CyTOF) data. Some of the\nmost effective algorithms for clustering HD data are based on representing the\ndata by nodes in a graph, with edges connecting neighbouring nodes according to\nsome measure of similarity or distance. However, users of graph-based\nalgorithms are typically faced with the critical but challenging task of\nchoosing the value of an input parameter that sets the size of neighbourhoods\nin the graph, e.g. the number of nearest neighbours to which to connect each\nnode or a threshold distance for connecting nodes. The burden on the user could\nbe alleviated by a measure of inter-node similarity that can have value 0 for\ndissimilar nodes without requiring any user-defined parameters or thresholds.\nThis would determine the neighbourhoods automatically while still yielding a\nsparse graph. To this end, I propose a new method called ASTRICS to measure\nsimilarity between clusters of HD data points based on local dimensionality\nreduction and triangulation of critical alpha shapes. I show that my ASTRICS\nsimilarity measure can facilitate both clustering and visualization of HD data\nby using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial\nclustering of the data by any method; Stage 2 = let graph nodes represent\ninitial clusters instead of individual data points and use ASTRICS to\nautomatically define edges between nodes; Stage 3 = use the graph for further\nclustering and visualization. This trades the critical task of choosing a graph\nneighbourhood size for the easier task of essentially choosing a resolution at\nwhich to view the data. The graph and consequently downstream clustering and\nvisualization are then automatically adapted to the chosen resolution.",
          "link": "http://arxiv.org/abs/2107.07603",
          "publishedOn": "2021-07-19T00:49:06.719Z",
          "wordCount": 764,
          "title": "Measuring inter-cluster similarities with Alpha Shape TRIangulation in loCal Subspaces (ASTRICS) facilitates visualization and clustering of high-dimensional data. (arXiv:2107.07603v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel D. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Daniel Tarlow</a>",
          "description": "Denoising diffusion probabilistic models (DDPMs) have shown impressive\nresults on sequence generation by iteratively corrupting each example and then\nlearning to map corrupted versions back to the original. However, previous work\nhas largely focused on in-place corruption, adding noise to each pixel or token\nindividually while keeping their locations the same. In this work, we consider\na broader class of corruption processes and denoising models over sequence data\nthat can insert and delete elements, while still being efficient to train and\nsample from. We demonstrate that these models outperform standard in-place\nmodels on an arithmetic sequence task, and that when trained on the text8\ndataset they can be used to fix spelling errors without any fine-tuning.",
          "link": "http://arxiv.org/abs/2107.07675",
          "publishedOn": "2021-07-19T00:49:06.696Z",
          "wordCount": 572,
          "title": "Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models. (arXiv:2107.07675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yukun Jiang</a>",
          "description": "Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.",
          "link": "http://arxiv.org/abs/2107.07682",
          "publishedOn": "2021-07-19T00:49:06.689Z",
          "wordCount": 600,
          "title": "The Application of Active Query K-Means in Text Classification. (arXiv:2107.07682v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alarab_I/0/1/0/all/0/1\">Ismail Alarab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakoonwit_S/0/1/0/all/0/1\">Simant Prakoonwit</a>",
          "description": "We propose a novel method to capture data points near decision boundary in\nneural network that are often referred to a specific type of uncertainty. In\nour approach, we sought to perform uncertainty estimation based on the idea of\nadversarial attack method. In this paper, uncertainty estimates are derived\nfrom the input perturbations, unlike previous studies that provide\nperturbations on the model's parameters as in Bayesian approach. We are able to\nproduce uncertainty with couple of perturbations on the inputs. Interestingly,\nwe apply the proposed method to datasets derived from blockchain. We compare\nthe performance of model uncertainty with the most recent uncertainty methods.\nWe show that the proposed method has revealed a significant outperformance over\nother methods and provided less risk to capture model uncertainty in machine\nlearning.",
          "link": "http://arxiv.org/abs/2107.07618",
          "publishedOn": "2021-07-19T00:49:06.684Z",
          "wordCount": 576,
          "title": "Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks. (arXiv:2107.07618v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganassali_L/0/1/0/all/0/1\">Luca Ganassali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1\">Marc Lelarge</a>",
          "description": "We consider alignment of sparse graphs, which consists in finding a mapping\nbetween the nodes of two graphs which preserves most of the edges. Our approach\nis to compare local structures in the two graphs, matching two nodes if their\nneighborhoods are 'close enough': for correlated Erd\\H{o}s-R\\'enyi random\ngraphs, this problem can be locally rephrased in terms of testing whether a\npair of branching trees is drawn from either a product distribution, or a\ncorrelated distribution. We design an optimal test for this problem which gives\nrise to a message-passing algorithm for graph alignment, which provably returns\nin polynomial time a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. With an average degree $\\lambda = O(1)$ in\nthe graphs, and a correlation parameter $s \\in [0,1]$, this result holds with\n$\\lambda s$ large enough, and $1-s$ small enough, completing the recent\nstate-of-the-art diagram. Tighter conditions for determining whether partial\ngraph alignment (or correlation detection in trees) is feasible in polynomial\ntime are given in terms of Kullback-Leibler divergences.",
          "link": "http://arxiv.org/abs/2107.07623",
          "publishedOn": "2021-07-19T00:49:06.669Z",
          "wordCount": 629,
          "title": "Correlation detection in trees for partial graph alignment. (arXiv:2107.07623v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1\">Alena Kopani&#x10d;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1\">Rolf Krause</a>",
          "description": "We propose a globally convergent multilevel training method for deep residual\nnetworks (ResNets). The devised method can be seen as a novel variant of the\nrecursive multilevel trust-region (RMTR) method, which operates in hybrid\n(stochastic-deterministic) settings by adaptively adjusting mini-batch sizes\nduring the training. The multilevel hierarchy and the transfer operators are\nconstructed by exploiting a dynamical system's viewpoint, which interprets\nforward propagation through the ResNet as a forward Euler discretization of an\ninitial value problem. In contrast to traditional training approaches, our\nnovel RMTR method also incorporates curvature information on all levels of the\nmultilevel hierarchy by means of the limited-memory SR1 method. The overall\nperformance and the convergence properties of our multilevel training method\nare numerically investigated using examples from the field of classification\nand regression.",
          "link": "http://arxiv.org/abs/2107.07572",
          "publishedOn": "2021-07-19T00:49:06.659Z",
          "wordCount": 564,
          "title": "Globally Convergent Multilevel Training of Deep Residual Networks. (arXiv:2107.07572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitros_J/0/1/0/all/0/1\">John Mitros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>",
          "description": "Neural networks are often utilised in critical domain applications\n(e.g.~self-driving cars, financial markets, and aerospace engineering), even\nthough they exhibit overconfident predictions for ambiguous inputs. This\ndeficiency demonstrates a fundamental flaw indicating that neural networks\noften overfit on spurious correlations. To address this problem in this work we\npresent two novel objectives that improve the ability of a network to detect\nout-of-distribution samples and therefore avoid overconfident predictions for\nambiguous inputs. We empirically demonstrate that our methods outperform the\nbaseline and perform better than the majority of existing approaches, while\nperforming competitively those that they don't outperform. Additionally, we\nempirically demonstrate the robustness of our approach against common\ncorruptions and demonstrate the importance of regularisation and auxiliary\ninformation in out-of-distribution detection.",
          "link": "http://arxiv.org/abs/2107.07564",
          "publishedOn": "2021-07-19T00:49:06.653Z",
          "wordCount": 554,
          "title": "On the Importance of Regularisation & Auxiliary Information in OOD Detection. (arXiv:2107.07564v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Rajesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Meta-learning provides a popular and effective family of methods for\ndata-efficient learning of new tasks. However, several important issues in\nmeta-learning have proven hard to study thus far. For example, performance\ndegrades in real-world settings where meta-learners must learn from a wide and\npotentially multi-modal distribution of training tasks; and when distribution\nshift exists between meta-train and meta-test task distributions. These issues\nare typically hard to study since the shape of task distributions, and shift\nbetween them are not straightforward to measure or control in standard\nbenchmarks. We propose the channel coding problem as a benchmark for\nmeta-learning. Channel coding is an important practical application where task\ndistributions naturally arise, and fast adaptation to new tasks is practically\nvaluable. We use this benchmark to study several aspects of meta-learning,\nincluding the impact of task distribution breadth and shift, which can be\ncontrolled in the coding problem. Going forward, this benchmark provides a tool\nfor the community to study the capabilities and limitations of meta-learning,\nand to drive research on practically robust and effective meta-learners.",
          "link": "http://arxiv.org/abs/2107.07579",
          "publishedOn": "2021-07-19T00:49:06.648Z",
          "wordCount": 616,
          "title": "A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>",
          "description": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.",
          "link": "http://arxiv.org/abs/2107.07647",
          "publishedOn": "2021-07-19T00:49:06.633Z",
          "wordCount": 664,
          "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling. (arXiv:2107.07647v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Gradient descent algorithm is the most utilized method when optimizing\nmachine learning issues. However, there exists many local minimums and saddle\npoints in the loss function, especially for high dimensional non-convex\noptimization problems like deep learning. Gradient descent may make loss\nfunction trapped in these local intervals which impedes further optimization,\nresulting in poor generalization ability. This paper proposes the SA-GD\nalgorithm which introduces the thought of simulated annealing algorithm to\ngradient descent. SA-GD method offers model the ability of mounting hills in\nprobability, tending to enable the model to jump out of these local areas and\nconverge to a optimal state finally. We took CNN models as an example and\ntested the basic CNN models on various benchmark datasets. Compared to the\nbaseline models with traditional gradient descent algorithm, models with SA-GD\nalgorithm possess better generalization ability without sacrificing the\nefficiency and stability of model convergence. In addition, SA-GD can be\nutilized as an effective ensemble learning approach which improves the final\nperformance significantly.",
          "link": "http://arxiv.org/abs/2107.07558",
          "publishedOn": "2021-07-19T00:49:06.611Z",
          "wordCount": 596,
          "title": "SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing. (arXiv:2107.07558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-16T00:48:26.348Z",
          "wordCount": 725,
          "title": "The Causal-Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-07-16T00:48:26.334Z",
          "wordCount": 614,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>",
          "description": "Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n\nThis paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2106.05825",
          "publishedOn": "2021-07-16T00:48:26.309Z",
          "wordCount": 677,
          "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1\">Fernando E. Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1\">Dylan Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1\">Marcos F. Criado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1\">Roberto Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1\">Carlos V. Regueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1\">Sen&#xe9;n Barro</a>",
          "description": "Smart devices, such as smartphones, wearables, robots, and others, can\ncollect vast amounts of data from their environment. This data is suitable for\ntraining machine learning models, which can significantly improve their\nbehavior, and therefore, the user experience. Federated learning is a young and\npopular framework that allows multiple distributed devices to train deep\nlearning models collaboratively while preserving data privacy. Nevertheless,\nthis approach may not be optimal for scenarios where data distribution is\nnon-identical among the participants or changes over time, causing what is\nknown as concept drift. Little research has yet been done in this field, but\nthis kind of situation is quite frequent in real life and poses new challenges\nto both continual and federated learning. Therefore, in this work, we present a\nnew method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our\nproposal is an extension of the most popular federated algorithm, Federated\nAveraging (FedAvg), enhancing it for continual adaptation under concept drift.\nWe empirically demonstrate the weaknesses of regular FedAvg and prove that\nCDA-FedAvg outperforms it in this type of scenario.",
          "link": "http://arxiv.org/abs/2105.13309",
          "publishedOn": "2021-07-16T00:48:26.297Z",
          "wordCount": 643,
          "title": "Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Machine learning approached through supervised learning requires expensive\nannotation of data. This motivates weakly supervised learning, where data are\nannotated with incomplete yet discriminative information. In this paper, we\nfocus on partial labelling, an instance of weak supervision where, from a given\ninput, we are given a set of potential targets. We review a disambiguation\nprinciple to recover full supervision from weak supervision, and propose an\nempirical disambiguation algorithm. We prove exponential convergence rates of\nour algorithm under classical learnability assumptions, and we illustrate the\nusefulness of our method on practical examples.",
          "link": "http://arxiv.org/abs/2102.02789",
          "publishedOn": "2021-07-16T00:48:26.220Z",
          "wordCount": 592,
          "title": "Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chen Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lei Lyu</a>",
          "description": "Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.",
          "link": "http://arxiv.org/abs/2105.12485",
          "publishedOn": "2021-07-16T00:48:26.207Z",
          "wordCount": 640,
          "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12684",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Overes_B/0/1/0/all/0/1\">Bart H.L. Overes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wel_M/0/1/0/all/0/1\">Michel van der Wel</a>",
          "description": "Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.",
          "link": "http://arxiv.org/abs/2101.12684",
          "publishedOn": "2021-07-16T00:48:26.183Z",
          "wordCount": 627,
          "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques. (arXiv:2101.12684v2 [q-fin.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-07-16T00:48:26.178Z",
          "wordCount": 722,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.",
          "link": "http://arxiv.org/abs/2105.04030",
          "publishedOn": "2021-07-16T00:48:26.165Z",
          "wordCount": 609,
          "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02579",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1\">F. Llorente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1\">E. Curbelo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1\">L. Martino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1\">V. Elvira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1\">D. Delgado</a>",
          "description": "Monte Carlo methods are the standard procedure for estimating complicated\nintegrals of multidimensional Bayesian posterior distributions. In this work,\nwe focus on LAIS, a class of adaptive importance samplers where Markov chain\nMonte Carlo (MCMC) algorithms are employed to drive an underlying multiple\nimportance sampling (IS) scheme. Its power lies in the simplicity of the\nlayered framework: the upper layer locates proposal densities by means of MCMC\nalgorithms; while the lower layer handles the multiple IS scheme, in order to\ncompute the final estimators. The modular nature of LAIS allows for different\npossible choices in the upper and lower layers, that will have different\nperformance and computational costs. In this work, we propose different\nenhancements in order to increase the efficiency and reduce the computational\ncost, of both upper and lower layers. The different variants are essential if\nwe aim to address computational challenges arising in real-world applications,\nsuch as highly concentrated posterior distributions (due to large amounts of\ndata, etc.). Hamiltonian-driven importance samplers are presented and tested.\nFurthermore, we introduce different strategies for designing cheaper schemes,\nfor instance, recycling samples generated in the upper layer and using them in\nthe final estimators in the lower layer. Numerical experiments show the\nbenefits of the proposed schemes as compared to the vanilla version of LAIS and\nother benchmark methods.",
          "link": "http://arxiv.org/abs/2105.02579",
          "publishedOn": "2021-07-16T00:48:26.151Z",
          "wordCount": 669,
          "title": "MCMC-driven importance samplers. (arXiv:2105.02579v3 [stat.CO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-16T00:48:26.119Z",
          "wordCount": 744,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1\">Juan S. Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>",
          "description": "The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae",
          "link": "http://arxiv.org/abs/2006.08177",
          "publishedOn": "2021-07-16T00:48:26.114Z",
          "wordCount": 622,
          "title": "Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10620",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Perdomo_J/0/1/0/all/0/1\">Juan C. Perdomo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter Bartlett</a>",
          "description": "We study the problem of adaptive control of the linear quadratic regulator\nfor systems in very high, or even infinite dimension. We demonstrate that while\nsublinear regret requires finite dimensional inputs, the ambient state\ndimension of the system need not be bounded in order to perform online control.\nWe provide the first regret bounds for LQR which hold for infinite dimensional\nsystems, replacing dependence on ambient dimension with more natural notions of\nproblem complexity. Our guarantees arise from a novel perturbation bound for\ncertainty equivalence which scales with the prediction error in estimating the\nsystem parameters, without requiring consistent parameter recovery in more\nstringent measures like the operator norm. When specialized to finite\ndimensional settings, our bounds recover near optimal dimension and time\nhorizon dependence.",
          "link": "http://arxiv.org/abs/2103.10620",
          "publishedOn": "2021-07-16T00:48:26.106Z",
          "wordCount": 589,
          "title": "Towards a Dimension-Free Understanding of Adaptive Linear Control. (arXiv:2103.10620v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1\">Christos Kaplanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1\">Jovana Mitrovi&#x107;</a>",
          "description": "We present an architecture that is effective for continual learning in an\nespecially demanding setting, where task boundaries do not exist or are\nunknown. Our architecture comprises an encoder, pre-trained on a separate\ndataset, and an ensemble of simple one-layer classifiers. Two main innovations\nare required to make this combination work. First, the provision of suitably\ngeneric pre-trained encoders has been made possible thanks to recent progress\nin self-supervised training methods. Second, pairing each classifier in the\nensemble with a key, where the key-space is identical to the latent space of\nthe encoder, allows them to be used collectively, yet selectively, via\nk-nearest neighbour lookup. We show that models trained with the\nencoders-and-ensembles architecture are state-of-the-art for the task-free\nsetting on standard image classification continual learning benchmarks, and\nimprove on prior state-of-the-art by a large margin in the most challenging\ncases. We also show that the architecture learns well in a fully incremental\nsetting, where one class is learned at a time, and we demonstrate its\neffectiveness in this setting with up to 100 classes. Finally, we show that the\narchitecture works in a task-free continual learning context where the data\ndistribution changes gradually, and existing approaches requiring knowledge of\ntask boundaries cannot be applied.",
          "link": "http://arxiv.org/abs/2105.13327",
          "publishedOn": "2021-07-16T00:48:26.099Z",
          "wordCount": 658,
          "title": "Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. Given an approximate solution to the classic $k$-means and\n$k$-medians, our algorithm for $k$-medians runs in time $O(kd \\log^2 k )$ and\nour algorithm for $k$-means runs in time $ O(k^2 d)$.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-16T00:48:26.079Z",
          "wordCount": 720,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00760",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1\">Vivien Cabannes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "Discrete supervised learning problems such as classification are often\ntackled by introducing a continuous surrogate problem akin to regression.\nBounding the original error, between estimate and solution, by the surrogate\nerror endows discrete problems with convergence rates already shown for\ncontinuous instances. Yet, current approaches do not leverage the fact that\ndiscrete problems are essentially predicting a discrete output when continuous\nproblems are predicting a continuous value. In this paper, we tackle this issue\nfor general structured prediction problems, opening the way to \"super fast\"\nrates, that is, convergence rates for the excess risk faster than $n^{-1}$,\nwhere $n$ is the number of observations, with even exponential rates with the\nstrongest assumptions. We first illustrate it for predictors based on nearest\nneighbors, generalizing rates known for binary classification to any discrete\nproblem within the framework of structured prediction. We then consider kernel\nridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast\nrates, depending on a parameter characterizing the hardness of the problem,\nthus allowing, under smoothness assumptions, to bypass the curse of\ndimensionality.",
          "link": "http://arxiv.org/abs/2102.00760",
          "publishedOn": "2021-07-16T00:48:26.072Z",
          "wordCount": 668,
          "title": "Fast rates in structured prediction. (arXiv:2102.00760v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1\">Syed Hasib Akhter Faruqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1\">Adel Alaeddini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaramillo_C/0/1/0/all/0/1\">Carlos A. Jaramillo</a>",
          "description": "Bayesian networks are powerful statistical models to study the probabilistic\nrelationships among set random variables with major applications in disease\nmodeling and prediction. Here, we propose a continuous time Bayesian network\nwith conditional dependencies, represented as Poisson regression, to model the\nimpact of exogenous variables on the conditional dependencies of the network.\nWe also propose an adaptive regularization method with an intuitive early\nstopping feature based on density based clustering for efficient learning of\nthe structure and parameters of the proposed network. Using a dataset of\npatients with multiple chronic conditions extracted from electronic health\nrecords of the Department of Veterans Affairs we compare the performance of the\nproposed approach with some of the existing methods in the literature for both\nshort-term (one-year ahead) and long-term (multi-year ahead) predictions. The\nproposed approach provides a sparse intuitive representation of the complex\nfunctional relationships between multiple chronic conditions. It also provides\nthe capability of analyzing multiple disease trajectories over time given any\ncombination of prior conditions.",
          "link": "http://arxiv.org/abs/2007.15847",
          "publishedOn": "2021-07-16T00:48:26.063Z",
          "wordCount": 671,
          "title": "A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions. (arXiv:2007.15847v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1\">Chris Cummins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Path planning, the problem of efficiently discovering high-reward\ntrajectories, often requires optimizing a high-dimensional and multimodal\nreward function. Popular approaches like CEM and CMA-ES greedily focus on\npromising regions of the search space and may get trapped in local maxima. DOO\nand VOOT balance exploration and exploitation, but use space partitioning\nstrategies independent of the reward function to be optimized. Recently, LaMCTS\nempirically learns to partition the search space in a reward-sensitive manner\nfor black-box optimization. In this paper, we develop a novel formal regret\nanalysis for when and why such an adaptive region partitioning scheme works. We\nalso propose a new path planning method PlaLaM which improves the function\nvalue estimation within each sub-region, and uses a latent representation of\nthe search space. Empirically, PlaLaM outperforms existing path planning\nmethods in 2D navigation tasks, especially in the presence of\ndifficult-to-escape local optima, and shows benefits when plugged into\nmodel-based RL with planning components such as PETS. These gains transfer to\nhighly multimodal real-world tasks, where we outperform strong baselines in\ncompiler phase ordering by up to 245% and in molecular design by up to 0.4 on\nproperties on a 0-1 scale. Code is available at\nhttps://github.com/yangkevin2/plalam.",
          "link": "http://arxiv.org/abs/2106.10544",
          "publishedOn": "2021-07-16T00:48:26.048Z",
          "wordCount": 674,
          "title": "Learning Space Partitions for Path Planning. (arXiv:2106.10544v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xilin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.",
          "link": "http://arxiv.org/abs/2103.02644",
          "publishedOn": "2021-07-16T00:48:26.042Z",
          "wordCount": 674,
          "title": "Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yingjun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Huan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">Qiang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "In this paper, we introduce variational semantic memory into meta-learning to\nacquire long-term knowledge for few-shot learning. The variational semantic\nmemory accrues and stores semantic information for the probabilistic inference\nof class prototypes in a hierarchical Bayesian framework. The semantic memory\nis grown from scratch and gradually consolidated by absorbing information from\ntasks it experiences. By doing so, it is able to accumulate long-term, general\nknowledge that enables it to learn new concepts of objects. We formulate memory\nrecall as the variational inference of a latent memory variable from addressed\ncontents, which offers a principled way to adapt the knowledge to individual\ntasks. Our variational semantic memory, as a new long-term memory module,\nconfers principled recall and update mechanisms that enable semantic\ninformation to be efficiently accrued and adapted for few-shot learning.\nExperiments demonstrate that the probabilistic modelling of prototypes achieves\na more informative representation of object classes compared to deterministic\nvectors. The consistent new state-of-the-art performance on four benchmarks\nshows the benefit of variational semantic memory in boosting few-shot\nrecognition.",
          "link": "http://arxiv.org/abs/2010.10341",
          "publishedOn": "2021-07-16T00:48:26.030Z",
          "wordCount": 654,
          "title": "Learning to Learn Variational Semantic Memory. (arXiv:2010.10341v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daniel Wontae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Y. Park</a>",
          "description": "In this paper, we devise a distributional framework on actor-critic as a\nsolution to distributional instability, action type restriction, and conflation\nbetween samples and statistics. We propose a new method that minimizes the\nCram\\'er distance with the multi-step Bellman target distribution generated\nfrom a novel Sample-Replacement algorithm denoted SR($\\lambda$), which learns\nthe correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further\nimproves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value\ndistributions and improves the performance of a conventional actor-critic\nmethod with low computational cost, in both discrete and continuous action\nspaces using Arcade Learning Environment (ALE) and PyBullet environment.",
          "link": "http://arxiv.org/abs/2105.11366",
          "publishedOn": "2021-07-16T00:48:26.025Z",
          "wordCount": 588,
          "title": "GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1\">Yury Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>",
          "description": "We present an $(e^{O(p)} \\frac{\\log \\ell}{\\log\\log\\ell})$-approximation\nalgorithm for socially fair clustering with the $\\ell_p$-objective. In this\nproblem, we are given a set of points in a metric space. Each point belongs to\none (or several) of $\\ell$ groups. The goal is to find a $k$-medians,\n$k$-means, or, more generally, $\\ell_p$-clustering that is simultaneously good\nfor all of the groups. More precisely, we need to find a set of $k$ centers $C$\nso as to minimize the maximum over all groups $j$ of $\\sum_{u \\text{ in group\n}j} d(u,C)^p$. The socially fair clustering problem was independently proposed\nby Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and\nVenkatasubramanian [2021]. Our algorithm improves and generalizes their\n$O(\\ell)$-approximation algorithms for the problem. The natural LP relaxation\nfor the problem has an integrality gap of $\\Omega(\\ell)$. In order to obtain\nour result, we introduce a strengthened LP relaxation and show that it has an\nintegrality gap of $\\Theta(\\frac{\\log \\ell}{\\log\\log\\ell})$ for a fixed $p$.\nAdditionally, we present a bicriteria approximation algorithm, which\ngeneralizes the bicriteria approximation of Abbasi et al. [2021].",
          "link": "http://arxiv.org/abs/2103.02512",
          "publishedOn": "2021-07-16T00:48:26.007Z",
          "wordCount": 642,
          "title": "Approximation Algorithms for Socially Fair Clustering. (arXiv:2103.02512v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>",
          "description": "Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.",
          "link": "http://arxiv.org/abs/2107.07511",
          "publishedOn": "2021-07-16T00:48:26.000Z",
          "wordCount": 676,
          "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. (arXiv:2107.07511v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tino_P/0/1/0/all/0/1\">Peter Ti&#x148;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople's trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.",
          "link": "http://arxiv.org/abs/2012.14261",
          "publishedOn": "2021-07-16T00:48:25.993Z",
          "wordCount": 650,
          "title": "A Survey on Neural Network Interpretability. (arXiv:2012.14261v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Derek_K/0/1/0/all/0/1\">Kenneth Derek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>",
          "description": "In the natural world, life has found innumerable ways to survive and often\nthrive. Between and even within species, each individual is in some manner\nunique, and this diversity lends adaptability and robustness to life. In this\nwork, we aim to learn a space of diverse and high-reward policies on any given\nenvironment. To this end, we introduce a generative model of policies, which\nmaps a low-dimensional latent space to an agent policy space. Our method\nenables learning an entire population of agent policies, without requiring the\nuse of separate policy parameters. Just as real world populations can adapt and\nevolve via natural selection, our method is able to adapt to changes in our\nenvironment solely by selecting for policies in latent space. We test our\ngenerative model's capabilities in a variety of environments, including an\nopen-ended grid-world and a two-player soccer environment. Code,\nvisualizations, and additional experiments can be found at\nhttps://kennyderek.github.io/adap/.",
          "link": "http://arxiv.org/abs/2107.07506",
          "publishedOn": "2021-07-16T00:48:25.958Z",
          "wordCount": 594,
          "title": "Adaptable Agent Populations via a Generative Model of Policies. (arXiv:2107.07506v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1\">Chuyang Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1\">Jean Honorio</a>",
          "description": "In this paper we propose an algorithm for exact partitioning of high-order\nmodels. We define a general class of $m$-degree Homogeneous Polynomial Models,\nwhich subsumes several examples motivated from prior literature. Exact\npartitioning can be formulated as a tensor optimization problem. We relax this\nhigh-order combinatorial problem to a convex conic form problem. To this end,\nwe carefully define the Carath\\'eodory symmetric tensor cone, and show its\nconvexity, and the convexity of its dual cone. This allows us to construct a\nprimal-dual certificate to show that the solution of the convex relaxation is\ncorrect (equal to the unobserved true group assignment) and to analyze the\nstatistical upper bound of exact partitioning.",
          "link": "http://arxiv.org/abs/1911.02161",
          "publishedOn": "2021-07-16T00:48:25.941Z",
          "wordCount": 577,
          "title": "Exact Partitioning of High-order Models with a Novel Convex Tensor Cone Relaxation. (arXiv:1911.02161v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1\">Bo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Tianyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yixin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiao Tu</a>",
          "description": "Structured pruning is a commonly used technique in deploying deep neural\nnetworks (DNNs) onto resource-constrained devices. However, the existing\npruning methods are usually heuristic, task-specified, and require an extra\nfine-tuning procedure. To overcome these limitations, we propose a framework\nthat compresses DNNs into slimmer architectures with competitive performances\nand significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two\nkeys: (i) we partition the parameters of DNNs into zero-invariant groups,\nenabling us to prune zero groups without affecting the output; and (ii) to\npromote zero groups, we then formulate a structured-sparsity optimization\nproblem and propose a novel optimization algorithm, Half-Space Stochastic\nProjected Gradient (HSPG), to solve it, which outperforms the standard proximal\nmethods on group sparsity exploration and maintains comparable convergence. To\ndemonstrate the effectiveness of OTO, we train and compress full models\nsimultaneously from scratch without fine-tuning for inference speedup and\nparameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10,\nResNet50 for CIFAR10/ImageNet and Bert for SQuAD.",
          "link": "http://arxiv.org/abs/2107.07467",
          "publishedOn": "2021-07-16T00:48:25.932Z",
          "wordCount": 608,
          "title": "Only Train Once: A One-Shot Neural Network Training And Pruning Framework. (arXiv:2107.07467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1\">Theodoros Tsiligkaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jay Roberts</a>",
          "description": "Deep neural networks are easily fooled by small perturbations known as\nadversarial attacks. Adversarial Training (AT) is a technique that\napproximately solves a robust optimization problem to minimize the worst-case\nloss and is widely regarded as the most effective defense against such attacks.\nWe develop a theoretical framework for adversarial training with FW\noptimization (FW-AT) that reveals a geometric connection between the loss\nlandscape and the distortion of $\\ell_\\infty$ FW attacks (the attack's $\\ell_2$\nnorm). Specifically, we show that high distortion of FW attacks is equivalent\nto low variation along the attack path. It is then experimentally demonstrated\non various deep neural network architectures that $\\ell_\\infty$ attacks against\nrobust models achieve near maximal $\\ell_2$ distortion. This mathematical\ntransparency differentiates FW from the more popular Projected Gradient Descent\n(PGD) optimization. To demonstrate the utility of our theoretical framework we\ndevelop FW-Adapt, a novel adversarial training algorithm which uses simple\ndistortion measure to adaptively change number of attack steps during training.\nFW-Adapt provides strong robustness at lower training times in comparison to\nPGD-AT for a variety of white-box and black-box attacks.",
          "link": "http://arxiv.org/abs/2012.12368",
          "publishedOn": "2021-07-16T00:48:25.925Z",
          "wordCount": 649,
          "title": "Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>",
          "description": "In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.",
          "link": "http://arxiv.org/abs/2008.00394",
          "publishedOn": "2021-07-16T00:48:25.918Z",
          "wordCount": 661,
          "title": "Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Mikolaj Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>",
          "description": "We study the image retrieval problem at the wireless edge, where an edge\ndevice captures an image, which is then used to retrieve similar images from an\nedge server. These can be images of the same person or a vehicle taken from\nother cameras at different times and locations. Our goal is to maximize the\naccuracy of the retrieval task under power and bandwidth constraints over the\nwireless link. Due to the stringent delay constraint of the underlying\napplication, sending the whole image at a sufficient quality is not possible.\nWe propose two alternative schemes based on digital and analog communications,\nrespectively. In the digital approach, we first propose a deep neural network\n(DNN) aided retrieval-oriented image compression scheme, whose output bit\nsequence is transmitted over the channel using conventional channel codes. In\nthe analog joint source and channel coding (JSCC) approach, the feature vectors\nare directly mapped into channel symbols. We evaluate both schemes on image\nbased re-identification (re-ID) tasks under different channel conditions,\nincluding both static and fading channels. We show that the JSCC scheme\nsignificantly increases the end-to-end accuracy, speeds up the encoding\nprocess, and provides graceful degradation with channel conditions. The\nproposed architecture is evaluated through extensive simulations on different\ndatasets and channel conditions, as well as through ablation studies.",
          "link": "http://arxiv.org/abs/2007.10915",
          "publishedOn": "2021-07-16T00:48:25.911Z",
          "wordCount": 671,
          "title": "Wireless Image Retrieval at the Edge. (arXiv:2007.10915v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1\">Ramesh Kumar Sah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Machine learning is used for inference and decision making in wearable sensor\nsystems. However, recent studies have found that machine learning algorithms\nare easily fooled by the addition of adversarial perturbations to their inputs.\nWhat is more interesting is that adversarial examples generated for one machine\nlearning system is also effective against other systems. This property of\nadversarial examples is called transferability. In this work, we take the first\nstride in studying adversarial transferability in wearable sensor systems from\nthe following perspectives: 1) transferability between machine learning\nsystems, 2) transferability across subjects, 3) transferability across sensor\nbody locations, and 4) transferability across datasets. We found strong\nuntargeted transferability in most cases. Targeted attacks were less successful\nwith success scores from $0\\%$ to $80\\%$. The transferability of adversarial\nexamples depends on many factors such as the inclusion of data from all\nsubjects, sensor body position, number of samples in the dataset, type of\nlearning algorithm, and the distribution of source and target system dataset.\nThe transferability of adversarial examples decreases sharply when the data\ndistribution of the source and target system becomes more distinct. We also\nprovide guidelines for the community for designing robust sensor systems.",
          "link": "http://arxiv.org/abs/2003.07982",
          "publishedOn": "2021-07-16T00:48:25.897Z",
          "wordCount": 661,
          "title": "Adversarial Transferability in Wearable Sensor Systems. (arXiv:2003.07982v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11094",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pradhan_H/0/1/0/all/0/1\">Hrusikesha Pradhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1\">Ketan Rajawat</a>",
          "description": "Gaussian processes provide a framework for nonlinear nonparametric Bayesian\ninference widely applicable across science and engineering. Unfortunately,\ntheir computational burden scales cubically with the training sample size,\nwhich in the case that samples arrive in perpetuity, approaches infinity. This\nissue necessitates approximations for use with streaming data, which to date\nmostly lack convergence guarantees. Thus, we develop the first online Gaussian\nprocess approximation that preserves convergence to the population posterior,\ni.e., asymptotic posterior consistency, while ameliorating its intractable\ncomplexity growth with the sample size. We propose an online compression scheme\nthat, following each a posteriori update, fixes an error neighborhood with\nrespect to the Hellinger metric centered at the current posterior, and greedily\ntosses out past kernel dictionary elements until its boundary is hit. We call\nthe resulting method Parsimonious Online Gaussian Processes (POG). For\ndiminishing error radius, exact asymptotic consistency is preserved (Theorem\n1(i)) at the cost of unbounded memory in the limit. On the other hand, for\nconstant error radius, POG converges to a neighborhood of the population\nposterior (Theorem 1(ii))but with finite memory at-worst determined by the\nmetric entropy of the feature space (Theorem 2). Experimental results are\npresented on several nonlinear regression problems which illuminates the merits\nof this approach as compared with alternatives that fix the subspace dimension\ndefining the history of past points.",
          "link": "http://arxiv.org/abs/2004.11094",
          "publishedOn": "2021-07-16T00:48:25.890Z",
          "wordCount": 678,
          "title": "Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck. (arXiv:2004.11094v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11658",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1\">Chihiro Watanabe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Biclustering is a method for detecting homogeneous submatrices in a given\nobserved matrix, and it is an effective tool for relational data analysis.\nAlthough there are many studies that estimate the underlying bicluster\nstructure of a matrix, few have enabled us to determine the appropriate number\nof biclusters in an observed matrix. Recently, a statistical test on the number\nof biclusters has been proposed for a regular-grid bicluster structure, where\nwe assume that the latent bicluster structure can be represented by row-column\nclustering. However, when the latent bicluster structure does not satisfy such\nregular-grid assumption, the previous test requires a larger number of\nbiclusters than necessary (i.e., a finer bicluster structure than necessary)\nfor the null hypothesis to be accepted, which is not desirable in terms of\ninterpreting the accepted bicluster structure. In this study, we propose a new\nstatistical test on the number of biclusters that does not require the\nregular-grid assumption and derive the asymptotic behavior of the proposed test\nstatistic in both null and alternative cases. We illustrate the effectiveness\nof the proposed method by applying it to both synthetic and practical\nrelational data matrices.",
          "link": "http://arxiv.org/abs/2102.11658",
          "publishedOn": "2021-07-16T00:48:25.882Z",
          "wordCount": 656,
          "title": "A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix. (arXiv:2102.11658v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>",
          "description": "Natural language often exhibits inherent hierarchical structure ingrained\nwith complex syntax and semantics. However, most state-of-the-art deep\ngenerative models learn embeddings only in Euclidean vector space, without\naccounting for this structural property of language. In this paper, we\ninvestigate text generation in a hyperbolic latent space to learn continuous\nhierarchical representations. An Adversarial Poincare Variational Autoencoder\n(APo-VAE) is presented, where both the prior and variational posterior of\nlatent variables are defined over a Poincare ball via wrapped normal\ndistributions. By adopting the primal-dual formulation of KL divergence, an\nadversarial learning procedure is introduced to empower robust model training.\nExtensive experiments in language modeling and dialog-response generation tasks\ndemonstrate the winning effectiveness of the proposed APo-VAE model over VAEs\nin Euclidean latent space, thanks to its superb capabilities in capturing\nlatent language hierarchies in hyperbolic space.",
          "link": "http://arxiv.org/abs/2005.00054",
          "publishedOn": "2021-07-16T00:48:25.876Z",
          "wordCount": 610,
          "title": "APo-VAE: Text Generation in Hyperbolic Space. (arXiv:2005.00054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gural_A/0/1/0/all/0/1\">Albert Gural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1\">Phillip Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikekar_M/0/1/0/all/0/1\">Mehul Tikekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1\">Boris Murmann</a>",
          "description": "The recent success of neural networks for solving difficult decision tasks\nhas incentivized incorporating smart decision making \"at the edge.\" However,\nthis work has traditionally focused on neural network inference, rather than\ntraining, due to memory and compute limitations, especially in emerging\nnon-volatile memory systems, where writes are energetically costly and reduce\nlifespan. Yet, the ability to train at the edge is becoming increasingly\nimportant as it enables real-time adaptability to device drift and\nenvironmental variation, user customization, and federated learning across\ndevices. In this work, we address two key challenges for training on edge\ndevices with non-volatile memory: low write density and low auxiliary memory.\nWe present a low-rank training scheme that addresses these challenges while\nmaintaining computational efficiency. We then demonstrate the technique on a\nrepresentative convolutional neural network across several adaptation problems,\nwhere it out-performs standard SGD both in accuracy and in number of weight\nwrites.",
          "link": "http://arxiv.org/abs/2009.03887",
          "publishedOn": "2021-07-16T00:48:25.859Z",
          "wordCount": 615,
          "title": "Low-Rank Training of Deep Neural Networks for Emerging Memory Technology. (arXiv:2009.03887v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1\">Dativa K. Tizikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1\">Jonathan Serugunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1\">Andrew Katumba</a>",
          "description": "Future communication systems are faced with increased demand for high\ncapacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet\nthese requirements, networks have become more complex and thus require new\ndesign methods and monitoring techniques, as they evolve towards becoming\nautonomous. Machine learning has come to the forefront in recent years as a\npromising technology to aid in this evolution. Optical fiber communications can\nalready provide the high capacity required for most applications, however,\nthere is a need for increased scalability and adaptability to changing user\ndemands and link conditions. Accurate performance monitoring is an integral\npart of this transformation. In this paper we review optical performance\nmonitoring techniques where machine learning algorithms have been applied.\nMoreover, since alot of OPM depends on knowledge of the signal type, we also\nreview work for modulation format recognition and bitrate identification. We\nadditionally briefly introduce a neuromorphic approach to OPM as an emerging\ntechnique that has only recently been applied to this domain.",
          "link": "http://arxiv.org/abs/2107.07338",
          "publishedOn": "2021-07-16T00:48:25.825Z",
          "wordCount": 596,
          "title": "An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palermo_J/0/1/0/all/0/1\">Joseph Palermo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Johnny Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Alok Singh</a>",
          "description": "We convert the DeepMind Mathematics Dataset into a reinforcement learning\nenvironment by interpreting it as a program synthesis problem. Each action\ntaken in the environment adds an operator or an input into a discrete compute\ngraph. Graphs which compute correct answers yield positive reward, enabling the\noptimization of a policy to construct compute graphs conditioned on problem\nstatements. Baseline models are trained using Double DQN on various subsets of\nproblem types, demonstrating the capability to learn to correctly construct\ngraphs despite the challenges of combinatorial explosion and noisy rewards.",
          "link": "http://arxiv.org/abs/2107.07373",
          "publishedOn": "2021-07-16T00:48:25.816Z",
          "wordCount": 524,
          "title": "A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis. (arXiv:2107.07373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1\">Rahul Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;nez-Ram&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busani_T/0/1/0/all/0/1\">Tito Busani</a>",
          "description": "This work investigates application of different machine learning based\nprediction methodologies to estimate the performance of silicon based textured\ncells. Concept of confidence bound regions is introduced and advantages of this\nconcept are discussed in detail. Results show that reflection profiles and\ndepth dependent optical generation profiles can be accurately estimated using\nGaussian processes with exact knowledge of uncertainty in the prediction\nvalues.It is also shown that cell design parameters can be estimated for a\ndesired performance metric.",
          "link": "http://arxiv.org/abs/2107.07342",
          "publishedOn": "2021-07-16T00:48:25.802Z",
          "wordCount": 522,
          "title": "Probabilistic analysis of solar cell optical performance using Gaussian processes. (arXiv:2107.07342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "We argue that immature data pipelines are preventing a large portion of\nindustry practitioners from leveraging the latest research on recommender\nsystems. We propose our template data stack for machine learning at \"reasonable\nscale\", and show how many challenges are solved by embracing a serverless\nparadigm. Leveraging our experience, we detail how modern open source can\nprovide a pipeline processing terabytes of data with limited infrastructure\nwork.",
          "link": "http://arxiv.org/abs/2107.07346",
          "publishedOn": "2021-07-16T00:48:25.796Z",
          "wordCount": 525,
          "title": "You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (arXiv:2107.07346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07480",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lacotte_J/0/1/0/all/0/1\">Jonathan Lacotte</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "In second-order optimization, a potential bottleneck can be computing the\nHessian matrix of the optimized function at every iteration. Randomized\nsketching has emerged as a powerful technique for constructing estimates of the\nHessian which can be used to perform approximate Newton steps. This involves\nmultiplication by a random sketching matrix, which introduces a trade-off\nbetween the computational cost of sketching and the convergence rate of the\noptimization algorithm. A theoretically desirable but practically much too\nexpensive choice is to use a dense Gaussian sketching matrix, which produces\nunbiased estimates of the exact Newton step and which offers strong\nproblem-independent convergence guarantees. We show that the Gaussian sketching\nmatrix can be drastically sparsified, significantly reducing the computational\ncost of sketching, without substantially affecting its convergence properties.\nThis approach, called Newton-LESS, is based on a recently introduced sketching\ntechnique: LEverage Score Sparsified (LESS) embeddings. We prove that\nNewton-LESS enjoys nearly the same problem-independent local convergence rate\nas Gaussian embeddings, not just up to constant factors but even down to lower\norder terms, for a large class of optimization tasks. In particular, this leads\nto a new state-of-the-art convergence result for an iterative least squares\nsolver. Finally, we extend LESS embeddings to include uniformly sparsified\nrandom sign matrices which can be implemented efficiently and which perform\nwell in numerical experiments.",
          "link": "http://arxiv.org/abs/2107.07480",
          "publishedOn": "2021-07-16T00:48:25.778Z",
          "wordCount": 666,
          "title": "Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update. (arXiv:2107.07480v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1\">Neil Band</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1\">German Chesnokov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1\">Alexey Noskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1\">Andrey Ploskonosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vatsal Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1\">Mariya Shmatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panos Tigas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "There has been significant research done on developing methods for improving\nrobustness to distributional shift and uncertainty estimation. In contrast,\nonly limited work has examined developing standard datasets and benchmarks for\nassessing these approaches. Additionally, most work on uncertainty estimation\nand robustness has developed new techniques based on small-scale regression or\nimage classification tasks. However, many tasks of practical interest have\ndifferent modalities, such as tabular data, audio, text, or sensor data, which\noffer significant challenges involving regression and discrete or continuous\nstructured prediction. Thus, given the current state of the field, a\nstandardized large-scale dataset of tasks across a range of modalities affected\nby distributional shifts is necessary. This will enable researchers to\nmeaningfully evaluate the plethora of recently developed uncertainty\nquantification methods, as well as assessment criteria and state-of-the-art\nbaselines. In this work, we propose the \\emph{Shifts Dataset} for evaluation of\nuncertainty estimates and robustness to distributional shift. The dataset,\nwhich has been collected from industrial sources and services, is composed of\nthree tasks, with each corresponding to a particular data modality: tabular\nweather prediction, machine translation, and self-driving car (SDC) vehicle\nmotion prediction. All of these data modalities and tasks are affected by real,\n`in-the-wild' distributional shifts and pose interesting challenges with\nrespect to uncertainty estimation. In this work we provide a description of the\ndataset and baseline results for all tasks.",
          "link": "http://arxiv.org/abs/2107.07455",
          "publishedOn": "2021-07-16T00:48:25.772Z",
          "wordCount": 693,
          "title": "Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fang-Yi Yu</a>",
          "description": "This paper introduces an optimization problem for proper scoring rule design.\nConsider a principal who wants to collect an agent's prediction about an\nunknown state. The agent can either report his prior prediction or access a\ncostly signal and report the posterior prediction. Given a collection of\npossible distributions containing the agent's posterior prediction\ndistribution, the principal's objective is to design a bounded scoring rule to\nmaximize the agent's worst-case payoff increment between reporting his\nposterior prediction and reporting his prior prediction.\n\nWe study two settings of such optimization for proper scoring rules: static\nand asymptotic settings. In the static setting, where the agent can access one\nsignal, we propose an efficient algorithm to compute an optimal scoring rule\nwhen the collection of distributions is finite. The agent can adaptively and\nindefinitely refine his prediction in the asymptotic setting. We first consider\na sequence of collections of posterior distributions with vanishing covariance,\nwhich emulates general estimators with large samples, and show the optimality\nof the quadratic scoring rule. Then, when the agent's posterior distribution is\na Beta-Bernoulli process, we find that the log scoring rule is optimal. We also\nprove the optimality of the log scoring rule over a smaller set of functions\nfor categorical distributions with Dirichlet priors.",
          "link": "http://arxiv.org/abs/2107.07420",
          "publishedOn": "2021-07-16T00:48:25.765Z",
          "wordCount": 643,
          "title": "Optimal Scoring Rule Design. (arXiv:2107.07420v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07423",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ginige_N/0/1/0/all/0/1\">Nipuni Ginige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manosha_K/0/1/0/all/0/1\">K. B. Shashika Manosha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1\">Nandana Rajatheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Latva_aho_M/0/1/0/all/0/1\">Matti Latva-aho</a>",
          "description": "Reconfigurable intelligent surface (RIS) is an emerging technology for\nimproving performance in fifth-generation (5G) and beyond networks. Practically\nchannel estimation of RIS-assisted systems is challenging due to the passive\nnature of the RIS. The purpose of this paper is to introduce a deep\nlearning-based, low complexity channel estimator for the RIS-assisted\nmulti-user single-input-multiple-output (SIMO) orthogonal frequency division\nmultiplexing (OFDM) system with hardware impairments. We propose an untrained\ndeep neural network (DNN) based on the deep image prior (DIP) network to\ndenoise the effective channel of the system obtained from the conventional\npilot-based least-square (LS) estimation and acquire a more accurate\nestimation. We have shown that our proposed method has high performance in\nterms of accuracy and low complexity compared to conventional methods. Further,\nwe have shown that the proposed estimator is robust to interference caused by\nthe hardware impairments at the transceiver and RIS.",
          "link": "http://arxiv.org/abs/2107.07423",
          "publishedOn": "2021-07-16T00:48:25.725Z",
          "wordCount": 595,
          "title": "Untrained DNN for Channel Estimation of RIS-Assisted Multi-User OFDM System with Hardware Impairments. (arXiv:2107.07423v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_J/0/1/0/all/0/1\">Juliane Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1\">Sebastian Gerwinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weilbach_C/0/1/0/all/0/1\">Christian Weilbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Melih Kandemir</a>",
          "description": "Understanding physical phenomena oftentimes means understanding the\nunderlying dynamical system that governs observational measurements. While\naccurate prediction can be achieved with black box systems, they often lack\ninterpretability and are less amenable for further expert investigation.\nAlternatively, the dynamics can be analysed via symbolic regression. In this\npaper, we extend the approach by (Udrescu et al., 2020) called AIFeynman to the\ndynamic setting to perform symbolic regression on ODE systems based on\nobservations from the resulting trajectories. We compare this extension to\nstate-of-the-art approaches for symbolic regression empirically on several\ndynamical systems for which the ground truth equations of increasing complexity\nare available. Although the proposed approach performs best on this benchmark,\nwe observed difficulties of all the compared symbolic regression approaches on\nmore complex systems, such as Cart-Pole.",
          "link": "http://arxiv.org/abs/2107.07345",
          "publishedOn": "2021-07-16T00:48:25.703Z",
          "wordCount": 557,
          "title": "Inferring the Structure of Ordinary Differential Equations. (arXiv:2107.07345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.",
          "link": "http://arxiv.org/abs/2107.07436",
          "publishedOn": "2021-07-16T00:48:25.693Z",
          "wordCount": 533,
          "title": "FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07483",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Valente_F/0/1/0/all/0/1\">Francisco Valente</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paredes_S/0/1/0/all/0/1\">Sim&#xe3;o Paredes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henriques_J/0/1/0/all/0/1\">Jorge Henriques</a>",
          "description": "In this study, we present a novel clinical decision support system and\ndiscuss its interpretability-related properties. It combines a decision set of\nrules with a machine learning scheme to offer global and local\ninterpretability. More specifically, machine learning is used to predict the\nlikelihood of each of those rules to be correct for a particular patient, which\nmay also contribute to better predictive performances. Moreover, the\nreliability analysis of individual predictions is also addressed, contributing\nto further personalized interpretability. The combination of these several\nelements may be crucial to obtain the clinical stakeholders' trust, leading to\na better assessment of patients' conditions and improvement of the physicians'\ndecision-making.",
          "link": "http://arxiv.org/abs/2107.07483",
          "publishedOn": "2021-07-16T00:48:25.687Z",
          "wordCount": 559,
          "title": "Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems. (arXiv:2107.07483v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>",
          "description": "The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.",
          "link": "http://arxiv.org/abs/2107.07431",
          "publishedOn": "2021-07-16T00:48:25.677Z",
          "wordCount": 606,
          "title": "High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1\">Mike Laszkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1\">Johannes Lederer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Asja Fischer</a>",
          "description": "Normalizing flows, which learn a distribution by transforming the data to\nsamples from a Gaussian base distribution, have proven powerful density\napproximations. But their expressive power is limited by this choice of the\nbase distribution. We, therefore, propose to generalize the base distribution\nto a more elaborate copula distribution to capture the properties of the target\ndistribution more accurately. In a first empirical analysis, we demonstrate\nthat this replacement can dramatically improve the vanilla normalizing flows in\nterms of flexibility, stability, and effectivity for heavy-tailed data. Our\nresults suggest that the improvements are related to an increased local\nLipschitz-stability of the learned flow.",
          "link": "http://arxiv.org/abs/2107.07352",
          "publishedOn": "2021-07-16T00:48:25.658Z",
          "wordCount": 550,
          "title": "Copula-Based Normalizing Flows. (arXiv:2107.07352v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiying Li</a>",
          "description": "Gradient Episodic Memory is indeed a novel method for continual learning,\nwhich solves new problems quickly without forgetting previously acquired\nknowledge. However, in the process of studying the paper, we found there were\nsome problems in the proof of the dual problem of Quadratic Program, so here we\ngive our fixed version for this problem.",
          "link": "http://arxiv.org/abs/2107.07384",
          "publishedOn": "2021-07-16T00:48:25.640Z",
          "wordCount": 485,
          "title": "A Fixed Version of Quadratic Program in Gradient Episodic Memory. (arXiv:2107.07384v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fickinger_A/0/1/0/all/0/1\">Arnaud Fickinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1\">Samyak Parajuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1\">Nicholas Rhinehart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1\">Glen Berseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Reinforcement learning (RL) provides a framework for learning goal-directed\npolicies given user-specified rewards. However, since designing rewards often\nrequires substantial engineering effort, we are interested in the problem of\nlearning without rewards, where agents must discover useful behaviors in the\nabsence of task-specific incentives. Intrinsic motivation is a family of\nunsupervised RL techniques which develop general objectives for an RL agent to\noptimize that lead to better exploration or the discovery of skills. In this\npaper, we propose a new unsupervised RL technique based on an adversarial game\nwhich pits two policies against each other to compete over the amount of\nsurprise an RL agent experiences. The policies each take turns controlling the\nagent. The Explore policy maximizes entropy, putting the agent into surprising\nor unfamiliar situations. Then, the Control policy takes over and seeks to\nrecover from those situations by minimizing entropy. The game harnesses the\npower of multi-agent competition to drive the agent to seek out increasingly\nsurprising parts of the environment while learning to gain mastery over them.\nWe show empirically that our method leads to the emergence of complex skills by\nexhibiting clear phase transitions. Furthermore, we show both theoretically\n(via a latent state space coverage argument) and empirically that our method\nhas the potential to be applied to the exploration of stochastic,\npartially-observed environments. We show that Adversarial Surprise learns more\ncomplex behaviors, and explores more effectively than competitive baselines,\noutperforming intrinsic motivation methods based on active inference,\nnovelty-seeking (Random Network Distillation (RND)), and multi-agent\nunsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom\nenvironments.",
          "link": "http://arxiv.org/abs/2107.07394",
          "publishedOn": "2021-07-16T00:48:25.631Z",
          "wordCount": 698,
          "title": "Explore and Control with Adversarial Surprise. (arXiv:2107.07394v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to\nits potential sample efficiency compared to model-free RL. However, existing\nempirical model-based RL approaches lack the ability to explore. This work\nstudies a computationally and statistically efficient model-based algorithm for\nboth Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes\n(MDPs). For both models, our algorithm guarantees polynomial sample complexity\nand only uses access to a planning oracle. Experimentally, we first demonstrate\nthe flexibility and efficacy of our algorithm on a set of exploration\nchallenging control tasks where existing empirical model-based RL approaches\ncompletely fail. We then show that our approach retains excellent performance\neven in common dense reward control benchmarks that do not require heavy\nexploration. Finally, we demonstrate that our method can also perform\nreward-free exploration efficiently. Our code can be found at\nhttps://github.com/yudasong/PCMLP.",
          "link": "http://arxiv.org/abs/2107.07410",
          "publishedOn": "2021-07-16T00:48:25.625Z",
          "wordCount": 567,
          "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration. (arXiv:2107.07410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_E/0/1/0/all/0/1\">Elaine Pimentel</a> (UFRN), <a href=\"http://arxiv.org/find/cs/1/au:+Tassi_E/0/1/0/all/0/1\">Enrico Tassi</a> (Inria)",
          "description": "Logical frameworks and meta-languages form a common substrate for\nrepresenting, implementing and reasoning about a wide variety of deductive\nsystems of interest in logic and computer science. Their design, implementation\nand their use in reasoning tasks, ranging from the correctness of software to\nthe properties of formal systems, have been the focus of considerable research\nover the last two decades. This workshop brings together designers,\nimplementors and practitioners to discuss various aspects impinging on the\nstructure and utility of logical frameworks, including the treatment of\nvariable binding, inductive and co-inductive reasoning techniques and the\nexpressiveness and lucidity of the reasoning process.",
          "link": "http://arxiv.org/abs/2107.07376",
          "publishedOn": "2021-07-16T00:48:25.611Z",
          "wordCount": 567,
          "title": "Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice. (arXiv:2107.07376v1 [cs.LO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1\">Alon Talmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "Models pre-trained with a language modeling objective possess ample world\nknowledge and language skills, but are known to struggle in tasks that require\nreasoning. In this work, we propose to leverage semi-structured tables, and\nautomatically generate at scale question-paragraph pairs, where answering the\nquestion requires reasoning over multiple facts in the paragraph. We add a\npre-training step over this synthetic data, which includes examples that\nrequire 16 different reasoning skills such as number comparison, conjunction,\nand fact composition. To improve data efficiency, we propose sampling\nstrategies that focus training on reasoning skills the model is currently\nlacking. We evaluate our approach on three reading comprehension datasets that\nare focused on reasoning, and show that our model, PReasM, substantially\noutperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling\nexamples based on current model errors leads to faster training and higher\noverall performance.",
          "link": "http://arxiv.org/abs/2107.07261",
          "publishedOn": "2021-07-16T00:48:25.604Z",
          "wordCount": 587,
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rutwik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1\">Bruno Astuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1\">Tyler Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1\">Will Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1\">Justin Banaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1\">Kevin Sweetwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Allen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rina Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1\">Kevin McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1\">Thomas Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1\">Jason Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1\">Valentina Pedoia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sharmila Majumdar</a>",
          "description": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.",
          "link": "http://arxiv.org/abs/2107.07341",
          "publishedOn": "2021-07-16T00:48:25.589Z",
          "wordCount": 771,
          "title": "Leveraging wisdom of the crowds to improve consensus among radiologists by real time, blinded collaborations on a digital swarm platform. (arXiv:2107.07341v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bej_S/0/1/0/all/0/1\">Saptarshi Bej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_K/0/1/0/all/0/1\">Kristian Schultz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Prashant Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfien_M/0/1/0/all/0/1\">Markus Wolfien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolkenhauer_O/0/1/0/all/0/1\">Olaf Wolkenhauer</a>",
          "description": "Over 85 oversampling algorithms, mostly extensions of the SMOTE algorithm,\nhave been built over the past two decades, to solve the problem of imbalanced\ndatasets. However, it has been evident from previous studies that different\noversampling algorithms have different degrees of efficiency with different\nclassifiers. With numerous algorithms available, it is difficult to decide on\nan oversampling algorithm for a chosen classifier. Here, we overcome this\nproblem with a multi-schematic and classifier-independent oversampling\napproach: ProWRAS(Proximity Weighted Random Affine Shadowsampling). ProWRAS\nintegrates the Localized Random Affine Shadowsampling (LoRAS)algorithm and the\nProximity Weighted Synthetic oversampling (ProWSyn) algorithm. By controlling\nthe variance of the synthetic samples, as well as a proximity-weighted\nclustering system of the minority classdata, the ProWRAS algorithm improves\nperformance, compared to algorithms that generate synthetic samples through\nmodelling high dimensional convex spaces of the minority class. ProWRAS has\nfour oversampling schemes, each of which has its unique way to model the\nvariance of the generated data. Most importantly, the performance of ProWRAS\nwith proper choice of oversampling schemes, is independent of the classifier\nused. We have benchmarked our newly developed ProWRAS algorithm against five\nsate-of-the-art oversampling models and four different classifiers on 20\npublicly available datasets. ProWRAS outperforms other oversampling algorithms\nin a statistically significant way, in terms of both F1-score and Kappa-score.\nMoreover, we have introduced a novel measure for classifier independence\nI-score, and showed quantitatively that ProWRAS performs better, independent of\nthe classifier used. In practice, ProWRAS customizes synthetic sample\ngeneration according to a classifier of choice and thereby reduces benchmarking\nefforts.",
          "link": "http://arxiv.org/abs/2107.07349",
          "publishedOn": "2021-07-16T00:48:25.573Z",
          "wordCount": 695,
          "title": "A multi-schematic classifier-independent oversampling approach for imbalanced datasets. (arXiv:2107.07349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azam_M/0/1/0/all/0/1\">Md Ali Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossen_A/0/1/0/all/0/1\">Abir Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Hafizur Rahman</a>",
          "description": "Biologically inspired computing techniques are very effective and useful in\nmany areas of research including data clustering. Ant clustering algorithm is a\nnature-inspired clustering technique which is extensively studied for over two\ndecades. In this study, we extend the ant clustering algorithm (ACA) to a\nhybrid ant clustering algorithm (hACA). Specifically, we include a genetic\nalgorithm in standard ACA to extend the hybrid algorithm for better\nperformance. We also introduced novel pick up and drop off rules to speed up\nthe clustering performance. We study the performance of the hACA algorithm and\ncompare with standard ACA as a benchmark.",
          "link": "http://arxiv.org/abs/2107.07382",
          "publishedOn": "2021-07-16T00:48:25.558Z",
          "wordCount": 545,
          "title": "Hybrid Ant Swarm-Based Data Clustering. (arXiv:2107.07382v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chulin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>",
          "description": "We study the realistic potential of conducting backdoor attack against deep\nneural networks (DNNs) during deployment stage. Specifically, our goal is to\ndesign a deployment-stage backdoor attack algorithm that is both threatening\nand realistically implementable. To this end, we propose Subnet Replacement\nAttack (SRA), which is capable of embedding backdoor into DNNs by directly\nmodifying a limited number of model parameters. Considering the realistic\npracticability, we abandon the strong white-box assumption widely adopted in\nexisting studies, instead, our algorithm works in a gray-box setting, where\narchitecture information of the victim model is available but the adversaries\ndo not have any knowledge of parameter values. The key philosophy underlying\nour approach is -- given any neural network instance (regardless of its\nspecific parameter values) of a certain architecture, we can always embed a\nbackdoor into that model instance, by replacing a very narrow subnet of a\nbenign model (without backdoor) with a malicious backdoor subnet, which is\ndesigned to be sensitive (fire large activation value) to a particular backdoor\ntrigger pattern.",
          "link": "http://arxiv.org/abs/2107.07240",
          "publishedOn": "2021-07-16T00:48:25.541Z",
          "wordCount": 628,
          "title": "Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (arXiv:2107.07240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1\">Brian Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>",
          "description": "We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.",
          "link": "http://arxiv.org/abs/2107.07110",
          "publishedOn": "2021-07-16T00:48:25.503Z",
          "wordCount": 666,
          "title": "Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafrasteh_B/0/1/0/all/0/1\">Bahram Jafrasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villacampa_Calvo_C/0/1/0/all/0/1\">Carlos Villacampa-Calvo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1\">Daniel Hern&#xe1;ndez-Lobato</a>",
          "description": "Gaussian Processes (GPs) are Bayesian models that provide uncertainty\nestimates associated to the predictions made. They are also very flexible due\nto their non-parametric nature. Nevertheless, GPs suffer from poor scalability\nas the number of training instances N increases. More precisely, they have a\ncubic cost with respect to $N$. To overcome this problem, sparse GP\napproximations are often used, where a set of $M \\ll N$ inducing points is\nintroduced during training. The location of the inducing points is learned by\nconsidering them as parameters of an approximate posterior distribution $q$.\nSparse GPs, combined with variational inference for inferring $q$, reduce the\ntraining cost of GPs to $\\mathcal{O}(M^3)$. Critically, the inducing points\ndetermine the flexibility of the model and they are often located in regions of\nthe input space where the latent function changes. A limitation is, however,\nthat for some learning tasks a large number of inducing points may be required\nto obtain a good prediction performance. To address this limitation, we propose\nhere to amortize the computation of the inducing points locations, as well as\nthe parameters of the variational posterior approximation q. For this, we use a\nneural network that receives the observed data as an input and outputs the\ninducing points locations and the parameters of $q$. We evaluate our method in\nseveral experiments, showing that it performs similar or better than other\nstate-of-the-art sparse variational GP approaches. However, with our method the\nnumber of inducing points is reduced drastically due to their dependency on the\ninput data. This makes our method scale to larger datasets and have faster\ntraining and prediction times.",
          "link": "http://arxiv.org/abs/2107.07281",
          "publishedOn": "2021-07-16T00:48:25.487Z",
          "wordCount": 695,
          "title": "Input Dependent Sparse Gaussian Processes. (arXiv:2107.07281v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kungurtsev_V/0/1/0/all/0/1\">Vyacheslav Kungurtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1\">Adam Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1\">Brian Jalaian</a>",
          "description": "Federated learning performed by a decentralized networks of agents is\nbecoming increasingly important with the prevalence of embedded software on\nautonomous devices. Bayesian approaches to learning benefit from offering more\ninformation as to the uncertainty of a random quantity, and Langevin and\nHamiltonian methods are effective at realizing sampling from an uncertain\ndistribution with large parameter dimensions. Such methods have only recently\nappeared in the decentralized setting, and either exclusively use stochastic\ngradient Langevin and Hamiltonian Monte Carlo approaches that require a\ndiminishing stepsize to asymptotically sample from the posterior and are known\nin practice to characterize uncertainty less faithfully than constant step-size\nmethods with a Metropolis adjustment, or assume strong convexity properties of\nthe potential function. We present the first approach to incorporating constant\nstepsize Metropolis-adjusted HMC in the decentralized sampling framework, show\ntheoretical guarantees for consensus and probability distance to the posterior\nstationary distribution, and demonstrate their effectiveness numerically on\nstandard real world problems, including decentralized learning of neural\nnetworks which is known to be highly non-convex.",
          "link": "http://arxiv.org/abs/2107.07211",
          "publishedOn": "2021-07-16T00:48:25.480Z",
          "wordCount": 609,
          "title": "Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. (arXiv:2107.07211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.",
          "link": "http://arxiv.org/abs/2107.07314",
          "publishedOn": "2021-07-16T00:48:25.471Z",
          "wordCount": 653,
          "title": "Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Mansheej Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>",
          "description": "The recent success of deep learning has partially been driven by training\nincreasingly overparametrized networks on ever larger datasets. It is therefore\nnatural to ask: how much of the data is superfluous, which examples are\nimportant for generalization, and how do we find them? In this work, we make\nthe striking observation that, on standard vision benchmarks, the initial loss\ngradient norm of individual training examples, averaged over several weight\ninitializations, can be used to identify a smaller set of training data that is\nimportant for generalization. Furthermore, after only a few epochs of training,\nthe information in gradient norms is reflected in the normed error--L2 distance\nbetween the predicted probabilities and one hot labels--which can be used to\nprune a significant fraction of the dataset without sacrificing test accuracy.\nBased on this, we propose data pruning methods which use only local information\nearly in training, and connect them to recent work that prunes data by\ndiscarding examples that are rarely forgotten over the course of training. Our\nmethods also shed light on how the underlying data distribution shapes the\ntraining dynamics: they rank examples based on their importance for\ngeneralization, detect noisy examples and identify subspaces of the model's\ndata representation that are relatively stable over training.",
          "link": "http://arxiv.org/abs/2107.07075",
          "publishedOn": "2021-07-16T00:48:25.464Z",
          "wordCount": 650,
          "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>",
          "description": "Robust physics discovery is of great interest for many scientific and\nengineering fields. Inspired by the principle that a representative model is\nthe one simplest possible, a new model selection criteria considering both\nmodel's Parsimony and Sparsity is proposed. A Parsimony Enhanced Sparse\nBayesian Learning (PeSBL) method is developed for discovering the governing\nPartial Differential Equations (PDEs) of nonlinear dynamical systems. Compared\nwith the conventional Sparse Bayesian Learning (SBL) method, the PeSBL method\npromotes parsimony of the learned model in addition to its sparsity. In this\nmethod, the parsimony of model terms is evaluated using their locations in the\nprescribed candidate library, for the first time, considering the increased\ncomplexity with the power of polynomials and the order of spatial derivatives.\nSubsequently, the model parameters are updated through Bayesian inference with\nthe raw data. This procedure aims to reduce the error associated with the\npossible loss of information in data preprocessing and numerical\ndifferentiation prior to sparse regression. Results of numerical case studies\nindicate that the governing PDEs of many canonical dynamical systems can be\ncorrectly identified using the proposed PeSBL method from highly noisy data (up\nto 50% in the current study). Next, the proposed methodology is extended for\nstochastic PDE learning where all parameters and modeling error are considered\nas random variables. Hierarchical Bayesian Inference (HBI) is integrated with\nthe proposed framework for stochastic PDE learning from a population of\nobservations. Finally, the proposed PeSBL is demonstrated for system response\nprediction with uncertainties and anomaly diagnosis. Codes of all demonstrated\nexamples in this study are available on the website: https://github.com/ymlasu.",
          "link": "http://arxiv.org/abs/2107.07040",
          "publishedOn": "2021-07-16T00:48:25.449Z",
          "wordCount": 704,
          "title": "Parsimony-Enhanced Sparse Bayesian Learning for Robust Discovery of Partial Differential Equations. (arXiv:2107.07040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zohdinasab_T/0/1/0/all/0/1\">Tahereh Zohdinasab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccio_V/0/1/0/all/0/1\">Vincenzo Riccio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambi_A/0/1/0/all/0/1\">Alessio Gambi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1\">Paolo Tonella</a>",
          "description": "Deep Learning (DL) has been successfully applied to a wide range of\napplication domains, including safety-critical ones. Several DL testing\napproaches have been recently proposed in the literature but none of them aims\nto assess how different interpretable features of the generated inputs affect\nthe system's behaviour. In this paper, we resort to Illumination Search to find\nthe highest-performing test cases (i.e., misbehaving and closest to\nmisbehaving), spread across the cells of a map representing the feature space\nof the system. We introduce a methodology that guides the users of our approach\nin the tasks of identifying and quantifying the dimensions of the feature space\nfor a given domain. We developed DeepHyperion, a search-based tool for DL\nsystems that illuminates, i.e., explores at large, the feature space, by\nproviding developers with an interpretable feature map where automatically\ngenerated inputs are placed along with information about the exposed\nbehaviours.",
          "link": "http://arxiv.org/abs/2107.06997",
          "publishedOn": "2021-07-16T00:48:25.429Z",
          "wordCount": 629,
          "title": "DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search. (arXiv:2107.06997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07271",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1\">Andrew Moyes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1\">Richard Gault</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1\">Ji Ming</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>",
          "description": "Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.",
          "link": "http://arxiv.org/abs/2107.07271",
          "publishedOn": "2021-07-16T00:48:25.382Z",
          "wordCount": 669,
          "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06572",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cohen_M/0/1/0/all/0/1\">Michael B. Cohen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1\">Aaron Sidford</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We show that standard extragradient methods (i.e. mirror prox and dual\nextrapolation) recover optimal accelerated rates for first-order minimization\nof smooth convex functions. To obtain this result we provide a fine-grained\ncharacterization of the convergence rates of extragradient methods for solving\nmonotone variational inequalities in terms of a natural condition we call\nrelative Lipschitzness. We further generalize this framework to handle local\nand randomized notions of relative Lipschitzness and thereby recover rates for\nbox-constrained $\\ell_\\infty$ regression based on area convexity and complexity\nbounds achieved by accelerated (randomized) coordinate descent for smooth\nconvex function minimization.",
          "link": "http://arxiv.org/abs/2011.06572",
          "publishedOn": "2021-07-16T00:48:25.365Z",
          "wordCount": 581,
          "title": "Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. (arXiv:2011.06572v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1\">Filippo Neri</a>",
          "description": "In the paper, we propose a novel methodology to map learning algorithms on\ndata (performance map) in order to gain more insights in the distribution of\ntheir performances across their parameter space. This methodology provides\nuseful information when selecting a learner's best configuration for the data\nat hand, and it also enhances the comparison of learners across learning\ncontexts. In order to explain the proposed methodology, the study introduces\nthe notions of learning context, performance map, and high performance\nfunction. It then applies these concepts to a variety of learning contexts to\nshow how their use can provide more insights in a learner's behavior, and can\nenhance the comparison of learners across learning contexts. The study is\ncompleted by an extensive experimental study describing how the proposed\nmethodology can be applied.",
          "link": "http://arxiv.org/abs/2107.06981",
          "publishedOn": "2021-07-16T00:48:25.343Z",
          "wordCount": 577,
          "title": "Mapping Learning Algorithms on Data, a useful step for optimizing performances and their comparison. (arXiv:2107.06981v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zetian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jason Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Leslie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Michelle A. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.",
          "link": "http://arxiv.org/abs/2107.07502",
          "publishedOn": "2021-07-16T00:48:25.336Z",
          "wordCount": 722,
          "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07412",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sahmoud_T/0/1/0/all/0/1\">Thaer Sahmoud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashor_W/0/1/0/all/0/1\">Wesam Ashor</a>",
          "description": "Gaza Strip suffers from a chronic electricity deficit that affects all\nindustries including the telecommunication field, so there is a need to\noptimize and reduce power consumption of the telecommunication equipment. In\nthis paper we propose a new model that helps GSM radio frequency engineers to\nchoose the optimal value of hysteresis parameter for Ericsson BTS power saving\nalgorithm which aims to switch OFF unused frequency channels, our model is\nbased on unsupervised machine learning clustering K-means algorithm. By using\nour model with BTS power saving algorithm we reduce number of active TRX by\n20.9%.",
          "link": "http://arxiv.org/abs/2107.07412",
          "publishedOn": "2021-07-16T00:48:25.316Z",
          "wordCount": 543,
          "title": "Assign Hysteresis Parameter For Ericsson BTS Power Saving Algorithm Using Unsupervised Learning. (arXiv:2107.07412v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakides_G/0/1/0/all/0/1\">George V. Moustakides</a>",
          "description": "We are interested in the design of generative networks. The training of these\nmathematical structures is mostly performed with the help of adversarial\n(min-max) optimization problems. We propose a simple methodology for\nconstructing such problems assuring, at the same time, consistency of the\ncorresponding solution. We give characteristic examples developed by our\nmethod, some of which can be recognized from other applications, and some are\nintroduced here for the first time. We present a new metric, the likelihood\nratio, that can be employed online to examine the convergence and stability\nduring the training of different Generative Adversarial Networks (GANs).\nFinally, we compare various possibilities by applying them to well-known\ndatasets using neural networks of different configurations and sizes.",
          "link": "http://arxiv.org/abs/2002.00865",
          "publishedOn": "2021-07-16T00:48:25.301Z",
          "wordCount": 595,
          "title": "Designing GANs: A Likelihood Ratio Approach. (arXiv:2002.00865v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "We propose a generative adversarial network with multiple discriminators,\nwhere each discriminator is specialized to distinguish the subset of a real\ndataset. This approach facilitates learning a generator coinciding with the\nunderlying data distribution and thus mitigates the chronic mode collapse\nproblem. From the inspiration of multiple choice learning, we guide each\ndiscriminator to have expertise in the subset of the entire data and allow the\ngenerator to find reasonable correspondences between the latent and real data\nspaces automatically without supervision for training examples and the number\nof discriminators. Despite the use of multiple discriminators, the backbone\nnetworks are shared across the discriminators and the increase of training cost\nis minimized. We demonstrate the effectiveness of our algorithm in the standard\ndatasets using multiple evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.07260",
          "publishedOn": "2021-07-16T00:48:25.283Z",
          "wordCount": 551,
          "title": "MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators. (arXiv:2107.07260v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07312",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Chong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vishwakarma_S/0/1/0/all/0/1\">Shelly Vishwakarma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1\">Fangzhan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Julier_S/0/1/0/all/0/1\">Simon Julier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chetty_K/0/1/0/all/0/1\">Kevin Chetty</a>",
          "description": "Micro-Doppler signatures contain considerable information about target\ndynamics. However, the radar sensing systems are easily affected by noisy\nsurroundings, resulting in uninterpretable motion patterns on the micro-Doppler\nspectrogram. Meanwhile, radar returns often suffer from multipath, clutter and\ninterference. These issues lead to difficulty in, for example motion feature\nextraction, activity classification using micro Doppler signatures ($\\mu$-DS),\netc. In this paper, we propose a latent feature-wise mapping strategy, called\nFeature Mapping Network (FMNet), to transform measured spectrograms so that\nthey more closely resemble the output from a simulation under the same\nconditions. Based on measured spectrogram and the matched simulated data, our\nframework contains three parts: an Encoder which is used to extract latent\nrepresentations/features, a Decoder outputs reconstructed spectrogram according\nto the latent features, and a Discriminator minimizes the distance of latent\nfeatures of measured and simulated data. We demonstrate the FMNet with six\nactivities data and two experimental scenarios, and final results show strong\nenhanced patterns and can keep actual motion information to the greatest\nextent. On the other hand, we also propose a novel idea which trains a\nclassifier with only simulated data and predicts new measured samples after\ncleaning them up with the FMNet. From final classification results, we can see\nsignificant improvements.",
          "link": "http://arxiv.org/abs/2107.07312",
          "publishedOn": "2021-07-16T00:48:25.277Z",
          "wordCount": 654,
          "title": "FMNet: Latent Feature-wise Mapping Network for Cleaning up Noisy Micro-Doppler Spectrogram. (arXiv:2107.07312v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haiyong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuechun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yida Zhu</a>",
          "description": "Human activity recognition (HAR) based on IMU sensors is an essential domain\nin ubiquitous computing. Because of the improving trend to deploy artificial\nintelligence into IoT devices or smartphones, more researchers design the HAR\nmodels for embedded devices. We propose a plug-and-play HAR modeling pipeline\nwith multi-level distillation to build deep convolutional HAR models with\nnative support of embedded devices. SMLDist consists of stage distillation,\nmemory distillation, and logits distillation, which covers all the information\nflow of the deep models. Stage distillation constrains the learning direction\nof the intermediate features. Memory distillation teaches the student models\nhow to explain and store the inner relationship between high-dimensional\nfeatures based on Hopfield networks. Logits distillation constructs distilled\nlogits by a smoothed conditional rule to keep the probable distribution and\nimprove the correctness of the soft target. We compare the performance of\naccuracy, F1 macro score, and energy cost on the embedded platform of various\nstate-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The\nproduced model has well balance with robustness, efficiency, and accuracy.\nSMLDist can also compress the models with minor performance loss in an equal\ncompression rate than other state-of-the-art knowledge distillation methods on\nseven public datasets.",
          "link": "http://arxiv.org/abs/2107.07331",
          "publishedOn": "2021-07-16T00:48:25.265Z",
          "wordCount": 674,
          "title": "Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1\">Amirreza Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1\">Manolis Sifalakis</a>",
          "description": "Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.",
          "link": "http://arxiv.org/abs/2107.07305",
          "publishedOn": "2021-07-16T00:48:25.258Z",
          "wordCount": 728,
          "title": "Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1\">L&#xea;-Nguy&#xea;n Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faucon_L/0/1/0/all/0/1\">Louis Faucon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jungo_A/0/1/0/all/0/1\">Aidan Jungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volodin_S/0/1/0/all/0/1\">Sergei Volodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papuc_D/0/1/0/all/0/1\">Dalia Papuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liossatos_O/0/1/0/all/0/1\">Orfeas Liossatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crulis_B/0/1/0/all/0/1\">Ben Crulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighanimine_M/0/1/0/all/0/1\">Mariame Tighanimine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_I/0/1/0/all/0/1\">Isabela Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucherenko_A/0/1/0/all/0/1\">Anastasiia Kucherenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Alexandre Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimberg_F/0/1/0/all/0/1\">Felix Grimberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitu_V/0/1/0/all/0/1\">Vlad Nitu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_C/0/1/0/all/0/1\">Chris Vossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1\">S&#xe9;bastien Rouault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1\">El-Mahdi El-Mhamdi</a>",
          "description": "Today's large-scale algorithms have become immensely influential, as they\nrecommend and moderate the content that billions of humans are exposed to on a\ndaily basis. They are the de-facto regulators of our societies' information\ndiet, from shaping opinions on public health to organizing groups for social\nmovements. This creates serious concerns, but also great opportunities to\npromote quality information. Addressing the concerns and seizing the\nopportunities is a challenging, enormous and fabulous endeavor, as intuitively\nappealing ideas often come with unwanted {\\it side effects}, and as it requires\nus to think about what we deeply prefer.\n\nUnderstanding how today's large-scale algorithms are built is critical to\ndetermine what interventions will be most effective. Given that these\nalgorithms rely heavily on {\\it machine learning}, we make the following key\nobservation: \\emph{any algorithm trained on uncontrolled data must not be\ntrusted}. Indeed, a malicious entity could take control over the data, poison\nit with dangerously manipulative fabricated inputs, and thereby make the\ntrained algorithm extremely unsafe. We thus argue that the first step towards\nsafe and ethical large-scale algorithms must be the collection of a large,\nsecure and trustworthy dataset of reliable human judgments.\n\nTo achieve this, we introduce \\emph{Tournesol}, an open source platform\navailable at \\url{https://tournesol.app}. Tournesol aims to collect a large\ndatabase of human judgments on what algorithms ought to widely recommend (and\nwhat they ought to stop widely recommending). We outline the structure of the\nTournesol database, the key features of the Tournesol platform and the main\nhurdles that must be overcome to make it a successful project. Most\nimportantly, we argue that, if successful, Tournesol may then serve as the\nessential foundation for any safe and ethical large-scale algorithm.",
          "link": "http://arxiv.org/abs/2107.07334",
          "publishedOn": "2021-07-16T00:48:25.252Z",
          "wordCount": 766,
          "title": "Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. (arXiv:2107.07334v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07322",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyu Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1\">Ruodu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1\">Aaditya Ramdas</a>",
          "description": "In bandit multiple hypothesis testing, each arm corresponds to a different\nnull hypothesis that we wish to test, and the goal is to design adaptive\nalgorithms that correctly identify large set of interesting arms (true\ndiscoveries), while only mistakenly identifying a few uninteresting ones (false\ndiscoveries). One common metric in non-bandit multiple testing is the false\ndiscovery rate (FDR). We propose a unified, modular framework for bandit FDR\ncontrol that emphasizes the decoupling of exploration and summarization of\nevidence. We utilize the powerful martingale-based concept of ``e-processes''\nto ensure FDR control for arbitrary composite nulls, exploration rules and\nstopping times in generic problem settings. In particular, valid FDR control\nholds even if the reward distributions of the arms could be dependent, multiple\narms may be queried simultaneously, and multiple (cooperating or competing)\nagents may be querying arms, covering combinatorial semi-bandit type settings\nas well. Prior work has considered in great detail the setting where each arm's\nreward distribution is independent and sub-Gaussian, and a single arm is\nqueried at each step. Our framework recovers matching sample complexity\nguarantees in this special case, and performs comparably or better in practice.\nFor other settings, sample complexities will depend on the finer details of the\nproblem (composite nulls being tested, exploration algorithm, data dependence\nstructure, stopping rule) and we do not explore these; our contribution is to\nshow that the FDR guarantee is clean and entirely agnostic to these details.",
          "link": "http://arxiv.org/abs/2107.07322",
          "publishedOn": "2021-07-16T00:48:25.245Z",
          "wordCount": 675,
          "title": "A unified framework for bandit multiple testing. (arXiv:2107.07322v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1\">Lennart Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1\">Florian Pfisterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "Neural architecture search (NAS) promises to make deep learning accessible to\nnon-experts by automating architecture engineering of deep neural networks.\nBANANAS is one state-of-the-art NAS method that is embedded within the Bayesian\noptimization framework. Recent experimental findings have demonstrated the\nstrong performance of BANANAS on the NAS-Bench-101 benchmark being determined\nby its path encoding and not its choice of surrogate model. We present\nexperimental results suggesting that the performance of BANANAS on the\nNAS-Bench-301 benchmark is determined by its acquisition function optimizer,\nwhich minimally mutates the incumbent.",
          "link": "http://arxiv.org/abs/2107.07343",
          "publishedOn": "2021-07-16T00:48:25.203Z",
          "wordCount": 535,
          "title": "Mutation is all you need. (arXiv:2107.07343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.",
          "link": "http://arxiv.org/abs/2107.07344",
          "publishedOn": "2021-07-16T00:48:25.194Z",
          "wordCount": 731,
          "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yufeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingsong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Deep neural networks (DNNs) have successfully learned useful data\nrepresentations in various tasks, however, assessing the reliability of these\nrepresentations remains a challenge. Deep Ensemble is widely considered the\nstate-of-the-art method for uncertainty estimation, but it is very expensive to\ntrain and test. MC-Dropout is another alternative method, which is less\nexpensive but lacks the diversity of predictions. To get more diverse\npredictions in less time, we introduce Randomized ReLU Activation (RRA)\nframework. Under the framework, we propose two strategies, MC-DropReLU and\nMC-RReLU, to estimate uncertainty. Instead of randomly dropping some neurons of\nthe network as in MC-Dropout, the RRA framework adds randomness to the\nactivation function module, making the outputs diverse. As far as we know, this\nis the first attempt to add randomness to the activation function module to\ngenerate predictive uncertainty. We analyze and compare the output diversity of\nMC-Dropout and our method from the variance perspective and obtain the\nrelationship between the hyperparameters and output diversity in the two\nmethods. Moreover, our method is simple to implement and does not need to\nmodify the existing model. We experimentally validate the RRA framework on\nthree widely used datasets, CIFAR10, CIFAR100, and TinyImageNet. The\nexperiments demonstrate that our method has competitive performance but is more\nfavorable in training time and memory requirements.",
          "link": "http://arxiv.org/abs/2107.07197",
          "publishedOn": "2021-07-16T00:48:25.172Z",
          "wordCount": 649,
          "title": "Randomized ReLU Activation for Uncertainty Estimation of Deep Neural Networks. (arXiv:2107.07197v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1\">Zona Kostic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1\">Aleksandar Jevremovic</a>",
          "description": "The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.",
          "link": "http://arxiv.org/abs/2107.07148",
          "publishedOn": "2021-07-16T00:48:25.154Z",
          "wordCount": 649,
          "title": "What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1\">Alex Egg</a>",
          "description": "We propose a method to easily modify existing offline Recommender Systems to\nrun online using Transfer Learning. Online Learning for Recommender Systems has\ntwo main advantages: quality and scale. Like many Machine Learning algorithms\nin production if not regularly retrained will suffer from Concept Drift. A\npolicy that is updated frequently online can adapt to drift faster than a batch\nsystem. This is especially true for user-interaction systems like recommenders\nwhere the underlying distribution can shift drastically to follow user\nbehaviour. As a platform grows rapidly like Grubhub, the cost of running batch\ntraining jobs becomes material. A shift from stateless batch learning offline\nto stateful incremental learning online can recover, for example, at Grubhub,\nup to a 45x cost savings and a +20% metrics increase. There are a few\nchallenges to overcome with the transition to online stateful learning, namely\nconvergence, non-stationary embeddings and off-policy evaluation, which we\nexplore from our experiences running this system in production.",
          "link": "http://arxiv.org/abs/2107.07106",
          "publishedOn": "2021-07-16T00:48:25.137Z",
          "wordCount": 583,
          "title": "Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruijuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_F/0/1/0/all/0/1\">Feng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xinlei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Federated learning enables a large number of clients to participate in\nlearning a shared model while maintaining the training data stored in each\nclient, which protects data privacy and security. Till now, federated learning\nframeworks are built in a centralized way, in which a central client is needed\nfor collecting and distributing information from every other client. This not\nonly leads to high communication pressure at the central client, but also\nrenders the central client highly vulnerable to failure and attack. Here we\npropose a principled decentralized federated learning algorithm (DeFed), which\nremoves the central client in the classical Federated Averaging (FedAvg)\nsetting and only relies information transmission between clients and their\nlocal neighbors. The proposed DeFed algorithm is proven to reach the global\nminimum with a convergence rate of $O(1/T)$ when the loss function is smooth\nand strongly convex, where $T$ is the number of iterations in gradient descent.\nFinally, the proposed algorithm has been applied to a number of toy examples to\ndemonstrate its effectiveness.",
          "link": "http://arxiv.org/abs/2107.07171",
          "publishedOn": "2021-07-16T00:48:25.121Z",
          "wordCount": 621,
          "title": "DeFed: A Principled Decentralized and Privacy-Preserving Federated Learning Algorithm. (arXiv:2107.07171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kevin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1\">Ashwin Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1\">Vitchyr Pong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aurick Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Justin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Exploration in reinforcement learning is a challenging problem: in the worst\ncase, the agent must search for reward states that could be hidden anywhere in\nthe state space. Can we define a more tractable class of RL problems, where the\nagent is provided with examples of successful outcomes? In this problem\nsetting, the reward function can be obtained automatically by training a\nclassifier to categorize states as successful or not. If trained properly, such\na classifier can not only afford a reward function, but actually provide a\nwell-shaped objective landscape that both promotes progress toward good states\nand provides a calibrated exploration bonus. In this work, we we show that an\nuncertainty aware classifier can solve challenging reinforcement learning\nproblems by both encouraging exploration and provided directed guidance towards\npositive outcomes. We propose a novel mechanism for obtaining these calibrated,\nuncertainty-aware classifiers based on an amortized technique for computing the\nnormalized maximum likelihood (NML) distribution, also showing how these\ntechniques can be made computationally tractable by leveraging tools from\nmeta-learning. We show that the resulting algorithm has a number of intriguing\nconnections to both count-based exploration methods and prior algorithms for\nlearning reward functions, while also providing more effective guidance towards\nthe goal. We demonstrate that our algorithm solves a number of challenging\nnavigation and robotic manipulation tasks which prove difficult or impossible\nfor prior methods.",
          "link": "http://arxiv.org/abs/2107.07184",
          "publishedOn": "2021-07-16T00:48:25.115Z",
          "wordCount": 675,
          "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning. (arXiv:2107.07184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>",
          "description": "In humans, perceptual awareness facilitates the fast recognition and\nextraction of information from sensory input. This awareness largely depends on\nhow the human agent interacts with the environment. In this work, we propose\nactive neural generative coding, a computational framework for learning\naction-driven generative models without backpropagation of errors (backprop) in\ndynamic environments. Specifically, we develop an intelligent agent that\noperates even with sparse rewards, drawing inspiration from the cognitive\ntheory of planning as inference. We demonstrate on several control problems, in\nthe online learning setting, that our proposed modeling framework performs\ncompetitively with deep Q-learning models. The robust performance of our agent\noffers promising evidence that a backprop-free approach for neural inference\nand learning can drive goal-directed behavior.",
          "link": "http://arxiv.org/abs/2107.07046",
          "publishedOn": "2021-07-16T00:48:25.095Z",
          "wordCount": 545,
          "title": "Backprop-Free Reinforcement Learning with Active Neural Generative Coding. (arXiv:2107.07046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1\">Dylan Robert Harp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_R/0/1/0/all/0/1\">Rajesh J. Pawar</a>",
          "description": "This paper contributes to the development and evaluation of a deep learning\nworkflow that accurately and efficiently predicts the temporal-spatial\nevolution of pressure and CO2 plumes during injection and post-injection\nperiods of geologic CO2 sequestration (GCS) operations. Based on a Fourier\nNeuron Operator, the deep learning workflow takes input variables or features\nincluding rock properties, well operational controls and time steps, and\npredicts the state variables of pressure and CO2 saturation. To further improve\nthe predictive fidelity, separate deep learning models are trained for CO2\ninjection and post-injection periods due the difference in primary driving\nforce of fluid flow and transport during these two phases. We also explore\ndifferent combinations of features to predict the state variables. We use a\nrealistic example of CO2 injection and storage in a 3D heterogeneous saline\naquifer, and apply the deep learning workflow that is trained from\nphysics-based simulation data and emulate the physics process. Through this\nnumerical experiment, we demonstrate that using two separate deep learning\nmodels to distinguish post-injection from injection period generates the most\naccurate prediction of pressure, and a single deep learning model of the whole\nGCS process including the cumulative injection volume of CO2 as a deep learning\nfeature, leads to the most accurate prediction of CO2 saturation. For the\npost-injection period, it is key to use cumulative CO2 injection volume to\ninform the deep learning models about the total carbon storage when predicting\neither pressure or saturation. The deep learning workflow not only provides\nhigh predictive fidelity across temporal and spatial scales, but also offers a\nspeedup of 250 times compared to full physics reservoir simulation, and thus\nwill be a significant predictive tool for engineers to manage the long term\nprocess of GCS.",
          "link": "http://arxiv.org/abs/2107.07274",
          "publishedOn": "2021-07-16T00:48:25.087Z",
          "wordCount": 747,
          "title": "A Robust Deep Learning Workflow to Predict Multiphase Flow Behavior during Geological CO2 Sequestration Injection and Post-Injection Periods. (arXiv:2107.07274v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>",
          "description": "We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.",
          "link": "http://arxiv.org/abs/2105.04727",
          "publishedOn": "2021-07-16T00:48:25.080Z",
          "wordCount": 627,
          "title": "Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1\">Sergio Rojas-Galeano</a>",
          "description": "One of the stratagems used to deceive spam filters is to substitute vocables\nwith synonyms or similar words that turn the message unrecognisable by the\ndetection algorithms. In this paper we investigate whether the recent\ndevelopment of language models sensitive to the semantics and context of words,\nsuch as Google's BERT, may be useful to overcome this adversarial attack\n(called \"Mad-lib\" as per the word substitution game). Using a dataset of 5572\nSMS spam messages, we first established a baseline of detection performance\nusing widely known document representation models (BoW and TFIDF) and the novel\nBERT model, coupled with a variety of classification algorithms (Decision Tree,\nkNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we\nbuilt a thesaurus of the vocabulary contained in these messages, and set up a\nMad-lib attack experiment in which we modified each message of a held out\nsubset of data (not used in the baseline experiment) with different rates of\nsubstitution of original words with synonyms from the thesaurus. Lastly, we\nevaluated the detection performance of the three representation models (BoW,\nTFIDF and BERT) coupled with the best classifier from the baseline experiment\n(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)\nin the original dataset, whereas the BERT model obtained 96%. On the other\nhand, the Mad-lib attack experiment showed that BERT encodings manage to\nmaintain a similar BA performance of 96% with an average substitution rate of\n1.82 words per message, and 95% with 3.34 words substituted per message. In\ncontrast, the BA performance of the BoW and TFIDF encoders dropped to chance.\nThese results hint at the potential advantage of BERT models to combat these\ntype of ingenious attacks, offsetting to some extent for the inappropriate use\nof semantic relationships in language.",
          "link": "http://arxiv.org/abs/2107.06400",
          "publishedOn": "2021-07-16T00:48:25.059Z",
          "wordCount": 736,
          "title": "Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verine_A/0/1/0/all/0/1\">Alexandre Verine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1\">Benjamin Negrevergne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1\">Fabrice Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1\">Yann Chevaleyre</a>",
          "description": "An invertible function is bi-Lipschitz if both the function and its inverse\nhave bounded Lipschitz constants. Nowadays, most Normalizing Flows are\nbi-Lipschitz by design or by training to limit numerical errors (among other\nthings). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing\nFlows and identify several target distributions that are difficult to\napproximate using such models. Then, we characterize the expressivity of\nbi-Lipschitz Normalizing Flows by giving several lower bounds on the Total\nVariation distance between these particularly unfavorable distributions and\ntheir best possible approximation. Finally, we discuss potential remedies which\ninclude using more complex latent distributions.",
          "link": "http://arxiv.org/abs/2107.07232",
          "publishedOn": "2021-07-16T00:48:25.051Z",
          "wordCount": 532,
          "title": "On the expressivity of bi-Lipschitz normalizing flows. (arXiv:2107.07232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1\">Elliot Creager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1\">J&#xf6;rn-Henrik Jacobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>",
          "description": "Learning models that gracefully handle distribution shifts is central to\nresearch on domain generalization, robust optimization, and fairness. A\npromising formulation is domain-invariant learning, which identifies the key\nissue of learning which features are domain-specific versus domain-invariant.\nAn important assumption in this area is that the training examples are\npartitioned into \"domains\" or \"environments\". Our focus is on the more common\nsetting where such partitions are not provided. We propose EIIL, a general\nframework for domain-invariant learning that incorporates Environment Inference\nto directly infer partitions that are maximally informative for downstream\nInvariant Learning. We show that EIIL outperforms invariant learning methods on\nthe CMNIST benchmark without using environment labels, and significantly\noutperforms ERM on worst-group performance in the Waterbirds and CivilComments\ndatasets. Finally, we establish connections between EIIL and algorithmic\nfairness, which enables EIIL to improve accuracy and calibration in a fair\nprediction problem.",
          "link": "http://arxiv.org/abs/2010.07249",
          "publishedOn": "2021-07-16T00:48:25.044Z",
          "wordCount": 626,
          "title": "Environment Inference for Invariant Learning. (arXiv:2010.07249v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1\">Charlotte Bunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1\">Laetitia Meng-Papaxanthos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>",
          "description": "Consider a heterogeneous population of points evolving with time. While the\npopulation evolves, both in size and nature, we can observe it periodically,\nthrough snapshots taken at different timestamps. Each of these snapshots is\nformed by sampling points from the population at that time, and then creating\nfeatures to recover point clouds. While these snapshots describe the\npopulation's evolution on aggregate, they do not provide directly insights on\nindividual trajectories. This scenario is encountered in several applications,\nnotably single-cell genomics experiments, tracking of particles, or when\nstudying crowd motion. In this paper, we propose to model that dynamic as\nresulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.\nThe JKO scheme posits that the configuration taken by a population at time $t$\nis one that trades off a decrease w.r.t. an energy (the model we seek to learn)\npenalized by an optimal transport distance w.r.t. the previous configuration.\nTo that end, we propose JKOnet, a neural architecture that combines an energy\nmodel on measures, with (small) optimal displacements solved with input convex\nneural networks (ICNN). We demonstrate the applicability of our model to\nexplain and predict population dynamics.",
          "link": "http://arxiv.org/abs/2106.06345",
          "publishedOn": "2021-07-16T00:48:25.020Z",
          "wordCount": 644,
          "title": "JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Yilin Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1\">Jennifer DeJong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1\">Tom Halverson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1\">David I Shuman</a>",
          "description": "Ranked data sets, where m judges/voters specify a preference ranking of n\nobjects/candidates, are increasingly prevalent in contexts such as political\nelections, computer vision, recommender systems, and bioinformatics. The vote\ncounts for each ranking can be viewed as an n! data vector lying on the\npermutahedron, which is a Cayley graph of the symmetric group with vertices\nlabeled by permutations and an edge when two permutations differ by an adjacent\ntransposition. Leveraging combinatorial representation theory and recent\nprogress in signal processing on graphs, we investigate a novel, scalable\ntransform method to interpret and exploit structure in ranked data. We\nrepresent data on the permutahedron using an overcomplete dictionary of atoms,\neach of which captures both smoothness information about the data (typically\nthe focus of spectral graph decomposition methods in graph signal processing)\nand structural information about the data (typically the focus of symmetry\ndecomposition methods from representation theory). These atoms have a more\nnaturally interpretable structure than any known basis for signals on the\npermutahedron, and they form a Parseval frame, ensuring beneficial numerical\nproperties such as energy preservation. We develop specialized algorithms and\nopen software that take advantage of the symmetry and structure of the\npermutahedron to improve the scalability of the proposed method, making it more\napplicable to the high-dimensional ranked data found in applications.",
          "link": "http://arxiv.org/abs/2103.04150",
          "publishedOn": "2021-07-16T00:48:25.014Z",
          "wordCount": 687,
          "title": "Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01708",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Stephen Y. Zhang</a>",
          "description": "Non-negative matrix and tensor factorisations are a classical tool for\nfinding low-dimensional representations of high-dimensional datasets. In\napplications such as imaging, datasets can be regarded as distributions\nsupported on a space with metric structure. In such a setting, a loss function\nbased on the Wasserstein distance of optimal transportation theory is a natural\nchoice since it incorporates the underlying geometry of the data. We introduce\na general mathematical framework for computing non-negative factorisations of\nboth matrices and tensors with respect to an optimal transport loss. We derive\nan efficient computational method for its solution using a convex dual\nformulation, and demonstrate the applicability of this approach with several\nnumerical illustrations with both matrix and tensor-valued data.",
          "link": "http://arxiv.org/abs/2104.01708",
          "publishedOn": "2021-07-16T00:48:24.998Z",
          "wordCount": 584,
          "title": "A unified framework for non-negative matrix and tensor factorisations with a smoothed Wasserstein loss. (arXiv:2104.01708v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1\">Lucas F. F. Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1\">Vitor C. A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1\">Regiane S. Kawasaki Franc&#xea;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1\">Ricardo B. C. Prud&#xea;ncio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1\">Ronnie C. O. Alves</a>",
          "description": "The classification experiments covered by machine learning (ML) are composed\nby two important parts: the data and the algorithm. As they are a fundamental\npart of the problem, both must be considered when evaluating a model's\nperformance against a benchmark. The best classifiers need robust benchmarks to\nbe properly evaluated. For this, gold standard benchmarks such as OpenML-CC18\nare used. However, data complexity is commonly not considered along with the\nmodel during a performance evaluation. Recent studies employ Item Response\nTheory (IRT) as a new approach to evaluating datasets and algorithms, capable\nof evaluating both simultaneously. This work presents a new evaluation\nmethodology based on IRT and Glicko-2, jointly with the decodIRT tool developed\nto guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate\nthe OpenML-CC18 benchmark for its algorithmic evaluation capability and checks\nif there is a subset of datasets more efficient than the original benchmark.\nSeveral classifiers, from classics to ensemble, are also evaluated using the\nIRT models. The Glicko-2 rating system was applied together with IRT to\nsummarize the innate ability and classifiers performance. It was noted that not\nall OpenML-CC18 datasets are really useful for evaluating algorithms, where\nonly 10% were rated as being really difficult. Furthermore, it was verified the\nexistence of a more efficient subset containing only 50% of the original size.\nWhile Randon Forest was singled out as the algorithm with the best innate\nability.",
          "link": "http://arxiv.org/abs/2107.07451",
          "publishedOn": "2021-07-16T00:48:24.981Z",
          "wordCount": 685,
          "title": "Data vs classifiers, who wins?. (arXiv:2107.07451v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.05383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-An Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Siang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Liang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>",
          "description": "We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.",
          "link": "http://arxiv.org/abs/2003.05383",
          "publishedOn": "2021-07-16T00:48:24.971Z",
          "wordCount": 699,
          "title": "xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1\">Guangmo Tong</a>",
          "description": "Real-world decision-making systems are often subject to uncertainties that\nhave to be resolved through observational data. Therefore, we are frequently\nconfronted with combinatorial optimization problems of which the objective\nfunction is unknown and thus has to be debunked using empirical evidence. In\ncontrast to the common practice that relies on a learning-and-optimization\nstrategy, we consider the regression between combinatorial spaces, aiming to\ninfer high-quality optimization solutions from samples of input-solution pairs\n-- without the need to learn the objective function. Our main deliverable is a\nuniversal solver that is able to handle abstract undetermined stochastic\ncombinatorial optimization problems. For learning foundations, we present\nlearning-error analysis under the PAC-Bayesian framework using a new\nmargin-based analysis. In empirical studies, we demonstrate our design using\nproof-of-concept experiments, and compare it with other methods that are\npotentially applicable. Overall, we obtain highly encouraging experimental\nresults for several classic combinatorial problems on both synthetic and\nreal-world datasets.",
          "link": "http://arxiv.org/abs/2107.07508",
          "publishedOn": "2021-07-16T00:48:24.957Z",
          "wordCount": 574,
          "title": "USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1\">Neeraj Chimmwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>",
          "description": "We present a CLSRIL-23, a self supervised learning based audio pre-trained\nmodel which learns cross lingual speech representations from raw audio across\n23 Indic languages. It is built on top of wav2vec 2.0 which is solved by\ntraining a contrastive task over masked latent speech representations and\njointly learns the quantization of latents shared across all languages. We\ncompare the language wise loss during pretraining to compare effects of\nmonolingual and multilingual pretraining. Performance on some downstream\nfine-tuning tasks for speech recognition is also compared and our experiments\nshow that multilingual pretraining outperforms monolingual training, in terms\nof learning speech representations which encodes phonetic similarity of\nlanguages and also in terms of performance on down stream tasks. A decrease of\n5% is observed in WER and 9.5% in CER when a multilingual pretrained model is\nused for finetuning in Hindi. All the code models are also open sourced.\nCLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio\ndata to facilitate research in speech recognition for Indic languages. We hope\nthat new state of the art systems will be created using the self supervised\napproach, especially for low resources Indic languages.",
          "link": "http://arxiv.org/abs/2107.07402",
          "publishedOn": "2021-07-16T00:48:24.904Z",
          "wordCount": 652,
          "title": "CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1\">Ladislav Ramp&#xe1;&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1\">Guy Wolf</a>",
          "description": "Graph neural networks (GNNs) based on message passing between neighboring\nnodes are known to be insufficient for capturing long-range interactions in\ngraphs. In this project we study hierarchical message passing models that\nleverage a multi-resolution representation of a given graph. This facilitates\nlearning of features that span large receptive fields without loss of local\ninformation, an aspect not studied in preceding work on hierarchical GNNs. We\nintroduce Hierarchical Graph Net (HGNet), which for any two connected nodes\nguarantees existence of message-passing paths of at most logarithmic length\nw.r.t. the input graph size. Yet, under mild assumptions, its internal\nhierarchy maintains asymptotic size equivalent to that of the input graph. We\nobserve that our HGNet outperforms conventional stacking of GCN layers\nparticularly in molecular property prediction benchmarks. Finally, we propose\ntwo benchmarking tasks designed to elucidate capability of GNNs to leverage\nlong-range interactions in graphs.",
          "link": "http://arxiv.org/abs/2107.07432",
          "publishedOn": "2021-07-16T00:48:24.898Z",
          "wordCount": 575,
          "title": "Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "The development of active and passive biometric authentication and\nidentification technology plays an increasingly important role in\ncybersecurity. Keystroke dynamics can be used to analyze the way that a user\ntypes based on various keyboard input. Previous work has shown that user\nauthentication and classification can be achieved based on keystroke dynamics.\nIn this research, we consider the problem of user classification based on\nkeystroke dynamics features collected from free-text. We implement and analyze\na novel a deep learning model that combines a convolutional neural network\n(CNN) and a gated recurrent unit (GRU). We optimize the resulting model and\nconsider several relevant related problems. Our model is competitive with the\nbest results obtained in previous comparable research.",
          "link": "http://arxiv.org/abs/2107.07409",
          "publishedOn": "2021-07-16T00:48:24.892Z",
          "wordCount": 542,
          "title": "Machine Learning-Based Analysis of Free-Text Keystroke Dynamics. (arXiv:2107.07409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_D/0/1/0/all/0/1\">Dhasarathy Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_A/0/1/0/all/0/1\">Anton Johansson</a>",
          "description": "Automotive software testing continues to rely largely upon expensive field\ntests to ensure quality because alternatives like simulation-based testing are\nrelatively immature. As a step towards lowering reliance on field tests, we\npresent SilGAN, a deep generative model that eases specification, stimulus\ngeneration, and automation of automotive software-in-the-loop testing. The\nmodel is trained using data recorded from vehicles in the field. Upon training,\nthe model uses a concise specification for a driving scenario to generate\nrealistic vehicle state transitions that can occur during such a scenario. Such\nauthentic emulation of internal vehicle behavior can be used for rapid,\nsystematic and inexpensive testing of vehicle control software. In addition, by\npresenting a targeted method for searching through the information learned by\nthe model, we show how a test objective like code coverage can be automated.\nThe data driven end-to-end testing pipeline that we present vastly expands the\nscope and credibility of automotive simulation-based testing. This reduces time\nto market while helping maintain required standards of quality.",
          "link": "http://arxiv.org/abs/2107.07364",
          "publishedOn": "2021-07-16T00:48:24.885Z",
          "wordCount": 613,
          "title": "SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing. (arXiv:2107.07364v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_B/0/1/0/all/0/1\">Bijan Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddharth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruck_J/0/1/0/all/0/1\">Jehoshua Bruck</a>",
          "description": "Consider multiple experts with overlapping expertise working on a\nclassification problem under uncertain input. What constitutes a consistent set\nof opinions? How can we predict the opinions of experts on missing sub-domains?\nIn this paper, we define a framework of to analyze this problem, termed \"expert\ngraphs.\" In an expert graph, vertices represent classes and edges represent\nbinary opinions on the topics of their vertices. We derive necessary conditions\nfor expert graph validity and use them to create \"synthetic experts\" which\ndescribe opinions consistent with the observed opinions of other experts. We\nshow this framework to be equivalent to the well-studied linear ordering\npolytope. We show our conditions are not sufficient for describing all expert\ngraphs on cliques, but are sufficient for cycles.",
          "link": "http://arxiv.org/abs/2107.07054",
          "publishedOn": "2021-07-16T00:48:24.879Z",
          "wordCount": 573,
          "title": "Expert Graphs: Synthesizing New Expertise via Collaboration. (arXiv:2107.07054v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07494",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1\">Tian Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ren_L/0/1/0/all/0/1\">Lihua Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karlsson_N/0/1/0/all/0/1\">Niklas Karlsson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Flores_A/0/1/0/all/0/1\">Aaron Flores</a>",
          "description": "For Verizon MediaDemand Side Platform(DSP), forecasting of ad campaign\nperformance not only feeds key information to the optimization server to allow\nthe system to operate on a high-performance mode, but also produces actionable\ninsights to the advertisers. In this paper, the forecasting problem for CPA\nlines in the middle of the flight is investigated by taking the bidding\nmechanism into account. The proposed methodology generates relationships\nbetween various key performance metrics and optimization signals. It can also\nbe used to estimate the sensitivity of ad campaign performance metrics to the\nadjustments of optimization signal, which is important to the design of a\ncampaign management system. The relationship between advertiser spends and\neffective Cost Per Action(eCPA) is also characterized, which serves as a\nguidance for mid-flight line adjustment to the advertisers. Several practical\nissues in implementation, such as downsampling of the dataset, are also\ndiscussed in the paper. At last, the forecasting results are validated against\nactual deliveries and demonstrates promising accuracy.",
          "link": "http://arxiv.org/abs/2107.07494",
          "publishedOn": "2021-07-16T00:48:24.872Z",
          "wordCount": 605,
          "title": "Mid-flight Forecasting for CPA Lines in Online Advertising. (arXiv:2107.07494v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1\">Peter W. Glynn</a>",
          "description": "We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime where the gaps between arm means scale as\n$1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson\nsampling evolve according to discrete versions of SDEs and random ODEs. As $n\n\\to \\infty$, we show that the dynamics converge weakly to solutions of the\ncorresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)\nindependently proposed this regime and developed similar SDE and random ODE\napproximations for Thompson sampling in the multi-armed bandit setting.) Our\nweak convergence theory, which covers both multi-armed and linear bandit\nsettings, is developed from first principles using the Continuous Mapping\nTheorem and can be directly adapted to analyze other sampling-based bandit\nalgorithms, for example, algorithms using the bootstrap for exploration. We\nalso establish an invariance principle for multi-armed bandits with gaps\nscaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms\ninvolving posterior approximation or the bootstrap, the weak diffusion limits\nare in general the same regardless of the specifics of the reward distributions\nor the choice of prior. In particular, as suggested by the classical\nBernstein-von Mises normal approximation for posterior distributions, the weak\ndiffusion limits generally coincide with the limit for normally-distributed\nrewards and priors.",
          "link": "http://arxiv.org/abs/2105.09232",
          "publishedOn": "2021-07-16T00:48:24.856Z",
          "wordCount": 671,
          "title": "Diffusion Approximations for Thompson Sampling. (arXiv:2105.09232v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "An innovations sequence of a time series is a sequence of independent and\nidentically distributed random variables with which the original time series\nhas a causal representation. The innovation at a time is statistically\nindependent of the history of the time series. As such, it represents the new\ninformation contained at present but not in the past. Because of its simple\nprobability structure, an innovations sequence is the most efficient signature\nof the original. Unlike the principle or independent component analysis\nrepresentations, an innovations sequence preserves not only the complete\nstatistical properties but also the temporal order of the original time series.\nAn long-standing open problem is to find a computationally tractable way to\nextract an innovations sequence of non-Gaussian processes. This paper presents\na deep learning approach, referred to as Innovations Autoencoder (IAE), that\nextracts innovations sequences using a causal convolutional neural network. An\napplication of IAE to the one-class anomalous sequence detection problem with\nunknown anomaly and anomaly-free models is also presented.",
          "link": "http://arxiv.org/abs/2106.12382",
          "publishedOn": "2021-07-16T00:48:24.848Z",
          "wordCount": 616,
          "title": "Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection. (arXiv:2106.12382v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.06992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1\">Ivan Garcia Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>",
          "description": "Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.",
          "link": "http://arxiv.org/abs/2107.06992",
          "publishedOn": "2021-07-16T00:48:24.841Z",
          "wordCount": 677,
          "title": "Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alarcon_Y/0/1/0/all/0/1\">Yonatan Carlos Carranza Alarc&#xf3;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Destercke_S/0/1/0/all/0/1\">S&#xe9;bastien Destercke</a>",
          "description": "We present two different strategies to extend the classical multi-label\nchaining approach to handle imprecise probability estimates. These estimates\nuse convex sets of distributions (or credal sets) in order to describe our\nuncertainty rather than a precise one. The main reasons one could have for\nusing such estimations are (1) to make cautious predictions (or no decision at\nall) when a high uncertainty is detected in the chaining and (2) to make better\nprecise predictions by avoiding biases caused in early decisions in the\nchaining. Through the use of the naive credal classifier, we propose efficient\nprocedures with theoretical justifications to solve both strategies. Our\nexperimental results on missing labels, which investigate how reliable these\npredictions are in both approaches, indicate that our approaches produce\nrelevant cautiousness on those hard-to-predict instances where the precise\nmodels fail.",
          "link": "http://arxiv.org/abs/2107.07443",
          "publishedOn": "2021-07-16T00:48:24.835Z",
          "wordCount": 566,
          "title": "Multi-label Chaining with Imprecise Probabilities. (arXiv:2107.07443v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1\">Sean Kulinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1\">David I. Inouye</a>",
          "description": "While previous distribution shift detection approaches can identify if a\nshift has occurred, these approaches cannot localize which specific features\nhave caused a distribution shift -- a critical step in diagnosing or fixing any\nunderlying issue. For example, in military sensor networks, users will want to\ndetect when one or more of the sensors has been compromised, and critically,\nthey will want to know which specific sensors might be compromised. Thus, we\nfirst define a formalization of this problem as multiple conditional\ndistribution hypothesis tests and propose both non-parametric and parametric\nstatistical tests. For both efficiency and flexibility, we then propose to use\na test statistic based on the density model score function (i.e. gradient with\nrespect to the input) -- which can easily compute test statistics for all\ndimensions in a single forward and backward pass. Any density model could be\nused for computing the necessary statistics including deep density models such\nas normalizing flows or autoregressive models. We additionally develop methods\nfor identifying when and where a shift occurs in multivariate time-series data\nand show results for multiple scenarios using realistic attack models on both\nsimulated and real world data.",
          "link": "http://arxiv.org/abs/2107.06929",
          "publishedOn": "2021-07-16T00:48:24.821Z",
          "wordCount": 645,
          "title": "Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests. (arXiv:2107.06929v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>",
          "description": "Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.",
          "link": "http://arxiv.org/abs/2107.07438",
          "publishedOn": "2021-07-16T00:48:24.807Z",
          "wordCount": 565,
          "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Dobrik Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1\">Dmitry Kazhdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>",
          "description": "Recent research on graph neural network (GNN) models successfully applied\nGNNs to classical graph algorithms and combinatorial optimisation problems.\nThis has numerous benefits, such as allowing applications of algorithms when\npreconditions are not satisfied, or reusing learned models when sufficient\ntraining data is not available or can't be generated. Unfortunately, a key\nhindrance of these approaches is their lack of explainability, since GNNs are\nblack-box models that cannot be interpreted directly. In this work, we address\nthis limitation by applying existing work on concept-based explanations to GNN\nmodels. We introduce concept-bottleneck GNNs, which rely on a modification to\nthe GNN readout mechanism. Using three case studies we demonstrate that: (i)\nour proposed model is capable of accurately learning concepts and extracting\npropositional formulas based on the learned concepts for each target class;\n(ii) our concept-based GNN models achieve comparative performance with\nstate-of-the-art models; (iii) we can derive global graph concepts, without\nexplicitly providing any supervision on graph-level concepts.",
          "link": "http://arxiv.org/abs/2107.07493",
          "publishedOn": "2021-07-16T00:48:24.799Z",
          "wordCount": 584,
          "title": "Algorithmic Concept-based Explainable Reasoning. (arXiv:2107.07493v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1\">Vijay Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1\">L. Elisa Celis</a>",
          "description": "Assessing the diversity of a dataset of information associated with people is\ncrucial before using such data for downstream applications. For a given\ndataset, this often involves computing the imbalance or disparity in the\nempirical marginal distribution of a protected attribute (e.g. gender, dialect,\netc.). However, real-world datasets, such as images from Google Search or\ncollections of Twitter posts, often do not have protected attributes labeled.\nConsequently, to derive disparity measures for such datasets, the elements need\nto hand-labeled or crowd-annotated, which are expensive processes.\n\nWe propose a cost-effective approach to approximate the disparity of a given\nunlabeled dataset, with respect to a protected attribute, using a control set\nof labeled representative examples. Our proposed algorithm uses the pairwise\nsimilarity between elements in the dataset and elements in the control set to\neffectively bootstrap an approximation to the disparity of the dataset.\nImportantly, we show that using a control set whose size is much smaller than\nthe size of the dataset is sufficient to achieve a small approximation error.\nFurther, based on our theoretical framework, we also provide an algorithm to\nconstruct adaptive control sets that achieve smaller approximation errors than\nrandomly chosen control sets. Simulations on two image datasets and one Twitter\ndataset demonstrate the efficacy of our approach (using random and adaptive\ncontrol sets) in auditing the diversity of a wide variety of datasets.",
          "link": "http://arxiv.org/abs/2107.07393",
          "publishedOn": "2021-07-16T00:48:24.792Z",
          "wordCount": 659,
          "title": "Auditing for Diversity using Representative Examples. (arXiv:2107.07393v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valdes_G/0/1/0/all/0/1\">Gilmer Valdes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelo_W/0/1/0/all/0/1\">Wilmer Arbelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Interian_Y/0/1/0/all/0/1\">Yannet Interian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1\">Jerome H. Friedman</a>",
          "description": "Many regression and classification procedures fit a parameterized function\n$f(x;w)$ of predictor variables $x$ to data $\\{x_{i},y_{i}\\}_1^N$ based on some\nloss criterion $L(y,f)$. Often, regularization is applied to improve accuracy\nby placing a constraint $P(w)\\leq t$ on the values of the parameters $w$.\nAlthough efficient methods exist for finding solutions to these constrained\noptimization problems for all values of $t\\geq0$ in the special case when $f$\nis a linear function, none are available when $f$ is non-linear (e.g. Neural\nNetworks). Here we present a fast algorithm that provides all such solutions\nfor any differentiable function $f$ and loss $L$, and any constraint $P$ that\nis an increasing monotone function of the absolute value of each parameter.\nApplications involving sparsity inducing regularization of arbitrary Neural\nNetworks are discussed. Empirical results indicate that these sparse solutions\nare usually superior to their dense counterparts in both accuracy and\ninterpretability. This improvement in accuracy can often make Neural Networks\ncompetitive with, and sometimes superior to, state-of-the-art methods in the\nanalysis of tabular data.",
          "link": "http://arxiv.org/abs/2107.07160",
          "publishedOn": "2021-07-16T00:48:24.785Z",
          "wordCount": 601,
          "title": "Lockout: Sparse Regularization of Neural Networks. (arXiv:2107.07160v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1\">Han shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip L.H. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Transformer-based pre-trained language models like BERT and its variants have\nrecently achieved promising performance in various natural language processing\n(NLP) tasks. However, the conventional paradigm constructs the backbone by\npurely stacking the manually designed global self-attention layers, introducing\ninductive bias and thus leading to sub-optimal. In this work, we propose an\nOperation-Priority Neural Architecture Search (OP-NAS) algorithm to\nautomatically search for promising hybrid backbone architectures. Our\nwell-designed search space (i) contains primitive math operations in the\nintra-layer level to explore novel attention structures, and (ii) leverages\nconvolution blocks to be the supplementary for attention structure in the\ninter-layer level to better learn local dependency. We optimize both the search\nalgorithm and evaluation of candidate models to boost the efficiency of our\nproposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution\nstrategy to facilitate model search via balancing exploration and exploitation.\nFurthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for\nfast model evaluation. Extensive experiments show that the searched\narchitecture (named AutoBERT-Zero) significantly outperforms BERT and its\nvariants of different model capacities in various downstream tasks, proving the\narchitecture's transfer and generalization abilities. Remarkably,\nAutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and\nBERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE\ntest set. Code and pre-trained models will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.07445",
          "publishedOn": "2021-07-16T00:48:24.767Z",
          "wordCount": 660,
          "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gohel_P/0/1/0/all/0/1\">Prashant Gohel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Priyanka Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1\">Manoranjan Mohanty</a>",
          "description": "Explainable Artificial Intelligence (XAI) is an emerging area of research in\nthe field of Artificial Intelligence (AI). XAI can explain how AI obtained a\nparticular solution (e.g., classification or object detection) and can also\nanswer other \"wh\" questions. This explainability is not possible in traditional\nAI. Explainability is essential for critical applications, such as defense,\nhealth care, law and order, and autonomous driving vehicles, etc, where the\nknow-how is required for trust and transparency. A number of XAI techniques so\nfar have been purposed for such applications. This paper provides an overview\nof these techniques from a multimedia (i.e., text, image, audio, and video)\npoint of view. The advantages and shortcomings of these techniques have been\ndiscussed, and pointers to some future directions have also been provided.",
          "link": "http://arxiv.org/abs/2107.07045",
          "publishedOn": "2021-07-16T00:48:24.755Z",
          "wordCount": 556,
          "title": "Explainable AI: current status and future directions. (arXiv:2107.07045v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07105",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Stokes_J/0/1/0/all/0/1\">James Stokes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+De_S/0/1/0/all/0/1\">Saibal De</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Veerapaneni_S/0/1/0/all/0/1\">Shravan Veerapaneni</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We initiate the study of neural-network quantum state algorithms for\nanalyzing continuous-variable lattice quantum systems in first quantization. A\nsimple family of continuous-variable trial wavefunctons is introduced which\nnaturally generalizes the restricted Boltzmann machine (RBM) wavefunction\nintroduced for analyzing quantum spin systems. By virtue of its simplicity, the\nsame variational Monte Carlo training algorithms that have been developed for\nground state determination and time evolution of spin systems have natural\nanalogues in the continuum. We offer a proof of principle demonstration in the\ncontext of ground state determination of a stoquastic quantum rotor\nHamiltonian. Results are compared against those obtained from partial\ndifferential equation (PDE) based scalable eigensolvers. This study serves as a\nbenchmark against which future investigation of continuous-variable neural\nquantum states can be compared, and points to the need to consider deep network\narchitectures and more sophisticated training algorithms.",
          "link": "http://arxiv.org/abs/2107.07105",
          "publishedOn": "2021-07-16T00:48:24.740Z",
          "wordCount": 591,
          "title": "Continuous-variable neural-network quantum states and the quantum rotor model. (arXiv:2107.07105v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>",
          "description": "Few-shot NLP research is highly active, yet conducted in disjoint research\nthreads with evaluation suites that lack challenging-yet-realistic testing\nsetups and fail to employ careful experimental design. Consequently, the\ncommunity does not know which techniques perform best or even if they\noutperform simple baselines. We formulate desiderata for an ideal few-shot NLP\nbenchmark and present FLEX, the first benchmark, public leaderboard, and\nframework that provides unified, comprehensive measurement for few-shot NLP\ntechniques. FLEX incorporates and introduces new best practices for few-shot\nevaluation, including measurement of four transfer settings, textual labels for\nzero-shot evaluation, and a principled approach to benchmark design that\noptimizes statistical accuracy while keeping evaluation costs accessible to\nresearchers without large compute resources. In addition, we present UniFew, a\nsimple yet strong prompt-based model for few-shot learning which unifies the\npretraining and finetuning prompt formats, eschewing complex machinery of\nrecent prompt-based approaches in adapting downstream task formats to language\nmodel pretraining objectives. We demonstrate that despite simplicity UniFew\nachieves results competitive with both popular meta-learning and prompt-based\napproaches.",
          "link": "http://arxiv.org/abs/2107.07170",
          "publishedOn": "2021-07-16T00:48:24.711Z",
          "wordCount": 619,
          "title": "FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1\">Andrew Joohun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a> (Stanford University)",
          "description": "Despite the groundbreaking successes of neural networks, contemporary models\nrequire extensive training with massive datasets and exhibit poor out-of-sample\ngeneralization. One proposed solution is to build systematicity and\ndomain-specific constraints into the model, echoing the tenets of classical,\nsymbolic cognitive architectures. In this paper, we consider the limitations of\nthis approach by examining human adults' ability to learn an abstract reasoning\ntask from a brief instructional tutorial and explanatory feedback for incorrect\nresponses, demonstrating that human learning dynamics and ability to generalize\noutside the range of the training examples differ drastically from those of a\nrepresentative neural network model, and that the model is brittle to changes\nin features not anticipated by its authors. We present further evidence from\nhuman data that the ability to consistently solve the puzzles was associated\nwith education, particularly basic mathematics education, and with the ability\nto provide a reliably identifiable, valid description of the strategy used. We\npropose that rapid learning and systematic generalization in humans may depend\non a gradual, experience-dependent process of learning-to-learn using\ninstructions and explanations to guide the construction of explicit abstract\nrules that support generalizable inferences.",
          "link": "http://arxiv.org/abs/2107.06994",
          "publishedOn": "2021-07-16T00:48:24.704Z",
          "wordCount": 644,
          "title": "What underlies rapid learning and systematic generalization in humans. (arXiv:2107.06994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07049",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Keshavamurthy_B/0/1/0/all/0/1\">Bharath Keshavamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicolo Michelusi</a>",
          "description": "A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is\nproposed, wherein a cognitive radio (CR) learns a time-frequency correlation\nmodel underlying spectrum occupancy of licensed users (LUs) in a radio\necosystem; concurrently, it devises an approximately optimal spectrum sensing\nand access policy under sensing constraints. A Baum-Welch algorithm is proposed\nto learn a parametric Markov transition model of LU spectrum occupancy based on\nnoisy spectrum measurements. Spectrum sensing and access are cast as a\nPartially-Observable Markov Decision Process, approximately optimized via\nrandomized point-based value iteration. Fragmentation, Hamming-distance state\nfilters and Monte-Carlo methods are proposed to alleviate the inherent\ncomputational complexity, and a weighted reward metric to regulate the\ntrade-off between CR throughput and LU interference. Numerical evaluations\ndemonstrate that LESSA performs within 5 percent of a genie-aided upper bound\nwith foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art\nalgorithms across the entire trade-off region: 71 percent over\ncorrelation-based clustering, 26 percent over Neyman-Pearson detection, 6\npercent over the Viterbi algorithm, and 9 percent over an adaptive Deep\nQ-Network. LESSA is then extended to a distributed Multi-Agent setting\n(MA-LESSA), by proposing novel neighbor discovery and channel access rank\nallocation. MA-LESSA improves CR throughput by 43 percent over cooperative\nTD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over\nnon-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is\nimplemented on the DARPA SC2 platform, manifesting superior performance over\ncompetitors in a real-world TDWR-UNII WLAN emulation; its implementation\nfeasibility is further validated on a testbed of ESP32 radios, exhibiting 96\npercent success probability.",
          "link": "http://arxiv.org/abs/2107.07049",
          "publishedOn": "2021-07-16T00:48:24.684Z",
          "wordCount": 712,
          "title": "Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs. (arXiv:2107.07049v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07098",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dowling_M/0/1/0/all/0/1\">Matthew Dowling</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sokol_P/0/1/0/all/0/1\">Piotr Sok&#xf3;&#x142;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>",
          "description": "We present the class of Hida-Mat\\'ern kernels, which is the canonical family\nof covariance functions over the entire space of stationary Gauss-Markov\nProcesses. It extends upon Mat\\'ern kernels, by allowing for flexible\nconstruction of priors over processes with oscillatory components. Any\nstationary kernel, including the widely used squared-exponential and spectral\nmixture kernels, are either directly within this class or are appropriate\nasymptotic limits, demonstrating the generality of this class. Taking advantage\nof its Markovian nature we show how to represent such processes as state space\nmodels using only the kernel and its derivatives. In turn this allows us to\nperform Gaussian Process inference more efficiently and side step the usual\ncomputational burdens. We also show how exploiting special properties of the\nstate space representation enables improved numerical stability in addition to\nfurther reductions of computational complexity.",
          "link": "http://arxiv.org/abs/2107.07098",
          "publishedOn": "2021-07-16T00:48:24.677Z",
          "wordCount": 556,
          "title": "Hida-Mat\\'ern Kernel. (arXiv:2107.07098v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kyeongbo Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1\">Youngchul Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Young-Rae Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Woo-Jin Song</a>",
          "description": "Because deep learning is vulnerable to noisy labels, sample selection\ntechniques, which train networks with only clean labeled data, have attracted a\ngreat attention. However, if the labels are dominantly corrupted by few\nclasses, these noisy samples are called dominant-noisy-labeled samples, the\nnetwork also learns dominant-noisy-labeled samples rapidly via content-aware\noptimization. In this study, we propose a compelling criteria to penalize\ndominant-noisy-labeled samples intensively through class-wise penalty labels.\nBy averaging prediction confidences for the each observed label, we obtain\nsuitable penalty labels that have high values if the labels are largely\ncorrupted by some classes. Experiments were performed using benchmarks\n(CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N,\nClothing1M) to evaluate the proposed criteria in various scenarios with\ndifferent noise rates. Using the proposed sample selection, the learning\nprocess of the network becomes significantly robust to noisy labels compared to\nexisting methods in several noise types.",
          "link": "http://arxiv.org/abs/2107.07041",
          "publishedOn": "2021-07-16T00:48:24.619Z",
          "wordCount": 600,
          "title": "Mitigating Memorization in Sample Selection for Learning with Noisy Labels. (arXiv:2107.07041v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fojtik_M/0/1/0/all/0/1\">Matthew Fojtik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>",
          "description": "High quality standard cell layout automation in advanced technology nodes is\nstill challenging in the industry today because of complex design rules. In\nthis paper we introduce an automatic standard cell layout generator called\nNVCell that can generate layouts with equal or smaller area for over 90% of\nsingle row cells in an industry standard cell library on an advanced technology\nnode. NVCell leverages reinforcement learning (RL) to fix design rule\nviolations during routing and to generate efficient placements.",
          "link": "http://arxiv.org/abs/2107.07044",
          "publishedOn": "2021-07-16T00:48:24.611Z",
          "wordCount": 513,
          "title": "NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning. (arXiv:2107.07044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07087",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Finkelstein_N/0/1/0/all/0/1\">Noam Finkelstein</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zjawin_B/0/1/0/all/0/1\">Beata Zjawin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wolfe_E/0/1/0/all/0/1\">Elie Wolfe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1\">Ilya Shpitser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spekkens_R/0/1/0/all/0/1\">Robert W. Spekkens</a>",
          "description": "Directed acyclic graphs (DAGs) with hidden variables are often used to\ncharacterize causal relations between variables in a system. When some\nvariables are unobserved, DAGs imply a notoriously complicated set of\nconstraints on the distribution of observed variables. In this work, we present\nentropic inequality constraints that are implied by $e$-separation relations in\nhidden variable DAGs with discrete observed variables. The constraints can\nintuitively be understood to follow from the fact that the capacity of\nvariables along a causal pathway to convey information is restricted by their\nentropy; e.g. at the extreme case, a variable with entropy $0$ can convey no\ninformation. We show how these constraints can be used to learn about the true\ncausal model from an observed data distribution. In addition, we propose a\nmeasure of causal influence called the minimal mediary entropy, and demonstrate\nthat it can augment traditional measures such as the average causal effect.",
          "link": "http://arxiv.org/abs/2107.07087",
          "publishedOn": "2021-07-16T00:48:24.603Z",
          "wordCount": 636,
          "title": "Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables. (arXiv:2107.07087v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael Ng</a>",
          "description": "Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.",
          "link": "http://arxiv.org/abs/2107.07058",
          "publishedOn": "2021-07-16T00:48:24.585Z",
          "wordCount": 707,
          "title": "A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1\">Laihyuk Park</a>",
          "description": "Video streaming services strive to support high-quality videos at higher\nresolutions and frame rates to improve the quality of experience (QoE).\nHowever, high-quality videos consume considerable amounts of energy on mobile\ndevices. This paper proposes NeuSaver, which reduces the power consumption of\nmobile devices when streaming videos by applying an adaptive frame rate to each\nvideo chunk without compromising user experience. NeuSaver generates an optimal\npolicy that determines the appropriate frame rate for each video chunk using\nreinforcement learning (RL). The RL model automatically learns the policy that\nmaximizes the QoE goals based on previous observations. NeuSaver also uses an\nasynchronous advantage actor-critic algorithm to reinforce the RL model quickly\nand robustly. Streaming servers that support NeuSaver preprocesses videos into\nsegments with various frame rates, which is similar to the process of creating\nvideos with multiple bit rates in dynamic adaptive streaming over HTTP.\nNeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in\nvarious experiments and a user study through four video categories along with\nthe state-of-the-art model. Our experiments showed that NeuSaver effectively\nreduces the power consumption of mobile devices when streaming video by an\naverage of 16.14% and up to 23.12% while achieving high QoE.",
          "link": "http://arxiv.org/abs/2107.07127",
          "publishedOn": "2021-07-16T00:48:24.579Z",
          "wordCount": 672,
          "title": "NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1\">Mohammad Khairul Bashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1\">Nikhil Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijaykrishnan Narayanan</a>",
          "description": "CNF-based SAT and MaxSAT solvers are central to logic synthesis and\nverification systems. The increasing popularity of these constraint problems in\nelectronic design automation encourages studies on different SAT problems and\ntheir properties for further computational efficiency. There has been both\ntheoretical and practical success of modern Conflict-driven clause learning SAT\nsolvers, which allows solving very large industrial instances in a relatively\nshort amount of time. Recently, machine learning approaches provide a new\ndimension to solving this challenging problem. Neural symbolic models could\nserve as generic solvers that can be specialized for specific domains based on\ndata without any changes to the structure of the model. In this work, we\npropose a one-shot model derived from the Transformer architecture to solve the\nMaxSAT problem, which is the optimization version of SAT where the goal is to\nsatisfy the maximum number of clauses. Our model has a scale-free structure\nwhich could process varying size of instances. We use meta-path and\nself-attention mechanism to capture interactions among homogeneous nodes. We\nadopt cross-attention mechanisms on the bipartite graph to capture interactions\namong heterogeneous nodes. We further apply an iterative algorithm to our model\nto satisfy additional clauses, enabling a solution approaching that of an\nexact-SAT problem. The attention mechanisms leverage the parallelism for\nspeedup. Our evaluation indicates improved speedup compared to heuristic\napproaches and improved completion rate compared to machine learning\napproaches.",
          "link": "http://arxiv.org/abs/2107.07116",
          "publishedOn": "2021-07-16T00:48:24.572Z",
          "wordCount": 680,
          "title": "Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis. (arXiv:2107.07116v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Bayesian neural networks provide a direct and natural way to extend standard\ndeep neural networks to support probabilistic deep learning through the use of\nprobabilistic layers that, traditionally, encode weight (and bias) uncertainty.\nIn particular, hybrid Bayesian neural networks utilize standard deterministic\nlayers together with few probabilistic layers judicially positioned in the\nnetworks for uncertainty estimation. A major aspect and benefit of Bayesian\ninference is that priors, in principle, provide the means to encode prior\nknowledge for use in inference and prediction. However, it is difficult to\nspecify priors on weights since the weights have no intuitive interpretation.\nFurther, the relationships of priors on weights to the functions computed by\nnetworks are difficult to characterize. In contrast, functions are intuitive to\ninterpret and are direct since they map inputs to outputs. Therefore, it is\nnatural to specify priors on functions to encode prior knowledge, and to use\nthem in inference and prediction based on functions. To support this, we\npropose hybrid Bayesian neural networks with functional probabilistic layers\nthat encode function (and activation) uncertainty. We discuss their foundations\nin functional Bayesian inference, functional variational inference, sparse\nGaussian processes, and sparse variational Gaussian processes. We further\nperform few proof-of-concept experiments using GPflus, a new library that\nprovides Gaussian process layers and supports their use with deterministic\nKeras layers to form hybrid neural network and Gaussian process models.",
          "link": "http://arxiv.org/abs/2107.07014",
          "publishedOn": "2021-07-16T00:48:24.565Z",
          "wordCount": 656,
          "title": "Hybrid Bayesian Neural Networks with Functional Probabilistic Layers. (arXiv:2107.07014v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1\">Muhammed Sit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiray_B/0/1/0/all/0/1\">Bekir Demiray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1\">Ibrahim Demir</a>",
          "description": "The frequency and impact of floods are expected to increase due to climate\nchange. It is crucial to predict streamflow, consequently flooding, in order to\nprepare and mitigate its consequences in terms of property damage and\nfatalities. This paper presents a Graph Convolutional GRUs based model to\npredict the next 36 hours of streamflow for a sensor location using the\nupstream river network. As shown in experiment results, the model presented in\nthis study provides better performance than the persistence baseline and a\nConvolutional Bidirectional GRU network for the selected study area in\nshort-term streamflow prediction.",
          "link": "http://arxiv.org/abs/2107.07039",
          "publishedOn": "2021-07-16T00:48:24.552Z",
          "wordCount": 546,
          "title": "Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks. (arXiv:2107.07039v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>",
          "description": "Vehicle routing problem (VRP) is a typical discrete combinatorial\noptimization problem, and many models and algorithms have been proposed to\nsolve VRP and variants. Although existing approaches has contributed a lot to\nthe development of this field, these approaches either are limited in problem\nsize or need manual intervening in choosing parameters. To tackle these\ndifficulties, many studies consider learning-based optimization algorithms to\nsolve VRP. This paper reviews recent advances in this field and divides\nrelevant approaches into end-to-end approaches and step-by-step approaches. We\ndesign three part experiments to justly evaluate performance of four\nrepresentative learning-based optimization algorithms and conclude that\ncombining heuristic search can effectively improve learning ability and sampled\nefficiency of LBO models. Finally we point out that research trend of LBO\nalgorithms is to solve large-scale and multiple constraints problems from real\nworld.",
          "link": "http://arxiv.org/abs/2107.07076",
          "publishedOn": "2021-07-16T00:48:24.544Z",
          "wordCount": 583,
          "title": "An Overview and Experimental Study of Learning-based Optimization Algorithms for Vehicle Routing Problem. (arXiv:2107.07076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ishibashi_H/0/1/0/all/0/1\">Hideaki Ishibashi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "This paper proposes an extension of principal component analysis for Gaussian\nprocess posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional\nspace of GP posteriors, it can be used for meta-learning, which is a framework\nfor improving the precision of a new task by estimating a structure of a set of\ntasks. The issue is how to define a structure of a set of GPs with an\ninfinite-dimensional parameter, such as coordinate system and a divergence. In\nthis study, we reduce the infiniteness of GP to the finite-dimensional case\nunder the information geometrical framework by considering a space of GP\nposteriors that has the same prior. In addition, we propose an approximation\nmethod of GP-PCA based on variational inference and demonstrate the\neffectiveness of GP-PCA as meta-learning through experiments.",
          "link": "http://arxiv.org/abs/2107.07115",
          "publishedOn": "2021-07-16T00:48:24.525Z",
          "wordCount": 558,
          "title": "Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ayush Manish Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1\">Atharva Tendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1\">Harshvardhan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>",
          "description": "Interpreting the learning dynamics of neural networks can provide useful\ninsights into how networks learn and the development of better training and\ndesign approaches. We present an approach to interpret learning in neural\nnetworks by measuring relative weight change on a per layer basis and\ndynamically aggregating emerging trends through combination of dimensionality\nreduction and clustering which allows us to scale to very deep networks. We use\nthis approach to investigate learning in the context of vision tasks across a\nvariety of state-of-the-art networks and provide insights into the learning\nbehavior of these networks, including how task complexity affects layer-wise\nlearning in deeper layers of networks.",
          "link": "http://arxiv.org/abs/2107.07005",
          "publishedOn": "2021-07-16T00:48:24.513Z",
          "wordCount": 550,
          "title": "WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1\">Vincenzo Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1\">Kaveh Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1\">Bryony DuPont</a>",
          "description": "Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.",
          "link": "http://arxiv.org/abs/2107.07042",
          "publishedOn": "2021-07-16T00:48:24.472Z",
          "wordCount": 660,
          "title": "Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey A. Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1\">Fernando Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>",
          "description": "The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.07002",
          "publishedOn": "2021-07-16T00:48:24.464Z",
          "wordCount": 600,
          "title": "The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hugo Flores Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1\">Aldo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1\">Ethan Manilow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1\">Bryan Pardo</a>",
          "description": "Deep learning work on musical instrument recognition has generally focused on\ninstrument classes for which we have abundant data. In this work, we exploit\nhierarchical relationships between instruments in a few-shot learning setup to\nenable classification of a wider set of musical instruments, given a few\nexamples at inference. We apply a hierarchical loss function to the training of\nprototypical networks, combined with a method to aggregate prototypes\nhierarchically, mirroring the structure of a predefined musical instrument\nhierarchy. These extensions require no changes to the network architecture and\nnew levels can be easily added or removed. Compared to a non-hierarchical\nfew-shot baseline, our method leads to a significant increase in classification\naccuracy and significant decrease mistake severity on instrument classes unseen\nin training.",
          "link": "http://arxiv.org/abs/2107.07029",
          "publishedOn": "2021-07-16T00:48:24.445Z",
          "wordCount": 563,
          "title": "Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Piqueras_M/0/1/0/all/0/1\">Manuel Garcia-Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1\">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>",
          "description": "Recent research in machine teaching has explored the instruction of any\nconcept expressed in a universal language. In this compositional context, new\nexperimental results have shown that there exist data teaching sets\nsurprisingly shorter than the concept description itself. However, there exists\na bound for those remarkable experimental findings through teaching size and\nconcept complexity that we further explore here. As concepts are rarely taught\nin isolation we investigate the best configuration of concepts to teach a given\nset of concepts, where those that have been acquired first can be reused for\nthe description of new ones. This new notion of conditional teaching size\nuncovers new insights, such as the interposition phenomenon: certain prior\nknowledge generates simpler compatible concepts that increase the teaching size\nof the concept that we want to teach. This does not happen for conditional\nKolmogorov complexity. Furthermore, we provide an algorithm that constructs\noptimal curricula based on interposition avoidance. This paper presents a\nseries of theoretical results, including their proofs, and some directions for\nfuture work. New research possibilities in curriculum teaching in compositional\nscenarios are now wide open to exploration.",
          "link": "http://arxiv.org/abs/2107.07038",
          "publishedOn": "2021-07-16T00:48:24.440Z",
          "wordCount": 618,
          "title": "Conditional Teaching Size. (arXiv:2107.07038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shabani_M/0/1/0/all/0/1\">Mostafa Shabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Financial market analysis, especially the prediction of movements of stock\nprices, is a challenging problem. The nature of financial time-series data,\nbeing non-stationary and nonlinear, is the main cause of these challenges. Deep\nlearning models have led to significant performance improvements in many\nproblems coming from different domains, including prediction problems of\nfinancial time-series data. Although the prediction performance is the main\ngoal of such models, dealing with ultra high-frequency data sets restrictions\nin terms of the number of model parameters and its inference speed. The\nTemporal Attention-Augmented Bilinear network was recently proposed as an\nefficient and high-performing model for Limit Order Book time-series\nforecasting. In this paper, we propose a low-rank tensor approximation of the\nmodel to further reduce the number of trainable parameters and increase its\nspeed.",
          "link": "http://arxiv.org/abs/2107.06995",
          "publishedOn": "2021-07-16T00:48:24.433Z",
          "wordCount": 558,
          "title": "Low-Rank Temporal Attention-Augmented Bilinear Network for financial time-series forecasting. (arXiv:2107.06995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "In this research, we consider the problem of verifying user identity based on\nkeystroke dynamics obtained from free-text. We employ a novel feature\nengineering method that generates image-like transition matrices. For this\nimage-like feature, a convolution neural network (CNN) with cutout achieves the\nbest results. A hybrid model consisting of a CNN and a recurrent neural network\n(RNN) is also shown to outperform previous research in this field.",
          "link": "http://arxiv.org/abs/2107.07009",
          "publishedOn": "2021-07-16T00:48:24.419Z",
          "wordCount": 491,
          "title": "Free-Text Keystroke Dynamics for User Authentication. (arXiv:2107.07009v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elaine Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1\">Lindsay C. Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1\">Richard Correnti</a>",
          "description": "Automated writing evaluation systems can improve students' writing insofar as\nstudents attend to the feedback provided and revise their essay drafts in ways\naligned with such feedback. Existing research on revision of argumentative\nwriting in such systems, however, has focused on the types of revisions\nstudents make (e.g., surface vs. content) rather than the extent to which\nrevisions actually respond to the feedback provided and improve the essay. We\nintroduce an annotation scheme to capture the nature of sentence-level\nrevisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-\nand 6th-grade students' argumentative essays. We show that reliable manual\nannotation can be achieved and that revision annotations correlate with a\nholistic assessment of essay improvement in line with the feedback provided.\nFurthermore, we explore the feasibility of automatically classifying revisions\naccording to our scheme.",
          "link": "http://arxiv.org/abs/2107.06990",
          "publishedOn": "2021-07-16T00:48:24.413Z",
          "wordCount": 605,
          "title": "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1\">Gauri Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1\">H. Brendan McMahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1\">Galen Andrew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Katharine Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1\">Deepesh Data</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichner_H/0/1/0/all/0/1\">Hubert Eichner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadhikar_A/0/1/0/all/0/1\">Advait Gadhikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1\">Zachary Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1\">Antonious M. Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanzely_F/0/1/0/all/0/1\">Filip Hanzely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hard_A/0/1/0/all/0/1\">Andrew Hard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1\">Samuel Horvath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingerman_A/0/1/0/all/0/1\">Alex Ingerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1\">Satyen Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Konecny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1\">Sashank J. Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richtarik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weikang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shanshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Honglin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chunxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, et al. (1 additional author not shown)",
          "description": "Federated learning and analytics are a distributed approach for\ncollaboratively learning models (or statistics) from decentralized data,\nmotivated by and designed for privacy protection. The distributed learning\nprocess can be formulated as solving federated optimization problems, which\nemphasize communication efficiency, data heterogeneity, compatibility with\nprivacy and system requirements, and other constraints that are not primary\nconsiderations in other problem settings. This paper provides recommendations\nand guidelines on formulating, designing, evaluating and analyzing federated\noptimization algorithms through concrete examples and practical implementation,\nwith a focus on conducting effective simulations to infer real-world\nperformance. The goal of this work is not to survey the current literature, but\nto inspire researchers and practitioners to design federated learning\nalgorithms that can be used in various practical applications.",
          "link": "http://arxiv.org/abs/2107.06917",
          "publishedOn": "2021-07-16T00:48:24.404Z",
          "wordCount": 656,
          "title": "A Field Guide to Federated Optimization. (arXiv:2107.06917v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07064",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1\">Dae-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sung-Jin Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "Brain-computer interface (BCI) is one of the tools which enables the\ncommunication between humans and devices by reflecting intention and status of\nhumans. With the development of artificial intelligence, the interest in\ncommunication between humans and drones using electroencephalogram (EEG) is\nincreased. Especially, in the case of controlling drone swarms such as\ndirection or formation, there are many advantages compared with controlling a\ndrone unit. Imagined speech is one of the endogenous BCI paradigms, which can\nidentify intentions of users. When conducting imagined speech, the users\nimagine the pronunciation as if actually speaking. In contrast, overt speech is\na task in which the users directly pronounce the words. When controlling drone\nswarms using imagined speech, complex commands can be delivered more\nintuitively, but decoding performance is lower than that of other endogenous\nBCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of\novert speech for imagined speech-based EEG signals classification. To the best\nof our knowledge, this study is the first attempt to use EEG features of overt\nspeech to decode imagined speech-based EEG signals with an autoencoder. A total\nof eight subjects participated in the experiment. When classifying four words,\nthe average accuracy of the DAL was 48.41%. In addition, when comparing the\nperformance between w/o and w/ EEG features of overt speech, there was a\nperformance improvement of 7.42% when including EEG features of overt speech.\nHence, we demonstrated that EEG features of overt speech could improve the\ndecoding performance of imagined speech.",
          "link": "http://arxiv.org/abs/2107.07064",
          "publishedOn": "2021-07-16T00:48:24.396Z",
          "wordCount": 715,
          "title": "DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder. (arXiv:2107.07064v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1\">Jackson Farley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1\">Andreas Gerstlauer</a>",
          "description": "A rising research challenge is running costly machine learning (ML) networks\nlocally on resource-constrained edge devices. ML networks with large\nconvolutional layers can easily exceed available memory, increasing latency due\nto excessive swapping. Previous memory reduction techniques such as pruning and\nquantization reduce model accuracy and often require retraining. Alternatively,\ndistributed methods partition the convolutions into equivalent smaller\nsub-computations, but the implementations introduce communication costs and\nrequire a network of devices. However, a distributed partitioning approach can\nalso be used to run in a reduced memory footprint on a single device by\nsubdividing the network into smaller operations.\n\nThis report extends prior work on distributed partitioning using tiling and\nfusing of convolutional layers into a memory-aware execution on a single\ndevice. Our approach extends prior fusing strategies to allow for two groups of\nconvolutional layers that are fused and tiled independently. This approach\nreduces overhead via data reuse, and reduces the memory footprint further. We\nalso propose a memory usage predictor coupled with a search algorithm to\nprovide fusing and tiling configurations for an arbitrary set of convolutional\nlayers. When applied to the YOLOv2 object detection network, results show that\nour approach can run in less than half the memory, and with a speedup of up to\n2.78 under severe memory constraints. Additionally, our algorithm will return a\nconfiguration with a latency that is within 6% of the best latency measured in\na manual search.",
          "link": "http://arxiv.org/abs/2107.06960",
          "publishedOn": "2021-07-16T00:48:24.390Z",
          "wordCount": 676,
          "title": "Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jingyang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>",
          "description": "Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples,\nthe detection of which is crucial for the wide application of these DNN models.\nRecently, a number of deep testing methods in software engineering were\nproposed to find the vulnerability of DNN systems, and one of them, i.e., Model\nMutation Testing (MMT), was used to successfully detect various adversarial\nsamples generated by different kinds of adversarial attacks. However, the\nmutated models in MMT are always huge in number (e.g., over 100 models) and\nlack diversity (e.g., can be easily circumvented by high-confidence adversarial\nsamples), which makes it less efficient in real applications and less effective\nin detecting high-confidence adversarial samples. In this study, we propose\nGraph-Guided Testing (GGT) for adversarial sample detection to overcome these\naforementioned challenges. GGT generates pruned models with the guide of graph\ncharacteristics, each of them has only about 5% parameters of the mutated model\nin MMT, and graph guided models have higher diversity. The experiments on\nCIFAR10 and SVHN validate that GGT performs much better than MMT with respect\nto both effectiveness and efficiency.",
          "link": "http://arxiv.org/abs/2107.07043",
          "publishedOn": "2021-07-16T00:48:24.382Z",
          "wordCount": 635,
          "title": "GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (arXiv:2107.07043v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheng Yan</a>",
          "description": "The troposphere is one of the atmospheric layers where most weather phenomena\noccur. Temperature variations in the troposphere, especially at 500 hPa, a\ntypical level of the middle troposphere, are significant indicators of future\nweather changes. Numerical weather prediction is effective for temperature\nprediction, but its computational complexity hinders a timely response. This\npaper proposes a novel temperature prediction approach in framework\nofphysics-informed deep learning. The new model, called PGnet, builds upon a\ngenerative neural network with a mask matrix. The mask is designed to\ndistinguish the low-quality predicted regions generated by the first physical\nstage. The generative neural network takes the mask as prior for the\nsecond-stage refined predictions. A mask-loss and a jump pattern strategy are\ndeveloped to train the generative neural network without accumulating errors\nduring making time-series predictions. Experiments on ERA5 demonstrate that\nPGnet can generate more refined temperature predictions than the\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2107.06991",
          "publishedOn": "2021-07-16T00:48:24.377Z",
          "wordCount": 590,
          "title": "Physics-informed generative neural network: an application to troposphere temperature prediction. (arXiv:2107.06991v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1\">Carlos Pinz&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1\">Catuscia Palamidessi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_F/0/1/0/all/0/1\">Frank Valencia</a>",
          "description": "One of the main concerns about fairness in machine learning (ML) is that, in\norder to achieve it, one may have to renounce to some accuracy. Having this\ntrade-off in mind, Hardt et al. have proposed the notion of equal opportunities\n(EO), designed so as to be compatible with accuracy. In fact, it can be shown\nthat if the source of input data is deterministic, the two notions go well\nalong with each other. In the probabilistic case, however, things change.\n\nAs we show, there are probabilistic data sources for which EO can only be\nachieved at the total detriment of accuracy, i.e. among the models that achieve\nEO, those whose prediction does not depend on the input have the highest\naccuracy.",
          "link": "http://arxiv.org/abs/2107.06944",
          "publishedOn": "2021-07-16T00:48:24.361Z",
          "wordCount": 561,
          "title": "On the impossibility of non-trivial accuracy under fairness constraints. (arXiv:2107.06944v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "We introduce HTLM, a hyper-text language model trained on a large-scale web\ncrawl. Modeling hyper-text has a number of advantages: (1) it is easily\ngathered at scale, (2) it provides rich document-level and end-task-adjacent\nsupervision (e.g. class and id attributes often encode document category\ninformation), and (3) it allows for new structured prompting that follows the\nestablished semantics of HTML (e.g. to do zero-shot summarization by infilling\ntitle tags for a webpage that contains the input text). We show that\npretraining with a BART-style denoising loss directly on simplified HTML\nprovides highly effective transfer for a wide range of end tasks and\nsupervision levels. HTLM matches or exceeds the performance of comparably sized\ntext-only LMs for zero-shot prompting and fine-tuning for classification\nbenchmarks, while also setting new state-of-the-art performance levels for\nzero-shot summarization. We also find that hyper-text prompts provide more\nvalue to HTLM, in terms of data efficiency, than plain text prompts do for\nexisting LMs, and that HTLM is highly effective at auto-prompting itself, by\nsimply generating the most likely hyper-text formatting for any available\ntraining data. We will release all code and models to support future HTLM\nresearch.",
          "link": "http://arxiv.org/abs/2107.06955",
          "publishedOn": "2021-07-16T00:48:24.349Z",
          "wordCount": 633,
          "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>",
          "description": "While many existing graph neural networks (GNNs) have been proven to perform\n$\\ell_2$-based graph smoothing that enforces smoothness globally, in this work\nwe aim to further enhance the local smoothness adaptivity of GNNs via\n$\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs\n(Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In\nparticular, we propose a novel and general message passing scheme into GNNs.\nThis message passing algorithm is not only friendly to back-propagation\ntraining but also achieves the desired smoothing properties with a theoretical\nconvergence guarantee. Experiments on semi-supervised learning tasks\ndemonstrate that the proposed Elastic GNNs obtain better adaptivity on\nbenchmark datasets and are significantly robust to graph adversarial attacks.\nThe implementation of Elastic GNNs is available at\n\\url{https://github.com/lxiaorui/ElasticGNN}.",
          "link": "http://arxiv.org/abs/2107.06996",
          "publishedOn": "2021-07-16T00:48:24.332Z",
          "wordCount": 575,
          "title": "Elastic Graph Neural Networks. (arXiv:2107.06996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulay_A/0/1/0/all/0/1\">Ajinkya Mulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1\">Ahmed Zamzam</a>",
          "description": "In recent years, machine learning techniques utilizing large-scale datasets\nhave achieved remarkable performance. Differential privacy, by means of adding\nnoise, provides strong privacy guarantees for such learning algorithms. The\ncost of differential privacy is often a reduced model accuracy and a lowered\nconvergence speed. This paper investigates the impact of differential privacy\non learning algorithms in terms of their carbon footprint due to either longer\nrun-times or failed experiments. Through extensive experiments, further\nguidance is provided on choosing the noise levels which can strike a balance\nbetween desired privacy levels and reduced carbon emissions.",
          "link": "http://arxiv.org/abs/2107.06946",
          "publishedOn": "2021-07-16T00:48:24.316Z",
          "wordCount": 550,
          "title": "Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. (arXiv:2107.06946v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sourav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>",
          "description": "In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.",
          "link": "http://arxiv.org/abs/2107.06993",
          "publishedOn": "2021-07-16T00:48:24.295Z",
          "wordCount": 650,
          "title": "Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06898",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erdmenger_J/0/1/0/all/0/1\">Johanna Erdmenger</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Grosvenor_K/0/1/0/all/0/1\">Kevin T. Grosvenor</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Jefferson_R/0/1/0/all/0/1\">Ro Jefferson</a>",
          "description": "We investigate the analogy between the renormalization group (RG) and deep\nneural networks, wherein subsequent layers of neurons are analogous to\nsuccessive steps along the RG. In particular, we quantify the flow of\ninformation by explicitly computing the relative entropy or Kullback-Leibler\ndivergence in both the one- and two-dimensional Ising models under decimation\nRG, as well as in a feedforward neural network as a function of depth. We\nobserve qualitatively identical behavior characterized by the monotonic\nincrease to a parameter-dependent asymptotic value. On the quantum field theory\nside, the monotonic increase confirms the connection between the relative\nentropy and the c-theorem. For the neural networks, the asymptotic behavior may\nhave implications for various information maximization methods in machine\nlearning, as well as for disentangling compactness and generalizability.\nFurthermore, while both the two-dimensional Ising model and the random neural\nnetworks we consider exhibit non-trivial critical points, the relative entropy\nappears insensitive to the phase structure of either system. In this sense,\nmore refined probes are required in order to fully elucidate the flow of\ninformation in these models.",
          "link": "http://arxiv.org/abs/2107.06898",
          "publishedOn": "2021-07-16T00:48:24.270Z",
          "wordCount": 650,
          "title": "Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group. (arXiv:2107.06898v1 [hep-th])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06936",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Barbier_J/0/1/0/all/0/1\">Jean Barbier</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Kuo Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Panchenko_D/0/1/0/all/0/1\">Dmitry Panchenko</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saenz_M/0/1/0/all/0/1\">Manuel S&#xe1;enz</a>",
          "description": "For a model of high-dimensional linear regression with random design, we\nanalyze the performance of an estimator given by the mean of a log-concave\nBayesian posterior distribution with gaussian prior. The model is mismatched in\nthe following sense: like the model assumed by the statistician, the\nlabels-generating process is linear in the input data, but both the classifier\nground-truth prior and gaussian noise variance are unknown to her. This\ninference model can be rephrased as a version of the Gardner model in spin\nglasses and, using the cavity method, we provide fixed point equations for\nvarious overlap order parameters, yielding in particular an expression for the\nmean-square reconstruction error on the classifier (under an assumption of\nuniqueness of solutions). As a direct corollary we obtain an expression for the\nfree energy. Similar models have already been studied by Shcherbina and Tirozzi\nand by Talagrand, but our arguments are more straightforward and some\nassumptions are relaxed. An interesting consequence of our analysis is that in\nthe random design setting of ridge regression, the performance of the posterior\nmean is independent of the noise variance (or \"temperature\") assumed by the\nstatistician, and matches the one of the usual (zero temperature) ridge\nestimator.",
          "link": "http://arxiv.org/abs/2107.06936",
          "publishedOn": "2021-07-16T00:48:24.262Z",
          "wordCount": 660,
          "title": "Performance of Bayesian linear regression in a model with mismatch. (arXiv:2107.06936v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shigang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>",
          "description": "Training large deep learning models at scale is very challenging. This paper\nproposes Chimera, a novel pipeline parallelism scheme which combines\nbidirectional pipelines for efficiently training large-scale models. Chimera is\na synchronous approach and therefore no loss of accuracy, which is more\nconvergence-friendly than asynchronous approaches. Compared with the latest\nsynchronous pipeline approach, Chimera reduces the number of bubbles by up to\n50%; benefiting from the sophisticated scheduling of bidirectional pipelines,\nChimera has a more balanced activation memory consumption. Evaluations are\nconducted on Transformer based language models. For a GPT-2 model with 1.3\nbillion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,\nChimera improves the training throughput by 1.16x-2.34x over the\nstate-of-the-art synchronous and asynchronous pipeline approaches.",
          "link": "http://arxiv.org/abs/2107.06925",
          "publishedOn": "2021-07-16T00:48:24.229Z",
          "wordCount": 585,
          "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. (arXiv:2107.06925v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily H. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1\">Mark Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Deep generative models (DGMs) seem a natural fit for detecting\nout-of-distribution (OOD) inputs, but such models have been shown to assign\nhigher probabilities or densities to OOD images than images from the training\ndistribution. In this work, we explain why this behavior should be attributed\nto model misestimation. We first prove that no method can guarantee performance\nbeyond random chance without assumptions on which out-distributions are\nrelevant. We then interrogate the typical set hypothesis, the claim that\nrelevant out-distributions can lie in high likelihood regions of the data\ndistribution, and that OOD detection should be defined based on the data\ndistribution's typical set. We highlight the consequences implied by assuming\nsupport overlap between in- and out-distributions, as well as the arbitrariness\nof the typical set for OOD detection. Our results suggest that estimation error\nis a more plausible explanation than the misalignment between likelihood-based\nOOD detection and out-distributions of interest, and we illustrate how even\nminimal estimation error can lead to OOD detection failures, yielding\nimplications for future work in deep generative modeling and OOD detection.",
          "link": "http://arxiv.org/abs/2107.06908",
          "publishedOn": "2021-07-16T00:48:24.223Z",
          "wordCount": 611,
          "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. (arXiv:2107.06908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.06943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.",
          "link": "http://arxiv.org/abs/2107.06943",
          "publishedOn": "2021-07-16T00:48:24.213Z",
          "wordCount": 682,
          "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}