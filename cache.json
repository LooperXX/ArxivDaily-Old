{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.00621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>",
          "description": "The majority of Chinese characters are monophonic, while a special group of\ncharacters, called polyphonic characters, have multiple pronunciations. As a\nprerequisite of performing speech-related generative tasks, the correct\npronunciation must be identified among several candidates. This process is\ncalled Polyphone Disambiguation. Although the problem has been well explored\nwith both knowledge-based and learning-based approaches, it remains challenging\ndue to the lack of publicly available labeled datasets and the irregular nature\nof polyphone in Mandarin Chinese. In this paper, we propose a novel\nsemi-supervised learning (SSL) framework for Mandarin Chinese polyphone\ndisambiguation that can potentially leverage unlimited unlabeled text data. We\nexplore the effect of various proxy labeling strategies including\nentropy-thresholding and lexicon-based labeling. Qualitative and quantitative\nexperiments demonstrate that our method achieves state-of-the-art performance.\nIn addition, we publish a novel dataset specifically for the polyphone\ndisambiguation task to promote further researches.",
          "link": "http://arxiv.org/abs/2102.00621",
          "publishedOn": "2021-07-06T01:58:07.256Z",
          "wordCount": 606,
          "title": "Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning. (arXiv:2102.00621v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:07.208Z",
          "wordCount": 641,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "In sequence-to-sequence learning, the decoder relies on the attention\nmechanism to efficiently extract information from the encoder. While it is\ncommon practice to draw information from only the last encoder layer, recent\nwork has proposed to use representations from different encoder layers for\ndiversified levels of information. Nonetheless, the decoder still obtains only\na single view of the source sequences, which might lead to insufficient\ntraining of the encoder layer stack due to the hierarchy bypassing problem. In\nthis work, we propose layer-wise cross-view decoding, where for each decoder\nlayer, together with the representations from the last encoder layer, which\nserve as a global view, those from other encoder layers are supplemented for a\nstereoscopic view of the source sequences. Systematic experiments show that we\nsuccessfully address the hierarchy bypassing problem and substantially improve\nthe performance of sequence-to-sequence learning with deep representations on\ndiverse tasks.",
          "link": "http://arxiv.org/abs/2005.08081",
          "publishedOn": "2021-07-06T01:58:07.152Z",
          "wordCount": 637,
          "title": "Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning. (arXiv:2005.08081v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>",
          "description": "Scarcity of parallel data causes formality style transfer models to have\nscarce success in preserving content. We show that fine-tuning pre-trained\nlanguage (GPT-2) and sequence-to-sequence (BART) models boosts content\npreservation, and that this is possible even with limited amounts of parallel\ndata. Augmenting these models with rewards that target style and content -- the\ntwo core aspects of the task -- we achieve a new state-of-the-art.",
          "link": "http://arxiv.org/abs/2105.06947",
          "publishedOn": "2021-07-06T01:58:07.137Z",
          "wordCount": 527,
          "title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer. (arXiv:2105.06947v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>",
          "description": "Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n\nOur contribution is an improvement of the former construction, making it\nconceptually simpler and more direct. On the way we improve the lower bound for\nvector addition systems with states in fixed dimension (or, equivalently, Petri\nnets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski prove\n$F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in dimension\n$6k$, and Leroux in dimension $4k+5$, our simplified construction yields\n$F_k$-hardness already in dimension $3k+2$.",
          "link": "http://arxiv.org/abs/2105.08551",
          "publishedOn": "2021-07-06T01:58:07.119Z",
          "wordCount": 694,
          "title": "Improved Ackermannian lower bound for the Petri nets reachability problem. (arXiv:2105.08551v2 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1\">Zeinab Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1\">Mehrnoush ShamsFard</a>",
          "description": "Detection of semantic contradictory sentences is one of the most challenging\nand fundamental issues for NLP applications such as recognition of textual\nentailments. Contradiction in this study includes different types of semantic\nconfrontation, such as conflict and antonymy. Due to lack of sufficient data to\napply precise machine learning and specifically deep learning methods to\nPersian and other low resource languages, rule-based approaches that can\nfunction similarly to these systems will be of a great interest. Also recently,\nemergence of new methods such as transfer learning, has opened up the\npossibility of deep learning for low-resource languages. Considering two above\npoints, in this study, along with a simple rule-base baseline, a novel\nrule-base system for identifying semantic contradiction along with a Bert base\ndeep contradiction detection system for Persian texts have been introduced. The\nrule base system has used frequent rule mining method to extract appropriate\ncontradiction rules using a development set. Extracted rules are tested for\ndifferent categories of contradictory sentences. In this system the maximum\nf-measure among contradiction categories is obtained for negation about 90% and\nthe average F-measure of system for all classes is about 76% which outperforms\nother algorithms on Persian texts. On the other hand, because of medium\nperformance of rule base system for some categories of contradiction, we use a\nBert base deep learning system using our translated dataset; with average\nF-measure of 73. Our hybrid system has f-measure of about 80.",
          "link": "http://arxiv.org/abs/2107.01987",
          "publishedOn": "2021-07-06T01:58:07.064Z",
          "wordCount": 667,
          "title": "Contradiction Detection in Persian Text. (arXiv:2107.01987v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_J/0/1/0/all/0/1\">Jade Abbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_D/0/1/0/all/0/1\">Daniel D&#x27;souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzaaba_H/0/1/0/all/0/1\">Happy Buzaaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayhew_S/0/1/0/all/0/1\">Stephen Mayhew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakatumba_Nabende_J/0/1/0/all/0/1\">Joyce Nakatumba-Nabende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gitau_C/0/1/0/all/0/1\">Catherine Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mbaye_D/0/1/0/all/0/1\">Derguene Mbaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezeani_I/0/1/0/all/0/1\">Ignatius Ezeani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyongabo_R/0/1/0/all/0/1\">Rubungo Andre Niyongabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otiende_V/0/1/0/all/0/1\">Verrah Otiende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orife_I/0/1/0/all/0/1\">Iroro Orife</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Davis David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngom_S/0/1/0/all/0/1\">Samba Ngom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayson_P/0/1/0/all/0/1\">Paul Rayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muriuki_G/0/1/0/all/0/1\">Gerald Muriuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anebi_E/0/1/0/all/0/1\">Emmanuel Anebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">Chiamaka Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odu_N/0/1/0/all/0/1\">Nkiruka Odu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wairagala_E/0/1/0/all/0/1\">Eric Peter Wairagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyerinde_S/0/1/0/all/0/1\">Samuel Oyerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bateesa_T/0/1/0/all/0/1\">Tobius Saul Bateesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oloyede_T/0/1/0/all/0/1\">Temilola Oloyede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wambui_Y/0/1/0/all/0/1\">Yvonne Wambui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinode_V/0/1/0/all/0/1\">Victor Akinode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabagereka_D/0/1/0/all/0/1\">Deborah Nabagereka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katusiime_M/0/1/0/all/0/1\">Maurice Katusiime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1\">Ayodele Awokoya</a>, et al. (15 additional authors not shown)",
          "description": "We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.",
          "link": "http://arxiv.org/abs/2103.11811",
          "publishedOn": "2021-07-06T01:58:07.037Z",
          "wordCount": 691,
          "title": "MasakhaNER: Named Entity Recognition for African Languages. (arXiv:2103.11811v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>",
          "description": "Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.",
          "link": "http://arxiv.org/abs/2106.08087",
          "publishedOn": "2021-07-06T01:58:07.010Z",
          "wordCount": 653,
          "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dongwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Miao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wei Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>",
          "description": "Self-supervised visual pretraining has shown significant progress recently.\nAmong those methods, SimCLR greatly advanced the state of the art in\nself-supervised and semi-supervised learning on ImageNet. The input feature\nrepresentations for speech and visual tasks are both continuous, so it is\nnatural to consider applying similar objective on speech representation\nlearning. In this paper, we propose Speech SimCLR, a new self-supervised\nobjective for speech representation learning. During training, Speech SimCLR\napplies augmentation on raw speech and its spectrogram. Its objective is the\ncombination of contrastive loss that maximizes agreement between differently\naugmented samples in the latent space and reconstruction loss of input\nrepresentation. The proposed method achieved competitive results on speech\nemotion recognition and speech recognition.",
          "link": "http://arxiv.org/abs/2010.13991",
          "publishedOn": "2021-07-06T01:58:06.990Z",
          "wordCount": 588,
          "title": "Speech SIMCLR: Combining Contrastive and Reconstruction Objective for Self-supervised Speech Representation Learning. (arXiv:2010.13991v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1\">Anastasiia Sedova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>",
          "description": "Strategies for improving the training and prediction quality of weakly\nsupervised machine learning models vary in how much they are tailored to a\nspecific task or integrated with a specific model architecture. In this work,\nwe introduce Knodle, a software framework that treats weak data annotations,\ndeep learning models, and methods for improving weakly supervised training as\nseparate, modular components. This modularization gives the training process\naccess to fine-grained information such as data set characteristics, matches of\nheuristic rules, or elements of the deep learning model ultimately used for\nprediction. Hence, our framework can encompass a wide range of training methods\nfor improving weak supervision, ranging from methods that only look at\ncorrelations of rules and output classes (independently of the machine learning\nmodel trained with the resulting labels), to those that harness the interplay\nof neural networks and weakly labeled data. We illustrate the benchmarking\npotential of the framework with a performance comparison of several reference\nimplementations on a selection of datasets that are already available in\nKnodle.\n\nThe framework is published as an open-source Python package knodle and\navailable at https://github.com/knodle/knodle.",
          "link": "http://arxiv.org/abs/2104.11557",
          "publishedOn": "2021-07-06T01:58:06.975Z",
          "wordCount": 652,
          "title": "Knodle: Modular Weakly Supervised Learning with PyTorch. (arXiv:2104.11557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Zikun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>",
          "description": "Query reformulation aims to alter noisy or ambiguous text sequences into\ncoherent ones closer to natural language questions. This is to prevent errors\nfrom propagating in a client-facing pipeline and promote better communication\nwith users. Besides, it is crucial to maintain performance in downstream\nenvironments like question answering when rephrased queries are given as input.\nWe show that under the previous framework (AQA), attempts to alter RL\nalgorithms do not bring significant benefits to either reward acquisition or\nsequence fluency. Instead, we leverage a query-reformulating text-to-text\ntransformer (QRT5) and apply policy-based RL algorithms to further nudge this\nreformulator and obtain better answers downstream by generating\nreward-acquiring query trajectories. QRT5 shows better sample efficiency in RL\nto achieve the same level of QA performance as the previous approach. It can\ngenerate reformulations with more readability based on query well-formedness\nevaluations and can generalize to out-of-sample data. Our framework is\ndemonstrated to be flexible, allowing reward signals to be sourced from\ndifferent downstream environments such as intent classification.",
          "link": "http://arxiv.org/abs/2012.10033",
          "publishedOn": "2021-07-06T01:58:06.967Z",
          "wordCount": 649,
          "title": "Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning. (arXiv:2012.10033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-07-06T01:58:06.924Z",
          "wordCount": 681,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Feng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Junyi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Weikang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>",
          "description": "Subtext is a kind of deep semantics which can be acquired after one or more\nrounds of expression transformation. As a popular way of expressing one's\nintentions, it is well worth studying. In this paper, we try to make computers\nunderstand whether there is a subtext by means of machine learning. We build a\nChinese dataset whose source data comes from the popular social media (e.g.\nWeibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a\nbaseline model called SASICM to deal with subtext recognition. The F1 score of\nSASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%\nhigher than that of BERT based model, 12.7% higher than that of traditional\nmethods on average, including support vector machine, logistic regression\nclassifier, maximum entropy classifier, naive bayes classifier and decision\ntree and 2.39% higher than that of the state-of-the-art, including MARIN and\nBTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,\nwhich is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and\nSASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of\nother methods which are mentioned before.",
          "link": "http://arxiv.org/abs/2106.06944",
          "publishedOn": "2021-07-06T01:58:06.912Z",
          "wordCount": 683,
          "title": "SASICM A Multi-Task Benchmark For Subtext Recognition. (arXiv:2106.06944v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">M. A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corvonato_A/0/1/0/all/0/1\">A. Corvonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">C. Heumann</a>",
          "description": "The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.",
          "link": "http://arxiv.org/abs/2102.12330",
          "publishedOn": "2021-07-06T01:58:06.886Z",
          "wordCount": 642,
          "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models. (arXiv:2102.12330v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-07-06T01:58:06.869Z",
          "wordCount": 670,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>",
          "description": "We describe the DCU-EPFL submission to the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies. The task involves parsing Enhanced UD\ngraphs, which are an extension of the basic dependency trees designed to be\nmore facilitative towards representing semantic structure. Evaluation is\ncarried out on 29 treebanks in 17 languages and participants are required to\nparse the data from each language starting from raw strings. Our approach uses\nthe Stanza pipeline to preprocess the text files, XLMRoBERTa to obtain\ncontextualized token representations, and an edge-scoring and labeling model to\npredict the enhanced graph. Finally, we run a post-processing script to ensure\nall of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9\nparticipants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57.\nWe carry out additional post-deadline experiments which include using Trankit\nfor pre-processing, XLM-RoBERTa-LARGE, treebank concatenation, and multitask\nlearning between a basic and an enhanced dependency parser. All of these\nmodifications improve our initial score and our final system has a coarse ELAS\nof 88.04.",
          "link": "http://arxiv.org/abs/2107.01982",
          "publishedOn": "2021-07-06T01:58:06.856Z",
          "wordCount": 635,
          "title": "The DCU-EPFL Enhanced Dependency Parser at the IWPT 2021 Shared Task. (arXiv:2107.01982v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Question: I have five fingers but I am not alive. What am I? Answer: a glove.\nAnswering such a riddle-style question is a challenging cognitive process, in\nthat it requires complex commonsense reasoning abilities, an understanding of\nfigurative language, and counterfactual reasoning skills, which are all\nimportant abilities for advanced natural language understanding (NLU). However,\nthere are currently no dedicated datasets aiming to test these abilities.\nHerein, we present RiddleSense, a new multiple-choice question answering task,\nwhich comes with the first large dataset (5.7k examples) for answering\nriddle-style commonsense questions. We systematically evaluate a wide range of\nmodels over the challenge, and point out that there is a large gap between the\nbest-supervised model and human performance -- suggesting intriguing future\nresearch in the direction of higher-order commonsense reasoning and linguistic\ncreativity towards building advanced NLU systems.",
          "link": "http://arxiv.org/abs/2101.00376",
          "publishedOn": "2021-07-06T01:58:06.653Z",
          "wordCount": 623,
          "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. (arXiv:2101.00376v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhung Thi-Hong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Phan-Dieu Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Customer product reviews play a role in improving the quality of products and\nservices for business organizations or their brands. Complaining is an attitude\nthat expresses dissatisfaction with an event or a product not meeting customer\nexpectations. In this paper, we build a Open-domain Complaint Detection dataset\n(UIT-ViOCD), including 5,485 human-annotated reviews on four categories about\nproduct reviews on e-commerce sites. After the data collection phase, we\nproceed to the annotation task and achieve the inter-annotator agreement Am of\n87%. Then, we present an extensive methodology for the research purposes and\nachieve 92.16% by F1-score for identifying complaints. With the results, in the\nfuture, we aim to build a system for open-domain complaint detection in\nE-commerce websites.",
          "link": "http://arxiv.org/abs/2104.11969",
          "publishedOn": "2021-07-06T01:58:06.456Z",
          "wordCount": 587,
          "title": "Vietnamese Complaint Detection on E-Commerce Websites. (arXiv:2104.11969v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Duocai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.",
          "link": "http://arxiv.org/abs/2107.01875",
          "publishedOn": "2021-07-06T01:58:06.448Z",
          "wordCount": 636,
          "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling. (arXiv:2107.01875v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.",
          "link": "http://arxiv.org/abs/2107.01598",
          "publishedOn": "2021-07-06T01:58:06.385Z",
          "wordCount": 624,
          "title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation. (arXiv:2107.01598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>",
          "description": "Sentiment analysis in conversations has gained increasing attention in recent\nyears for the growing amount of applications it can serve, e.g., sentiment\nanalysis, recommender systems, and human-robot interaction. The main difference\nbetween conversational sentiment analysis and single sentence sentiment\nanalysis is the existence of context information which may influence the\nsentiment of an utterance in a dialogue. How to effectively encode contextual\ninformation in dialogues, however, remains a challenge. Existing approaches\nemploy complicated deep learning structures to distinguish different parties in\na conversation and then model the context information. In this paper, we\npropose a fast, compact and parameter-efficient party-ignorant framework named\nbidirectional emotional recurrent unit for conversational sentiment analysis.\nIn our system, a generalized neural tensor block followed by a two-channel\nclassifier is designed to perform context compositionality and sentiment\nclassification, respectively. Extensive experiments on three standard datasets\ndemonstrate that our model outperforms the state of the art in most cases.",
          "link": "http://arxiv.org/abs/2006.00492",
          "publishedOn": "2021-07-06T01:58:06.375Z",
          "wordCount": 633,
          "title": "BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis. (arXiv:2006.00492v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-07-06T01:58:06.367Z",
          "wordCount": 672,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingyue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinglin Wang</a>",
          "description": "Pretrained language models (PLM) achieve surprising performance on the Choice\nof Plausible Alternatives (COPA) task. However, whether PLMs have truly\nacquired the ability of causal reasoning remains a question. In this paper, we\ninvestigate the problem of semantic similarity bias and reveal the\nvulnerability of current COPA models by certain attacks. Previous solutions\nthat tackle the superficial cues of unbalanced token distribution still\nencounter the same problem of semantic bias, even more seriously due to the\nutilization of more training data. We mitigate this problem by simply adding a\nregularization loss and experimental results show that this solution not only\nimproves the model's generalization ability, but also assists the models to\nperform more robustly on a challenging dataset, BCOPA-CE, which has unbiased\ntoken distribution and is more difficult for models to distinguish cause and\neffect.",
          "link": "http://arxiv.org/abs/2107.01791",
          "publishedOn": "2021-07-06T01:58:06.331Z",
          "wordCount": 578,
          "title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models. (arXiv:2107.01791v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>",
          "description": "In this paper, we propose FFCI, a framework for fine-grained summarization\nevaluation that comprises four elements: faithfulness (degree of factual\nconsistency with the source), focus (precision of summary content relative to\nthe reference), coverage (recall of summary content relative to the reference),\nand inter-sentential coherence (document fluency between adjacent sentences).\nWe construct a novel dataset for focus, coverage, and inter-sentential\ncoherence, and develop automatic methods for evaluating each of the four\ndimensions of FFCI based on cross-comparison of evaluation metrics and\nmodel-based evaluation methods, including question answering (QA) approaches,\nSTS, next-sentence prediction (NSP), and scores derived from 19 pre-trained\nlanguage models. We then apply the developed metrics in evaluating a broad\nrange of summarization models across two datasets, with some surprising\nfindings.",
          "link": "http://arxiv.org/abs/2011.13662",
          "publishedOn": "2021-07-06T01:58:06.323Z",
          "wordCount": 594,
          "title": "FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. (arXiv:2011.13662v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language and\ndialectal variation of spoken content. Recent studies show its efficacy over\nmonolingual systems. In this study, we design a large multilingual end-to-end\nASR using self-attention based conformer architecture. We trained the system\nusing Arabic (Ar), English (En) and French (Fr) languages. We evaluate the\nsystem performance handling: (i) monolingual (Ar, En and Fr); (ii)\nmulti-dialectal (Modern Standard Arabic, along with dialectal variation such as\nEgyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and\ndialectal (MSA-Egyptian dialect) test cases, and compare with current\nstate-of-the-art systems. Furthermore, we investigate the influence of\ndifferent embedding/character representations including character vs\nword-piece; shared vs distinct input symbol per language. Our findings\ndemonstrate the strength of such a model by outperforming state-of-the-art\nmonolingual dialectal Arabic and code-switching Arabic ASR.",
          "link": "http://arxiv.org/abs/2105.14779",
          "publishedOn": "2021-07-06T01:58:06.313Z",
          "wordCount": 651,
          "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR. (arXiv:2105.14779v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>",
          "description": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.",
          "link": "http://arxiv.org/abs/2106.07250",
          "publishedOn": "2021-07-06T01:58:06.289Z",
          "wordCount": 570,
          "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doo Soon Kim</a>",
          "description": "Since the first end-to-end neural coreference resolution model was\nintroduced, many extensions to the model have been proposed, ranging from using\nhigher-order inference to directly optimizing evaluation metrics using\nreinforcement learning. Despite improving the coreference resolution\nperformance by a large margin, these extensions add a lot of extra complexity\nto the original model. Motivated by this observation and the recent advances in\npre-trained Transformer language models, we propose a simple yet effective\nbaseline for coreference resolution. Our model is a simplified version of the\noriginal neural coreference resolution model, however, it achieves impressive\nperformance, outperforming all recent extended works on the public English\nOntoNotes benchmark. Our work provides evidence for the necessity of carefully\njustifying the complexity of existing or newly proposed models, as introducing\na conceptual or practical simplification to an existing model can still yield\ncompetitive results.",
          "link": "http://arxiv.org/abs/2107.01700",
          "publishedOn": "2021-07-06T01:58:06.276Z",
          "wordCount": 576,
          "title": "End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.",
          "link": "http://arxiv.org/abs/2107.01569",
          "publishedOn": "2021-07-06T01:58:06.201Z",
          "wordCount": 629,
          "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition. (arXiv:2107.01569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yawen Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>",
          "description": "Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.",
          "link": "http://arxiv.org/abs/2107.01545",
          "publishedOn": "2021-07-06T01:58:06.189Z",
          "wordCount": 647,
          "title": "Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.",
          "link": "http://arxiv.org/abs/2010.04389",
          "publishedOn": "2021-07-06T01:58:06.176Z",
          "wordCount": 660,
          "title": "A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>",
          "description": "Event extraction (EE) is a crucial information extraction task that aims to\nextract event information in texts. Most existing methods assume that events\nappear in sentences without overlaps, which are not applicable to the\ncomplicated overlapping event extraction. This work systematically studies the\nrealistic event overlapping problem, where a word may serve as triggers with\nseveral types or arguments with different roles. To tackle the above problem,\nwe propose a novel joint learning framework with cascade decoding for\noverlapping event extraction, termed as CasEE. Particularly, CasEE sequentially\nperforms type detection, trigger extraction and argument extraction, where the\noverlapped targets are extracted separately conditioned on the specific former\nprediction. All the subtasks are jointly learned in a framework to capture\ndependencies among the subtasks. The evaluation on a public event extraction\nbenchmark FewFC demonstrates that CasEE achieves significant improvements on\noverlapping event extraction over previous competitive methods.",
          "link": "http://arxiv.org/abs/2107.01583",
          "publishedOn": "2021-07-06T01:58:06.158Z",
          "wordCount": 599,
          "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction. (arXiv:2107.01583v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Neural Machine Translation (NMT) is a predominant machine translation\ntechnology nowadays because of its end-to-end trainable flexibility. However,\nNMT still struggles to translate properly in low-resource settings specifically\non distant language pairs. One way to overcome this is to use the information\nfrom other modalities if available. The idea is that despite differences in\nlanguages, both the source and target language speakers see the same thing and\nthe visual representation of both the source and target is the same, which can\npositively assist the system. Multimodal information can help the NMT system to\nimprove the translation by removing ambiguity on some phrases or words. We\nparticipate in the 8th Workshop on Asian Translation (WAT - 2021) for\nEnglish-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU\npoints for Evaluation and Challenge subset, respectively.",
          "link": "http://arxiv.org/abs/2107.01656",
          "publishedOn": "2021-07-06T01:58:06.118Z",
          "wordCount": 573,
          "title": "IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task. (arXiv:2107.01656v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>",
          "description": "It is prevalent to utilize external knowledge to help machine answer\nquestions that need background commonsense, which faces a problem that\nunlimited knowledge will transmit noisy and misleading information. Towards the\nissue of introducing related knowledge, we propose a semantic-driven\nknowledge-aware QA framework, which controls the knowledge injection in a\ncoarse-to-careful fashion. We devise a tailoring strategy to filter extracted\nknowledge under monitoring of the coarse semantic of question on the knowledge\nextraction stage. And we develop a semantic-aware knowledge fetching module\nthat engages structural knowledge information and fuses proper knowledge\naccording to the careful semantic of questions in a hierarchical way.\nExperiments demonstrate that the proposed approach promotes the performance on\nthe CommonsenseQA dataset comparing with strong baselines.",
          "link": "http://arxiv.org/abs/2107.01592",
          "publishedOn": "2021-07-06T01:58:06.105Z",
          "wordCount": 557,
          "title": "Coarse-to-Careful: Seeking Semantic-related Knowledge for Open-domain Commonsense Question Answering. (arXiv:2107.01592v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okamura_D/0/1/0/all/0/1\">Daiki Okamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>",
          "description": "In this paper, we present a novel modeling method for single-channel\nmulti-talker overlapped automatic speech recognition (ASR) systems. Fully\nneural network based end-to-end models have dramatically improved the\nperformance of multi-taker overlapped ASR tasks. One promising approach for\nend-to-end modeling is autoregressive modeling with serialized output training\nin which transcriptions of multiple speakers are recursively generated one\nafter another. This enables us to naturally capture relationships between\nspeakers. However, the conventional modeling method cannot explicitly take into\naccount the speaker attributes of individual utterances such as gender and age\ninformation. In fact, the performance deteriorates when each speaker is the\nsame gender or is close in age. To address this problem, we propose unified\nautoregressive modeling for joint end-to-end multi-talker overlapped ASR and\nspeaker attribute estimation. Our key idea is to handle gender and age\nestimation tasks within the unified autoregressive modeling. In the proposed\nmethod, transformer-based autoregressive model recursively generates not only\ntextual tokens but also attribute tokens of each speaker. This enables us to\neffectively utilize speaker attributes for improving multi-talker overlapped\nASR. Experiments on Japanese multi-talker overlapped ASR tasks demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.01549",
          "publishedOn": "2021-07-06T01:58:06.097Z",
          "wordCount": 655,
          "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation. (arXiv:2107.01549v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Helin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "While Machine Comprehension (MC) has attracted extensive research interests\nin recent years, existing approaches mainly belong to the category of Machine\nReading Comprehension task which mines textual inputs (paragraphs and\nquestions) to predict the answers (choices or text spans). However, there are a\nlot of MC tasks that accept audio input in addition to the textual input, e.g.\nEnglish listening comprehension test. In this paper, we target the problem of\nAudio-Oriented Multimodal Machine Comprehension, and its goal is to answer\nquestions based on the given audio and textual information. To solve this\nproblem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model\nto effectively fuse the two modalities (audio and textual). DIIA can work as an\nindependent component and thus be easily integrated into existing MC models.\nMoreover, we further develop a Multimodal Knowledge Distillation (MKD) module\nto enable our multimodal MC model to accurately predict the answers based only\non either the text or the audio. As a result, the proposed approach can handle\nvarious tasks including: Audio-Oriented Multimodal Machine Comprehension,\nMachine Reading Comprehension and Machine Listening Comprehension, in a single\nmodel, making fair comparisons possible between our model and the existing\nunimodal MC models. Experimental results and analysis prove the effectiveness\nof the proposed approaches. First, the proposed DIIA boosts the baseline models\nby up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the\nMKD module allows our multimodal MC model to significantly outperform the\nunimodal models by up to 18.87%, which are trained and tested with only audio\nor textual data.",
          "link": "http://arxiv.org/abs/2107.01571",
          "publishedOn": "2021-07-06T01:58:06.088Z",
          "wordCount": 698,
          "title": "Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model. (arXiv:2107.01571v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01275",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwarz_P/0/1/0/all/0/1\">Patrick Schwarz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter. Upon acceptance, models will be\npublished on github.",
          "link": "http://arxiv.org/abs/2107.01275",
          "publishedOn": "2021-07-06T01:58:06.079Z",
          "wordCount": 610,
          "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition. (arXiv:2107.01275v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A.Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Modern neural text generation systems can produce remarkably fluent and\ngrammatical texts. While earlier language models suffered from repetition and\nsyntactic errors, the errors made by contemporary models are often semantic,\nnarrative, or discourse failures.\n\nTo facilitate research of these complex error types, we introduce a new\nstructured, crowdsourced error annotation schema called Scarecrow. The error\ncategories used in Scarecrow -- such as redundancy, commonsense errors, and\nincoherence -- were identified by combining expert analysis with several pilot\nrounds of ontology-free crowd annotation to arrive at a schema which covers the\nerror phenomena found in real machine generated text.\n\nWe use Scarecrow to collect 13k annotations of 1.3k human and machine\ngenerate paragraphs of English language news text, amounting to over 41k spans\neach labeled with its error category, severity, a natural language explanation,\nand antecedent span (where relevant). We collect annotations for text generated\nby state-of-the-art systems with varying known performance levels, from GPT-2\nSmall through the largest GPT-3. We isolate several factors for detailed\nanalysis, including parameter count, training data, and decoding technique. Our\nresults show both expected and surprising differences across these settings.\nThese findings demonstrate the value of Scarecrow annotations in the assessment\nof current and future text generation systems. We release our complete\nannotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.",
          "link": "http://arxiv.org/abs/2107.01294",
          "publishedOn": "2021-07-06T01:58:06.051Z",
          "wordCount": 656,
          "title": "Scarecrow: A Framework for Scrutinizing Machine Text. (arXiv:2107.01294v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hifny_Y/0/1/0/all/0/1\">Yasser Hifny</a>",
          "description": "Code-switching in automatic speech recognition (ASR) is an important\nchallenge due to globalization. Recent research in multilingual ASR shows\npotential improvement over monolingual systems. We study key issues related to\nmultilingual modeling for ASR through a series of large-scale ASR experiments.\nOur innovative framework deploys a multi-graph approach in the weighted finite\nstate transducers (WFST) framework. We compare our WFST decoding strategies\nwith a transformer sequence to sequence system trained on the same data. Given\na code-switching scenario between Arabic and English languages, our results\nshow that the WFST decoding approaches were more suitable for the\nintersentential code-switching datasets. In addition, the transformer system\nperformed better for intrasentential code-switching task. With this study, we\nrelease an artificially generated development and test sets, along with\necological code-switching test set, to benchmark the ASR performance.",
          "link": "http://arxiv.org/abs/2107.01573",
          "publishedOn": "2021-07-06T01:58:06.029Z",
          "wordCount": 586,
          "title": "Arabic Code-Switching Speech Recognition using Monolingual Data. (arXiv:2107.01573v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>",
          "description": "Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.",
          "link": "http://arxiv.org/abs/2107.01366",
          "publishedOn": "2021-07-06T01:58:06.005Z",
          "wordCount": 621,
          "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jinghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jianheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Previous math word problem solvers following the encoder-decoder paradigm\nfail to explicitly incorporate essential math symbolic constraints, leading to\nunexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic\nSolver (NS-Solver) to explicitly and seamlessly incorporate different levels of\nsymbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem\nreader to encode problems, a programmer to generate symbolic equations, and a\nsymbolic executor to obtain answers. Along with target expression supervision,\nour solver is also optimized via 4 new auxiliary objectives to enforce\ndifferent symbolic reasoning: a) self-supervised number prediction task\npredicting both number quantity and number locations; b) commonsense constant\nprediction task predicting what prior knowledge (e.g. how many legs a chicken\nhas) is required; c) program consistency checker computing the semantic loss\nbetween predicted equation and target equation to ensure reasonable equation\nmapping; d) duality exploiting task exploiting the quasi duality between\nsymbolic equation generation and problem's part-of-speech generation to enhance\nthe understanding ability of a solver. Besides, to provide a more realistic and\nchallenging benchmark for developing a universal and scalable solver, we also\nconstruct a new large-scale MWP benchmark CM17K consisting of 4 kinds of MWPs\n(arithmetic, one-unknown linear, one-unknown non-linear, equation set) with\nmore than 17K samples. Extensive experiments on Math23K and our CM17k\ndemonstrate the superiority of our NS-Solver compared to state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.01431",
          "publishedOn": "2021-07-06T01:58:05.922Z",
          "wordCount": 656,
          "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks. (arXiv:2107.01431v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouhizadeh_H/0/1/0/all/0/1\">Hossein Rouhizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajalli_V/0/1/0/all/0/1\">Vahideh Tajalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhziadeh_M/0/1/0/all/0/1\">Masoud Rouhziadeh</a>",
          "description": "Word Sense Disambiguation (WSD) is a long-standing task in Natural Language\nProcessing(NLP) that aims to automatically identify the most relevant meaning\nof the words in a given context. Developing standard WSD test collections can\nbe mentioned as an important prerequisite for developing and evaluating\ndifferent WSD systems in the language of interest. Although many WSD test\ncollections have been developed for a variety of languages, no standard\nAll-words WSD benchmark is available for Persian. In this paper, we address\nthis shortage for the Persian language by introducing SBU-WSD-Corpus, as the\nfirst standard test set for the Persian All-words WSD task. SBU-WSD-Corpus is\nmanually annotated with senses from the Persian WordNet (FarsNet) sense\ninventory. To this end, three annotators used SAMP (a tool for sense annotation\nbased on FarsNet lexical graph) to perform the annotation task. SBU-WSD-Corpus\nconsists of 19 Persian documents in different domains such as Sports, Science,\nArts, etc. It includes 5892 content words of Persian running text and 3371\nmanually sense annotated words (2073 nouns, 566 verbs, 610 adjectives, and 122\nadverbs). Providing baselines for future studies on the Persian All-words WSD\ntask, we evaluate several WSD models on SBU-WSD-Corpus. The corpus is publicly\navailable at https://github.com/hrouhizadeh/SBU-WSD-Corpus.",
          "link": "http://arxiv.org/abs/2107.01540",
          "publishedOn": "2021-07-06T01:58:05.906Z",
          "wordCount": 639,
          "title": "Persian-WSD-Corpus: A Sense Annotated Corpus for Persian All-words Word Sense Disambiguation. (arXiv:2107.01540v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: this http URL for research purposes.",
          "link": "http://arxiv.org/abs/2105.01542",
          "publishedOn": "2021-07-05T01:54:57.338Z",
          "wordCount": 637,
          "title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>",
          "description": "Automatic speech recognition (ASR) technologies today are primarily optimized\nfor given datasets; thus, any changes in the application environment (e.g.,\nacoustic conditions or topic domains) may inevitably degrade the performance.\nWe can collect new data describing the new environment and fine-tune the\nsystem, but this naturally leads to higher error rates for the earlier\ndatasets, referred to as catastrophic forgetting. The concept of lifelong\nlearning (LLL) aiming to enable a machine to sequentially learn new tasks from\nnew datasets describing the changing real world without forgetting the\npreviously learned knowledge is thus brought to attention. This paper reports,\nto our knowledge, the first effort to extensively consider and analyze the use\nof various approaches of LLL in end-to-end (E2E) ASR, including proposing novel\nmethods in saving data for past domains to mitigate the catastrophic forgetting\nproblem. An overall relative reduction of 28.7% in WER was achieved compared to\nthe fine-tuning baseline when sequentially learning on three very different\nbenchmark corpora. This can be the first step toward the highly desired ASR\ntechnologies capable of synchronizing with the continuously changing real\nworld.",
          "link": "http://arxiv.org/abs/2104.01616",
          "publishedOn": "2021-07-05T01:54:57.284Z",
          "wordCount": 663,
          "title": "Towards Lifelong Learning of End-to-end ASR. (arXiv:2104.01616v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1\">Julia Pritzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1\">Dietlind Z&#xfc;hlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>",
          "description": "Loanwords, such as Anglicisms, are a challenge in German speech recognition.\nDue to their irregular pronunciation compared to native German words,\nautomatically generated pronunciation dictionaries often include faulty phoneme\nsequences for Anglicisms. In this work, we propose a multitask\nsequence-to-sequence approach for grapheme-to-phoneme conversion to improve the\nphonetization of Anglicisms. We extended a grapheme-to-phoneme model with a\nclassifier to distinguish Anglicisms from native German words. With this\napproach, the model learns to generate pronunciations differently depending on\nthe classification result. We used our model to create supplementary Anglicism\npronunciation dictionaries that are added to an existing German speech\nrecognition model. Tested on a dedicated Anglicism evaluation set, we improved\nthe recognition of Anglicisms compared to a baseline model, reducing the word\nerror rate by 1 % and the Anglicism error rate by 3 %. We show that multitask\nlearning can help solving the challenge of loanwords in German speech\nrecognition.",
          "link": "http://arxiv.org/abs/2105.12708",
          "publishedOn": "2021-07-05T01:54:57.265Z",
          "wordCount": 629,
          "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:57.257Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Th&#xe9;o Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeiren_W/0/1/0/all/0/1\">Walter Vermeiren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1\">Sylvie Ranwez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>",
          "description": "Patent analysis and mining are time-consuming and costly processes for\ncompanies, but nevertheless essential if they are willing to remain\ncompetitive. To face the overload induced by numerous patents, the idea is to\nautomatically filter them, bringing only few to read to experts. This paper\nreports a successful application of fine-tuning and retraining on pre-trained\ndeep Natural Language Processing models on patent classification. The solution\nthat we propose combines several state-of-the-art treatments to achieve our\ngoal - decrease the workload while preserving recall and precision metrics.",
          "link": "http://arxiv.org/abs/2105.03979",
          "publishedOn": "2021-07-05T01:54:57.250Z",
          "wordCount": 557,
          "title": "Improving Patent Mining and Relevance Classification using Transformers. (arXiv:2105.03979v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basta_C/0/1/0/all/0/1\">Christine Basta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>",
          "description": "The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n\nThis paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.",
          "link": "http://arxiv.org/abs/2010.14465",
          "publishedOn": "2021-07-05T01:54:57.243Z",
          "wordCount": 611,
          "title": "Evaluating Gender Bias in Speech Translation. (arXiv:2010.14465v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>",
          "description": "In this paper, we investigate the driving factors behind concatenation, a\nsimple but effective data augmentation method for low-resource neural machine\ntranslation. Our experiments suggest that discourse context is unlikely the\ncause for the improvement of about +1 BLEU across four language pairs. Instead,\nwe demonstrate that the improvement comes from three other factors unrelated to\ndiscourse: context diversity, length diversity, and (to a lesser extent)\nposition shifting.",
          "link": "http://arxiv.org/abs/2105.01691",
          "publishedOn": "2021-07-05T01:54:57.227Z",
          "wordCount": 542,
          "title": "Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution. (arXiv:2105.01691v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.",
          "link": "http://arxiv.org/abs/2105.00171",
          "publishedOn": "2021-07-05T01:54:57.220Z",
          "wordCount": 649,
          "title": "AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:57.212Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-05T01:54:57.199Z",
          "wordCount": 582,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:57.191Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>",
          "description": "Several high-profile events, such as the use of biased recidivism systems and\nmass testing of emotion recognition systems on vulnerable sub-populations, have\nhighlighted how technology will often lead to more adverse outcomes for those\nthat are already marginalized. In this paper, I will make a case for thinking\nabout ethical considerations not just at the level of individual models and\ndatasets, but also at the level of AI tasks. I will present a new form of such\nan effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the\nassumptions and ethical considerations hidden in how a task is commonly framed\nand in the choices we make regarding the data, method, and evaluation. Finally,\nI will provide an example ethics sheet for automatic emotion recognition.\nTogether with Data Sheets for datasets and Model Cards for AI systems, Ethics\nSheets aid in the development and deployment of responsible AI systems.",
          "link": "http://arxiv.org/abs/2107.01183",
          "publishedOn": "2021-07-05T01:54:57.155Z",
          "wordCount": 574,
          "title": "Ethics Sheets for AI Tasks. (arXiv:2107.01183v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lewen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Sequence learning has attracted much research attention from the machine\nlearning community in recent years. In many applications, a sequence learning\ntask is usually associated with multiple temporally correlated auxiliary tasks,\nwhich are different in terms of how much input information to use or which\nfuture step to predict. For example, (i) in simultaneous machine translation,\none can conduct translation under different latency (i.e., how many input words\nto read/wait before translation); (ii) in stock trend forecasting, one can\npredict the price of a stock in different future days (e.g., tomorrow, the day\nafter tomorrow). While it is clear that those temporally correlated tasks can\nhelp each other, there is a very limited exploration on how to better leverage\nmultiple auxiliary tasks to boost the performance of the main task. In this\nwork, we introduce a learnable scheduler to sequence learning, which can\nadaptively select auxiliary tasks for training depending on the model status\nand the current training data. The scheduler and the model for the main task\nare jointly trained through bi-level optimization. Experiments show that our\nmethod significantly improves the performance of simultaneous machine\ntranslation and stock trend forecasting.",
          "link": "http://arxiv.org/abs/2007.05290",
          "publishedOn": "2021-07-05T01:54:57.097Z",
          "wordCount": 665,
          "title": "Temporally Correlated Task Scheduling for Sequence Learning. (arXiv:2007.05290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.",
          "link": "http://arxiv.org/abs/2104.10100",
          "publishedOn": "2021-07-05T01:54:57.086Z",
          "wordCount": 552,
          "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erev_I/0/1/0/all/0/1\">Ido Erev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>",
          "description": "Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence, and serve as a\nsolid foundation for powerful applications. However, in the classic persuasion\ngames setting, the messages sent from the expert to the decision-maker are\nabstract or well-structured application-specific signals rather than natural\n(human) language messages, although natural language is a very common\ncommunication signal in real-world persuasion setups. This paper addresses the\nuse of natural language in persuasion games, exploring its impact on the\ndecisions made by the players and aiming to construct effective models for the\nprediction of these decisions. For this purpose, we conduct an online repeated\ninteraction experiment. At each trial of the interaction, an informed expert\naims to sell an uninformed decision-maker a vacation in a hotel, by sending her\na review that describes the hotel. While the expert is exposed to several\nscored reviews, the decision-maker observes only the single review sent by the\nexpert, and her payoff in case she chooses to take the hotel is a random draw\nfrom the review score distribution available to the expert only. The expert's\npayoff, in turn, depends on the number of times the decision-maker chooses the\nhotel. We consider a number of modeling approaches for this setup, differing\nfrom each other in the model type (deep neural network (DNN) vs. linear\nclassifier), the type of features used by the model (textual, behavioral or\nboth) and the source of the textual features (DNN-based vs. hand-crafted). Our\nresults demonstrate that given a prefix of the interaction sequence, our models\ncan predict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.",
          "link": "http://arxiv.org/abs/2012.09966",
          "publishedOn": "2021-07-05T01:54:57.073Z",
          "wordCount": 767,
          "title": "Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Unsupervised concept identification through clustering, i.e., identification\nof semantically related words and phrases, is a common approach to identify\ncontextual primitives employed in various use cases, e.g., text dimension\nreduction, i.e., replace words with the concepts to reduce the vocabulary size,\nsummarization, and named entity resolution. We demonstrate the first results of\nan unsupervised approach for the identification of groups of persons as actors\nextracted from a set of related articles. Specifically, the approach clusters\nmentions of groups of persons that act as non-named entity actors in the texts,\ne.g., \"migrant families\" = \"asylum-seekers.\" Compared to our baseline, the\napproach keeps the mentions of the geopolitical entities separated, e.g., \"Iran\nleaders\" != \"European leaders,\" and clusters (in)directly related mentions with\ndiverse wording, e.g., \"American officials\" = \"Trump Administration.\"",
          "link": "http://arxiv.org/abs/2107.00955",
          "publishedOn": "2021-07-05T01:54:57.046Z",
          "wordCount": 578,
          "title": "Concept Identification of Directly and Indirectly Related Mentions Referring to Groups of Persons. (arXiv:2107.00955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-05T01:54:57.037Z",
          "wordCount": 585,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:57.024Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:54:57.016Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:57.006Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>",
          "description": "An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.",
          "link": "http://arxiv.org/abs/2107.01068",
          "publishedOn": "2021-07-05T01:54:56.986Z",
          "wordCount": 639,
          "title": "Unsupervised Spoken Utterance Classification. (arXiv:2107.01068v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:56.956Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:56.933Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:56.896Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorsley_D/0/1/0/all/0/1\">David Thorsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "A major challenge in deploying transformer models is their prohibitive\ninference cost, which quadratically scales with the input sequence length. This\nmakes it especially difficult to use transformers for processing long\nsequences. To address this, we present a novel Learned Token Pruning (LTP)\nmethod that reduces redundant tokens as the data passes through the different\nlayers of the transformer. In particular, LTP prunes tokens with an attention\nscore below a threshold value, which is learned during training. Importantly,\nour threshold based method avoids algorithmically expensive operations such as\ntop-k token selection which are used in prior token pruning methods, and also\nleads to structured pruning. We extensively test the performance of our\napproach on multiple GLUE tasks and show that our learned threshold based\nmethod consistently outperforms the prior state-of-the-art top-k token based\nmethod by up to ~2% higher accuracy with the same amount of FLOPs. Furthermore,\nour preliminary results show up to 1.4x and 1.9x throughput improvement on\nTesla T4 GPU and Intel Haswell CPU, respectively, with less than 1% of accuracy\ndrop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch\nand has been open-sourced.",
          "link": "http://arxiv.org/abs/2107.00910",
          "publishedOn": "2021-07-05T01:54:56.867Z",
          "wordCount": 621,
          "title": "Learned Token Pruning for Transformers. (arXiv:2107.00910v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:56.804Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:56.794Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian-Cheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zi-Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan-Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_H/0/1/0/all/0/1\">Hamido Fujita</a>",
          "description": "Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing, which requires more reasoning ability and explainability.\nSpectral models based on graph convolutional networks grant the inferring\nabilities and lead to competitive results, however, part of them still face the\nchallenge of analyzing the reasoning in a human-understandable way. Inspired by\nthe concept of the Grandmother Cells in cognitive neuroscience, a spatial graph\nattention framework named crname, imitating the procedure was proposed. This\nmodel is designed to assemble the semantic features in multi-angle\nrepresentations and automatically concentrate or alleviate the information for\nreasoning. The name \"crname\" is a metaphor for the pattern of the model: regard\nthe subjects of queries as the start points of clues, take the reasoning\nentities as bridge points, and consider the latent candidate entities as the\ngrandmother cells, and the clues end up in candidate entities. The proposed\nmodel allows us to visualize the reasoning graph and analyze the importance of\nedges connecting two entities and the selectivity in the mention and candidate\nnodes, which can be easier to be comprehended empirically. The official\nevaluations in open-domain multi-hop reading dataset WikiHop and Drug-drug\nInteractions dataset MedHop prove the validity of our approach and show the\nprobability of the application of the model in the molecular biology domain.",
          "link": "http://arxiv.org/abs/2107.00841",
          "publishedOn": "2021-07-05T01:54:56.783Z",
          "wordCount": 651,
          "title": "Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:56.773Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marneffe_M/0/1/0/all/0/1\">Marie-Catherine de Marneffe</a>",
          "description": "We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.",
          "link": "http://arxiv.org/abs/2107.00807",
          "publishedOn": "2021-07-05T01:54:56.744Z",
          "wordCount": 522,
          "title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality Fails on Pragmatics. (arXiv:2107.00807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>",
          "description": "Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.",
          "link": "http://arxiv.org/abs/2107.00676",
          "publishedOn": "2021-07-05T01:54:56.663Z",
          "wordCount": 578,
          "title": "A Primer on Pretrained Multilingual Language Models. (arXiv:2107.00676v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shillingford_B/0/1/0/all/0/1\">Brendan Shillingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assael_Y/0/1/0/all/0/1\">Yannis Assael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>",
          "description": "This work describes an interactive decoding method to improve the performance\nof visual speech recognition systems using user input to compensate for the\ninherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines,\nwhich produce phonemes and feed these through a finite state transducer, our\nmethod instead expands words in lockstep, facilitating the insertion of\ninteraction points at each word position. Interaction points enable us to\nsolicit input during decoding, allowing users to interactively direct the\ndecoding process. We simulate the behavior of user input using an oracle to\ngive an automated evaluation, and show promise for the use of this method for\ntext input.",
          "link": "http://arxiv.org/abs/2107.00692",
          "publishedOn": "2021-07-05T01:54:56.570Z",
          "wordCount": 540,
          "title": "Interactive decoding of words from visual speech recognition models. (arXiv:2107.00692v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:56.469Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:56.430Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fantinuoli_C/0/1/0/all/0/1\">Claudio Fantinuoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prandi_B/0/1/0/all/0/1\">Bianca Prandi</a>",
          "description": "In recent years, automatic speech-to-speech and speech-to-text translation\nhas gained momentum thanks to advances in artificial intelligence, especially\nin the domains of speech recognition and machine translation. The quality of\nsuch applications is commonly tested with automatic metrics, such as BLEU,\nprimarily with the goal of assessing improvements of releases or in the context\nof evaluation campaigns. However, little is known about how the output of such\nsystems is perceived by end users or how they compare to human performances in\nsimilar communicative tasks.\n\nIn this paper, we present the results of an experiment aimed at evaluating\nthe quality of a real-time speech translation engine by comparing it to the\nperformance of professional simultaneous interpreters. To do so, we adopt a\nframework developed for the assessment of human interpreters and use it to\nperform a manual evaluation on both human and machine performances. In our\nsample, we found better performance for the human interpreters in terms of\nintelligibility, while the machine performs slightly better in terms of\ninformativeness. The limitations of the study and the possible enhancements of\nthe chosen framework are discussed. Despite its intrinsic limitations, the use\nof this framework represents a first step towards a user-centric and\ncommunication-oriented methodology for evaluating real-time automatic speech\ntranslation.",
          "link": "http://arxiv.org/abs/2103.08364",
          "publishedOn": "2021-07-02T01:57:59.329Z",
          "wordCount": 672,
          "title": "Towards the evaluation of automatic simultaneous speech translation from a communicative perspective. (arXiv:2103.08364v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anibal_J/0/1/0/all/0/1\">James Anibal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong-Son Nguyen</a>",
          "description": "In this paper, we propose SPBERT, a transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodeling objectives and the word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage. We investigate how SPBERT and encoder-decoder architecture can be\nadapted for Knowledge-based QA corpora. We conduct exhaustive experiments on\ntwo additional tasks, including SPARQL Query Construction and Answer\nVerbalization Generation. The experimental results show that SPBERT can obtain\npromising results, achieving state-of-the-art BLEU scores on several of these\ntasks.",
          "link": "http://arxiv.org/abs/2106.09997",
          "publishedOn": "2021-07-02T01:57:59.304Z",
          "wordCount": 554,
          "title": "SPBERT: An Efficient Pre-training BERT on SPARQL Queries for Question Answering over Knowledge Graphs. (arXiv:2106.09997v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:57:59.271Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pingali_S/0/1/0/all/0/1\">Sriram Pingali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Pratik Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>",
          "description": "The recent advancement of pre-trained Transformer models has propelled the\ndevelopment of effective text mining models across various biomedical tasks.\nHowever, these models are primarily learned on the textual data and often lack\nthe domain knowledge of the entities to capture the context beyond the\nsentence. In this study, we introduced a novel framework that enables the model\nto learn multi-omnics biological information about entities (proteins) with the\nhelp of additional multi-modal cues like molecular structure. Towards this,\nrather developing modality-specific architectures, we devise a generalized and\noptimized graph based multi-modal learning mechanism that utilizes the\nGraphBERT model to encode the textual and molecular structure information and\nexploit the underlying features of various modalities to enable end-to-end\nlearning. We evaluated our proposed method on ProteinProtein Interaction task\nfrom the biomedical corpus, where our proposed generalized approach is observed\nto be benefited by the additional domain-specific modality.",
          "link": "http://arxiv.org/abs/2107.00596",
          "publishedOn": "2021-07-02T01:57:59.262Z",
          "wordCount": 585,
          "title": "Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction. (arXiv:2107.00596v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "End-to-end DNN architectures have pushed the state-of-the-art in speech\ntechnologies, as well as in other spheres of AI, leading researchers to train\nmore complex and deeper models. These improvements came at the cost of\ntransparency. DNNs are innately opaque and difficult to interpret. We no longer\nunderstand what features are learned, where they are preserved, and how they\ninter-operate. Such an analysis is important for better model understanding,\ndebugging and to ensure fairness in ethical decision making. In this work, we\nanalyze the representations trained within deep speech models, towards the task\nof speaker recognition, dialect identification and reconstruction of masked\nsignals. We carry a layer- and neuron-level analysis on the utterance-level\nrepresentations captured within pretrained speech models for speaker, language\nand channel properties. We study: is this information captured in the learned\nrepresentations? where is it preserved? how is it distributed? and can we\nidentify a minimal subset of network that posses this information. Using\ndiagnostic classifiers, we answered these questions. Our results reveal: (i)\nchannel and gender information is omnipresent and is redundantly distributed\n(ii) complex properties such as dialectal information is encoded only in the\ntask-oriented pretrained network and is localised in the upper layers (iii) a\nminimal subset of neurons can be extracted to encode the predefined property\n(iv) salient neurons are sometimes shared between properties and can highlights\npresence of biases in the network. Our cross-architectural comparison indicates\nthat (v) the pretrained models captures speaker-invariant information and (vi)\nthe pretrained CNNs models are competitive to the Transformers for encoding\ninformation for the studied properties. To the best of our knowledge, this is\nthe first study to investigate neuron analysis on the speech models.",
          "link": "http://arxiv.org/abs/2107.00439",
          "publishedOn": "2021-07-02T01:57:59.255Z",
          "wordCount": 752,
          "title": "What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "Although attention mechanisms have been applied to a variety of deep learning\nmodels and have been shown to improve the prediction performance, it has been\nreported to be vulnerable to perturbations to the mechanism. To overcome the\nvulnerability to perturbations in the mechanism, we are inspired by adversarial\ntraining (AT), which is a powerful regularization technique for enhancing the\nrobustness of the models. In this paper, we propose a general training\ntechnique for natural language processing tasks, including AT for attention\n(Attention AT) and more interpretable AT for attention (Attention iAT). The\nproposed techniques improved the prediction performance and the model\ninterpretability by exploiting the mechanisms with AT. In particular, Attention\niAT boosts those advantages by introducing adversarial perturbation, which\nenhances the difference in the attention of the sentences. Evaluation\nexperiments with ten open datasets revealed that AT for attention mechanisms,\nespecially Attention iAT, demonstrated (1) the best performance in nine out of\nten tasks and (2) more interpretable attention (i.e., the resulting attention\ncorrelated more strongly with gradient-based word importance) for all tasks.\nAdditionally, the proposed techniques are (3) much less dependent on\nperturbation size in AT. Our code is available at\nhttps://github.com/shunk031/attention-meets-perturbation",
          "link": "http://arxiv.org/abs/2009.12064",
          "publishedOn": "2021-07-02T01:57:59.248Z",
          "wordCount": 683,
          "title": "Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training. (arXiv:2009.12064v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.",
          "link": "http://arxiv.org/abs/2107.00635",
          "publishedOn": "2021-07-02T01:57:59.240Z",
          "wordCount": 645,
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.",
          "link": "http://arxiv.org/abs/2103.15722",
          "publishedOn": "2021-07-02T01:57:59.220Z",
          "wordCount": 705,
          "title": "Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-07-02T01:57:59.211Z",
          "wordCount": 633,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geran_Y/0/1/0/all/0/1\">Yoan G&#xe9;ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laboureix_B/0/1/0/all/0/1\">Bastien Laboureix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascle_C/0/1/0/all/0/1\">Corto Mascle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_V/0/1/0/all/0/1\">Valentin D. Richard</a>",
          "description": "We introduce a new formalisation of languages, called keyboards. We consider\na set of elementary operations (writing/erasing a letter, going to the right or\nto the left,...) and we define a keyboard as a set of finite sequences of such\noperations, called keys. The corresponding language is the set of words\nobtained by applying some sequence of those keys. Unlike classical models of\ncomputation, every key can be applied anytime. We define various classes of\nlanguages based on different sets of elementary operations, and compare their\nexpressive powers. We also compare them to well-known classes of languages\n(Chomsky hierarchy). We obtain a strict hierarchy of languages, whose\nexpressivity is orthogonal to the one of the aforementionned classical models.\n\n--\n\nNous introduisons une nouvelle repr\\'esentation de langages, les claviers. On\nse munit d'un ensemble d'op\\'erations \\'el\\'ementaires (ajout, effacement d'une\nlettre, d\\'eplacement \\`a droite, \\`a gauche, ...), et on d\\'efinit un clavier\ncomme un ensemble de suites finies d'op\\'erations \\'el\\'ementaires, appel\\'ees\ntouches. Son langage sera l'ensemble des mots obtenus en appliquant une suite\nquelconque de touches. Contrairement \\`a des mod\\`eles de calcul classiques,\ntoutes les touches peuvent \\^etre appliqu\\'ees \\`a tout moment. En premier lieu\nnous d\\'efinissons diff\\'erentes classes de claviers en faisant varier\nl'ensemble des op\\'erations \\'el\\'ementaires autoris\\'ees, et nous comparons\nl'expressivit\\'e des classes de langages obtenues. Nous comparons \\'egalement\nces classes \\`a la hi\\'erarchie de Chomsky. Nous obtenons que toutes les\nclasses \\'etudi\\'ees sont diff\\'erentes, et nous caract\\'erisons les classes\ninclues dans les rationnels et les alg\\'ebriques. L'expressivit\\'e des claviers\nsemble orthogonale \\`a celle des mod\\`eles \\'evoqu\\'es pr\\'ec\\'edemment.",
          "link": "http://arxiv.org/abs/2102.10182",
          "publishedOn": "2021-07-02T01:57:59.203Z",
          "wordCount": 740,
          "title": "Keyboards as a new model of computation. (arXiv:2102.10182v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>",
          "description": "Models of narrative schema knowledge have proven useful for a range of\nevent-related tasks, but they typically do not capture the temporal\nrelationships between events. We propose a single model that addresses both\ntemporal ordering, sorting given events into the order they occurred, and event\ninfilling, predicting new events which fit into an existing temporally-ordered\nsequence. We use a BART-based conditional generation model that can capture\nboth temporality and common event co-occurrence, meaning it can be flexibly\napplied to different tasks in this space. Our model is trained as a denoising\nautoencoder: we take temporally-ordered event sequences, shuffle them, delete\nsome events, and then attempt to recover the original event sequence. This task\nteaches the model to make inferences given incomplete knowledge about the\nevents in an underlying scenario. On the temporal ordering task, we show that\nour model is able to unscramble event sequences from existing datasets without\naccess to explicitly labeled temporal training data, outperforming both a\nBERT-based pairwise model and a BERT-based pointer network. On event infilling,\nhuman evaluation shows that our model is able to generate events that fit\nbetter temporally into the input events when compared to GPT-2 story completion\nmodels.",
          "link": "http://arxiv.org/abs/2012.15786",
          "publishedOn": "2021-07-02T01:57:59.195Z",
          "wordCount": 652,
          "title": "Conditional Generation of Temporally-ordered Event Sequences. (arXiv:2012.15786v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.06605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halvani_O/0/1/0/all/0/1\">Oren Halvani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graner_L/0/1/0/all/0/1\">Lukas Graner</a>",
          "description": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.",
          "link": "http://arxiv.org/abs/2005.06605",
          "publishedOn": "2021-07-02T01:57:59.187Z",
          "wordCount": 724,
          "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis. (arXiv:2005.06605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Training the large deep neural networks that dominate NLP requires large\ndatasets. Many of these are collected automatically or via crowdsourcing, and\nmay exhibit systematic biases or annotation artifacts. By the latter, we mean\ncorrelations between inputs and outputs that are spurious, insofar as they do\nnot represent a generally held causal relationship between features and\nclasses; models that exploit such correlations may appear to perform a given\ntask well, but fail on out of sample data. In this paper we propose methods to\nfacilitate identification of training data artifacts, using new hybrid\napproaches that combine saliency maps (which highlight important input\nfeatures) with instance attribution methods (which retrieve training samples\ninfluential to a given prediction). We show that this proposed training-feature\nattribution approach can be used to uncover artifacts in training data, and use\nit to identify previously unreported artifacts in a few standard NLP datasets.\nWe execute a small user study to evaluate whether these methods are useful to\nNLP researchers in practice, with promising results. We make code for all\nmethods and experiments in this paper available.",
          "link": "http://arxiv.org/abs/2107.00323",
          "publishedOn": "2021-07-02T01:57:59.166Z",
          "wordCount": 617,
          "title": "Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>",
          "description": "Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.",
          "link": "http://arxiv.org/abs/2005.00642",
          "publishedOn": "2021-07-02T01:57:59.158Z",
          "wordCount": 640,
          "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction. (arXiv:2005.00642v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:57:59.150Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>",
          "description": "Despite pre-trained language models have proven useful for learning\nhigh-quality semantic representations, these models are still vulnerable to\nsimple perturbations. Recent works aimed to improve the robustness of\npre-trained models mainly focus on adversarial training from perturbed examples\nwith similar semantics, neglecting the utilization of different or even\nopposite semantics. Different from the image processing field, the text is\ndiscrete and few word substitutions can cause significant semantic changes. To\nstudy the impact of semantics caused by small perturbations, we conduct a\nseries of pilot experiments and surprisingly find that adversarial training is\nuseless or even harmful for the model to detect these semantic changes. To\naddress this problem, we propose Contrastive Learning with semantIc Negative\nExamples (CLINE), which constructs semantic negative examples unsupervised to\nimprove the robustness under semantically adversarial attacking. By comparing\nwith similar and opposite semantic examples, the model can effectively perceive\nthe semantic changes caused by small perturbations. Empirical results show that\nour approach yields substantial improvements on a range of sentiment analysis,\nreasoning, and reading comprehension tasks. And CLINE also ensures the\ncompactness within the same semantics and separability across different\nsemantics in sentence-level.",
          "link": "http://arxiv.org/abs/2107.00440",
          "publishedOn": "2021-07-02T01:57:59.141Z",
          "wordCount": 638,
          "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding. (arXiv:2107.00440v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>",
          "description": "Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}",
          "link": "http://arxiv.org/abs/2101.03988",
          "publishedOn": "2021-07-02T01:57:59.134Z",
          "wordCount": 619,
          "title": "Identification of COVID-19 related Fake News via Neural Stacking. (arXiv:2101.03988v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Multilingual neural machine translation has shown the capability of directly\ntranslating between language pairs unseen in training, i.e. zero-shot\ntranslation. Despite being conceptually attractive, it often suffers from low\noutput quality. The difficulty of generalizing to new translation directions\nsuggests the model representations are highly specific to those language pairs\nseen in training. We demonstrate that a main factor causing the\nlanguage-specific representations is the positional correspondence to input\ntokens. We show that this can be easily alleviated by removing residual\nconnections in an encoder layer. With this modification, we gain up to 18.5\nBLEU points on zero-shot translation while retaining quality on supervised\ndirections. The improvements are particularly prominent between related\nlanguages, where our proposed model outperforms pivot-based translation.\nMoreover, our approach allows easy integration of new languages, which\nsubstantially expands translation coverage. By thorough inspections of the\nhidden layer outputs, we show that our approach indeed leads to more\nlanguage-independent representations.",
          "link": "http://arxiv.org/abs/2012.15127",
          "publishedOn": "2021-07-02T01:57:59.115Z",
          "wordCount": 617,
          "title": "Improving Zero-Shot Translation by Disentangling Positional Information. (arXiv:2012.15127v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_P/0/1/0/all/0/1\">Pengcheng Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "This paper describes the ESPnet-ST group's IWSLT 2021 submission in the\noffline speech translation track. This year we made various efforts on training\ndata, architecture, and audio segmentation. On the data side, we investigated\nsequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech\ntranslation. Specifically, we used multi-referenced SeqKD from multiple\nteachers trained on different amounts of bitext. On the architecture side, we\nadopted the Conformer encoder and the Multi-Decoder architecture, which equips\ndedicated decoders for speech recognition and translation tasks in a unified\nencoder-decoder model and enables search in both source and target language\nspaces during inference. We also significantly improved audio segmentation by\nusing the pyannote.audio toolkit and merging multiple short segments for long\ncontext modeling. Experimental evaluations showed that each of them contributed\nto large improvements in translation performance. Our best E2E system combined\nall the above techniques with model ensembling and achieved 31.4 BLEU on the\n2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of\ntst2021.",
          "link": "http://arxiv.org/abs/2107.00636",
          "publishedOn": "2021-07-02T01:57:59.098Z",
          "wordCount": 618,
          "title": "ESPnet-ST IWSLT 2021 Offline Speech Translation System. (arXiv:2107.00636v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.",
          "link": "http://arxiv.org/abs/2107.00368",
          "publishedOn": "2021-07-02T01:57:59.088Z",
          "wordCount": 608,
          "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.",
          "link": "http://arxiv.org/abs/2107.00395",
          "publishedOn": "2021-07-02T01:57:59.069Z",
          "wordCount": 683,
          "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph. (arXiv:2107.00395v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>",
          "description": "Placeholder translation systems enable the users to specify how a specific\nphrase is translated in the output sentence. The system is trained to output\nspecial placeholder tokens, and the user-specified term is injected into the\noutput through the context-free replacement of the placeholder token. However,\nthis approach could result in ungrammatical sentences because it is often the\ncase that the specified term needs to be inflected according to the context of\nthe output, which is unknown before the translation. To address this problem,\nwe propose a novel method of placeholder translation that can inflect specified\nterms according to the grammatical construction of the output sentence. We\nextend the sequence-to-sequence architecture with a character-level decoder\nthat takes the lemma of a user-specified term and the words generated from the\nword-level decoder to output the correct inflected form of the lemma. We\nevaluate our approach with a Japanese-to-English translation task in the\nscientific writing domain, and show that our model can incorporate specified\nterms in the correct form more successfully than other comparable models.",
          "link": "http://arxiv.org/abs/2107.00334",
          "publishedOn": "2021-07-02T01:57:59.062Z",
          "wordCount": 601,
          "title": "Modeling Target-side Inflection in Placeholder Translation. (arXiv:2107.00334v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Brandon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_B/0/1/0/all/0/1\">Bailey Kuhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\nbeing studied for decades, traditional frameworks for CCA have largely relied\non overly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.",
          "link": "http://arxiv.org/abs/2107.00414",
          "publishedOn": "2021-07-02T01:57:59.034Z",
          "wordCount": 630,
          "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>",
          "description": "Despite extensive research in the past years, the computational modeling of\nargumentation remains challenging. The primary reason lies in the inherent\ncomplexity of the human processes behind, which commonly requires the\nintegration of extensive knowledge far beyond what is needed for many other\nnatural language understanding tasks. Existing work on the mining, assessment,\nreasoning, and generation of arguments acknowledges this issue, calling for\nmore research on the integration of common sense and world knowledge into\ncomputational models. However, a systematic effort to collect and organize the\ntypes of knowledge needed is still missing, hindering targeted progress in the\nfield. In this opinionated survey paper, we address the issue by (1) proposing\na pyramid of types of knowledge required in computational argumentation, (2)\nbriefly discussing the state of the art on the role and integration of these\ntypes in the field, and (3) outlining the main challenges for future work.",
          "link": "http://arxiv.org/abs/2107.00281",
          "publishedOn": "2021-07-02T01:57:59.002Z",
          "wordCount": 588,
          "title": "Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gajbhiye_A/0/1/0/all/0/1\">Amit Gajbhiye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_Manchego_F/0/1/0/all/0/1\">Fernando Alva-Manchego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obamuyide_A/0/1/0/all/0/1\">Abiola Obamuyide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "Quality Estimation (QE) is the task of automatically predicting Machine\nTranslation quality in the absence of reference translations, making it\napplicable in real-time settings, such as translating online social media\nconversations. Recent success in QE stems from the use of multilingual\npre-trained representations, where very large models lead to impressive\nresults. However, the inference time, disk and memory requirements of such\nmodels do not allow for wide usage in the real world. Models trained on\ndistilled pre-trained representations remain prohibitively large for many usage\nscenarios. We instead propose to directly transfer knowledge from a strong QE\nteacher model to a much smaller model with a different, shallower architecture.\nWe show that this approach, in combination with data augmentation, leads to\nlight-weight QE models that perform competitively with distilled pre-trained\nrepresentations with 8x fewer parameters.",
          "link": "http://arxiv.org/abs/2107.00411",
          "publishedOn": "2021-07-02T01:57:58.995Z",
          "wordCount": 569,
          "title": "Knowledge Distillation for Quality Estimation. (arXiv:2107.00411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guinovart_X/0/1/0/all/0/1\">Xavier G&#xf3;mez Guinovart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_A/0/1/0/all/0/1\">Antoni Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>",
          "description": "Language resources are necessary for language processing,but building them is\ncostly, involves many researches from different areas and needs constant\nupdating. In this paper, we describe the crosslingual framework used for\ndeveloping the Multilingual Central Repository (MCR), a multilingual knowledge\nbase that includes wordnets of Basque, Catalan, English, Galician, Portuguese,\nSpanish and the following ontologies: Base Concepts, Top Ontology, WordNet\nDomains and Suggested Upper Merged Ontology. We present the story of MCR, its\nstate in 2017 and the developed tools.",
          "link": "http://arxiv.org/abs/2107.00333",
          "publishedOn": "2021-07-02T01:57:58.897Z",
          "wordCount": 542,
          "title": "Multilingual Central Repository: a Cross-lingual Framework for Developing Wordnets. (arXiv:2107.00333v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1\">Bence Mark Halpern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritsch_J/0/1/0/all/0/1\">Julian Fritsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_E/0/1/0/all/0/1\">Enno Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1\">Rob van Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+_Doss_M/0/1/0/all/0/1\">Mathew Magimai.-Doss</a>",
          "description": "The development of pathological speech systems is currently hindered by the\nlack of a standardised objective evaluation framework. In this work, (1) we\nutilise existing detection and analysis techniques to propose a general\nframework for the consistent evaluation of synthetic pathological speech. This\nframework evaluates the voice quality and the intelligibility aspects of speech\nand is shown to be complementary using our experiments. (2) Using our proposed\nevaluation framework, we develop and test a dysarthric voice conversion system\n(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\nshow that the developed system is able to synthesise dysarthric speech with\ndifferent levels of speech intelligibility.",
          "link": "http://arxiv.org/abs/2107.00308",
          "publishedOn": "2021-07-02T01:57:58.677Z",
          "wordCount": 570,
          "title": "An Objective Evaluation Framework for Pathological Speech Synthesis. (arXiv:2107.00308v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1\">Xiangyu Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanxiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huixing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Capturing interactions among event arguments is an essential step towards\nrobust event argument extraction (EAE). However, existing efforts in this\ndirection suffer from two limitations: 1) The argument role type information of\ncontextual entities is mainly utilized as training signals, ignoring the\npotential merits of directly adopting it as semantically rich input features;\n2) The argument-level sequential semantics, which implies the overall\ndistribution pattern of argument roles over an event mention, is not well\ncharacterized. To tackle the above two bottlenecks, we formalize EAE as a\nSeq2Seq-like learning problem for the first time, where a sentence with a\nspecific event trigger is mapped to a sequence of event argument roles. A\nneural architecture with a novel Bi-directional Entity-level Recurrent Decoder\n(BERD) is proposed to generate argument roles by incorporating contextual\nentities' argument role predictions, like a word-by-word text generation\nprocess, thereby distinguishing implicit argument distribution patterns within\nan event more accurately.",
          "link": "http://arxiv.org/abs/2107.00189",
          "publishedOn": "2021-07-02T01:57:58.668Z",
          "wordCount": 594,
          "title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder. (arXiv:2107.00189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>",
          "description": "For Japanese-to-English translation, zero pronouns in Japanese pose a\nchallenge, since the model needs to infer and produce the corresponding pronoun\nin the target side of the English sentence. However, although fully resolving\nzero pronouns often needs discourse context, in some cases, the local context\nwithin a sentence gives clues to the inference of the zero pronoun. In this\nstudy, we propose a data augmentation method that provides additional training\nsignals for the translation model to learn correlations between local context\nand zero pronouns. We show that the proposed method significantly improves the\naccuracy of zero pronoun translation with machine translation experiments in\nthe conversational domain.",
          "link": "http://arxiv.org/abs/2107.00318",
          "publishedOn": "2021-07-02T01:57:58.653Z",
          "wordCount": 533,
          "title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation. (arXiv:2107.00318v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhiyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>",
          "description": "Spoken dialogue systems such as Siri and Alexa provide great convenience to\npeople's everyday life. However, current spoken language understanding (SLU)\npipelines largely depend on automatic speech recognition (ASR) modules, which\nrequire a large amount of language-specific training data. In this paper, we\npropose a Transformer-based SLU system that works directly on phones. This\nacoustic-based SLU system consists of only two blocks and does not require the\npresence of ASR module. The first block is a universal phone recognition\nsystem, and the second block is a Transformer-based language model for phones.\nWe verify the effectiveness of the system on an intent classification dataset\nin Mandarin Chinese.",
          "link": "http://arxiv.org/abs/2107.00186",
          "publishedOn": "2021-07-02T01:57:58.643Z",
          "wordCount": 546,
          "title": "Word-Free Spoken Language Understanding for Mandarin-Chinese. (arXiv:2107.00186v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengge Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>",
          "description": "This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous\nSpeech Translation task. We proposed a novel simultaneous translation model,\nCross Attention Augmented Transducer (CAAT), which extends conventional RNN-T\nto sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous\ntranslation. Experiments on speech-to-text (S2T) and text-to-text (T2T)\nsimultaneous translation tasks shows CAAT achieves better quality-latency\ntrade-offs compared to \\textit{wait-k}, one of the previous state-of-the-art\napproaches. Based on CAAT architecture and data augmentation, we build S2T and\nT2T simultaneous translation systems in this evaluation campaign. Compared to\nlast year's optimal systems, our S2T simultaneous translation system improves\nby an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous\ntranslation system improves by an average of 4.6 BLEU.",
          "link": "http://arxiv.org/abs/2107.00279",
          "publishedOn": "2021-07-02T01:57:58.634Z",
          "wordCount": 565,
          "title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021. (arXiv:2107.00279v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:57:58.602Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1\">Tal August</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1\">Sofia Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haduong_N/0/1/0/all/0/1\">Nikita Haduong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Human evaluations are typically considered the gold standard in natural\nlanguage generation, but as models' fluency improves, how well can evaluators\ndetect and judge machine-generated text? We run a study assessing non-experts'\nability to distinguish between human- and machine-authored text (GPT2 and GPT3)\nin three domains (stories, news articles, and recipes). We find that, without\ntraining, evaluators distinguished between GPT3- and human-authored text at\nrandom chance level. We explore three approaches for quickly training\nevaluators to better identify GPT3-authored text (detailed instructions,\nannotated examples, and paired examples) and find that while evaluators'\naccuracy improved up to 55%, it did not significantly improve across the three\ndomains. Given the inconsistent results across text domains and the often\ncontradictory reasons evaluators gave for their judgments, we examine the role\nuntrained human evaluations play in NLG evaluation and provide recommendations\nto NLG researchers for improving human evaluations of text generated from\nstate-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00061",
          "publishedOn": "2021-07-02T01:57:58.594Z",
          "wordCount": 594,
          "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text. (arXiv:2107.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abacha_A/0/1/0/all/0/1\">Asma Ben Abacha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>",
          "description": "The growth of online consumer health questions has led to the necessity for\nreliable and accurate question answering systems. A recent study showed that\nmanual summarization of consumer health questions brings significant\nimprovement in retrieving relevant answers. However, the automatic\nsummarization of long questions is a challenging task due to the lack of\ntraining data and the complexity of the related subtasks, such as the question\nfocus and type recognition. In this paper, we introduce a reinforcement\nlearning-based framework for abstractive question summarization. We propose two\nnovel rewards obtained from the downstream tasks of (i) question-type\nidentification and (ii) question-focus recognition to regularize the question\ngeneration model. These rewards ensure the generation of semantically valid\nquestions and encourage the inclusion of key medical entities/foci in the\nquestion summary. We evaluated our proposed method on two benchmark datasets\nand achieved higher performance over state-of-the-art models. The manual\nevaluation of the summaries reveals that the generated questions are more\ndiverse and have fewer factual inconsistencies than the baseline summaries",
          "link": "http://arxiv.org/abs/2107.00176",
          "publishedOn": "2021-07-02T01:57:58.568Z",
          "wordCount": 609,
          "title": "Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards. (arXiv:2107.00176v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.",
          "link": "http://arxiv.org/abs/2107.00080",
          "publishedOn": "2021-07-02T01:57:58.498Z",
          "wordCount": 565,
          "title": "Regressing Location on Text for Probabilistic Geocoding. (arXiv:2107.00080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_W/0/1/0/all/0/1\">William P. McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1\">Cameron Holdaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>",
          "description": "Many real-world tasks require agents to coordinate their behavior to achieve\nshared goals. Successful collaboration requires not only adopting the same\ncommunicative conventions, but also grounding these conventions in the same\ntask-appropriate conceptual abstractions. We investigate how humans use natural\nlanguage to collaboratively solve physical assembly problems more effectively\nover time. Human participants were paired up in an online environment to\nreconstruct scenes containing two block towers. One participant could see the\ntarget towers, and sent assembly instructions for the other participant to\nreconstruct. Participants provided increasingly concise instructions across\nrepeated attempts on each pair of towers, using higher-level referring\nexpressions that captured each scene's hierarchical structure. To explain these\nfindings, we extend recent probabilistic models of ad-hoc convention formation\nwith an explicit perceptual learning mechanism. These results shed light on the\ninductive biases that enable intelligent agents to coordinate upon shared\nprocedural abstractions.",
          "link": "http://arxiv.org/abs/2107.00077",
          "publishedOn": "2021-07-02T01:57:58.489Z",
          "wordCount": 579,
          "title": "Learning to communicate about shared procedural abstractions. (arXiv:2107.00077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Catala_N/0/1/0/all/0/1\">Neus Catal&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baixeries_J/0/1/0/all/0/1\">Jaume Baixeries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padro_L/0/1/0/all/0/1\">Llu&#xed;s Padr&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Fernandez_A/0/1/0/all/0/1\">Antoni Hern&#xe1;ndez-Fern&#xe1;ndez</a>",
          "description": "In his pioneering research, G. K. Zipf formulated a couple of statistical\nlaws on the relationship between the frequency of a word with its number of\nmeanings: the law of meaning distribution, relating the frequency of a word and\nits frequency rank, and the meaning-frequency law, relating the frequency of a\nword with its number of meanings. Although these laws were formulated more than\nhalf a century ago, they have been only investigated in a few languages. Here\nwe present the first study of these laws in Catalan.\n\nWe verify these laws in Catalan via the relationship among their exponents\nand that of the rank-frequency law. We present a new protocol for the analysis\nof these Zipfian laws that can be extended to other languages. We report the\nfirst evidence of two marked regimes for these laws in written language and\nspeech, paralleling the two regimes in Zipf's rank-frequency law in large\nmulti-author corpora discovered in early 2000s. Finally, the implications of\nthese two regimes will be discussed.",
          "link": "http://arxiv.org/abs/2107.00042",
          "publishedOn": "2021-07-02T01:57:58.444Z",
          "wordCount": 603,
          "title": "Zipf's laws of meaning in Catalan. (arXiv:2107.00042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Ashwinkumar Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1\">Tim Oates</a>",
          "description": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.",
          "link": "http://arxiv.org/abs/2107.00124",
          "publishedOn": "2021-07-02T01:57:58.400Z",
          "wordCount": 555,
          "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment. (arXiv:2107.00124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>",
          "description": "We investigate the less-explored task of generating open-ended questions that\nare typically answered by multiple sentences. We first define a new question\ntype ontology which differentiates the nuanced nature of questions better than\nwidely used question words. A new dataset with 4,959 questions is labeled based\non the new ontology. We then propose a novel question type-aware question\ngeneration framework, augmented by a semantic graph representation, to jointly\npredict question focuses and produce the question. Based on this framework, we\nfurther use both exemplars and automatically generated templates to improve\ncontrollability and diversity. Experiments on two newly collected large-scale\ndatasets show that our model improves question quality over competitive\ncomparisons based on automatic metrics. Human judges also rate our model\noutputs highly in answerability, coverage of scope, and overall quality.\nFinally, our model variants with templates can produce questions with enhanced\ncontrollability and diversity.",
          "link": "http://arxiv.org/abs/2107.00152",
          "publishedOn": "2021-07-02T01:57:58.365Z",
          "wordCount": 578,
          "title": "Controllable Open-ended Question Generation with A New Question Type Ontology. (arXiv:2107.00152v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Keli Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Siyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>",
          "description": "Despite the great success in Natural Language Processing (NLP) area, large\npre-trained language models like BERT are not well-suited for\nresource-constrained or real-time applications owing to the large number of\nparameters and slow inference speed. Recently, compressing and accelerating\nBERT have become important topics. By incorporating a parameter-sharing\nstrategy, ALBERT greatly reduces the number of parameters while achieving\ncompetitive performance. Nevertheless, ALBERT still suffers from a long\ninference time. In this work, we propose the ELBERT, which significantly\nimproves the average inference speed compared to ALBERT due to the proposed\nconfidence-window based early exit mechanism, without introducing additional\nparameters or extra training overhead. Experimental results show that ELBERT\nachieves an adaptive inference speedup varying from 2$\\times$ to 10$\\times$\nwith negligible accuracy degradation compared to ALBERT on various datasets.\nBesides, ELBERT achieves higher accuracy than existing early exit methods used\nfor accelerating BERT under the same computation cost. Furthermore, to\nunderstand the principle of the early exit mechanism, we also visualize the\ndecision-making process of it in ELBERT.",
          "link": "http://arxiv.org/abs/2107.00175",
          "publishedOn": "2021-07-02T01:57:58.312Z",
          "wordCount": 599,
          "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit. (arXiv:2107.00175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zijun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>",
          "description": "Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.",
          "link": "http://arxiv.org/abs/2106.16038",
          "publishedOn": "2021-07-01T01:59:34.663Z",
          "wordCount": 613,
          "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information. (arXiv:2106.16038v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1\">Robert Zev Mahari</a>",
          "description": "This paper demonstrate how NLP can be used to address an unmet need of the\nlegal community and increase access to justice. The paper introduces Legal\nPrecedent Prediction (LPP), the task of predicting relevant passages from\nprecedential court decisions given the context of a legal argument. To this\nend, the paper showcases a BERT model, trained on 530,000 examples of legal\narguments made by U.S. federal judges, to predict relevant passages from\nprecedential court decisions given the context of a legal argument. In 96% of\nunseen test examples the correct target passage is among the top-10 predicted\npassages. The same model is able to predict relevant precedent given a short\nsummary of a complex and unseen legal brief, predicting the precedent that was\nactually cited by the brief's co-author, former U.S. Solicitor General and\ncurrent U.S. Supreme Court Justice Elena Kagan.",
          "link": "http://arxiv.org/abs/2106.16034",
          "publishedOn": "2021-07-01T01:59:34.535Z",
          "wordCount": 568,
          "title": "AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction. (arXiv:2106.16034v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.123Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech\nTranslation Shared Task by IMS team. We utilize state-of-the-art models\ncombined with several data augmentation, multi-task and transfer learning\napproaches for the automatic speech recognition (ASR) and machine translation\n(MT) steps of our cascaded system. Moreover, we also explore the feasibility of\na full end-to-end speech translation (ST) model in the case of very constrained\namount of ground truth labeled data. Our best system achieves the best\nperformance among all submitted systems for Congolese Swahili to English and\nFrench with BLEU scores 7.7 and 13.7 respectively, and the second best result\nfor Coastal Swahili to English with BLEU score 14.9.",
          "link": "http://arxiv.org/abs/2106.16055",
          "publishedOn": "2021-07-01T01:59:34.116Z",
          "wordCount": 559,
          "title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task. (arXiv:2106.16055v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egetenmeyer_J/0/1/0/all/0/1\">Jakob Egetenmeyer</a>",
          "description": "German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.",
          "link": "http://arxiv.org/abs/2106.15872",
          "publishedOn": "2021-07-01T01:59:34.081Z",
          "wordCount": 652,
          "title": "Genre determining prediction: Non-standard TAM marking in football language. (arXiv:2106.15872v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1\">Sohail Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>",
          "description": "Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.",
          "link": "http://arxiv.org/abs/2106.15896",
          "publishedOn": "2021-07-01T01:59:33.016Z",
          "wordCount": 686,
          "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection. (arXiv:2106.15896v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:32.858Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.",
          "link": "http://arxiv.org/abs/2106.16138",
          "publishedOn": "2021-07-01T01:59:32.852Z",
          "wordCount": 507,
          "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1\">Robert M. Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>",
          "description": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set to a cross-topic/open-set AV\ntask over a collection of fanfiction texts. In this work, we present our\nmodified hybrid neural-probabilistic framework. It is based on our 2020 winning\nsubmission, with updates to significantly reduce sensitivities to topical\nvariations and to further improve the system's calibration by means of an\nuncertainty-adaptation layer. Our framework additionally includes an\nOut-Of-Distribution Detector (O2D2) for defining non-responses, outperforming\nall other systems that participated in the PAN 2021 AV task.",
          "link": "http://arxiv.org/abs/2106.15825",
          "publishedOn": "2021-07-01T01:59:32.847Z",
          "wordCount": 528,
          "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:32.824Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>",
          "description": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 17\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
          "link": "http://arxiv.org/abs/2106.03706",
          "publishedOn": "2021-07-01T01:59:32.792Z",
          "wordCount": 658,
          "title": "A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:32.765Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.",
          "link": "http://arxiv.org/abs/2106.15760",
          "publishedOn": "2021-07-01T01:59:32.756Z",
          "wordCount": 588,
          "title": "A Conditional Splitting Framework for Efficient Constituency Parsing. (arXiv:2106.15760v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:32.577Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.518Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:32.475Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "Early risk detection of mental illnesses has a massive positive impact upon\nthe well-being of people. The eRisk workshop has been at the forefront of\nenabling interdisciplinary research in developing computational methods to\nautomatically estimate early risk factors for mental issues such as depression,\nself-harm, anorexia and pathological gambling. In this paper, we present the\ncontributions of the BLUE team in the 2021 edition of the workshop, in which we\ntackle the problems of early detection of gambling addiction, self-harm and\nestimating depression severity from social media posts. We employ pre-trained\nBERT transformers and data crawled automatically from mental health subreddits\nand obtain reasonable results on all three tasks.",
          "link": "http://arxiv.org/abs/2106.16175",
          "publishedOn": "2021-07-01T01:59:32.469Z",
          "wordCount": 564,
          "title": "Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT. (arXiv:2106.16175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Shen-Yun Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao-Chun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Keh-Yih Su</a>",
          "description": "We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms\nof both language patterns and problem types) English math word problem (MWP)\ncorpus for evaluating the capability of various MWP solvers. Existing MWP\ncorpora for studying AI progress remain limited either in language usage\npatterns or in problem types. We thus present a new English MWP corpus with\n2,305 MWPs that cover more text patterns and most problem types taught in\nelementary school. Each MWP is annotated with its problem type and grade level\n(for indicating the level of difficulty). Furthermore, we propose a metric to\nmeasure the lexicon usage diversity of a given MWP corpus, and demonstrate that\nASDiv is more diverse than existing corpora. Experiments show that our proposed\ncorpus reflects the true capability of MWP solvers more faithfully.",
          "link": "http://arxiv.org/abs/2106.15772",
          "publishedOn": "2021-07-01T01:59:32.440Z",
          "wordCount": 574,
          "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. (arXiv:2106.15772v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "In neural machine translation, cross entropy (CE) is the standard loss\nfunction in two training methods of auto-regressive models, i.e., teacher\nforcing and scheduled sampling. In this paper, we propose mixed cross entropy\nloss (mixed CE) as a substitute for CE in both training approaches. In teacher\nforcing, the model trained with CE regards the translation problem as a\none-to-one mapping process, while in mixed CE this process can be relaxed to\none-to-many. In scheduled sampling, we show that mixed CE has the potential to\nencourage the training and testing behaviours to be similar to each other, more\neffectively mitigating the exposure bias problem. We demonstrate the\nsuperiority of mixed CE over CE on several machine translation datasets, WMT'16\nRo-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled\nsampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE\nconsistently outperforms CE on a multi-reference set as well as a challenging\nparaphrased reference set. We also found the model trained with mixed CE is\nable to provide a better probability distribution defined over the translation\noutput space. Our code is available at https://github.com/haorannlp/mix.",
          "link": "http://arxiv.org/abs/2106.15880",
          "publishedOn": "2021-07-01T01:59:32.425Z",
          "wordCount": 619,
          "title": "Mixed Cross Entropy Loss for Neural Machine Translation. (arXiv:2106.15880v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenson_M/0/1/0/all/0/1\">McKenzie Steenson</a>",
          "description": "Automated speech and text interfaces are continuing to improve, resulting in\nincreased research in the area of dialogue systems. Moreover, conferences and\nworkshops from various fields are focusing more on language through speech and\ntext mediums as candidates for interaction with applications such as search\ninterfaces and robots. In this paper, we explore how visible the SigDial\nconference is to outside conferences by analysing papers from top Natural\nLangauge Processing conferences since 2015 to determine the popularity of\ncertain SigDial-related topics, as well as analysing what SigDial papers are\nbeing cited by others outside of SigDial. We find that despite a dramatic\nincrease in dialogue-related research, SigDial visibility has not increased. We\nconclude by offering some suggestions.",
          "link": "http://arxiv.org/abs/2106.16196",
          "publishedOn": "2021-07-01T01:59:32.007Z",
          "wordCount": 548,
          "title": "An Analysis of the Recent Visibility of the SigDial Conference. (arXiv:2106.16196v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:31.930Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilal_I/0/1/0/all/0/1\">Iman Munire Bilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>",
          "description": "Collecting together microblogs representing opinions about the same topics\nwithin the same timeframe is useful to a number of different tasks and\npractitioners. A major question is how to evaluate the quality of such thematic\nclusters. Here we create a corpus of microblog clusters from three different\ndomains and time windows and define the task of evaluating thematic coherence.\nWe provide annotation guidelines and human annotations of thematic coherence by\njournalist experts. We subsequently investigate the efficacy of different\nautomated evaluation metrics for the task. We consider a range of metrics\nincluding surface level metrics, ones for topic model coherence and text\ngeneration metrics (TGMs). While surface level metrics perform well,\noutperforming topic coherence metrics, they are not as consistent as TGMs. TGMs\nare more reliable than all other metrics considered for capturing thematic\ncoherence in microblog clusters due to being less sensitive to the effect of\ntime windows.",
          "link": "http://arxiv.org/abs/2106.15971",
          "publishedOn": "2021-07-01T01:59:31.923Z",
          "wordCount": 592,
          "title": "Evaluation of Thematic Coherence in Microblogs. (arXiv:2106.15971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:31.768Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Josiah Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Josiel Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_C/0/1/0/all/0/1\">Chiraag Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.",
          "link": "http://arxiv.org/abs/2103.01910",
          "publishedOn": "2021-07-01T01:59:31.758Z",
          "wordCount": 695,
          "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhiyuan_W/0/1/0/all/0/1\">Wen Zhiyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiannong_C/0/1/0/all/0/1\">Cao Jiannong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruosong_Y/0/1/0/all/0/1\">Yang Ruosong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuaiqi_L/0/1/0/all/0/1\">Liu Shuaiqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaxing_S/0/1/0/all/0/1\">Shen Jiaxing</a>",
          "description": "To provide consistent emotional interaction with users, dialog systems should\nbe capable to automatically select appropriate emotions for responses like\nhumans. However, most existing works focus on rendering specified emotions in\nresponses or empathetically respond to the emotion of users, yet the individual\ndifference in emotion expression is overlooked. This may lead to inconsistent\nemotional expressions and disinterest users. To tackle this issue, we propose\nto equip the dialog system with personality and enable it to automatically\nselect emotions in responses by simulating the emotion transition of humans in\nconversation. In detail, the emotion of the dialog system is transitioned from\nits preceding emotion in context. The transition is triggered by the preceding\ndialog context and affected by the specified personality trait. To achieve\nthis, we first model the emotion transition in the dialog system as the\nvariation between the preceding emotion and the response emotion in the\nValence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks\nto encode the preceding dialog context and the specified personality traits to\ncompose the variation. Finally, the emotion for response is selected from the\nsum of the preceding emotion and the variation. We construct a dialog dataset\nwith emotion and personality labels and conduct emotion prediction tasks for\nevaluation. Experimental results validate the effectiveness of the\npersonality-affected emotion transition.",
          "link": "http://arxiv.org/abs/2106.15846",
          "publishedOn": "2021-07-01T01:59:31.750Z",
          "wordCount": 663,
          "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition. (arXiv:2106.15846v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1\">Jian Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 8.1 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We will\npublish our code and model to facilitate both future research works and\nindustrial applications.\n\nThis paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 7.9 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We release\nour code and model at\n\\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to\nfacilitate both future research works and industrial applications.",
          "link": "http://arxiv.org/abs/2105.07319",
          "publishedOn": "2021-07-01T01:59:31.741Z",
          "wordCount": 701,
          "title": "The Volctrans Neural Speech Translation System for IWSLT 2021. (arXiv:2105.07319v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>",
          "description": "Whereas existing literature on unsupervised machine translation (MT) focuses\non exploiting unsupervised techniques for low-resource language pairs where\nbilingual training data is scare or unavailable, we investigate whether\nunsupervised MT can also improve translation quality of high-resource language\npairs where sufficient bitext does exist. We compare the style of correct\ntranslations generated by either supervised or unsupervised MT and find that\nthe unsupervised output is less monotonic and more natural than supervised\noutput. We demonstrate a way to combine the benefits of unsupervised and\nsupervised MT into a single system, resulting in better human evaluation of\nquality and fluency. Our results open the door to discussions about the\npotential contributions of unsupervised MT in high-resource settings, and how\nsupervised and unsupervised systems might be mutually-beneficial.",
          "link": "http://arxiv.org/abs/2106.15818",
          "publishedOn": "2021-07-01T01:59:31.718Z",
          "wordCount": 559,
          "title": "What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?. (arXiv:2106.15818v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.712Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "One of the main challenges of the machine reading comprehension (MRC) models\nis their fragile out-of-domain generalization, which makes these models not\nproperly applicable to real-world general-purpose question answering problems.\nIn this paper, we leverage a zero-shot weighted ensemble method for improving\nthe robustness of out-of-domain generalization in MRC models. In the proposed\nmethod, a weight estimation module is used to estimate out-of-domain weights,\nand an ensemble module aggregate several base models' predictions based on\ntheir weights. The experiments indicate that the proposed method not only\nimproves the final accuracy, but also is robust against domain changes.",
          "link": "http://arxiv.org/abs/2106.16013",
          "publishedOn": "2021-07-01T01:59:31.705Z",
          "wordCount": 541,
          "title": "Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine Reading Comprehension Systems for Robust Generalization. (arXiv:2106.16013v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for low-resource\nlanguages. Cross-lingual embeddings map word embeddings from a low-resource\nlanguage to a high-resource language so that a prediction model trained on data\nfrom the high-resource language can also be used in the low-resource language.\nTo produce cross-lingual mappings of recent contextual embeddings, anchor\npoints between the embedding spaces have to be words in the same context. We\naddress this issue with a new method for creating datasets for cross-lingual\ncontextual alignments. Based on that, we propose novel cross-lingual mapping\nmethods for ELMo embeddings. Our linear mapping methods use existing vecmap and\nMUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN\nmapping method is based on GANs and does not assume isomorphic embedding\nspaces. We evaluate the proposed mapping methods on nine languages, using two\ndownstream tasks, NER and dependency parsing. The ELMoGAN method performs well\non the NER task, with low cross-lingual loss compared to direct training on\nsome languages. In the dependency parsing, linear alignment variants are more\nsuccessful.",
          "link": "http://arxiv.org/abs/2106.15986",
          "publishedOn": "2021-07-01T01:59:31.698Z",
          "wordCount": 611,
          "title": "Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>",
          "description": "Text-to-Graph extraction aims to automatically extract information graphs\nconsisting of mentions and types from natural language texts. Existing\napproaches, such as table filling and pairwise scoring, have shown impressive\nperformance on various information extraction tasks, but they are difficult to\nscale to datasets with longer input texts because of their second-order\nspace/time complexities with respect to the input length. In this work, we\npropose a Hybrid Span Generator (HySPA) that invertibly maps the information\ngraph to an alternating sequence of nodes and edge types, and directly\ngenerates such sequences via a hybrid span decoder which can decode both the\nspans and the types recurrently in linear time and space complexities.\nExtensive experiments on the ACE05 dataset show that our approach also\nsignificantly outperforms state-of-the-art on the joint entity and relation\nextraction task.",
          "link": "http://arxiv.org/abs/2106.15838",
          "publishedOn": "2021-07-01T01:59:31.691Z",
          "wordCount": 569,
          "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction. (arXiv:2106.15838v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>",
          "description": "Machine translation systems are vulnerable to domain mismatch, especially\nwhen the task is low-resource. In this setting, out of domain translations are\noften of poor quality and prone to hallucinations, due to the translation model\npreferring to predict common words it has seen during training, as opposed to\nthe more uncommon ones from a different domain. We present two simple methods\nfor improving translation quality in this particular setting: First, we use\nlexical shortlisting in order to restrict the neural network predictions by IBM\nmodel computed alignments. Second, we perform $n$-best list reordering by\nreranking all translations based on the amount they overlap with each other.\nOur methods are computationally simpler and faster than alternative approaches,\nand show a moderate success on low-resource settings with explicit out of\ndomain test sets. However, our methods lose their effectiveness when the domain\nmismatch is too great, or in high resource setting.",
          "link": "http://arxiv.org/abs/2101.00421",
          "publishedOn": "2021-07-01T01:59:31.657Z",
          "wordCount": 606,
          "title": "Decoding Time Lexical Domain Adaptation for Neural Machine Translation. (arXiv:2101.00421v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1\">Alexander D&#x27;Amour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>",
          "description": "Experiments with pretrained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact (i.e., the\nparticular instance of the model), it is not always clear whether they hold for\nthe more general procedure (which includes the model architecture, training\ndata, initialization scheme, and loss function). Recent work has shown that\nre-running pretraining can lead to substantially different conclusions about\nperformance, suggesting that alternative evaluations are needed to make\nprincipled statements about procedures. To address this question, we introduce\nMultiBERTs: a set of 25 BERT-base checkpoints, trained with similar\nhyper-parameters as the original BERT model but differing in random\ninitialization and data shuffling. The aim is to enable researchers to draw\nrobust and statistically justified conclusions about pretraining procedures.\nThe full release includes 25 fully trained checkpoints, as well as statistical\nguidelines and a code library implementing our recommended hypothesis testing\nmethods. Finally, for five of these models we release a set of 28 intermediate\ncheckpoints in order to support research on learning dynamics.",
          "link": "http://arxiv.org/abs/2106.16163",
          "publishedOn": "2021-07-01T01:59:31.644Z",
          "wordCount": 624,
          "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>",
          "description": "We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.",
          "link": "http://arxiv.org/abs/2106.15919",
          "publishedOn": "2021-07-01T01:59:31.560Z",
          "wordCount": 582,
          "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1\">Julian Hough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>",
          "description": "We present two multimodal fusion-based deep learning models that consume ASR\ntranscribed speech and acoustic data simultaneously to classify whether a\nspeaker in a structured diagnostic task has Alzheimer's Disease and to what\ndegree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM\nwith highway layers using words, word probabilities, disfluency features, pause\ninformation, and a variety of acoustic features, achieves an accuracy of 84%\nand RSME error prediction of 4.26 on MMSE cognitive scores. While predicting\ncognitive decline is more challenging, our models show improvement using the\nmultimodal approach and word probabilities, disfluency and pause information\nover word-only models. We show considerable gains for AD classification using\nmultimodal fusion and gating, which can effectively deal with noisy inputs from\nacoustic features and ASR hypotheses.",
          "link": "http://arxiv.org/abs/2106.15684",
          "publishedOn": "2021-07-01T01:59:31.528Z",
          "wordCount": 592,
          "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs. (arXiv:2106.15684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>",
          "description": "Despite their success, large pre-trained multilingual models have not\ncompletely alleviated the need for labeled data, which is cumbersome to collect\nfor all target languages. Zero-shot cross-lingual transfer is emerging as a\npractical solution: pre-trained models later fine-tuned on one transfer\nlanguage exhibit surprising performance when tested on many target languages.\nEnglish is the dominant source language for transfer, as reinforced by popular\nzero-shot benchmarks. However, this default choice has not been systematically\nvetted. In our study, we compare English against other transfer languages for\nfine-tuning, on two pre-trained multilingual models (mBERT and mT5) and\nmultiple classification and question answering tasks. We find that other\nhigh-resource languages such as German and Russian often transfer more\neffectively, especially when the set of target languages is diverse or unknown\na priori. Unexpectedly, this can be true even when the training sets were\nautomatically translated from English. This finding can have immediate impact\non multilingual zero-shot systems, and should inform future benchmark designs.",
          "link": "http://arxiv.org/abs/2106.16171",
          "publishedOn": "2021-07-01T01:59:31.491Z",
          "wordCount": 596,
          "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer. (arXiv:2106.16171v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we will release our code\nas well as reference implementation of those baseline models in the final\nversion.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-07-06T01:58:06.138Z",
          "wordCount": 642,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>",
          "description": "WikiKG90M in KDD Cup 2021 is a large encyclopedic knowledge graph, which\ncould benefit various downstream applications such as question answering and\nrecommender systems. Participants are invited to complete the knowledge graph\nby predicting missing triplets. Recent representation learning methods have\nachieved great success on standard datasets like FB15k-237. Thus, we train the\nadvanced algorithms in different domains to learn the triplets, including OTE,\nQuatE, RotatE and TransE. Significantly, we modified OTE into NOTE (short for\nNorm-OTE) for better performance. Besides, we use both the DeepWalk and the\npost-smoothing technique to capture the graph structure for supplementation. In\naddition to the representations, we also use various statistical probabilities\namong the head entities, the relations and the tail entities for the final\nprediction. Experimental results show that the ensemble of state-of-the-art\nrepresentation learning methods could draw on each others strengths. And we\ndevelop feature engineering from validation candidates for further\nimprovements. Please note that we apply the same strategy on the test set for\nfinal inference. And these features may not be practical in the real world when\nconsidering ranking against all the entities.",
          "link": "http://arxiv.org/abs/2107.01892",
          "publishedOn": "2021-07-06T01:58:06.039Z",
          "wordCount": 637,
          "title": "NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC. (arXiv:2107.01892v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.14531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Draws_T/0/1/0/all/0/1\">Tim Draws</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tintarev_N/0/1/0/all/0/1\">Nava Tintarev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadiraju_U/0/1/0/all/0/1\">Ujwal Gadiraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozzon_A/0/1/0/all/0/1\">Alessandro Bozzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmermans_B/0/1/0/all/0/1\">Benjamin Timmermans</a>",
          "description": "The way pages are ranked in search results influences whether the users of\nsearch engines are exposed to more homogeneous, or rather to more diverse\nviewpoints. However, this viewpoint diversity is not trivial to assess. In this\npaper we use existing and novel ranking fairness metrics to evaluate viewpoint\ndiversity in search result rankings. We conduct a controlled simulation study\nthat shows how ranking fairness metrics can be used for viewpoint diversity,\nhow their outcome should be interpreted, and which metric is most suitable\ndepending on the situation. This paper lays out important ground work for\nfuture research to measure and assess viewpoint diversity in real search result\nrankings.",
          "link": "http://arxiv.org/abs/2010.14531",
          "publishedOn": "2021-07-06T01:58:05.948Z",
          "wordCount": 589,
          "title": "Assessing Viewpoint Diversity in Search Results Using Ranking Fairness Metrics. (arXiv:2010.14531v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Modelling mix-and-match relationships among fashion items has become\nincreasingly demanding yet challenging for modern E-commerce recommender\nsystems. When performing clothes matching, most existing approaches leverage\nthe latent visual features extracted from fashion item images for compatibility\nmodelling, which lacks explainability of generated matching results and can\nhardly convince users of the recommendations. Though recent methods start to\nincorporate pre-defined attribute information (e.g., colour, style, length,\netc.) for learning item representations and improving the model\ninterpretability, their utilisation of attribute information is still mainly\nreserved for enhancing the learned item representations and generating\nexplanations via post-processing. As a result, this creates a severe bottleneck\nwhen we are trying to advance the recommendation accuracy and generating\nfine-grained explanations since the explicit attributes have only loose\nconnections to the actual recommendation process. This work aims to tackle the\nexplainability challenge in fashion recommendation tasks by proposing a novel\nAttribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender\nassesses the outfit compatibility by explicitly leveraging the extracted\nattribute-level representations from each item's visual feature. The attributes\nserve as the bridge between two fashion items, where we quantify the affinity\nof a pair of items through the learned compatibility between their attributes.\nExtensive experiments have demonstrated that, by making full use of the\nexplicit attributes in the recommendation process, AFRec is able to achieve\nstate-of-the-art recommendation accuracy and generate intuitive explanations at\nthe same time.",
          "link": "http://arxiv.org/abs/2107.01655",
          "publishedOn": "2021-07-06T01:58:05.936Z",
          "wordCount": 654,
          "title": "Attribute-aware Explainable Complementary Clothing Recommendation. (arXiv:2107.01655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yakhchi_S/0/1/0/all/0/1\">Shahpar Yakhchi</a>",
          "description": "Recommender systems (RSs) have emerged as very useful tools to help customers\nwith their decision-making process, find items of their interest, and alleviate\nthe information overload problem. There are two different lines of approaches\nin RSs: (1) general recommenders with the main goal of discovering long-term\nusers' preferences, and (2) sequential recommenders with the main focus of\ncapturing short-term users' preferences in a session of user-item interaction\n(here, a session refers to a record of purchasing multiple items in one\nshopping event). While considering short-term users' preferences may satisfy\ntheir current needs and interests, long-term users' preferences provide users\nwith the items that they may interact with, eventually. In this thesis, we\nfirst focus on improving the performance of general RSs. Most of the existing\ngeneral RSs tend to exploit the users' rating patterns on common items to\ndetect similar users. The data sparsity problem (i.e. the lack of available\ninformation) is one of the major challenges for the current general RSs, and\nthey may fail to have any recommendations when there are no common items of\ninterest among users. We call this problem data sparsity with no feedback on\ncommon items (DSW-n-FCI). To overcome this problem, we propose a\npersonality-based RS in which similar users are identified based on the\nsimilarity of their personality traits.",
          "link": "http://arxiv.org/abs/2107.01529",
          "publishedOn": "2021-07-06T01:58:05.483Z",
          "wordCount": 655,
          "title": "Learning Complex Users' Preferences for Recommender Systems. (arXiv:2107.01529v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method outperforms the existing methods on real-world benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-07-06T01:58:05.449Z",
          "wordCount": 540,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingjing_L/0/1/0/all/0/1\">Li Jingjing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "Different from the traditional recommender system, the session-based\nrecommender system introduces the concept of the session, i.e., a sequence of\ninteractions between a user and multiple items within a period, to preserve the\nuser's recent interest. The existing work on the session-based recommender\nsystem mainly relies on mining sequential patterns within individual sessions,\nwhich are not expressive enough to capture more complicated dependency\nrelationships among items. In addition, it does not consider the cross-session\ninformation due to the anonymity of the session data, where the linkage between\ndifferent sessions is prevented. In this paper, we solve these problems with\nthe graph neural networks technique. First, each session is represented as a\ngraph rather than a linear sequence structure, based on which a novel Full\nGraph Neural Network (FGNN) is proposed to learn complicated item dependency.\nTo exploit and incorporate cross-session information in the individual\nsession's representation learning, we further construct a Broadly Connected\nSession (BCS) graph to link different sessions and a novel Mask-Readout\nfunction to improve session embedding based on the BCS graph. Extensive\nexperiments have been conducted on two e-commerce benchmark datasets, i.e.,\nYoochoose and Diginetica, and the experimental results demonstrate the\nsuperiority of our proposal through comparisons with state-of-the-art\nsession-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00852",
          "publishedOn": "2021-07-05T01:54:56.559Z",
          "wordCount": 651,
          "title": "Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_M/0/1/0/all/0/1\">Malte Brockmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yawen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pateer_S/0/1/0/all/0/1\">Sunita Pateer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "Modern large-scale knowledge graphs, such as DBpedia, are datasets which\nrequire large computational resources to serve and process. Moreover, they\noften have longer release cycles, which leads to outdated information in those\ngraphs. In this paper, we present DBpedia on Demand -- a system which serves\nDBpedia resources on demand without the need to materialize and store the\nentire graph, and which even provides limited querying functionality.",
          "link": "http://arxiv.org/abs/2107.00873",
          "publishedOn": "2021-07-05T01:54:56.528Z",
          "wordCount": 516,
          "title": "On-Demand and Lightweight Knowledge Graph Generation -- a Demonstration with DBpedia. (arXiv:2107.00873v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Chen Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "For present e-commerce platforms, session-based recommender systems are\ndeveloped to predict users' preference for next-item recommendation. Although a\nsession can usually reflect a user's current preference, a local shift of the\nuser's intention within the session may still exist. Specifically, the\ninteractions that take place in the early positions within a session generally\nindicate the user's initial intention, while later interactions are more likely\nto represent the latest intention. Such positional information has been rarely\nconsidered in existing methods, which restricts their ability to capture the\nsignificance of interactions at different positions. To thoroughly exploit the\npositional information within a session, a theoretical framework is developed\nin this paper to provide an in-depth analysis of the positional information. We\nformally define the properties of forward-awareness and backward-awareness to\nevaluate the ability of positional encoding schemes in capturing the initial\nand the latest intention. According to our analysis, existing positional\nencoding schemes are generally forward-aware only, which can hardly represent\nthe dynamics of the intention in a session. To enhance the positional encoding\nscheme for the session-based recommendation, a dual positional encoding (DPE)\nis proposed to account for both forward-awareness and backward-awareness. Based\non DPE, we propose a novel Positional Recommender (PosRec) model with a\nwell-designed Position-aware Gated Graph Neural Network module to fully exploit\nthe positional information for session-based recommendation tasks. Extensive\nexperiments are conducted on two e-commerce benchmark datasets, Yoochoose and\nDiginetica and the experimental results show the superiority of the PosRec by\ncomparing it with the state-of-the-art session-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00846",
          "publishedOn": "2021-07-05T01:54:56.486Z",
          "wordCount": 680,
          "title": "Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:56.398Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigdel_M/0/1/0/all/0/1\">Madhav Sigdel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Phuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korayem_M/0/1/0/all/0/1\">Mohammed Korayem</a>",
          "description": "The online recruitment matching system has been the core technology and\nservice platform in CareerBuilder. One of the major challenges in an online\nrecruitment scenario is to provide good matches between job posts and\ncandidates using a recommender system on the scale. In this paper, we discussed\nthe techniques for applying an embedding-based recommender system for the large\nscale of job to candidates matching. To learn the comprehensive and effective\nembedding for job posts and candidates, we have constructed a fused-embedding\nvia different levels of representation learning from raw text, semantic\nentities and location information. The clusters of fused-embedding of job and\ncandidates are then used to build and train the Faiss index that supports\nruntime approximate nearest neighbor search for candidate retrieval. After the\nfirst stage of candidate retrieval, a second stage reranking model that\nutilizes other contextual information was used to generate the final matching\nresult. Both offline and online evaluation results indicate a significant\nimprovement of our proposed two-staged embedding-based system in terms of\nclick-through rate (CTR), quality and normalized discounted accumulated gain\n(nDCG), compared to those obtained from our baseline system. We further\ndescribed the deployment of the system that supports the million-scale job and\ncandidate matching process at CareerBuilder. The overall improvement of our job\nto candidate matching system has demonstrated its feasibility and scalability\nat a major online recruitment site.",
          "link": "http://arxiv.org/abs/2107.00221",
          "publishedOn": "2021-07-02T01:57:58.582Z",
          "wordCount": 668,
          "title": "Embedding-based Recommender System for Job to Candidate Matching on Scale. (arXiv:2107.00221v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_P/0/1/0/all/0/1\">Parul Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_G/0/1/0/all/0/1\">Geetha Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gulshan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Kiran Sharma</a>",
          "description": "Bibliometrics is useful to analyze the research impact for measuring the\nresearch quality. Different bibliographic databases like Scopus, Web of\nScience, Google Scholar etc. are accessed for evaluating the trend of\npublications and citations from time to time. Some of these databases are free\nand some are subscription based. Its always debatable that which bibliographic\ndatabase is better and in what terms. To provide an optimal solution to\navailability of multiple bibliographic databases, we have implemented a single\nauthentic database named as ``conflate'' which can be used for fetching\npublication and citation trend of an author. To further strengthen the\ngenerated database and to provide the transparent system to the stakeholders, a\nconsensus mechanism ``proof of reference (PoR)'' is proposed. Due to three\nconsent based checks implemented in PoR, we feel that it could be considered as\na authentic and honest citation data source for the calculation of unified\ninformetrics for an author.",
          "link": "http://arxiv.org/abs/2107.00214",
          "publishedOn": "2021-07-02T01:57:58.507Z",
          "wordCount": 594,
          "title": "Proof of Reference(PoR): A unified informetrics based consensus mechanism. (arXiv:2107.00214v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xinlin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Songlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sulong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen-Yun Yang</a>",
          "description": "Graph convolution networks (GCN), which recently becomes new state-of-the-art\nmethod for graph node classification, recommendation and other applications,\nhas not been successfully applied to industrial-scale search engine yet. In\nthis proposal, we introduce our approach, namely SearchGCN, for embedding-based\ncandidate retrieval in one of the largest e-commerce search engine in the\nworld. Empirical studies demonstrate that SearchGCN learns better embedding\nrepresentations than existing methods, especially for long tail queries and\nitems. Thus, SearchGCN has been deployed into JD.com's search production since\nJuly 2020.",
          "link": "http://arxiv.org/abs/2107.00525",
          "publishedOn": "2021-07-02T01:57:58.469Z",
          "wordCount": 543,
          "title": "SearchGCN: Powering Embedding Retrieval by Graph Convolution Networks for E-Commerce Search. (arXiv:2107.00525v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiarui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kounianhua Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiarui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuchen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Heterogeneous information network (HIN) has been widely used to characterize\nentities of various types and their complex relations. Recent attempts either\nrely on explicit path reachability to leverage path-based semantic relatedness\nor graph neighborhood to learn heterogeneous network representations before\npredictions. These weakly coupled manners overlook the rich interactions among\nneighbor nodes, which introduces an early summarization issue. In this paper,\nwe propose GraphHINGE (Heterogeneous INteract and aggreGatE), which captures\nand aggregates the interactive patterns between each pair of nodes through\ntheir structured neighborhoods. Specifically, we first introduce\nNeighborhood-based Interaction (NI) module to model the interactive patterns\nunder the same metapaths, and then extend it to Cross Neighborhood-based\nInteraction (CNI) module to deal with different metapaths. Next, in order to\naddress the complexity issue on large-scale networks, we formulate the\ninteraction modules via a convolutional framework and learn the parameters\nefficiently with fast Fourier transform. Furthermore, we design a novel\nneighborhood-based selection (NS) mechanism, a sampling strategy, to filter\nhigh-order neighborhood information based on their low-order performance. The\nextensive experiments on six different types of heterogeneous graphs\ndemonstrate the performance gains by comparing with state-of-the-arts in both\nclick-through rate prediction and top-N recommendation tasks.",
          "link": "http://arxiv.org/abs/2011.12683",
          "publishedOn": "2021-07-02T01:57:58.431Z",
          "wordCount": 688,
          "title": "GraphHINGE: Learning Interaction Models of Structured Neighborhood on Heterogeneous Information Network. (arXiv:2011.12683v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users' initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval -- through the use\nof neural contextual language models such as BERT for analysing the documents'\nand queries' contents and computing their relevance scores -- has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT's [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) -- while ensuring\nthat these embeddings discriminate among passages (based on IDF) -- which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.",
          "link": "http://arxiv.org/abs/2106.11251",
          "publishedOn": "2021-07-02T01:57:58.414Z",
          "wordCount": 716,
          "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval. (arXiv:2106.11251v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:57:58.387Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_N/0/1/0/all/0/1\">Nuno Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sousa_N/0/1/0/all/0/1\">Norberto Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praca_I/0/1/0/all/0/1\">Isabel Pra&#xe7;a</a>",
          "description": "Cybersecurity is a very challenging topic of research nowadays, as\ndigitalization increases the interaction of people, software and services on\nthe Internet by means of technology devices and networks connected to it. The\nfield is broad and has a lot of unexplored ground under numerous disciplines\nsuch as management, psychology, and data science. Its large disciplinary\nspectrum and many significant research topics generate a considerable amount of\ninformation, making it hard for us to find what we are looking for when\nresearching a particular subject. This work proposes a new search engine for\nscientific publications which combines both information retrieval and reading\ncomprehension algorithms to extract answers from a collection of\ndomain-specific documents. The proposed solution although being applied to the\ncontext of cybersecurity exhibited great generalization capabilities and can be\neasily adapted to perform under other distinct knowledge domains.",
          "link": "http://arxiv.org/abs/2107.00082",
          "publishedOn": "2021-07-02T01:57:57.911Z",
          "wordCount": 575,
          "title": "A Search Engine for Scientific Publications: a Cybersecurity Case Study. (arXiv:2107.00082v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "In today's business marketplace, many high-tech Internet enterprises\nconstantly explore innovative ways to provide optimal online user experiences\nfor gaining competitive advantages. The great needs of developing intelligent\ninteractive recommendation systems are indicated, which could sequentially\nsuggest users the most proper items by accurately predicting their preferences,\nwhile receiving the up-to-date feedback to refine the recommendation results,\ncontinuously. Multi-armed bandit algorithms, which have been widely applied\ninto various online systems, are quite capable of delivering such efficient\nrecommendation services. However, few existing bandit models are able to adapt\nto new changes introduced by the modern recommender systems.",
          "link": "http://arxiv.org/abs/2107.00161",
          "publishedOn": "2021-07-02T01:57:57.853Z",
          "wordCount": 532,
          "title": "The Use of Bandit Algorithms in Intelligent Interactive Recommender Systems. (arXiv:2107.00161v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Zitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1\">Felipe Montano-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1\">Wlodek Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1\">Evan Canfield</a>",
          "description": "The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.",
          "link": "http://arxiv.org/abs/2106.16102",
          "publishedOn": "2021-07-01T01:59:31.579Z",
          "wordCount": 632,
          "title": "Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:31.548Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Binbin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>",
          "description": "We discuss a novel task, Chorus Recognition, which could potentially benefit\ndownstream tasks such as song search and music summarization. Different from\nthe existing tasks such as music summarization or lyrics summarization relying\non single-modal information, this paper models chorus recognition as a\nmulti-modal one by utilizing both the lyrics and the tune information of songs.\nWe propose a multi-modal Chorus Recognition model that considers diverse\nfeatures. Besides, we also create and publish the first Chorus Recognition\ndataset containing 627 songs for public use. Our empirical study performed on\nthe dataset demonstrates that our approach outperforms several baselines in\nchorus recognition. In addition, our approach also helps to improve the\naccuracy of its downstream task - song search by more than 10.6%.",
          "link": "http://arxiv.org/abs/2106.16153",
          "publishedOn": "2021-07-01T01:59:31.536Z",
          "wordCount": 577,
          "title": "Multi-Modal Chorus Recognition for Improving Song Search. (arXiv:2106.16153v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehpanah_A/0/1/0/all/0/1\">Arman Dehpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghori_M/0/1/0/all/0/1\">Muheeb Faizan Ghori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemmell_J/0/1/0/all/0/1\">Jonathan Gemmell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>",
          "description": "Online competitive games have become a mainstream entertainment platform. To\ncreate a fair and exciting experience, these games use rating systems to match\nplayers with similar skills. While there has been an increasing amount of\nresearch on improving the performance of these systems, less attention has been\npaid to how their performance is evaluated. In this paper, we explore the\nutility of several metrics for evaluating three popular rating systems on a\nreal-world dataset of over 25,000 team battle royale matches. Our results\nsuggest considerable differences in their evaluation patterns. Some metrics\nwere highly impacted by the inclusion of new players. Many could not capture\nthe real differences between certain groups of players. Among all metrics\nstudied, normalized discounted cumulative gain (NDCG) demonstrated more\nreliable performance and more flexibility. It alleviated most of the challenges\nfaced by the other metrics while adding the freedom to adjust the focus of the\nevaluations on different groups of players.",
          "link": "http://arxiv.org/abs/2105.14069",
          "publishedOn": "2021-07-01T01:59:31.521Z",
          "wordCount": 633,
          "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games. (arXiv:2105.14069v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:31.503Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:31.458Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:31.292Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1\">Shazia W. Sadiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>",
          "description": "With the rapid growth of location-based social networks (LBSNs),\nPoint-Of-Interest (POI) recommendation has been broadly studied in this decade.\nRecently, the next POI recommendation, a natural extension of POI\nrecommendation, has attracted much attention. It aims at suggesting the next\nPOI to a user in spatial and temporal context, which is a practical yet\nchallenging task in various applications. Existing approaches mainly model the\nspatial and temporal information, and memorize historical patterns through\nuser's trajectories for recommendation. However, they suffer from the negative\nimpact of missing and irregular check-in data, which significantly influences\nthe model performance. In this paper, we propose an attention-based\nsequence-to-sequence generative model, namely POI-Augmentation Seq2Seq\n(PA-Seq2Seq), to address the sparsity of training set by making check-in\nrecords to be evenly-spaced. Specifically, the encoder summarises each check-in\nsequence and the decoder predicts the possible missing check-ins based on the\nencoded information. In order to learn time-aware correlation among user\nhistory, we employ local attention mechanism to help the decoder focus on a\nspecific range of context information when predicting a certain missing\ncheck-in point. Extensive experiments have been conducted on two real-world\ncheck-in datasets, Gowalla and Brightkite, for performance and effectiveness\nevaluation.",
          "link": "http://arxiv.org/abs/2106.15984",
          "publishedOn": "2021-07-01T01:59:31.223Z",
          "wordCount": 646,
          "title": "Context-Aware Attention-Based Data Augmentation for POI Recommendation. (arXiv:2106.15984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.127Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qiaomin Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Ning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Robust recommendation aims at capturing true preference of users from noisy\ndata, for which there are two lines of methods have been proposed. One is based\non noise injection, and the other is to adopt the generative model Variational\nAuto-encoder (VAE). However, the existing works still face two challenges.\nFirst, the noise injection based methods often draw the noise from a fixed\nnoise distribution given in advance, while in real world, the noise\ndistributions of different users and items may differ from each other due to\npersonal behaviors and item usage patterns. Second, the VAE based models are\nnot expressive enough to capture the true preference since VAE often yields an\nembedding space of a single modal, while in real world, user-item interactions\nusually exhibit multi-modality on user preference distribution. In this paper,\nwe propose a novel model called Dual Adversarial Variational Embedding (DAVE)\nfor robust recommendation, which can provide personalized noise reduction for\ndifferent users and items, and capture the multi-modality of the embedding\nspace, by combining the advantages of VAE and adversarial training between the\nintroduced auxiliary discriminators and the variational inference networks. The\nextensive experiments conducted on real datasets verify the effectiveness of\nDAVE on robust recommendation.",
          "link": "http://arxiv.org/abs/2106.15779",
          "publishedOn": "2021-07-01T01:59:31.084Z",
          "wordCount": 626,
          "title": "Dual Adversarial Variational Embedding for Robust Recommendation. (arXiv:2106.15779v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Being an indispensable component in location-based social networks, next\npoint-of-interest (POI) recommendation recommends users unexplored POIs based\non their recent visiting histories. However, existing work mainly models\ncheck-in data as isolated POI sequences, neglecting the crucial collaborative\nsignals from cross-sequence check-in information. Furthermore, the sparse\nPOI-POI transitions restrict the ability of a model to learn effective\nsequential patterns for recommendation. In this paper, we propose\nSequence-to-Graph (Seq2Graph) augmentation for each POI sequence, allowing\ncollaborative signals to be propagated from correlated POIs belonging to other\nsequences. We then devise a novel Sequence-to-Graph POI Recommender (SGRec),\nwhich jointly learns POI embeddings and infers a user's temporal preferences\nfrom the graph-augmented POI sequence. To overcome the sparsity of POI-level\ninteractions, we further infuse category-awareness into SGRec with a multi-task\nlearning scheme that captures the denser category-wise transitions. As such,\nSGRec makes full use of the collaborative signals for learning expressive POI\nrepresentations, and also comprehensively uncovers multi-level sequential\npatterns for user preference modelling. Extensive experiments on two real-world\ndatasets demonstrate the superiority of SGRec against state-of-the-art methods\nin next POI recommendation.",
          "link": "http://arxiv.org/abs/2106.15814",
          "publishedOn": "2021-07-01T01:59:31.058Z",
          "wordCount": 614,
          "title": "Discovering Collaborative Signals for Next POI Recommendation with Iterative Seq2Graph Augmentation. (arXiv:2106.15814v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.14076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>",
          "description": "Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.14076",
          "publishedOn": "2021-07-06T01:58:06.226Z",
          "wordCount": 659,
          "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-07-06T01:58:06.018Z",
          "wordCount": 647,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1711.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1\">Yan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1\">Tingting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyuan Yang</a>",
          "description": "The distortion in steganography that usually comes from the modification or\nrecoding on the cover image during the embedding process leaves the\nsteganalyzer with possibility of discriminating. Faced with such a risk, we\npropose generative steganography with Kerckhoffs' principle (GSK) in this\nletter. In GSK, the secret messages are generated by a cover image using a\ngenerator rather than embedded into the cover, thus resulting in no\nmodifications in the cover. To ensure the security, the generators are trained\nto meet Kerckhoffs' principle based on generative adversarial networks (GAN).\nEverything about the GSK system, except the extraction key, is public knowledge\nfor the receivers. The secret messages can be outputted by the generator if and\nonly if the extraction key and the cover image are both inputted. In the\ngenerator training procedures, there are two GANs, Message- GAN and Cover-GAN,\ndesigned to work jointly making the generated results under the control of the\nextraction key and the cover image. We provide experimental results on the\ntraining process and give an example of the working process by adopting a\ngenerator trained on MNIST, which demonstrate that GSK can use a cover image\nwithout any modification to generate messages, and without the extraction key\nor the cover image, only meaningless results would be obtained.",
          "link": "http://arxiv.org/abs/1711.04916",
          "publishedOn": "2021-07-06T01:58:05.972Z",
          "wordCount": 673,
          "title": "Generative Steganography with Kerckhoffs' Principle. (arXiv:1711.04916v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:05.872Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:56.329Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+de_Lima_Santos_M/0/1/0/all/0/1\">Mathias-Felipe de-Lima-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooli_A/0/1/0/all/0/1\">Arwa Kooli</a>",
          "description": "News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.",
          "link": "http://arxiv.org/abs/2107.00938",
          "publishedOn": "2021-07-05T01:54:56.294Z",
          "wordCount": 635,
          "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ Labs Instagram Page. (arXiv:2107.00938v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2107.00650",
          "publishedOn": "2021-07-02T01:57:58.543Z",
          "wordCount": 635,
          "title": "CLIP-It! Language-Guided Video Summarization. (arXiv:2107.00650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shurun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yan Ye</a>",
          "description": "The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.",
          "link": "http://arxiv.org/abs/2107.00328",
          "publishedOn": "2021-07-02T01:57:58.517Z",
          "wordCount": 619,
          "title": "End-to-end Compression Towards Machine Vision: Network Architecture Design and Optimization. (arXiv:2107.00328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:57:58.479Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1\">Foong Ming Moy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Nadine Kong</a>",
          "description": "Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, different types of cancers, etc. Evidence\nsuggests that diet-related mobile applications play a vital role in assisting\nan individual in making healthier choices and keeping track of food intake.\nHowever, due to an abundance of similar applications, it becomes pertinent to\nevaluate each of them in terms of functionality, usability, and possible design\nissues to truly determine state-of-the-art solutions for the future. Since\nthese applications involve implementing multiple user requirements and\nrecommendations from different dietitians, the evaluation becomes quite\ncomplex. Therefore, this study aims to review existing dietary applications at\nlength to highlight key features and problems that enhance or undermine an\napplication's usability. For this purpose, we have examined the published\nliterature from various scientific databases of the CINAHL, Science Direct, and\nPUBMED. Out of our findings, fifty-six primary studies met our inclusion\ncriteria after filtering out titles, abstracts, and full text. A total of 35\napps are analyzed from the selected studies. Our detailed analysis concluded\nthe comprehensiveness of freely available mHealth applications from users and\ndietitians' frames of reference. Furthermore, we have also specified potential\nfuture challenges and stated recommendations to help develop clinically\naccurate diet-related applications.",
          "link": "http://arxiv.org/abs/2008.09883",
          "publishedOn": "2021-07-01T01:59:31.601Z",
          "wordCount": 712,
          "title": "A Systematic Literature Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhenqiu Shu</a>",
          "description": "Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.",
          "link": "http://arxiv.org/abs/2009.12148",
          "publishedOn": "2021-07-01T01:59:31.469Z",
          "wordCount": 641,
          "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:31.361Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Prasanta Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>",
          "description": "Emotional expressions form a key part of user behavior on today's digital\nplatforms. While multimodal emotion recognition techniques are gaining research\nattention, there is a lack of deeper understanding on how visual and non-visual\nfeatures can be used to better recognize emotions in certain contexts, but not\nothers. This study analyzes the interplay between the effects of multimodal\nemotion features derived from facial expressions, tone and text in conjunction\nwith two key contextual factors: i) gender of the speaker, and ii) duration of\nthe emotional episode. Using a large public dataset of 2,176 manually annotated\nYouTube videos, we found that while multimodal features consistently\noutperformed bimodal and unimodal features, their performance varied\nsignificantly across different emotions, gender and duration contexts.\nMultimodal features performed particularly better for male speakers in\nrecognizing most emotions. Furthermore, multimodal features performed\nparticularly better for shorter than for longer videos in recognizing neutral\nand happiness, but not sadness and anger. These findings offer new insights\ntowards the development of more context-aware emotion recognition and\nempathetic systems.",
          "link": "http://arxiv.org/abs/2004.13274",
          "publishedOn": "2021-07-01T01:59:31.319Z",
          "wordCount": 668,
          "title": "Exploring the contextual factors affecting multimodal emotion recognition in videos. (arXiv:2004.13274v5 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:31.307Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:31.271Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.259Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:09.964Z",
          "wordCount": null,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajora_H/0/1/0/all/0/1\">Harish Rajora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Worldwide, several cases go undiagnosed due to poor healthcare support in\nremote areas. In this context, a centralized system is needed for effective\nmonitoring and analysis of the medical records. A web-based patient diagnostic\nsystem is a central platform to store the medical history and predict the\npossible disease based on the current symptoms experienced by a patient to\nensure faster and accurate diagnosis. Early disease prediction can help the\nusers determine the severity of the disease and take quick action. The proposed\nweb-based disease prediction system utilizes machine learning based\nclassification techniques on a data set acquired from the National Centre of\nDisease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive\nbayes classification approaches are utilized and an ensemble voting algorithm\nis also proposed where each classifier is assigned weights dynamically based on\nthe prediction confidence. The proposed system is also equipped with a\nrecommendation scheme to recommend the type of tests based on the existing\nsymptoms of the patient, so that necessary precautions can be taken. A\ncentralized database ensures that the medical data is preserved and there is\ntransparency in the system. The tampering into the system is prevented by\ngiving the no \"updation\" rights once the diagnosis is created.",
          "link": "http://arxiv.org/abs/2106.02813",
          "publishedOn": "2021-07-06T01:58:09.963Z",
          "wordCount": null,
          "title": "Machine learning equipped web based disease prediction and recommender system. (arXiv:2106.02813v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-07-06T01:58:09.961Z",
          "wordCount": null,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiangshi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengfei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sham_C/0/1/0/all/0/1\">Chiu-Wing Sham</a>",
          "description": "Modeling implicit feature interaction patterns is of significant importance\nto object detection tasks. However, in the two-stage detectors, due to the\nexcessive use of hand-crafted components, it is very difficult to reason about\nthe implicit relationship of the instance features. To tackle this problem, we\nanalyze three different levels of feature interaction relationships, namely,\nthe dependency relationship between the cropped local features and global\nfeatures, the feature autocorrelation within the instance, and the\ncross-correlation relationship between the instances. To this end, we propose a\nmore compact object detector head network (CODH), which can not only preserve\nglobal context information and condense the information density, but also\nallows instance-wise feature enhancement and relational reasoning in a larger\nmatrix space. Without bells and whistles, our method can effectively improve\nthe detection performance while significantly reducing the parameters of the\nmodel, e.g., with our method, the parameters of the head network is 0.6 times\nsmaller than the state-of-the-art Cascade R-CNN, yet the performance boost is\n1.3% on COCO test-dev. Without losing generality, we can also build a more\nlighter head network for other multi-stage detectors by assembling our method.",
          "link": "http://arxiv.org/abs/2106.14475",
          "publishedOn": "2021-07-06T01:58:09.944Z",
          "wordCount": null,
          "title": "A More Compact Object Detector Head Network with Feature Enhancement and Relational Reasoning. (arXiv:2106.14475v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>",
          "description": "In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at https://github.com/ZENGXH/NPDRAW.",
          "link": "http://arxiv.org/abs/2106.13435",
          "publishedOn": "2021-07-06T01:58:09.943Z",
          "wordCount": null,
          "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image Generation. (arXiv:2106.13435v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved remarkable success in\nvarious computer vision tasks but rely on tremendous computational cost. To\nsolve this problem, existing approaches either compress well-trained\nlarge-scale models or learn lightweight models with carefully designed network\nstructures. In this work, we make a close study of the convolution operator,\nwhich is the basic unit used in CNNs, to reduce its computing load. In\nparticular, we propose a compact convolution module, called CompConv, to\nfacilitate efficient feature learning. With the divide-and-conquer strategy,\nCompConv is able to save a great many computations as well as parameters to\nproduce a certain dimensional feature map. Furthermore, CompConv discreetly\nintegrates the input features into the outputs to efficiently inherit the input\ninformation. More importantly, the novel CompConv is a plug-and-play module\nthat can be directly applied to modern CNN structures to replace the vanilla\nconvolution layers without further effort. Extensive experimental results\nsuggest that CompConv can adequately compress the benchmark CNN structures yet\nbarely sacrifice the performance, surpassing other competitors.",
          "link": "http://arxiv.org/abs/2106.10486",
          "publishedOn": "2021-07-06T01:58:09.936Z",
          "wordCount": null,
          "title": "CompConv: A Compact Convolution Module for Efficient Feature Learning. (arXiv:2106.10486v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1\">Safa Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.",
          "link": "http://arxiv.org/abs/2106.02994",
          "publishedOn": "2021-07-06T01:58:09.935Z",
          "wordCount": null,
          "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changbin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Feature pyramid network (FPN) is a critical component in modern object\ndetection frameworks. The performance gain in most of the existing FPN variants\nis mainly attributed to the increase of computational burden. An attempt to\nenhance the FPN is enriching the spatial information by expanding the receptive\nfields, which is promising to largely improve the detection accuracy. In this\npaper, we first investigate how expanding the receptive fields affect the\naccuracy and computational costs of FPN. We explore a baseline model called\ninception FPN in which each lateral connection contains convolution filters\nwith different kernel sizes. Moreover, we point out that not all objects need\nsuch a complicated calculation and propose a new dynamic FPN (DyFPN). The\noutput features of DyFPN will be calculated by using the adaptively selected\nbranch according to a dynamic gating operation. Therefore, the proposed method\ncan provide a more efficient dynamic inference for achieving a better trade-off\nbetween accuracy and computational cost. Extensive experiments conducted on\nMS-COCO benchmark demonstrate that the proposed DyFPN significantly improves\nperformance with the optimal allocation of computation resources. For instance,\nreplacing inception FPN with DyFPN reduces about 40% of its FLOPs while\nmaintaining similar high performance.",
          "link": "http://arxiv.org/abs/2012.00779",
          "publishedOn": "2021-07-06T01:58:09.933Z",
          "wordCount": null,
          "title": "Dynamic Feature Pyramid Networks for Object Detection. (arXiv:2012.00779v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Recently, many deep neural networks were designed to process 3D point clouds,\nbut a common drawback is that rotation invariance is not ensured, leading to\npoor generalization to arbitrary orientations. In this paper, we introduce a\nnew low-level purely rotation-invariant representation to replace common 3D\nCartesian coordinates as the network inputs. Also, we present a network\narchitecture to embed these representations into features, encoding local\nrelations between points and their neighbors, and the global shape structure.\nTo alleviate inevitable global information loss caused by the\nrotation-invariant representations, we further introduce a region relation\nconvolution to encode local and non-local information. We evaluate our method\non multiple point cloud analysis tasks, including shape classification, part\nsegmentation, and shape retrieval. Experimental results show that our method\nachieves consistent, and also the best performance, on inputs at arbitrary\norientations, compared with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2003.07238",
          "publishedOn": "2021-07-06T01:58:09.932Z",
          "wordCount": null,
          "title": "A Rotation-Invariant Framework for Deep Point Cloud Analysis. (arXiv:2003.07238v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>",
          "description": "Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. Code is available at\n\\url{https://github.com/harrywuhust2022/Adv-Crowd-analysis}.",
          "link": "http://arxiv.org/abs/2104.10868",
          "publishedOn": "2021-07-06T01:58:09.925Z",
          "wordCount": null,
          "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting. (arXiv:2104.10868v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13634",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noura_H/0/1/0/all/0/1\">Hassan N. Noura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_O/0/1/0/all/0/1\">Ola Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sider_A/0/1/0/all/0/1\">Abderrahmane Sider</a>",
          "description": "Clustering is an unsupervised machine learning method grouping data samples\ninto clusters of similar objects. In practice, clustering has been used in\nnumerous applications such as banking customers profiling, document retrieval,\nimage segmentation, and e-commerce recommendation engines. However, the\nexisting clustering techniques present significant limitations, from which is\nthe dependability of their stability on the initialization parameters (e.g.\nnumber of clusters, centroids). Different solutions were presented in the\nliterature to overcome this limitation (i.e. internal and external validation\nmetrics). However, these solutions require high computational complexity and\nmemory consumption, especially when dealing with big data. In this paper, we\napply the recent object detection Deep Learning (DL) model, named YOLO-v5, to\ndetect the initial clustering parameters such as the number of clusters with\ntheir sizes and centroids. Mainly, the proposed solution consists of adding a\nDL-based initialization phase making the clustering algorithms free of\ninitialization. Two model solutions are provided in this work, one for isolated\nclusters and the other one for overlapping clusters. The features of the\nincoming dataset determine which model to use. Moreover, The results show that\nthe proposed solution can provide near-optimal clusters initialization\nparameters with low computational and resources overhead compared to existing\nsolutions.",
          "link": "http://arxiv.org/abs/2104.13634",
          "publishedOn": "2021-07-06T01:58:09.922Z",
          "wordCount": null,
          "title": "A Deep Learning Object Detection Method for an Efficient Clusters Initialization. (arXiv:2104.13634v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>",
          "description": "Recent deep learning-based methods have shown promising results and runtime\nadvantages in deformable image registration. However, analyzing the effects of\nhyperparameters and searching for optimal regularization parameters prove to be\ntoo prohibitive in deep learning-based methods. This is because it involves\ntraining a substantial number of separate models with distinct hyperparameter\nvalues. In this paper, we propose a conditional image registration method and a\nnew self-supervised learning paradigm for deep deformable image registration.\nBy learning the conditional features that are correlated with the\nregularization hyperparameter, we demonstrate that optimal solutions with\narbitrary hyperparameters can be captured by a single deep convolutional neural\nnetwork. In addition, the smoothness of the resulting deformation field can be\nmanipulated with arbitrary strength of smoothness regularization during\ninference. Extensive experiments on a large-scale brain MRI dataset show that\nour proposed method enables the precise control of the smoothness of the\ndeformation field without sacrificing the runtime advantage or registration\naccuracy.",
          "link": "http://arxiv.org/abs/2106.12673",
          "publishedOn": "2021-07-06T01:58:08.145Z",
          "wordCount": 636,
          "title": "Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>",
          "description": "Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13939",
          "publishedOn": "2021-07-06T01:58:08.117Z",
          "wordCount": 609,
          "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.",
          "link": "http://arxiv.org/abs/2105.13502",
          "publishedOn": "2021-07-06T01:58:08.110Z",
          "wordCount": 667,
          "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey. (arXiv:2105.13502v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiulou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxia Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shouju Wang</a>",
          "description": "Intratumoral nanoparticles (NPs) distribution is critical for the success of\nnanomedicine in imaging and treatment, but computational models to describe the\nNPs distribution remain unavailable due to the complex tumor-nano interactions.\nHere, we develop a Generative Adversarial Network for Distribution Analysis\n(GANDA) to describe and conditionally generates the intratumoral quantum dots\n(QDs) distribution after i.v. injection. This deep generative model is trained\nautomatically by 27 775 patches of tumor vessels and cell nuclei decomposed\nfrom whole-slide images of 4T1 breast cancer sections. The GANDA model can\nconditionally generate images of intratumoral QDs distribution under the\nconstraint of given tumor vessels and cell nuclei channels with the same\nspatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =\n1.871) and excellent reliability (intraclass correlation, ICC = 0.94).\nQuantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea\ndistribution (ICC = 0.99) is allowed on the generated images without knowing\nthe real QDs distribution. We believe this deep generative model may provide\nopportunities to investigate how influencing factors affect NPs distribution in\nindividual tumors and guide nanomedicine optimization for molecular imaging and\npersonalized treatment.",
          "link": "http://arxiv.org/abs/2012.12561",
          "publishedOn": "2021-07-06T01:58:08.104Z",
          "wordCount": 668,
          "title": "GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly. (arXiv:2012.12561v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1\">Cassidy Laidlaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "A key challenge in adversarial robustness is the lack of a precise\nmathematical characterization of human perception, used in the very definition\nof adversarial attacks that are imperceptible to human eyes. Most current\nattacks and defenses try to avoid this issue by considering restrictive\nadversarial threat models such as those bounded by $L_2$ or $L_\\infty$\ndistance, spatial perturbations, etc. However, models that are robust against\nany of these restrictive threat models are still fragile against other threat\nmodels. To resolve this issue, we propose adversarial training against the set\nof all imperceptible adversarial examples, approximated using deep neural\nnetworks. We call this threat model the neural perceptual threat model (NPTM);\nit includes adversarial examples with a bounded neural perceptual distance (a\nneural network-based approximation of the true perceptual distance) to natural\nimages. Through an extensive perceptual study, we show that the neural\nperceptual distance correlates well with human judgements of perceptibility of\nadversarial examples, validating our threat model.\n\nUnder the NPTM, we develop novel perceptual adversarial attacks and defenses.\nBecause the NPTM is very broad, we find that Perceptual Adversarial Training\n(PAT) against a perceptual attack gives robustness against many other types of\nadversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five\ndiverse adversarial attacks. We find that PAT achieves state-of-the-art\nrobustness against the union of these five attacks, more than doubling the\naccuracy over the next best model, without training against any of them. That\nis, PAT generalizes well to unforeseen perturbation types. This is vital in\nsensitive applications where a particular threat model cannot be assumed, and\nto the best of our knowledge, PAT is the first adversarial training defense\nwith this property.",
          "link": "http://arxiv.org/abs/2006.12655",
          "publishedOn": "2021-07-06T01:58:08.097Z",
          "wordCount": 777,
          "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. (arXiv:2006.12655v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n\nWith these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-07-06T01:58:08.074Z",
          "wordCount": 581,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hoang Phan</a>",
          "description": "This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.",
          "link": "http://arxiv.org/abs/2106.07568",
          "publishedOn": "2021-07-06T01:58:08.064Z",
          "wordCount": 612,
          "title": "Full interpretable machine learning in 2D with inline coordinates. (arXiv:2106.07568v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_C/0/1/0/all/0/1\">Ciaran Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1\">Jonathan Horgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varley_P/0/1/0/all/0/1\">Padraig Varley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODea_D/0/1/0/all/0/1\">Derek O&#x27;Dea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1\">Martin Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amende_K/0/1/0/all/0/1\">Karl Amende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sanjaya Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_S/0/1/0/all/0/1\">Saquib Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perroton_X/0/1/0/all/0/1\">Xavier Perroton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick Perez</a>",
          "description": "Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.",
          "link": "http://arxiv.org/abs/1905.01489",
          "publishedOn": "2021-07-06T01:58:08.054Z",
          "wordCount": 681,
          "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving. (arXiv:1905.01489v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deziel_J/0/1/0/all/0/1\">Jean-Luc D&#xe9;ziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merriaux_P/0/1/0/all/0/1\">Pierre Merriaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_F/0/1/0/all/0/1\">Francis Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessard_D/0/1/0/all/0/1\">Dave Lessard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plourde_D/0/1/0/all/0/1\">Dominique Plourde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanguennec_J/0/1/0/all/0/1\">Julien Stanguennec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulet_P/0/1/0/all/0/1\">Pierre Goulet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivier_P/0/1/0/all/0/1\">Pierre Olivier</a>",
          "description": "Leddar PixSet is a new publicly available dataset (dataset.leddartech.com)\nfor autonomous driving research and development. One key novelty of this\ndataset is the presence of full-waveform data from the Leddar Pixell sensor, a\nsolid-state flash LiDAR. Full-waveform data has been shown to improve the\nperformance of perception algorithms in airborne applications but is yet to be\ndemonstrated for terrestrial applications such as autonomous driving. The\nPixSet dataset contains approximately 29k frames from 97 sequences recorded in\nhigh-density urban areas, using a set of various sensors (cameras, LiDARs,\nradar, IMU, etc.) Each frame has been manually annotated with 3D bounding\nboxes.",
          "link": "http://arxiv.org/abs/2102.12010",
          "publishedOn": "2021-07-06T01:58:08.047Z",
          "wordCount": 600,
          "title": "PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset. (arXiv:2102.12010v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1\">Jun Hao Liew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "We consider the challenging multi-person 3D body mesh estimation task in this\nwork. Existing methods are mostly two-stage based--one stage for person\nlocalization and the other stage for individual body mesh estimation, leading\nto redundant pipelines with high computation cost and degraded performance for\ncomplex scenes (e.g., occluded person instances). In this work, we present a\nsingle-stage model, Body Meshes as Points (BMP), to simplify the pipeline and\nlift both efficiency and performance. In particular, BMP adopts a new method\nthat represents multiple person instances as points in the spatial-depth space\nwhere each point is associated with one body mesh. Hinging on such\nrepresentations, BMP can directly predict body meshes for multiple persons in a\nsingle stage by concurrently localizing person instance points and estimating\nthe corresponding body meshes. To better reason about depth ordering of all the\npersons within the same scene, BMP designs a simple yet effective\ninter-instance ordinal depth loss to obtain depth-coherent body mesh\nestimation. BMP also introduces a novel keypoint-aware augmentation to enhance\nmodel robustness to occluded person instances. Comprehensive experiments on\nbenchmarks Panoptic, MuPoTS-3D and 3DPW clearly demonstrate the\nstate-of-the-art efficiency of BMP for multi-person body mesh estimation,\ntogether with outstanding accuracy. Code can be found at:\nhttps://github.com/jfzhang95/BMP.",
          "link": "http://arxiv.org/abs/2105.02467",
          "publishedOn": "2021-07-06T01:58:08.039Z",
          "wordCount": 672,
          "title": "Body Meshes as Points. (arXiv:2105.02467v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Louiset_R/0/1/0/all/0/1\">Robin Louiset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Houenou_J/0/1/0/all/0/1\">Josselin Houenou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grigis_A/0/1/0/all/0/1\">Antoine Grigis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>",
          "description": "Subtype Discovery consists in finding interpretable and consistent sub-parts\nof a dataset, which are also relevant to a certain supervised task. From a\nmathematical point of view, this can be defined as a clustering task driven by\nsupervised learning in order to uncover subgroups in line with the supervised\nprediction. In this paper, we propose a general Expectation-Maximization\nensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised\nLearning). Our method is generic, it can integrate any clustering method and\ncan be driven by both binary classification and regression. We propose to\nconstruct a non-linear model by merging multiple linear estimators, one per\ncluster. Each hyperplane is estimated so that it correctly discriminates - or\npredict - only one cluster. We use SVC or Logistic Regression for\nclassification and SVR for regression. Furthermore, to perform cluster analysis\nwithin a more suitable space, we also propose a dimension-reduction algorithm\nthat projects the data onto an orthonormal space relevant to the supervised\ntask. We analyze the robustness and generalization capability of our algorithm\nusing synthetic and experimental datasets. In particular, we validate its\nability to identify suitable consistent sub-types by conducting a\npsychiatric-diseases cluster analysis with known ground-truth labels. The gain\nof the proposed method over previous state-of-the-art techniques is about +1.9\npoints in terms of balanced accuracy. Finally, we make codes and examples\navailable in a scikit-learn-compatible Python package at\nhttps://github.com/neurospin-projects/2021_rlouiset_ucsl",
          "link": "http://arxiv.org/abs/2107.01988",
          "publishedOn": "2021-07-06T01:58:08.032Z",
          "wordCount": 699,
          "title": "UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning. (arXiv:2107.01988v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorbeer_B/0/1/0/all/0/1\">Boris Lorbeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botler_M/0/1/0/all/0/1\">Max Botler</a>",
          "description": "In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.",
          "link": "http://arxiv.org/abs/2009.02755",
          "publishedOn": "2021-07-06T01:58:08.012Z",
          "wordCount": 647,
          "title": "Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarca_J/0/1/0/all/0/1\">Jos&#xe9; Lamarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facil_J/0/1/0/all/0/1\">Jos&#xe9; M. F&#xe1;cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J. M. M. Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>",
          "description": "Estimating a scene reconstruction and the camera motion from in-body videos\nis challenging due to several factors, e.g. the deformation of in-body cavities\nor the lack of texture. In this paper we present Endo-Depth-and-Motion, a\npipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene\nmodels from monocular endoscopic videos. Our approach leverages recent advances\nin self-supervised depth networks to generate pseudo-RGBD frames, then tracks\nthe camera pose using photometric residuals and fuses the registered depth maps\nin a volumetric representation. We present an extensive experimental evaluation\nin the public dataset Hamlyn, showing high-quality results and comparisons\nagainst relevant baselines. We also release all models and code for future\ncomparisons.",
          "link": "http://arxiv.org/abs/2103.16525",
          "publishedOn": "2021-07-06T01:58:08.006Z",
          "wordCount": 600,
          "title": "Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints. (arXiv:2103.16525v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>",
          "description": "For artificial learning systems, continual learning over time from a stream\nof data is essential. The burgeoning studies on supervised continual learning\nhave achieved great progress, while the study of catastrophic forgetting in\nunsupervised learning is still blank. Among unsupervised learning methods,\nself-supervise learning method shows tremendous potential on visual\nrepresentation without any labeled data at scale. To improve the visual\nrepresentation of self-supervised learning, larger and more varied data is\nneeded. In the real world, unlabeled data is generated at all times. This\ncircumstance provides a huge advantage for the learning of the self-supervised\nmethod. However, in the current paradigm, packing previous data and current\ndata together and training it again is a waste of time and resources. Thus, a\ncontinual self-supervised learning method is badly needed. In this paper, we\nmake the first attempt to implement the continual contrastive self-supervised\nlearning by proposing a rehearsal method, which keeps a few exemplars from the\nprevious data. Instead of directly combining saved exemplars with the current\ndata set for training, we leverage self-supervised knowledge distillation to\ntransfer contrastive information among previous data to the current network by\nmimicking similarity score distribution inferred by the old network over a set\nof saved exemplars. Moreover, we build an extra sample queue to assist the\nnetwork to distinguish between previous and current data and prevent mutual\ninterference while learning their own feature representation. Experimental\nresults show that our method performs well on CIFAR100 and ImageNet-Sub.\nCompared with self-supervised baselines, which learning tasks one by one\nwithout taking any technique, we improve the image classification top-1\naccuracy by 1.60% on CIFAR100 and 2.86% on ImageNet-Sub under 10 incremental\nsteps setting.",
          "link": "http://arxiv.org/abs/2107.01776",
          "publishedOn": "2021-07-06T01:58:07.999Z",
          "wordCount": 711,
          "title": "Continual Contrastive Self-supervised Learning for Image Classification. (arXiv:2107.01776v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerroumi_M/0/1/0/all/0/1\">Mohamed Kerroumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayem_O/0/1/0/all/0/1\">Othmane Sayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabou_A/0/1/0/all/0/1\">Aymen Shabou</a>",
          "description": "We introduce a novel approach for scanned document representation to perform\nfield extraction. It allows the simultaneous encoding of the textual, visual\nand layout information in a 3-axis tensor used as an input to a segmentation\nmodel. We improve the recent Chargrid and Wordgrid \\cite{chargrid} models in\nseveral ways, first by taking into account the visual modality, then by\nboosting its robustness in regards to small datasets while keeping the\ninference time low. Our approach is tested on public and private document-image\ndatasets, showing higher performances compared to the recent state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2010.02358",
          "publishedOn": "2021-07-06T01:58:07.990Z",
          "wordCount": 585,
          "title": "VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach. (arXiv:2010.02358v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manigrasso_F/0/1/0/all/0/1\">Francesco Manigrasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miro_F/0/1/0/all/0/1\">Filomeno Davide Miro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morra_L/0/1/0/all/0/1\">Lia Morra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberti_F/0/1/0/all/0/1\">Fabrizio Lamberti</a>",
          "description": "The detection of semantic relationships between objects represented in an\nimage is one of the fundamental challenges in image interpretation.\nNeural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the\ncombination of semantic knowledge representation and reasoning with the ability\nto efficiently learn from examples typical of neural networks. We here propose\nFaster-LTN, an object detector composed of a convolutional backbone and an LTN.\nTo the best of our knowledge, this is the first attempt to combine both\nframeworks in an end-to-end training setting. This architecture is trained by\noptimizing a grounded theory which combines labelled examples with prior\nknowledge, in the form of logical axioms. Experimental comparisons show\ncompetitive performance with respect to the traditional Faster R-CNN\narchitecture.",
          "link": "http://arxiv.org/abs/2107.01877",
          "publishedOn": "2021-07-06T01:58:07.971Z",
          "wordCount": 571,
          "title": "Faster-LTN: a neuro-symbolic, end-to-end object detection architecture. (arXiv:2107.01877v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodabakhsh_A/0/1/0/all/0/1\">Ali Khodabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>",
          "description": "Despite the impressive progress in the field of presentation attack detection\nand multimedia forensics over the last decade, these systems are still\nvulnerable to attacks in real-life settings. Some of the challenges for\nexisting solutions are the detection of unknown attacks, the ability to perform\nin adversarial settings, few-shot learning, and explainability. In this study,\nthese limitations are approached by reliance on a game-theoretic view for\nmodeling the interactions between the attacker and the detector. Consequently,\na new optimization criterion is proposed and a set of requirements are defined\nfor improving the performance of these systems in real-life settings.\nFurthermore, a novel detection technique is proposed using generator-based\nfeature sets that are not biased towards any specific attack species. To\nfurther optimize the performance on known attacks, a new loss function coined\ncategorical margin maximization loss (C-marmax) is proposed which gradually\nimproves the performance against the most powerful attack. The proposed\napproach provides a more balanced performance across known and unknown attacks\nand achieves state-of-the-art performance in known and unknown attack detection\ncases against rational attackers. Lastly, the few-shot learning potential of\nthe proposed approach is studied as well as its ability to provide pixel-level\nexplainability.",
          "link": "http://arxiv.org/abs/2010.01592",
          "publishedOn": "2021-07-06T01:58:07.965Z",
          "wordCount": 668,
          "title": "Unknown Presentation Attack Detection against Rational Attackers. (arXiv:2010.01592v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bojian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>",
          "description": "It is important to learn joint embedding for 3D shapes and text in different\nshape understanding tasks, such as shape-text matching, retrieval, and shape\ncaptioning. Current multi-view based methods learn a mapping from multiple\nrendered views to text. However, these methods can not analyze 3D shapes well\ndue to the self-occlusion and limitation of learning manifolds. To resolve this\nissue, we propose a method to learn joint embedding of point clouds and text by\nmatching parts from shapes to words from sentences in a common space.\nSpecifically, we first learn segmentation prior to segment point clouds into\nparts. Then, we map parts and words into an optimized space, where the parts\nand words can be matched with each other. In the optimized space, we represent\na part by aggregating features of all points within the part, while\nrepresenting each word with its context information, where we train our network\nto minimize the triplet ranking loss. Moreover, we also introduce cross-modal\nattention to capture the relationship of part-word in this matching procedure,\nwhich enhances joint embedding learning. Our experimental results outperform\nthe state-of-the-art in multi-modal retrieval under the widely used benchmark.",
          "link": "http://arxiv.org/abs/2107.01872",
          "publishedOn": "2021-07-06T01:58:07.959Z",
          "wordCount": 640,
          "title": "Part2Word: Learning Joint Embedding of Point Clouds and Text by Matching Parts to Words. (arXiv:2107.01872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sterneck_R/0/1/0/all/0/1\">Rachel Sterneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Abhishek Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>",
          "description": "Neural networks have achieved remarkable performance in computer vision,\nhowever they are vulnerable to adversarial examples. Adversarial examples are\ninputs that have been carefully perturbed to fool classifier networks, while\nappearing unchanged to humans. Based on prior works on detecting adversaries,\nwe propose a structured methodology of augmenting a deep neural network (DNN)\nwith a detector subnetwork. We use $\\textit{Adversarial Noise Sensitivity}$\n(ANS), a novel metric for measuring the adversarial gradient contribution of\ndifferent intermediate layers of a network. Based on the ANS value, we append a\ndetector to the most sensitive layer. In prior works, more complex detectors\nwere added to a DNN, increasing the inference computational cost of the model.\nIn contrast, our structured and strategic addition of a detector to a DNN\nreduces the complexity of the model while making the overall network\nadversarially resilient. Through comprehensive white-box and black-box\nexperiments on MNIST, CIFAR-10, and CIFAR-100, we show that our method improves\nstate-of-the-art detector robustness against adversarial examples. Furthermore,\nwe validate the energy efficiency of our proposed adversarial detection\nmethodology through an extensive energy analysis on various hardware scalable\nCMOS accelerator platforms. We also demonstrate the effects of quantization on\nour detector-appended networks.",
          "link": "http://arxiv.org/abs/2101.01543",
          "publishedOn": "2021-07-06T01:58:07.952Z",
          "wordCount": 678,
          "title": "Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection in Neural Networks. (arXiv:2101.01543v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:07.945Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an $81.5%$ top-1 accuracy on the ImageNet, which is about\n$1.7%$ higher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch, and the\nMindSpore code is at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT.",
          "link": "http://arxiv.org/abs/2103.00112",
          "publishedOn": "2021-07-06T01:58:07.937Z",
          "wordCount": 695,
          "title": "Transformer in Transformer. (arXiv:2103.00112v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>",
          "description": "We present a novel approach for unsupervised activity segmentation, which\nuses video frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport and temporal coherence loss. In particular, we incorporate a\ntemporal regularization term into the standard optimal transport module, which\npreserves the temporal order of the activity, yielding the temporal optimal\ntransport module for computing pseudo-label cluster assignments. Next, the\ntemporal coherence loss encourages neighboring video frames to be mapped to\nnearby points while distant video frames are mapped to farther away points in\nthe embedding space. The combination of these two components results in\neffective representations for unsupervised activity segmentation. Furthermore,\nprevious methods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par or better than\nprevious methods for unsupervised activity segmentation, despite having\nsignificantly less memory constraints.",
          "link": "http://arxiv.org/abs/2105.13353",
          "publishedOn": "2021-07-06T01:58:07.930Z",
          "wordCount": 690,
          "title": "Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yingxue Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Image-to-image translation (I2I) aims to transfer images from a source domain\nto a target domain while preserving the content representations. I2I has drawn\nincreasing attention and made tremendous progress in recent years because of\nits wide range of applications in many computer vision and image processing\nproblems, such as image synthesis, segmentation, style transfer, restoration,\nand pose estimation. In this paper, we provide an overview of the I2I works\ndeveloped in recent years. We will analyze the key techniques of the existing\nI2I works and clarify the main progress the community has made. Additionally,\nwe will elaborate on the effect of I2I on the research and industry community\nand point out remaining challenges in related fields.",
          "link": "http://arxiv.org/abs/2101.08629",
          "publishedOn": "2021-07-06T01:58:07.922Z",
          "wordCount": 579,
          "title": "Image-to-Image Translation: Methods and Applications. (arXiv:2101.08629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hager_J/0/1/0/all/0/1\">Janik Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_R/0/1/0/all/0/1\">Ruben Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mainprice_J/0/1/0/all/0/1\">Jim Mainprice</a>",
          "description": "In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect\ngrasp affordances for objects directly in 2D camera images. To perform\nmanipulation tasks autonomously it is crucial for robots to have such\ngraspability models of the surrounding objects. Grasp manifolds have the\nadvantage of providing continuously infinitely many grasps, which is not the\ncase when using other grasp representations such as predefined grasp points.\nFor instance, this property can be leveraged in motion optimization to define\ngoal sets as implicit surface constraints in the robot configuration space. In\nthis work, we restrict ourselves to the case of estimating possible\nend-effector positions directly from 2D camera images. To this extend, we\ndefine grasp manifolds via a set of key points and locate them in images using\na Mask R-CNN backbone. Using learned features allows generalizing to different\nview angles, with potentially noisy images, and objects that were not part of\nthe training set. We rely on simulation data only and perform experiments on\nsimple and complex objects, including unseen ones. Our framework achieves an\ninference speed of 11.5 fps on a GPU, an average precision for keypoint\nestimation of 94.5% and a mean pixel distance of only 1.29. This shows that we\ncan estimate the objects very well via bounding boxes and segmentation masks as\nwell as approximate the correct grasp manifold's keypoint coordinates.",
          "link": "http://arxiv.org/abs/2107.01836",
          "publishedOn": "2021-07-06T01:58:07.874Z",
          "wordCount": 662,
          "title": "GraspME -- Grasp Manifold Estimator. (arXiv:2107.01836v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14711",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ce Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hui_Y/0/1/0/all/0/1\">Yuan Hui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_S/0/1/0/all/0/1\">Shiwei Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Mengke Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shuxin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1\">You Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pengbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1\">Honghu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chunpeng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinbao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Spine-related diseases have high morbidity and cause a huge burden of social\ncost. Spine imaging is an essential tool for noninvasively visualizing and\nassessing spinal pathology. Segmenting vertebrae in computed tomography (CT)\nimages is the basis of quantitative medical image analysis for clinical\ndiagnosis and surgery planning of spine diseases. Current publicly available\nannotated datasets on spinal vertebrae are small in size. Due to the lack of a\nlarge-scale annotated spine image dataset, the mainstream deep learning-based\nsegmentation methods, which are data-driven, are heavily restricted. In this\npaper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated\nfrom multiple sources for vertebra segmentation, which contains 1,005 CT\nvolumes with over 11,100 labeled vertebrae belonging to different spinal\nconditions. Based on this dataset, we conduct several spinal vertebrae\nsegmentation experiments to set the first benchmark. We believe that this\nlarge-scale dataset will facilitate further research in many spine-related\nimage analysis tasks, including but not limited to vertebrae segmentation,\nlabeling, 3D spine reconstruction from biplanar radiographs, image\nsuper-resolution, and enhancement.",
          "link": "http://arxiv.org/abs/2105.14711",
          "publishedOn": "2021-07-06T01:58:07.857Z",
          "wordCount": 668,
          "title": "CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography. (arXiv:2105.14711v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-07-06T01:58:07.850Z",
          "wordCount": 670,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v10 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00113",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_D/0/1/0/all/0/1\">Damien Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorado_I/0/1/0/all/0/1\">Ignacio Garcia Dorado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>",
          "description": "Could we compress images via standard codecs while avoiding visible\nartifacts? The answer is obvious -- this is doable as long as the bit budget is\ngenerous enough. What if the allocated bit-rate for compression is\ninsufficient? Then unfortunately, artifacts are a fact of life. Many attempts\nwere made over the years to fight this phenomenon, with various degrees of\nsuccess. In this work we aim to break the unholy connection between bit-rate\nand image quality, and propose a way to circumvent compression artifacts by\npre-editing the incoming image and modifying its content to fit the given bits.\nWe design this editing operation as a learned convolutional neural network, and\nformulate an optimization problem for its training. Our loss takes into account\na proximity between the original image and the edited one, a bit-budget penalty\nover the proposed image, and a no-reference image quality measure for forcing\nthe outcome to be visually pleasing. The proposed approach is demonstrated on\nthe popular JPEG compression, showing savings in bits and/or improvements in\nvisual quality, obtained with intricate editing effects.",
          "link": "http://arxiv.org/abs/2002.00113",
          "publishedOn": "2021-07-06T01:58:07.843Z",
          "wordCount": 640,
          "title": "Better Compression with Deep Pre-Editing. (arXiv:2002.00113v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mai Zhu</a>",
          "description": "The attributes of object contours has great significance for instance\nsegmentation task. However, most of the current popular deep neural networks do\nnot pay much attention to the object edge information. Inspired by the human\nannotation process when making instance segmentation datasets, in this paper,\nwe propose Mask Point R-CNN aiming at promoting the neural network's attention\nto the object boundary. Specifically, we innovatively extend the original human\nkeypoint detection task to the contour point detection of any object. Based on\nthis analogy, we present an contour point detection auxiliary task to Mask\nR-CNN, which can boost the gradient flow between different tasks by effectively\nusing feature fusion strategies and multi-task joint training. As a\nconsequence, the model will be more sensitive to the edges of the object and\ncan capture more geometric features. Quantitatively, the experimental results\nshow that our approach outperforms vanilla Mask R-CNN by 3.8\\% on Cityscapes\ndataset and 0.8\\% on COCO dataset.",
          "link": "http://arxiv.org/abs/2008.00460",
          "publishedOn": "2021-07-06T01:58:07.835Z",
          "wordCount": 628,
          "title": "Joint Object Contour Points and Semantics for Instance Segmentation. (arXiv:2008.00460v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1\">Reza Esfandiarpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajabdollahi_M/0/1/0/all/0/1\">Mohsen Hajabdollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>",
          "description": "In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2012.07176",
          "publishedOn": "2021-07-06T01:58:07.816Z",
          "wordCount": 656,
          "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks. (arXiv:2012.07176v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.",
          "link": "http://arxiv.org/abs/2103.15890",
          "publishedOn": "2021-07-06T01:58:07.809Z",
          "wordCount": 581,
          "title": "Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1\">Michael Carbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>",
          "description": "We show that the error of iteratively magnitude-pruned networks empirically\nfollows a scaling law with interpretable coefficients that depend on the\narchitecture and task. We functionally approximate the error of the pruned\nnetworks, showing it is predictable in terms of an invariant tying width,\ndepth, and pruning level, such that networks of vastly different pruned\ndensities are interchangeable. We demonstrate the accuracy of this\napproximation over orders of magnitude in depth, width, dataset size, and\ndensity. We show that the functional form holds (generalizes) for large scale\ndata (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks\nbecome ever larger and costlier to train, our findings suggest a framework for\nreasoning conceptually and analytically about a standard method for\nunstructured pruning.",
          "link": "http://arxiv.org/abs/2006.10621",
          "publishedOn": "2021-07-06T01:58:07.800Z",
          "wordCount": 599,
          "title": "On the Predictability of Pruning Across Scales. (arXiv:2006.10621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1\">Nontawat Tritrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewatbowornwong_P/0/1/0/all/0/1\">Pitchaporn Rewatbowornwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1\">Supasorn Suwajanakorn</a>",
          "description": "While GANs have shown success in realistic image generation, the idea of\nusing GANs for other tasks unrelated to synthesis is underexplored. Do GANs\nlearn meaningful structural parts of objects during their attempt to reproduce\nthose objects? In this work, we test this hypothesis and propose a simple and\neffective approach based on GANs for semantic part segmentation that requires\nas few as one label example along with an unlabeled dataset. Our key idea is to\nleverage a trained GAN to extract pixel-wise representation from the input\nimage and use it as feature vectors for a segmentation network. Our experiments\ndemonstrate that GANs representation is \"readily discriminative\" and produces\nsurprisingly good results that are comparable to those from supervised\nbaselines trained with significantly more labels. We believe this novel\nrepurposing of GANs underlies a new class of unsupervised representation\nlearning that is applicable to many other tasks. More results are available at\nhttps://repurposegans.github.io/.",
          "link": "http://arxiv.org/abs/2103.04379",
          "publishedOn": "2021-07-06T01:58:07.792Z",
          "wordCount": 648,
          "title": "Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07279",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1\">Mohammad Saber Iraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1\">Jafar Tanha</a>",
          "description": "The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.",
          "link": "http://arxiv.org/abs/2104.07279",
          "publishedOn": "2021-07-06T01:58:07.785Z",
          "wordCount": 679,
          "title": "COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images. (arXiv:2104.07279v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangtong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>",
          "description": "Image composition aims to generate realistic composite image by inserting an\nobject from one image into another background image, where the placement (e.g.,\nlocation, size, occlusion) of inserted object may be unreasonable, which would\nsignificantly degrade the quality of the composite image. Although some works\nattempted to learn object placement to create realistic composite images, they\ndid not focus on assessing the plausibility of object placement. In this paper,\nwe focus on object placement assessment task, which verifies whether a\ncomposite image is plausible in terms of the object placement. To accomplish\nthis task, we construct the first Object Placement Assessment (OPA) dataset\nconsisting of composite images and their rationality labels. Dataset is\navailable at https://github.com/bcmi/Object-Placement-Assessment-Dataset-OPA.",
          "link": "http://arxiv.org/abs/2107.01889",
          "publishedOn": "2021-07-06T01:58:07.765Z",
          "wordCount": 552,
          "title": "OPA: Object Placement Assessment Dataset. (arXiv:2107.01889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08727",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Left atrial (LA) segmentation from late gadolinium enhanced magnetic\nresonance imaging (LGE MRI) is a crucial step needed for planning the treatment\nof atrial fibrillation. However, automatic LA segmentation from LGE MRI is\nstill challenging, due to the poor image quality, high variability in LA\nshapes, and unclear LA boundary. Though deep learning-based methods can provide\npromising LA segmentation results, they often generalize poorly to unseen\ndomains, such as data from different scanners and/or sites. In this work, we\ncollect 210 LGE MRIs from different centers with different levels of image\nquality. To evaluate the domain generalization ability of models on the LA\nsegmentation task, we employ four commonly used semantic segmentation networks\nfor the LA segmentation from multi-center LGE MRIs. Besides, we investigate\nthree domain generalization strategies, i.e., histogram matching, mutual\ninformation based disentangled representation, and random style transfer, where\na simple histogram matching is proved to be most effective.",
          "link": "http://arxiv.org/abs/2106.08727",
          "publishedOn": "2021-07-06T01:58:07.758Z",
          "wordCount": 635,
          "title": "AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs. (arXiv:2106.08727v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balayn_A/0/1/0/all/0/1\">Agathe Balayn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1\">Bogdan Kulynych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerses_S/0/1/0/all/0/1\">Seda Guerses</a>",
          "description": "Researchers have identified datasets used for training computer vision (CV)\nmodels as an important source of hazardous outcomes, and continue to examine\npopular CV datasets to expose their harms. These works tend to treat datasets\nas objects, or focus on particular steps in data production pipelines. We argue\nhere that we could further systematize our analysis of harms by examining CV\ndata pipelines through a process-oriented lens that captures the creation, the\nevolution and use of these datasets. As a step towards cultivating a\nprocess-oriented lens, we embarked on an empirical study of CV data pipelines\ninformed by the field of method engineering. We present here a preliminary\nresult: a reference model of CV data pipelines. Besides exploring the questions\nthat this endeavor raises, we discuss how the process lens could support\nresearchers in discovering understudied issues, and could help practitioners in\nmaking their processes more transparent.",
          "link": "http://arxiv.org/abs/2107.01824",
          "publishedOn": "2021-07-06T01:58:07.751Z",
          "wordCount": 599,
          "title": "Exploring Data Pipelines through the Process Lens: a Reference Model forComputer Vision. (arXiv:2107.01824v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minkyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "This paper studies probability distributions ofpenultimate activations of\nclassification networks.We show that, when a classification network istrained\nwith the cross-entropy loss, its final classi-fication layer forms\naGenerative-Discriminativepairwith a generative classifier based on a\nspecificdistribution of penultimate activations. More im-portantly, the\ndistribution is parameterized by theweights of the final fully-connected layer,\nand canbe considered as a generative model that synthe-sizes the penultimate\nactivations without feedinginput data. We empirically demonstrate that\nthisgenerative model enables stable knowledge dis-tillation in the presence of\ndomain shift, and cantransfer knowledge from a classifier to\nvariationalautoencoders and generative adversarial networksfor\nclass-conditional image generation.",
          "link": "http://arxiv.org/abs/2107.01900",
          "publishedOn": "2021-07-06T01:58:07.744Z",
          "wordCount": 538,
          "title": "On The Distribution of Penultimate Activations of Classification Networks. (arXiv:2107.01900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Graph learning has emerged as a promising technique for multi-view clustering\nwith its ability to learn a unified and robust graph from multiple views.\nHowever, existing graph learning methods mostly focus on the multi-view\nconsistency issue, yet often neglect the inconsistency across multiple views,\nwhich makes them vulnerable to possibly low-quality or noisy datasets. To\novercome this limitation, we propose a new multi-view graph learning framework,\nwhich for the first time simultaneously and explicitly models multi-view\nconsistency and multi-view inconsistency in a unified objective function,\nthrough which the consistent and inconsistent parts of each single-view graph\nas well as the unified graph that fuses the consistent parts can be iteratively\nlearned. Though optimizing the objective function is NP-hard, we design a\nhighly efficient optimization algorithm which is able to obtain an approximate\nsolution with linear time complexity in the number of edges in the unified\ngraph. Furthermore, our multi-view graph learning approach can be applied to\nboth similarity graphs and dissimilarity graphs, which lead to two graph\nfusion-based variants in our framework. Experiments on twelve multi-view\ndatasets have demonstrated the robustness and efficiency of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2008.10208",
          "publishedOn": "2021-07-06T01:58:07.737Z",
          "wordCount": 671,
          "title": "Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency. (arXiv:2008.10208v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhishan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dawei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>",
          "description": "As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we will release our code\nas well as reference implementation of those baseline models in the final\nversion.",
          "link": "http://arxiv.org/abs/2107.01999",
          "publishedOn": "2021-07-06T01:58:07.719Z",
          "wordCount": 642,
          "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popli_A/0/1/0/all/0/1\">Additya Popli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_S/0/1/0/all/0/1\">Saraansh Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1\">Naoyuki Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okubo_A/0/1/0/all/0/1\">Atsushi Okubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_A/0/1/0/all/0/1\">Anoop Namboodiri</a>",
          "description": "Typical fingerprint recognition systems are comprised of a spoof detection\nmodule and a subsequent recognition module, running one after the other. In\nthis paper, we reformulate the workings of a typical fingerprint recognition\nsystem. In particular, we posit that both spoof detection and fingerprint\nrecognition are correlated tasks. Therefore, rather than performing the two\ntasks separately, we propose a joint model for spoof detection and matching to\nsimultaneously perform both tasks without compromising the accuracy of either\ntask. We demonstrate the capability of our joint model to obtain an\nauthentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC\n2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet\n2015 dataset, both maintaining the performance of stand-alone methods. In\npractice, this reduces the time and memory requirements of the fingerprint\nrecognition system by 50% and 40%, respectively; a significant advantage for\nrecognition systems running on resource-constrained devices and communication\nchannels.",
          "link": "http://arxiv.org/abs/2104.03255",
          "publishedOn": "2021-07-06T01:58:07.703Z",
          "wordCount": 637,
          "title": "A Unified Model for Fingerprint Authentication and Presentation Attack Detection. (arXiv:2104.03255v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bi&#x27;an Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>",
          "description": "Point clouds have attracted increasing attention as a natural representation\nof 3D shapes. Significant progress has been made in developing methods for\npoint cloud analysis, which often requires costly human annotation as\nsupervision in practice. To address this issue, we propose a novel\nself-contrastive learning for self-supervised point cloud representation\nlearning, aiming to capture both local geometric patterns and nonlocal semantic\nprimitives based on the nonlocal self-similarity of point clouds. The\ncontributions are two-fold: on the one hand, instead of contrasting among\ndifferent point clouds as commonly employed in contrastive learning, we exploit\nself-similar point cloud patches within a single point cloud as positive\nsamples and otherwise negative ones to facilitate the task of contrastive\nlearning. Such self-contrastive learning is well aligned with the emerging\nparadigm of self-supervised learning for point cloud analysis. On the other\nhand, we actively learn hard negative samples that are close to positive\nsamples in the representation space for discriminative feature learning, which\nare sampled conditional on each anchor patch leveraging on the degree of\nself-similarity. Experimental results show that the proposed method achieves\nstate-of-the-art performance on widely used benchmark datasets for\nself-supervised point cloud segmentation and transfer learning for\nclassification.",
          "link": "http://arxiv.org/abs/2107.01886",
          "publishedOn": "2021-07-06T01:58:07.696Z",
          "wordCount": 644,
          "title": "Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning. (arXiv:2107.01886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeeseung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younggeun Kim</a>",
          "description": "We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.",
          "link": "http://arxiv.org/abs/2106.07023",
          "publishedOn": "2021-07-06T01:58:07.688Z",
          "wordCount": 641,
          "title": "Styleformer: Transformer based Generative Adversarial Networks with Style Vector. (arXiv:2106.07023v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongbao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "In real applications, new object classes often emerge after the detection\nmodel has been trained on a prepared dataset with fixed classes. Due to the\nstorage burden and the privacy of old data, sometimes it is impractical to\ntrain the model from scratch with both old and new data. Fine-tuning the old\nmodel with only new data will lead to a well-known phenomenon of catastrophic\nforgetting, which severely degrades the performance of modern object detectors.\nIn this paper, we propose a novel \\textbf{M}ulti-\\textbf{V}iew\n\\textbf{C}orrelation \\textbf{D}istillation (MVCD) based incremental object\ndetection method, which explores the correlations in the feature space of the\ntwo-stage object detector (Faster R-CNN). To better transfer the knowledge\nlearned from the old classes and maintain the ability to learn new classes, we\ndesign correlation distillation losses from channel-wise, point-wise and\ninstance-wise views to regularize the learning of the incremental model. A new\nmetric named Stability-Plasticity-mAP is proposed to better evaluate both the\nstability for old classes and the plasticity for new classes in incremental\nobject detection. The extensive experiments conducted on VOC2007 and COCO\ndemonstrate that MVCD can effectively learn to detect objects of new classes\nand mitigate the problem of catastrophic forgetting.",
          "link": "http://arxiv.org/abs/2107.01787",
          "publishedOn": "2021-07-06T01:58:07.681Z",
          "wordCount": 628,
          "title": "Multi-View Correlation Distillation for Incremental Object Detection. (arXiv:2107.01787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.03572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rongguang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>",
          "description": "Deep learning-based object detection and instance segmentation have achieved\nunprecedented progress. In this paper, we propose Complete-IoU (CIoU) loss and\nCluster-NMS for enhancing geometric factors in both bounding box regression and\nNon-Maximum Suppression (NMS), leading to notable gains of average precision\n(AP) and average recall (AR), without the sacrifice of inference efficiency. In\nparticular, we consider three geometric factors, i.e., overlap area, normalized\ncentral point distance and aspect ratio, which are crucial for measuring\nbounding box regression in object detection and instance segmentation. The\nthree geometric factors are then incorporated into CIoU loss for better\ndistinguishing difficult regression cases. The training of deep models using\nCIoU loss results in consistent AP and AR improvements in comparison to widely\nadopted $\\ell_n$-norm loss and IoU-based loss. Furthermore, we propose\nCluster-NMS, where NMS during inference is done by implicitly clustering\ndetected boxes and usually requires less iterations. Cluster-NMS is very\nefficient due to its pure GPU implementation, and geometric factors can be\nincorporated to improve both AP and AR. In the experiments, CIoU loss and\nCluster-NMS have been applied to state-of-the-art instance segmentation (e.g.,\nYOLACT and BlendMask-RT), and object detection (e.g., YOLO v3, SSD and Faster\nR-CNN) models. Taking YOLACT on MS COCO as an example, our method achieves\nperformance gains as +1.7 AP and +6.2 AR$_{100}$ for object detection, and +0.9\nAP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA\nGTX 1080Ti GPU. All the source code and trained models are available at\nhttps://github.com/Zzh-tju/CIoU",
          "link": "http://arxiv.org/abs/2005.03572",
          "publishedOn": "2021-07-06T01:58:07.655Z",
          "wordCount": 771,
          "title": "Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation. (arXiv:2005.03572v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1\">Philip Sellars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>",
          "description": "Can one learn to diagnose COVID-19 under extreme minimal supervision? Since\nthe outbreak of the novel COVID-19 there has been a rush for developing\nArtificial Intelligence techniques for expert-level disease identification on\nChest X-ray data. In particular, the use of deep supervised learning has become\nthe go-to paradigm. However, the performance of such models is heavily\ndependent on the availability of a large and representative labelled dataset.\nThe creation of which is a heavily expensive and time consuming task, and\nespecially imposes a great challenge for a novel disease. Semi-supervised\nlearning has shown the ability to match the incredible performance of\nsupervised models whilst requiring a small fraction of the labelled examples.\nThis makes the semi-supervised paradigm an attractive option for identifying\nCOVID-19. In this work, we introduce a graph based deep semi-supervised\nframework for classifying COVID-19 from chest X-rays. Our framework introduces\nan optimisation model for graph diffusion that reinforces the natural relation\namong the tiny labelled set and the vast unlabelled data. We then connect the\ndiffusion prediction output as pseudo-labels that are used in an iterative\nscheme in a deep net. We demonstrate, through our experiments, that our model\nis able to outperform the current leading supervised model with a tiny fraction\nof the labelled examples. Finally, we provide attention maps to accommodate the\nradiologist's mental model, better fitting their perceptual and cognitive\nabilities. These visualisation aims to assist the radiologist in judging\nwhether the diagnostic is correct or not, and in consequence to accelerate the\ndecision.",
          "link": "http://arxiv.org/abs/2010.00378",
          "publishedOn": "2021-07-06T01:58:07.645Z",
          "wordCount": 777,
          "title": "GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays. (arXiv:2010.00378v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.04320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>",
          "description": "Direct methods are widely used for alignment of models to images, due to\ntheir accuracy, since they minimize errors in the domain of measurement noise.\nThey have leveraged least squares minimizations, for simple, efficient,\nvariational optimization, since the seminal 1981 work of Lucas & Kanade, and\nnormalized cross correlation (NCC), for robustness to intensity variations,\nsince at least 1972. Despite the complementary benefits of these two well known\nmethods, they have not been effectively combined to address local variations in\nintensity. Many ad-hoc NCC frameworks, sub-optimal least squares methods and\nimage transformation approaches have thus been proposed instead, each with\ntheir own limitations. This work shows that a least squares optimization of NCC\nwithout approximation is not only possible, but straightforward and efficient.\nA robust, locally normalized formulation is introduced to mitigate local\nintensity variations and partial occlusions. Finally, sparse features with\noriented patches are proposed for further efficiency. The resulting framework\nis simple to implement, computationally efficient and robust to local intensity\nvariations. It is evaluated on the image alignment problem, showing\nimprovements in both convergence rate and computation time over existing\nlighting invariant methods.",
          "link": "http://arxiv.org/abs/1810.04320",
          "publishedOn": "2021-07-06T01:58:07.637Z",
          "wordCount": 650,
          "title": "Least Squares Normalized Cross Correlation. (arXiv:1810.04320v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Taeeon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Byeongjoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baek_J/0/1/0/all/0/1\">Jongduk Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one,\nBM3D. The official code of our method is available at\nhttps://github.com/csm9493/GAN2GAN.",
          "link": "http://arxiv.org/abs/1905.10488",
          "publishedOn": "2021-07-06T01:58:07.618Z",
          "wordCount": 738,
          "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images. (arXiv:1905.10488v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.",
          "link": "http://arxiv.org/abs/2105.07830",
          "publishedOn": "2021-07-06T01:58:07.594Z",
          "wordCount": 684,
          "title": "Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation. (arXiv:2105.07830v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiabei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yunjia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>",
          "description": "This paper presents a method for gaze estimation according to face images. We\ntrain several gaze estimators adopting four different network architectures,\nincluding an architecture designed for gaze estimation (i.e.,iTracker-MHSA) and\nthree originally designed for general computer vision tasks(i.e., BoTNet,\nHRNet, ResNeSt). Then, we select the best six estimators and ensemble their\npredictions through a linear combination. The method ranks the first on the\nleader-board of ETH-XGaze Competition, achieving an average angular error of\n$3.11^{\\circ}$ on the ETH-XGaze test set.",
          "link": "http://arxiv.org/abs/2107.01980",
          "publishedOn": "2021-07-06T01:58:07.586Z",
          "wordCount": 528,
          "title": "Gaze Estimation with an Ensemble of Four Architectures. (arXiv:2107.01980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzillotta_G/0/1/0/all/0/1\">Guilia Lanzillotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annadani_Y/0/1/0/all/0/1\">Yashas Annadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We study the problem of self-supervised structured representation learning\nusing autoencoders for generative modeling. Unlike most methods which rely on\nmatching an arbitrary, relatively unstructured, prior distribution for\nsampling, we propose a sampling technique that relies solely on the\nindependence of latent variables, thereby avoiding the trade-off between\nreconstruction quality and generative performance inherent to VAEs. We design a\nnovel autoencoder architecture capable of learning a structured representation\nwithout the need for aggressive regularization. Our structural decoders learn a\nhierarchy of latent variables, akin to structural causal models, thereby\nordering the information without any additional regularization. We demonstrate\nhow these models learn a representation that improves results in a variety of\ndownstream tasks including generation, disentanglement, and extrapolation using\nseveral challenging and natural image datasets.",
          "link": "http://arxiv.org/abs/2006.07796",
          "publishedOn": "2021-07-06T01:58:07.579Z",
          "wordCount": 613,
          "title": "Structure by Architecture: Disentangled Representations without Regularization. (arXiv:2006.07796v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liulei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1\">Gustav Bredell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>",
          "description": "Despite recent progress of automatic medical image segmentation techniques,\nfully automatic results usually fail to meet the clinical use and typically\nrequire further refinement. In this work, we propose a quality-aware memory\nnetwork for interactive segmentation of 3D medical images. Provided by user\nguidance on an arbitrary slice, an interaction network is firstly employed to\nobtain an initial 2D segmentation. The quality-aware memory network\nsubsequently propagates the initial segmentation estimation bidirectionally\nover the entire volume. Subsequent refinement based on additional user guidance\non other slices can be incorporated in the same manner. To further facilitate\ninteractive segmentation, a quality assessment module is introduced to suggest\nthe next slice to segment based on the current segmentation quality of each\nslice. The proposed network has two appealing characteristics: 1) The\nmemory-augmented network offers the ability to quickly encode past segmentation\ninformation, which will be retrieved for the segmentation of other slices; 2)\nThe quality assessment module enables the model to directly estimate the\nqualities of segmentation predictions, which allows an active learning paradigm\nwhere users preferentially label the lowest-quality slice for multi-round\nrefinement. The proposed network leads to a robust interactive segmentation\nengine, which can generalize well to various types of user annotations (e.g.,\nscribbles, boxes). Experimental results on various medical datasets demonstrate\nthe superiority of our approach in comparison with existing techniques.",
          "link": "http://arxiv.org/abs/2106.10686",
          "publishedOn": "2021-07-06T01:58:07.570Z",
          "wordCount": 694,
          "title": "Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. (arXiv:2106.10686v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification (Re-ID) via gait features within 3D skeleton\nsequences is a newly-emerging topic with several advantages. Existing solutions\neither rely on hand-crafted descriptors or supervised gait representation\nlearning. This paper proposes a self-supervised gait encoding approach that can\nleverage unlabeled skeleton data to learn gait representations for person\nRe-ID. Specifically, we first create self-supervision by learning to\nreconstruct unlabeled skeleton sequences reversely, which involves richer\nhigh-level semantics to obtain better gait representations. Other pretext tasks\nare also explored to further improve self-supervised learning. Second, inspired\nby the fact that motion's continuity endows adjacent skeletons in one skeleton\nsequence and temporally consecutive skeleton sequences with higher correlations\n(referred as locality in 3D skeleton data), we propose a locality-aware\nattention mechanism and a locality-aware contrastive learning scheme, which aim\nto preserve locality-awareness on intra-sequence level and inter-sequence level\nrespectively during self-supervised learning. Last, with context vectors\nlearned by our locality-aware attention mechanism and contrastive learning\nscheme, a novel feature named Constrastive Attention-based Gait Encodings\n(CAGEs) is designed to represent gait effectively. Empirical evaluations show\nthat our approach significantly outperforms skeleton-based counterparts by\n15-40% Rank-1 accuracy, and it even achieves superior performance to numerous\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/Locality-Awareness-SGE.",
          "link": "http://arxiv.org/abs/2009.03671",
          "publishedOn": "2021-07-06T01:58:07.562Z",
          "wordCount": 742,
          "title": "A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification. (arXiv:2009.03671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zixin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n\nIn this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.",
          "link": "http://arxiv.org/abs/2105.15134",
          "publishedOn": "2021-07-06T01:58:07.536Z",
          "wordCount": 725,
          "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1\">Wenjing Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>",
          "description": "We propose Ray-ONet to reconstruct detailed 3D models from monocular images\nefficiently. By predicting a series of occupancy probabilities along a ray that\nis back-projected from a pixel in the camera coordinate, our method Ray-ONet\nimproves the reconstruction accuracy in comparison with Occupancy Networks\n(ONet), while reducing the network inference complexity to O($N^2$). As a\nresult, Ray-ONet achieves state-of-the-art performance on the ShapeNet\nbenchmark with more than 20$\\times$ speed-up at $128^3$ resolution and\nmaintains a similar memory footprint during inference.",
          "link": "http://arxiv.org/abs/2107.01899",
          "publishedOn": "2021-07-06T01:58:07.516Z",
          "wordCount": 518,
          "title": "Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image. (arXiv:2107.01899v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiemin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Recent advances in self-supervised learning have experienced remarkable\nprogress, especially for contrastive learning based methods, which regard each\nimage as well as its augmentations as an individual class and try to\ndistinguish them from all other images. However, due to the large quantity of\nexemplars, this kind of pretext task intrinsically suffers from slow\nconvergence and is hard for optimization. This is especially true for small\nscale models, which we find the performance drops dramatically comparing with\nits supervised counterpart. In this paper, we propose a simple but effective\ndistillation strategy for unsupervised learning. The highlight is that the\nrelationship among similar samples counts and can be seamlessly transferred to\nthe student to boost the performance. Our method, termed as BINGO, which is\nshort for \\textbf{B}ag of \\textbf{I}nsta\\textbf{N}ces\na\\textbf{G}gregati\\textbf{O}n, targets at transferring the relationship learned\nby the teacher to the student. Here bag of instances indicates a set of similar\nsamples constructed by the teacher and are grouped within a bag, and the goal\nof distillation is to aggregate compact representations over the student with\nrespect to instances in a bag. Notably, BINGO achieves new state-of-the-art\nperformance on small scale models, \\emph{i.e.}, 65.5% and 68.9% top-1\naccuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as\nbackbone, respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies)\nby a significant margin. The code will be available at\n\\url{https://github.com/haohang96/bingo}.",
          "link": "http://arxiv.org/abs/2107.01691",
          "publishedOn": "2021-07-06T01:58:07.319Z",
          "wordCount": 671,
          "title": "Bag of Instances Aggregation Boosts Self-supervised Learning. (arXiv:2107.01691v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Indu Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh_A/0/1/0/all/0/1\">Ayush Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Riya Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sumantra Dutta Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalra_P/0/1/0/all/0/1\">Prem Kumar Kalra</a>",
          "description": "A fingerprint region of interest (roi) segmentation algorithm is designed to\nseparate the foreground fingerprint from the background noise. All the learning\nbased state-of-the-art fingerprint roi segmentation algorithms proposed in the\nliterature are benchmarked on scenarios when both training and testing\ndatabases consist of fingerprint images acquired from the same sensors.\nHowever, when testing is conducted on a different sensor, the segmentation\nperformance obtained is often unsatisfactory. As a result, every time a new\nfingerprint sensor is used for testing, the fingerprint roi segmentation model\nneeds to be re-trained with the fingerprint image acquired from the new sensor\nand its corresponding manually marked ROI. Manually marking fingerprint ROI is\nexpensive because firstly, it is time consuming and more importantly, requires\ndomain expertise. In order to save the human effort in generating annotations\nrequired by state-of-the-art, we propose a fingerprint roi segmentation model\nwhich aligns the features of fingerprint images derived from the unseen sensor\nsuch that they are similar to the ones obtained from the fingerprints whose\nground truth roi masks are available for training. Specifically, we propose a\nrecurrent adversarial learning based feature alignment network that helps the\nfingerprint roi segmentation model to learn sensor-invariant features.\nConsequently, sensor-invariant features learnt by the proposed roi segmentation\nmodel help it to achieve improved segmentation performance on fingerprints\nacquired from the new sensor. Experiments on publicly available FVC databases\ndemonstrate the efficacy of the proposed work.",
          "link": "http://arxiv.org/abs/2107.01361",
          "publishedOn": "2021-07-06T01:58:07.307Z",
          "wordCount": 691,
          "title": "Sensor-invariant Fingerprint ROI Segmentation Using Recurrent Adversarial Learning. (arXiv:2107.01361v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">David Robert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Kazuya Takeda</a>",
          "description": "Interconnected road lanes are a central concept for navigating urban roads.\nCurrently, most autonomous vehicles rely on preconstructed lane maps as\ndesigning an algorithmic model is difficult. However, the generation and\nmaintenance of such maps is costly and hinders large-scale adoption of\nautonomous vehicle technology. This paper presents the first self-supervised\nlearning method to train a model to infer a spatially grounded lane-level road\nnetwork graph based on a dense segmented representation of the road scene\ngenerated from onboard sensors. A formal road lane network model is presented\nand proves that any structured road scene can be represented by a directed\nacyclic graph of at most depth three while retaining the notion of intersection\nregions, and that this is the most compressed representation. The formal model\nis implemented by a hybrid neural and search-based model, utilizing a novel\nbarrier function loss formulation for robust learning from partial labels.\nExperiments are conducted for all common road intersection layouts. Results\nshow that the model can generalize to new road layouts, unlike previous\napproaches, demonstrating its potential for real-world application as a\npractical learning-based lane-level map generator.",
          "link": "http://arxiv.org/abs/2107.01784",
          "publishedOn": "2021-07-06T01:58:07.297Z",
          "wordCount": 650,
          "title": "Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision. (arXiv:2107.01784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01422",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>",
          "description": "Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard methods aim to form images based upon optical absorption or\nfluorescent emission, it is also well-established that the temporal correlation\nof scattered coherent light diffuses through tissue much like optical\nintensity. Few works to date, however, have aimed to experimentally measure and\nprocess such data to demonstrate deep-tissue imaging of decorrelation dynamics.\nIn this work, we take advantage of a single-photon avalanche diode (SPAD) array\ncamera, with over one thousand detectors, to simultaneously detect speckle\nfluctuations at the single-photon level from 12 different phantom tissue\nsurface locations delivered via a customized fiber bundle array. We then apply\na deep neural network to convert the acquired single-photon measurements into\nvideo of scattering dynamics beneath rapidly decorrelating liquid tissue\nphantoms. We demonstrate the ability to record video of dynamic events\noccurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale\nresolution and at a 2.5-10 Hz frame rate.",
          "link": "http://arxiv.org/abs/2107.01422",
          "publishedOn": "2021-07-06T01:58:07.271Z",
          "wordCount": 631,
          "title": "Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01502",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinglong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_N/0/1/0/all/0/1\">Ning Huang</a>",
          "description": "Pulmonary vessel segmentation is important for clinical diagnosis of\npulmonary diseases, while is also challenging due to the complicated structure.\nIn this work, we present an effective framework and refinement process of\npulmonary vessel segmentation from chest computed tomographic (CT) images. The\nkey to our approach is a 2.5D segmentation network applied from three\northogonal axes, which presents a robust and fully automated pulmonary vessel\nsegmentation result with lower network complexity and memory usage compared to\n3D networks. The slice radius is introduced to convolve the adjacent\ninformation of the center slice and the multi-planar fusion optimizes the\npresentation of intra- and inter- slice features. Besides, the tree-like\nstructure of the pulmonary vessel is extracted in the post-processing process,\nwhich is used for segmentation refining and pruning. In the evaluation\nexperiments, three fusion methods are tested and the most promising one is\ncompared with the state-of-the-art 2D and 3D structures on 300 cases of lung\nimages randomly selected from LIDC dataset. Our method outperforms other\nnetwork structures by a large margin and achieves by far the highest average\nDICE score of 0.9272 and precision of 0.9310, as per our knowledge from the\npulmonary vessel segmentation models available in the literature.",
          "link": "http://arxiv.org/abs/2107.01502",
          "publishedOn": "2021-07-06T01:58:07.263Z",
          "wordCount": 677,
          "title": "Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images. (arXiv:2107.01502v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Indu Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utkarsh_A/0/1/0/all/0/1\">Ayush Utkarsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_R/0/1/0/all/0/1\">Riya Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod K Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sumantra Dutta Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalra_P/0/1/0/all/0/1\">Prem Kumar Kalra</a>",
          "description": "The effectiveness of fingerprint-based authentication systems on good quality\nfingerprints is established long back. However, the performance of standard\nfingerprint matching systems on noisy and poor quality fingerprints is far from\nsatisfactory. Towards this, we propose a data uncertainty-based framework which\nenables the state-of-the-art fingerprint preprocessing models to quantify noise\npresent in the input image and identify fingerprint regions with background\nnoise and poor ridge clarity. Quantification of noise helps the model two\nfolds: firstly, it makes the objective function adaptive to the noise in a\nparticular input fingerprint and consequently, helps to achieve robust\nperformance on noisy and distorted fingerprint regions. Secondly, it provides a\nnoise variance map which indicates noisy pixels in the input fingerprint image.\nThe predicted noise variance map enables the end-users to understand erroneous\npredictions due to noise present in the input image. Extensive experimental\nevaluation on 13 publicly available fingerprint databases, across different\narchitectural choices and two fingerprint processing tasks demonstrate\neffectiveness of the proposed framework.",
          "link": "http://arxiv.org/abs/2107.01248",
          "publishedOn": "2021-07-06T01:58:07.247Z",
          "wordCount": 613,
          "title": "Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints. (arXiv:2107.01248v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Briq_R/0/1/0/all/0/1\">Rania Briq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochar_P/0/1/0/all/0/1\">Pratika Kochar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>",
          "description": "This paper proposes an approach that generates multiple 3D human meshes from\ntext. The human shapes are represented by 3D meshes based on the SMPL model.\nThe model's performance is evaluated on the COCO dataset, which contains\nchallenging human shapes and intricate interactions between individuals. The\nmodel is able to capture the dynamics of the scene and the interactions between\nindividuals based on text. We further show how using such a shape as input to\nimage synthesis frameworks helps to constrain the network to synthesize humans\nwith realistic human shapes.",
          "link": "http://arxiv.org/abs/2107.01869",
          "publishedOn": "2021-07-06T01:58:07.241Z",
          "wordCount": 525,
          "title": "Towards Better Adversarial Synthesis of Human Images from Text. (arXiv:2107.01869v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01456",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1\">Quoc Huy Trinh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>",
          "description": "3D CT-scan base on chest is one of the controversial topisc of the researcher\nnowadays. There are many tasks to diagnose the disease through CT-scan images,\ninclude Covid19. In this paper, we propose a method that custom and combine\nDeep Neural Network to classify the series of 3D CT-scans chest images. In our\nmethods, we experiment with 2 backbones is DenseNet 121 and ResNet 101. In this\nproposal, we separate the experiment into 2 tasks, one is for 2 backbones\ncombination of ResNet and DenseNet, one is for DenseNet backbones combination.",
          "link": "http://arxiv.org/abs/2107.01456",
          "publishedOn": "2021-07-06T01:58:07.222Z",
          "wordCount": 582,
          "title": "Custom Deep Neural Network for 3D Covid Chest CT-scan Classification. (arXiv:2107.01456v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01327",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang C. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_T/0/1/0/all/0/1\">Tung T. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>",
          "description": "We introduce a new benchmark dataset, namely VinDr-RibCXR, for automatic\nsegmentation and labeling of individual ribs from chest X-ray (CXR) scans. The\nVinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations\nprovided by human experts. A set of state-of-the-art segmentation models are\ntrained on 196 images from the VinDr-RibCXR to segment and label 20 individual\nribs. Our best performing model obtains a Dice score of 0.834 (95% CI,\n0.810--0.853) on an independent test set of 49 images. Our study, therefore,\nserves as a proof of concept and baseline performance for future research.",
          "link": "http://arxiv.org/abs/2107.01327",
          "publishedOn": "2021-07-06T01:58:07.215Z",
          "wordCount": 576,
          "title": "VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and Labeling of Individual Ribs on Chest X-rays. (arXiv:2107.01327v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
          "link": "http://arxiv.org/abs/2107.01579",
          "publishedOn": "2021-07-06T01:58:07.193Z",
          "wordCount": 663,
          "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation. (arXiv:2107.01579v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "Coronavirus is a large virus family consisting of diverse viruses, some of\nwhich disseminate among mammals and others cause sickness among humans.\nCOVID-19 is highly contagious and is rapidly spreading, rendering its early\ndiagnosis of preeminent status. Researchers, medical specialists and\norganizations all over the globe have been working tirelessly to combat this\nvirus and help in its containment. In this paper, a novel neural network called\nWisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.\nThe WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is\na two-layered convolutional Neural Network (CNN), which takes chest x-ray\nimages as input. Both layers of the proposed neural network consist of a number\nof neural networks each. The dataset used for this study consists of chest\nx-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on\nGitHub, and the chest x-ray images of healthy lungs and lungs affected by viral\nand bacterial pneumonia were obtained from Kaggle. The network not only\npinpoints the presence of COVID-19, but also gives the probability of the\ndisease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,\npredicting the progression of the disease in the COVID-19 positive patients.\nThe network also slender the occurrences of false negative cases by employing a\nhigh threshold value, thus aids in curbing the spread of the disease and gives\nan accuracy of 100% for successfully predicting COVID-19 among the chest x-rays\nof patients affected with COVID-19, bacterial and viral pneumonia.",
          "link": "http://arxiv.org/abs/2107.01392",
          "publishedOn": "2021-07-06T01:58:07.185Z",
          "wordCount": 795,
          "title": "WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays. (arXiv:2107.01392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">David Wipf Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-06T01:58:07.165Z",
          "wordCount": 608,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01748",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Acquiring annotated data at scale with rare diseases or conditions remains a\nchallenge. It would be extremely useful to have a method that controllably\nsynthesizes images that can correct such underrepresentation. Assuming a proper\nlatent representation, the idea of a \"latent vector arithmetic\" could offer the\nmeans of achieving such synthesis. A proper representation must encode the\nfidelity of the input data, preserve invariance and equivariance, and permit\narithmetic operations. Motivated by the ability to disentangle images into\nspatial anatomy (tensor) factors and accompanying imaging (vector)\nrepresentations, we propose a framework termed \"disentangled anatomy\narithmetic\", in which a generative model learns to combine anatomical factors\nof different input images such that when they are re-entangled with the desired\nimaging modality (e.g. MRI), plausible new cardiac images are created with the\ntarget characteristics. To encourage a realistic combination of anatomy factors\nafter the arithmetic step, we propose a localized noise injection network that\nprecedes the generator. Our model is used to generate realistic images,\npathology labels, and segmentation masks that are used to augment the existing\ndatasets and subsequently improve post-hoc classification and segmentation\ntasks. Code is publicly available at https://github.com/vios-s/DAA-GAN.",
          "link": "http://arxiv.org/abs/2107.01748",
          "publishedOn": "2021-07-06T01:58:07.158Z",
          "wordCount": 642,
          "title": "Controllable cardiac synthesis via disentangled anatomy arithmetic. (arXiv:2107.01748v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01318",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Toledo_M/0/1/0/all/0/1\">Marcelo Toledo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_D/0/1/0/all/0/1\">Daniel Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krieger_J/0/1/0/all/0/1\">Jos&#xe9; Krieger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutierrez_M/0/1/0/all/0/1\">Marco Gutierrez</a>",
          "description": "CNN (Convolutional Neural Network) models have been successfully used for\nsegmentation of the left ventricle (LV) in cardiac MRI (Magnetic Resonance\nImaging), providing clinical measurements.In practice, two questions arise with\ndeployment of CNNs: 1) when is it better to use a shallow model instead of a\ndeeper one? 2) how the size of a dataset might change the network performance?\nWe propose a framework to answer them, by experimenting with deep and shallow\nversions of three U-Net families, trained from scratch in six subsets varying\nfrom 100 to 10,000 images, different network sizes, learning rates and\nregularization values. 1620 models were evaluated using 5-foldcross-validation\nby loss and DICE. The results indicate that: sample size affects performance\nmore than architecture or hyper-parameters; in small samples the performance is\nmore sensitive to hyper-parameters than architecture; the performance\ndifference between shallow and deeper networks is not the same across families.",
          "link": "http://arxiv.org/abs/2107.01318",
          "publishedOn": "2021-07-06T01:58:07.144Z",
          "wordCount": 623,
          "title": "A study of CNN capacity applied to Left Venticle Segmentation in Cardiac MRI. (arXiv:2107.01318v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pricope_T/0/1/0/all/0/1\">Tidor-Vlad Pricope</a>",
          "description": "Classifying hand-written digits and letters has taken a big leap with the\nintroduction of ConvNets. However, on very constrained hardware the time\nnecessary to train such models would be high. Our main contribution is twofold.\nFirst, we extensively test an end-to-end vanilla neural network (MLP) approach\nin pure numpy without any pre-processing or feature extraction done beforehand.\nSecond, we show that basic data mining operations can significantly improve the\nperformance of the models in terms of computational time, without sacrificing\nmuch accuracy. We illustrate our claims on a simpler variant of the Extended\nMNIST dataset, called Balanced EMNIST dataset. Our experiments show that,\nwithout any data mining, we get increased generalization performance when using\nmore hidden layers and regularization techniques, the best model achieving\n84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA\nwe were able to increase that figure to 85.08% with only 10% of the original\nfeature space, reducing the memory size needed by 64%. Finally, adding methods\nto remove possibly harmful training samples like deviation from the mean helped\nus to still achieve over 84% test accuracy but with only 32.8% of the original\nmemory size for the training set. This compares favorably to the majority of\nliterature results obtained through similar architectures. Although this\napproach gets outshined by state-of-the-art models, it does scale to some\n(AlexNet, VGGNet) trained on 50% of the same dataset.",
          "link": "http://arxiv.org/abs/2107.01782",
          "publishedOn": "2021-07-06T01:58:07.103Z",
          "wordCount": 675,
          "title": "A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources. (arXiv:2107.01782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Naftali Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_S/0/1/0/all/0/1\">Srijan Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>",
          "description": "In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.",
          "link": "http://arxiv.org/abs/2107.01273",
          "publishedOn": "2021-07-06T01:58:07.095Z",
          "wordCount": 554,
          "title": "Visual Time Series Forecasting: An Image-driven Approach. (arXiv:2107.01273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_F/0/1/0/all/0/1\">Fatemeh Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabi_R/0/1/0/all/0/1\">Roozbeh Rajabi</a>",
          "description": "In image processing, it is essential to detect and track air targets,\nespecially UAVs. In this paper, we detect the flying drone using a fisheye\ncamera. In the field of diagnosis and classification of objects, there are\nalways many problems that prevent the development of rapid and significant\nprogress in this area. During the previous decades, a couple of advanced\nclassification methods such as convolutional neural networks and support vector\nmachines have been developed. In this study, the drone was detected using three\nmethods of classification of convolutional neural network (CNN), support vector\nmachine (SVM), and nearest neighbor. The outcomes show that CNN, SVM, and\nnearest neighbor have total accuracy of 95%, 88%, and 80%, respectively.\nCompared with other classifiers with the same experimental conditions, the\naccuracy of the convolutional neural network classifier is satisfactory.",
          "link": "http://arxiv.org/abs/2107.01435",
          "publishedOn": "2021-07-06T01:58:07.085Z",
          "wordCount": 567,
          "title": "Drone Detection Using Convolutional Neural Networks. (arXiv:2107.01435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>",
          "description": "Visual Commonsense Reasoning (VCR) predicts an answer with corresponding\nrationale, given a question-image input. VCR is a recently introduced visual\nscene understanding task with a wide range of applications, including visual\nquestion answering, automated vehicle systems, and clinical decision support.\nPrevious approaches to solving the VCR task generally rely on pre-training or\nexploiting memory with long dependency relationship encoded models. However,\nthese approaches suffer from a lack of generalizability and prior knowledge. In\nthis paper we propose a dynamic working memory based cognitive VCR network,\nwhich stores accumulated commonsense between sentences to provide prior\nknowledge for inference. Extensive experiments show that the proposed model\nyields significant improvements over existing methods on the benchmark VCR\ndataset. Moreover, the proposed model provides intuitive interpretation into\nvisual commonsense reasoning. A Python implementation of our mechanism is\npublicly available at https://github.com/tanjatang/DMVCR",
          "link": "http://arxiv.org/abs/2107.01671",
          "publishedOn": "2021-07-06T01:58:07.075Z",
          "wordCount": 568,
          "title": "Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.01330",
          "publishedOn": "2021-07-06T01:58:07.024Z",
          "wordCount": 611,
          "title": "SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network. (arXiv:2107.01330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jareno_S/0/1/0/all/0/1\">Santos J. N&#xfa;&#xf1;ez Jare&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helden_D/0/1/0/all/0/1\">Dani&#xeb;l P. van Helden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M. Mirkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allison_P/0/1/0/all/0/1\">Penelope M. Allison</a>",
          "description": "In this article we consider a version of the challenging problem of learning\nfrom datasets whose size is too limited to allow generalisation beyond the\ntraining set. To address the challenge we propose to use a transfer learning\napproach whereby the model is first trained on a synthetic dataset replicating\nfeatures of the original objects. In this study the objects were smartphone\nphotographs of near-complete Roman terra sigillata pottery vessels from the\ncollection of the Museum of London. Taking the replicated features from\npublished profile drawings of pottery forms allowed the integration of expert\nknowledge into the process through our synthetic data generator. After this\nfirst initial training the model was fine-tuned with data from photographs of\nreal vessels. We show, through exhaustive experiments across several popular\ndeep learning architectures, different test priors, and considering the impact\nof the photograph viewpoint and excessive damage to the vessels, that the\nproposed hybrid approach enables the creation of classifiers with appropriate\ngeneralisation performance. This performance is significantly better than that\nof classifiers trained exclusively on the original data which shows the promise\nof the approach to alleviate the fundamental issue of learning from small\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01401",
          "publishedOn": "2021-07-06T01:58:07.017Z",
          "wordCount": 652,
          "title": "Learning from scarce information: using synthetic data to classify Roman fine ware pottery. (arXiv:2107.01401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01527",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Enshaei_N/0/1/0/all/0/1\">Nastaran Enshaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rafiee_M/0/1/0/all/0/1\">Moezedin Javad Rafiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>",
          "description": "Novel Coronavirus disease (COVID-19) is a highly contagious respiratory\ninfection that has had devastating effects on the world. Recently, new COVID-19\nvariants are emerging making the situation more challenging and threatening.\nEvaluation and quantification of COVID-19 lung abnormalities based on chest\nComputed Tomography (CT) scans can help determining the disease stage,\nefficiently allocating limited healthcare resources, and making informed\ntreatment decisions. During pandemic era, however, visual assessment and\nquantification of COVID-19 lung lesions by expert radiologists become expensive\nand prone to error, which raises an urgent quest to develop practical\nautonomous solutions. In this context, first, the paper introduces an open\naccess COVID-19 CT segmentation dataset containing 433 CT images from 82\npatients that have been annotated by an expert radiologist. Second, a Deep\nNeural Network (DNN)-based framework is proposed, referred to as the\nCOVID-Rate, that autonomously segments lung abnormalities associated with\nCOVID-19 from chest CT scans. Performance of the proposed COVID-Rate framework\nis evaluated through several experiments based on the introduced and external\ndatasets. The results show a dice score of 0:802 and specificity and\nsensitivity of 0:997 and 0:832, respectively. Furthermore, the results indicate\nthat the COVID-Rate model can efficiently segment COVID-19 lesions in both 2D\nCT images and whole lung volumes. Results on the external dataset illustrate\ngeneralization capabilities of the COVID-Rate model to CT images obtained from\na different scanner.",
          "link": "http://arxiv.org/abs/2107.01527",
          "publishedOn": "2021-07-06T01:58:06.983Z",
          "wordCount": 733,
          "title": "COVID-Rate: An Automated Framework for Segmentation of COVID-19 Lesions from Chest CT Scans. (arXiv:2107.01527v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification via 3D skeletons is an emerging topic with great\npotential in security-critical applications. Existing methods typically learn\nbody and motion features from the body-joint trajectory, whereas they lack a\nsystematic way to model body structure and underlying relations of body\ncomponents beyond the scale of body joints. In this paper, we for the first\ntime propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE)\nframework that comprehensively models human body, component relations, and\nskeleton dynamics from unlabeled skeleton graphs of various scales to learn an\neffective skeleton representation for person Re-ID. Specifically, we first\ndevise multi-scale skeleton graphs with coarse-to-fine human body partitions,\nwhich enables us to model body structure and skeleton dynamics at multiple\nlevels. Second, to mine inherent correlations between body components in\nskeletal motion, we propose a multi-scale graph relation network to learn\nstructural relations between adjacent body-component nodes and collaborative\nrelations among nodes of different scales, so as to capture more discriminative\nskeleton graph features. Last, we propose a novel multi-scale skeleton\nreconstruction mechanism to enable our framework to encode skeleton dynamics\nand high-level semantics from unlabeled skeleton graphs, which encourages\nlearning a discriminative skeleton representation for person Re-ID. Extensive\nexperiments show that SM-SGE outperforms most state-of-the-art skeleton-based\nmethods. We further demonstrate its effectiveness on 3D skeleton data estimated\nfrom large-scale RGB videos. Our codes are open at\nhttps://github.com/Kali-Hac/SM-SGE.",
          "link": "http://arxiv.org/abs/2107.01903",
          "publishedOn": "2021-07-06T01:58:06.957Z",
          "wordCount": 691,
          "title": "SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification. (arXiv:2107.01903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>",
          "description": "RGB-D salient object detection (SOD) recently has attracted increasing\nresearch interest by benefiting conventional RGB SOD with extra depth\ninformation. However, existing RGB-D SOD models often fail to perform well in\nterms of both efficiency and accuracy, which hinders their potential\napplications on mobile devices and real-world problems. An underlying challenge\nis that the model accuracy usually degrades when the model is simplified to\nhave few parameters. To tackle this dilemma and also inspired by the fact that\ndepth quality is a key factor influencing the accuracy, we propose a novel\ndepth quality-inspired feature manipulation (DQFM) process, which is efficient\nitself and can serve as a gating mechanism for filtering depth features to\ngreatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and\ndepth features, as well as holistic attention of the depth stream to explicitly\ncontrol and enhance cross-modal fusion. We embed DQFM to obtain an efficient\nlight-weight model called DFM-Net, where we also design a tailored depth\nbackbone and a two-stage decoder for further efficiency consideration.\nExtensive experimental results demonstrate that our DFM-Net achieves\nstate-of-the-art accuracy when comparing to existing non-efficient models, and\nmeanwhile runs at 140ms on CPU (2.2$\\times$ faster than the prior fastest\nefficient model) with only $\\sim$8.5Mb model size (14.9% of the prior\nlightest). Our code will be made publicly available.",
          "link": "http://arxiv.org/abs/2107.01779",
          "publishedOn": "2021-07-06T01:58:06.949Z",
          "wordCount": 660,
          "title": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection. (arXiv:2107.01779v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Stephen Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Saurajit Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phadke_U/0/1/0/all/0/1\">Unmesh Phadke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tingting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yada_R/0/1/0/all/0/1\">Ravi Theja Yada</a>",
          "description": "In this paper, we present Generic Object Detection (GenOD), one of the\nlargest object detection systems deployed to a web-scale general visual search\nengine that can detect over 900 categories for all Microsoft Bing Visual Search\nqueries in near real-time. It acts as a fundamental visual query understanding\nservice that provides object-centric information and shows gains in multiple\nproduction scenarios, improving upon domain-specific models. We discuss the\nchallenges of collecting data, training, deploying and updating such a\nlarge-scale object detection model with multiple dependencies. We discuss a\ndata collection pipeline that reduces per-bounding box labeling cost by 81.5%\nand latency by 61.2% while improving on annotation quality. We show that GenOD\ncan improve weighted average precision by over 20% compared to multiple\ndomain-specific models. We also improve the model update agility by nearly 2\ntimes with the proposed disjoint detector training compared to joint\nfine-tuning. Finally we demonstrate how GenOD benefits visual search\napplications by significantly improving object-level search relevance by 54.9%\nand user engagement by 59.9%.",
          "link": "http://arxiv.org/abs/2107.01814",
          "publishedOn": "2021-07-06T01:58:06.905Z",
          "wordCount": 630,
          "title": "Web-Scale Generic Object Detection at Microsoft Bing. (arXiv:2107.01814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01682",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiaohong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yu Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1\">Alice Gao</a>",
          "description": "This paper is responding to the MIA-COV19 challenge to classify COVID from\nnon-COVID based on CT lung images. The COVID-19 virus has devastated the world\nin the last eighteen months by infecting more than 182 million people and\ncausing over 3.9 million deaths. The overarching aim is to predict the\ndiagnosis of the COVID-19 virus from chest radiographs, through the development\nof explainable vision transformer deep learning techniques, leading to\npopulation screening in a more rapid, accurate and transparent way. In this\ncompetition, there are 5381 three-dimensional (3D) datasets in total, including\n1552 for training, 374 for evaluation and 3455 for testing. While most of the\ndata volumes are in axial view, there are a number of subjects' data are in\ncoronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D\ndata based classification is investigated, in this competition, 2D images\nremains the main focus. Two deep learning methods are studied, which are vision\ntransformer (ViT) based on attention models and DenseNet that is built upon\nconventional convolutional neural network (CNN). Initial evaluation results\nbased on validation datasets whereby the ground truth is known indicate that\nViT performs better than DenseNet with F1 scores being 0.76 and 0.72\nrespectively. Codes are available at GitHub at\n<https://github/xiaohong1/COVID-ViT>.",
          "link": "http://arxiv.org/abs/2107.01682",
          "publishedOn": "2021-07-06T01:58:06.878Z",
          "wordCount": 709,
          "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models. (arXiv:2107.01682v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shengang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yu-an Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanxin Zhang</a>",
          "description": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples. Adversarial examples are malicious images with visually imperceptible\nperturbations. While these carefully crafted perturbations restricted with\ntight $\\Lp$ norm bounds are small, they are still easily perceivable by humans.\nThese perturbations also have limited success rates when attacking black-box\nmodels or models with defenses like noise reduction filters. To solve these\nproblems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations\nwith Perceptual Similarity. Specifically, we can create powerful and\nphotorealistic adversarial examples by manipulating semantic information based\non Perceptual Similarity. Adversarial examples we generate are friendly to the\nhuman visual system (HVS), although the perturbations are of large magnitudes.\nWe extend widely-used attacks with our approach, enhancing adversarial\neffectiveness impressively while contributing to imperceptibility. Extensive\nexperiments show that the proposed method not only outperforms various\nstate-of-the-art attacks in terms of fooling rate, transferability, and\nrobustness against defenses but can also improve attacks effectively. In\naddition, we also notice that our implementation can simulate illumination and\ncontrast changes that occur in real-world scenarios, which will contribute to\nexposing the blind spots of DNNs.",
          "link": "http://arxiv.org/abs/2107.01396",
          "publishedOn": "2021-07-06T01:58:06.849Z",
          "wordCount": 632,
          "title": "Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity. (arXiv:2107.01396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1\">Fahim Faisal Niloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arif/0/1/0/all/0/1\">Arif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayem_A/0/1/0/all/0/1\">Abu Bakar Siddik Nayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1\">Anis Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_O/0/1/0/all/0/1\">Ovi Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M. Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaber_M/0/1/0/all/0/1\">Moinul Islam Zaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">AKM Mahbubur Rahman</a>",
          "description": "The advancement of deep learning technology has enabled us to develop systems\nthat outperform any other classification technique. However, success of any\nempirical system depends on the quality and diversity of the data available to\ntrain the proposed system. In this research, we have carefully accumulated a\nrelatively challenging dataset that contains images collected from various\nsources for three different disasters: fire, water and land. Besides this, we\nhave also collected images for various damaged infrastructure due to natural or\nman made calamities and damaged human due to war or accidents. We have also\naccumulated image data for a class named non-damage that contains images with\nno such disaster or sign of damage in them. There are 13,720 manually annotated\nimages in this dataset, each image is annotated by three individuals. We are\nalso providing discriminating image class information annotated manually with\nbounding box for a set of 200 test images. Images are collected from different\nnews portals, social media, and standard datasets made available by other\nresearchers. A three layer attention model (TLAM) is trained and average five\nfold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen\ntest images this accuracy is 96.48%. We also generate and compare attention\nmaps for these test images to determine the characteristics of the trained\nattention model. Our dataset is available at\nhttps://niloy193.github.io/Disaster-Dataset",
          "link": "http://arxiv.org/abs/2107.01284",
          "publishedOn": "2021-07-06T01:58:06.839Z",
          "wordCount": 688,
          "title": "A Novel Disaster Image Dataset and Characteristics Analysis using Attention Model. (arXiv:2107.01284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yunfeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>",
          "description": "Traditional crowd counting approaches usually use Gaussian assumption to\ngenerate pseudo density ground truth, which suffers from problems like\ninaccurate estimation of the Gaussian kernel sizes. In this paper, we propose a\nnew measure-based counting approach to regress the predicted density maps to\nthe scattered point-annotated ground truth directly. First, crowd counting is\nformulated as a measure matching problem. Second, we derive a semi-balanced\nform of Sinkhorn divergence, based on which a Sinkhorn counting loss is\ndesigned for measure matching. Third, we propose a self-supervised mechanism by\ndevising a Sinkhorn scale consistency loss to resist scale changes. Finally, an\nefficient optimization method is provided to minimize the overall loss\nfunction. Extensive experiments on four challenging crowd counting datasets\nnamely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed\nmethod.",
          "link": "http://arxiv.org/abs/2107.01558",
          "publishedOn": "2021-07-06T01:58:06.829Z",
          "wordCount": 577,
          "title": "Direct Measure Matching for Crowd Counting. (arXiv:2107.01558v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01337",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Selim_M/0/1/0/all/0/1\">Md Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guo-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>",
          "description": "While remarkable advances have been made in Computed Tomography (CT),\ncapturing CT images with non-standardized protocols causes low reproducibility\nregarding radiomic features, forming a barrier on CT image analysis in a large\nscale. RadiomicGAN is developed to effectively mitigate the discrepancy caused\nby using non-standard reconstruction kernels. RadiomicGAN consists of hybrid\nneural blocks including both pre-trained and trainable layers adopted to learn\nradiomic feature distributions efficiently. A novel training approach, called\nDynamic Window-based Training, has been developed to smoothly transform the\npre-trained model to the medical imaging domain. Model performance evaluated\nusing 1401 radiomic features show that RadiomicGAN clearly outperforms the\nstate-of-art image standardization models.",
          "link": "http://arxiv.org/abs/2107.01337",
          "publishedOn": "2021-07-06T01:58:06.817Z",
          "wordCount": 553,
          "title": "CT Image Harmonization for Enhancing Radiomics Studies. (arXiv:2107.01337v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingbo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>",
          "description": "With the increasing demand for search and rescue, it is highly demanded to\ndetect objects of interest in large-scale images captured by Unmanned Aerial\nVehicles (UAVs), which is quite challenging due to extremely small scales of\nobjects. Most existing methods employed Feature Pyramid Network (FPN) to enrich\nshallow layers' features by combing deep layers' contextual features. However,\nunder the limitation of the inconsistency in gradient computation across\ndifferent layers, the shallow layers in FPN are not fully exploited to detect\ntiny objects. In this paper, we propose a Scale Selection Pyramid network\n(SSPNet) for tiny person detection, which consists of three components: Context\nAttention Module (CAM), Scale Enhancement Module (SEM), and Scale Selection\nModule (SSM). CAM takes account of context information to produce hierarchical\nattention heatmaps. SEM highlights features of specific scales at different\nlayers, leading the detector to focus on objects of specific scales instead of\nvast backgrounds. SSM exploits adjacent layers' relationships to fulfill\nsuitable feature sharing between deep layers and shallow layers, thereby\navoiding the inconsistency in gradient computation across different layers.\nBesides, we propose a Weighted Negative Sampling (WNS) strategy to guide the\ndetector to select more representative samples. Experiments on the TinyPerson\nbenchmark show that our method outperforms other state-of-the-art (SOTA)\ndetectors.",
          "link": "http://arxiv.org/abs/2107.01548",
          "publishedOn": "2021-07-06T01:58:06.781Z",
          "wordCount": 654,
          "title": "SSPNet: Scale Selection Pyramid Network for Tiny Person Detection from UAV Images. (arXiv:2107.01548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentelli_A/0/1/0/all/0/1\">Alberto Sangiovanni Vincentelli</a>",
          "description": "Object detection is essential to safe autonomous or assisted driving.\nPrevious works usually utilize RGB images or LiDAR point clouds to identify and\nlocalize multiple objects in self-driving. However, cameras tend to fail in bad\ndriving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are\ntoo expensive to get widely deployed in commercial applications. Radar has been\ndrawing more and more attention due to its robustness and low cost. In this\npaper, we propose a scene-aware radar learning framework for accurate and\nrobust object detection. First, the learning framework contains branches\nconditioning on the scene category of the radar sequence; with each branch\noptimized for a specific type of scene. Second, three different 3D\nautoencoder-based architectures are proposed for radar object detection and\nensemble learning is performed over the different architectures to further\nboost the final performance. Third, we propose novel scene-aware sequence mix\naugmentation (SceneMix) and scene-specific post-processing to generate more\nrobust detection results. In the ROD2021 Challenge, we achieved a final result\nof average precision of 75.0% and an average recall of 81.0%. Moreover, in the\nparking lot scene, our framework ranks first with an average precision of 97.8%\nand an average recall of 98.6%, which demonstrates the effectiveness of our\nframework.",
          "link": "http://arxiv.org/abs/2107.01469",
          "publishedOn": "2021-07-06T01:58:06.752Z",
          "wordCount": 641,
          "title": "Scene-aware Learning Network for Radar Object Detection. (arXiv:2107.01469v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eungyeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeonghoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Somi Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Choonghyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Deep image colorization networks often suffer from the color-bleeding\nartifact, a problematic color spreading near the boundaries between adjacent\nobjects. The color-bleeding artifacts debase the reality of generated outputs,\nlimiting the applicability of colorization models on a practical application.\nAlthough previous approaches have tackled this problem in an automatic manner,\nthey often generate imperfect outputs because their enhancements are available\nonly in limited cases, such as having a high contrast of gray-scale value in an\ninput image. Instead, leveraging user interactions would be a promising\napproach, since it can help the edge correction in the desired regions. In this\npaper, we propose a novel edge-enhancing framework for the regions of interest,\nby utilizing user scribbles that indicate where to enhance. Our method requires\nminimal user effort to obtain satisfactory enhancements. Experimental results\non various datasets demonstrate that our interactive approach has outstanding\nperformance in improving color-bleeding artifacts against the existing\nbaselines.",
          "link": "http://arxiv.org/abs/2107.01619",
          "publishedOn": "2021-07-06T01:58:06.735Z",
          "wordCount": 588,
          "title": "Deep Edge-Aware Interactive Colorization against Color-Bleeding Effects. (arXiv:2107.01619v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01351",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>",
          "description": "The precise detection of blood vessels in retinal images is crucial to the\nearly diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive\nand solar retinopathies. Existing works often fail in predicting the abnormal\nareas, e.g, sudden brighter and darker areas and are inclined to predict a\npixel to background due to the significant class imbalance, leading to high\naccuracy and specificity while low sensitivity. To that end, we propose a novel\nerror attention refining network (ERA-Net) that is capable of learning and\npredicting the potential false predictions in a two-stage manner for effective\nretinal vessel segmentation. The proposed ERA-Net in the refine stage drives\nthe model to focus on and refine the segmentation errors produced in the\ninitial training stage. To achieve this, unlike most previous attention\napproaches that run in an unsupervised manner, we introduce a novel error\nattention mechanism which considers the differences between the ground truth\nand the initial segmentation masks as the ground truth to supervise the\nattention map learning. Experimental results demonstrate that our method\nachieves state-of-the-art performance on two common retinal blood vessel\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01351",
          "publishedOn": "2021-07-06T01:58:06.346Z",
          "wordCount": 631,
          "title": "EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation. (arXiv:2107.01351v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yanwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_H/0/1/0/all/0/1\">Haixu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fazheng Wang</a>",
          "description": "Offline Chinese handwriting text recognition is a long-standing research\ntopic in the field of pattern recognition. In previous studies, text detection\nand recognition are separated, which leads to the fact that text recognition is\nhighly dependent on the detection results. In this paper, we propose a robust\nend-to-end Chinese text page spotter framework. It unifies text detection and\ntext recognition with text kernel that integrates global text feature\ninformation to optimize the recognition from multiple scales, which reduces the\ndependence of detection and improves the robustness of the system. Our method\nachieves state-of-the-art results on the CASIA-HWDB2.0-2.2 dataset and\nICDAR-2013 competition dataset. Without any language model, the correct rates\nare 99.12% and 94.27% for line-level recognition, and 99.03% and 94.20% for\npage-level recognition, respectively.",
          "link": "http://arxiv.org/abs/2107.01547",
          "publishedOn": "2021-07-06T01:58:06.338Z",
          "wordCount": 567,
          "title": "Robust End-to-End Offline Chinese Handwriting Text Page Spotter with Text Kernel. (arXiv:2107.01547v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Sandeep Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufraisse_M/0/1/0/all/0/1\">Marius Dufraisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_G/0/1/0/all/0/1\">Girish Varma</a>",
          "description": "Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.",
          "link": "http://arxiv.org/abs/2107.01358",
          "publishedOn": "2021-07-06T01:58:06.267Z",
          "wordCount": 623,
          "title": "CInC Flow: Characterizable Invertible 3x3 Convolution. (arXiv:2107.01358v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "Continual learning has been a major problem in the deep learning community,\nwhere the main challenge is how to effectively learn a series of newly arriving\ntasks without forgetting the knowledge of previous tasks. Initiated by Learning\nwithout Forgetting (LwF), many of the existing works report that knowledge\ndistillation is effective to preserve the previous knowledge, and hence they\ncommonly use a soft label for the old task, namely a knowledge distillation\n(KD) loss, together with a class label for the new task, namely a cross entropy\n(CE) loss, to form a composite loss for a single neural network. However, this\napproach suffers from learning the knowledge by a CE loss as a KD loss often\nmore strongly influences the objective function when they are in a competitive\nsituation within a single network. This could be a critical problem\nparticularly in a class incremental scenario, where the knowledge across tasks\nas well as within the new task, both of which can only be acquired by a CE\nloss, is essentially learned due to the existence of a unified classifier. In\nthis paper, we propose a novel continual learning method, called\nSplit-and-Bridge, which can successfully address the above problem by partially\nsplitting a neural network into two partitions for training the new task\nseparated from the old task and re-connecting them for learning the knowledge\nacross tasks. In our thorough experimental analysis, our Split-and-Bridge\nmethod outperforms the state-of-the-art competitors in KD-based continual\nlearning.",
          "link": "http://arxiv.org/abs/2107.01349",
          "publishedOn": "2021-07-06T01:58:06.257Z",
          "wordCount": 704,
          "title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network. (arXiv:2107.01349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "This paper studies the model compression problem of vision transformers.\nBenefit from the self-attention module, transformer architectures have shown\nextraordinary performance on many computer vision tasks. Although the network\nperformance is boosted, transformers are often required more computational\nresources including memory usage and the inference complexity. Compared with\nthe existing knowledge distillation approaches, we propose to excavate useful\ninformation from the teacher transformer through the relationship between\nimages and the divided patches. We then explore an efficient fine-grained\nmanifold distillation approach that simultaneously calculates cross-images,\ncross-patch, and random-selected manifolds in teacher and student models.\nExperimental results conducted on several benchmarks demonstrate the\nsuperiority of the proposed algorithm for distilling portable transformer\nmodels with higher performance. For example, our approach achieves 75.06% Top-1\naccuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which\noutperforms other ViT distillation methods.",
          "link": "http://arxiv.org/abs/2107.01378",
          "publishedOn": "2021-07-06T01:58:06.166Z",
          "wordCount": 578,
          "title": "Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.207Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:58.200Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications,\ne.g., visual effect assistance in movie, scene understanding in autonomous\ndriving, and virtual background creation in video conferencing, to name a few.\nRecently, due to the renaissance of connectionism in computer vision, there has\nbeen an influx of numerous deep learning based approaches that have been\ndedicated to video segmentation and delivered compelling performance. In this\nsurvey, we comprehensively review two basic lines of research in this area,\ni.e., generic object segmentation (of unknown categories) in videos and video\nsemantic segmentation, by introducing their respective task settings,\nbackground concepts, perceived need, development history, and main challenges.\nWe also provide a detailed overview of representative literature on both\nmethods and datasets. Additionally, we present quantitative performance\ncomparisons of the reviewed methods on benchmark datasets. At last, we point\nout a set of unsolved open issues in this field, and suggest possible\nopportunities for further research.",
          "link": "http://arxiv.org/abs/2107.01153",
          "publishedOn": "2021-07-05T01:54:58.182Z",
          "wordCount": 607,
          "title": "A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:54:58.151Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:58.144Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1\">Gregor K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1\">Paul F. J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1\">David Zimmerer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1\">Ulf Neuberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1\">Wolfgang Wick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1\">J&#xfc;rgen Debus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1\">Sabine Heiland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1\">Martin Bendszus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1\">Philipp Vollmuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>",
          "description": "The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.",
          "link": "http://arxiv.org/abs/2106.12917",
          "publishedOn": "2021-07-05T01:54:58.137Z",
          "wordCount": 643,
          "title": "Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Road-boundary detection is important for autonomous driving. It can be used\nto constrain autonomous vehicles running on road areas to ensure driving\nsafety. Compared with online road-boundary detection using on-vehicle\ncameras/Lidars, offline detection using aerial images could alleviate the\nsevere occlusion issue. Moreover, the offline detection results can be directly\nemployed to annotate high-definition (HD) maps. In recent years, deep-learning\ntechnologies have been used in offline detection. But there still lacks a\npublicly available dataset for this task, which hinders the research progress\nin this area. So in this paper, we propose a new benchmark dataset, named\n\\textit{Topo-boundary}, for offline topological road-boundary detection. The\ndataset contains 25,295 $1000\\times1000$-sized 4-channel aerial images. Each\nimage is provided with 8 training labels for different sub-tasks. We also\ndesign a new entropy-based metric for connectivity evaluation, which could\nbetter handle noises or outliers. We implement and evaluate 3\nsegmentation-based baselines and 5 graph-based baselines using the dataset. We\nalso propose a new imitation-learning-based baseline which is enhanced from our\nprevious work. The superiority of our enhancement is demonstrated from the\ncomparison. The dataset and our-implemented code for the baselines are\navailable at \\texttt{\\url{https://tonyxuqaq.github.io/Topo-boundary/}}.",
          "link": "http://arxiv.org/abs/2103.17119",
          "publishedOn": "2021-07-05T01:54:58.117Z",
          "wordCount": 687,
          "title": "Topo-boundary: A Benchmark Dataset on Topological Road-boundary Detection Using Aerial Images for Autonomous Driving. (arXiv:2103.17119v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jameel Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhayek_A/0/1/0/all/0/1\">Ahmed Elhayek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sk Aziz Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. Existing methods\naddressing it directly regress hand meshes via 2D convolutional neural\nnetworks, which leads to artifacts due to perspective distortions in the\nimages. To address the limitations of the existing methods, we develop\nHandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions\ntrained in a fully supervised manner. The input to our network is a 3D\nvoxelized-depth-map-based on the truncated signed distance function (TSDF).\nHandVoxNet++ relies on two hand shape representations. The first one is the 3D\nvoxelized grid of hand shape, which does not preserve the mesh topology and\nwhich is the most accurate representation. The second representation is the\nhand surface that preserves the mesh topology. We combine the advantages of\nboth representations by aligning the hand surface to the voxelized hand shape\neither with a new neural Graph-Convolutions-based Mesh Registration\n(GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach\n(NRGA++) which does not rely on training data. In extensive evaluations on\nthree public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and\nHO-3D, the proposed HandVoxNet++ achieves the state-of-the-art performance. In\nthis journal extension of our previous approach presented at CVPR 2020, we gain\n41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19\ndatasets, respectively. Our method is ranked first on the HANDS19 challenge\ndataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the\nsubmission of our results to the portal in August 2020.",
          "link": "http://arxiv.org/abs/2107.01205",
          "publishedOn": "2021-07-05T01:54:58.108Z",
          "wordCount": 721,
          "title": "HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. (arXiv:2107.01205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.101Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-05T01:54:58.093Z",
          "wordCount": 635,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (\\ie, reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K~\\cite{yangzx} and NS8K~\\cite{wang}, well demonstrate the\neffectiveness of our ReGO.",
          "link": "http://arxiv.org/abs/2106.10601",
          "publishedOn": "2021-07-05T01:54:58.084Z",
          "wordCount": 623,
          "title": "ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>",
          "description": "Image reconstruction is likely the most predominant auxiliary task for image\nclassification, but we would like to think twice about this convention. In this\npaper, we investigated \"approximating the Fourier Transform of the input image\"\nas a potential alternative, in the hope that it may further boost the\nperformances on the primary task or introduce novel constraints not well\ncovered by image reconstruction. We experimented with five popular\nclassification architectures on the CIFAR-10 dataset, and the empirical results\nindicated that our proposed auxiliary task generally improves the\nclassification accuracy. More notably, the results showed that in certain cases\nour proposed auxiliary task may enhance the classifiers' resistance to\nadversarial attacks generated using the fast gradient sign method.",
          "link": "http://arxiv.org/abs/2106.11478",
          "publishedOn": "2021-07-05T01:54:58.075Z",
          "wordCount": 610,
          "title": "Fourier Transform Approximation as an Auxiliary Task for Image Classification. (arXiv:2106.11478v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1\">Jhair Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.",
          "link": "http://arxiv.org/abs/2103.14010",
          "publishedOn": "2021-07-05T01:54:58.055Z",
          "wordCount": 633,
          "title": "Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:54:58.049Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonante_P/0/1/0/all/0/1\">Pasquale Antonante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1\">Vasileios Tzoumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "Nonlinear estimation in robotics and vision is typically plagued with\noutliers due to wrong data association, or to incorrect detections from signal\nprocessing and machine learning methods. This paper introduces two unifying\nformulations for outlier-robust estimation, Generalized Maximum Consensus\n(G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates\nfundamental limits, practical algorithms, and applications. Our first\ncontribution is a proof that outlier-robust estimation is inapproximable: in\nthe worst case, it is impossible to (even approximately) find the set of\noutliers, even with slower-than-polynomial-time algorithms (particularly,\nalgorithms running in quasi-polynomial time). As a second contribution, we\nreview and extend two general-purpose algorithms. The first, Adaptive Trimming\n(ADAPT), is combinatorial, and is suitable for G-MC; the second, Graduated\nNon-Convexity (GNC), is based on homotopy methods, and is suitable for G-TLS.\nWe extend ADAPT and GNC to the case where the user does not have prior\nknowledge of the inlier-noise statistics (or the statistics may vary over time)\nand is unable to guess a reasonable threshold to separate inliers from outliers\n(as the one commonly used in RANSAC). We propose the first minimally tuned\nalgorithms for outlier rejection, that dynamically decide how to separate\ninliers from outliers. Our third contribution is an evaluation of the proposed\nalgorithms on robot perception problems: mesh registration, image-based object\ndetection (shape alignment), and pose graph optimization. ADAPT and GNC execute\nin real-time, are deterministic, outperform RANSAC, and are robust up to 80-90%\noutliers. Their minimally tuned versions also compare favorably with the state\nof the art, even though they do not rely on a noise bound for the inliers.",
          "link": "http://arxiv.org/abs/2007.15109",
          "publishedOn": "2021-07-05T01:54:58.041Z",
          "wordCount": 739,
          "title": "Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and Applications. (arXiv:2007.15109v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-07-05T01:54:58.034Z",
          "wordCount": 625,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:54:58.017Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>",
          "description": "This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.",
          "link": "http://arxiv.org/abs/2106.09672",
          "publishedOn": "2021-07-05T01:54:58.004Z",
          "wordCount": 645,
          "title": "The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:57.997Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:54:57.982Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Li Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yangjun Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Real-world scenarios often require the anticipation of object interactions in\nunknown future, which would assist the decision-making process of both humans\nand agents. To meet this challenge, we present a new task named Visual\nRelationship Forecasting (VRF) in videos to explore the prediction of visual\nrelationships in a reasoning manner. Specifically, given a subject-object pair\nwith H existing frames, VRF aims to predict their future interactions for the\nnext T frames without visual evidence. To evaluate the VRF task, we introduce\ntwo video datasets named VRF-AG and VRF-VidOR, with a series of\nspatio-temporally localized visual relation annotations in a video. These two\ndatasets densely annotate 13 and 35 visual relationships in 1923 and 13447\nvideo clips, respectively. In addition, we present a novel Graph Convolutional\nTransformer (GCT) framework, which captures both object-level and frame-level\ndependencies by spatio-temporal Graph Convolution Network and Transformer.\nExperimental results on both VRF-AG and VRF-VidOR datasets demonstrate that GCT\noutperforms the state-of-the-art sequence modelling methods on visual\nrelationship forecasting.",
          "link": "http://arxiv.org/abs/2107.01181",
          "publishedOn": "2021-07-05T01:54:57.974Z",
          "wordCount": 595,
          "title": "Visual Relationship Forecasting in Videos. (arXiv:2107.01181v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_C/0/1/0/all/0/1\">Christian Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1\">Max Argus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>",
          "description": "This work presents improvements in monocular hand shape estimation by\nbuilding on top of recent advances in unsupervised learning. We extend momentum\ncontrastive learning and contribute a structured collection of hand images,\nwell suited for visual representation learning, which we call HanCo. We find\nthat the representation learned by established contrastive learning methods can\nbe improved significantly by exploiting advanced background removal techniques\nand multi-view information. These allow us to generate more diverse instance\npairs than those obtained by augmentations commonly used in exemplar based\napproaches. Our method leads to a more suitable representation for the hand\nshape estimation task and shows a 4.7% reduction in mesh error and a 3.6%\nimprovement in F-score compared to an ImageNet pretrained baseline. We make our\nbenchmark dataset publicly available, to encourage further research into this\ndirection.",
          "link": "http://arxiv.org/abs/2106.04324",
          "publishedOn": "2021-07-05T01:54:57.968Z",
          "wordCount": 593,
          "title": "Contrastive Representation Learning for Hand Shape Estimation. (arXiv:2106.04324v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:57.951Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in deep models. In this paper, we find that existing\ncontrastive learning based solutions for self-supervised video recognition\nfocus on inter-variance encoding but ignore the intra-variance existing in\nclips within the same video. We thus propose to learn dual representations for\neach clip which (\\romannumeral 1) encode intra-variance through a shuffle-rank\npretext task; (\\romannumeral 2) encode inter-variance through a temporal\ncoherent contrastive loss. Experiment results show that our method plays an\nessential role in balancing inter and intra variances and brings consistent\nperformance gains on multiple backbones and contrastive learning frameworks.\nIntegrated with SimCLR and pretrained on Kinetics-400, our method achieves\n$\\textbf{82.0\\%}$ and $\\textbf{51.2\\%}$ downstream classification accuracy on\nUCF101 and HMDB51 test sets respectively and $\\textbf{46.1\\%}$ video retrieval\naccuracy on UCF101, outperforming both pretext-task based and contrastive\nlearning based counterparts.",
          "link": "http://arxiv.org/abs/2107.01194",
          "publishedOn": "2021-07-05T01:54:57.944Z",
          "wordCount": 595,
          "title": "How Incomplete is Contrastive Learning? AnInter-intra Variant Dual Representation Method forSelf-supervised Video Recognition. (arXiv:2107.01194v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>",
          "description": "It is essential but challenging to predict future trajectories of various\nagents in complex scenes. Whether it is internal personality factors of agents,\ninteractive behavior of the neighborhood, or the influence of surroundings, it\nwill have an impact on their future behavior styles. It means that even for the\nsame physical type of agents, there are huge differences in their behavior\npreferences. Although recent works have made significant progress in studying\nagents' multi-modal plannings, most of them still apply the same prediction\nstrategy to all agents, which makes them difficult to fully show the multiple\nstyles of vast agents. In this paper, we propose the Multi-Style Network (MSN)\nto focus on this problem by divide agents' preference styles into several\nhidden behavior categories adaptively and train each category's prediction\nnetwork separately, therefore giving agents all styles of predictions\nsimultaneously. Experiments demonstrate that our deterministic MSN-D and\ngenerative MSN-G outperform many recent state-of-the-art methods and show\nbetter multi-style characteristics in the visualized results.",
          "link": "http://arxiv.org/abs/2107.00932",
          "publishedOn": "2021-07-05T01:54:57.937Z",
          "wordCount": 594,
          "title": "MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01125",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "The deep image prior has demonstrated the remarkable ability that untrained\nnetworks can address inverse imaging problems, such as denoising, inpainting\nand super-resolution, by optimizing on just a single degraded image. Despite\nits promise, it suffers from two limitations. First, it remains unclear how one\ncan control the prior beyond the choice of the network architecture. Second, it\nrequires an oracle to determine when to stop the optimization as the\nperformance degrades after reaching a peak. In this paper, we study the deep\nimage prior from a spectral bias perspective to address these problems. By\nintroducing a frequency-band correspondence measure, we observe that deep image\npriors for inverse imaging exhibit a spectral bias during optimization, where\nlow-frequency image signals are learned faster and better than high-frequency\nnoise signals. This pinpoints why degraded images can be denoised or inpainted\nwhen the optimization is stopped at the right time. Based on our observations,\nwe propose to control the spectral bias in the deep image prior to prevent\nperformance degradation and to speed up optimization convergence. We do so in\nthe two core layer types of inverse imaging networks: the convolution layer and\nthe upsampling layer. We present a Lipschitz-controlled approach for the\nconvolution and a Gaussian-controlled approach for the upsampling layer. We\nfurther introduce a stopping criterion to avoid superfluous computation. The\nexperiments on denoising, inpainting and super-resolution show that our method\nno longer suffers from performance degradation during optimization, relieving\nus from the need for an oracle criterion to stop early. We further outline a\nstopping criterion to avoid superfluous computation. Finally, we show that our\napproach obtains favorable restoration results compared to current approaches,\nacross all tasks.",
          "link": "http://arxiv.org/abs/2107.01125",
          "publishedOn": "2021-07-05T01:54:57.860Z",
          "wordCount": 739,
          "title": "On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:57.853Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozo_D/0/1/0/all/0/1\">David Pozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>",
          "description": "This paper addresses the problem of building an affordable easy-to-setup\nsynchronized multi-view camera system, which is in demand for many Computer\nVision and Robotics applications in high-dynamic environments. In our work, we\npropose a solution for this problem - a publicly-available Android application\nfor synchronized video recording on multiple smartphones with sub-millisecond\naccuracy. We present a generalized mathematical model of timestamping for\nAndroid smartphones and prove its applicability on 47 different physical\ndevices. Also, we estimate the time drift parameter for those smartphones,\nwhich is less than 1.2 millisecond per minute for most of the considered\ndevices, that makes smartphones' camera system a worthy analog for professional\nmulti-view systems. Finally, we demonstrate Android-app performance on the\ncamera system built from Android smartphones quantitatively, showing less than\n300 microseconds synchronization error, and qualitatively - on panorama\nstitching task.",
          "link": "http://arxiv.org/abs/2107.00987",
          "publishedOn": "2021-07-05T01:54:57.845Z",
          "wordCount": 572,
          "title": "Sub-millisecond Video Synchronization of Multiple Android Smartphones. (arXiv:2107.00987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Airaksinen_M/0/1/0/all/0/1\">Manu Airaksinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhatalo_S/0/1/0/all/0/1\">Sampsa Vanhatalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Infant motility assessment using intelligent wearables is a promising new\napproach for assessment of infant neurophysiological development, and where\nefficient signal analysis plays a central role. This study investigates the use\nof different end-to-end neural network architectures for processing infant\nmotility data from wearable sensors. We focus on the performance and\ncomputational burden of alternative sensor encoder and time-series modelling\nmodules and their combinations. In addition, we explore the benefits of data\naugmentation methods in ideal and non-ideal recording conditions. The\nexperiments are conducted using a data-set of multi-sensor movement recordings\nfrom 7-month-old infants, as captured by a recently proposed smart jumpsuit for\ninfant motility assessment. Our results indicate that the choice of the encoder\nmodule has a major impact on classifier performance. For sensor encoders, the\nbest performance was obtained with parallel 2-dimensional convolutions for\nintra-sensor channel fusion with shared weights for all sensors. The results\nalso indicate that a relatively compact feature representation is obtainable\nfor within-sensor feature extraction without a drastic loss to classifier\nperformance. Comparison of time-series models revealed that feed-forward\ndilated convolutions with residual and skip connections outperformed all\nRNN-based models in performance, training time, and training stability. The\nexperiments also indicate that data augmentation improves model robustness in\nsimulated packet loss or sensor dropout scenarios. In particular, signal- and\nsensor-dropout-based augmentation strategies provided considerable boosts to\nperformance without negatively affecting the baseline performance. Overall the\nresults provide tangible suggestions on how to optimize end-to-end neural\nnetwork training for multi-channel movement sensor data.",
          "link": "http://arxiv.org/abs/2107.01086",
          "publishedOn": "2021-07-05T01:54:57.833Z",
          "wordCount": 705,
          "title": "Comparison of end-to-end neural network architectures and data augmentation methods for automatic infant motility assessment using wearable sensors. (arXiv:2107.01086v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiufan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>",
          "description": "In this work, we address the problem of cross-view geo-localization, which\nestimates the geospatial location of a street view image by matching it with a\ndatabase of geo-tagged aerial images. The cross-view matching task is extremely\nchallenging due to drastic appearance and geometry differences across views.\nUnlike existing methods that predominantly fall back on CNN, here we devise a\nnovel evolving geo-localization Transformer (EgoTR) that utilizes the\nproperties of self-attention in Transformer to model global dependencies, thus\nsignificantly decreasing visual ambiguities in cross-view geo-localization. We\nalso exploit the positional encoding of Transformer to help the EgoTR\nunderstand and correspond geometric configurations between ground and aerial\nimages. Compared to state-of-the-art methods that impose strong assumption on\ngeometry knowledge, the EgoTR flexibly learns the positional embeddings through\nthe training objective and hence becomes more practical in many real-world\nscenarios. Although Transformer is well suited to our task, its vanilla\nself-attention mechanism independently interacts within image patches in each\nlayer, which overlooks correlations between layers. Instead, this paper propose\na simple yet effective self-cross attention mechanism to improve the quality of\nlearned representations. The self-cross attention models global dependencies\nbetween adjacent layers, which relates between image patches while modeling how\nfeatures evolve in the previous layer. As a result, the proposed self-cross\nattention leads to more stable training, improves the generalization ability\nand encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, fine-grained and cross-dataset cross-view\ngeo-localization tasks.",
          "link": "http://arxiv.org/abs/2107.00842",
          "publishedOn": "2021-07-05T01:54:57.826Z",
          "wordCount": 679,
          "title": "Cross-view Geo-localization with Evolving Transformer. (arXiv:2107.00842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:57.820Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.",
          "link": "http://arxiv.org/abs/2107.01151",
          "publishedOn": "2021-07-05T01:54:57.814Z",
          "wordCount": 596,
          "title": "Collaborative Visual Navigation. (arXiv:2107.01151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanam_Z/0/1/0/all/0/1\">Zeba Khanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usmani_A/0/1/0/all/0/1\">Atiya Usmani</a>",
          "description": "Braille has empowered visually challenged community to read and write. But at\nthe same time, it has created a gap due to widespread inability of non-Braille\nusers to understand Braille scripts. This gap has fuelled researchers to\npropose Optical Braille Recognition techniques to convert Braille documents to\nnatural language. The main motivation of this work is to cement the\ncommunication gap at academic institutions by translating personal documents of\nblind students. This has been accomplished by proposing an economical and\neffective technique which digitizes Braille documents using a smartphone\ncamera. For any given Braille image, a dot detection mechanism based on Hough\ntransform is proposed which is invariant to skewness, noise and other\ndeterrents. The detected dots are then clustered into Braille cells using\ndistance-based clustering algorithm. In succession, the standard physical\nparameters of each Braille cells are estimated for feature extraction and\nclassification as natural language characters. The comprehensive evaluation of\nthis technique on the proposed dataset of 54 Braille scripts has yielded into\naccuracy of 98.71%.",
          "link": "http://arxiv.org/abs/2107.00993",
          "publishedOn": "2021-07-05T01:54:57.793Z",
          "wordCount": 598,
          "title": "Optical Braille Recognition using Circular Hough Transform. (arXiv:2107.00993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:57.734Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:57.709Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:57.695Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00875",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putzgruber_Adamitsch_D/0/1/0/all/0/1\">Doris Putzgruber-Adamitsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarny_S/0/1/0/all/0/1\">Stephanie Sarny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+El_Shabrawi_Y/0/1/0/all/0/1\">Yosuf El-Shabrawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoeffmann_K/0/1/0/all/0/1\">Klaus Schoeffmann</a>",
          "description": "A critical complication after cataract surgery is the dislocation of the lens\nimplant leading to vision deterioration and eye trauma. In order to reduce the\nrisk of this complication, it is vital to discover the risk factors during the\nsurgery. However, studying the relationship between lens dislocation and its\nsuspicious risk factors using numerous videos is a time-extensive procedure.\nHence, the surgeons demand an automatic approach to enable a larger-scale and,\naccordingly, more reliable study. In this paper, we propose a novel framework\nas the major step towards lens irregularity detection. In particular, we\npropose (I) an end-to-end recurrent neural network to recognize the\nlens-implantation phase and (II) a novel semantic segmentation network to\nsegment the lens and pupil after the implantation phase. The phase recognition\nresults reveal the effectiveness of the proposed surgical phase recognition\napproach. Moreover, the segmentation results confirm the proposed segmentation\nnetwork's effectiveness compared to state-of-the-art rival approaches.",
          "link": "http://arxiv.org/abs/2107.00875",
          "publishedOn": "2021-07-05T01:54:57.683Z",
          "wordCount": 628,
          "title": "LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in Cataract Surgery Videos. (arXiv:2107.00875v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:57.676Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "While the researches on single image super-resolution (SISR), especially\nequipped with deep neural networks (DNNs), have achieved tremendous successes\nrecently, they still suffer from two major limitations. Firstly, the real image\ndegradation is usually unknown and highly variant from one to another, making\nit extremely hard to train a single model to handle the general SISR task.\nSecondly, most of current methods mainly focus on the downsampling process of\nthe degradation, but ignore or underestimate the inevitable noise\ncontamination. For example, the commonly-used independent and identically\ndistributed (i.i.d.) Gaussian noise distribution always largely deviates from\nthe real image noise (e.g., camera sensor noise), which limits their\nperformance in real scenarios. To address these issues, this paper proposes a\nmodel-based unsupervised SISR method to deal with the general SISR task with\nunknown degradations. Instead of the traditional i.i.d. Gaussian noise\nassumption, a novel patch-based non-i.i.d. noise modeling method is proposed to\nfit the complex real noise. Besides, a deep generator parameterized by a DNN is\nused to map the latent variable to the high-resolution image, and the\nconventional hyper-Laplacian prior is also elaborately embedded into such\ngenerator to further constrain the image gradients. Finally, a Monte Carlo EM\nalgorithm is designed to solve our model, which provides a general inference\nframework to update the image generator both w.r.t. the latent variable and the\nnetwork parameters. Comprehensive experiments demonstrate that the proposed\nmethod can evidently surpass the current state of the art (SotA) method (about\n1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster\nspeed.",
          "link": "http://arxiv.org/abs/2107.00986",
          "publishedOn": "2021-07-05T01:54:57.667Z",
          "wordCount": 696,
          "title": "Unsupervised Single Image Super-resolution Under Complex Noise. (arXiv:2107.00986v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1\">Hadrien Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeson_P/0/1/0/all/0/1\">Paul Leeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.",
          "link": "http://arxiv.org/abs/2107.00977",
          "publishedOn": "2021-07-05T01:54:57.648Z",
          "wordCount": 602,
          "title": "Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation. (arXiv:2107.00977v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:57.638Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Weak supervision learning on classification labels has demonstrated high\nperformance in various tasks. When a few pixel-level fine annotations are also\naffordable, it is natural to leverage both of the pixel-level (e.g.,\nsegmentation) and image level (e.g., classification) annotation to further\nimprove the performance. In computational pathology, however, such weak or\nmixed supervision learning is still a challenging task, since the high\nresolution of whole slide images makes it unattainable to perform end-to-end\ntraining of classification models. An alternative approach is to analyze such\ndata by patch-base model training, i.e., using self-supervised learning to\ngenerate pixel-level pseudo labels for patches. However, such methods usually\nhave model drifting issues, i.e., hard to converge, because the noise\naccumulates during the self-training process. To handle those problems, we\npropose a mixed supervision learning framework for super high-resolution images\nto effectively utilize their various labels (e.g., sufficient image-level\ncoarse annotations and a few pixel-level fine labels). During the patch\ntraining stage, this framework can make use of coarse image-level labels to\nrefine self-supervised learning and generate high-quality pixel-level pseudo\nlabels. A comprehensive strategy is proposed to suppress pixel-level false\npositives and false negatives. Three real-world datasets with very large number\nof images (i.e., more than 10,000 whole slide images) and various types of\nlabels are used to evaluate the effectiveness of mixed supervision learning. We\nreduced the false positive rate by around one third compared to state of the\nart while retaining 100\\% sensitivity, in the task of image-level\nclassification.",
          "link": "http://arxiv.org/abs/2107.00934",
          "publishedOn": "2021-07-05T01:54:57.630Z",
          "wordCount": 690,
          "title": "Mixed Supervision Learning for Whole Slide Image Classification. (arXiv:2107.00934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>",
          "description": "The classification of histopathological images is of great value in both\ncancer diagnosis and pathological studies. However, multiple reasons, such as\nvariations caused by magnification factors and class imbalance, make it a\nchallenging task where conventional methods that learn from image-label\ndatasets perform unsatisfactorily in many cases. We observe that tumours of the\nsame class often share common morphological patterns. To exploit this fact, we\npropose an approach that learns similarity-based multi-scale embeddings (SMSE)\nfor magnification-independent histopathological image classification. In\nparticular, a pair loss and a triplet loss are leveraged to learn\nsimilarity-based embeddings from image pairs or image triplets. The learned\nembeddings provide accurate measurements of similarities between images, which\nare regarded as a more effective form of representation for histopathological\nmorphology than normal image features. Furthermore, in order to ensure the\ngenerated models are magnification-independent, images acquired at different\nmagnification factors are simultaneously fed to networks during training for\nlearning multi-scale embeddings. In addition to the SMSE, to eliminate the\nimpact of class imbalance, instead of using the hard sample mining strategy\nthat intuitively discards some easy samples, we introduce a new reinforced\nfocal loss to simultaneously punish hard misclassified samples while\nsuppressing easy well-classified samples. Experimental results show that the\nSMSE improves the performance for histopathological image classification tasks\nfor both breast and liver cancers by a large margin compared to previous\nmethods. In particular, the SMSE achieves the best performance on the BreakHis\nbenchmark with an improvement ranging from 5% to 18% compared to previous\nmethods using traditional features.",
          "link": "http://arxiv.org/abs/2107.01063",
          "publishedOn": "2021-07-05T01:54:57.623Z",
          "wordCount": 690,
          "title": "Magnification-independent Histopathological Image Classification with Similarity-based Multi-scale Embeddings. (arXiv:2107.01063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:57.615Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hampali_S/0/1/0/all/0/1\">Shreyas Hampali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sayan Deb Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>",
          "description": "HO-3D is a dataset providing image sequences of various hand-object\ninteraction scenarios annotated with the 3D pose of the hand and the object and\nwas originally introduced as HO-3D_v2. The annotations were obtained\nautomatically using an optimization method, 'HOnnotate', introduced in the\noriginal paper. HO-3D_v3 provides more accurate annotations for both the hand\nand object poses thus resulting in better estimates of contact regions between\nthe hand and the object. In this report, we elaborate on the improvements to\nthe HOnnotate method and provide evaluations to compare the accuracy of\nHO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to\nHO-3D_v2 for hand poses while exhibiting higher contact regions with the object\nsurface.",
          "link": "http://arxiv.org/abs/2107.00887",
          "publishedOn": "2021-07-05T01:54:57.597Z",
          "wordCount": 559,
          "title": "HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. (arXiv:2107.00887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:57.588Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:57.581Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>",
          "description": "This paper presents a novel intrinsic image transfer (IIT) algorithm for\nillumination manipulation, which creates a local image translation between two\nillumination surfaces. This model is built on an optimization-based framework\nconsisting of three photo-realistic losses defined on the sub-layers factorized\nby an intrinsic image decomposition. We illustrate that all losses can be\nreduced without the necessity of taking an intrinsic image decomposition under\nthe well-known spatial-varying illumination illumination-invariant reflectance\nprior knowledge. Moreover, with a series of relaxations, all of them can be\ndirectly defined on images, giving a closed-form solution for image\nillumination manipulation. This new paradigm differs from the prevailing\nRetinex-based algorithms, as it provides an implicit way to deal with the\nper-pixel image illumination. We finally demonstrate its versatility and\nbenefits to the illumination-related tasks such as illumination compensation,\nimage enhancement, and high dynamic range (HDR) image compression, and show the\nhigh-quality results on natural image datasets.",
          "link": "http://arxiv.org/abs/2107.00704",
          "publishedOn": "2021-07-05T01:54:57.572Z",
          "wordCount": 581,
          "title": "Intrinsic Image Transfer for Illumination Manipulation. (arXiv:2107.00704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoni Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Hierarchical classification is significant for complex tasks by providing\nmulti-granular predictions and encouraging better mistakes. As the label\nstructure decides its performance, many existing approaches attempt to\nconstruct an excellent label structure for promoting the classification\nresults. In this paper, we consider that different label structures provide a\nvariety of prior knowledge for category recognition, thus fusing them is\nhelpful to achieve better hierarchical classification results. Furthermore, we\npropose a multi-task multi-structure fusion model to integrate different label\nstructures. It contains two kinds of branches: one is the traditional\nclassification branch to classify the common subclasses, the other is\nresponsible for identifying the heterogeneous superclasses defined by different\nlabel structures. Besides the effect of multiple label structures, we also\nexplore the architecture of the deep model for better hierachical\nclassification and adjust the hierarchical evaluation metrics for multiple\nlabel structures. Experimental results on CIFAR100 and Car196 show that our\nmethod obtains significantly better results than using a flat classifier or a\nhierarchical classifier with any single label structure.",
          "link": "http://arxiv.org/abs/2107.00808",
          "publishedOn": "2021-07-05T01:54:57.476Z",
          "wordCount": 609,
          "title": "MMF: Multi-Task Multi-Structure Fusion for Hierarchical Image Classification. (arXiv:2107.00808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:57.469Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebol_M/0/1/0/all/0/1\">Manuel Rebol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutl_C/0/1/0/all/0/1\">Christian G&#xfc;tl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietroszek_K/0/1/0/all/0/1\">Krzysztof Pietroszek</a>",
          "description": "In real life, people communicate using both speech and non-verbal signals\nsuch as gestures, face expression or body pose. Non-verbal signals impact the\nmeaning of the spoken utterance in an abundance of ways. An absence of\nnon-verbal signals impoverishes the process of communication. Yet, when users\nare represented as avatars, it is difficult to translate non-verbal signals\nalong with the speech into the virtual world without specialized motion-capture\nhardware. In this paper, we propose a novel, data-driven technique for\ngenerating gestures directly from speech. Our approach is based on the\napplication of Generative Adversarial Neural Networks (GANs) to model the\ncorrelation rather than causation between speech and gestures. This approach\napproximates neuroscience findings on how non-verbal communication and speech\nare correlated. We create a large dataset which consists of speech and\ncorresponding gestures in a 3D human pose format from which our model learns\nthe speaker-specific correlation. We evaluate the proposed technique in a user\nstudy that is inspired by the Turing test. For the study, we animate the\ngenerated gestures on a virtual character. We find that users are not able to\ndistinguish between the generated and the recorded gestures. Moreover, users\nare able to identify our synthesized gestures as related or not related to a\ngiven utterance.",
          "link": "http://arxiv.org/abs/2107.00712",
          "publishedOn": "2021-07-05T01:54:57.461Z",
          "wordCount": 662,
          "title": "Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech. (arXiv:2107.00712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lingqiao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "In this technical report, we briefly introduce the solution of our team\n\"TAL-ai\" for (Semi-) supervised Face detection in the low light condition in\nUG2+ Challenge in CVPR 2021. By conducting several experiments with popular\nimage enhancement methods and image transfer methods, we pulled the low light\nimage and the normal image to a more closer domain. And it is observed that\nusing these data to training can achieve better performance. We also adapt\nseveral popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and\nlarge backbone like Swin-transformer. Finally, we ensemble several models which\nachieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.",
          "link": "http://arxiv.org/abs/2107.00818",
          "publishedOn": "2021-07-05T01:54:57.455Z",
          "wordCount": 559,
          "title": "1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition. (arXiv:2107.00818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address the multi-robot collaborative perception problem,\nspecifically in the context of multi-view infilling for distributed semantic\nsegmentation. This setting entails several real-world challenges, especially\nthose relating to unregistered multi-agent image data. Solutions must\neffectively leverage multiple, non-static, and intermittently-overlapping RGB\nperspectives. To this end, we propose the Multi-Agent Infilling Network: an\nextensible neural architecture that can be deployed (in a distributed manner)\nto each agent in a robotic swarm. Specifically, each robot is in charge of\nlocally encoding and decoding visual information, and an extensible neural\nmechanism allows for an uncertainty-aware and context-based exchange of\nintermediate features. We demonstrate improved performance on a realistic\nmulti-robot AirSim dataset.",
          "link": "http://arxiv.org/abs/2107.00769",
          "publishedOn": "2021-07-05T01:54:57.448Z",
          "wordCount": 572,
          "title": "Enhancing Multi-Robot Perception via Learned Data Association. (arXiv:2107.00769v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:57.432Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsadeghi_S/0/1/0/all/0/1\">S. Ehsan Mirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>",
          "description": "Semantic segmentation is one of the basic, yet essential scene understanding\ntasks for an autonomous agent. The recent developments in supervised machine\nlearning and neural networks have enjoyed great success in enhancing the\nperformance of the state-of-the-art techniques for this task. However, their\nsuperior performance is highly reliant on the availability of a large-scale\nannotated dataset. In this paper, we propose a novel fully unsupervised\nsemantic segmentation method, the so-called Information Maximization and\nAdversarial Regularization Segmentation (InMARS). Inspired by human perception\nwhich parses a scene into perceptual groups, rather than analyzing each pixel\nindividually, our proposed approach first partitions an input image into\nmeaningful regions (also known as superpixels). Next, it utilizes\nMutual-Information-Maximization followed by an adversarial training strategy to\ncluster these regions into semantically meaningful classes. To customize an\nadversarial training scheme for the problem, we incorporate adversarial pixel\nnoise along with spatial perturbations to impose photometrical and geometrical\ninvariance on the deep neural network. Our experiments demonstrate that our\nmethod achieves the state-of-the-art performance on two commonly used\nunsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.",
          "link": "http://arxiv.org/abs/2107.00691",
          "publishedOn": "2021-07-05T01:54:57.425Z",
          "wordCount": 623,
          "title": "Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization. (arXiv:2107.00691v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.",
          "link": "http://arxiv.org/abs/2107.00781",
          "publishedOn": "2021-07-05T01:54:57.418Z",
          "wordCount": 623,
          "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. (arXiv:2107.00781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huajun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.",
          "link": "http://arxiv.org/abs/2107.00782",
          "publishedOn": "2021-07-05T01:54:57.411Z",
          "wordCount": 648,
          "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression. (arXiv:2107.00782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>",
          "description": "Image super-resolution (SR) research has witnessed impressive progress thanks\nto the advance of convolutional neural networks (CNNs) in recent years.\nHowever, most existing SR methods are non-blind and assume that degradation has\na single fixed and known distribution (e.g., bicubic) which struggle while\nhandling degradation in real-world data that usually follows a multi-modal,\nspatially variant, and unknown distribution. The recent blind SR studies\naddress this issue via degradation estimation, but they do not generalize well\nto multi-source degradation and cannot handle spatially variant degradation. We\ndesign CRL-SR, a contrastive representation learning network that focuses on\nblind SR of images with multi-modal and spatially variant distributions. CRL-SR\naddresses the blind SR challenges from two perspectives. The first is\ncontrastive decoupling encoding which introduces contrastive learning to\nextract resolution-invariant embedding and discard resolution-variant embedding\nunder the guidance of a bidirectional contrastive loss. The second is\ncontrastive feature refinement which generates lost or corrupted high-frequency\ndetails under the guidance of a conditional contrastive loss. Extensive\nexperiments on synthetic datasets and real images show that the proposed CRL-SR\ncan handle multi-modal and spatially variant degradation effectively under\nblind settings and it also outperforms state-of-the-art SR methods\nqualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2107.00708",
          "publishedOn": "2021-07-05T01:54:57.395Z",
          "wordCount": 629,
          "title": "Blind Image Super-Resolution via Contrastive Representation Learning. (arXiv:2107.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address bandwidth-limited and obstruction-prone\ncollaborative perception, specifically in the context of multi-agent semantic\nsegmentation. This setting presents several key challenges, including\nprocessing and exchanging unregistered robotic swarm imagery. To be successful,\nsolutions must effectively leverage multiple non-static and\nintermittently-overlapping RGB perspectives, while heeding bandwidth\nconstraints and overcoming unwanted foreground obstructions. As such, we\npropose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH)\nto process, compress, and propagate visual information across a robotic swarm.\nOur distributed communication module operates directly (and exclusively) on raw\nimage data, without additional input requirements such as pose, depth, or\nwarping data. We demonstrate superior performance of our model compared against\nseveral baselines in a photo-realistic multi-robot AirSim environment,\nespecially in the presence of image occlusions. Our method achieves an absolute\n11% IoU improvement over strong baselines.",
          "link": "http://arxiv.org/abs/2107.00771",
          "publishedOn": "2021-07-05T01:54:57.385Z",
          "wordCount": 576,
          "title": "Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial Handshaking. (arXiv:2107.00771v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>",
          "description": "This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.",
          "link": "http://arxiv.org/abs/2107.00689",
          "publishedOn": "2021-07-05T01:54:57.312Z",
          "wordCount": 572,
          "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansizoglou_I/0/1/0/all/0/1\">Ioannis Kansizoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bampis_L/0/1/0/all/0/1\">Loukas Bampis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasteratos_A/0/1/0/all/0/1\">Antonios Gasteratos</a>",
          "description": "One of the most prominent attributes of Neural Networks (NNs) constitutes\ntheir capability of learning to extract robust and descriptive features from\nhigh dimensional data, like images. Hence, such an ability renders their\nexploitation as feature extractors particularly frequent in an abundant of\nmodern reasoning systems. Their application scope mainly includes complex\ncascade tasks, like multi-modal recognition and deep Reinforcement Learning\n(RL). However, NNs induce implicit biases that are difficult to avoid or to\ndeal with and are not met in traditional image descriptors. Moreover, the lack\nof knowledge for describing the intra-layer properties -- and thus their\ngeneral behavior -- restricts the further applicability of the extracted\nfeatures. With the paper at hand, a novel way of visualizing and understanding\nthe vector space before the NNs' output layer is presented, aiming to enlighten\nthe deep feature vectors' properties under classification tasks. Main attention\nis paid to the nature of overfitting in the feature space and its adverse\neffect on further exploitation. We present the findings that can be derived\nfrom our model's formulation, and we evaluate them on realistic recognition\nscenarios, proving its prominence by improving the obtained results.",
          "link": "http://arxiv.org/abs/2007.00062",
          "publishedOn": "2021-07-02T01:58:01.431Z",
          "wordCount": 671,
          "title": "Deep Feature Space: A Geometrical Perspective. (arXiv:2007.00062v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:58:01.304Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-07-02T01:58:01.219Z",
          "wordCount": 668,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_E/0/1/0/all/0/1\">Erik B Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>",
          "description": "Tensor networks provide an efficient approximation of operations involving\nhigh dimensional tensors and have been extensively used in modelling quantum\nmany-body systems. More recently, supervised learning has been attempted with\ntensor networks, primarily focused on tasks such as image classification. In\nthis work, we propose a novel formulation of tensor networks for supervised\nimage segmentation which allows them to operate on high resolution medical\nimages. We use the matrix product state (MPS) tensor network on non-overlapping\npatches of a given input image to predict the segmentation mask by learning a\npixel-wise linear classification rule in a high dimensional space. The proposed\nmodel is end-to-end trainable using backpropagation. It is implemented as a\nStrided Tensor Network to reduce the parameter complexity. The performance of\nthe proposed method is evaluated on two public medical imaging datasets and\ncompared to relevant baselines. The evaluation shows that the strided tensor\nnetwork yields competitive performance compared to CNN-based models while using\nfewer resources. Additionally, based on the experiments we discuss the\nfeasibility of using fully linear models for segmentation tasks.",
          "link": "http://arxiv.org/abs/2102.06900",
          "publishedOn": "2021-07-02T01:58:01.211Z",
          "wordCount": 673,
          "title": "Segmenting two-dimensional structures with strided tensor networks. (arXiv:2102.06900v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14196",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1\">Seyed Mohsen Hosseini</a>",
          "description": "A novel method for feature fusion in convolutional neural networks is\nproposed in this paper. Different feature fusion techniques are suggested to\nfacilitate the flow of information and improve the training of deep neural\nnetworks. Some of these techniques as well as the proposed network can be\nconsidered a type of Directed Acyclic Graph (DAG) Network, where a layer can\nreceive inputs from other layers and have outputs to other layers. In the\nproposed general framework of Lattice Fusion Network (LFNet), feature maps of\neach convolutional layer are passed to other layers based on a lattice graph\nstructure, where nodes are convolutional layers. To evaluate the performance of\nthe proposed architecture, different designs based on the general framework of\nLFNet are implemented for the task of image denoising. This task is used as an\nexample where training deep convolutional networks is needed. Results are\ncompared with state of the art methods. The proposed network is able to achieve\nbetter results with far fewer learnable parameters, which shows the\neffectiveness of LFNets for training of deep neural networks.",
          "link": "http://arxiv.org/abs/2011.14196",
          "publishedOn": "2021-07-02T01:58:01.199Z",
          "wordCount": 643,
          "title": "Lattice Fusion Networks for Image Denoising. (arXiv:2011.14196v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:58:01.192Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fumanal_Idocin_J/0/1/0/all/0/1\">Javier Fumanal-Idocin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takac_Z/0/1/0/all/0/1\">Zdenko Tak&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_J/0/1/0/all/0/1\">Javier Fern&#xe1;ndez Jose Antonio Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyena_H/0/1/0/all/0/1\">Harkaitz Goyena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Ching-Teng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1\">Humberto Bustince</a>",
          "description": "In this work we study the use of moderate deviation functions to measure\nsimilarity and dissimilarity among a set of given interval-valued data. To do\nso, we introduce the notion of interval-valued moderate deviation function and\nwe study in particular those interval-valued moderate deviation functions which\npreserve the width of the input intervals. Then, we study how to apply these\nfunctions to construct interval-valued aggregation functions. We have applied\nthem in the decision making phase of two Motor-Imagery Brain Computer Interface\nframeworks, obtaining better results than those obtained using other numerical\nand intervalar aggregations.",
          "link": "http://arxiv.org/abs/2011.09831",
          "publishedOn": "2021-07-02T01:58:01.160Z",
          "wordCount": 588,
          "title": "Interval-valued aggregation functions based on moderate deviations applied to Motor-Imagery-Based Brain Computer Interface. (arXiv:2011.09831v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent, and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time\nshort-time human action recognition. Extensive experimentation on MPOSE2021\nwith our proposed methodology and several previous architectural solutions\nproves the effectiveness of the AcT model and poses the base for future work on\nHAR.",
          "link": "http://arxiv.org/abs/2107.00606",
          "publishedOn": "2021-07-02T01:58:01.145Z",
          "wordCount": 607,
          "title": "Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition. (arXiv:2107.00606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_U/0/1/0/all/0/1\">Ujjal Kr Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repakula_S/0/1/0/all/0/1\">Sandeep Repakula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1\">Abhinav Ravi</a>",
          "description": "In this paper, we utilize deep visual Representation Learning to address an\nimportant problem in fashion e-commerce: color variants identification, i.e.,\nidentifying fashion products that match exactly in their design (or style), but\nonly to differ in their color. At first we attempt to tackle the problem by\nobtaining manual annotations (depicting whether two products are color\nvariants), and train a supervised triplet loss based neural network model to\nlearn representations of fashion products. However, for large scale real-world\nindustrial datasets such as addressed in our paper, it is infeasible to obtain\nannotations for the entire dataset, while capturing all the difficult corner\ncases. Interestingly, we observed that color variants are essentially\nmanifestations of color jitter based augmentations. Thus, we instead explore\nSelf-Supervised Learning (SSL) to solve this problem. We observed that existing\nstate-of-the-art SSL methods perform poor, for our problem. To address this, we\npropose a novel SSL based color variants model that simultaneously focuses on\ndifferent parts of an apparel. Quantitative and qualitative evaluation shows\nthat our method outperforms existing SSL methods, and at times, the supervised\nmodel.",
          "link": "http://arxiv.org/abs/2104.08581",
          "publishedOn": "2021-07-02T01:58:01.060Z",
          "wordCount": 663,
          "title": "Color Variants Identification in Fashion e-commerce via Contrastive Self-Supervised Representation Learning. (arXiv:2104.08581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pernus_M/0/1/0/all/0/1\">Martin Pernu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobrisek_S/0/1/0/all/0/1\">Simon Dobri&#x161;ek</a>",
          "description": "Face editing represents a popular research topic within the computer vision\nand image processing communities. While significant progress has been made\nrecently in this area, existing solutions: (i) are still largely focused on\nlow-resolution images, (ii) often generate editing results with visual\nartefacts, or (iii) lack fine-grained control and alter multiple (entangled)\nattributes at once, when trying to generate the desired facial semantics. In\nthis paper, we aim to address these issues though a novel attribute editing\napproach called MaskFaceGAN. The proposed approach is based on an optimization\nprocedure that directly optimizes the latent code of a pre-trained\n(state-of-the-art) Generative Adversarial Network (i.e., StyleGAN2) with\nrespect to several constraints that ensure: (i) preservation of relevant image\ncontent, (ii) generation of the targeted facial attributes, and (iii)\nspatially--selective treatment of local image areas. The constraints are\nenforced with the help of an (differentiable) attribute classifier and face\nparser that provide the necessary reference information for the optimization\nprocedure. MaskFaceGAN is evaluated in extensive experiments on the CelebA-HQ,\nHelen and SiblingsDB-HQf datasets and in comparison with several\nstate-of-the-art techniques from the literature, i.e., StarGAN, AttGAN, STGAN,\nand two versions of InterFaceGAN. Our experimental results show that the\nproposed approach is able to edit face images with respect to several facial\nattributes with unprecedented image quality and at high-resolutions\n(1024x1024), while exhibiting considerably less problems with attribute\nentanglement than competing solutions. The source code is made freely available\nfrom: https://github.com/MartinPernus/MaskFaceGAN.",
          "link": "http://arxiv.org/abs/2103.11135",
          "publishedOn": "2021-07-02T01:58:00.681Z",
          "wordCount": 738,
          "title": "High Resolution Face Editing with Masked GAN Latent Code Optimization. (arXiv:2103.11135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>",
          "description": "RGB-D salient object detection(SOD) demonstrates its superiority on detecting\nin complex environments due to the additional depth information introduced in\nthe data. Inevitably, an independent stream is introduced to extract features\nfrom depth images, leading to extra computation and parameters. This\nmethodology which sacrifices the model size to improve the detection accuracy\nmay impede the practical application of SOD problems. To tackle this dilemma,\nwe propose a dynamic distillation method along with a lightweight framework,\nwhich significantly reduces the parameters. This method considers the factors\nof both teacher and student performance within the training stage and\ndynamically assigns the distillation weight instead of applying a fixed weight\non the student model. Extensive experiments are conducted on five public\ndatasets to demonstrate that our method can achieve competitive performance\ncompared to 10 prior methods through a 78.2MB lightweight structure.",
          "link": "http://arxiv.org/abs/2106.09517",
          "publishedOn": "2021-07-02T01:58:00.655Z",
          "wordCount": 597,
          "title": "Dynamic Knowledge Distillation with A Single Stream Structure for RGB-D Salient Object Detection. (arXiv:2106.09517v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junwei Liang</a>",
          "description": "With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.",
          "link": "http://arxiv.org/abs/2011.10670",
          "publishedOn": "2021-07-02T01:58:00.605Z",
          "wordCount": 617,
          "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03244",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1\">Anindo Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1\">Matin Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>",
          "description": "We present a multi-stage 3D computer-aided detection and diagnosis (CAD)\nmodel for automated localization of clinically significant prostate cancer\n(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive\nits detection network, targeting salient structures and highly discriminative\nfeature dimensions across multiple resolutions. Its goal is to accurately\nidentify csPCa lesions from indolent cancer and the wide range of benign\npathology that can afflict the prostate gland. Simultaneously, a decoupled\nresidual classifier is used to achieve consistent false positive reduction,\nwithout sacrificing high sensitivity or computational efficiency. In order to\nguide model generalization with domain-specific clinical knowledge, a\nprobabilistic anatomical prior is used to encode the spatial prevalence and\nzonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired\nwith radiologically-estimated annotations, we hypothesize that such CNN-based\nmodels can be trained to detect biopsy-confirmed malignancies in an independent\ncohort.\n\nFor 486 institutional testing scans, the 3D CAD system achieves\n83.69$\\pm$5.22% and 93.19$\\pm$2.96% detection sensitivity at 0.50 and 1.46\nfalse positive(s) per patient, respectively, with 0.882$\\pm$0.030 AUROC in\npatient-based diagnosis $-$significantly outperforming four state-of-the-art\nbaseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from\nrecent literature. For 296 external biopsy-confirmed testing scans, the\nensembled CAD system shares moderate agreement with a consensus of expert\nradiologists (76.69%; $kappa$ $=$ 0.51$\\pm$0.04) and independent pathologists\n(81.08%; $kappa$ $=$ 0.56$\\pm$0.06); demonstrating strong generalization to\nhistologically-confirmed csPCa diagnosis.",
          "link": "http://arxiv.org/abs/2101.03244",
          "publishedOn": "2021-07-02T01:58:00.583Z",
          "wordCount": 802,
          "title": "End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction. (arXiv:2101.03244v10 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-07-02T01:58:00.556Z",
          "wordCount": 626,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1\">Tadayoshi Kohno</a>",
          "description": "When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.",
          "link": "http://arxiv.org/abs/2106.06654",
          "publishedOn": "2021-07-02T01:58:00.546Z",
          "wordCount": 544,
          "title": "Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "Does having visual priors (e.g. the ability to detect objects) facilitate\nlearning to perform vision-based manipulation (e.g. picking up objects)? We\nstudy this problem under the framework of transfer learning, where the model is\nfirst trained on a passive vision task, and adapted to perform an active\nmanipulation task. We find that pre-training on vision tasks significantly\nimproves generalization and sample efficiency for learning to manipulate\nobjects. However, realizing these gains requires careful selection of which\nparts of the model to transfer. Our key insight is that outputs of standard\nvision models highly correlate with affordance maps commonly used in\nmanipulation. Therefore, we explore directly transferring model parameters from\nvision networks to affordance prediction networks, and show that this can\nresult in successful zero-shot adaptation, where a robot can pick up certain\nobjects with zero robotic experience. With just a small amount of robotic\nexperience, we can further fine-tune the affordance model to achieve better\nresults. With just 10 minutes of suction experience or 1 hour of grasping\nexperience, our method achieves ~80% success rate at picking up novel objects.",
          "link": "http://arxiv.org/abs/2107.00646",
          "publishedOn": "2021-07-02T01:58:00.539Z",
          "wordCount": 633,
          "title": "Learning to See before Learning to Act: Visual Pre-training for Manipulation. (arXiv:2107.00646v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.2 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-02T01:58:00.520Z",
          "wordCount": 721,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "6D robotic grasping beyond top-down bin-picking scenarios is a challenging\ntask. Previous solutions based on 6D grasp synthesis with robot motion planning\nusually operate in an open-loop setting, which are sensitive to grasp synthesis\nerrors. In this work, we propose a new method for learning closed-loop control\npolicies for 6D grasping. Our policy takes a segmented point cloud of an object\nfrom an egocentric camera as input, and outputs continuous 6D control actions\nof the robot gripper for grasping the object. We combine imitation learning and\nreinforcement learning and introduce a goal-auxiliary actor-critic algorithm\nfor policy learning. We demonstrate that our learned policy can be integrated\ninto a tabletop 6D grasping system and a human-robot handover system to improve\nthe grasping performance of unseen objects. Our videos and code can be found at\nhttps://sites.google.com/view/gaddpg .",
          "link": "http://arxiv.org/abs/2010.00824",
          "publishedOn": "2021-07-02T01:58:00.513Z",
          "wordCount": 621,
          "title": "Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds. (arXiv:2010.00824v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changlin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>",
          "description": "Individual tree detection and crown delineation (ITDD) are critical in forest\ninventory management and remote sensing based forest surveys are largely\ncarried out through satellite images. However, most of these surveys only use\n2D spectral information which normally has not enough clues for ITDD. To fully\nexplore the satellite images, we propose a ITDD method using the orthophoto and\ndigital surface model (DSM) derived from the multi-view satellite data. Our\nalgorithm utilizes the top-hat morphological operation to efficiently extract\nthe local maxima from DSM as treetops, and then feed them to a modi-fied\nsuperpixel segmentation that combines both 2D and 3D information for tree crown\ndelineation. In subsequent steps, our method incorporates the biological\ncharacteristics of the crowns through plant allometric equation to falsify\npotential outliers. Experiments against manually marked tree plots on three\nrepresentative regions have demonstrated promising results - the best overall\ndetection accuracy can be 89%.",
          "link": "http://arxiv.org/abs/2107.00592",
          "publishedOn": "2021-07-02T01:58:00.506Z",
          "wordCount": 594,
          "title": "Individual Tree Detection and Crown Delineation with 3D Information from Multi-view Satellite Images. (arXiv:2107.00592v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.06148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we explore the mask representation in instance segmentation\nwith Point-of-Interest (PoI) features. Differentiating multiple potential\ninstances within a single PoI feature is challenging because learning a\nhigh-dimensional mask feature for each instance using vanilla convolution\ndemands a heavy computing burden. To address this challenge, we propose an\ninstance-aware convolution. It decomposes this mask representation learning\ntask into two tractable modules as instance-aware weights and instance-agnostic\nfeatures. The former is to parametrize convolution for producing mask features\ncorresponding to different instances, improving mask learning efficiency by\navoiding employing several independent convolutions. Meanwhile, the latter\nserves as mask templates in a single point. Together, instance-aware mask\nfeatures are computed by convolving the template with dynamic weights, used for\nthe mask prediction. Along with instance-aware convolution, we propose\nPointINS, a simple and practical instance segmentation approach, building upon\ndense one-stage detectors. Through extensive experiments, we evaluated the\neffectiveness of our framework built upon RetinaNet and FCOS. PointINS in\nResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO\ndataset, outperforming existing point-based methods by a large margin. It gives\na comparable performance to the region-based Mask R-CNN with faster inference.",
          "link": "http://arxiv.org/abs/2003.06148",
          "publishedOn": "2021-07-02T01:58:00.500Z",
          "wordCount": 668,
          "title": "PointINS: Point-based Instance Segmentation. (arXiv:2003.06148v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>",
          "description": "The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and learning\nefficiency of downstream tasks in scenes with multiple objects. In this paper,\nwe train state-of-the-art unsupervised models on five common multi-object\ndatasets and evaluate segmentation accuracy and downstream object property\nprediction. In addition, we study systematic generalization and robustness by\ninvestigating the settings where either single objects are out-of-distribution\n-- e.g., having unseen colors, textures, and shapes -- or global properties of\nthe scene are altered -- e.g., by occlusions, cropping, or increasing the\nnumber of objects. From our experimental study, we find object-centric\nrepresentations to be generally useful for downstream tasks and robust to\nshifts in the data distribution, especially if shifts affect single objects.",
          "link": "http://arxiv.org/abs/2107.00637",
          "publishedOn": "2021-07-02T01:58:00.493Z",
          "wordCount": 593,
          "title": "Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haoyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Z/0/1/0/all/0/1\">Zhihao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Convolutional neural networks (CNNs) have been successfully used in a range\nof tasks. However, CNNs are often viewed as \"black-box\" and lack of\ninterpretability. One main reason is due to the filter-class entanglement -- an\nintricate many-to-many correspondence between filters and classes. Most\nexisting works attempt post-hoc interpretation on a pre-trained model, while\nneglecting to reduce the entanglement underlying the model. In contrast, we\nfocus on alleviating filter-class entanglement during training. Inspired by\ncellular differentiation, we propose a novel strategy to train interpretable\nCNNs by encouraging class-specific filters, among which each filter responds to\nonly one (or few) class. Concretely, we design a learnable sparse\nClass-Specific Gate (CSG) structure to assign each filter with one (or few)\nclass in a flexible way. The gate allows a filter's activation to pass only\nwhen the input samples come from the specific class. Extensive experiments\ndemonstrate the fabulous performance of our method in generating a sparse and\nhighly class-related representation of the input, which leads to stronger\ninterpretability. Moreover, comparing with the standard training strategy, our\nmodel displays benefits in applications like object localization and\nadversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.",
          "link": "http://arxiv.org/abs/2007.08194",
          "publishedOn": "2021-07-02T01:58:00.486Z",
          "wordCount": 693,
          "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters. (arXiv:2007.08194v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sk Aziz Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahraman_K/0/1/0/all/0/1\">Kerem Kahraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>",
          "description": "This article introduces a new physics-based method for rigid point set\nalignment called Fast Gravitational Approach (FGA). In FGA, the source and\ntarget point sets are interpreted as rigid particle swarms with masses\ninteracting in a globally multiply-linked manner while moving in a simulated\ngravitational force field. The optimal alignment is obtained by explicit\nmodeling of forces acting on the particles as well as their velocities and\ndisplacements with second-order ordinary differential equations of motion.\nAdditional alignment cues (point-based or geometric features, and other\nboundary conditions) can be integrated into FGA through particle masses. We\npropose a smooth-particle mass function for point mass initialization, which\nimproves robustness to noise and structural discontinuities. To avoid\nprohibitive quadratic complexity of all-to-all point interactions, we adapt a\nBarnes-Hut tree for accelerated force computation and achieve quasilinear\ncomputational complexity. We show that the new method class has characteristics\nnot found in previous alignment methods such as efficient handling of partial\noverlaps, inhomogeneous point sampling densities, and coping with large point\nclouds with reduced runtime compared to the state of the art. Experiments show\nthat our method performs on par with or outperforms all compared competing\nnon-deep-learning-based and general-purpose techniques (which do not assume the\navailability of training data and a scene prior) in resolving transformations\nfor LiDAR data and gains state-of-the-art accuracy and speed when coping with\ndifferent types of data disturbances.",
          "link": "http://arxiv.org/abs/2009.14005",
          "publishedOn": "2021-07-02T01:58:00.478Z",
          "wordCount": 730,
          "title": "Fast Gravitational Approach for Rigid Point Set Registration with Ordinary Differential Equations. (arXiv:2009.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">W. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">P. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">L. Han</a>",
          "description": "The goal of convective storm nowcasting is local prediction of severe and\nimminent convective storms. Here, we consider the convective storm nowcasting\nproblem from the perspective of machine learning. First, we use a pixel-wise\nsampling method to construct spatiotemporal features for nowcasting, and\nflexibly adjust the proportions of positive and negative samples in the\ntraining set to mitigate class-imbalance issues. Second, we employ a concise\ntwo-stream convolutional neural network to extract spatial and temporal cues\nfor nowcasting. This simplifies the network structure, reduces the training\ntime requirement, and improves classification accuracy. The two-stream network\nused both radar and satellite data. In the resulting two-stream, fused\nconvolutional neural network, some of the parameters are entered into a\nsingle-stream convolutional neural network, but it can learn the features of\nmany data. Further, considering the relevance of classification and regression\ntasks, we develop a multi-task learning strategy that predicts the labels used\nin such tasks. We integrate two-stream multi-task learning into a single\nconvolutional neural network. Given the compact architecture, this network is\nmore efficient and easier to optimize than existing recurrent neural networks.",
          "link": "http://arxiv.org/abs/2010.14100",
          "publishedOn": "2021-07-02T01:58:00.470Z",
          "wordCount": 683,
          "title": "A Multi-task Two-stream Spatiotemporal Convolutional Neural Network for Convective Storm Nowcasting. (arXiv:2010.14100v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1\">Stefan K. Piechnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1\">Stefan Neubauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1\">Steffen E. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>",
          "description": "The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.",
          "link": "http://arxiv.org/abs/2106.12387",
          "publishedOn": "2021-07-02T01:58:00.437Z",
          "wordCount": 779,
          "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Feng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "LiDAR-based SLAM system is admittedly more accurate and stable than others,\nwhile its loop closure detection is still an open issue. With the development\nof 3D semantic segmentation for point cloud, semantic information can be\nobtained conveniently and steadily, essential for high-level intelligence and\nconductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM\nwith loop closure based on LOAM, named SA-LOAM, which leverages semantics in\nodometry as well as loop closure detection. Specifically, we propose a\nsemantic-assisted ICP, including semantically matching, downsampling and plane\nconstraint, and integrates a semantic graph-based place recognition method in\nour loop closure detection module. Benefitting from semantics, we can improve\nthe localization accuracy, detect loop closures effectively, and construct a\nglobal consistent semantic map even in large-scale scenes. Extensive\nexperiments on KITTI and Ford Campus dataset show that our system significantly\nimproves baseline performance, has generalization ability to unseen data and\nachieves competitive results compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11516",
          "publishedOn": "2021-07-02T01:58:00.415Z",
          "wordCount": 622,
          "title": "SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1\">Diaa Dabawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>",
          "description": "In this paper, we propose a novel layer based on fast Walsh-Hadamard\ntransform (WHT) and smooth-thresholding to replace $1\\times 1$ convolution\nlayers in deep neural networks. In the WHT domain, we denoise the transform\ndomain coefficients using the new smooth-thresholding non-linearity, a smoothed\nversion of the well-known soft-thresholding operator. We also introduce a\nfamily of multiplication-free operators from the basic 2$\\times$2 Hadamard\ntransform to implement $3\\times 3$ depthwise separable convolution layers.\nUsing these two types of layers, we replace the bottleneck layers in\nMobileNet-V2 to reduce the network's number of parameters with a slight loss in\naccuracy. For example, by replacing the final third bottleneck layers, we\nreduce the number of parameters from 2.270M to 540K. This reduces the accuracy\nfrom 95.21\\% to 92.98\\% on the CIFAR-10 dataset. Our approach significantly\nimproves the speed of data processing. The fast Walsh-Hadamard transform has a\ncomputational complexity of $O(m\\log_2 m)$. As a result, it is computationally\nmore efficient than the $1\\times1$ convolution layer. The fast Walsh-Hadamard\nlayer processes a tensor in $\\mathbb{R}^{10\\times32\\times32\\times1024}$ about 2\ntimes faster than $1\\times1$ convolution layer on NVIDIA Jetson Nano computer\nboard.",
          "link": "http://arxiv.org/abs/2104.07085",
          "publishedOn": "2021-07-02T01:58:00.397Z",
          "wordCount": 714,
          "title": "Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_B/0/1/0/all/0/1\">Bob McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Skeleton sequences are lightweight and compact, thus are ideal candidates for\naction recognition on edge devices. Recent skeleton-based action recognition\nmethods extract features from 3D joint coordinates as spatial-temporal cues,\nusing these representations in a graph neural network for feature fusion to\nboost recognition performance. The use of first- and second-order features,\n\\ie{} joint and bone representations, has led to high accuracy. Nonetheless,\nmany models are still confused by actions that have similar motion\ntrajectories. To address these issues, we propose fusing third-order features\nin the form of angular encoding into modern architectures to robustly capture\nthe relationships between joints and body parts. This simple fusion with\npopular spatial-temporal graph neural networks achieves new state-of-the-art\naccuracy in two large benchmarks, including NTU60 and NTU120, while employing\nfewer parameters and reduced run time. Our source code is publicly available\nat: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.",
          "link": "http://arxiv.org/abs/2105.01563",
          "publishedOn": "2021-07-02T01:58:00.389Z",
          "wordCount": 632,
          "title": "Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition. (arXiv:2105.01563v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-07-02T01:58:00.381Z",
          "wordCount": 613,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-07-02T01:58:00.361Z",
          "wordCount": 648,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While agents trained by Reinforcement Learning (RL) can solve increasingly\nchallenging tasks directly from visual observations, generalizing learned\nskills to novel environments remains very challenging. Extensive use of data\naugmentation is a promising technique for improving generalization in RL, but\nit is often found to decrease sample efficiency and can even lead to\ndivergence. In this paper, we investigate causes of instability when using data\naugmentation in common off-policy RL algorithms. We identify two problems, both\nrooted in high-variance Q-targets. Based on our findings, we propose a simple\nyet effective technique for stabilizing this class of algorithms under\naugmentation. We perform extensive empirical evaluation of image-based RL using\nboth ConvNets and Vision Transformers (ViT) on a family of benchmarks based on\nDeepMind Control Suite, as well as in robotic manipulation tasks. Our method\ngreatly improves stability and sample efficiency of ConvNets under\naugmentation, and achieves generalization results competitive with\nstate-of-the-art methods for image-based RL. We further show that our method\nscales to RL with ViT-based architectures, and that data augmentation may be\nespecially important in this setting.",
          "link": "http://arxiv.org/abs/2107.00644",
          "publishedOn": "2021-07-02T01:58:00.355Z",
          "wordCount": 630,
          "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation. (arXiv:2107.00644v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Recently, pure transformer-based models have shown great potentials for\nvision tasks such as image classification and detection. However, the design of\ntransformer networks is challenging. It has been observed that the depth,\nembedding dimension, and number of heads can largely affect the performance of\nvision transformers. Previous models configure these dimensions based upon\nmanual crafting. In this work, we propose a new one-shot architecture search\nframework, namely AutoFormer, dedicated to vision transformer search.\nAutoFormer entangles the weights of different blocks in the same layers during\nsupernet training. Benefiting from the strategy, the trained supernet allows\nthousands of subnets to be very well-trained. Specifically, the performance of\nthese subnets with weights inherited from the supernet is comparable to those\nretrained from scratch. Besides, the searched models, which we refer to\nAutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In\nparticular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy\non ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify\nthe transferability of AutoFormer by providing the performance on downstream\nbenchmarks and distillation experiments. Code and models are available at\nhttps://github.com/microsoft/AutoML.",
          "link": "http://arxiv.org/abs/2107.00651",
          "publishedOn": "2021-07-02T01:58:00.336Z",
          "wordCount": 615,
          "title": "AutoFormer: Searching Transformers for Visual Recognition. (arXiv:2107.00651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2107.00650",
          "publishedOn": "2021-07-02T01:58:00.325Z",
          "wordCount": 635,
          "title": "CLIP-It! Language-Guided Video Summarization. (arXiv:2107.00650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellanos_F/0/1/0/all/0/1\">Francisco J. Castellanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Antonio-Javier Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1\">Jorge Calvo-Zaragoza</a>",
          "description": "Binarization is a well-known image processing task, whose objective is to\nseparate the foreground of an image from the background. One of the many tasks\nfor which it is useful is that of preprocessing document images in order to\nidentify relevant information, such as text or symbols. The wide variety of\ndocument types, alphabets, and formats makes binarization challenging. There\nare multiple proposals with which to solve this problem, from classical\nmanually-adjusted methods, to more recent approaches based on machine learning.\nThe latter techniques require a large amount of training data in order to\nobtain good results; however, labeling a portion of each existing collection of\ndocuments is not feasible in practice. This is a common problem in supervised\nlearning, which can be addressed by using the so-called Domain Adaptation (DA)\ntechniques. These techniques take advantage of the knowledge learned in one\ndomain, for which labeled data are available, to apply it to other domains for\nwhich there are no labeled data. This paper proposes a method that combines\nneural networks and DA in order to carry out unsupervised document\nbinarization. However, when both the source and target domains are very\nsimilar, this adaptation could be detrimental. Our methodology, therefore,\nfirst measures the similarity between domains in an innovative manner in order\nto determine whether or not it is appropriate to apply the adaptation process.\nThe results reported in the experimentation, when evaluating up to 20 possible\ncombinations among five different domains, show that our proposal successfully\ndeals with the binarization of new document domains without the need for\nlabeled data.",
          "link": "http://arxiv.org/abs/2012.01204",
          "publishedOn": "2021-07-02T01:58:00.317Z",
          "wordCount": 734,
          "title": "Unsupervised Neural Domain Adaptation for Document Image Binarization. (arXiv:2012.01204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "Bundle adjustment (BA) is a technique for refining sensor orientations of\nsatellite images, while adjustment accuracy is correlated with feature matching\nresults. Feature match-ing often contains high uncertainties in weak/repeat\ntextures, while BA results are helpful in reducing these uncertainties. To\ncompute more accurate orientations, this article incorpo-rates BA and feature\nmatching in a unified framework and formulates the union as the optimization of\na global energy function so that the solutions of the BA and feature matching\nare constrained with each other. To avoid a degeneracy in the optimization, we\npropose a comprised solution by breaking the optimization of the global energy\nfunction into two-step suboptimizations and compute the local minimums of each\nsuboptimization in an incremental manner. Experiments on multi-view\nhigh-resolution satellite images show that our proposed method outperforms\nstate-of-the-art orientation techniques with or without accurate least-squares\nmatching.",
          "link": "http://arxiv.org/abs/2107.00598",
          "publishedOn": "2021-07-02T01:58:00.296Z",
          "wordCount": 583,
          "title": "A Unified Framework of Bundle Adjustment and Feature Matching for High-Resolution Satellite Images. (arXiv:2107.00598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuli_S/0/1/0/all/0/1\">Shikhar Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.",
          "link": "http://arxiv.org/abs/2105.07197",
          "publishedOn": "2021-07-02T01:58:00.289Z",
          "wordCount": 684,
          "title": "Are Convolutional Neural Networks or Transformers more like human vision?. (arXiv:2105.07197v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuechao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>",
          "description": "In this paper, we propose an interesting semi-sparsity smoothing algorithm\nbased on a novel sparsity-inducing optimization framework. This method is\nderived from the multiple observations, that is, semi-sparsity prior knowledge\nis more universally applicable, especially in areas where sparsity is not fully\nadmitted, such as polynomial-smoothing surfaces. We illustrate that this\nsemi-sparsity can be identified into a generalized $L_0$-norm minimization in\nhigher-order gradient domains, thereby giving rise to a new ``feature-aware''\nfiltering method with a powerful simultaneous-fitting ability in both sparse\nfeatures (singularities and sharpening edges) and non-sparse regions\n(polynomial-smoothing surfaces). Notice that a direct solver is always\nunavailable due to the non-convexity and combinatorial nature of $L_0$-norm\nminimization. Instead, we solve the model based on an efficient half-quadratic\nsplitting minimization with fast Fourier transforms (FFTs) for acceleration. We\nfinally demonstrate its versatility and many benefits to a series of\nsignal/image processing and computer vision applications.",
          "link": "http://arxiv.org/abs/2107.00627",
          "publishedOn": "2021-07-02T01:58:00.282Z",
          "wordCount": 573,
          "title": "Semi-Sparsity for Smoothing Filters. (arXiv:2107.00627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Andres Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">He Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1\">Jessica Hodgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>",
          "description": "Recent work has shown great progress in building photorealistic animatable\nfull-body codec avatars, but these avatars still face difficulties in\ngenerating high-fidelity animation of clothing. To address the difficulties, we\npropose a method to build an animatable clothed body avatar with an explicit\nrepresentation of the clothing on the upper body from multi-view captured\nvideos. We use a two-layer mesh representation to separately register the 3D\nscans with templates. In order to improve the photometric correspondence across\ndifferent frames, texture alignment is then performed through inverse rendering\nof the clothing geometry and texture predicted by a variational autoencoder. We\nthen train a new two-layer codec avatar with separate modeling of the upper\nclothing and the inner body layer. To learn the interaction between the body\ndynamics and clothing states, we use a temporal convolution network to predict\nthe clothing latent code based on a sequence of input skeletal poses. We show\nphotorealistic animation output for three different actors, and demonstrate the\nadvantage of our clothed-body avatars over single-layer avatars in the previous\nwork. We also show the benefit of an explicit clothing model which allows the\nclothing texture to be edited in the animation output.",
          "link": "http://arxiv.org/abs/2106.14879",
          "publishedOn": "2021-07-02T01:58:00.275Z",
          "wordCount": 659,
          "title": "Explicit Clothing Modeling for an Animatable Full-Body Avatar. (arXiv:2106.14879v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, evaluate their calibration under continuous distributional\nshifts and their performance on OOD detection for image classification tasks.\nThen, we extend the most promising approaches to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under realistic distributional shifts.",
          "link": "http://arxiv.org/abs/2107.00649",
          "publishedOn": "2021-07-02T01:58:00.268Z",
          "wordCount": 585,
          "title": "On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "Recently, Vision Transformer and its variants have shown great promise on\nvarious computer vision tasks. The ability of capturing short- and long-range\nvisual dependencies through self-attention is arguably the main source for the\nsuccess. But it also brings challenges due to quadratic computational overhead,\nespecially for the high-resolution vision tasks (e.g., object detection). In\nthis paper, we present focal self-attention, a new mechanism that incorporates\nboth fine-grained local and coarse-grained global interactions. Using this new\nmechanism, each token attends the closest surrounding tokens at fine\ngranularity but the tokens far away at coarse granularity, and thus can capture\nboth short- and long-range visual dependencies efficiently and effectively.\nWith focal self-attention, we propose a new variant of Vision Transformer\nmodels, called Focal Transformer, which achieves superior performance over the\nstate-of-the-art vision Transformers on a range of public image classification\nand object detection benchmarks. In particular, our Focal Transformer models\nwith a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8\nTop-1 accuracy, respectively, on ImageNet classification at 224x224 resolution.\nUsing Focal Transformers as the backbones, we obtain consistent and substantial\nimprovements over the current state-of-the-art Swin Transformers for 6\ndifferent object detection methods trained with standard 1x and 3x schedules.\nOur largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs\non COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation,\ncreating new SoTA on three of the most challenging computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.00641",
          "publishedOn": "2021-07-02T01:58:00.249Z",
          "wordCount": 689,
          "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers. (arXiv:2107.00641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_P/0/1/0/all/0/1\">Pratik Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pravendra Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Deep learning models generally learn the biases present in the training data.\nResearchers have proposed several approaches to mitigate such biases and make\nthe model fair. Bias mitigation techniques assume that a sufficiently large\nnumber of training examples are present. However, we observe that if the\ntraining data is limited, then the effectiveness of bias mitigation methods is\nseverely degraded. In this paper, we propose a novel approach to address this\nproblem. Specifically, we adapt self-supervision and self-distillation to\nreduce the impact of biases on the model in this setting. Self-supervision and\nself-distillation are not used for bias mitigation. However, through this work,\nwe demonstrate for the first time that these techniques are very effective in\nbias mitigation. We empirically show that our approach can significantly reduce\nthe biases learned by the model. Further, we experimentally demonstrate that\nour approach is complementary to other bias mitigation strategies. Our approach\nsignificantly improves their performance and further reduces the model biases\nin the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset,\nour approach significantly reduces the bias score of the baseline model by\n78.22% and outperforms it in terms of accuracy by a significant absolute margin\nof 8.89%. It also significantly reduces the bias score for the state-of-the-art\ndomain independent bias mitigation method by 59.26% and improves its\nperformance by a significant absolute margin of 7.08%.",
          "link": "http://arxiv.org/abs/2107.00067",
          "publishedOn": "2021-07-02T01:58:00.241Z",
          "wordCount": 669,
          "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation. (arXiv:2107.00067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
          "link": "http://arxiv.org/abs/2107.00645",
          "publishedOn": "2021-07-02T01:58:00.234Z",
          "wordCount": 616,
          "title": "Global Filter Networks for Image Classification. (arXiv:2107.00645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00418",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Suh_S/0/1/0/all/0/1\">Sungho Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheon_S/0/1/0/all/0/1\">Sojeong Cheon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_W/0/1/0/all/0/1\">Wonseo Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_Y/0/1/0/all/0/1\">Yeon Woong Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_W/0/1/0/all/0/1\">Won-Kyung Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paik_J/0/1/0/all/0/1\">Ji-Sun Paik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sung Eun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_D/0/1/0/all/0/1\">Dong-Jin Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Oh Lee</a>",
          "description": "Deep neural networks (DNNs) have been widely used for medical image analysis.\nHowever, the lack of access a to large-scale annotated dataset poses a great\nchallenge, especially in the case of rare diseases, or new domains for the\nresearch society. Transfer of pre-trained features, from the relatively large\ndataset is a considerable solution. In this paper, we have explored supervised\nsegmentation using domain adaptation for optic nerve and orbital tumor, when\nonly small sampled CT images are given. Even the lung image database consortium\nimage collection (LIDC-IDRI) is a cross-domain to orbital CT, but the proposed\ndomain adaptation method improved the performance of attention U-Net for the\nsegmentation in public optic nerve dataset and our clinical orbital tumor\ndataset. The code and dataset are available at https://github.com/cmcbigdata.",
          "link": "http://arxiv.org/abs/2107.00418",
          "publishedOn": "2021-07-02T01:58:00.226Z",
          "wordCount": 595,
          "title": "Supervised Segmentation with Domain Adaptation for Small Sampled Orbital CT Images. (arXiv:2107.00418v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1\">Hessah Albanwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaohu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Desheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guldmann_J/0/1/0/all/0/1\">Jean-Michel Guldmann</a>",
          "description": "The current practice in land cover/land use change analysis relies heavily on\nthe individually classified maps of the multitemporal data set. Due to varying\nacquisition conditions (e.g., illumination, sensors, seasonal differences), the\nclassification maps yielded are often inconsistent through time for robust\nstatistical analysis. 3D geometric features have been shown to be stable for\nassessing differences across the temporal data set. Therefore, in this article\nwe investigate he use of a multitemporal orthophoto and digital surface model\nderived from satellite data for spatiotemporal classification. Our approach\nconsists of two major steps: generating per-class probability distribution maps\nusing the random-forest classifier with limited training samples, and making\nspatiotemporal inferences using an iterative 3D spatiotemporal filter operating\non per-class probability maps. Our experimental results demonstrate that the\nproposed methods can consistently improve the individual classification results\nby 2%-6% and thus can be an important postclassification refinement approach.",
          "link": "http://arxiv.org/abs/2107.00590",
          "publishedOn": "2021-07-02T01:58:00.218Z",
          "wordCount": 590,
          "title": "3D Iterative Spatiotemporal Filtering for Classification of Multitemporal Satellite Data Sets. (arXiv:2107.00590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>",
          "description": "This report presents the approach used in the submission of Generic Event\nBoundary Detection (GEBD) Challenge at CVPR21. In this work, we design a\nCascaded Temporal Attention Network (CASTANET) for GEBD, which is formed by\nthree parts, the backbone network, the temporal attention module, and the\nclassification module. Specifically, the Channel-Separated Convolutional\nNetwork (CSN) is used as the backbone network to extract features, and the\ntemporal attention module is designed to enforce the network to focus on the\ndiscriminative features. After that, the cascaded architecture is used in the\nclassification module to generate more accurate boundaries. In addition, the\nensemble strategy is used to further improve the performance of the proposed\nmethod. The proposed method achieves 83.30% F1 score on Kinetics-GEBD test set,\nwhich improves 20.5% F1 score compared to the baseline method. Code is\navailable at https://github.com/DexiangHong/Cascade-PC.",
          "link": "http://arxiv.org/abs/2107.00239",
          "publishedOn": "2021-07-02T01:58:00.193Z",
          "wordCount": 591,
          "title": "Generic Event Boundary Detection Challenge at CVPR 2021 Technical Report: Cascaded Temporal Attention Network (CASTANET). (arXiv:2107.00239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1\">Mohamed Amine Kerkouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1\">Marouane Tliba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harba_R/0/1/0/all/0/1\">Rachid Harba</a>",
          "description": "Human vision is naturally more attracted by some regions within their field\nof view than others. This intrinsic selectivity mechanism, so-called visual\nattention, is influenced by both high- and low-level factors; such as the\nglobal environment (illumination, background texture, etc.), stimulus\ncharacteristics (color, intensity, orientation, etc.), and some prior visual\ninformation. Visual attention is useful for many computer vision applications\nsuch as image compression, recognition, and captioning. In this paper, we\npropose an end-to-end deep-based method, so-called SALYPATH (SALiencY and\nscanPATH), that efficiently predicts the scanpath of an image through features\nof a saliency model. The idea is predict the scanpath by exploiting the\ncapacity of a deep-based model to predict the saliency. The proposed method was\nevaluated through 2 well-known datasets. The results obtained showed the\nrelevance of the proposed framework comparing to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00559",
          "publishedOn": "2021-07-02T01:58:00.186Z",
          "wordCount": 584,
          "title": "SALYPATH: A Deep-Based Architecture for visual attention prediction. (arXiv:2107.00559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Training deep neural networks with an $L_0$ regularization is one of the\nprominent approaches for network pruning or sparsification. The method prunes\nthe network during training by encouraging weights to become exactly zero.\nHowever, recent work of Gale et al. reveals that although this method yields\nhigh compression rates on smaller datasets, it performs inconsistently on\nlarge-scale learning tasks, such as ResNet50 on ImageNet. We analyze this\nphenomenon through the lens of variational inference and find that it is likely\ndue to the independent modeling of binary gates, the mean-field approximation,\nwhich is known in Bayesian statistics for its poor performance due to the crude\napproximation. To mitigate this deficiency, we propose a dependency modeling of\nbinary gates, which can be modeled effectively as a multi-layer perceptron\n(MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a\ndependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10,\nCIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$\noutperforms the original $L_0$-HC algorithm of Louizos et al. by a significant\nmargin, especially on ImageNet. Compared with the state-of-the-arts network\nsparsification algorithms, our dependency modeling makes the $L_0$-based\nsparsification once again very competitive on large-scale learning tasks. Our\nsource code is available at https://github.com/leo-yangli/dep-l0.",
          "link": "http://arxiv.org/abs/2107.00070",
          "publishedOn": "2021-07-02T01:58:00.180Z",
          "wordCount": 646,
          "title": "Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency Modeling. (arXiv:2107.00070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>",
          "description": "We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.",
          "link": "http://arxiv.org/abs/2107.00228",
          "publishedOn": "2021-07-02T01:58:00.172Z",
          "wordCount": 545,
          "title": "Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00400",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quach_M/0/1/0/all/0/1\">Maurice Quach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenzise_G/0/1/0/all/0/1\">Giuseppe Valenzise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duhamel_P/0/1/0/all/0/1\">Pierre Duhamel</a>",
          "description": "This paper proposes a lossless point cloud (PC) geometry compression method\nthat uses neural networks to estimate the probability distribution of voxel\noccupancy. First, to take into account the PC sparsity, our method adaptively\npartitions a point cloud into multiple voxel block sizes. This partitioning is\nsignalled via an octree. Second, we employ a deep auto-regressive generative\nmodel to estimate the occupancy probability of each voxel given the previously\nencoded ones. We then employ the estimated probabilities to code efficiently a\nblock using a context-based arithmetic coder. Our context has variable size and\ncan expand beyond the current block to learn more accurate probabilities. We\nalso consider using data augmentation techniques to increase the generalization\ncapability of the learned probability models, in particular in the presence of\nnoise and lower-density point clouds. Experimental evaluation, performed on a\nvariety of point clouds from four different datasets and with diverse\ncharacteristics, demonstrates that our method reduces significantly (by up to\n30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.",
          "link": "http://arxiv.org/abs/2107.00400",
          "publishedOn": "2021-07-02T01:58:00.163Z",
          "wordCount": 652,
          "title": "Lossless Coding of Point Cloud Geometry using a Deep Generative Model. (arXiv:2107.00400v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1\">Erik Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacVittie_K/0/1/0/all/0/1\">Korey MacVittie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilly_J/0/1/0/all/0/1\">John Lilly</a>",
          "description": "Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.",
          "link": "http://arxiv.org/abs/2107.00436",
          "publishedOn": "2021-07-02T01:58:00.143Z",
          "wordCount": 570,
          "title": "Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00283",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Detection of colon polyps has become a trending topic in the intersecting\nfields of machine learning and gastrointestinal endoscopy. The focus has mainly\nbeen on per-frame classification. More recently, polyp segmentation has gained\nattention in the medical community. Segmentation has the advantage of being\nmore accurate than per-frame classification or object detection as it can show\nthe affected area in greater detail. For our contribution to the EndoCV 2021\nsegmentation challenge, we propose two separate approaches. First, a\nsegmentation model named TriUNet composed of three separate UNet models.\nSecond, we combine TriUNet with an ensemble of well-known segmentation models,\nnamely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called\nDivergentNets to produce more generalizable medical image segmentation masks.\nIn addition, we propose a modified Dice loss that calculates loss only for a\nsingle class when performing multiclass segmentation, forcing the model to\nfocus on what is most important. Overall, the proposed methods achieved the\nbest average scores for each respective round in the challenge, with TriUNet\nbeing the winning model in Round I and DivergentNets being the winning model in\nRound II of the segmentation generalization challenge at EndoCV 2021. The\nimplementation of our approach is made publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2107.00283",
          "publishedOn": "2021-07-02T01:58:00.136Z",
          "wordCount": 691,
          "title": "DivergentNets: Medical Image Segmentation by Network Ensemble. (arXiv:2107.00283v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xufeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maple_C/0/1/0/all/0/1\">Carsten Maple</a>",
          "description": "Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.",
          "link": "http://arxiv.org/abs/2107.00500",
          "publishedOn": "2021-07-02T01:58:00.128Z",
          "wordCount": 632,
          "title": "On the detection-to-track association for online multi-object tracking. (arXiv:2107.00500v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_N/0/1/0/all/0/1\">Nina Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitri_O/0/1/0/all/0/1\">Omar de Mitri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hang Beom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Windberger_A/0/1/0/all/0/1\">Alexander Windberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>",
          "description": "Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.",
          "link": "http://arxiv.org/abs/2107.00360",
          "publishedOn": "2021-07-02T01:58:00.121Z",
          "wordCount": 606,
          "title": "Towards Measuring Bias in Image Classification. (arXiv:2107.00360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.",
          "link": "http://arxiv.org/abs/2107.00395",
          "publishedOn": "2021-07-02T01:58:00.109Z",
          "wordCount": 683,
          "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph. (arXiv:2107.00395v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Tatsuya Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.",
          "link": "http://arxiv.org/abs/2107.00456",
          "publishedOn": "2021-07-02T01:58:00.009Z",
          "wordCount": 705,
          "title": "Crowdsourcing Evaluation of Saliency-based XAI Methods. (arXiv:2107.00456v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "In natural conversation and interaction, our hands often overlap or are in\ncontact with each other. Due to the homogeneous appearance of hands, this makes\nestimating the 3D pose of interacting hands from images difficult. In this\npaper we demonstrate that self-similarity, and the resulting ambiguities in\nassigning pixel observations to the respective hands and their parts, is a\nmajor cause of the final 3D pose error. Motivated by this insight, we propose\nDIGIT, a novel method for estimating the 3D poses of two interacting hands from\na single monocular image. The method consists of two interwoven branches that\nprocess the input imagery into a per-pixel semantic part segmentation mask and\na visual feature volume. In contrast to prior work, we do not decouple the\nsegmentation from the pose estimation stage, but rather leverage the per-pixel\nprobabilities directly in the downstream pose estimation task. To do so, the\npart probabilities are merged with the visual features and processed via\nfully-convolutional layers. We experimentally show that the proposed approach\nachieves new state-of-the-art performance on the InterHand2.6M dataset for both\nsingle and interacting hands across all metrics. We provide detailed ablation\nstudies to demonstrate the efficacy of our method and to provide insights into\nhow the modelling of pixel ownership affects single and interacting hand pose\nestimation. Our code will be released for research purposes.",
          "link": "http://arxiv.org/abs/2107.00434",
          "publishedOn": "2021-07-02T01:57:59.988Z",
          "wordCount": 669,
          "title": "Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation. (arXiv:2107.00434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Longteng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingzhen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>",
          "description": "In this paper, we propose an Omni-perception Pre-Trainer (OPT) for\ncross-modal understanding and generation, by jointly modeling visual, text and\naudio resources. OPT is constructed in an encoder-decoder framework, including\nthree single-modal encoders to generate token-based embeddings for each\nmodality, a cross-modal encoder to encode the correlations among the three\nmodalities, and two cross-modal decoders to generate text and image\nrespectively. For the OPT's pre-training, we design a multi-task pretext\nlearning scheme to model multi-modal resources from three different data\ngranularities, \\ie, token-, modality-, and sample-level modeling, through which\nOPT learns to align and translate among different modalities. The pre-training\ntask is carried out on a large amount of image-text-audio triplets from Open\nImages. Experimental results show that OPT can learn strong image-text-audio\nmulti-modal representations and achieve promising results on a variety of\ncross-modal understanding and generation tasks.",
          "link": "http://arxiv.org/abs/2107.00249",
          "publishedOn": "2021-07-02T01:57:59.951Z",
          "wordCount": 583,
          "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation. (arXiv:2107.00249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasar_M/0/1/0/all/0/1\">Mohammad Samin Yasar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_T/0/1/0/all/0/1\">Tariq Iqbal</a>",
          "description": "Human motion prediction is an essential component for enabling closer\nhuman-robot collaboration. The task of accurately predicting human motion is\nnon-trivial. It is compounded by the variability of human motion, both at a\nskeletal level due to the varying size of humans and at a motion level due to\nindividual movement's idiosyncrasies. These variables make it challenging for\nlearning algorithms to obtain a general representation that is robust to the\ndiverse spatio-temporal patterns of human motion. In this work, we propose a\nmodular sequence learning approach that allows end-to-end training while also\nhaving the flexibility of being fine-tuned. Our approach relies on the\ndiversity of training samples to first learn a robust representation, which can\nthen be fine-tuned in a continual learning setup to predict the motion of new\nsubjects. We evaluated the proposed approach by comparing its performance\nagainst state-of-the-art baselines. The results suggest that our approach\noutperforms other methods over all the evaluated temporal horizons, using a\nsmall amount of data for fine-tuning. The improved performance of our approach\nopens up the possibility of using continual learning for personalized and\nreliable motion prediction.",
          "link": "http://arxiv.org/abs/2107.00544",
          "publishedOn": "2021-07-02T01:57:59.944Z",
          "wordCount": 615,
          "title": "Improving Human Motion Prediction Through Continual Learning. (arXiv:2107.00544v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1\">Aaron Kujawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "We introduce $\\textit{InExtremIS}$, a weakly supervised 3D approach to train\na deep image segmentation network using particularly weak train-time\nannotations: only 6 extreme clicks at the boundary of the objects of interest.\nOur fully-automatic method is trained end-to-end and does not require any\ntest-time annotations. From the extreme points, 3D bounding boxes are extracted\naround objects of interest. Then, deep geodesics connecting extreme points are\ngenerated to increase the amount of \"annotated\" voxels within the bounding\nboxes. Finally, a weakly supervised regularised loss derived from a Conditional\nRandom Field formulation is used to encourage prediction consistency over\nhomogeneous regions. Extensive experiments are performed on a large open\ndataset for Vestibular Schwannoma segmentation. $\\textit{InExtremIS}$ obtained\ncompetitive performance, approaching full supervision and outperforming\nsignificantly other weakly supervised techniques based on bounding boxes.\nMoreover, given a fixed annotation time budget, $\\textit{InExtremIS}$\noutperforms full supervision. Our code and data are available online.",
          "link": "http://arxiv.org/abs/2107.00583",
          "publishedOn": "2021-07-02T01:57:59.914Z",
          "wordCount": 603,
          "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation. (arXiv:2107.00583v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1\">Tehrim Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sumin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.",
          "link": "http://arxiv.org/abs/2107.00233",
          "publishedOn": "2021-07-02T01:57:59.904Z",
          "wordCount": 604,
          "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning. (arXiv:2107.00233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shurun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yan Ye</a>",
          "description": "The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.",
          "link": "http://arxiv.org/abs/2107.00328",
          "publishedOn": "2021-07-02T01:57:59.897Z",
          "wordCount": 619,
          "title": "End-to-end Compression Towards Machine Vision: Network Architecture Design and Optimization. (arXiv:2107.00328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00296",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuhao Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain diseases, most of them give few\nexplanations on how to make predictions. Inspired by Koch's Postulates, the\nfoundation in evidence-based medicine (EBM) to identify the pathogen, we\npropose to exploit the interpretability of deep learning application in medical\ndiagnosis. By determining and isolating the neuron activation patterns on which\ndiabetic retinopathy (DR) detector relies to make decisions, we demonstrate the\ndirect relation between the isolated neuron activation and lesions for a\npathological explanation. To be specific, we first define novel pathological\ndescriptors using activated neurons of the DR detector to encode both spatial\nand appearance information of lesions. Then, to visualize the symptom encoded\nin the descriptor, we propose Patho-GAN, a new network to synthesize medically\nplausible retinal images. By manipulating these descriptors, we could even\narbitrarily control the position, quantity, and categories of generated\nlesions. We also show that our synthesized images carry the symptoms directly\nrelated to diabetic retinopathy diagnosis. Our generated images are both\nqualitatively and quantitatively superior to the ones by previous methods.\nBesides, compared to existing methods that take hours to generate an image, our\nsecond level speed endows the potential to be an effective solution for data\naugmentation.",
          "link": "http://arxiv.org/abs/2107.00296",
          "publishedOn": "2021-07-02T01:57:59.888Z",
          "wordCount": 667,
          "title": "Explainable Diabetic Retinopathy Detection and Retinal Image Generation. (arXiv:2107.00296v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.",
          "link": "http://arxiv.org/abs/2107.00451",
          "publishedOn": "2021-07-02T01:57:59.881Z",
          "wordCount": 597,
          "title": "VideoLightFormer: Lightweight Action Recognition using Transformers. (arXiv:2107.00451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Place recognition gives a SLAM system the ability to correct cumulative\nerrors. Unlike images that contain rich texture features, point clouds are\nalmost pure geometric information which makes place recognition based on point\nclouds challenging. Existing works usually encode low-level features such as\ncoordinate, normal, reflection intensity, etc., as local or global descriptors\nto represent scenes. Besides, they often ignore the translation between point\nclouds when matching descriptors. Different from most existing methods, we\nexplore the use of high-level features, namely semantics, to improve the\ndescriptor's representation ability. Also, when matching descriptors, we try to\ncorrect the translation between point clouds to improve accuracy. Concretely,\nwe propose a novel global descriptor, Semantic Scan Context, which explores\nsemantic information to represent scenes more effectively. We also present a\ntwo-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align\nthe point cloud to improve matching performance. Our experiments on the KITTI\ndataset show that our approach outperforms the state-of-the-art methods with a\nlarge margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.",
          "link": "http://arxiv.org/abs/2107.00382",
          "publishedOn": "2021-07-02T01:57:59.874Z",
          "wordCount": 621,
          "title": "SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_K/0/1/0/all/0/1\">Konstantin Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emelianova_E/0/1/0/all/0/1\">Ekaterina Emelianova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1\">Daniil Tropin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoryukina_N/0/1/0/all/0/1\">Natalya Skoryukina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyshova_Y/0/1/0/all/0/1\">Yulia Chernyshova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheshkus_A/0/1/0/all/0/1\">Alexander Sheshkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usilin_S/0/1/0/all/0/1\">Sergey Usilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burie_J/0/1/0/all/0/1\">Jean-Christophe Burie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luqman_M/0/1/0/all/0/1\">Muhammad Muzzamil Luqman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">Vladimir V. Arlazarov</a>",
          "description": "Identity documents recognition is an important sub-field of document\nanalysis, which deals with tasks of robust document detection, type\nidentification, text fields recognition, as well as identity fraud prevention\nand document authenticity validation given photos, scans, or video frames of an\nidentity document capture. Significant amount of research has been published on\nthis topic in recent years, however a chief difficulty for such research is\nscarcity of datasets, due to the subject matter being protected by security\nrequirements. A few datasets of identity documents which are available lack\ndiversity of document types, capturing conditions, or variability of document\nfield values. In addition, the published datasets were typically designed only\nfor a subset of document recognition problems, not for a complex identity\ndocument analysis. In this paper, we present a dataset MIDV-2020 which consists\nof 1000 video clips, 2000 scanned images, and 1000 photos of 1000 unique mock\nidentity documents, each with unique text field values and unique artificially\ngenerated faces, with rich annotation. For the presented benchmark dataset\nbaselines are provided for such tasks as document location and identification,\ntext fields recognition, and face detection. With 72409 annotated images in\ntotal, to the date of publication the proposed dataset is the largest publicly\navailable identity documents dataset with variable artificially generated data,\nand we believe that it will prove invaluable for advancement of the field of\ndocument analysis and recognition. The dataset is available for download at\nthis ftp URL and this http URL .",
          "link": "http://arxiv.org/abs/2107.00396",
          "publishedOn": "2021-07-02T01:57:59.854Z",
          "wordCount": 705,
          "title": "MIDV-2020: A Comprehensive Benchmark Dataset for Identity Document Analysis. (arXiv:2107.00396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Henglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhaoz%3F_G/0/1/0/all/0/1\">Guoying Zhaoz?</a>",
          "description": "We introduce a new dataset for the emotional artificial intelligence\nresearch: identity-free video dataset for Micro-Gesture Understanding and\nEmotion analysis (iMiGUE). Different from existing public datasets, iMiGUE\nfocuses on nonverbal body gestures without using any identity information,\nwhile the predominant researches of emotion analysis concern sensitive\nbiometric data, like face and speech. Most importantly, iMiGUE focuses on\nmicro-gestures, i.e., unintentional behaviors driven by inner feelings, which\nare different from ordinary scope of gestures from other gesture datasets which\nare mostly intentionally performed for illustrative purposes. Furthermore,\niMiGUE is designed to evaluate the ability of models to analyze the emotional\nstates by integrating information of recognized micro-gesture, rather than just\nrecognizing prototypes in the sequences separately (or isolatedly). This is\nbecause the real need for emotion AI is to understand the emotional states\nbehind gestures in a holistic way. Moreover, to counter for the challenge of\nimbalanced sample distribution of this dataset, an unsupervised learning method\nis proposed to capture latent representations from the micro-gesture sequences\nthemselves. We systematically investigate representative methods on this\ndataset, and comprehensive experimental results reveal several interesting\ninsights from the iMiGUE, e.g., micro-gesture-based analysis can promote\nemotion understanding. We confirm that the new iMiGUE dataset could advance\nstudies of micro-gesture and emotion AI.",
          "link": "http://arxiv.org/abs/2107.00285",
          "publishedOn": "2021-07-02T01:57:59.846Z",
          "wordCount": 655,
          "title": "iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis. (arXiv:2107.00285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have achieved impressive performance in various biomedical\napplications. Most existing methods tend to define the adjacency matrix among\nsamples manually based on meta-features, and then obtain the node embeddings\nfor downstream tasks by Graph Representation Learning (GRL). However, it is not\neasy for these approaches to generalize to unseen samples. Meanwhile, the\ncomplex correlation between modalities is also ignored. As a result, these\nfactors inevitably yield the inadequacy of providing valid information about\nthe patient's condition for a reliable diagnosis. In this paper, we propose an\nend-to-end Multimodal Graph Learning framework (MMGL) for disease prediction.\nTo effectively exploit the rich information across multi-modality associated\nwith diseases, amodal-attentional multi-modal fusion is proposed to integrate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the adjacency matrix\nmanually as existing methods, the latent graph structure can be captured\nthrough a novel way of adaptive graph learning. It could be jointly optimized\nwith the prediction model, thus revealing the intrinsic connections among\nsamples. Unlike the previous transductive methods, our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction problems is then carefully designed\nand presented, demonstrating that MMGL obtains more favorable performances. In\naddition, we also visualize and analyze the learned graph structure to provide\nmore reliable decision support for doctors in real medical applications and\ninspiration for disease research.",
          "link": "http://arxiv.org/abs/2107.00206",
          "publishedOn": "2021-07-02T01:57:59.834Z",
          "wordCount": 693,
          "title": "Multi-modal Graph Learning for Disease Prediction. (arXiv:2107.00206v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_P/0/1/0/all/0/1\">Pegah Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheshkal_S/0/1/0/all/0/1\">Sajad Amouei Sheshkal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L.Hammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Processing medical data to find abnormalities is a time-consuming and costly\ntask, requiring tremendous efforts from medical experts. Therefore, Ai has\nbecome a popular tool for the automatic processing of medical data, acting as a\nsupportive tool for doctors. AI tools highly depend on data for training the\nmodels. However, there are several constraints to access to large amounts of\nmedical data to train machine learning algorithms in the medical domain, e.g.,\ndue to privacy concerns and the costly, time-consuming medical data annotation\nprocess. To address this, in this paper we present a novel synthetic data\ngeneration pipeline called SinGAN-Seg to produce synthetic medical data with\nthe corresponding annotated ground truth masks. We show that these synthetic\ndata generation pipelines can be used as an alternative to bypass privacy\nconcerns and as an alternative way to produce artificial segmentation datasets\nwith corresponding ground truth masks to avoid the tedious medical data\nannotation process. As a proof of concept, we used an open polyp segmentation\ndataset. By training UNet++ using both the real polyp segmentation dataset and\nthe corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we\nshow that the synthetic data can achieve a very close performance to the real\ndata when the real segmentation datasets are large enough. In addition, we show\nthat synthetic data generated from the SinGAN-Seg pipeline improving the\nperformance of segmentation algorithms when the training dataset is very small.\nSince our SinGAN-Seg pipeline is applicable for any medical dataset, this\npipeline can be used with any other segmentation datasets.",
          "link": "http://arxiv.org/abs/2107.00471",
          "publishedOn": "2021-07-02T01:57:59.826Z",
          "wordCount": 718,
          "title": "SinGAN-Seg: Synthetic Training Data Generation for Medical Image Segmentation. (arXiv:2107.00471v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>",
          "description": "Humans perceive the world by concurrently processing and fusing\nhigh-dimensional inputs from multiple modalities such as vision and audio.\nMachine perception models, in stark contrast, are typically modality-specific\nand optimised for unimodal benchmarks, and hence late-stage fusion of final\nrepresentations or predictions from each modality (`late-fusion') is still a\ndominant paradigm for multimodal video classification. Instead, we introduce a\nnovel transformer based architecture that uses `fusion bottlenecks' for\nmodality fusion at multiple layers. Compared to traditional pairwise\nself-attention, our model forces information between different modalities to\npass through a small number of bottleneck latents, requiring the model to\ncollate and condense the most relevant information in each modality and only\nshare what is necessary. We find that such a strategy improves fusion\nperformance, at the same time reducing computational cost. We conduct thorough\nablation studies, and achieve state-of-the-art results on multiple audio-visual\nclassification benchmarks including Audioset, Epic-Kitchens and VGGSound. All\ncode and models will be released.",
          "link": "http://arxiv.org/abs/2107.00135",
          "publishedOn": "2021-07-02T01:57:59.818Z",
          "wordCount": 590,
          "title": "Attention Bottlenecks for Multimodal Fusion. (arXiv:2107.00135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:57:59.798Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pira_G/0/1/0/all/0/1\">Giacomo Pira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Spiking Neural Networks (SNNs), despite being energy-efficient when\nimplemented on neuromorphic hardware and coupled with event-based Dynamic\nVision Sensors (DVS), are vulnerable to security threats, such as adversarial\nattacks, i.e., small perturbations added to the input for inducing a\nmisclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet\nefficient adversarial attack methodologies targeted to perturb the event\nsequences that compose the input of the SNNs. First, we show that noise filters\nfor DVS can be used as defense mechanisms against adversarial attacks.\nAfterwards, we implement several attacks and test them in the presence of two\ntypes of noise filters for DVS cameras. The experimental results show that the\nfilters can only partially defend the SNNs against our proposed DVS-Attacks.\nUsing the best settings for the noise filters, our proposed Mask Filter-Aware\nDash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset\nand by more than 65% on the MNIST dataset, compared to the original clean\nframes. The source code of all the proposed DVS-Attacks and noise filters is\nreleased at https://github.com/albertomarchisio/DVS-Attacks.",
          "link": "http://arxiv.org/abs/2107.00415",
          "publishedOn": "2021-07-02T01:57:59.790Z",
          "wordCount": 633,
          "title": "DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks. (arXiv:2107.00415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Knowledge Distillation (KD) is a popular technique to transfer knowledge from\na teacher model or ensemble to a student model. Its success is generally\nattributed to the privileged information on similarities/consistency between\nthe class distributions or intermediate feature representations of the teacher\nmodel and the student model. However, directly pushing the student model to\nmimic the probabilities/features of the teacher model to a large extent limits\nthe student model in learning undiscovered knowledge/features. In this paper,\nwe propose a novel inheritance and exploration knowledge distillation framework\n(IE-KD), in which a student model is split into two parts - inheritance and\nexploration. The inheritance part is learned with a similarity loss to transfer\nthe existing learned knowledge from the teacher model to the student model,\nwhile the exploration part is encouraged to learn representations different\nfrom the inherited ones with a dis-similarity loss. Our IE-KD framework is\ngeneric and can be easily combined with existing distillation or mutual\nlearning methods for training deep neural networks. Extensive experiments\ndemonstrate that these two parts can jointly push the student model to learn\nmore diversified and effective representations, and our IE-KD can be a general\ntechnique to improve the student network to achieve SOTA performance.\nFurthermore, by applying our IE-KD to the training of two networks, the\nperformance of both can be improved w.r.t. deep mutual learning. The code and\nmodels of IE-KD will be make publicly available at\nhttps://github.com/yellowtownhz/IE-KD.",
          "link": "http://arxiv.org/abs/2107.00181",
          "publishedOn": "2021-07-02T01:57:59.783Z",
          "wordCount": 693,
          "title": "Revisiting Knowledge Distillation: An Inheritance and Exploration Framework. (arXiv:2107.00181v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00235",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pavlov_S/0/1/0/all/0/1\">Stoyan Pavlov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momcheva_G/0/1/0/all/0/1\">Galina Momcheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burlakova_P/0/1/0/all/0/1\">Pavlina Burlakova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atanasov_S/0/1/0/all/0/1\">Simeon Atanasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Dimo Stoyanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ivanov_M/0/1/0/all/0/1\">Martin Ivanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tonchev_A/0/1/0/all/0/1\">Anton Tonchev</a>",
          "description": "This paper presents a proof of concept for the usefulness of second-order\ntexture features for the qualitative analysis and classification of chromogenic\nin-situ hybridization whole slide images in high-throughput imaging\nexperiments. The challenge is that currently, the gold standard for gene\nexpression grading in such images is expert assessment. The idea of the\nresearch team is to use different approaches in the analysis of these images\nthat will be used for structural segmentation and functional analysis in gene\nexpression. The article presents such perspective idea to select a number of\ntextural features that are going to be used for classification. In our\nexperiment, natural grouping of image samples (tiles) depending on their local\ntexture properties was explored in an unsupervised classification procedure.\nThe features are reduced to two dimensions with fuzzy c-means clustering. The\noverall conclusion of this experiment is that Haralick features are a viable\nchoice for classification and analysis of chromogenic in-situ hybridization\nimage data. The principal component analysis approach produced slightly more\n\"understandable\" from an annotator's point of view classes.",
          "link": "http://arxiv.org/abs/2107.00235",
          "publishedOn": "2021-07-02T01:57:59.776Z",
          "wordCount": 659,
          "title": "Feasibility of Haralick's Texture Features for the Classification of Chromogenic In-situ Hybridization Images. (arXiv:2107.00235v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piciarelli_C/0/1/0/all/0/1\">Claudio Piciarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>",
          "description": "Swarms of drones are being more and more used in many practical scenarios,\nsuch as surveillance, environmental monitoring, search and rescue in\nhardly-accessible areas, etc.. While a single drone can be guided by a human\noperator, the deployment of a swarm of multiple drones requires proper\nalgorithms for automatic task-oriented control. In this paper, we focus on\nvisual coverage optimization with drone-mounted camera sensors. In particular,\nwe consider the specific case in which the coverage requirements are uneven,\nmeaning that different parts of the environment have different coverage\npriorities. We model these coverage requirements with relevance maps and\npropose a deep reinforcement learning algorithm to guide the swarm. The paper\nfirst defines a proper learning model for a single drone, and then extends it\nto the case of multiple drones both with greedy and cooperative strategies.\nExperimental results show the performance of the proposed method, also compared\nwith a standard patrolling algorithm.",
          "link": "http://arxiv.org/abs/2107.00362",
          "publishedOn": "2021-07-02T01:57:59.767Z",
          "wordCount": 610,
          "title": "Drone swarm patrolling with uneven coverage requirements. (arXiv:2107.00362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.",
          "link": "http://arxiv.org/abs/2107.00166",
          "publishedOn": "2021-07-02T01:57:59.746Z",
          "wordCount": 620,
          "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wonju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Seok-Yong Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jooeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minje Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechil_K/0/1/0/all/0/1\">Kirill Chechil</a>",
          "description": "While many real-world data streams imply that they change frequently in a\nnonstationary way, most of deep learning methods optimize neural networks on\ntraining data, and this leads to severe performance degradation when dataset\nshift happens. However, it is less possible to annotate or inspect newly\nstreamed data by humans, and thus it is desired to measure model drift at\ninference time in an unsupervised manner. In this paper, we propose a novel\nmethod of model drift estimation by exploiting statistics of batch\nnormalization layer on unlabeled test data. To remedy possible sampling error\nof streamed input data, we adopt low-rank approximation to each\nrepresentational layer. We show the effectiveness of our method not only on\ndataset shift detection but also on model selection when there are multiple\ncandidate models among model zoo or training trajectories in an unsupervised\nway. We further demonstrate the consistency of our method by comparing model\ndrift scores between different network architectures.",
          "link": "http://arxiv.org/abs/2107.00191",
          "publishedOn": "2021-07-02T01:57:59.739Z",
          "wordCount": 616,
          "title": "Unsupervised Model Drift Estimation with Batch Normalization Statistics for Dataset Shift Detection and Model Selection. (arXiv:2107.00191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "In real-world applications, data often come in a growing manner, where the\ndata volume and the number of classes may increase dynamically. This will bring\na critical challenge for learning: given the increasing data volume or the\nnumber of classes, one has to instantaneously adjust the neural model capacity\nto obtain promising performance. Existing methods either ignore the growing\nnature of data or seek to independently search an optimal architecture for a\ngiven dataset, and thus are incapable of promptly adjusting the architectures\nfor the changed data. To address this, we present a neural architecture\nadaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust\nprevious architectures on the growing data. Specifically, we introduce an\narchitecture adjuster to generate a suitable architecture for each data\nsnapshot, based on the previous architecture and the different extent between\ncurrent and previous data distributions. Furthermore, we propose an adaptation\ncondition to determine the necessity of adjustment, thereby avoiding\nunnecessary and time-consuming adjustments. Extensive experiments on two growth\nscenarios (increasing data volume and number of classes) demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.00254",
          "publishedOn": "2021-07-02T01:57:59.731Z",
          "wordCount": 626,
          "title": "AdaXpert: Adapting Neural Architecture for Growing Data. (arXiv:2107.00254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00462",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wurster_S/0/1/0/all/0/1\">Skylar W. Wurster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hanqi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterka_T/0/1/0/all/0/1\">Thomas Peterka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_M/0/1/0/all/0/1\">Mukund Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiayi Xu</a>",
          "description": "We present an approach for hierarchical super resolution (SR) using neural\nnetworks on an octree data representation. We train a hierarchy of neural\nnetworks, each capable of 2x upscaling in each spatial dimension between two\nlevels of detail, and use these networks in tandem to facilitate large scale\nfactor super resolution, scaling with the number of trained networks. We\nutilize these networks in a hierarchical super resolution algorithm that\nupscales multiresolution data to a uniform high resolution without introducing\nseam artifacts on octree node boundaries. We evaluate application of this\nalgorithm in a data reduction framework by dynamically downscaling input data\nto an octree-based data structure to represent the multiresolution data before\ncompressing for additional storage reduction. We demonstrate that our approach\navoids seam artifacts common to multiresolution data formats, and show how\nneural network super resolution assisted data reduction can preserve global\nfeatures better than compressors alone at the same compression ratios.",
          "link": "http://arxiv.org/abs/2107.00462",
          "publishedOn": "2021-07-02T01:57:59.724Z",
          "wordCount": 609,
          "title": "Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization. (arXiv:2107.00462v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1\">Emanuele Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>",
          "description": "In this report, we describe the technical details of our submission to the\nEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action\nRecognition. To tackle the domain-shift which exists under the UDA setting, we\nfirst exploited a recent Domain Generalization (DG) technique, called Relative\nNorm Alignment (RNA). It consists in designing a model able to generalize well\nto any unseen domain, regardless of the possibility to access target data at\ntraining time. Then, in a second phase, we extended the approach to work on\nunlabelled target data, allowing the model to adapt to the target distribution\nin an unsupervised fashion. For this purpose, we included in our framework\nexisting UDA algorithms, such as Temporal Attentive Adversarial Adaptation\nNetwork (TA3N), jointly with new multi-stream consistency losses, namely\nTemporal Hard Norm Alignment (T-HNA) and Min-Entropy Consistency (MEC). Our\nsubmission (entry 'plnet') is visible on the leaderboard and it achieved the\n1st position for 'verb', and the 3rd position for both 'noun' and 'action'.",
          "link": "http://arxiv.org/abs/2107.00337",
          "publishedOn": "2021-07-02T01:57:59.717Z",
          "wordCount": 618,
          "title": "PoliTO-IIT Submission to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2107.00337v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>",
          "description": "In this paper, we look at the problem of cross-domain few-shot classification\nthat aims to learn a classifier from previously unseen classes and domains with\nfew labeled samples. We study several strategies including various adapter\ntopologies and operations in terms of their performance and efficiency that can\nbe easily attached to existing methods with different meta-training strategies\nand adapt them for a given task during meta-test phase. We show that parametric\nadapters attached to convolutional layers with residual connections performs\nthe best, and significantly improves the performance of the state-of-the-art\nmodels in the Meta-Dataset benchmark with minor additional cost. Our code will\nbe available at https://github.com/VICO-UoE/URL.",
          "link": "http://arxiv.org/abs/2107.00358",
          "publishedOn": "2021-07-02T01:57:59.697Z",
          "wordCount": 549,
          "title": "Improving Task Adaptation for Cross-domain Few-shot Learning. (arXiv:2107.00358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibing Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. However, designing or searching for a new\nbackbone and pre-training it on ImageNet may require a large number of\ncomputational resources, making it costly to obtain better detection\nperformance. In this paper, we propose a novel backbone network, namely\nCBNetV2, by constructing compositions of existing open-sourced pre-trained\nbackbones. In particular, CBNetV2 architecture groups multiple identical\nbackbones, which are connected through composite connections. We also propose a\nbetter training strategy with the Assistant Supervision for CBNet-based\ndetectors. Without additional pre-training, CBNetV2 can be integrated into\nmainstream detectors, including one-stage and two-stage detectors, as well as\nanchor-based and anchor-free-based ones, and significantly improve their\nperformance by more than 3.0% AP over the baseline on COCO. Also, experiments\nprovide strong evidence showing that composite backbones are more efficient and\nresource-friendly than pre-trained wider and deeper networks, including\nmanual-based and NAS-based, as well as CNN-based and Transformer-based ones.\nParticularly, with single-model and single-scale testing, our HTC Dual-Swin-B\nachieves 58.6% box AP and 51.1% mask AP on COCO test-dev, which is\nsignificantly better than the state-of-the-art result (i.e., 57.7% box AP and\n50.2% mask AP) achieved by a stronger baseline HTC++ with a larger backbone\nSwin-L. Code will be released at https://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-02T01:57:59.689Z",
          "wordCount": 670,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1\">Chi Hang Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1\">Chi Fai Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Reconstructing the scene of robotic surgery from the stereo endoscopic video\nis an important and promising topic in surgical data science, which potentially\nsupports many applications such as surgical visual perception, robotic surgery\neducation and intra-operative context awareness. However, current methods are\nmostly restricted to reconstructing static anatomy assuming no tissue\ndeformation, tool occlusion and de-occlusion, and camera movement. However,\nthese assumptions are not always satisfied in minimal invasive robotic\nsurgeries. In this work, we present an efficient reconstruction pipeline for\nhighly dynamic surgical scenes that runs at 28 fps. Specifically, we design a\ntransformer-based stereoscopic depth perception for efficient depth estimation\nand a light-weight tool segmentor to handle tool occlusion. After that, a\ndynamic reconstruction algorithm which can estimate the tissue deformation and\ncamera movement, and aggregate the information over time is proposed for\nsurgical scene reconstruction. We evaluate the proposed pipeline on two\ndatasets, the public Hamlyn Centre Endoscopic Video Dataset and our in-house\nDaVinci robotic surgery dataset. The results demonstrate that our method can\nrecover the scene obstructed by the surgical tool and handle the movement of\ncamera in realistic surgical scenarios effectively at real-time speed.",
          "link": "http://arxiv.org/abs/2107.00229",
          "publishedOn": "2021-07-02T01:57:59.676Z",
          "wordCount": 649,
          "title": "E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with Transformer-based Stereoscopic Depth Perception. (arXiv:2107.00229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>",
          "description": "Recently, deep hashing with Hamming distance metric has drawn increasing\nattention for face image retrieval tasks. However, its counterpart deep\nquantization methods, which learn binary code representations with\ndictionary-related distance metrics, have seldom been explored for the task.\nThis paper makes the first attempt to integrate product quantization into an\nend-to-end deep learning framework for face image retrieval. Unlike prior deep\nquantization methods where the codewords for quantization are learned from\ndata, we propose a novel scheme using predefined orthonormal vectors as\ncodewords, which aims to enhance the quantization informativeness and reduce\nthe codewords' redundancy. To make the most of the discriminative information,\nwe design a tailored loss function that maximizes the identity discriminability\nin each quantization subspace for both the quantized and the original features.\nFurthermore, an entropy-based regularization term is imposed to reduce the\nquantization error. We conduct experiments on three commonly-used datasets\nunder the settings of both single-domain and cross-domain retrieval. It shows\nthat the proposed method outperforms all the compared deep hashing/quantization\nmethods under both settings with significant superiority. The proposed\ncodewords scheme consistently improves both regular model performance and model\ngeneralization ability, verifying the importance of codewords' distribution for\nthe quantization quality. Besides, our model's better generalization ability\nthan deep hashing models indicates that it is more suitable for scalable face\nimage retrieval tasks.",
          "link": "http://arxiv.org/abs/2107.00327",
          "publishedOn": "2021-07-02T01:57:59.664Z",
          "wordCount": 654,
          "title": "Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>",
          "description": "This work is to tackle the problem of point cloud semantic segmentation for\n3D hybrid scenes under the framework of zero-shot learning. Here by hybrid, we\nmean the scene consists of both seen-class and unseen-class 3D objects, a more\ngeneral and realistic setting in application. To our knowledge, this problem\nhas not been explored in the literature. To this end, we propose a network to\nsynthesize point features for various classes of objects by leveraging the\nsemantic features of both seen and unseen object classes, called PFNet. The\nproposed PFNet employs a GAN architecture to synthesize point features, where\nthe semantic relationship between seen-class and unseen-class features is\nconsolidated by adapting a new semantic regularizer, and the synthesized\nfeatures are used to train a classifier for predicting the labels of the\ntesting 3D scene points. Besides we also introduce two benchmarks for\nalgorithmic evaluation by re-organizing the public S3DIS and ScanNet datasets\nunder six different data splits. Experimental results on the two benchmarks\nvalidate our proposed method, and we hope our introduced two benchmarks and\nmethodology could be of help for more research on this new direction.",
          "link": "http://arxiv.org/abs/2107.00430",
          "publishedOn": "2021-07-02T01:57:59.656Z",
          "wordCount": 623,
          "title": "Segmenting 3D Hybrid Scenes via Zero-Shot Learning. (arXiv:2107.00430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "The speed-accuracy Pareto curve of object detection systems have advanced\nthrough a combination of better model architectures, training and inference\nmethods. In this paper, we methodically evaluate a variety of these techniques\nto understand where most of the improvements in modern detection systems come\nfrom. We benchmark these improvements on the vanilla ResNet-FPN backbone with\nRetinaNet and RCNN detectors. The vanilla detectors are improved by 7.7% in\naccuracy while being 30% faster in speed. We further provide simple scaling\nstrategies to generate family of models that form two Pareto curves, named\nRetinaNet-RS and Cascade RCNN-RS. These simple rescaled detectors explore the\nspeed-accuracy trade-off between the one-stage RetinaNet detectors and\ntwo-stage RCNN detectors. Our largest Cascade RCNN-RS models achieve 52.9% AP\nwith a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Finally,\nwe show the ResNet architecture, with three minor architectural changes,\noutperforms EfficientNet as the backbone for object detection and instance\nsegmentation systems.",
          "link": "http://arxiv.org/abs/2107.00057",
          "publishedOn": "2021-07-02T01:57:59.648Z",
          "wordCount": 597,
          "title": "Simple Training Strategies and Model Scaling for Object Detection. (arXiv:2107.00057v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_L/0/1/0/all/0/1\">Lu Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Few-shot learning (FSL) aims to train a strong classifier using limited\nlabeled examples. Many existing works take the meta-learning approach, sampling\nfew-shot tasks in turn and optimizing the few-shot learner's performance on\nclassifying the query examples. In this paper, we point out two potential\nweaknesses of this approach. First, the sampled query examples may not provide\nsufficient supervision for the few-shot learner. Second, the effectiveness of\nmeta-learning diminishes sharply with increasing shots (i.e., the number of\ntraining examples per class). To resolve these issues, we propose a novel\nobjective to directly train the few-shot learner to perform like a strong\nclassifier. Concretely, we associate each sampled few-shot task with a strong\nclassifier, which is learned with ample labeled examples. The strong classifier\nhas a better generalization ability and we use it to supervise the few-shot\nlearner. We present an efficient way to construct the strong classifier, making\nour proposed objective an easily plug-and-play term to existing meta-learning\nbased FSL methods. We validate our approach in combinations with many\nrepresentative meta-learning methods. On several benchmark datasets including\nminiImageNet and tiredImageNet, our approach leads to a notable improvement\nacross a variety of tasks. More importantly, with our approach, meta-learning\nbased FSL methods can consistently outperform non-meta-learning based ones,\neven in a many-shot setting, greatly strengthening their applicability.",
          "link": "http://arxiv.org/abs/2107.00197",
          "publishedOn": "2021-07-02T01:57:59.628Z",
          "wordCount": 654,
          "title": "Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1\">Kei Uchizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_H/0/1/0/all/0/1\">Haruki Abe</a>",
          "description": "We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n\nWe first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.",
          "link": "http://arxiv.org/abs/2107.00223",
          "publishedOn": "2021-07-02T01:57:59.570Z",
          "wordCount": 752,
          "title": "Circuit Complexity of Visual Search. (arXiv:2107.00223v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jobarteh_M/0/1/0/all/0/1\">Modou L. Jobarteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranowski_T/0/1/0/all/0/1\">Tom Baranowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_Asiedu_M/0/1/0/all/0/1\">Matilda Steiner-Asiedu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Alex K. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrory_M/0/1/0/all/0/1\">Megan A McCrory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sazonov_E/0/1/0/all/0/1\">Edward Sazonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_G/0/1/0/all/0/1\">Gary Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Camera-based passive dietary intake monitoring is able to continuously\ncapture the eating episodes of a subject, recording rich visual information,\nsuch as the type and volume of food being consumed, as well as the eating\nbehaviours of the subject. However, there currently is no method that is able\nto incorporate these visual clues and provide a comprehensive context of\ndietary intake from passive recording (e.g., is the subject sharing food with\nothers, what food the subject is eating, and how much food is left in the\nbowl). On the other hand, privacy is a major concern while egocentric wearable\ncameras are used for capturing. In this paper, we propose a privacy-preserved\nsecure solution (i.e., egocentric image captioning) for dietary assessment with\npassive monitoring, which unifies food recognition, volume estimation, and\nscene understanding. By converting images into rich text descriptions,\nnutritionists can assess individual dietary intake based on the captions\ninstead of the original images, reducing the risk of privacy leakage from\nimages. To this end, an egocentric dietary image captioning dataset has been\nbuilt, which consists of in-the-wild images captured by head-worn and\nchest-worn cameras in field studies in Ghana. A novel transformer-based\narchitecture is designed to caption egocentric dietary images. Comprehensive\nexperiments have been conducted to evaluate the effectiveness and to justify\nthe design of the proposed architecture for egocentric dietary image\ncaptioning. To the best of our knowledge, this is the first work that applies\nimage captioning to dietary intake assessment in real life settings.",
          "link": "http://arxiv.org/abs/2107.00372",
          "publishedOn": "2021-07-02T01:57:59.561Z",
          "wordCount": 706,
          "title": "Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring. (arXiv:2107.00372v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, traditional learning over patch-wise features using\nconvolutional neural networks limits the model when attempting to capture\nglobal contextual information. The phenotypical and topological distribution of\nconstituent histological entities play a critical role in tissue diagnosis. As\nsuch, graph data representations and deep learning have attracted significant\nattention for encoding tissue representations, and capturing intra- and inter-\nentity level interactions. In this review, we provide a conceptual grounding of\ngraph-based deep learning and discuss its current success for tumor\nlocalization and classification, tumor invasion and staging, image retrieval,\nand survival prediction. We provide an overview of these methods in a\nsystematic manner organized by the graph representation of the input image\nincluding whole slide images and tissue microarrays. We also outline the\nlimitations of existing techniques, and suggest potential future advances in\nthis domain.",
          "link": "http://arxiv.org/abs/2107.00272",
          "publishedOn": "2021-07-02T01:57:59.542Z",
          "wordCount": 617,
          "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ankit Singh</a>",
          "description": "Unsupervised Domain Adaptation (UDA) aims to align the labeled source\ndistribution with the unlabeled target distribution to obtain domain invariant\npredictive models. However, the application of well-known UDA approaches does\nnot generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where\nfew labeled samples from the target domain are available. In this paper, we\npropose a simple Contrastive Learning framework for semi-supervised Domain\nAdaptation (CLDA) that attempts to bridge the intra-domain gap between the\nlabeled and unlabeled target distributions and inter-domain gap between source\nand unlabeled target distribution in SSDA. We suggest employing class-wise\ncontrastive learning to reduce the inter-domain gap and instance-level\ncontrastive alignment between the original (input image) and strongly augmented\nunlabeled target images to minimize the intra-domain discrepancy. We have shown\nempirically that both of these modules complement each other to achieve\nsuperior performance. Experiments on three well-known domain adaptation\nbenchmark datasets namely DomainNet, Office-Home, and Office31 demonstrate the\neffectiveness of our approach. CLDA achieves state-of-the-art results on all\nthe above datasets.",
          "link": "http://arxiv.org/abs/2107.00085",
          "publishedOn": "2021-07-02T01:57:59.534Z",
          "wordCount": 591,
          "title": "CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation. (arXiv:2107.00085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasmal_P/0/1/0/all/0/1\">Pradipta Sasmal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Avinash Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuyan_M/0/1/0/all/0/1\">M.K. Bhuyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwahori_Y/0/1/0/all/0/1\">Yuji Iwahori</a>",
          "description": "A deep learning-based monocular depth estimation (MDE) technique is proposed\nfor selection of most informative frames (key frames) of an endoscopic video.\nIn most of the cases, ground truth depth maps of polyps are not readily\navailable and that is why the transfer learning approach is adopted in our\nmethod. An endoscopic modalities generally capture thousands of frames. In this\nscenario, it is quite important to discard low-quality and clinically\nirrelevant frames of an endoscopic video while the most informative frames\nshould be retained for clinical diagnosis. In this view, a key-frame selection\nstrategy is proposed by utilizing the depth information of polyps. In our\nmethod, image moment, edge magnitude, and key-points are considered for\nadaptively selecting the key frames. One important application of our proposed\nmethod could be the 3D reconstruction of polyps with the help of extracted key\nframes. Also, polyps are localized with the help of extracted depth maps.",
          "link": "http://arxiv.org/abs/2107.00005",
          "publishedOn": "2021-07-02T01:57:59.518Z",
          "wordCount": 591,
          "title": "Extraction of Key-frames of Endoscopic Videos by using Depth Information. (arXiv:2107.00005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lakshminarayanan_V/0/1/0/all/0/1\">Vasudevan Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kherdfallah_H/0/1/0/all/0/1\">Hoda Kherdfallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkar_A/0/1/0/all/0/1\">Arya Sarkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balaji_J/0/1/0/all/0/1\">J. Jothi Balaji</a>",
          "description": "Diabetic Retinopathy (DR) is a leading cause of vision loss in the world,. In\nthe past few Diabetic Retinopathy (DR) is a leading cause of vision loss in the\nworld. In the past few years, Artificial Intelligence (AI) based approaches\nhave been used to detect and grade DR. Early detection enables appropriate\ntreatment and thus prevents vision loss, Both fundus and optical coherence\ntomography (OCT) images are used to image the retina. With deep\nlearning/machine learning apprroaches it is possible to extract features from\nthe images and detect the presence of DR. Multiple strategies are implemented\nto detect and grade the presence of DR using classification, segmentation, and\nhybrid techniques. This review covers the literature dealing with AI approaches\nto DR that have been published in the open literature over a five year span\n(2016-2021). In addition a comprehensive list of available DR datasets is\nreported. Both the PICO (P-patient, I-intervention, C-control O-outcome) and\nPreferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA)2009\nsearch strategies were employed. We summarize a total of 114 published articles\nwhich conformed to the scope of the review. In addition a list of 43 major\ndatasets is presented.",
          "link": "http://arxiv.org/abs/2107.00115",
          "publishedOn": "2021-07-02T01:57:59.484Z",
          "wordCount": 660,
          "title": "Automated Detection and Diagnosis of Diabetic Retinopathy: A Comprehensive Survey. (arXiv:2107.00115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1\">Qiong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiahua Xia</a>",
          "description": "Visual localization is one of the most important components for robotics and\nautonomous driving. Recently, inspiring results have been shown with CNN-based\nmethods which provide a direct formulation to end-to-end regress 6-DoF absolute\npose. Additional information like geometric or semantic constraints is\ngenerally introduced to improve performance. Especially, the latter can\naggregate high-level semantic information into localization task, but it\nusually requires enormous manual annotations. To this end, we propose a novel\nauxiliary learning strategy for camera localization by introducing\nscene-specific high-level semantics from self-supervised representation\nlearning task. Viewed as a powerful proxy task, image colorization task is\nchosen as complementary task that outputs pixel-wise color version of grayscale\nphotograph without extra annotations. In our work, feature representations from\ncolorization network are embedded into localization network by design to\nproduce discriminative features for pose regression. Meanwhile an attention\nmechanism is introduced for the benefit of localization performance. Extensive\nexperiments show that our model significantly improve localization accuracy\nover state-of-the-arts on both indoor and outdoor datasets.",
          "link": "http://arxiv.org/abs/2107.00222",
          "publishedOn": "2021-07-02T01:57:59.435Z",
          "wordCount": 606,
          "title": "Deep auxiliary learning for visual localization using colorization task. (arXiv:2107.00222v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1\">Takato Yasuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_J/0/1/0/all/0/1\">Junichiro Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukami_S/0/1/0/all/0/1\">Sakura Fukami</a>",
          "description": "For steel product manufacturing in indoor factories, steel defect detection\nis important for quality control. For example, a steel sheet is extremely\ndelicate, and must be accurately inspected. However, to maintain the painted\nsteel parts of the infrastructure around a severe outdoor environment,\ncorrosion detection is critical for predictive maintenance. In this paper, we\npropose a general-purpose application for steel anomaly detection that consists\nof the following four components. The first, a learner, is a unit image\nclassification network to determine whether the region of interest or\nbackground has been recognised, after dividing the original large sized image\ninto 256 square unit images. The second, an extractor, is a discriminator\nfeature encoder based on a pre-trained steel generator with a patch generative\nadversarial network discriminator(GAN). The third, an anomaly detector, is a\none-class support vector machine(SVM) to predict the anomaly score using the\ndiscriminator feature. The fourth, an indicator, is an anomalous probability\nmap used to visually explain the anomalous features. Furthermore, we\ndemonstrated our method through the inspection of steel sheet defects with\n13,774 unit images using high-speed cameras, and painted steel corrosion with\n19,766 unit images based on an eye inspection of the photographs. Finally, we\nvisualise anomalous feature maps of steel using a strip and painted steel\ninspection dataset",
          "link": "http://arxiv.org/abs/2107.00143",
          "publishedOn": "2021-07-02T01:57:59.427Z",
          "wordCount": 671,
          "title": "One-class Steel Detector Using Patch GAN Discriminator for Visualising Anomalous Feature Map. (arXiv:2107.00143v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "At the heart of all automated driving systems is the ability to sense the\nsurroundings, e.g., through semantic segmentation of LiDAR sequences, which\nexperienced a remarkable progress due to the release of large datasets such as\nSemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse\nsegmentation of the LiDAR input, dense output masks provide self-driving cars\nwith almost complete environment information. In this paper, we introduce MASS\n- a Multi-Attentional Semantic Segmentation model specifically built for dense\ntop-view understanding of the driving scenes. Our framework operates on pillar-\nand occupancy features and comprises three attention-based building blocks: (1)\na keypoint-driven graph attention, (2) an LSTM-based attention computed from a\nvector embedding of the spatial input, and (3) a pillar-based attention,\nresulting in a dense 360-degree segmentation mask. With extensive experiments\non both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the\neffectiveness of our model, outperforming the state of the art by 19.0% on\nSemanticKITTI and reaching 32.7% in mIoU on nuScenes-LidarSeg, where MASS is\nthe first work addressing the dense segmentation task. Furthermore, our\nmulti-attention model is shown to be very effective for 3D object detection\nvalidated on the KITTI-3D dataset, showcasing its high generalizability to\nother tasks related to 3D vision.",
          "link": "http://arxiv.org/abs/2107.00346",
          "publishedOn": "2021-07-02T01:57:59.418Z",
          "wordCount": 674,
          "title": "MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding. (arXiv:2107.00346v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>",
          "description": "Deep learning-based models, such as recurrent neural networks (RNNs), have\nbeen applied to various sequence learning tasks with great success. Following\nthis, these models are increasingly replacing classic approaches in object\ntracking applications for motion prediction. On the one hand, these models can\ncapture complex object dynamics with less modeling required, but on the other\nhand, they depend on a large amount of training data for parameter tuning.\nTowards this end, we present an approach for generating synthetic trajectory\ndata of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather\nquadrotors are dynamical systems, they can not follow arbitrary trajectories.\nWith the prerequisite that UAV trajectories fulfill a smoothness criterion\ncorresponding to a minimal change of higher-order motion, methods for planning\naggressive quadrotors flights can be utilized to generate optimal trajectories\nthrough a sequence of 3D waypoints. By projecting these maneuver trajectories,\nwhich are suitable for controlling quadrotors, to image space, a versatile\ntrajectory data set is realized. To demonstrate the applicability of the\nsynthetic trajectory data, we show that an RNN-based prediction model solely\ntrained on the generated data can outperform classic reference models on a\nreal-world UAV tracking dataset. The evaluation is done on the publicly\navailable ANTI-UAV dataset.",
          "link": "http://arxiv.org/abs/2107.00422",
          "publishedOn": "2021-07-02T01:57:59.353Z",
          "wordCount": 644,
          "title": "Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction. (arXiv:2107.00422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:34.067Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.",
          "link": "http://arxiv.org/abs/2106.15765",
          "publishedOn": "2021-07-01T01:59:34.045Z",
          "wordCount": 666,
          "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1\">Rauf Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1\">Ali Shan Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khalid Rasheed</a>",
          "description": "Facial expressions are the most universal forms of body language and\nautomatic facial expression recognition is one of the challenging tasks due to\ndifferent uncertainties. However, it has been an active field of research for\nmany years. Nevertheless, efficiency and performance are yet essential aspects\nfor building robust systems. We proposed two models, EmoXNet which is an\nensemble learning technique for learning convoluted facial representations, and\nEmoXNetLite which is a distillation technique that is useful for transferring\nthe knowledge from our ensemble model to an efficient deep neural network using\nlabel-smoothen soft labels for able to effectively detect expressions in\nreal-time. Both of the techniques performed quite well, where the ensemble\nmodel (EmoXNet) helped to achieve 85.07% test accuracy on FER2013 with FER+\nannotations and 86.25% test accuracy on RAF-DB. Moreover, the distilled model\n(EmoXNetLite) showed 82.07% test accuracy on FER2013 with FER+ annotations and\n81.78% test accuracy on RAF-DB.",
          "link": "http://arxiv.org/abs/2106.16126",
          "publishedOn": "2021-07-01T01:59:33.706Z",
          "wordCount": 603,
          "title": "Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1\">Aneesh Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "Acquiring accurate 3D annotated data for hand pose estimation is a\nnotoriously difficult problem. This typically requires complex multi-camera\nsetups and controlled conditions, which in turn creates a domain gap that is\nhard to bridge to fully unconstrained settings. Encouraged by the success of\ncontrastive learning on image classification tasks, we propose a new\nself-supervised method for the structured regression task of 3D hand pose\nestimation. Contrastive learning makes use of unlabeled data for the purpose of\nrepresentation learning via a loss formulation that encourages the learned\nfeature representations to be invariant under any image transformation. For 3D\nhand pose estimation, it too is desirable to have invariance to appearance\ntransformation such as color jitter. However, the task requires equivariance\nunder affine transformations, such as rotation and translation. To address this\nissue, we propose an equivariant contrastive objective and demonstrate its\neffectiveness in the context of 3D hand pose estimation. We experimentally\ninvestigate the impact of invariant and equivariant contrastive objectives and\nshow that learning equivariant features leads to better representations for the\ntask of 3D hand pose estimation. Furthermore, we show that a standard\nResNet-152, trained on additional unlabeled data, attains an improvement of\n$7.6\\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance\nwithout any task specific, specialized architectures.",
          "link": "http://arxiv.org/abs/2106.05953",
          "publishedOn": "2021-07-01T01:59:33.603Z",
          "wordCount": 671,
          "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training under fine-grained scenarios. Based on our findings, we introduce\nCross-view Saliency Alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.",
          "link": "http://arxiv.org/abs/2106.15788",
          "publishedOn": "2021-07-01T01:59:33.560Z",
          "wordCount": 590,
          "title": "Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:33.537Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishi_S/0/1/0/all/0/1\">Shintaro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadota_T/0/1/0/all/0/1\">Takeaki Kadota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "This paper analyzes a large number of logo images from the LLD-logo dataset,\nby recent deep learning-based techniques, to understand not only design trends\nof logo images and but also the correlation to their owner company. Especially,\nwe focus on three correlations between logo images and their text areas,\nbetween the text areas and the number of followers on Twitter, and between the\nlogo images and the number of followers. Various findings include the weak\npositive correlation between the text area ratio and the number of followers of\nthe company. In addition, deep regression and deep ranking methods can catch\ncorrelations between the logo images and the number of followers.",
          "link": "http://arxiv.org/abs/2104.00327",
          "publishedOn": "2021-07-01T01:59:33.518Z",
          "wordCount": 592,
          "title": "Famous Companies Use More Letters in Logo:A Large-Scale Analysis of Text Area in Logo. (arXiv:2104.00327v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.",
          "link": "http://arxiv.org/abs/2106.12407",
          "publishedOn": "2021-07-01T01:59:33.459Z",
          "wordCount": 673,
          "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.376Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.183Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1\">Aite Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "Home-cage social behaviour analysis of mice is an invaluable tool to assess\ntherapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts\nmade within the research community, single-camera video recordings are mainly\nused for such analysis. Because of the potential to create rich descriptions of\nmouse social behaviors, the use of multi-view video recordings for rodent\nobservations is increasingly receiving much attention. However, identifying\nsocial behaviours from various views is still challenging due to the lack of\ncorrespondence across data sources. To address this problem, we here propose a\nnovel multiview latent-attention and dynamic discriminative model that jointly\nlearns view-specific and view-shared sub-structures, where the former captures\nunique dynamics of each view whilst the latter encodes the interaction between\nthe views. Furthermore, a novel multi-view latent-attention variational\nautoencoder model is introduced in learning the acquired features, enabling us\nto learn discriminative features in each view. Experimental results on the\nstandard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)\ndatasets demonstrate that our model outperforms the other state of the arts\ntechnologies and effectively deals with the imbalanced data problem.",
          "link": "http://arxiv.org/abs/2011.02451",
          "publishedOn": "2021-07-01T01:59:33.160Z",
          "wordCount": 660,
          "title": "Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model. (arXiv:2011.02451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1\">Mauro Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaliq_A/0/1/0/all/0/1\">Aleem Khaliq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "The increasing availability of large-scale remote sensing labeled data has\nprompted researchers to develop increasingly precise and accurate data-driven\nmodels for land cover and crop classification (LC&CC). Moreover, with the\nintroduction of self-attention and introspection mechanisms, deep learning\napproaches have shown promising results in processing long temporal sequences\nin the multi-spectral domain with a contained computational request.\nNevertheless, most practical applications cannot rely on labeled data, and in\nthe field, surveys are a time consuming solution that poses strict limitations\nto the number of collected samples. Moreover, atmospheric conditions and\nspecific geographical region characteristics constitute a relevant domain gap\nthat does not allow direct applicability of a trained model on the available\ndataset to the area of interest. In this paper, we investigate adversarial\ntraining of deep neural networks to bridge the domain discrepancy between\ndistinct geographical zones. In particular, we perform a thorough analysis of\ndomain adaptation applied to challenging multi-spectral, multi-temporal data,\naccurately highlighting the advantages of adapting state-of-the-art\nself-attention based models for LC&CC to different target zones where labeled\ndata are not available. Extensive experimentation demonstrated significant\nperformance and generalization gain in applying domain-adversarial training to\nsource and target regions with marked dissimilarities between the distribution\nof extracted features.",
          "link": "http://arxiv.org/abs/2104.00564",
          "publishedOn": "2021-07-01T01:59:33.154Z",
          "wordCount": 679,
          "title": "Domain-Adversarial Training of Self-Attention Based Networks for Land Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery. (arXiv:2104.00564v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1\">Anuja Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1\">Marius Pedersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_A/0/1/0/all/0/1\">Ahmed Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovde_O/0/1/0/all/0/1\">&#xd8;istein Hovde</a>",
          "description": "The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy\n(WCE) is thwarted by the lack of data. The inadequacy in richly representative\nhealthy and abnormal conditions results in isolated analyses of pathologies,\nthat can not handle realistic multi-pathology scenarios. In this work, we\nexplore how to learn more for free, from limited data through solving a WCE\nmulticentric, multi-pathology classification problem. Learning more implies to\nlearning more than full supervision would allow with the same data. This is\ndone by combining self supervision with full supervision, under multi task\nlearning. Additionally, we draw inspiration from the Human Visual System (HVS)\nin designing self supervision tasks and investigate if seemingly ineffectual\nsignals within the data itself can be exploited to gain performance, if so,\nwhich signals would be better than others. Further, we present our analysis of\nthe high level features as a stepping stone towards more robust multi-pathology\nCADx in WCE.",
          "link": "http://arxiv.org/abs/2106.16162",
          "publishedOn": "2021-07-01T01:59:33.142Z",
          "wordCount": 612,
          "title": "Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy. (arXiv:2106.16162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tautkute_I/0/1/0/all/0/1\">Ivona Tautkute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzcinski</a>",
          "description": "This paper addresses the problem of media retrieval using a multimodal query\n(a query which combines visual input with additional semantic information in\nnatural language feedback). We propose a SynthTriplet GAN framework which\nresolves this task by expanding the multimodal query with a synthetically\ngenerated image that captures semantic information from both image and text\ninput. We introduce a novel triplet mining method that uses a synthetic image\nas an anchor to directly optimize for embedding distances of generated and\ntarget images. We demonstrate that apart from the added value of retrieval\nillustration with synthetic image with the focus on customization and user\nfeedback, the proposed method greatly surpasses other multimodal generation\nmethods and achieves state of the art results in the multimodal retrieval task.\nWe also show that in contrast to other retrieval methods, our method provides\nexplainable embeddings.",
          "link": "http://arxiv.org/abs/2102.08871",
          "publishedOn": "2021-07-01T01:59:33.136Z",
          "wordCount": 619,
          "title": "I Want This Product but Different : Multimodal Retrieval with Synthetic Query Expansion. (arXiv:2102.08871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongzi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>",
          "description": "Monocular 3D object detection is an important task in autonomous driving. It\ncan be easily intractable where there exists ego-car pose change w.r.t. ground\nplane. This is common due to the slight fluctuation of road smoothness and\nslope. Due to the lack of insight in industrial application, existing methods\non open datasets neglect the camera pose information, which inevitably results\nin the detector being susceptible to camera extrinsic parameters. The\nperturbation of objects is very popular in most autonomous driving cases for\nindustrial products. To this end, we propose a novel method to capture camera\npose to formulate the detector free from extrinsic perturbation. Specifically,\nthe proposed framework predicts camera extrinsic parameters by detecting\nvanishing point and horizon change. A converter is designed to rectify\nperturbative features in the latent space. By doing so, our 3D detector works\nindependent of the extrinsic parameter variations and produces accurate results\nin realistic cases, e.g., potholed and uneven roads, where almost all existing\nmonocular detectors fail to handle. Experiments demonstrate our method yields\nthe best performance compared with the other state-of-the-arts by a large\nmargin on both KITTI 3D and nuScenes datasets.",
          "link": "http://arxiv.org/abs/2106.15796",
          "publishedOn": "2021-07-01T01:59:33.129Z",
          "wordCount": 631,
          "title": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach. (arXiv:2106.15796v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>",
          "description": "Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn\na model from several labeled source domains while performing well on a\ndifferent target domain where only unlabeled data are available at training\ntime. To align source and target features distributions, several recent works\nuse source and target explicit statistics matching such as features moments or\nclass centroids. Yet, these approaches do not guarantee class conditional\ndistributions alignment across domains. In this work, we propose a new\nframework called Contrastive Multi-Source Domain Adaptation (CMSDA) for\nmulti-source UDA that addresses this limitation. Discriminative features are\nlearned from interpolated source examples via cross entropy minimization and\nfrom target examples via consistency regularization and hard pseudo-labeling.\nSimultaneously, interpolated source examples are leveraged to align source\nclass conditional distributions through an interpolated version of the\nsupervised contrastive loss. This alignment leads to more general and\ntransferable features which further improve the generalization on the target\ndomain. Extensive experiments have been carried out on three standard\nmulti-source UDA datasets where our method reports state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.16093",
          "publishedOn": "2021-07-01T01:59:33.106Z",
          "wordCount": 609,
          "title": "Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15944",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1\">Runmu Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nie_Y/0/1/0/all/0/1\">Yunfeng Nie</a>",
          "description": "Hyperspectral imaging enables versatile applications due to its competence in\ncapturing abundant spatial and spectral information, which are crucial for\nidentifying substances. However, the devices for acquiring hyperspectral images\nare expensive and complicated. Therefore, many alternative spectral imaging\nmethods have been proposed by directly reconstructing the hyperspectral\ninformation from lower-cost, more available RGB images. We present a thorough\ninvestigation of these state-of-the-art spectral reconstruction methods from\nthe widespread RGB images. A systematic study and comparison of more than 25\nmethods has revealed that most of the data-driven deep learning methods are\nsuperior to prior-based methods in terms of reconstruction accuracy and quality\ndespite lower speeds. This comprehensive review can serve as a fruitful\nreference source for peer researchers, thus further inspiring future\ndevelopment directions in related domains.",
          "link": "http://arxiv.org/abs/2106.15944",
          "publishedOn": "2021-07-01T01:59:33.073Z",
          "wordCount": 580,
          "title": "Learnable Reconstruction Methods from RGB Images to Hyperspectral Imaging: A Survey. (arXiv:2106.15944v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shubao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Face anti-spoofing approaches based on domain generalization (DG) have drawn\ngrowing attention due to their robustness for unseen scenarios. Previous\nmethods treat each sample from multiple domains indiscriminately during the\ntraining process, and endeavor to extract a common feature space to improve the\ngeneralization. However, due to complex and biased data distribution, directly\ntreating them equally will corrupt the generalization ability. To settle the\nissue, we propose a novel Dual Reweighting Domain Generalization (DRDG)\nframework which iteratively reweights the relative importance between samples\nto further improve the generalization. Concretely, Sample Reweighting Module is\nfirst proposed to identify samples with relatively large domain bias, and\nreduce their impact on the overall optimization. Afterwards, Feature\nReweighting Module is introduced to focus on these samples and extract more\ndomain-irrelevant features via a self-distilling mechanism. Combined with the\ndomain discriminator, the iteration of the two modules promotes the extraction\nof generalized features. Extensive experiments and visualizations are presented\nto demonstrate the effectiveness and interpretability of our method against the\nstate-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2106.16128",
          "publishedOn": "2021-07-01T01:59:33.066Z",
          "wordCount": 620,
          "title": "Dual Reweighting Domain Generalization for Face Presentation Attack Detection. (arXiv:2106.16128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-01T01:59:33.054Z",
          "wordCount": 556,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15753",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.",
          "link": "http://arxiv.org/abs/2106.15753",
          "publishedOn": "2021-07-01T01:59:33.047Z",
          "wordCount": 657,
          "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Diaz_N/0/1/0/all/0/1\">Nuria Rodriguez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspandi_D/0/1/0/all/0/1\">Decky Aspandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1\">Federico Sukno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binefa_X/0/1/0/all/0/1\">Xavier Binefa</a>",
          "description": "Lie detection is considered a concern for everyone in their day to day life\ngiven its impact on human interactions. Thus, people normally pay attention to\nboth what their interlocutors are saying and also to their visual appearances,\nincluding faces, to try to find any signs that indicate whether the person is\ntelling the truth or not. While automatic lie detection may help us to\nunderstand this lying characteristics, current systems are still fairly\nlimited, partly due to lack of adequate datasets to evaluate their performance\nin realistic scenarios. In this work, we have collected an annotated dataset of\nfacial images, comprising both 2D and 3D information of several participants\nduring a card game that encourages players to lie. Using our collected dataset,\nWe evaluated several types of machine learning-based lie detectors in terms of\ntheir generalization, person-specific and cross-domain experiments. Our results\nshow that models based on deep learning achieve the best accuracy, reaching up\nto 57\\% for the generalization task and 63\\% when dealing with a single\nparticipant. Finally, we also highlight the limitation of the deep learning\nbased lie detector when dealing with cross-domain lie detection tasks.",
          "link": "http://arxiv.org/abs/2104.12345",
          "publishedOn": "2021-07-01T01:59:33.040Z",
          "wordCount": 661,
          "title": "Machine Learning-based Lie Detector applied to a Novel Annotated Game Dataset. (arXiv:2104.12345v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:32.988Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:32.909Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:32.900Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wider-Context Network (WiCNet) for the semantic segmentation of HR\nRSIs. In the WiCNet, apart from a conventional feature extraction network to\naggregate the local information, an extra context branch is designed to\nexplicitly model the context information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. Additionally, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which we hope will boost future studies in\nthis field.",
          "link": "http://arxiv.org/abs/2106.15754",
          "publishedOn": "2021-07-01T01:59:32.830Z",
          "wordCount": 621,
          "title": "Looking Outside the Window: Wider-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:32.811Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-01T01:59:32.694Z",
          "wordCount": 639,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:32.668Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>",
          "description": "Semi-Supervised Learning (SSL) can decrease the amount of required labeled\nimage data and thus the cost for deep learning. Most SSL methods only consider\na clear distinction between classes but in many real-world datasets, this clear\ndistinction is not given due to intra- or interobserver variability. This\nvariability can lead to different annotations per image. Thus many images have\nambiguous annotations and their label needs to be considered \"fuzzy\". This\nfuzziness of labels must be addressed as it will limit the performance of\nSemi-Supervised Learning (SSL) and deep learning in general. We propose\nSemi-Supervised Classification & Clustering (S2C2) which can extend many deep\nSSL algorithms. S2C2 can estimate the fuzziness of a label and applies SSL as a\nclassification to certainly labeled data while creating distinct clusters for\nimages with similar but fuzzy labels. We show that S2C2 results in median 7.4%\nbetter F1-score for classifications and 5.4% lower inner distance of clusters\nacross multiple SSL algorithms and datasets while being more interpretable due\nto the fuzziness estimation of our method. Overall, a combination of\nSemi-Supervised Learning with our method S2C2 leads to better handling of the\nfuzziness of labels and thus real-world datasets.",
          "link": "http://arxiv.org/abs/2106.16209",
          "publishedOn": "2021-07-01T01:59:32.652Z",
          "wordCount": 644,
          "title": "S2C2 - An orthogonal method for Semi-Supervised Learning on fuzzy labels. (arXiv:2106.16209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1\">Ali Anil Demircali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1\">Berk Dilsizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>",
          "description": "Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.",
          "link": "http://arxiv.org/abs/2106.16139",
          "publishedOn": "2021-07-01T01:59:32.643Z",
          "wordCount": 649,
          "title": "Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Temporal language grounding (TLG) is a fundamental and challenging problem\nfor vision and language understanding. Existing methods mainly focus on fully\nsupervised setting with temporal boundary labels for training, which, however,\nsuffers expensive cost of annotation. In this work, we are dedicated to weakly\nsupervised TLG, where multiple description sentences are given to an untrimmed\nvideo without temporal boundary labels. In this task, it is critical to learn a\nstrong cross-modal semantic alignment between sentence semantics and visual\ncontent. To this end, we introduce a novel weakly supervised temporal adjacent\nnetwork (WSTAN) for temporal language grounding. Specifically, WSTAN learns\ncross-modal semantic alignment by exploiting temporal adjacent network in a\nmultiple instance learning (MIL) paradigm, with a whole description paragraph\nas input. Moreover, we integrate a complementary branch into the framework,\nwhich explicitly refines the predictions with pseudo supervision from the MIL\nstage. An additional self-discriminating loss is devised on both the MIL branch\nand the complementary branch, aiming to enhance semantic discrimination by\nself-supervising. Extensive experiments are conducted on three widely used\nbenchmark datasets, \\emph{i.e.}, ActivityNet-Captions, Charades-STA, and\nDiDeMo, and the results demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.16136",
          "publishedOn": "2021-07-01T01:59:32.625Z",
          "wordCount": 632,
          "title": "Weakly Supervised Temporal Adjacent Network for Language Grounding. (arXiv:2106.16136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shihao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "With the rapid development of social media, tremendous videos with new\nclasses are generated daily, which raise an urgent demand for video\nclassification methods that can continuously update new classes while\nmaintaining the knowledge of old videos with limited storage and computing\nresources. In this paper, we summarize this task as \\textit{Class-Incremental\nVideo Classification (CIVC)} and propose a novel framework to address it. As a\nsubarea of incremental learning tasks, the challenge of \\textit{catastrophic\nforgetting} is unavoidable in CIVC. To better alleviate it, we utilize some\ncharacteristics of videos. First, we decompose the spatio-temporal knowledge\nbefore distillation rather than treating it as a whole in the knowledge\ntransfer process; trajectory is also used to refine the decomposition. Second,\nwe propose a dual granularity exemplar selection method to select and store\nrepresentative video instances of old classes and key-frames inside videos\nunder a tight storage budget. We benchmark our method and previous SOTA\nclass-incremental learning methods on Something-Something V2 and Kinetics\ndatasets, and our method outperforms previous methods significantly.",
          "link": "http://arxiv.org/abs/2106.15827",
          "publishedOn": "2021-07-01T01:59:32.619Z",
          "wordCount": 602,
          "title": "When Video Classification Meets Incremental Classes. (arXiv:2106.15827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xingxing Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_N/0/1/0/all/0/1\">Nathaniel Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoquan Huang</a>",
          "description": "In this work, we present a lightweight, tightly-coupled deep depth network\nand visual-inertial odometry (VIO) system, which can provide accurate state\nestimates and dense depth maps of the immediate surroundings. Leveraging the\nproposed lightweight Conditional Variational Autoencoder (CVAE) for depth\ninference and encoding, we provide the network with previously marginalized\nsparse features from VIO to increase the accuracy of initial depth prediction\nand generalization capability. The compact encoded depth maps are then updated\njointly with navigation states in a sliding window estimator in order to\nprovide the dense local scene geometry. We additionally propose a novel method\nto obtain the CVAE's Jacobian which is shown to be more than an order of\nmagnitude faster than previous works, and we additionally leverage\nFirst-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous\nworks relying on completely dense residuals, we propose to only provide sparse\nmeasurements to update the depth code and show through careful experimentation\nthat our choice of sparse measurements and FEJs can still significantly improve\nthe estimated depth maps. Our full system also exhibits state-of-the-art pose\nestimation accuracy, and we show that it can run in real-time with\nsingle-thread execution while utilizing GPU acceleration only for the network\nand code Jacobian.",
          "link": "http://arxiv.org/abs/2012.10133",
          "publishedOn": "2021-07-01T01:59:32.614Z",
          "wordCount": 675,
          "title": "CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth. (arXiv:2012.10133v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia L.Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_S/0/1/0/all/0/1\">Stanley F. Goes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "Resolution in deep convolutional neural networks (CNNs) is typically bounded\nby the receptive field size through filter sizes, and subsampling layers or\nstrided convolutions on feature maps. The optimal resolution may vary\nsignificantly depending on the dataset. Modern CNNs hard-code their resolution\nhyper-parameters in the network architecture which makes tuning such\nhyper-parameters cumbersome. We propose to do away with hard-coded resolution\nhyper-parameters and aim to learn the appropriate resolution from data. We use\nscale-space theory to obtain a self-similar parametrization of filters and make\nuse of the N-Jet: a truncated Taylor series to approximate a filter by a\nlearned combination of Gaussian derivative filters. The parameter sigma of the\nGaussian basis controls both the amount of detail the filter encodes and the\nspatial extent of the filter. Since sigma is a continuous parameter, we can\noptimize it with respect to the loss. The proposed N-Jet layer achieves\ncomparable performance when used in state-of-the art architectures, while\nlearning the correct resolution in each layer automatically. We evaluate our\nN-Jet layer on both classification and segmentation, and we show that learning\nsigma is especially beneficial for inputs at multiple sizes.",
          "link": "http://arxiv.org/abs/2106.03412",
          "publishedOn": "2021-07-01T01:59:32.606Z",
          "wordCount": 647,
          "title": "Resolution learning in deep convolutional networks using scale-space theory. (arXiv:2106.03412v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ye Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>",
          "description": "Real-time video deblurring still remains a challenging task due to the\ncomplexity of spatially and temporally varying blur itself and the requirement\nof low computational cost. To improve the network efficiency, we adopt residual\ndense blocks into RNN cells, so as to efficiently extract the spatial features\nof the current frame. Furthermore, a global spatio-temporal attention module is\nproposed to fuse the effective hierarchical features from past and future\nframes to help better deblur the current frame. Another issue needs to be\naddressed urgently is the lack of a real-world benchmark dataset. Thus, we\ncontribute a novel dataset (BSD) to the community, by collecting paired\nblurry/sharp video clips using a co-axis beam splitter acquisition system.\nExperimental results show that the proposed method (ESTRNN) can achieve better\ndeblurring performance both quantitatively and qualitatively with less\ncomputational cost against state-of-the-art video deblurring methods. In\naddition, cross-validation experiments between datasets illustrate the high\ngenerality of BSD over the synthetic datasets. The code and dataset are\nreleased at https://github.com/zzh-tech/ESTRNN.",
          "link": "http://arxiv.org/abs/2106.16028",
          "publishedOn": "2021-07-01T01:59:32.594Z",
          "wordCount": 615,
          "title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. (arXiv:2106.16028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoian_M/0/1/0/all/0/1\">Mihaela C&#x103;t&#x103;lina Stoian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_T/0/1/0/all/0/1\">Tommaso Cavallari</a>",
          "description": "Many man-made objects are characterised by a shape that is symmetric along\none or more planar directions. Estimating the location and orientation of such\nsymmetry planes can aid many tasks such as estimating the overall orientation\nof an object of interest or performing shape completion, where a partial scan\nof an object is reflected across the estimated symmetry plane in order to\nobtain a more detailed shape. Many methods processing 3D data rely on expensive\n3D convolutions. In this paper we present an alternative novel encoding that\ninstead slices the data along the height dimension and passes it sequentially\nto a 2D convolutional recurrent regression scheme. The method also comprises a\ndifferentiable least squares step, allowing for end-to-end accurate and fast\nprocessing of both full and partial scans of symmetric objects. We use this\napproach to efficiently handle 3D inputs to design a method to estimate planar\nreflective symmetries. We show that our approach has an accuracy comparable to\nstate-of-the-art techniques on the task of planar reflective symmetry\nestimation on full synthetic objects. Additionally, we show that it can be\ndeployed on partial scans of objects in a real-world pipeline to improve the\noutputs of a 3D object detector.",
          "link": "http://arxiv.org/abs/2106.16129",
          "publishedOn": "2021-07-01T01:59:32.570Z",
          "wordCount": 644,
          "title": "Recurrently Estimating Reflective Symmetry Planes from Partial Pointclouds. (arXiv:2106.16129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.560Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1\">Tiago de C. G. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>",
          "description": "In the world where big data reigns and there is plenty of hardware prepared\nto gather a huge amount of non structured data, data acquisition is no longer a\nproblem. Surveillance cameras are ubiquitous and they capture huge numbers of\npeople walking across different scenes. However, extracting value from this\ndata is challenging, specially for tasks that involve human images, such as\nface recognition and person re-identification. Annotation of this kind of data\nis a challenging and expensive task. In this work we propose a domain\nadaptation workflow to allow CNNs that were trained in one domain to be applied\nto another domain without the need for new annotation of the target data. Our\nmethod uses AlignedReID++ as the baseline, trained using a Triplet loss with\nbatch hard. Domain adaptation is done by using pseudo-labels generated using an\nunsupervised learning strategy. Our results show that domain adaptation\ntechniques really improve the performance of the CNN when applied in the target\ndomain.",
          "link": "http://arxiv.org/abs/2106.15693",
          "publishedOn": "2021-07-01T01:59:32.530Z",
          "wordCount": 633,
          "title": "Domain adaptation for person re-identification on new unlabeled data using AlignedReID++. (arXiv:2106.15693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:32.512Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1\">Bernadette Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>",
          "description": "We consider the problem of object goal navigation in unseen environments. In\nour view, solving this problem requires learning of contextual semantic priors,\na challenging endeavour given the spatial and semantic variability of indoor\nenvironments. Current methods learn to implicitly encode these priors through\ngoal-oriented navigation policy functions operating on spatial representations\nthat are limited to the agent's observable areas. In this work, we propose a\nnovel framework that actively learns to generate semantic maps outside the\nfield of view of the agent and leverages the uncertainty over the semantic\nclasses in the unobserved areas to decide on long term goals. We demonstrate\nthat through this spatial prediction strategy, we are able to learn semantic\npriors in scenes that can be leveraged in unknown environments. Additionally,\nwe show how different objectives can be defined by balancing exploration with\nexploitation during searching for semantic targets. Our method is validated in\nthe visually realistic environments offered by the Matterport3D dataset and\nshow state of the art results on the object goal navigation task.",
          "link": "http://arxiv.org/abs/2106.15648",
          "publishedOn": "2021-07-01T01:59:32.462Z",
          "wordCount": 612,
          "title": "Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:32.418Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:32.401Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
          "link": "http://arxiv.org/abs/2106.08049",
          "publishedOn": "2021-07-01T01:59:32.394Z",
          "wordCount": 646,
          "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:32.387Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:32.375Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>",
          "description": "Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.",
          "link": "http://arxiv.org/abs/2104.05575",
          "publishedOn": "2021-07-01T01:59:32.355Z",
          "wordCount": 704,
          "title": "GAttANet: Global attention agreement for convolutional neural networks. (arXiv:2104.05575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Last ten years have witnessed the growth of many computer vision applications\nfor food recognition. Dietary studies showed that dietary-related problem such\nas obesity is associated with other chronic diseases like hypertension,\nirregular blood sugar levels, and increased risk of heart attacks. The primary\ncause of these problems is poor lifestyle choices and unhealthy dietary habits,\nwhich are manageable by using interactive mHealth apps that use automatic\nvisual-based methods to assess dietary intake. This review discusses the most\nperforming methodologies that have been developed so far for automatic food\nrecognition. First, we will present the rationale of visual-based methods for\nfood recognition. The core of the paper is the presentation, discussion and\nevaluation of these methods on popular food image databases. We also discussed\nthe mobile applications that are implementing these methods. The review ends\nwith a discussion of research gaps and future challenges in this area.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-07-01T01:59:32.345Z",
          "wordCount": 605,
          "title": "A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kousha_S/0/1/0/all/0/1\">Shayan Kousha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>",
          "description": "The purpose of generative Zero-shot learning (ZSL) is to learning from seen\nclasses, transfer the learned knowledge, and create samples of unseen classes\nfrom the description of these unseen categories. To achieve better ZSL\naccuracies, models need to better understand the descriptions of unseen\nclasses. We introduce a novel form of regularization that encourages generative\nZSL models to pay more attention to the description of each category. Our\nempirical results demonstrate improvements over the performance of multiple\nstate-of-the-art models on the task of generalized zero-shot recognition and\nclassification when trained on textual description-based datasets like CUB and\nNABirds and attribute-based datasets like AWA2, aPY and SUN.",
          "link": "http://arxiv.org/abs/2106.16108",
          "publishedOn": "2021-07-01T01:59:32.310Z",
          "wordCount": 534,
          "title": "Zero-shot Learning with Class Description Regularization. (arXiv:2106.16108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:32.295Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:32.288Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:32.268Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_z/0/1/0/all/0/1\">zhenyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dandan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>",
          "description": "This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to\npursue better compression efficiency beyond the latest Versatile Video Coding\n(VVC), where we encode intra frames at original high resolution (HR), compress\ninter frames at a lower resolution (LR), and then super-resolve decoded LR\ninter frames with the help from preceding HR intra and neighboring LR inter\nframes. For a LR inter frame, a motion alignment and aggregation network (MAN)\nis devised to produce temporally aggregated motion representation (AMR) for the\nguarantee of temporal smoothness; Another texture compensation network (TCN)\ninputs decoded HR intra frame, re-sampled HR intra frame, and this LR inter\nframe to generate multiscale affinity map (MAM) and multiscale texture\nrepresentation (MTR) for better augmenting spatial details; Finally,\nsimilarity-driven fusion synthesizes AMR, MTR, MAM to upscale LR inter frame\nfor the removal of compression and resolution re-sampling noises. We enhance\nthe VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\\o}ntegaard\nDelta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA)\nand Low-delay P (LDP) settings respectively. In addition, experimental\ncomparisons to the state-of-the-art super-resolution (SR) based VVC enhancement\nmethods, and ablation studies are conducted to further report superior\nefficiency and generalization of proposed algorithm. All materials will be made\nto public at https://njuvision.github.io/CRS for reproducible research.",
          "link": "http://arxiv.org/abs/2012.00650",
          "publishedOn": "2021-07-01T01:59:32.263Z",
          "wordCount": 705,
          "title": "Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>",
          "description": "In applications such as object tracking, time-series data inevitably carry\nmissing observations. Following the success of deep learning-based models for\nvarious sequence learning tasks, these models increasingly replace classic\napproaches in object tracking applications for inferring the object motions\nstate. While traditional tracking approaches can deal with missing\nobservations, most of their deep counterparts are, by default, not suited for\nthis.\n\nTowards this end, this paper introduces a transformer-based approach for\nhandling missing observations in variable input length trajectory data. The\nmodel is formed indirectly by successively increasing the complexity of the\ndemanded inference tasks. Starting from reproducing noise-free trajectories,\nthe model then learns to infer trajectories from noisy inputs. By providing\nmissing tokens, binary-encoded missing events, the model learns to in-attend to\nmissing data and infers a complete trajectory conditioned on the remaining\ninputs. In the case of a sequence of successive missing events, the model then\nacts as a pure prediction model. The model's abilities are demonstrated on\nsynthetic data and real-world data reflecting prototypical object tracking\nscenarios.",
          "link": "http://arxiv.org/abs/2106.16009",
          "publishedOn": "2021-07-01T01:59:32.256Z",
          "wordCount": 615,
          "title": "MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction. (arXiv:2106.16009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Convolutional neural network (CNN) have proven its success for semantic\nsegmentation, which is a core task of emerging industrial applications such as\nautonomous driving. However, most progress in semantic segmentation of urban\nscenes is reported on standard scenarios, i.e., daytime scenes with favorable\nillumination conditions. In practical applications, the outdoor weather and\nillumination are changeable, e.g., cloudy and nighttime, which results in a\nsignificant drop of semantic segmentation accuracy of CNN only trained with\ndaytime data. In this paper, we propose a novel generative adversarial network\n(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained\nneural network is applied to videos captured under adverse weather conditions.\nThe proposed Mutual-GAN adopts mutual information constraint to preserve\nimage-objects during cross-weather adaptation, which is an unsolved problem for\nmost unsupervised image-to-image translation approaches (e.g., CycleGAN). The\nproposed Mutual-GAN is evaluated on two publicly available driving video\ndatasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that\nour Mutual-GAN can yield visually plausible translated images and significantly\nimprove the semantic segmentation accuracy of daytime-trained deep learning\nnetwork while processing videos under challenging weathers.",
          "link": "http://arxiv.org/abs/2106.16000",
          "publishedOn": "2021-07-01T01:59:32.248Z",
          "wordCount": 627,
          "title": "Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual Information Constraint. (arXiv:2106.16000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15893",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1\">Serop Baghdadlian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1\">David Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1\">Martin Weidenfeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1\">Susanne Merkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1\">Markus Eckstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>",
          "description": "Whole-slide-image cartography is the process of automatically detecting and\noutlining different tissue types in digitized histological specimen. This\nsemantic segmentation provides a basis for many follow-up analyses and can\npotentially guide subsequent medical decisions. Due to their large size,\nwhole-slide-images typically have to be divided into smaller patches which are\nthen analyzed individually using machine learning-based approaches. Thereby,\nlocal dependencies of image regions get lost and since a whole-slide-image\ncomprises many thousands of such patches this process is inherently slow. We\npropose to subdivide the image into coherent regions prior to classification by\ngrouping visually similar adjacent image pixels into larger segments, i.e.\nsuperpixels. Afterwards, only a random subset of patches per superpixel is\nclassified and patch labels are combined into a single superpixel label. The\nalgorithm has been developed and validated on a dataset of 159 hand-annotated\nwhole-slide-images of colon resections and its performance has been compared to\na standard patch-based approach. The algorithm shows an average speed-up of 41%\non the test data and the overall accuracy is increased from 93.8% to 95.7%. We\nadditionally propose a metric for identifying superpixels with an uncertain\nclassification so they can be excluded from further analysis. Finally, we\nevaluate two potential medical applications, namely tumor area estimation\nincluding tumor invasive margin generation and tumor composition analysis.",
          "link": "http://arxiv.org/abs/2106.15893",
          "publishedOn": "2021-07-01T01:59:32.242Z",
          "wordCount": 700,
          "title": "Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a>",
          "description": "ML-enabled software systems have been incorporated in many public\ndemonstrations for automated driving (AD) systems. Such solutions have also\nbeen considered as a crucial approach to aim at SAE Level 5 systems, where the\npassengers in such vehicles do not have to interact with the system at all\nanymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach\nfor training the complete software stack covering perception, planning and\ndecision making, and the actual vehicle control. While such approaches show the\ngreat potential of such ML-enabled systems, there have also been demonstrations\nwhere already changes to single pixels in a video frame can potentially lead to\ncompletely different decisions with dangerous consequences. In this paper, a\nstructured analysis has been conducted to explore video degradation effects on\nthe performance of an ML-enabled pedestrian detector. Firstly, a baseline of\napplying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision\nBenchmark Suite has been established. Next, video degradation candidates for\neach of these frames were generated using the leading video codecs libx264,\nlibx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets\nfor color and gray-scale frames resulting in 104 degradation candidates per\noriginal KITTI frame and 426,816 images in total. YOLO was applied to each\nimage to compute the intersection-over-union (IoU) metric to compare the\nperformance with the original baseline. While aggressively lossy compression\nsettings result in significant performance drops as expected, it was also\nobserved that some configurations actually result in slightly better IoU\nresults compared to the baseline. The findings show that carefully chosen lossy\nvideo configurations preserve a decent performance of particular ML-enabled\nsystems while allowing for substantial savings when storing or transmitting\ndata.",
          "link": "http://arxiv.org/abs/2106.15889",
          "publishedOn": "2021-07-01T01:59:32.233Z",
          "wordCount": 726,
          "title": "A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector. (arXiv:2106.15889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Christopher Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Segmenting unseen object instances in cluttered environments is an important\ncapability that robots need when functioning in unstructured environments.\nWhile previous methods have exhibited promising results, they still tend to\nprovide incorrect results in highly cluttered scenes. We postulate that a\nnetwork architecture that encodes relations between objects at a high-level can\nbe beneficial. Thus, in this work, we propose a novel framework that refines\nthe output of such methods by utilizing a graph-based representation of\ninstance masks. We train deep networks capable of sampling smart perturbations\nto the segmentations, and a graph neural network, which can encode relations\nbetween objects, to evaluate the perturbed segmentations. Our proposed method\nis orthogonal to previous works and achieves state-of-the-art performance when\ncombined with them. We demonstrate an application that uses uncertainty\nestimates generated by our method to guide a manipulator, leading to efficient\nunderstanding of cluttered scenes. Code, models, and video can be found at\nhttps://github.com/chrisdxie/rice .",
          "link": "http://arxiv.org/abs/2106.15711",
          "publishedOn": "2021-07-01T01:59:32.203Z",
          "wordCount": 601,
          "title": "RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks. (arXiv:2106.15711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_Y/0/1/0/all/0/1\">Yuanhong chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seon Ho Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively\nwith normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)\nsamples that do not conform to the expected normal patterns. UAD has two main\nadvantages over its fully supervised counterpart. Firstly, it is able to\ndirectly leverage large datasets available from health screening programs that\ncontain mostly normal image samples, avoiding the costly manual labelling of\nabnormal samples and the subsequent issues involved in training with extremely\nclass-imbalanced data. Further, UAD approaches can potentially detect and\nlocalise any type of lesions that deviate from the normal patterns. One\nsignificant challenge faced by UAD methods is how to learn effective\nlow-dimensional image representations to detect and localise subtle\nabnormalities, generally consisting of small lesions. To address this\nchallenge, we propose a novel self-supervised representation learning method,\ncalled Constrained Contrastive Distribution learning for anomaly detection\n(CCD), which learns fine-grained feature representations by simultaneously\npredicting the distribution of augmented data and image contexts using\ncontrastive learning with pretext constraints. The learned representations can\nbe leveraged to train more anomaly-sensitive detection models. Extensive\nexperiment results show that our method outperforms current state-of-the-art\nUAD approaches on three different colonoscopy and fundus screening datasets.\nOur code is available at https://github.com/tianyu0207/CCD.",
          "link": "http://arxiv.org/abs/2103.03423",
          "publishedOn": "2021-07-01T01:59:32.187Z",
          "wordCount": 694,
          "title": "Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images. (arXiv:2103.03423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:32.169Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:32.146Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:32.140Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:32.133Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>",
          "description": "Efficient long-short temporal modeling is key for enhancing the performance\nof action recognition task. In this paper, we propose a new two-stream action\nrecognition network, termed as MENet, consisting of a Motion Enhancement (ME)\nmodule and a Video-level Aggregation (VLA) module to achieve long-short\ntemporal modeling. Specifically, motion representations have been proved\neffective in capturing short-term and high-frequency action. However, current\nmotion representations are calculated from adjacent frames, which may have poor\ninterpretation and bring useless information (noisy or blank). Thus, for\nshort-term motions, we design an efficient ME module to enhance the short-term\nmotions by mingling the motion saliency among neighboring segments. As for\nlong-term aggregations, VLA is adopted at the top of the appearance branch to\nintegrate the long-term dependencies across all segments. The two components of\nMENet are complementary in temporal modeling. Extensive experiments are\nconducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and\nefficiency of our proposed MENet.",
          "link": "http://arxiv.org/abs/2106.15787",
          "publishedOn": "2021-07-01T01:59:32.113Z",
          "wordCount": 591,
          "title": "Long-Short Temporal Modeling for Efficient Action Recognition. (arXiv:2106.15787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droguett_E/0/1/0/all/0/1\">Enrique Lopez Droguett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1\">Andres Valenzuela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benalcazar_D/0/1/0/all/0/1\">Daniel Benalcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Causa_L/0/1/0/all/0/1\">Leonardo Causa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "This paper proposes a new framework to detect, segment, and estimate the\nlocalization of the eyes from a periocular Near-Infra-Red iris image under\nalcohol consumption. The purpose of the system is to measure the fitness for\nduty. Fitness systems allow us to determine whether a person is physically or\npsychologically able to perform their tasks. Our framework is based on an\nobject detector trained from scratch to detect both eyes from a single image.\nThen, two efficient networks were used for semantic segmentation; a Criss-Cross\nattention network and DenseNet10, with only 122,514 and 210,732 parameters,\nrespectively. These networks can find the pupil, iris, and sclera. In the end,\nthe binary output eye mask is used for pupil and iris diameter estimation with\nhigh precision. Five state-of-the-art algorithms were used for this purpose. A\nmixed proposal reached the best results. A second contribution is establishing\nan alcohol behavior curve to detect the alcohol presence utilizing a stream of\nimages captured from an iris instance. Also, a manually labeled database with\nmore than 20k images was created. Our best method obtains a mean\nIntersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters\nand an error of only 1-pixel on average.",
          "link": "http://arxiv.org/abs/2106.15828",
          "publishedOn": "2021-07-01T01:59:32.106Z",
          "wordCount": 643,
          "title": "Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under Alcohol Effects. (arXiv:2106.15828v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Compared to many other dense prediction tasks, e.g., semantic segmentation,\nit is the arbitrary number of instances that has made instance segmentation\nmuch more challenging. In order to predict a mask for each instance, mainstream\napproaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN),\nor predict embedding vectors first then cluster pixels into individual\ninstances. In this paper, we view the task of instance segmentation from a\ncompletely new perspective by introducing the notion of \"instance categories\",\nwhich assigns categories to each pixel within an instance according to the\ninstance's location. With this notion, we propose segmenting objects by\nlocations (SOLO), a simple, direct, and fast framework for instance\nsegmentation with strong performance. We derive a few SOLO variants (e.g.,\nVanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our\nmethod directly maps a raw input image to the desired object categories and\ninstance masks, eliminating the need for the grouping post-processing or the\nbounding box detection. Our approach achieves state-of-the-art results for\ninstance segmentation in terms of both speed and accuracy, while being\nconsiderably simpler than the existing methods. Besides instance segmentation,\nour method yields state-of-the-art results in object detection (from our mask\nbyproduct) and panoptic segmentation. We further demonstrate the flexibility\nand high-quality segmentation of SOLO by extending it to perform one-stage\ninstance-level image matting. Code is available at: https://git.io/AdelaiDet",
          "link": "http://arxiv.org/abs/2106.15947",
          "publishedOn": "2021-07-01T01:59:32.089Z",
          "wordCount": 672,
          "title": "SOLO: A Simple Framework for Instance Segmentation. (arXiv:2106.15947v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zipei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_F/0/1/0/all/0/1\">Fengqian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>",
          "description": "Cell detection in histopathology images is of great value in clinical\npractice. \\textit{Convolutional neural networks} (CNNs) have been applied to\ncell detection to improve the detection accuracy, where cell annotations are\nrequired for network training. However, due to the variety and large number of\ncells, complete annotations that include every cell of interest in the training\nimages can be challenging. Usually, incomplete annotations can be achieved,\nwhere positive labeling results are carefully examined to ensure their\nreliability but there can be other positive instances, i.e., cells of interest,\nthat are not included in the annotations. This annotation strategy leads to a\nlack of knowledge about true negative samples. Most existing methods simply\ntreat instances that are not labeled as positive as truly negative during\nnetwork training, which can adversely affect the network performance. In this\nwork, to address the problem of incomplete annotations, we formulate the\ntraining of detection networks as a positive-unlabeled learning problem.\nSpecifically, the classification loss in network training is revised to take\ninto account incomplete annotations, where the terms corresponding to negative\nsamples are approximated with the true positive samples and the other samples\nof which the labels are unknown. To evaluate the proposed method, experiments\nwere performed on a publicly available dataset for mitosis detection in breast\ncancer cells, and the experimental results show that our method improves the\nperformance of cell detection given incomplete annotations for training.",
          "link": "http://arxiv.org/abs/2106.15918",
          "publishedOn": "2021-07-01T01:59:32.068Z",
          "wordCount": 680,
          "title": "Positive-unlabeled Learning for Cell Detection in Histopathology Images with Incomplete Annotations. (arXiv:2106.15918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Saurabh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>",
          "description": "Shape completion is the problem of completing partial input shapes such as\npartial scans. This problem finds important applications in computer vision and\nrobotics due to issues such as occlusion or sparsity in real-world data.\nHowever, most of the existing research related to shape completion has been\nfocused on completing shapes by learning a one-to-one mapping which limits the\ndiversity and creativity of the produced results. We propose a novel multimodal\nshape completion technique that is effectively able to learn a one-to-many\nmapping and generates diverse complete shapes. Our approach is based on the\nconditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we\ncondition our inputs on partial 3D point clouds. We extensively evaluate our\napproach by comparing it to various baselines both quantitatively and\nqualitatively. We show that our method is superior to alternatives in terms of\ncompleteness and diversity of shapes",
          "link": "http://arxiv.org/abs/2106.16237",
          "publishedOn": "2021-07-01T01:59:32.031Z",
          "wordCount": 571,
          "title": "Shape Completion via IMLE. (arXiv:2106.16237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:31.999Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16031",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>",
          "description": "Multi-modal imaging is a key healthcare technology in the diagnosis and\nmanagement of disease, but it is often underutilized due to costs associated\nwith multiple separate scans. This limitation yields the need for synthesis of\nunacquired modalities from the subset of available modalities. In recent years,\ngenerative adversarial network (GAN) models with superior depiction of\nstructural details have been established as state-of-the-art in numerous\nmedical image synthesis tasks. However, GANs are characteristically based on\nconvolutional neural network (CNN) backbones that perform local processing with\ncompact filters. This inductive bias, in turn, compromises learning of\nlong-range spatial dependencies. While attention maps incorporated in GANs can\nmultiplicatively modulate CNN features to emphasize critical image regions,\ntheir capture of global context is mostly implicit. Here, we propose a novel\ngenerative adversarial approach for medical image synthesis, ResViT, to combine\nlocal precision of convolution operators with contextual sensitivity of vision\ntransformers. Based on an encoder-decoder architecture, ResViT employs a\ncentral bottleneck comprising novel aggregated residual transformer (ART)\nblocks that synergistically combine convolutional and transformer modules.\nComprehensive demonstrations are performed for synthesizing missing sequences\nin multi-contrast MRI and CT images from MRI. Our results indicate the\nsuperiority of ResViT against competing methods in terms of qualitative\nobservations and quantitative metrics.",
          "link": "http://arxiv.org/abs/2106.16031",
          "publishedOn": "2021-07-01T01:59:31.980Z",
          "wordCount": 650,
          "title": "ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaofo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.15797",
          "publishedOn": "2021-07-01T01:59:31.970Z",
          "wordCount": 656,
          "title": "Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiangxin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Association, aiming to link bounding boxes of the same identity in a video\nsequence, is a central component in multi-object tracking (MOT). To train\nassociation modules, e.g., parametric networks, real video data are usually\nused. However, annotating person tracks in consecutive video frames is\nexpensive, and such real data, due to its inflexibility, offer us limited\nopportunities to evaluate the system performance w.r.t changing tracking\nscenarios. In this paper, we study whether 3D synthetic data can replace\nreal-world videos for association training. Specifically, we introduce a\nlarge-scale synthetic data engine named MOTX, where the motion characteristics\nof cameras and objects are manually configured to be similar to those in\nreal-world datasets. We show that compared with real data, association\nknowledge obtained from synthetic data can achieve very similar performance on\nreal-world test sets without domain adaption techniques. Our intriguing\nobservation is credited to two factors. First and foremost, 3D engines can well\nsimulate motion factors such as camera movement, camera view and object\nmovement, so that the simulated videos can provide association modules with\neffective motion features. Second, experimental results show that the\nappearance domain gap hardly harms the learning of association knowledge. In\naddition, the strong customization ability of MOTX allows us to quantitatively\nassess the impact of motion factors on MOT, which brings new insights to the\ncommunity.",
          "link": "http://arxiv.org/abs/2106.16100",
          "publishedOn": "2021-07-01T01:59:31.963Z",
          "wordCount": 670,
          "title": "Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.957Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:31.936Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15953",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Cheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaifu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>",
          "description": "Images obtained in real-world low-light conditions are not only low in\nbrightness, but they also suffer from many other types of degradation, such as\ncolor bias, unknown noise, detail loss and halo artifacts. In this paper, we\npropose a very fast deep learning framework called Bringing the Lightness\n(denoted as BLNet) that consists of two U-Nets with a series of well-designed\nloss functions to tackle all of the above degradations. Based on Retinex\nTheory, the decomposition net in our model can decompose low-light images into\nreflectance and illumination and remove noise in the reflectance during the\ndecomposition phase. We propose a Noise and Color Bias Control module (NCBC\nModule) that contains a convolutional neural network and two loss functions\n(noise loss and color loss). This module is only used to calculate the loss\nfunctions during the training phase, so our method is very fast during the test\nphase. This module can smooth the reflectance to achieve the purpose of noise\nremoval while preserving details and edge information and controlling color\nbias. We propose a network that can be trained to learn the mapping between\nlow-light and normal-light illumination and enhance the brightness of images\ntaken in low-light illumination. We train and evaluate the performance of our\nproposed model over the real-world Low-Light (LOL) dataset), and we also test\nour model over several other frequently used datasets (LIME, DICM and MEF\ndatasets). We conduct extensive experiments to demonstrate that our approach\nachieves a promising effect with good rubustness and generalization and\noutperforms many other state-of-the-art methods qualitatively and\nquantitatively. Our method achieves high speed because we use loss functions\ninstead of introducing additional denoisers for noise removal and color\ncorrection. The code and model are available at\nhttps://github.com/weixinxu666/BLNet.",
          "link": "http://arxiv.org/abs/2106.15953",
          "publishedOn": "2021-07-01T01:59:31.910Z",
          "wordCount": 765,
          "title": "BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement with Noise Removal and Color Restoration. (arXiv:2106.15953v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bohao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1\">Kyle Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1\">Jordan M. Malof</a>",
          "description": "Recently deep neural networks (DNNs) have achieved tremendous success for\nobject detection in overhead (e.g., satellite) imagery. One ongoing challenge\nhowever is the acquisition of training data, due to high costs of obtaining\nsatellite imagery and annotating objects in it. In this work we present a\nsimple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and\nrapidly generate large quantities of synthetic overhead training data for\ncustom target objects. We demonstrate the effectiveness of using SIMPL\nsynthetic imagery for training DNNs in zero-shot scenarios where no real\nimagery is available; and few-shot learning scenarios, where limited real-world\nimagery is available. We also conduct experiments to study the sensitivity of\nSIMPL's effectiveness to some key design parameters, providing users for\ninsights when designing synthetic imagery for custom objects. We release a\nsoftware implementation of our SIMPL approach so that others can build upon it,\nor use it for their own custom problems.",
          "link": "http://arxiv.org/abs/2106.15681",
          "publishedOn": "2021-07-01T01:59:31.903Z",
          "wordCount": 599,
          "title": "SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and Few-Shot Detection Problems. (arXiv:2106.15681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:31.887Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wenming Tang Guoping Qiu</a>",
          "description": "This paper presents new designs of graph convolutional neural networks (GCNs)\non 3D meshes for 3D object segmentation and classification. We use the faces of\nthe mesh as basic processing units and represent a 3D mesh as a graph where\neach node corresponds to a face. To enhance the descriptive power of the graph,\nwe introduce a 1-ring face neighbourhood structure to derive novel\nmulti-dimensional spatial and structure features to represent the graph nodes.\nBased on this new graph representation, we then design a densely connected\ngraph convolutional block which aggregates local and regional features as the\nkey construction component to build effective and efficient practical GCN\nmodels for 3D object classification and segmentation. We will present\nexperimental results to show that our new technique outperforms state of the\nart where our models are shown to have the smallest number of parameters and\nconsietently achieve the highest accuracies across a number of benchmark\ndatasets. We will also present ablation studies to demonstrate the soundness of\nour design principles and the effectiveness of our practical models.",
          "link": "http://arxiv.org/abs/2106.15778",
          "publishedOn": "2021-07-01T01:59:31.848Z",
          "wordCount": 621,
          "title": "Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object Segmentation and Classification. (arXiv:2106.15778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:31.828Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.",
          "link": "http://arxiv.org/abs/2106.15686",
          "publishedOn": "2021-07-01T01:59:31.806Z",
          "wordCount": 647,
          "title": "Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15707",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Binghuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Segmentation of cardiac fibrosis and scar are essential for clinical\ndiagnosis and can provide invaluable guidance for the treatment of cardiac\ndiseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance\n(CMR) has been successful for its efficacy in guiding the clinical diagnosis\nand treatment reliably. For LGE CMR, many methods have demonstrated success in\naccurately segmenting scarring regions. Co-registration with other\nnon-contrast-agent (non-CA) modalities, balanced steady-state free precession\n(bSSFP) and cine magnetic resonance imaging (MRI) for example, can further\nenhance the efficacy of automated segmentation of cardiac anatomies. Many\nconventional methods have been proposed to provide automated or semi-automated\nsegmentation of scars. With the development of deep learning in recent years,\nwe can also see more advanced methods that are more efficient in providing more\naccurate segmentations. This paper conducts a state-of-the-art review of\nconventional and current state-of-the-art approaches utilising different\nmodalities for accurate cardiac fibrosis and scar segmentation.",
          "link": "http://arxiv.org/abs/2106.15707",
          "publishedOn": "2021-07-01T01:59:31.800Z",
          "wordCount": 622,
          "title": "Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A State-of-the-Art Review and Future Perspectives. (arXiv:2106.15707v1 [eess.IV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.07474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_D/0/1/0/all/0/1\">Dustin Hayes</a>",
          "description": "This paper contributes to interpretable machine learning via visual knowledge\ndiscovery in parallel coordinates. The concepts of hypercubes and hyper-blocks\nare used as easily understandable by end-users in the visual form in parallel\ncoordinates. The Hyper algorithm for classification with mixed and pure\nhyper-blocks (HBs) is proposed to discover hyper-blocks interactively and\nautomatically in individual, multiple, overlapping, and non-overlapping\nsetting. The combination of hyper-blocks with linguistic description of visual\npatterns is presented too. It is shown that Hyper models generalize decision\ntrees. The Hyper algorithm was tested on the benchmark data from UCI ML\nrepository. It allowed discovering pure and mixed HBs with all data and then\nwith 10-fold cross validation. The links between hyper-blocks, dimension\nreduction and visualization are established. Major benefits of hyper-block\ntechnology and the Hyper algorithm are in their ability to discover and observe\nhyper-blocks by end-users including side by side visualizations making patterns\nvisible for all classes. Another advantage of sets of HBs relative to the\ndecision trees is the ability to avoid both data overgeneralization and\noverfitting.",
          "link": "http://arxiv.org/abs/2106.07474",
          "publishedOn": "2021-07-06T01:58:11.980Z",
          "wordCount": 629,
          "title": "Discovering Interpretable Machine Learning Models in Parallel Coordinates. (arXiv:2106.07474v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazrae_P/0/1/0/all/0/1\">Pooya Rostami Mazrae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1\">Maliheh Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydarnoori_A/0/1/0/all/0/1\">Abbas Heydarnoori</a>",
          "description": "An issue documents discussions around required changes in issue-tracking\nsystems, while a commit contains the change itself in the version control\nsystems. Recovering links between issues and commits can facilitate many\nsoftware evolution tasks such as bug localization, and software documentation.\nA previous study on over half a million issues from GitHub reports only about\n42.2% of issues are manually linked by developers to their pertinent commits.\nAutomating the linking of commit-issue pairs can contribute to the improvement\nof the said tasks. By far, current state-of-the-art approaches for automated\ncommit-issue linking suffer from low precision, leading to unreliable results,\nsometimes to the point that imposes human supervision on the predicted links.\nThe low performance gets even more severe when there is a lack of textual\ninformation in either commits or issues. Current approaches are also proven\ncomputationally expensive.\n\nWe propose Hybrid-Linker to overcome such limitations by exploiting two\ninformation channels; (1) a non-textual-based component that operates on\nnon-textual, automatically recorded information of the commit-issue pairs to\npredict a link, and (2) a textual-based one which does the same using textual\ninformation of the commit-issue pairs. Then, combining the results from the two\nclassifiers, Hybrid-Linker makes the final prediction. Thus, every time one\ncomponent falls short in predicting a link, the other component fills the gap\nand improves the results. We evaluate Hybrid-Linker against competing\napproaches, namely FRLink and DeepLink on a dataset of 12 projects.\nHybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and\nF-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and\n41.3%, regarding the F-measure. Moreover, Hybrid-Linker exhibits extensive\nimprovements in terms of performance as well.",
          "link": "http://arxiv.org/abs/2107.01894",
          "publishedOn": "2021-07-06T01:58:11.963Z",
          "wordCount": 728,
          "title": "Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data. (arXiv:2107.01894v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Recent advances in deep learning have led to the development of accurate and\nefficient models for various computer vision applications such as\nclassification, segmentation, and detection. However, learning highly accurate\nmodels relies on the availability of large-scale annotated datasets. Due to\nthis, model performance drops drastically when evaluated on label-scarce\ndatasets having visually distinct images, termed as domain adaptation problem.\nThere is a plethora of works to adapt classification and segmentation models to\nlabel-scarce target datasets through unsupervised domain adaptation.\nConsidering that detection is a fundamental task in computer vision, many\nrecent works have focused on developing novel domain adaptive detection\ntechniques. Here, we describe in detail the domain adaptation problem for\ndetection and present an extensive survey of the various methods. Furthermore,\nwe highlight strategies proposed and the associated shortcomings. Subsequently,\nwe identify multiple aspects of the problem that are most promising for future\nresearch. We believe that this survey shall be valuable to the pattern\nrecognition experts working in the fields of computer vision, biometrics,\nmedical imaging, and autonomous navigation by introducing them to the problem,\nand familiarizing them with the current status of the progress while providing\npromising directions for future research.",
          "link": "http://arxiv.org/abs/2105.13502",
          "publishedOn": "2021-07-06T01:58:11.957Z",
          "wordCount": 667,
          "title": "Unsupervised Domain Adaptation of Object Detectors: A Survey. (arXiv:2105.13502v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagaraju_R/0/1/0/all/0/1\">Rakesh Nagaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Generative adversarial networks (GAN) are a class of powerful machine\nlearning techniques, where both a generative and discriminative model are\ntrained simultaneously. GANs have been used, for example, to successfully\ngenerate \"deep fake\" images. A recent trend in malware research consists of\ntreating executables as images and employing image-based analysis techniques.\nIn this research, we generate fake malware images using auxiliary classifier\nGANs (AC-GAN), and we consider the effectiveness of various techniques for\nclassifying the resulting images. Our results indicate that the resulting\nmulticlass classification problem is challenging, yet we can obtain strong\nresults when restricting the problem to distinguishing between real and fake\nsamples. While the AC-GAN generated images often appear to be very similar to\nreal malware images, we conclude that from a deep learning perspective, the\nAC-GAN generated samples do not rise to the level of deep fake malware images.",
          "link": "http://arxiv.org/abs/2107.01620",
          "publishedOn": "2021-07-06T01:58:11.950Z",
          "wordCount": 568,
          "title": "Auxiliary-Classifier GAN for Malware Analysis. (arXiv:2107.01620v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erven_T/0/1/0/all/0/1\">Tim van Erven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachs_S/0/1/0/all/0/1\">Sarah Sachs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1\">Wouter M. Koolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotlowski_W/0/1/0/all/0/1\">Wojciech Kot&#x142;owski</a>",
          "description": "We consider online convex optimization when a number k of data points are\noutliers that may be corrupted. We model this by introducing the notion of\nrobust regret, which measures the regret only on rounds that are not outliers.\nThe aim for the learner is to achieve small robust regret, without knowing\nwhere the outliers are. If the outliers are chosen adversarially, we show that\na simple filtering strategy on extreme gradients incurs O(k) additive overhead\ncompared to the usual regret bounds, and that this is unimprovable, which means\nthat k needs to be sublinear in the number of rounds. We further ask which\nadditional assumptions would allow for a linear number of outliers. It turns\nout that the usual benign cases of independently, identically distributed\n(i.i.d.) observations or strongly convex losses are not sufficient. However,\ncombining i.i.d. observations with the assumption that outliers are those\nobservations that are in an extreme quantile of the distribution, does lead to\nsublinear robust regret, even though the expected number of outliers is linear.",
          "link": "http://arxiv.org/abs/2107.01881",
          "publishedOn": "2021-07-06T01:58:11.944Z",
          "wordCount": 607,
          "title": "Robust Online Convex Optimization in the Presence of Outliers. (arXiv:2107.01881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01906",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Azizian_W/0/1/0/all/0/1\">Wa&#xef;ss Azizian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1\">Franck Iutzeler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Malick</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>",
          "description": "In this paper, we analyze the local convergence rate of optimistic mirror\ndescent methods in stochastic variational inequalities, a class of optimization\nproblems with important applications to learning theory and machine learning.\nOur analysis reveals an intricate relation between the algorithm's rate of\nconvergence and the local geometry induced by the method's underlying Bregman\nfunction. We quantify this relation by means of the Legendre exponent, a notion\nthat we introduce to measure the growth rate of the Bregman divergence relative\nto the ambient norm near a solution. We show that this exponent determines both\nthe optimal step-size policy of the algorithm and the optimal rates attained,\nexplaining in this way the differences observed for some popular Bregman\nfunctions (Euclidean projection, negative entropy, fractional power, etc.).",
          "link": "http://arxiv.org/abs/2107.01906",
          "publishedOn": "2021-07-06T01:58:11.935Z",
          "wordCount": 597,
          "title": "The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities. (arXiv:2107.01906v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2005.09310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>",
          "description": "Knowledge distillation has been widely used to compress existing deep\nlearning models while preserving the performance on a wide range of\napplications. In the specific context of Automatic Speech Recognition (ASR),\ndistillation from ensembles of acoustic models has recently shown promising\nresults in increasing recognition performance. In this paper, we propose an\nextension of multi-teacher distillation methods to joint CTC-attention\nend-to-end ASR systems. We also introduce three novel distillation strategies.\nThe core intuition behind them is to integrate the error rate metric to the\nteacher selection rather than solely focusing on the observed losses. In this\nway, we directly distill and optimize the student toward the relevant metric\nfor speech recognition. We evaluate these strategies under a selection of\ntraining procedures on different datasets (TIMIT, Librispeech, Common Voice)\nand various languages (English, French, Italian). In particular,\nstate-of-the-art error rates are reported on the Common Voice French, Italian\nand TIMIT datasets.",
          "link": "http://arxiv.org/abs/2005.09310",
          "publishedOn": "2021-07-06T01:58:11.917Z",
          "wordCount": 641,
          "title": "Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition. (arXiv:2005.09310v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1\">Safa Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.",
          "link": "http://arxiv.org/abs/2106.02994",
          "publishedOn": "2021-07-06T01:58:11.910Z",
          "wordCount": 614,
          "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.10488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Taeeon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_B/0/1/0/all/0/1\">Byeongjoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baek_J/0/1/0/all/0/1\">Jongduk Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one,\nBM3D. The official code of our method is available at\nhttps://github.com/csm9493/GAN2GAN.",
          "link": "http://arxiv.org/abs/1905.10488",
          "publishedOn": "2021-07-06T01:58:11.904Z",
          "wordCount": 738,
          "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images. (arXiv:1905.10488v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamarca_J/0/1/0/all/0/1\">Jos&#xe9; Lamarca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facil_J/0/1/0/all/0/1\">Jos&#xe9; M. F&#xe1;cil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J. M. M. Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>",
          "description": "Estimating a scene reconstruction and the camera motion from in-body videos\nis challenging due to several factors, e.g. the deformation of in-body cavities\nor the lack of texture. In this paper we present Endo-Depth-and-Motion, a\npipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene\nmodels from monocular endoscopic videos. Our approach leverages recent advances\nin self-supervised depth networks to generate pseudo-RGBD frames, then tracks\nthe camera pose using photometric residuals and fuses the registered depth maps\nin a volumetric representation. We present an extensive experimental evaluation\nin the public dataset Hamlyn, showing high-quality results and comparisons\nagainst relevant baselines. We also release all models and code for future\ncomparisons.",
          "link": "http://arxiv.org/abs/2103.16525",
          "publishedOn": "2021-07-06T01:58:11.896Z",
          "wordCount": 600,
          "title": "Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints. (arXiv:2103.16525v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baby_D/0/1/0/all/0/1\">Dheeraj Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "We consider the problem of the Zinkevich (2003)-style dynamic regret\nminimization in online learning with exp-concave losses. We show that whenever\nimproper learning is allowed, a Strongly Adaptive online learner achieves the\ndynamic regret of $\\tilde O^*(n^{1/3}C_n^{2/3} \\vee 1)$ where $C_n$ is the\ntotal variation (a.k.a. path length) of the an arbitrary sequence of\ncomparators that may not be known to the learner ahead of time. Achieving this\nrate was highly nontrivial even for squared losses in 1D where the best known\nupper bound was $O(\\sqrt{nC_n} \\vee \\log n)$ (Yuan and Lamperski, 2019). Our\nnew proof techniques make elegant use of the intricate structures of the primal\nand dual variables imposed by the KKT conditions and could be of independent\ninterest. Finally, we apply our results to the classical statistical problem of\nlocally adaptive non-parametric regression (Mammen, 1991; Donoho and Johnstone,\n1998) and obtain a stronger and more flexible algorithm that do not require any\nstatistical assumptions or any hyperparameter tuning.",
          "link": "http://arxiv.org/abs/2104.11824",
          "publishedOn": "2021-07-06T01:58:11.888Z",
          "wordCount": 634,
          "title": "Optimal Dynamic Regret in Exp-Concave Online Learning. (arXiv:2104.11824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">David Robert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Kazuya Takeda</a>",
          "description": "Interconnected road lanes are a central concept for navigating urban roads.\nCurrently, most autonomous vehicles rely on preconstructed lane maps as\ndesigning an algorithmic model is difficult. However, the generation and\nmaintenance of such maps is costly and hinders large-scale adoption of\nautonomous vehicle technology. This paper presents the first self-supervised\nlearning method to train a model to infer a spatially grounded lane-level road\nnetwork graph based on a dense segmented representation of the road scene\ngenerated from onboard sensors. A formal road lane network model is presented\nand proves that any structured road scene can be represented by a directed\nacyclic graph of at most depth three while retaining the notion of intersection\nregions, and that this is the most compressed representation. The formal model\nis implemented by a hybrid neural and search-based model, utilizing a novel\nbarrier function loss formulation for robust learning from partial labels.\nExperiments are conducted for all common road intersection layouts. Results\nshow that the model can generalize to new road layouts, unlike previous\napproaches, demonstrating its potential for real-world application as a\npractical learning-based lane-level map generator.",
          "link": "http://arxiv.org/abs/2107.01784",
          "publishedOn": "2021-07-06T01:58:11.869Z",
          "wordCount": 650,
          "title": "Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision. (arXiv:2107.01784v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1\">Max A. Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayas_U/0/1/0/all/0/1\">Ugur Kayas</a>",
          "description": "Dynamic programming (DP) is a broadly applicable algorithmic design paradigm\nfor the efficient, exact solution of otherwise intractable, combinatorial\nproblems. However, the design of such algorithms is often presented informally\nin an ad-hoc manner, and as a result is often difficult to apply correctly. In\nthis paper, we present a rigorous algebraic formalism for systematically\nderiving novel DP algorithms, either from existing DP algorithms or from simple\nfunctional recurrences. These derivations lead to algorithms which are provably\ncorrect and polymorphic over any semiring, which means that they can be applied\nto the full scope of combinatorial problems expressible in terms of semirings.\nThis includes, for example: optimization, optimal probability and Viterbi\ndecoding, probabilistic marginalization, logical inference, fuzzy sets,\ndifferentiable softmax, and relational and provenance queries. The approach,\nbuilding on many ideas from the existing literature on constructive\nalgorithmics, exploits generic properties of (semiring) polymorphic functions,\ntupling and formal sums (lifting), and algebraic simplifications arising from\nconstraint algebras. We demonstrate the effectiveness of this formalism for\nsome example applications arising in signal processing, bioinformatics and\nreliability engineering.",
          "link": "http://arxiv.org/abs/2107.01752",
          "publishedOn": "2021-07-06T01:58:11.863Z",
          "wordCount": 613,
          "title": "Polymorphic dynamic programming by algebraic shortcut fusion. (arXiv:2107.01752v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_J/0/1/0/all/0/1\">Jonathan S. Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1\">Michael Carbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>",
          "description": "We show that the error of iteratively magnitude-pruned networks empirically\nfollows a scaling law with interpretable coefficients that depend on the\narchitecture and task. We functionally approximate the error of the pruned\nnetworks, showing it is predictable in terms of an invariant tying width,\ndepth, and pruning level, such that networks of vastly different pruned\ndensities are interchangeable. We demonstrate the accuracy of this\napproximation over orders of magnitude in depth, width, dataset size, and\ndensity. We show that the functional form holds (generalizes) for large scale\ndata (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks\nbecome ever larger and costlier to train, our findings suggest a framework for\nreasoning conceptually and analytically about a standard method for\nunstructured pruning.",
          "link": "http://arxiv.org/abs/2006.10621",
          "publishedOn": "2021-07-06T01:58:11.855Z",
          "wordCount": 599,
          "title": "On the Predictability of Pruning Across Scales. (arXiv:2006.10621v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>",
          "description": "Offline reinforcement learning proposes to learn policies from large\ncollected datasets without interacting with the physical environment. These\nalgorithms have made it possible to learn useful skills from data that can then\nbe deployed in the environment in real-world settings where interactions may be\ncostly or dangerous, such as autonomous driving or factories. However, current\nalgorithms overfit to the dataset they are trained on and exhibit poor\nout-of-distribution generalization to the environment when deployed. In this\npaper, we study the effectiveness of performing data augmentations on the state\nspace, and study 7 different augmentation schemes and how they behave with\nexisting offline RL algorithms. We then combine the best data performing\naugmentation scheme with a state-of-the-art Q-learning technique, and improve\nthe function approximation of the Q-networks by smoothening out the learned\nstate-action space. We experimentally show that using this Surprisingly Simple\nSelf-Supervision technique in RL (S4RL), we significantly improve over the\ncurrent state-of-the-art algorithms on offline robot learning environments such\nas MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",
          "link": "http://arxiv.org/abs/2103.06326",
          "publishedOn": "2021-07-06T01:58:11.848Z",
          "wordCount": 628,
          "title": "S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning. (arXiv:2103.06326v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rohin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_C/0/1/0/all/0/1\">Cody Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Steven H. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_N/0/1/0/all/0/1\">Neel Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houghton_B/0/1/0/all/0/1\">Brandon Houghton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1\">William Guss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Sharada Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Stephanie Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topin_N/0/1/0/all/0/1\">Nicholay Topin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>",
          "description": "The last decade has seen a significant increase of interest in deep learning\nresearch, with many public successes that have demonstrated its potential. As\nsuch, these systems are now being incorporated into commercial products. With\nthis comes an additional challenge: how can we build AI systems that solve\ntasks where there is not a crisp, well-defined specification? While multiple\nsolutions have been proposed, in this competition we focus on one in\nparticular: learning from human feedback. Rather than training AI systems using\na predefined reward function or using a labeled dataset with a predefined set\nof categories, we instead train the AI system using a learning signal derived\nfrom some form of human feedback, which can evolve over time as the\nunderstanding of the task changes, or as the capabilities of the AI system\nimprove.\n\nThe MineRL BASALT competition aims to spur forward research on this important\nclass of techniques. We design a suite of four tasks in Minecraft for which we\nexpect it will be hard to write down hardcoded reward functions. These tasks\nare defined by a paragraph of natural language: for example, \"create a\nwaterfall and take a scenic picture of it\", with additional clarifying details.\nParticipants must train a separate agent for each task, using any method they\nwant. Agents are then evaluated by humans who have read the task description.\nTo help participants get started, we provide a dataset of human demonstrations\non each of the four tasks, as well as an imitation learning baseline that\nleverages these demonstrations.\n\nOur hope is that this competition will improve our ability to build AI\nsystems that do what their designers intend them to do, even when the intent\ncannot be easily formalized. Besides allowing AI to solve more tasks, this can\nalso enable more effective regulation of AI systems, as well as making progress\non the value alignment problem.",
          "link": "http://arxiv.org/abs/2107.01969",
          "publishedOn": "2021-07-06T01:58:11.842Z",
          "wordCount": 772,
          "title": "The MineRL BASALT Competition on Learning from Human Feedback. (arXiv:2107.01969v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01285",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hillman_J/0/1/0/all/0/1\">Jonathan Hillman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hocking_T/0/1/0/all/0/1\">Toby Dylan Hocking</a>",
          "description": "Receiver Operating Characteristic (ROC) curves are plots of true positive\nrate versus false positive rate which are useful for evaluating binary\nclassification models, but difficult to use for learning since the Area Under\nthe Curve (AUC) is non-convex. ROC curves can also be used in other problems\nthat have false positive and true positive rates such as changepoint detection.\nWe show that in this more general context, the ROC curve can have loops, points\nwith highly sub-optimal error rates, and AUC greater than one. This observation\nmotivates a new optimization objective: rather than maximizing the AUC, we\nwould like a monotonic ROC curve with AUC=1 that avoids points with large\nvalues for Min(FP,FN). We propose a convex relaxation of this objective that\nresults in a new surrogate loss function called the AUM, short for Area Under\nMin(FP, FN). Whereas previous loss functions are based on summing over all\nlabeled examples or pairs, the AUM requires a sort and a sum over the sequence\nof points on the ROC curve. We show that AUM directional derivatives can be\nefficiently computed and used in a gradient descent learning algorithm. In our\nempirical study of supervised binary classification and changepoint detection\nproblems, we show that our new AUM minimization learning algorithm results in\nimproved AUC and comparable speed relative to previous baselines.",
          "link": "http://arxiv.org/abs/2107.01285",
          "publishedOn": "2021-07-06T01:58:11.826Z",
          "wordCount": 663,
          "title": "Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection. (arXiv:2107.01285v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berns_S/0/1/0/all/0/1\">Sebastian Berns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broad_T/0/1/0/all/0/1\">Terence Broad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guckelsberger_C/0/1/0/all/0/1\">Christian Guckelsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colton_S/0/1/0/all/0/1\">Simon Colton</a>",
          "description": "We present a framework for automating generative deep learning with a\nspecific focus on artistic applications. The framework provides opportunities\nto hand over creative responsibilities to a generative system as targets for\nautomation. For the definition of targets, we adopt core concepts from\nautomated machine learning and an analysis of generative deep learning\npipelines, both in standard and artistic settings. To motivate the framework,\nwe argue that automation aligns well with the goal of increasing the creative\nresponsibility of a generative system, a central theme in computational\ncreativity research. We understand automation as the challenge of granting a\ngenerative system more creative autonomy, by framing the interaction between\nthe user and the system as a co-creative process. The development of the\nframework is informed by our analysis of the relationship between automation\nand creative autonomy. An illustrative example shows how the framework can give\ninspiration and guidance in the process of handing over creative\nresponsibility.",
          "link": "http://arxiv.org/abs/2107.01858",
          "publishedOn": "2021-07-06T01:58:11.819Z",
          "wordCount": 588,
          "title": "Automating Generative Deep Learning for Artistic Purposes: Challenges and Opportunities. (arXiv:2107.01858v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yulin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuni Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifa Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.",
          "link": "http://arxiv.org/abs/2106.09989",
          "publishedOn": "2021-07-06T01:58:11.811Z",
          "wordCount": 728,
          "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jiechao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "Policy gradient (PG) algorithms have been widely used in reinforcement\nlearning (RL). However, PG algorithms rely on exploiting the value function\nbeing learned with the first-order update locally, which results in limited\nsample efficiency. In this work, we propose an alternative method called\nZeroth-Order Supervised Policy Improvement (ZOSPI). ZOSPI exploits the\nestimated value function $Q$ globally while preserving the local exploitation\nof the PG methods based on zeroth-order policy optimization. This learning\nparadigm follows Q-learning but overcomes the difficulty of efficiently\noperating argmax in continuous action space. It finds max-valued action within\na small number of samples. The policy learning of ZOSPI has two steps: First,\nit samples actions and evaluates those actions with a learned value estimator,\nand then it learns to perform the action with the highest value through\nsupervised learning. We further demonstrate such a supervised learning\nframework can learn multi-modal policies. Experiments show that ZOSPI achieves\ncompetitive results on the continuous control benchmarks with a remarkable\nsample efficiency.",
          "link": "http://arxiv.org/abs/2006.06600",
          "publishedOn": "2021-07-06T01:58:11.804Z",
          "wordCount": 630,
          "title": "Zeroth-Order Supervised Policy Improvement. (arXiv:2006.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1\">Cassidy Laidlaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "A key challenge in adversarial robustness is the lack of a precise\nmathematical characterization of human perception, used in the very definition\nof adversarial attacks that are imperceptible to human eyes. Most current\nattacks and defenses try to avoid this issue by considering restrictive\nadversarial threat models such as those bounded by $L_2$ or $L_\\infty$\ndistance, spatial perturbations, etc. However, models that are robust against\nany of these restrictive threat models are still fragile against other threat\nmodels. To resolve this issue, we propose adversarial training against the set\nof all imperceptible adversarial examples, approximated using deep neural\nnetworks. We call this threat model the neural perceptual threat model (NPTM);\nit includes adversarial examples with a bounded neural perceptual distance (a\nneural network-based approximation of the true perceptual distance) to natural\nimages. Through an extensive perceptual study, we show that the neural\nperceptual distance correlates well with human judgements of perceptibility of\nadversarial examples, validating our threat model.\n\nUnder the NPTM, we develop novel perceptual adversarial attacks and defenses.\nBecause the NPTM is very broad, we find that Perceptual Adversarial Training\n(PAT) against a perceptual attack gives robustness against many other types of\nadversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five\ndiverse adversarial attacks. We find that PAT achieves state-of-the-art\nrobustness against the union of these five attacks, more than doubling the\naccuracy over the next best model, without training against any of them. That\nis, PAT generalizes well to unforeseen perturbation types. This is vital in\nsensitive applications where a particular threat model cannot be assumed, and\nto the best of our knowledge, PAT is the first adversarial training defense\nwith this property.",
          "link": "http://arxiv.org/abs/2006.12655",
          "publishedOn": "2021-07-06T01:58:11.797Z",
          "wordCount": 777,
          "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. (arXiv:2006.12655v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzillotta_G/0/1/0/all/0/1\">Guilia Lanzillotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annadani_Y/0/1/0/all/0/1\">Yashas Annadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We study the problem of self-supervised structured representation learning\nusing autoencoders for generative modeling. Unlike most methods which rely on\nmatching an arbitrary, relatively unstructured, prior distribution for\nsampling, we propose a sampling technique that relies solely on the\nindependence of latent variables, thereby avoiding the trade-off between\nreconstruction quality and generative performance inherent to VAEs. We design a\nnovel autoencoder architecture capable of learning a structured representation\nwithout the need for aggressive regularization. Our structural decoders learn a\nhierarchy of latent variables, akin to structural causal models, thereby\nordering the information without any additional regularization. We demonstrate\nhow these models learn a representation that improves results in a variety of\ndownstream tasks including generation, disentanglement, and extrapolation using\nseveral challenging and natural image datasets.",
          "link": "http://arxiv.org/abs/2006.07796",
          "publishedOn": "2021-07-06T01:58:11.519Z",
          "wordCount": 613,
          "title": "Structure by Architecture: Disentangled Representations without Regularization. (arXiv:2006.07796v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouritsas_G/0/1/0/all/0/1\">Giorgos Bouritsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1\">Fabrizio Frasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael M. Bronstein</a>",
          "description": "While Graph Neural Networks (GNNs) have achieved remarkable results in a\nvariety of applications, recent studies exposed important shortcomings in their\nability to capture the structure of the underlying graph. It has been shown\nthat the expressive power of standard GNNs is bounded by the Weisfeiler-Leman\n(WL) graph isomorphism test, from which they inherit proven limitations such as\nthe inability to detect and count graph substructures. On the other hand, there\nis significant empirical evidence, e.g. in network science and bioinformatics,\nthat substructures are often intimately related to downstream tasks. To this\nend, we propose \"Graph Substructure Networks\" (GSN), a topologically-aware\nmessage passing scheme based on substructure encoding. We theoretically analyse\nthe expressive power of our architecture, showing that it is strictly more\nexpressive than the WL test, and provide sufficient conditions for\nuniversality. Importantly, we do not attempt to adhere to the WL hierarchy;\nthis allows us to retain multiple attractive properties of standard GNNs such\nas locality and linear network complexity, while being able to disambiguate\neven hard instances of graph isomorphism. We perform an extensive experimental\nevaluation on graph classification and regression tasks and obtain\nstate-of-the-art results in diverse real-world settings including molecular\ngraphs and social networks. The code is publicly available at\nhttps://github.com/gbouritsas/graph-substructure-networks.",
          "link": "http://arxiv.org/abs/2006.09252",
          "publishedOn": "2021-07-06T01:58:11.509Z",
          "wordCount": 692,
          "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. (arXiv:2006.09252v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pinyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaojin Zhang</a>",
          "description": "We study the problem of identifying the best arm in a stochastic multi-armed\nbandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is\nassociated with an unknown reward distribution supported on $[0,1]$ with mean\n$\\theta_i$ and variance $\\sigma_i^2$. Assume $\\theta_1 > \\theta_2 \\geq \\cdots\n\\geq\\theta_n$. We propose an adaptive algorithm which explores the gaps and\nvariances of the rewards of the arms and makes future decisions based on the\ngathered information using a novel approach called \\textit{grouped median\nelimination}. The proposed algorithm guarantees to output the best arm with\nprobability $(1-\\delta)$ and uses at most $O \\left(\\sum_{i = 1}^n\n\\left(\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i}\\right)(\\ln \\delta^{-1}\n+ \\ln \\ln \\Delta_i^{-1})\\right)$ samples, where $\\Delta_i$ ($i \\geq 2$) denotes\nthe reward gap between arm $i$ and the best arm and we define $\\Delta_1 =\n\\Delta_2$. This achieves a significant advantage over the variance-independent\nalgorithms in some favorable scenarios and is the first result that removes the\nextra $\\ln n$ factor on the best arm compared with the state-of-the-art. We\nfurther show that $\\Omega \\left( \\sum_{i = 1}^n \\left(\n\\frac{\\sigma_i^2}{\\Delta_i^2} + \\frac{1}{\\Delta_i} \\right) \\ln \\delta^{-1}\n\\right)$ samples are necessary for an algorithm to achieve the same goal,\nthereby illustrating that our algorithm is optimal up to doubly logarithmic\nterms.",
          "link": "http://arxiv.org/abs/2106.10417",
          "publishedOn": "2021-07-06T01:58:11.503Z",
          "wordCount": 662,
          "title": "Variance-Dependent Best Arm Identification. (arXiv:2106.10417v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.",
          "link": "http://arxiv.org/abs/2105.09856",
          "publishedOn": "2021-07-06T01:58:11.496Z",
          "wordCount": 670,
          "title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling. (arXiv:2105.09856v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01994",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Riva_M/0/1/0/all/0/1\">Mateus Riva</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cesar_R/0/1/0/all/0/1\">Roberto M. Cesar Jr.</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>",
          "description": "We propose a novel graph clustering method guided by additional information\non the underlying structure of the clusters (or communities). The problem is\nformulated as the matching of a graph to a template with smaller dimension,\nhence matching $n$ vertices of the observed graph (to be clustered) to the $k$\nvertices of a template graph, using its edges as support information, and\nrelaxed on the set of orthonormal matrices in order to find a $k$ dimensional\nembedding. With relevant priors that encode the density of the clusters and\ntheir relationships, our method outperforms classical methods, especially for\nchallenging cases.",
          "link": "http://arxiv.org/abs/2107.01994",
          "publishedOn": "2021-07-06T01:58:11.480Z",
          "wordCount": 555,
          "title": "Template-Based Graph Clustering. (arXiv:2107.01994v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1\">Jackson A. Killian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lily Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "We introduce Robust Restless Bandits, a challenging generalization of\nrestless multi-arm bandits (RMAB). RMABs have been widely studied for\nintervention planning with limited resources. However, most works make the\nunrealistic assumption that the transition dynamics are known perfectly,\nrestricting the applicability of existing methods to real-world scenarios. To\nmake RMABs more useful in settings with uncertain dynamics: (i) We introduce\nthe Robust RMAB problem and develop solutions for a minimax regret objective\nwhen transitions are given by interval uncertainties; (ii) We develop a double\noracle algorithm for solving Robust RMABs and demonstrate its effectiveness on\nthree experimental domains; (iii) To enable our double oracle approach, we\nintroduce RMABPPO, a novel deep reinforcement learning algorithm for solving\nRMABs. RMABPPO hinges on learning an auxiliary \"$\\lambda$-network\" that allows\neach arm's learning to decouple, greatly reducing sample complexity required\nfor training; (iv) Under minimax regret, the adversary in the double oracle\napproach is notoriously difficult to implement due to non-stationarity. To\naddress this, we formulate the adversary oracle as a multi-agent reinforcement\nlearning problem and solve it with a multi-agent extension of RMABPPO, which\nmay be of independent interest as the first known algorithm for this setting.\nCode is available at https://github.com/killian-34/RobustRMAB.",
          "link": "http://arxiv.org/abs/2107.01689",
          "publishedOn": "2021-07-06T01:58:11.474Z",
          "wordCount": 641,
          "title": "Robust Restless Bandits: Tackling Interval Uncertainty with Deep Reinforcement Learning. (arXiv:2107.01689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouridi_C/0/1/0/all/0/1\">Christina Kouridi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1\">Yassine Nemmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We propose the adversarially robust kernel smoothing (ARKS) algorithm,\ncombining kernel smoothing, robust optimization, and adversarial training for\nrobust learning. Our methods are motivated by the convex analysis perspective\nof distributionally robust optimization based on probability metrics, such as\nthe Wasserstein distance and the maximum mean discrepancy. We adapt the\nintegral operator using supremal convolution in convex analysis to form a novel\nfunction majorant used for enforcing robustness. Our method is simple in form\nand applies to general loss functions and machine learning models. Furthermore,\nwe report experiments with general machine learning models, such as deep neural\nnetworks, to demonstrate that ARKS performs competitively with the\nstate-of-the-art methods based on the Wasserstein distance.",
          "link": "http://arxiv.org/abs/2102.08474",
          "publishedOn": "2021-07-06T01:58:11.467Z",
          "wordCount": 582,
          "title": "Adversarially Robust Kernel Smoothing. (arXiv:2102.08474v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tritrong_N/0/1/0/all/0/1\">Nontawat Tritrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rewatbowornwong_P/0/1/0/all/0/1\">Pitchaporn Rewatbowornwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1\">Supasorn Suwajanakorn</a>",
          "description": "While GANs have shown success in realistic image generation, the idea of\nusing GANs for other tasks unrelated to synthesis is underexplored. Do GANs\nlearn meaningful structural parts of objects during their attempt to reproduce\nthose objects? In this work, we test this hypothesis and propose a simple and\neffective approach based on GANs for semantic part segmentation that requires\nas few as one label example along with an unlabeled dataset. Our key idea is to\nleverage a trained GAN to extract pixel-wise representation from the input\nimage and use it as feature vectors for a segmentation network. Our experiments\ndemonstrate that GANs representation is \"readily discriminative\" and produces\nsurprisingly good results that are comparable to those from supervised\nbaselines trained with significantly more labels. We believe this novel\nrepurposing of GANs underlies a new class of unsupervised representation\nlearning that is applicable to many other tasks. More results are available at\nhttps://repurposegans.github.io/.",
          "link": "http://arxiv.org/abs/2103.04379",
          "publishedOn": "2021-07-06T01:58:11.457Z",
          "wordCount": 648,
          "title": "Repurposing GANs for One-shot Semantic Part Segmentation. (arXiv:2103.04379v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Sentiment analysis is a costly yet necessary task for enterprises to study\nthe opinions of their customers to improve their products and to determine\noptimal marketing strategies. Due to the existence of a wide range of domains\nacross different products and services, cross-domain sentiment analysis methods\nhave received significant attention. These methods mitigate the domain gap\nbetween different applications by training cross-domain generalizable\nclassifiers which help to relax the need for data annotation for each domain.\nMost existing methods focus on learning domain-agnostic representations that\nare invariant with respect to both the source and the target domains. As a\nresult, a classifier that is trained using the source domain annotated data\nwould generalize well in a related target domain. We introduce a new domain\nadaptation method which induces large margins between different classes in an\nembedding space. This embedding space is trained to be domain-agnostic by\nmatching the data distributions across the domains. Large intraclass margins in\nthe source domain help to reduce the effect of \"domain shift\" on the classifier\nperformance in the target domain. Theoretical and empirical analysis are\nprovided to demonstrate that the proposed method is effective.",
          "link": "http://arxiv.org/abs/2107.01598",
          "publishedOn": "2021-07-06T01:58:11.448Z",
          "wordCount": 624,
          "title": "Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation. (arXiv:2107.01598v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>",
          "description": "We propose a cross-modal transformer-based neural correction models that\nrefines the output of an automatic speech recognition (ASR) system so as to\nexclude ASR errors. Generally, neural correction models are composed of\nencoder-decoder networks, which can directly model sequence-to-sequence mapping\nproblems. The most successful method is to use both input speech and its ASR\noutput text as the input contexts for the encoder-decoder networks. However,\nthe conventional method cannot take into account the relationships between\nthese two different modal inputs because the input contexts are separately\nencoded for each modal. To effectively leverage the correlated information\nbetween the two different modal inputs, our proposed models encode two\ndifferent contexts jointly on the basis of cross-modal self-attention using a\ntransformer. We expect that cross-modal self-attention can effectively capture\nthe relationships between two different modals for refining ASR hypotheses. We\nalso introduce a shallow fusion technique to efficiently integrate the\nfirst-pass ASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed models\nachieve better ASR performance than conventional neural correction models.",
          "link": "http://arxiv.org/abs/2107.01569",
          "publishedOn": "2021-07-06T01:58:11.443Z",
          "wordCount": 629,
          "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition. (arXiv:2107.01569v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achddou_J/0/1/0/all/0/1\">Juliette Achddou</a> (PSL, DI-ENS, VALDA ), <a href=\"http://arxiv.org/find/cs/1/au:+Cappe_O/0/1/0/all/0/1\">Olivier Capp&#xe9;</a> (LTCI, VALDA ), <a href=\"http://arxiv.org/find/cs/1/au:+Garivier_A/0/1/0/all/0/1\">Aur&#xe9;lien Garivier</a> (UMPA-ENSL)",
          "description": "First-price auctions have largely replaced traditional bidding approaches\nbased on Vickrey auctions in programmatic advertising. As far as learning is\nconcerned, first-price auctions are more challenging because the optimal\nbidding strategy does not only depend on the value of the item but also\nrequires some knowledge of the other bids. They have already given rise to\nseveral works in sequential learning, many of which consider models for which\nthe value of the buyer or the opponents' maximal bid is chosen in an\nadversarial manner. Even in the simplest settings, this gives rise to\nalgorithms whose regret grows as $\\sqrt{T}$ with respect to the time horizon\n$T$. Focusing on the case where the buyer plays against a stationary stochastic\nenvironment, we show how to achieve significantly lower regret: when the\nopponents' maximal bid distribution is known we provide an algorithm whose\nregret can be as low as $\\log^2(T)$; in the case where the distribution must be\nlearnt sequentially, a generalization of this algorithm can achieve $T^{1/3+\n\\epsilon}$ regret, for any $\\epsilon>0$. To obtain these results, we introduce\ntwo novel ideas that can be of interest in their own right. First, by\ntransposing results obtained in the posted price setting, we provide conditions\nunder which the first-price biding utility is locally quadratic around its\noptimum. Second, we leverage the observation that, on small sub-intervals, the\nconcentration of the variations of the empirical distribution function may be\ncontrolled more accurately than by using the classical\nDvoretzky-Kiefer-Wolfowitz inequality. Numerical simulations confirm that our\nalgorithms converge much faster than alternatives proposed in the literature\nfor various bid distributions, including for bids collected on an actual\nprogrammatic advertising platform.",
          "link": "http://arxiv.org/abs/2107.01835",
          "publishedOn": "2021-07-06T01:58:11.426Z",
          "wordCount": 717,
          "title": "Fast Rate Learning in Stochastic First Price Bidding. (arXiv:2107.01835v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Zikun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>",
          "description": "Query reformulation aims to alter noisy or ambiguous text sequences into\ncoherent ones closer to natural language questions. This is to prevent errors\nfrom propagating in a client-facing pipeline and promote better communication\nwith users. Besides, it is crucial to maintain performance in downstream\nenvironments like question answering when rephrased queries are given as input.\nWe show that under the previous framework (AQA), attempts to alter RL\nalgorithms do not bring significant benefits to either reward acquisition or\nsequence fluency. Instead, we leverage a query-reformulating text-to-text\ntransformer (QRT5) and apply policy-based RL algorithms to further nudge this\nreformulator and obtain better answers downstream by generating\nreward-acquiring query trajectories. QRT5 shows better sample efficiency in RL\nto achieve the same level of QA performance as the previous approach. It can\ngenerate reformulations with more readability based on query well-formedness\nevaluations and can generalize to out-of-sample data. Our framework is\ndemonstrated to be flexible, allowing reward signals to be sourced from\ndifferent downstream environments such as intent classification.",
          "link": "http://arxiv.org/abs/2012.10033",
          "publishedOn": "2021-07-06T01:58:11.419Z",
          "wordCount": 649,
          "title": "Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning. (arXiv:2012.10033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruofan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaofu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Leilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_T/0/1/0/all/0/1\">Tao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuan Qi</a>",
          "description": "Deep learning provides a promising way to extract effective representations\nfrom raw data in an end-to-end fashion and has proven its effectiveness in\nvarious domains such as computer vision, natural language processing, etc.\nHowever, in domains such as content/product recommendation and risk management,\nwhere sequence of event data is the most used raw data form and experts derived\nfeatures are more commonly used, deep learning models struggle to dominate the\ngame. In this paper, we propose a symbolic testing framework that helps to\nanswer the question of what kinds of expert-derived features could be learned\nby a neural network. Inspired by this testing framework, we introduce an\nefficient architecture named SHORING, which contains two components:\n\\textit{event network} and \\textit{sequence network}. The \\textit{event}\nnetwork learns arbitrarily yet efficiently high-order \\textit{event-level}\nembeddings via a provable reparameterization trick, the \\textit{sequence}\nnetwork aggregates from sequence of \\textit{event-level} embeddings. We argue\nthat SHORING is capable of learning certain standard symbolic expressions which\nthe standard multi-head self-attention network fails to learn, and conduct\ncomprehensive experiments and ablation studies on four synthetic datasets and\nthree real-world datasets. The results show that SHORING empirically\noutperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.01326",
          "publishedOn": "2021-07-06T01:58:11.409Z",
          "wordCount": 647,
          "title": "SHORING: Design Provable Conditional High-Order Interaction Network via Symbolic Testing. (arXiv:2107.01326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1\">Max Simchowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosh_C/0/1/0/all/0/1\">Christopher Tosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">Daniel Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1\">Thodoris Lykouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1\">Miroslav Dud&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1\">Robert E. Schapire</a>",
          "description": "Thompson sampling and other Bayesian sequential decision-making algorithms\nare among the most popular approaches to tackle explore/exploit trade-offs in\n(contextual) bandits. The choice of prior in these algorithms offers\nflexibility to encode domain knowledge but can also lead to poor performance\nwhen misspecified. In this paper, we demonstrate that performance degrades\ngracefully with misspecification. We prove that the expected reward accrued by\nThompson sampling (TS) with a misspecified prior differs by at most\n$\\tilde{\\mathcal{O}}(H^2 \\epsilon)$ from TS with a well specified prior, where\n$\\epsilon$ is the total-variation distance between priors and $H$ is the\nlearning horizon. Our bound does not require the prior to have any parametric\nform. For priors with bounded support, our bound is independent of the\ncardinality or structure of the action space, and we show that it is tight up\nto universal constants in the worst case.\n\nBuilding on our sensitivity analysis, we establish generic PAC guarantees for\nalgorithms in the recently studied Bayesian meta-learning setting and derive\ncorollaries for various families of priors. Our results generalize along two\naxes: (1) they apply to a broader family of Bayesian decision-making\nalgorithms, including a Monte-Carlo implementation of the knowledge gradient\nalgorithm (KG), and (2) they apply to Bayesian POMDPs, the most general\nBayesian decision-making setting, encompassing contextual bandits as a special\ncase. Through numerical simulations, we illustrate how prior misspecification\nand the deployment of one-step look-ahead (as in KG) can impact the convergence\nof meta-learning in multi-armed and contextual bandits with structured and\ncorrelated priors.",
          "link": "http://arxiv.org/abs/2107.01509",
          "publishedOn": "2021-07-06T01:58:11.403Z",
          "wordCount": 698,
          "title": "Bayesian decision-making under misspecified priors with applications to meta-learning. (arXiv:2107.01509v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01333",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuyan Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1\">Peter Spirtes</a>",
          "description": "Kalisch and B\\\"{u}hlmann (2007) showed that for linear Gaussian models, under\nthe Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and\nthe assumption of causal sufficiency, the PC algorithm is a uniformly\nconsistent estimator of the Markov Equivalence Class of the true causal DAG for\nlinear Gaussian models; it follows from this that for the identifiable causal\neffects in the Markov Equivalence Class, there are uniformly consistent\nestimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption\nis a strictly weaker assumption that avoids some implausible implications of\nthe Strong Causal Faithfulness Assumption and also allows for uniformly\nconsistent estimates of Markov Equivalence Classes (in a weakened sense), and\nof identifiable causal effects. However, both of these assumptions are\nrestricted to linear Gaussian models. We propose the Generalized $k$-Triangle\nFaithfulness, which can be applied to any smooth distribution. In addition,\nunder the Generalized $k$-Triangle Faithfulness Assumption, we describe the\nEdge Estimation Algorithm that provides uniformly consistent estimates of\ncausal effects in some cases (and otherwise outputs \"can't tell\"), and the\n\\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is\na uniformly consistent estimator of the Markov equivalence class of the true\nDAG.",
          "link": "http://arxiv.org/abs/2107.01333",
          "publishedOn": "2021-07-06T01:58:11.386Z",
          "wordCount": 634,
          "title": "A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leporowski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Leporowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tola_D/0/1/0/all/0/1\">Daniella Tola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Detecting faults in manufacturing applications can be difficult, especially\nif each fault model is to be engineered by hand. Data-driven approaches, using\nMachine Learning (ML) for detecting faults have recently gained increasing\ninterest, where a ML model can be trained on a set of data from a manufacturing\nprocess. In this paper, we present a use case of using ML models for detecting\nfaults during automated screwdriving operations, and introduce a new dataset\ncontaining fully monitored and registered data from a Universal Robot and\nOnRobot screwdriver during both normal and anomalous operations. We illustrate,\nwith the use of two time-series ML models, how to detect faults in an automated\nscrewdriving application.",
          "link": "http://arxiv.org/abs/2107.01955",
          "publishedOn": "2021-07-06T01:58:11.380Z",
          "wordCount": 561,
          "title": "Detecting Faults during Automatic Screwdriving: A Dataset and Use Case of Anomaly Detection for Automatic Screwdriving. (arXiv:2107.01955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eungyeup Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "Image classification models tend to make decisions based on peripheral\nattributes of data items that have strong correlation with a target variable\n(i.e., dataset bias). These biased models suffer from the poor generalization\ncapability when evaluated on unbiased datasets. Existing approaches for\ndebiasing often identify and emphasize those samples with no such correlation\n(i.e., bias-conflicting) without defining the bias type in advance. However,\nsuch bias-conflicting samples are significantly scarce in biased datasets,\nlimiting the debiasing capability of these approaches. This paper first\npresents an empirical analysis revealing that training with \"diverse\"\nbias-conflicting samples beyond a given training set is crucial for debiasing\nas well as the generalization capability. Based on this observation, we propose\na novel feature-level data augmentation technique in order to synthesize\ndiverse bias-conflicting samples. To this end, our method learns the\ndisentangled representation of (1) the intrinsic attributes (i.e., those\ninherently defining a certain class) and (2) bias attributes (i.e., peripheral\nattributes causing the bias), from a large number of bias-aligned samples, the\nbias attributes of which have strong correlation with the target variable.\nUsing the disentangled representation, we synthesize bias-conflicting samples\nthat contain the diverse intrinsic attributes of bias-aligned samples by\nswapping their latent features. By utilizing these diversified bias-conflicting\nfeatures during the training, our approach achieves superior classification\naccuracy and debiasing results against the existing baselines on both synthetic\nas well as real-world datasets.",
          "link": "http://arxiv.org/abs/2107.01372",
          "publishedOn": "2021-07-06T01:58:11.374Z",
          "wordCount": 658,
          "title": "Learning Debiased Representation via Disentangled Feature Augmentation. (arXiv:2107.01372v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Hamamsy_L/0/1/0/all/0/1\">Laila El-Hamamsy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaspyros_V/0/1/0/all/0/1\">Vaios Papaspyros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangur_T/0/1/0/all/0/1\">Taavet Kangur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathex_L/0/1/0/all/0/1\">Laura Mathex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giang_C/0/1/0/all/0/1\">Christian Giang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skweres_M/0/1/0/all/0/1\">Melissa Skweres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_B/0/1/0/all/0/1\">Barbara Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondada_F/0/1/0/all/0/1\">Francesco Mondada</a>",
          "description": "Recently, introducing computer science and educational robots in compulsory\neducation has received increasing attention. However, the use of screens in\nclassrooms is often met with resistance, especially in primary school. To\naddress this issue, this study presents the development of a handwriting-based\nprogramming language for educational robots. Aiming to align better with\nexisting classroom practices, it allows students to program a robot by drawing\nsymbols with ordinary pens and paper. Regular smartphones are leveraged to\nprocess the hand-drawn instructions using computer vision and machine learning\nalgorithms, and send the commands to the robot for execution. To align with the\nlocal computer science curriculum, an appropriate playground and scaffolded\nlearning tasks were designed. The system was evaluated in a preliminary test\nwith eight teachers, developers and educational researchers. While the\nparticipants pointed out that some technical aspects could be improved, they\nalso acknowledged the potential of the approach to make computer science\neducation in primary school more accessible.",
          "link": "http://arxiv.org/abs/2105.04963",
          "publishedOn": "2021-07-06T01:58:11.367Z",
          "wordCount": 651,
          "title": "Exploring a Handwriting Programming Language for Educational Robots. (arXiv:2105.04963v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Graph learning has emerged as a promising technique for multi-view clustering\nwith its ability to learn a unified and robust graph from multiple views.\nHowever, existing graph learning methods mostly focus on the multi-view\nconsistency issue, yet often neglect the inconsistency across multiple views,\nwhich makes them vulnerable to possibly low-quality or noisy datasets. To\novercome this limitation, we propose a new multi-view graph learning framework,\nwhich for the first time simultaneously and explicitly models multi-view\nconsistency and multi-view inconsistency in a unified objective function,\nthrough which the consistent and inconsistent parts of each single-view graph\nas well as the unified graph that fuses the consistent parts can be iteratively\nlearned. Though optimizing the objective function is NP-hard, we design a\nhighly efficient optimization algorithm which is able to obtain an approximate\nsolution with linear time complexity in the number of edges in the unified\ngraph. Furthermore, our multi-view graph learning approach can be applied to\nboth similarity graphs and dissimilarity graphs, which lead to two graph\nfusion-based variants in our framework. Experiments on twelve multi-view\ndatasets have demonstrated the robustness and efficiency of the proposed\napproach.",
          "link": "http://arxiv.org/abs/2008.10208",
          "publishedOn": "2021-07-06T01:58:11.361Z",
          "wordCount": 671,
          "title": "Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency. (arXiv:2008.10208v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Despite their overwhelming capacity to overfit, deep neural networks trained\nby specific optimization algorithms tend to generalize well to unseen data.\nRecently, researchers explained it by investigating the implicit regularization\neffect of optimization algorithms. A remarkable progress is the work (Lyu&Li,\n2019), which proves gradient descent (GD) maximizes the margin of homogeneous\ndeep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp\nand Adam are popular owing to their rapid training process. However,\ntheoretical guarantee for the generalization of adaptive optimization\nalgorithms is still lacking. In this paper, we study the implicit\nregularization of adaptive optimization algorithms when they are optimizing the\nlogistic loss on homogeneous deep neural networks. We prove that adaptive\nalgorithms that adopt exponential moving average strategy in conditioner (such\nas Adam and RMSProp) can maximize the margin of the neural network, while\nAdaGrad that directly sums historical squared gradients in conditioner can not.\nIt indicates superiority on generalization of exponential moving average\nstrategy in the design of the conditioner. Technically, we provide a unified\nframework to analyze convergent direction of adaptive optimization algorithms\nby constructing novel adaptive gradient flow and surrogate margin. Our\nexperiments can well support the theoretical findings on convergent direction\nof adaptive optimization algorithms.",
          "link": "http://arxiv.org/abs/2012.06244",
          "publishedOn": "2021-07-06T01:58:11.354Z",
          "wordCount": 676,
          "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks. (arXiv:2012.06244v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>",
          "description": "Motivated by the recent discovery that the interpretation maps of CNNs could\neasily be manipulated by adversarial attacks against network interpretability,\nwe study the problem of interpretation robustness from a new perspective of\n\\Renyi differential privacy (RDP). The advantages of our Renyi-Robust-Smooth\n(RDP-based interpretation method) are three-folds. First, it can offer provable\nand certifiable top-$k$ robustness. That is, the top-$k$ important attributions\nof the interpretation map are provably robust under any input perturbation with\nbounded $\\ell_d$-norm (for any $d\\geq 1$, including $d = \\infty$). Second, our\nproposed method offers $\\sim10\\%$ better experimental robustness than existing\napproaches in terms of the top-$k$ attributions. Remarkably, the accuracy of\nRenyi-Robust-Smooth also outperforms existing approaches. Third, our method can\nprovide a smooth tradeoff between robustness and computational efficiency.\nExperimentally, its top-$k$ attributions are {\\em twice} more robust than\nexisting approaches when the computational resources are highly constrained.",
          "link": "http://arxiv.org/abs/2107.01561",
          "publishedOn": "2021-07-06T01:58:11.337Z",
          "wordCount": 585,
          "title": "Certifiably Robust Interpretation via Renyi Differential Privacy. (arXiv:2107.01561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afchar_D/0/1/0/all/0/1\">Darius Afchar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>",
          "description": "Feature attribution is often loosely presented as the process of selecting a\nsubset of relevant features as a rationale of a prediction. Task-dependent by\nnature, precise definitions of \"relevance\" encountered in the literature are\nhowever not always consistent. This lack of clarity stems from the fact that we\nusually do not have access to any notion of ground-truth attribution and from a\nmore general debate on what good interpretations are. In this paper we propose\nto formalise feature selection/attribution based on the concept of relaxed\nfunctional dependence. In particular, we extend our notions to the\ninstance-wise setting and derive necessary properties for candidate selection\nsolutions, while leaving room for task-dependence. By computing ground-truth\nattributions on synthetic datasets, we evaluate many state-of-the-art\nattribution methods and show that, even when optimised, some fail to verify the\nproposed properties and provide wrong solutions.",
          "link": "http://arxiv.org/abs/2104.12437",
          "publishedOn": "2021-07-06T01:58:11.330Z",
          "wordCount": 609,
          "title": "Towards Rigorous Interpretations: a Formalisation of Feature Attribution. (arXiv:2104.12437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaitang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.",
          "link": "http://arxiv.org/abs/2010.04389",
          "publishedOn": "2021-07-06T01:58:11.323Z",
          "wordCount": 660,
          "title": "A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.01489",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_C/0/1/0/all/0/1\">Ciaran Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horgan_J/0/1/0/all/0/1\">Jonathan Horgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varley_P/0/1/0/all/0/1\">Padraig Varley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODea_D/0/1/0/all/0/1\">Derek O&#x27;Dea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal Uricar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1\">Stefan Milz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1\">Martin Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amende_K/0/1/0/all/0/1\">Karl Amende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1\">Christian Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sanjaya Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_S/0/1/0/all/0/1\">Saquib Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perroton_X/0/1/0/all/0/1\">Xavier Perroton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick Perez</a>",
          "description": "Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.",
          "link": "http://arxiv.org/abs/1905.01489",
          "publishedOn": "2021-07-06T01:58:11.313Z",
          "wordCount": 681,
          "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving. (arXiv:1905.01489v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingliang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zhenhua Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Daren Yu</a>",
          "description": "Solar energy is a clean and renewable energy. Photovoltaic (PV) power is an\nimportant way to utilize solar energy. Accurate PV power forecast is crucial to\nthe large-scale application of PV power and the stability of electricity grid.\nThis paper proposes a novel method for short-term photovoltaic power forecast\nusing deep convolutional long short-term memory (ConvLSTM) network and kernel\ndensity estimation (KDE). In the proposed method, ConvLSTM is used to forecast\nthe future photovoltaic power and KDE is used for estimating the joint\nprobabilistic density function and giving the probabilistic confidence\ninterval. Experiments in an actual photovoltaic power station verify the\neffectiveness of the proposed method. Comparison experiments with convolutional\nneural network (CNN) and long short-term memory network (LSTM)shows that\nConvLSTM can combine the advantages of both CNN and LSTM and significantly\noutperform CNN and LSTM in terms of forecast accuracy. Through further\ncomparison with other five conventional methods including multilayer perceptron\n(MLP), support vector regression (SVR), extreme learning machine (ELM),\nclassification and regression tree (CART) and gradient boosting decision tree\n(GBDT), ConvLSTM can significantly improve the forecast accuracy by more than\n20% for most of the five methods and the superiorities of ConvLSTM are further\nverified.",
          "link": "http://arxiv.org/abs/2107.01343",
          "publishedOn": "2021-07-06T01:58:11.294Z",
          "wordCount": 651,
          "title": "Short-term probabilistic photovoltaic power forecast based on deep convolutional long short-term memory network and kernel density estimation. (arXiv:2107.01343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01466",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhenyu Yuan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_H/0/1/0/all/0/1\">Handong Huang</a>",
          "description": "Fractures are widely developed in hydrocarbon reservoirs and constitute the\naccumulation spaces and transport channels of oil and gas. Fracture detection\nis a fundamental task for reservoir characterization. From prestack seismic\ngathers, anisotropic analysis and inversion were commonly applied to\ncharacterize the dominant orientations and relative intensities of fractures.\nHowever, the existing methods were mostly based on the vertical aligned facture\nhypothesis, it is impossible for them to recognize fracture dip. Furthermore,\nit is difficult or impractical for existing methods to attain the real fracture\ndensities. Based on data-driven deep learning, this paper designed a\nconvolutional neural network to perform prestack fracture detection.\nCapitalizing on the connections between seismic responses and fracture\nparameters, a suitable azimuth dataset was firstly generated through fracture\neffective medium modeling and anisotropic plane wave analyzing. Then a\nmulti-input and multi-output convolutional neural network was constructed to\nsimultaneously detect fracture density, dip and strike azimuth. The application\non a practical survey validated the effectiveness of the proposed CNN model.",
          "link": "http://arxiv.org/abs/2107.01466",
          "publishedOn": "2021-07-06T01:58:11.288Z",
          "wordCount": 598,
          "title": "A convolutional neural network for prestack fracture detection. (arXiv:2107.01466v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bouritsas_G/0/1/0/all/0/1\">Giorgos Bouritsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1\">Andreas Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karalias_N/0/1/0/all/0/1\">Nikolaos Karalias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael M. Bronstein</a>",
          "description": "Can we use machine learning to compress graph data? The absence of ordering\nin graphs poses a significant challenge to conventional compression algorithms,\nlimiting their attainable gains as well as their ability to discover relevant\npatterns. On the other hand, most graph compression approaches rely on\ndomain-dependent handcrafted representations and cannot adapt to different\nunderlying graph distributions. This work aims to establish the necessary\nprinciples a lossless graph compression method should follow to approach the\nentropy storage lower bound. Instead of making rigid assumptions about the\ngraph distribution, we formulate the compressor as a probabilistic model that\ncan be learned from data and generalise to unseen instances. Our \"Partition and\nCode\" framework entails three steps: first, a partitioning algorithm decomposes\nthe graph into elementary structures, then these are mapped to the elements of\na small dictionary on which we learn a probability distribution, and finally,\nan entropy encoder translates the representation into bits. All three steps are\nparametric and can be trained with gradient descent. We theoretically compare\nthe compression quality of several graph encodings and prove, under mild\nconditions, a total ordering of their expected description lengths. Moreover,\nwe show that, under the same conditions, PnC achieves compression gains w.r.t.\nthe baselines that grow either linearly or quadratically with the number of\nvertices. Our algorithms are quantitatively evaluated on diverse real-world\nnetworks obtaining significant performance improvements with respect to\ndifferent families of non-parametric and parametric graph compressors.",
          "link": "http://arxiv.org/abs/2107.01952",
          "publishedOn": "2021-07-06T01:58:11.280Z",
          "wordCount": 686,
          "title": "Partition and Code: learning how to compress graphs. (arXiv:2107.01952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>",
          "description": "Recent studies assessing the efficacy of pruning neural networks methods\nuncovered a surprising finding: when conducting ablation studies on existing\npruning-at-initialization methods, namely SNIP, GraSP, SynFlow, and magnitude\npruning, performances of these methods remain unchanged and sometimes even\nimprove when randomly shuffling the mask positions within each layer (Layerwise\nShuffling) or sampling new initial weight values (Reinit), while keeping\npruning masks the same. We attempt to understand the reason behind such network\nimmunity towards weight/mask modifications, by studying layer-wise statistics\nbefore and after randomization operations. We found that under each of the\npruning-at-initialization methods, the distribution of unpruned weights changed\nminimally with randomization operations.",
          "link": "http://arxiv.org/abs/2107.01808",
          "publishedOn": "2021-07-06T01:58:11.274Z",
          "wordCount": 542,
          "title": "Why is Pruning at Initialization Immune to Reinitializing and Shuffling?. (arXiv:2107.01808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amason_J/0/1/0/all/0/1\">Joshua D. Amason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1\">Ricardo Henao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadziahmetovic_M/0/1/0/all/0/1\">Majda Hadziahmetovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1\">Miroslav Pajic</a>",
          "description": "Although recent works have developed methods that can generate estimations\n(or imputations) of the missing entries in a dataset to facilitate downstream\nanalysis, most depend on assumptions that may not align with real-world\napplications and could suffer from poor performance in subsequent tasks. This\nis particularly true if the data have large missingness rates or a small\npopulation. More importantly, the imputation error could be propagated into the\nprediction step that follows, causing the gradients used to train the\nprediction models to be biased. Consequently, in this work, we introduce the\nimportance guided stochastic gradient descent (IGSGD) method to train\nmultilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly\nperform inference from inputs containing missing values without imputation.\nSpecifically, we employ reinforcement learning (RL) to adjust the gradients\nused to train the models via back-propagation. This not only reduces bias but\nallows the model to exploit the underlying information behind missingness\npatterns. We test the proposed approach on real-world time-series (i.e.,\nMIMIC-III), tabular data obtained from an eye clinic, and a standard dataset\n(i.e., MNIST), where our imputation-free predictions outperform the traditional\ntwo-step imputation-based predictions using state-of-the-art imputation\nmethods.",
          "link": "http://arxiv.org/abs/2107.01983",
          "publishedOn": "2021-07-06T01:58:11.267Z",
          "wordCount": 628,
          "title": "Imputation-Free Learning from Incomplete Observations. (arXiv:2107.01983v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hadikhanloo_S/0/1/0/all/0/1\">Saeed Hadikhanloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laraki_R/0/1/0/all/0/1\">Rida Laraki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorin_S/0/1/0/all/0/1\">Sylvain Sorin</a>",
          "description": "We examine the long-run behavior of a wide range of dynamics for learning in\nnonatomic games, in both discrete and continuous time. The class of dynamics\nunder consideration includes fictitious play and its regularized variants, the\nbest-reply dynamics (again, possibly regularized), as well as the dynamics of\ndual averaging / \"follow the regularized leader\" (which themselves include as\nspecial cases the replicator dynamics and Friedman's projection dynamics). Our\nanalysis concerns both the actual trajectory of play and its time-average, and\nwe cover potential and monotone games, as well as games with an evolutionarily\nstable state (global or otherwise). We focus exclusively on games with finite\naction spaces; nonatomic games with continuous action spaces are treated in\ndetail in Part II of this paper.",
          "link": "http://arxiv.org/abs/2107.01595",
          "publishedOn": "2021-07-06T01:58:11.260Z",
          "wordCount": 589,
          "title": "Learning in nonatomic games, Part I: Finite action spaces and population games. (arXiv:2107.01595v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lijffijt_J/0/1/0/all/0/1\">Jefrey Lijffijt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>",
          "description": "In today's networked society, many real-world problems can be formalized as\npredicting links in networks, such as Facebook friendship suggestions,\ne-commerce recommendations, and the prediction of scientific collaborations in\ncitation networks. Increasingly often, link prediction problem is tackled by\nmeans of network embedding methods, owing to their state-of-the-art\nperformance. However, these methods lack transparency when compared to simpler\nbaselines, and as a result their robustness against adversarial attacks is a\npossible point of concern: could one or a few small adversarial modifications\nto the network have a large impact on the link prediction performance when\nusing a network embedding model? Prior research has already investigated\nadversarial robustness for network embedding models, focused on classification\nat the node and graph level. Robustness with respect to the link prediction\ndownstream task, on the other hand, has been explored much less.\n\nThis paper contributes to filling this gap, by studying adversarial\nrobustness of Conditional Network Embedding (CNE), a state-of-the-art\nprobabilistic network embedding model, for link prediction. More specifically,\ngiven CNE and a network, we measure the sensitivity of the link predictions of\nthe model to small adversarial perturbations of the network, namely changes of\nthe link status of a node pair. Thus, our approach allows one to identify the\nlinks and non-links in the network that are most vulnerable to such\nperturbations, for further investigation by an analyst. We analyze the\ncharacteristics of the most and least sensitive perturbations, and empirically\nconfirm that our approach not only succeeds in identifying the most vulnerable\nlinks and non-links, but also that it does so in a time-efficient manner thanks\nto an effective approximation.",
          "link": "http://arxiv.org/abs/2107.01936",
          "publishedOn": "2021-07-06T01:58:11.252Z",
          "wordCount": 709,
          "title": "Adversarial Robustness of Probabilistic Network Embedding for Link Prediction. (arXiv:2107.01936v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lanqing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Duocai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.",
          "link": "http://arxiv.org/abs/2107.01875",
          "publishedOn": "2021-07-06T01:58:11.245Z",
          "wordCount": 636,
          "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling. (arXiv:2107.01875v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hakbin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "In spite of the great success of deep learning technologies, training and\ndelivery of a practically serviceable model is still a highly time-consuming\nprocess. Furthermore, a resulting model is usually too generic and heavyweight,\nand hence essentially goes through another expensive model compression phase to\nfit in a resource-limited device like embedded systems. Inspired by the fact\nthat a machine learning task specifically requested by mobile users is often\nmuch simpler than it is supported by a massive generic model, this paper\nproposes a framework, called Pool of Experts (PoE), that instantly builds a\nlightweight and task-specific model without any training process. For a\nrealtime model querying service, PoE first extracts a pool of primitive\ncomponents, called experts, from a well-trained and sufficiently generic\nnetwork by exploiting a novel conditional knowledge distillation method, and\nthen performs our train-free knowledge consolidation to quickly combine\nnecessary experts into a lightweight network for a target task. Thanks to this\ntrain-free property, in our thorough empirical study, PoE can build a fairly\naccurate yet compact model in a realtime manner, whereas it takes a few minutes\nper query for the other training methods to achieve a similar level of the\naccuracy.",
          "link": "http://arxiv.org/abs/2107.01354",
          "publishedOn": "2021-07-06T01:58:11.205Z",
          "wordCount": 647,
          "title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks. (arXiv:2107.01354v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jinshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "As pairwise ranking becomes broadly employed for elections, sports\ncompetitions, recommendations, and so on, attackers have strong motivation and\nincentives to manipulate the ranking list. They could inject malicious\ncomparisons into the training data to fool the victim. Such a technique is\ncalled poisoning attack in regression and classification tasks. In this paper,\nto the best of our knowledge, we initiate the first systematic investigation of\ndata poisoning attacks on pairwise ranking algorithms, which can be formalized\nas the dynamic and static games between the ranker and the attacker and can be\nmodeled as certain kinds of integer programming problems. To break the\ncomputational hurdle of the underlying integer programming problems, we\nreformulate them into the distributionally robust optimization (DRO) problems,\nwhich are computationally tractable. Based on such DRO formulations, we propose\ntwo efficient poisoning attack algorithms and establish the associated\ntheoretical guarantees. The effectiveness of the suggested poisoning attack\nstrategies is demonstrated by a series of toy simulations and several real data\nexperiments. These experimental results show that the proposed methods can\nsignificantly reduce the performance of the ranker in the sense that the\ncorrelation between the true ranking list and the aggregated results can be\ndecreased dramatically.",
          "link": "http://arxiv.org/abs/2107.01854",
          "publishedOn": "2021-07-06T01:58:11.192Z",
          "wordCount": 649,
          "title": "Poisoning Attack against Estimating from Pairwise Comparisons. (arXiv:2107.01854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouranizadeh_A/0/1/0/all/0/1\">Amirhossein Nouranizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matinkia_M/0/1/0/all/0/1\">Mohammadjavad Matinkia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1\">Mohammad Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safabakhsh_R/0/1/0/all/0/1\">Reza Safabakhsh</a>",
          "description": "In this paper, we propose a novel pooling layer for graph neural networks\nbased on maximizing the mutual information between the pooled graph and the\ninput graph. Since the maximum mutual information is difficult to compute, we\nemploy the Shannon capacity of a graph as an inductive bias to our pooling\nmethod. More precisely, we show that the input graph to the pooling layer can\nbe viewed as a representation of a noisy communication channel. For such a\nchannel, sending the symbols belonging to an independent set of the graph\nyields a reliable and error-free transmission of information. We show that\nreaching the maximum mutual information is equivalent to finding a maximum\nweight independent set of the graph where the weights convey entropy contents.\nThrough this communication theoretic standpoint, we provide a distinct\nperspective for posing the problem of graph pooling as maximizing the\ninformation transmission rate across a noisy communication channel, implemented\nby a graph neural network. We evaluate our method, referred to as Maximum\nEntropy Weighted Independent Set Pooling (MEWISPool), on graph classification\ntasks and the combinatorial optimization problem of the maximum independent\nset. Empirical results demonstrate that our method achieves the\nstate-of-the-art and competitive results on graph classification tasks and the\nmaximum independent set problem in several benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.01410",
          "publishedOn": "2021-07-06T01:58:11.181Z",
          "wordCount": 679,
          "title": "Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks. (arXiv:2107.01410v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1\">Cyrus Rashtchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanlin Zhu</a>",
          "description": "We study statistical problems, such as planted clique, its variants, and\nsparse principal component analysis in the context of average-case\ncommunication complexity. Our motivation is to understand the\nstatistical-computational trade-offs in streaming, sketching, and query-based\nmodels. Communication complexity is the main tool for proving lower bounds in\nthese models, yet many prior results do not hold in an average-case setting. We\nprovide a general reduction method that preserves the input distribution for\nproblems involving a random graph or matrix with planted structure. Then, we\nderive two-party and multi-party communication lower bounds for detecting or\nfinding planted cliques, bipartite cliques, and related problems. As a\nconsequence, we obtain new bounds on the query complexity in the edge-probe,\nvector-matrix-vector, matrix-vector, linear sketching, and\n$\\mathbb{F}_2$-sketching models. Many of these results are nearly tight, and we\nuse our techniques to provide simple proofs of some known lower bounds for the\nedge-probe model.",
          "link": "http://arxiv.org/abs/2107.01335",
          "publishedOn": "2021-07-06T01:58:11.165Z",
          "wordCount": 594,
          "title": "Average-Case Communication Complexity of Statistical Problems. (arXiv:2107.01335v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1\">Philip Sellars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>",
          "description": "Can one learn to diagnose COVID-19 under extreme minimal supervision? Since\nthe outbreak of the novel COVID-19 there has been a rush for developing\nArtificial Intelligence techniques for expert-level disease identification on\nChest X-ray data. In particular, the use of deep supervised learning has become\nthe go-to paradigm. However, the performance of such models is heavily\ndependent on the availability of a large and representative labelled dataset.\nThe creation of which is a heavily expensive and time consuming task, and\nespecially imposes a great challenge for a novel disease. Semi-supervised\nlearning has shown the ability to match the incredible performance of\nsupervised models whilst requiring a small fraction of the labelled examples.\nThis makes the semi-supervised paradigm an attractive option for identifying\nCOVID-19. In this work, we introduce a graph based deep semi-supervised\nframework for classifying COVID-19 from chest X-rays. Our framework introduces\nan optimisation model for graph diffusion that reinforces the natural relation\namong the tiny labelled set and the vast unlabelled data. We then connect the\ndiffusion prediction output as pseudo-labels that are used in an iterative\nscheme in a deep net. We demonstrate, through our experiments, that our model\nis able to outperform the current leading supervised model with a tiny fraction\nof the labelled examples. Finally, we provide attention maps to accommodate the\nradiologist's mental model, better fitting their perceptual and cognitive\nabilities. These visualisation aims to assist the radiologist in judging\nwhether the diagnostic is correct or not, and in consequence to accelerate the\ndecision.",
          "link": "http://arxiv.org/abs/2010.00378",
          "publishedOn": "2021-07-06T01:58:11.145Z",
          "wordCount": 777,
          "title": "GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays. (arXiv:2010.00378v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01473",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Johansson_A/0/1/0/all/0/1\">Anton Johansson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Engsner_N/0/1/0/all/0/1\">Niklas Engsner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strannegaard_C/0/1/0/all/0/1\">Claes Stranneg&#xe5;rd</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mostad_P/0/1/0/all/0/1\">Petter Mostad</a>",
          "description": "Neural networks are very successful tools in for example advanced\nclassification. From a statistical point of view, fitting a neural network may\nbe seen as a kind of regression, where we seek a function from the input space\nto a space of classification probabilities that follows the \"general\" shape of\nthe data, but avoids overfitting by avoiding memorization of individual data\npoints. In statistics, this can be done by controlling the geometric complexity\nof the regression function. We propose to do something similar when fitting\nneural networks by controlling the slope of the network.\n\nAfter defining the slope and discussing some of its theoretical properties,\nwe go on to show empirically in examples, using ReLU networks, that the\ndistribution of the slope of a well-trained neural network classifier is\ngenerally independent of the width of the layers in a fully connected network,\nand that the mean of the distribution only has a weak dependence on the model\narchitecture in general. The slope is of similar size throughout the relevant\nvolume, and varies smoothly. It also behaves as predicted in rescaling\nexamples. We discuss possible applications of the slope concept, such as using\nit as a part of the loss function or stopping criterion during network\ntraining, or ranking data sets in terms of their complexity.",
          "link": "http://arxiv.org/abs/2107.01473",
          "publishedOn": "2021-07-06T01:58:11.112Z",
          "wordCount": 646,
          "title": "Slope and generalization properties of neural networks. (arXiv:2107.01473v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S. M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-07-06T01:58:11.104Z",
          "wordCount": 641,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Weidong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1\">Qi Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tijin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuanqing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Weipeng Cao</a>",
          "description": "Adaptive optimization methods have been widely used in deep learning. They\nscale the learning rates adaptively according to the past gradient, which has\nbeen shown to be effective to accelerate the convergence. However, they suffer\nfrom poor generalization performance compared with SGD. Recent studies point\nthat smoothing exponential gradient noise leads to generalization degeneration\nphenomenon. Inspired by this, we propose AdaL, with a transformation on the\noriginal gradient. AdaL accelerates the convergence by amplifying the gradient\nin the early stage, as well as dampens the oscillation and stabilizes the\noptimization by shrinking the gradient later. Such modification alleviates the\nsmoothness of gradient noise, which produces better generalization performance.\nWe have theoretically proved the convergence of AdaL and demonstrated its\neffectiveness on several benchmarks.",
          "link": "http://arxiv.org/abs/2107.01525",
          "publishedOn": "2021-07-06T01:58:11.068Z",
          "wordCount": 565,
          "title": "AdaL: Adaptive Gradient Transformation Contributes to Convergences and Generalizations. (arXiv:2107.01525v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1\">Luigi Penco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1\">Jean-Baptiste Mouret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1\">Serena Ivaldi</a>",
          "description": "Humanoid robots could be versatile and intuitive human avatars that operate\nremotely in inaccessible places: the robot could reproduce in the remote\nlocation the movements of an operator equipped with a wearable motion capture\ndevice while sending visual feedback to the operator. While substantial\nprogress has been made on transferring (\"retargeting\") human motions to\nhumanoid robots, a major problem preventing the deployment of such systems in\nreal applications is the presence of communication delays between the human\ninput and the feedback from the robot: even a few hundred milliseconds of delay\ncan irreversibly disturb the operator, let alone a few seconds. To overcome\nthese delays, we introduce a system in which a humanoid robot executes commands\nbefore it actually receives them, so that the visual feedback appears to be\nsynchronized to the operator, whereas the robot executed the commands in the\npast. To do so, the robot continuously predicts future commands by querying a\nmachine learning model that is trained on past trajectories and conditioned on\nthe last received commands. In our experiments, an operator was able to\nsuccessfully control a humanoid robot (32 degrees of freedom) with stochastic\ndelays up to 2 seconds in several whole-body manipulation tasks, including\nreaching different targets, picking up, and placing a box at distinct\nlocations.",
          "link": "http://arxiv.org/abs/2107.01281",
          "publishedOn": "2021-07-06T01:58:11.040Z",
          "wordCount": 641,
          "title": "Prescient teleoperation of humanoid robots. (arXiv:2107.01281v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06188",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Danilova_M/0/1/0/all/0/1\">Marina Danilova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1\">Pavel Dvurechensky</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Guminov_S/0/1/0/all/0/1\">Sergey Guminov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kamzolov_D/0/1/0/all/0/1\">Dmitry Kamzolov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Shibaev_I/0/1/0/all/0/1\">Innokentiy Shibaev</a>",
          "description": "Motivated by recent increased interest in optimization algorithms for\nnon-convex optimization in application to training deep neural networks and\nother optimization problems in data analysis, we give an overview of recent\ntheoretical results on global performance guarantees of optimization algorithms\nfor non-convex optimization. We start with classical arguments showing that\ngeneral non-convex problems could not be solved efficiently in a reasonable\ntime. Then we give a list of problems that can be solved efficiently to find\nthe global minimizer by exploiting the structure of the problem as much as it\nis possible. Another way to deal with non-convexity is to relax the goal from\nfinding the global minimum to finding a stationary point or a local minimum.\nFor this setting, we first present known results for the convergence rates of\ndeterministic first-order methods, which are then followed by a general\ntheoretical analysis of optimal stochastic and randomized gradient schemes, and\nan overview of the stochastic first-order methods. After that, we discuss quite\ngeneral classes of non-convex problems, such as minimization of\n$\\alpha$-weakly-quasi-convex functions and functions that satisfy\nPolyak--Lojasiewicz condition, which still allow obtaining theoretical\nconvergence guarantees of first-order methods. Then we consider higher-order\nand zeroth-order/derivative-free methods and their convergence rates for\nnon-convex optimization problems.",
          "link": "http://arxiv.org/abs/2012.06188",
          "publishedOn": "2021-07-06T01:58:11.018Z",
          "wordCount": 663,
          "title": "Recent Theoretical Advances in Non-Convex Optimization. (arXiv:2012.06188v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.09650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shaojun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haomin Zhou</a>",
          "description": "Inverse optimal transport (OT) refers to the problem of learning the cost\nfunction for OT from observed transport plan or its samples. In this paper, we\nderive an unconstrained convex optimization formulation of the inverse OT\nproblem, which can be further augmented by any customizable regularization. We\nprovide a comprehensive characterization of the properties of inverse OT,\nincluding uniqueness of solutions. We also develop two numerical algorithms,\none is a fast matrix scaling method based on the Sinkhorn-Knopp algorithm for\ndiscrete OT, and the other one is a learning based algorithm that parameterizes\nthe cost function as a deep neural network for continuous OT. The novel\nframework proposed in the work avoids repeatedly solving a forward OT in each\niteration which has been a thorny computational bottleneck for the bi-level\noptimization in existing inverse OT approaches. Numerical results demonstrate\npromising efficiency and accuracy advantages of the proposed algorithms over\nexisting state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2002.09650",
          "publishedOn": "2021-07-06T01:58:11.002Z",
          "wordCount": 613,
          "title": "Learning Cost Functions for Optimal Transport. (arXiv:2002.09650v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Suman Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We present an approach for encoding visual task relationships to improve\nmodel performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic\nsegmentation and monocular depth estimation are shown to be complementary\ntasks; in a multi-task learning setting, a proper encoding of their\nrelationships can further improve performance on both tasks. Motivated by this\nobservation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes\ntask dependencies between the semantic and depth predictions. To capture the\ncross-task relationships, we propose a neural network architecture that\ncontains task-specific and cross-task refinement heads. Furthermore, we propose\nan Iterative Self-Learning (ISL) training scheme, which exploits semantic\npseudo-labels to provide extra supervision on the target domain. We\nexperimentally observe improvements in both tasks' performance because the\ncomplementary information present in these tasks is better captured.\nSpecifically, we show that: (1) our approach improves performance on all tasks\nwhen they are complementary and mutually dependent; (2) the CTRL helps to\nimprove both semantic segmentation and depth estimation tasks performance in\nthe challenging UDA setting; (3) the proposed ISL training scheme further\nimproves the semantic segmentation performance. The implementation is available\nat https://github.com/susaha/ctrl-uda.",
          "link": "http://arxiv.org/abs/2105.07830",
          "publishedOn": "2021-07-06T01:58:10.994Z",
          "wordCount": 684,
          "title": "Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation. (arXiv:2105.07830v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiarui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>",
          "description": "Over the past few years, graph neural networks (GNN) and label\npropagation-based methods have made significant progress in addressing node\nclassification tasks on graphs. However, in addition to their reliance on\nelaborate architectures and algorithms, there are several key technical details\nthat are frequently overlooked, and yet nonetheless can play a vital role in\nachieving satisfactory performance. In this paper, we first summarize a series\nof existing tricks-of-the-trade, and then propose several new ones related to\nlabel usage, loss function formulation, and model design that can significantly\nimprove various GNN architectures. We empirically evaluate their impact on\nfinal node classification accuracy by conducting ablation studies and\ndemonstrate consistently-improved performance, often to an extent that\noutweighs the gains from more dramatic changes in the underlying GNN\narchitecture. Notably, many of the top-ranked models on the Open Graph\nBenchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC\nbenefit from these techniques.",
          "link": "http://arxiv.org/abs/2103.13355",
          "publishedOn": "2021-07-06T01:58:10.985Z",
          "wordCount": null,
          "title": "Bag of Tricks for Node Classification with Graph Neural Networks. (arXiv:2103.13355v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_P/0/1/0/all/0/1\">Pengcheng An</a>",
          "description": "Current research on Explainable AI (XAI) heavily targets on expert users\n(data scientists or AI developers). However, increasing importance has been\nargued for making AI more understandable to nonexperts, who are expected to\nleverage AI techniques, but have limited knowledge about AI. We present a\nmobile application to support nonexperts to interactively make sense of\nConvolutional Neural Networks (CNN); it allows users to play with a pretrained\nCNN by taking pictures of their surrounding objects. We use an up-to-date XAI\ntechnique (Class Activation Map) to intuitively visualize the model's decision\n(the most important image regions that lead to a certain result). Deployed in a\nuniversity course, this playful learning tool was found to support design\nstudents to gain vivid understandings about the capabilities and limitations of\npretrained CNNs in real-world environments. Concrete examples of students'\nplayful explorations are reported to characterize their sensemaking processes\nreflecting different depths of thought.",
          "link": "http://arxiv.org/abs/2107.01996",
          "publishedOn": "2021-07-06T01:58:10.982Z",
          "wordCount": null,
          "title": "Explainability via Interactivity? Supporting Nonexperts' Sensemaking of Pretrained CNN by Interacting with Their Daily Surroundings. (arXiv:2107.01996v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "A fundamental problem when aggregating Markov chains is the specification of\nthe number of state groups. Too few state groups may fail to sufficiently\ncapture the pertinent dynamics of the original, high-order Markov chain. Too\nmany state groups may lead to a non-parsimonious, reduced-order Markov chain\nwhose complexity rivals that of the original. In this paper, we show that an\naugmented value-of-information-based approach to aggregating Markov chains\nfacilitates the determination of the number of state groups. The optimal\nstate-group count coincides with the case where the complexity of the\nreduced-order chain is balanced against the mutual dependence between the\noriginal- and reduced-order chain dynamics.",
          "link": "http://arxiv.org/abs/2107.01799",
          "publishedOn": "2021-07-06T01:58:10.980Z",
          "wordCount": 564,
          "title": "An Information-Theoretic Approach for Automatically Determining the Number of States when Aggregating Markov Chains. (arXiv:2107.01799v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01734",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Passino_F/0/1/0/all/0/1\">Francesco Sanna Passino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heard_N/0/1/0/all/0/1\">Nicholas A. Heard</a>",
          "description": "Spectral embedding of network adjacency matrices often produces node\nrepresentations living approximately around low-dimensional submanifold\nstructures. In particular, hidden substructure is expected to arise when the\ngraph is generated from a latent position model. Furthermore, the presence of\ncommunities within the network might generate community-specific submanifold\nstructures in the embedding, but this is not explicitly accounted for in most\nstatistical models for networks. In this article, a class of models called\nlatent structure block models (LSBM) is proposed to address such scenarios,\nallowing for graph clustering when community-specific one dimensional manifold\nstructure is present. LSBMs focus on a specific class of latent space model,\nthe random dot product graph (RDPG), and assign a latent submanifold to the\nlatent positions of each community. A Bayesian model for the embeddings arising\nfrom LSBMs is discussed, and shown to have a good performance on simulated and\nreal world network data. The model is able to correctly recover the underlying\ncommunities living in a one-dimensional manifold, even when the parametric form\nof the underlying curves is unknown, achieving remarkable results on a variety\nof real data.",
          "link": "http://arxiv.org/abs/2107.01734",
          "publishedOn": "2021-07-06T01:58:10.972Z",
          "wordCount": 615,
          "title": "Latent structure blockmodels for Bayesian spectral graph clustering. (arXiv:2107.01734v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Struckmeier_O/0/1/0/all/0/1\">Oliver Struckmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_K/0/1/0/all/0/1\">Kshitij Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "The slowness principle is a concept inspired by the visual cortex of the\nbrain. It postulates that the underlying generative factors of a quickly\nvarying sensory signal change on a slower time scale. Unsupervised learning of\nintermediate representations utilizing abundant unlabeled sensory data can be\nleveraged to perform data-efficient supervised downstream regression. In this\npaper, we propose a general formulation of slowness for unsupervised\nrepresentation learning adding a slowness regularization term to the estimate\nlower bound of the beta-VAE to encourage temporal similarity in observation and\nlatent space. Within this framework we compare existing slowness regularization\nterms such as the L1 and L2 loss used in existing end-to-end methods, the\nSlowVAE and propose a new term based on Brownian motion. We empirically\nevaluate these slowness regularization terms with respect to their downstream\ntask performance and data efficiency. We find that slow representations lead to\nequal or better downstream task performance and data efficiency in different\nexperiment domains when compared to representations without slowness\nregularization. Finally, we discuss how the Frechet Inception Distance (FID),\ntraditionally used to determine the generative capabilities of GANs, can serve\nas a measure to predict the performance of pre-trained Autoencoder model in a\nsupervised downstream task and accelerate hyperparameter search.",
          "link": "http://arxiv.org/abs/2012.06279",
          "publishedOn": "2021-07-06T01:58:10.947Z",
          "wordCount": null,
          "title": "Autoencoding Slow Representations for Semi-supervised Data Efficient Regression. (arXiv:2012.06279v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.00291",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>",
          "description": "We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove an information theoretic lower bound on the number of\nstochastic gradient queries of the log density needed. Several popular sampling\nalgorithms (including many Markov chain Monte Carlo methods) operate by using\nstochastic gradients of the log density to generate a sample; our results\nestablish an information theoretic limit for all these algorithms.\n\nWe show that for every algorithm, there exists a well-conditioned strongly\nlog-concave target density for which the distribution of points generated by\nthe algorithm would be at least $\\varepsilon$ away from the target in total\nvariation distance if the number of gradient queries is less than\n$\\Omega(\\sigma^2 d/\\varepsilon^2)$, where $\\sigma^2 d$ is the variance of the\nstochastic gradient. Our lower bound follows by combining the ideas of Le Cam\ndeficiency routinely used in the comparison of statistical experiments along\nwith standard information theoretic tools used in lower bounding Bayes risk\nfunctions. To the best of our knowledge our results provide the first\nnontrivial dimension-dependent lower bound for this problem.",
          "link": "http://arxiv.org/abs/2002.00291",
          "publishedOn": "2021-07-06T01:58:10.939Z",
          "wordCount": null,
          "title": "Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms. (arXiv:2002.00291v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1\">Liam Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1\">Aryan Mokhtari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>",
          "description": "Model-Agnostic Meta-Learning (MAML) has become increasingly popular for\ntraining models that can quickly adapt to new tasks via one or few stochastic\ngradient descent steps. However, the MAML objective is significantly more\ndifficult to optimize compared to standard Empirical Risk Minimization (ERM),\nand little is understood about how much MAML improves over ERM in terms of the\nfast adaptability of their solutions in various scenarios. We analytically\naddress this issue in a linear regression setting consisting of a mixture of\neasy and hard tasks, where hardness is related to the condition number of the\ntask's loss function. Specifically, we prove that in order for MAML to achieve\nsubstantial gain over ERM, (i) there must be some discrepancy in hardness among\nthe tasks, and (ii) the optimal solutions of the hard tasks must be closely\npacked with the center far from the center of the easy tasks optimal solutions.\nWe also give numerical and analytical results suggesting that these insights\nalso apply to two-layer neural networks. Finally, we provide few-shot image\nclassification experiments that support our insights for when MAML should be\nused and emphasize the importance of training MAML on hard tasks in practice.",
          "link": "http://arxiv.org/abs/2010.14672",
          "publishedOn": "2021-07-06T01:58:10.887Z",
          "wordCount": 669,
          "title": "How Does the Task Landscape Affect MAML Performance?. (arXiv:2010.14672v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiulou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxia Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shouju Wang</a>",
          "description": "Intratumoral nanoparticles (NPs) distribution is critical for the success of\nnanomedicine in imaging and treatment, but computational models to describe the\nNPs distribution remain unavailable due to the complex tumor-nano interactions.\nHere, we develop a Generative Adversarial Network for Distribution Analysis\n(GANDA) to describe and conditionally generates the intratumoral quantum dots\n(QDs) distribution after i.v. injection. This deep generative model is trained\nautomatically by 27 775 patches of tumor vessels and cell nuclei decomposed\nfrom whole-slide images of 4T1 breast cancer sections. The GANDA model can\nconditionally generate images of intratumoral QDs distribution under the\nconstraint of given tumor vessels and cell nuclei channels with the same\nspatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =\n1.871) and excellent reliability (intraclass correlation, ICC = 0.94).\nQuantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea\ndistribution (ICC = 0.99) is allowed on the generated images without knowing\nthe real QDs distribution. We believe this deep generative model may provide\nopportunities to investigate how influencing factors affect NPs distribution in\nindividual tumors and guide nanomedicine optimization for molecular imaging and\npersonalized treatment.",
          "link": "http://arxiv.org/abs/2012.12561",
          "publishedOn": "2021-07-06T01:58:10.846Z",
          "wordCount": 668,
          "title": "GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly. (arXiv:2012.12561v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodabakhsh_A/0/1/0/all/0/1\">Ali Khodabakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>",
          "description": "Despite the impressive progress in the field of presentation attack detection\nand multimedia forensics over the last decade, these systems are still\nvulnerable to attacks in real-life settings. Some of the challenges for\nexisting solutions are the detection of unknown attacks, the ability to perform\nin adversarial settings, few-shot learning, and explainability. In this study,\nthese limitations are approached by reliance on a game-theoretic view for\nmodeling the interactions between the attacker and the detector. Consequently,\na new optimization criterion is proposed and a set of requirements are defined\nfor improving the performance of these systems in real-life settings.\nFurthermore, a novel detection technique is proposed using generator-based\nfeature sets that are not biased towards any specific attack species. To\nfurther optimize the performance on known attacks, a new loss function coined\ncategorical margin maximization loss (C-marmax) is proposed which gradually\nimproves the performance against the most powerful attack. The proposed\napproach provides a more balanced performance across known and unknown attacks\nand achieves state-of-the-art performance in known and unknown attack detection\ncases against rational attackers. Lastly, the few-shot learning potential of\nthe proposed approach is studied as well as its ability to provide pixel-level\nexplainability.",
          "link": "http://arxiv.org/abs/2010.01592",
          "publishedOn": "2021-07-06T01:58:10.792Z",
          "wordCount": 668,
          "title": "Unknown Presentation Attack Detection against Rational Attackers. (arXiv:2010.01592v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zizhou Su</a>",
          "description": "Many practical applications of reinforcement learning (RL) constrain the\nagent to learn from a fixed offline dataset of logged interactions, which has\nalready been gathered, without offering further possibility for data\ncollection. However, commonly used off-policy RL algorithms, such as the Deep Q\nNetwork and the Deep Deterministic Policy Gradient, are incapable of learning\nwithout data correlated to the distribution under the current policy, making\nthem ineffective for this offline setting. As the first step towards useful\noffline RL algorithms, we analysis the reason of instability in standard\noff-policy RL algorithms. It is due to the bootstrapping error. The key to\navoiding this error, is ensuring that the agent's action space does not go out\nof the fixed offline dataset. Based on our consideration, a creative offline RL\nframework, the Least Restriction (LR), is proposed in this paper. The LR\nregards selecting an action as taking a sample from the probability\ndistribution. It merely set a little limit for action selection, which not only\navoid the action being out of the offline dataset but also remove all the\nunreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep\nQ-Learning). In the further, we will demonstrate that the LR, is able to learn\nrobustly from different offline datasets, including random and suboptimal\ndemonstrations, on a range of practical control tasks.",
          "link": "http://arxiv.org/abs/2107.01757",
          "publishedOn": "2021-07-06T01:58:10.782Z",
          "wordCount": 642,
          "title": "The Least Restriction for Offline Reinforcement Learning. (arXiv:2107.01757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tax_N/0/1/0/all/0/1\">Niek Tax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_K/0/1/0/all/0/1\">Kees Jan de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Mathijs de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosoula_N/0/1/0/all/0/1\">Nikoleta Dosoula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akker_B/0/1/0/all/0/1\">Bram van den Akker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jon Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuong_O/0/1/0/all/0/1\">Olivier Thuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_L/0/1/0/all/0/1\">Lucas Bernardi</a>",
          "description": "Fraud detection and prevention play an important part in ensuring the\nsustained operation of any e-commerce business. Machine learning (ML) often\nplays an important role in these anti-fraud operations, but the organizational\ncontext in which these ML models operate cannot be ignored. In this paper, we\ntake an organization-centric view on the topic of fraud detection by\nformulating an operational model of the anti-fraud departments in e-commerce\norganizations. We derive 6 research topics and 12 practical challenges for\nfraud detection from this operational model. We summarize the state of the\nliterature for each research topic, discuss potential solutions to the\npractical challenges, and identify 22 open research challenges.",
          "link": "http://arxiv.org/abs/2107.01979",
          "publishedOn": "2021-07-06T01:58:10.757Z",
          "wordCount": 590,
          "title": "Machine Learning for Fraud Detection in E-Commerce: A Research Agenda. (arXiv:2107.01979v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_E/0/1/0/all/0/1\">Esha Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henricson_E/0/1/0/all/0/1\">Erik Henricson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>",
          "description": "With the rapid development of the internet of things (IoT) and artificial\nintelligence (AI) technologies, human activity recognition (HAR) has been\napplied in a variety of domains such as security and surveillance, human-robot\ninteraction, and entertainment. Even though a number of surveys and review\npapers have been published, there is a lack of HAR overview papers focusing on\nhealthcare applications that use wearable sensors. Therefore, we fill in the\ngap by presenting this overview paper. In particular, we present our projects\nto illustrate the system design of HAR applications for healthcare. Our\nprojects include early mobility identification of human activities for\nintensive care unit (ICU) patients and gait analysis of Duchenne muscular\ndystrophy (DMD) patients. We cover essential components of designing HAR\nsystems including sensor factors (e.g., type, number, and placement location),\nAI model selection (e.g., classical machine learning models versus deep\nlearning models), and feature engineering. In addition, we highlight the\nchallenges of such healthcare-oriented HAR systems and propose several research\nopportunities for both the medical and the computer science community.",
          "link": "http://arxiv.org/abs/2103.15990",
          "publishedOn": "2021-07-06T01:58:10.750Z",
          "wordCount": 657,
          "title": "An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence. (arXiv:2103.15990v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>",
          "description": "Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.",
          "link": "http://arxiv.org/abs/2103.15890",
          "publishedOn": "2021-07-06T01:58:10.743Z",
          "wordCount": 581,
          "title": "Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16161",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_D/0/1/0/all/0/1\">Ding Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kin_T/0/1/0/all/0/1\">Taichi Kin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>",
          "description": "The exact shape of intracranial aneurysms is critical in medical diagnosis\nand surgical planning. While voxel-based deep learning frameworks have been\nproposed for this segmentation task, their performance remains limited. In this\nstudy, we offer a two-step surface-based deep learning pipeline that achieves\nsignificantly higher performance. Our proposed model takes a surface model of\nentire principal brain arteries containing aneurysms as input and returns\naneurysms surfaces as output. A user first generates a surface model by\nmanually specifying multiple thresholds for time-of-flight magnetic resonance\nangiography images. The system then samples small surface fragments from the\nentire brain arteries and classifies the surface fragments according to whether\naneurysms are present using a point-based deep learning network (PointNet++).\nFinally, the system applies surface segmentation (SO-Net) to surface fragments\ncontaining aneurysms. We conduct a direct comparison of segmentation\nperformance by counting voxels between the proposed surface-based framework and\nthe existing voxel-based method, in which our framework achieves a much higher\ndice similarity coefficient score (72%) than the prior approach (46%).",
          "link": "http://arxiv.org/abs/2006.16161",
          "publishedOn": "2021-07-06T01:58:10.737Z",
          "wordCount": 640,
          "title": "A Two-step Surface-based 3D Deep Learning Pipeline for Segmentation of Intracranial Aneurysms. (arXiv:2006.16161v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fiky_A/0/1/0/all/0/1\">Ahmed Hashem El Fiky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenawy_A/0/1/0/all/0/1\">Ayman El Shenawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madkour_M/0/1/0/all/0/1\">Mohamed Ashraf Madkour</a>",
          "description": "Android malware is one of the most dangerous threats on the internet, and\nit's been on the rise for several years. Despite significant efforts in\ndetecting and classifying android malware from innocuous android applications,\nthere is still a long way to go. As a result, there is a need to provide a\nbasic understanding of the behavior displayed by the most common Android\nmalware categories and families. Each Android malware family and category has a\ndistinct objective. As a result, it has impacted every corporate area,\nincluding healthcare, banking, transportation, government, and e-commerce. In\nthis paper, we presented two machine-learning approaches for Dynamic Analysis\nof Android Malware: one for detecting and identifying Android Malware\nCategories and the other for detecting and identifying Android Malware\nFamilies, which was accomplished by analyzing a massive malware dataset with 14\nprominent malware categories and 180 prominent malware families of\nCCCS-CIC-AndMal2020 dataset on Dynamic Layers. Our approach achieves in Android\nMalware Category detection more than 96 % accurate and achieves in Android\nMalware Family detection more than 99% accurate. Our approach provides a method\nfor high-accuracy Dynamic Analysis of Android Malware while also shortening the\ntime required to analyze smartphone malware.",
          "link": "http://arxiv.org/abs/2107.01927",
          "publishedOn": "2021-07-06T01:58:10.731Z",
          "wordCount": 644,
          "title": "Android Malware Category and Family Detection and Identification using Machine Learning. (arXiv:2107.01927v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>",
          "description": "Supervised learning, while deployed in real-life scenarios, often encounters\ninstances of unknown classes. Conventional algorithms for training a supervised\nlearning model do not provide an option to detect such instances, so they\nmiss-classify such instances with 100% probability. Open Set Recognition (OSR)\nand Non-Exhaustive Learning (NEL) are potential solutions to overcome this\nproblem. Most existing methods of OSR first classify members of existing\nclasses and then identify instances of new classes. However, many of the\nexisting methods of OSR only makes a binary decision, i.e., they only identify\nthe existence of the unknown class. Hence, such methods cannot distinguish test\ninstances belonging to incremental unseen classes. On the other hand, the\nmajority of NEL methods often make a parametric assumption over the data\ndistribution, which either fail to return good results, due to the reason that\nreal-life complex datasets may not follow a well-known data distribution. In\nthis paper, we propose a new online non-exhaustive learning model, namely,\nNon-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to\naddress these issues. Our proposed model synthesizes Gaussian mixture based\nlatent representation over a deep generative model, such as GAN, for\nincremental detection of instances of emerging classes in the test data.\nExtensive experimental results on several benchmark datasets show that\nNE-GM-GAN significantly outperforms the state-of-the-art methods in detecting\ninstances of novel classes in streaming data.",
          "link": "http://arxiv.org/abs/2106.14344",
          "publishedOn": "2021-07-06T01:58:10.704Z",
          "wordCount": 680,
          "title": "Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlechtinger_M/0/1/0/all/0/1\">Michael Schlechtinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosack_D/0/1/0/all/0/1\">Damaris Kosack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzer_T/0/1/0/all/0/1\">Thomas Fetzer</a>",
          "description": "Pricing decisions are increasingly made by AI. Thanks to their ability to\ntrain with live market data while making decisions on the fly, deep\nreinforcement learning algorithms are especially effective in taking such\npricing decisions. In e-commerce scenarios, multiple reinforcement learning\nagents can set prices based on their competitor's prices. Therefore, research\nstates that agents might end up in a state of collusion in the long run. To\nfurther analyze this issue, we build a scenario that is based on a modified\nversion of a prisoner's dilemma where three agents play the game of rock paper\nscissors. Our results indicate that the action selection can be dissected into\nspecific stages, establishing the possibility to develop collusion prevention\nsystems that are able to recognize situations which might lead to a collusion\nbetween competitors. We furthermore provide evidence for a situation where\nagents are capable of performing a tacit cooperation strategy without being\nexplicitly trained to do so.",
          "link": "http://arxiv.org/abs/2107.01856",
          "publishedOn": "2021-07-06T01:58:10.691Z",
          "wordCount": 618,
          "title": "Winning at Any Cost -- Infringing the Cartel Prohibition With Reinforcement Learning. (arXiv:2107.01856v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>",
          "description": "Intelligence necessitates memory. Without memory, humans fail to perform\nvarious nontrivial tasks such as reading novels, playing games or solving\nmaths. As the ultimate goal of machine learning is to derive intelligent\nsystems that learn and act automatically just like human, memory construction\nfor machine is inevitable. Artificial neural networks model neurons and\nsynapses in the brain by interconnecting computational units via weights, which\nis a typical class of machine learning algorithms that resembles memory\nstructure. Their descendants with more complicated modeling techniques (a.k.a\ndeep learning) have been successfully applied to many practical problems and\ndemonstrated the importance of memory in the learning process of machinery\nsystems. Recent progresses on modeling memory in deep learning have revolved\naround external memory constructions, which are highly inspired by\ncomputational Turing models and biological neuronal systems. Attention\nmechanisms are derived to support acquisition and retention operations on the\nexternal memory. Despite the lack of theoretical foundations, these approaches\nhave shown promises to help machinery systems reach a higher level of\nintelligence. The aim of this thesis is to advance the understanding on memory\nand attention in deep learning. Its contributions include: (i) presenting a\ncollection of taxonomies for memory, (ii) constructing new memory-augmented\nneural networks (MANNs) that support multiple control and memory units, (iii)\nintroducing variability via memory in sequential generative models, (iv)\nsearching for optimal writing operations to maximise the memorisation capacity\nin slot-based memory networks, and (v) simulating the Universal Turing Machine\nvia Neural Stored-program Memory-a new kind of external memory for neural\nnetworks.",
          "link": "http://arxiv.org/abs/2107.01390",
          "publishedOn": "2021-07-06T01:58:10.680Z",
          "wordCount": 681,
          "title": "Memory and attention in deep learning. (arXiv:2107.01390v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01275",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1\">Timo Lohrenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwarz_P/0/1/0/all/0/1\">Patrick Schwarz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>",
          "description": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter. Upon acceptance, models will be\npublished on github.",
          "link": "http://arxiv.org/abs/2107.01275",
          "publishedOn": "2021-07-06T01:58:10.629Z",
          "wordCount": 610,
          "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition. (arXiv:2107.01275v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_P/0/1/0/all/0/1\">Peeyush Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gangal_A/0/1/0/all/0/1\">Ayushe Gangal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1\">Sunita Kumari</a>",
          "description": "Coronavirus is a large virus family consisting of diverse viruses, some of\nwhich disseminate among mammals and others cause sickness among humans.\nCOVID-19 is highly contagious and is rapidly spreading, rendering its early\ndiagnosis of preeminent status. Researchers, medical specialists and\norganizations all over the globe have been working tirelessly to combat this\nvirus and help in its containment. In this paper, a novel neural network called\nWisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.\nThe WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is\na two-layered convolutional Neural Network (CNN), which takes chest x-ray\nimages as input. Both layers of the proposed neural network consist of a number\nof neural networks each. The dataset used for this study consists of chest\nx-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on\nGitHub, and the chest x-ray images of healthy lungs and lungs affected by viral\nand bacterial pneumonia were obtained from Kaggle. The network not only\npinpoints the presence of COVID-19, but also gives the probability of the\ndisease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,\npredicting the progression of the disease in the COVID-19 positive patients.\nThe network also slender the occurrences of false negative cases by employing a\nhigh threshold value, thus aids in curbing the spread of the disease and gives\nan accuracy of 100% for successfully predicting COVID-19 among the chest x-rays\nof patients affected with COVID-19, bacterial and viral pneumonia.",
          "link": "http://arxiv.org/abs/2107.01392",
          "publishedOn": "2021-07-06T01:58:10.620Z",
          "wordCount": 795,
          "title": "WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays. (arXiv:2107.01392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Louiset_R/0/1/0/all/0/1\">Robin Louiset</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Houenou_J/0/1/0/all/0/1\">Josselin Houenou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grigis_A/0/1/0/all/0/1\">Antoine Grigis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>",
          "description": "Subtype Discovery consists in finding interpretable and consistent sub-parts\nof a dataset, which are also relevant to a certain supervised task. From a\nmathematical point of view, this can be defined as a clustering task driven by\nsupervised learning in order to uncover subgroups in line with the supervised\nprediction. In this paper, we propose a general Expectation-Maximization\nensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised\nLearning). Our method is generic, it can integrate any clustering method and\ncan be driven by both binary classification and regression. We propose to\nconstruct a non-linear model by merging multiple linear estimators, one per\ncluster. Each hyperplane is estimated so that it correctly discriminates - or\npredict - only one cluster. We use SVC or Logistic Regression for\nclassification and SVR for regression. Furthermore, to perform cluster analysis\nwithin a more suitable space, we also propose a dimension-reduction algorithm\nthat projects the data onto an orthonormal space relevant to the supervised\ntask. We analyze the robustness and generalization capability of our algorithm\nusing synthetic and experimental datasets. In particular, we validate its\nability to identify suitable consistent sub-types by conducting a\npsychiatric-diseases cluster analysis with known ground-truth labels. The gain\nof the proposed method over previous state-of-the-art techniques is about +1.9\npoints in terms of balanced accuracy. Finally, we make codes and examples\navailable in a scikit-learn-compatible Python package at\nhttps://github.com/neurospin-projects/2021_rlouiset_ucsl",
          "link": "http://arxiv.org/abs/2107.01988",
          "publishedOn": "2021-07-06T01:58:10.591Z",
          "wordCount": 699,
          "title": "UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning. (arXiv:2107.01988v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01658",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dallakyan_A/0/1/0/all/0/1\">Aramayis Dallakyan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pourahmadi_M/0/1/0/all/0/1\">Mohsen Pourahmadi</a>",
          "description": "We establish a novel framework for learning a directed acyclic graph (DAG)\nwhen data are generated from a Gaussian, linear structural equation model. It\nconsists of two parts: (1) introduce a permutation matrix as a new parameter\nwithin a regularized Gaussian log-likelihood to represent variable ordering;\nand (2) given the ordering, estimate the DAG structure through sparse Cholesky\nfactor of the inverse covariance matrix. For permutation matrix estimation, we\npropose a relaxation technique that avoids the NP-hard combinatorial problem of\norder estimation. Given an ordering, a sparse Cholesky factor is estimated\nusing a cyclic coordinatewise descent algorithm which decouples row-wise. Our\nframework recovers DAGs without the need for an expensive verification of the\nacyclicity constraint or enumeration of possible parent sets. We establish\nnumerical convergence of the algorithm, and consistency of the Cholesky factor\nestimator when the order of variables is known. Through several simulated and\nmacro-economic datasets, we study the scope and performance of the proposed\nmethodology.",
          "link": "http://arxiv.org/abs/2107.01658",
          "publishedOn": "2021-07-06T01:58:10.527Z",
          "wordCount": 589,
          "title": "Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method. (arXiv:2107.01658v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bin Hu</a>",
          "description": "Person re-identification (Re-ID) via gait features within 3D skeleton\nsequences is a newly-emerging topic with several advantages. Existing solutions\neither rely on hand-crafted descriptors or supervised gait representation\nlearning. This paper proposes a self-supervised gait encoding approach that can\nleverage unlabeled skeleton data to learn gait representations for person\nRe-ID. Specifically, we first create self-supervision by learning to\nreconstruct unlabeled skeleton sequences reversely, which involves richer\nhigh-level semantics to obtain better gait representations. Other pretext tasks\nare also explored to further improve self-supervised learning. Second, inspired\nby the fact that motion's continuity endows adjacent skeletons in one skeleton\nsequence and temporally consecutive skeleton sequences with higher correlations\n(referred as locality in 3D skeleton data), we propose a locality-aware\nattention mechanism and a locality-aware contrastive learning scheme, which aim\nto preserve locality-awareness on intra-sequence level and inter-sequence level\nrespectively during self-supervised learning. Last, with context vectors\nlearned by our locality-aware attention mechanism and contrastive learning\nscheme, a novel feature named Constrastive Attention-based Gait Encodings\n(CAGEs) is designed to represent gait effectively. Empirical evaluations show\nthat our approach significantly outperforms skeleton-based counterparts by\n15-40% Rank-1 accuracy, and it even achieves superior performance to numerous\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/Locality-Awareness-SGE.",
          "link": "http://arxiv.org/abs/2009.03671",
          "publishedOn": "2021-07-06T01:58:10.516Z",
          "wordCount": 742,
          "title": "A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification. (arXiv:2009.03671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1\">Arnu Pretorius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1\">Kale-ab Tessera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smit_A/0/1/0/all/0/1\">Andries P. Smit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Formanek_C/0/1/0/all/0/1\">Claude Formanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimbly_S/0/1/0/all/0/1\">St John Grimbly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloff_K/0/1/0/all/0/1\">Kevin Eloff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danisa_S/0/1/0/all/0/1\">Siphelele Danisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_L/0/1/0/all/0/1\">Lawrence Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shock_J/0/1/0/all/0/1\">Jonathan Shock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brink_W/0/1/0/all/0/1\">Willie Brink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelbrecht_H/0/1/0/all/0/1\">Herman Engelbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1\">Alexandre Laterre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beguir_K/0/1/0/all/0/1\">Karim Beguir</a>",
          "description": "Breakthrough advances in reinforcement learning (RL) research have led to a\nsurge in the development and application of RL. To support the field and its\nrapid growth, several frameworks have emerged that aim to help the community\nmore easily build effective and scalable agents. However, very few of these\nframeworks exclusively support multi-agent RL (MARL), an increasingly active\nfield in itself, concerned with decentralised decision-making problems. In this\nwork, we attempt to fill this gap by presenting Mava: a research framework\nspecifically designed for building scalable MARL systems. Mava provides useful\ncomponents, abstractions, utilities and tools for MARL and allows for simple\nscaling for multi-process system training and execution, while providing a high\nlevel of flexibility and composability. Mava is built on top of DeepMind's Acme\n\\citep{hoffman2020acme}, and therefore integrates with, and greatly benefits\nfrom, a wide range of already existing single-agent RL components made\navailable in Acme. Several MARL baseline systems have already been implemented\nin Mava. These implementations serve as examples showcasing Mava's reusable\nfeatures, such as interchangeable system architectures, communication and\nmixing modules. Furthermore, these implementations allow existing MARL\nalgorithms to be easily reproduced and extended. We provide experimental\nresults for these implementations on a wide range of multi-agent environments\nand highlight the benefits of distributed system training.",
          "link": "http://arxiv.org/abs/2107.01460",
          "publishedOn": "2021-07-06T01:58:10.500Z",
          "wordCount": 673,
          "title": "Mava: a research framework for distributed multi-agent reinforcement learning. (arXiv:2107.01460v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Sen Yan</a>",
          "description": "Knowledge distillation uses both real hard labels and soft labels predicted\nby teacher models as supervision. Intuitively, we expect the soft labels and\nhard labels to be concordant w.r.t. their orders of probabilities. However, we\nfound {\\it critical order violations} between hard labels and soft labels in\naugmented samples. For example, for an augmented sample $x=0.7*panda+0.3*cat$,\nwe expect the order of meaningful soft labels to be\n$P_\\text{soft}(panda|x)>P_\\text{soft}(cat|x)>P_\\text{soft}(other|x)$. But real\nsoft labels usually violate the order, e.g.\n$P_\\text{soft}(tiger|x)>P_\\text{soft}(panda|x)>P_\\text{soft}(cat|x)$. We\nattribute this to the unsatisfactory generalization ability of the teacher,\nwhich leads to the prediction error of augmented samples. Empirically, we found\nthe violations are common and injure the knowledge transfer.In this paper, we\nintroduce order restrictions to data augmentation for knowledge distillation,\nwhich is denoted as isotonic data augmentation (IDA). We use isotonic\nregression (IR) -- a classic technique from statistics -- to eliminate the\norder violations. We show that IDA can be modeled as a tree-structured IR\nproblem. We thereby adapt the classical IRT-BIN algorithm for optimal solutions\nwith $O(c \\log c)$ time complexity, where $c$ is the number of labels. In order\nto further reduce the time complexity, we also \\cwy{propose} a GPU-friendly\napproximation with linear time complexity. We have verified on variant datasets\nand data augmentation techniques that our proposed IDA algorithms effectively\nincreases the accuracy of knowledge distillation by eliminating the rank\nviolations.",
          "link": "http://arxiv.org/abs/2107.01412",
          "publishedOn": "2021-07-06T01:58:10.440Z",
          "wordCount": 656,
          "title": "Isotonic Data Augmentation for Knowledge Distillation. (arXiv:2107.01412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Arshad Shaikh</a>",
          "description": "Session-based recommendation systems suggest relevant items to users by\nmodeling user behavior and preferences using short-term anonymous sessions.\nExisting methods leverage Graph Neural Networks (GNNs) that propagate and\naggregate information from neighboring nodes i.e., local message passing. Such\ngraph-based architectures have representational limits, as a single sub-graph\nis susceptible to overfit the sequential dependencies instead of accounting for\ncomplex transitions between items in different sessions. We propose using a\nTransformer in combination with a target attentive GNN, which allows richer\nRepresentation Learning. Our experimental results and ablation show that our\nproposed method outperforms the existing methods on real-world benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2107.01516",
          "publishedOn": "2021-07-06T01:58:10.395Z",
          "wordCount": 540,
          "title": "Improved Representation Learning for Session-based Recommendation. (arXiv:2107.01516v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01863",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1\">Reymond Mesuga</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1\">Brian James Bayanay</a>",
          "description": "LIGO is considered the most sensitive and complicated gravitational\nexperiment ever built. Its main objective is to detect the gravitational wave\nfrom the strongest events in the universe by observing if the length of its\n4-kilometer arms change by a distance 10,000 times smaller than the diameter of\na proton. Due to its sensitivity, LIGO is prone to the disturbance of external\nnoises which affects the data being collected to detect the gravitational wave.\nThese noises are commonly called by the LIGO community as glitches. The\nobjective of this study is to evaluate the effeciency of various deep trasnfer\nlearning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch\nwaveform in gravitational wave data. The accuracy achieved by the said models\nare 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models\nachieved fairly high accuracy, it is observed that all of the model suffered\nfrom the lack of data for certain classes which is the main concern found in\nthe experiment",
          "link": "http://arxiv.org/abs/2107.01863",
          "publishedOn": "2021-07-06T01:58:10.377Z",
          "wordCount": 633,
          "title": "On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/1602.03822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "We use random geometric graphs to describe clusters of higher dimensional\ndata points which are bijectively mapped to a (possibly) lower dimensional\nspace where an equivalent random cluster model is used to calculate the\nexpected number of modes to be found when separating the data of a multi-modal\ndata set into distinct clusters. Furthermore, as a function of the expected\nnumber of modes and the number of data points in the sample, an upper bound on\na given distance measure is found such that data points have the greatest\ncorrelation if their mutual distances from a common center is less than or\nequal to the calculated bound. Anomalies are exposed, which lie outside of the\nunion of all regularized clusters of data points. Finally, similarly to finding\na hyperplane which can be shifted along its normal to expose the maximal\ndistance between binary classes, it is shown that the union of regularized\nclusters can be used to define a hyperplane which can be shifted by a certain\namount to separate the data into binary classes.",
          "link": "http://arxiv.org/abs/1602.03822",
          "publishedOn": "2021-07-06T01:58:10.370Z",
          "wordCount": 725,
          "title": "A Critical Connectivity Radius for Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01269",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>",
          "description": "Attention-based end-to-end automatic speech recognition (ASR) systems have\nrecently demonstrated state-of-the-art results for numerous tasks. However, the\napplication of self-attention and attention-based encoder-decoder models\nremains challenging for streaming ASR, where each word must be recognized\nshortly after it was spoken. In this work, we present the dual\ncausal/non-causal self-attention (DCN) architecture, which in contrast to\nrestricted self-attention prevents the overall context to grow beyond the\nlook-ahead of a single layer when used in a deep architecture. DCN is compared\nto chunk-based and restricted self-attention using streaming transformer and\nconformer architectures, showing improved ASR performance over restricted\nself-attention and competitive ASR results compared to chunk-based\nself-attention, while providing the advantage of frame-synchronous processing.\nCombined with triggered attention, the proposed streaming end-to-end ASR\nsystems obtained state-of-the-art results on the LibriSpeech, HKUST, and\nSwitchboard ASR tasks.",
          "link": "http://arxiv.org/abs/2107.01269",
          "publishedOn": "2021-07-06T01:58:10.351Z",
          "wordCount": 582,
          "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition. (arXiv:2107.01269v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tupadha_L/0/1/0/all/0/1\">Lolitha Sresta Tupadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Malware evolves over time and antivirus must adapt to such evolution. Hence,\nit is critical to detect those points in time where malware has evolved so that\nappropriate countermeasures can be undertaken. In this research, we perform a\nvariety of experiments on a significant number of malware families to determine\nwhen malware evolution is likely to have occurred. All of the evolution\ndetection techniques that we consider are based on machine learning and can be\nfully automated -- in particular, no reverse engineering or other\nlabor-intensive manual analysis is required. Specifically, we consider analysis\nbased on hidden Markov models (HMM) and the word embedding techniques HMM2Vec\nand Word2Vec.",
          "link": "http://arxiv.org/abs/2107.01627",
          "publishedOn": "2021-07-06T01:58:10.345Z",
          "wordCount": 536,
          "title": "Machine Learning for Malware Evolution Detection. (arXiv:2107.01627v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chi Jin</a>",
          "description": "While over-parameterization is widely believed to be crucial for the success\nof optimization for the neural networks, most existing theories on\nover-parameterization do not fully explain the reason -- they either work in\nthe Neural Tangent Kernel regime where neurons don't move much, or require an\nenormous number of neurons. In practice, when the data is generated using a\nteacher neural network, even mildly over-parameterized neural networks can\nachieve 0 loss and recover the directions of teacher neurons. In this paper we\ndevelop a local convergence theory for mildly over-parameterized two-layer\nneural net. We show that as long as the loss is already lower than a threshold\n(polynomial in relevant parameters), all student neurons in an\nover-parameterized two-layer neural network will converge to one of teacher\nneurons, and the loss will go to 0. Our result holds for any number of student\nneurons as long as it is at least as large as the number of teacher neurons,\nand our convergence rate is independent of the number of student neurons. A key\ncomponent of our analysis is the new characterization of local optimization\nlandscape -- we show the gradient satisfies a special case of Lojasiewicz\nproperty which is different from local strong convexity or PL conditions used\nin previous work.",
          "link": "http://arxiv.org/abs/2102.02410",
          "publishedOn": "2021-07-06T01:58:10.329Z",
          "wordCount": 684,
          "title": "A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network. (arXiv:2102.02410v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chen Lv</a>",
          "description": "To further improve the learning efficiency and performance of reinforcement\nlearning (RL), in this paper we propose a novel uncertainty-aware model-based\nRL (UA-MBRL) framework, and then implement and validate it in autonomous\ndriving under various task scenarios. First, an action-conditioned ensemble\nmodel with the ability of uncertainty assessment is established as the virtual\nenvironment model. Then, a novel uncertainty-aware model-based RL framework is\ndeveloped based on the adaptive truncation approach, providing virtual\ninteractions between the agent and environment model, and improving RL's\ntraining efficiency and performance. The developed algorithms are then\nimplemented in end-to-end autonomous vehicle control tasks, validated and\ncompared with state-of-the-art methods under various driving scenarios. The\nvalidation results suggest that the proposed UA-MBRL method surpasses the\nexisting model-based and model-free RL approaches, in terms of learning\nefficiency and achieved performance. The results also demonstrate the good\nability of the proposed method with respect to the adaptiveness and robustness,\nunder various autonomous driving scenarios.",
          "link": "http://arxiv.org/abs/2106.12194",
          "publishedOn": "2021-07-06T01:58:10.234Z",
          "wordCount": null,
          "title": "Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1\">Lirong Xia</a>",
          "description": "Differential privacy (DP) is a widely-accepted and widely-applied notion of\nprivacy based on worst-case analysis. Often, DP classifies most mechanisms\nwithout external noise as non-private [Dwork et al., 2014], and external\nnoises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are\nintroduced to improve privacy. In many real-world applications, however, adding\nexternal noise is undesirable and sometimes prohibited. For example,\npresidential elections often require a deterministic rule to be used [Liu et\nal., 2020], and small noises can lead to dramatic decreases in the prediction\naccuracy of deep neural networks, especially the underrepresented classes\n[Bagdasaryan et al., 2019].\n\nIn this paper, we propose a natural extension and relaxation of DP following\nthe worst average-case idea behind the celebrated smoothed analysis [Spielman\nand Teng, 2004]. Our notion, the smoothed DP, can effectively measure the\nprivacy leakage of mechanisms without external noises under realistic settings.\n\nWe prove several strong properties of the smoothed DP, including\ncomposability, robustness to post-processing and etc. We proved that any\ndiscrete mechanism with sampling procedures is more private than what DP\npredicts. In comparison, many continuous mechanisms with sampling procedures\nare still non-private under smoothed DP. Experimentally, we first verified that\nthe discrete sampling mechanisms are private in real-world elections. Then, we\napply the smoothed DP notion on quantized gradient descent, which indicates\nsome neural networks can be private without adding any extra noises. We believe\nthat these results contribute to the theoretical foundation of realistic\nprivacy measures beyond worst-case analysis.",
          "link": "http://arxiv.org/abs/2107.01559",
          "publishedOn": "2021-07-06T01:58:10.224Z",
          "wordCount": null,
          "title": "Smoothed Differential Privacy. (arXiv:2107.01559v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murayama_T/0/1/0/all/0/1\">Taichi Murayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>",
          "description": "The accurate forecasting of infectious epidemic diseases such as influenza is\na crucial task undertaken by medical institutions. Although numerous flu\nforecasting methods and models based mainly on historical flu activity data and\nonline user-generated contents have been proposed in previous studies, no flu\nforecasting model targeting multiple countries using two types of data exists\nat present. Our paper leverages multi-task learning to tackle the challenge of\nbuilding one flu forecasting model targeting multiple countries; each country\nas each task. Also, to develop the flu prediction model with higher\nperformance, we solved two issues; finding suitable search queries, which are\npart of the user-generated contents, and how to leverage search queries\nefficiently in the model creation. For the first issue, we propose the transfer\napproaches from English to other languages. For the second issue, we propose a\nnovel flu forecasting model that takes advantage of search queries using an\nattention mechanism and extend the model to a multi-task model for multiple\ncountries' flu forecasts. Experiments on forecasting flu epidemics in five\ncountries demonstrate that our model significantly improved the performance by\nleveraging the search queries and multi-task learning compared to the\nbaselines.",
          "link": "http://arxiv.org/abs/2107.01760",
          "publishedOn": "2021-07-06T01:58:10.224Z",
          "wordCount": null,
          "title": "Single Model for Influenza Forecasting of Multiple Countries by Multi-task Learning. (arXiv:2107.01760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Withnell_E/0/1/0/all/0/1\">Eloise Withnell</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "The lack of explainability is one of the most prominent disadvantages of deep\nlearning applications in omics. This \"black box\" problem can undermine the\ncredibility and limit the practical implementation of biomedical deep learning\nmodels. Here we present XOmiVAE, a variational autoencoder (VAE) based\ninterpretable deep learning model for cancer classification using\nhigh-dimensional omics data. XOmiVAE is capable of revealing the contribution\nof each gene and latent dimension for each classification prediction, and the\ncorrelation between each gene and each latent dimension. It is also\ndemonstrated that XOmiVAE can explain not only the supervised classification\nbut the unsupervised clustering results from the deep learning network. To the\nbest of our knowledge, XOmiVAE is one of the first activation level-based\ninterpretable deep learning models explaining novel clusters generated by VAE.\nThe explainable results generated by XOmiVAE were validated by both the\nperformance of downstream tasks and the biomedical knowledge. In our\nexperiments, XOmiVAE explanations of deep learning based cancer classification\nand clustering aligned with current domain knowledge including biological\nannotation and academic literature, which shows great potential for novel\nbiomedical knowledge discovery from deep learning models.",
          "link": "http://arxiv.org/abs/2105.12807",
          "publishedOn": "2021-07-06T01:58:10.208Z",
          "wordCount": 661,
          "title": "XOmiVAE: an interpretable deep learning model for cancer classification using high-dimensional omics data. (arXiv:2105.12807v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07568",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hoang Phan</a>",
          "description": "This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.",
          "link": "http://arxiv.org/abs/2106.07568",
          "publishedOn": "2021-07-06T01:58:10.197Z",
          "wordCount": 612,
          "title": "Full interpretable machine learning in 2D with inline coordinates. (arXiv:2106.07568v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorbeer_B/0/1/0/all/0/1\">Boris Lorbeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botler_M/0/1/0/all/0/1\">Max Botler</a>",
          "description": "In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder\nEnSemble), a new method for unsupervised outlier detection (UOD). More\nprecisely, given any autoencoder for UOD, this technique can be used to improve\nits accuracy while at the same time removing the burden of tuning its\nregularization. The idea is to not regularize at all, but to rather randomly\npartition the data into sufficiently many equally sized parts, overfit each\npart with its own autoencoder, and to use the maximum over all autoencoder\nreconstruction errors as the anomaly score. We apply our model to various\nrealistic datasets and show that if the set of inliers is dense enough, our\nmethod indeed improves the UOD performance of a given autoencoder\nsignificantly. For reproducibility, the code is made available on github so the\nreader can recreate the results in this paper as well as apply the method to\nother autoencoders and datasets.",
          "link": "http://arxiv.org/abs/2009.02755",
          "publishedOn": "2021-07-06T01:58:10.174Z",
          "wordCount": 647,
          "title": "Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles. (arXiv:2009.02755v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shulman_Y/0/1/0/all/0/1\">Yaniv Shulman</a>",
          "description": "Quantization based model compression serves as high performing and fast\napproach for inference that yields highly compressed models compared to their\nfull-precision floating point counterparts. The most extreme quantization is a\n1-bit representation of parameters such that they have only two possible\nvalues, typically -1(0) or +1. Models that constrain the weights to binary\nvalues enable efficient implementation of the ubiquitous dot product by\nadditions only without requiring floating point multiplications which is\nbeneficial for resources constrained inference. The main contribution of this\nwork is the introduction of a method to smooth the combinatorial problem of\ndetermining a binary vector of weights to minimize the expected loss for a\ngiven objective by means of empirical risk minimization with backpropagation.\nThis is achieved by approximating a multivariate binary state over the weights\nutilizing a deterministic and differentiable transformation of real-valued\ncontinuous parameters. The proposed method adds little overhead in training,\ncan be readily applied without any substantial modifications to the original\narchitecture, does not introduce additional saturating non-linearities or\nauxiliary losses, and does not prohibit applying other methods for binarizing\nthe activations. It is demonstrated that contrary to common assertions made in\nthe literature, binary weighted networks can train well with the same standard\noptimization techniques and similar hyperparameters settings as their\nfull-precision counterparts, namely momentum SGD with large learning rates and\n$L_2$ regularization. The source code is publicly available at\nhttps://bitbucket.org/YanivShu/binary_weighted_networks_public",
          "link": "http://arxiv.org/abs/2107.01400",
          "publishedOn": "2021-07-06T01:58:10.168Z",
          "wordCount": 666,
          "title": "Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations. (arXiv:2107.01400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_A/0/1/0/all/0/1\">Adrien Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esling_P/0/1/0/all/0/1\">Philippe Esling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Granular sound synthesis is a popular audio generation technique based on\nrearranging sequences of small waveform windows. In order to control the\nsynthesis, all grains in a given corpus are analyzed through a set of acoustic\ndescriptors. This provides a representation reflecting some form of local\nsimilarities across the grains. However, the quality of this grain space is\nbound by that of the descriptors. Its traversal is not continuously invertible\nto signal and does not render any structured temporality.\n\nWe demonstrate that generative neural networks can implement granular\nsynthesis while alleviating most of its shortcomings. We efficiently replace\nits audio descriptor basis by a probabilistic latent space learned with a\nVariational Auto-Encoder. In this setting the learned grain space is\ninvertible, meaning that we can continuously synthesize sound when traversing\nits dimensions. It also implies that original grains are not stored for\nsynthesis. Another major advantage of our approach is to learn structured paths\ninside this latent space by training a higher-level temporal embedding over\narranged grain sequences.\n\nThe model can be applied to many types of libraries, including pitched notes\nor unpitched drums and environmental noises. We report experiments on the\ncommon granular synthesis processes as well as novel ones such as conditional\nsampling and morphing.",
          "link": "http://arxiv.org/abs/2008.01393",
          "publishedOn": "2021-07-06T01:58:10.161Z",
          "wordCount": 680,
          "title": "Neural Granular Sound Synthesis. (arXiv:2008.01393v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaabouni_R/0/1/0/all/0/1\">Rahma Chaabouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>",
          "description": "Despite their practical success, modern seq2seq architectures are unable to\ngeneralize systematically on several SCAN tasks. Hence, it is not clear if\nSCAN-style compositional generalization is useful in realistic NLP tasks. In\nthis work, we study the benefit that such compositionality brings about to\nseveral machine translation tasks. We present several focused modifications of\nTransformer that greatly improve generalization capabilities on SCAN and select\none that remains on par with a vanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its performance in low-resource settings\nand on a newly introduced distribution-shifted English-French translation task.\nOverall, we find that improvements of a SCAN-capable model do not directly\ntransfer to the resource-rich MT setup. In contrast, in the low-resource setup,\ngeneral modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a\nvanilla Transformer. Similarly, an improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compositional English-French translation\ntask. This provides experimental evidence that the compositional generalization\nassessed in SCAN is particularly useful in resource-starved and domain-shifted\nscenarios.",
          "link": "http://arxiv.org/abs/2107.01366",
          "publishedOn": "2021-07-06T01:58:10.154Z",
          "wordCount": 621,
          "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerroumi_M/0/1/0/all/0/1\">Mohamed Kerroumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayem_O/0/1/0/all/0/1\">Othmane Sayem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabou_A/0/1/0/all/0/1\">Aymen Shabou</a>",
          "description": "We introduce a novel approach for scanned document representation to perform\nfield extraction. It allows the simultaneous encoding of the textual, visual\nand layout information in a 3-axis tensor used as an input to a segmentation\nmodel. We improve the recent Chargrid and Wordgrid \\cite{chargrid} models in\nseveral ways, first by taking into account the visual modality, then by\nboosting its robustness in regards to small datasets while keeping the\ninference time low. Our approach is tested on public and private document-image\ndatasets, showing higher performances compared to the recent state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2010.02358",
          "publishedOn": "2021-07-06T01:58:10.147Z",
          "wordCount": 585,
          "title": "VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach. (arXiv:2010.02358v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sandeep Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowdur_J/0/1/0/all/0/1\">Jaya Shradha Fowdur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1\">Jakob Gawlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medina_D/0/1/0/all/0/1\">Daniel Medina</a>",
          "description": "Understanding and representing traffic patterns are key to detecting\nanomalies in the maritime domain. To this end, we propose a novel graph-based\ntraffic representation and association scheme to cluster trajectories of\nvessels using automatic identification system (AIS) data. We utilize the\n(un)clustered data to train a recurrent neural network (RNN)-based evidential\nregression model, which can predict a vessel's trajectory at future timesteps\nwith its corresponding prediction uncertainty. This paper proposes the usage of\na deep learning (DL)-based uncertainty estimation in detecting maritime\nanomalies, such as unusual vessel maneuvering. Furthermore, we utilize the\nevidential deep learning classifiers to detect unusual turns of vessels and the\nloss of AIS signal using predicted class probabilities with associated\nuncertainties. Our experimental results suggest that using graph-based\nclustered data improves the ability of the DL models to learn the\ntemporal-spatial correlation of data and associated uncertainties. Using\ndifferent AIS datasets and experiments, we demonstrate that the estimated\nprediction uncertainty yields fundamental information for the detection of\ntraffic anomalies in the maritime and, possibly in other domains.",
          "link": "http://arxiv.org/abs/2107.01557",
          "publishedOn": "2021-07-06T01:58:10.129Z",
          "wordCount": 619,
          "title": "Leveraging Evidential Deep Learning Uncertainties with Graph-based Clustering to Detect Anomalies. (arXiv:2107.01557v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Luonan Chen</a>",
          "description": "Making predictions in a robust way is not easy for nonlinear systems. In this\nwork, a neural network computing framework, i.e., a spatiotemporal\nconvolutional network (STCN), was developed to efficiently and accurately\nrender a multistep-ahead prediction of a time series by employing a\nspatial-temporal information (STI) transformation. The STCN combines the\nadvantages of both the temporal convolutional network (TCN) and the STI\nequation, which maps the high-dimensional/spatial data to the future temporal\nvalues of a target variable, thus naturally providing the prediction of the\ntarget variable. From the observed variables, the STCN also infers the causal\nfactors of the target variable in the sense of Granger causality, which are in\nturn selected as effective spatial information to improve the prediction\nrobustness. The STCN was successfully applied to both benchmark systems and\nreal-world datasets, all of which show superior and robust performance in\nmultistep-ahead prediction, even when the data were perturbed by noise. From\nboth theoretical and computational viewpoints, the STCN has great potential in\npractical applications in artificial intelligence (AI) or machine learning\nfields as a model-free method based only on the observed data, and also opens a\nnew way to explore the observed high-dimensional data in a dynamical manner for\nmachine learning.",
          "link": "http://arxiv.org/abs/2107.01353",
          "publishedOn": "2021-07-06T01:58:10.123Z",
          "wordCount": 651,
          "title": "Spatiotemporal convolutional network for time-series prediction and causal inference. (arXiv:2107.01353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.07006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1\">Reza Nasirigerdeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiari_M/0/1/0/all/0/1\">Mohammad Bakhtiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1\">Reihaneh Torkzadehmahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1\">Amirhossein Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+List_M/0/1/0/all/0/1\">Markus List</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumenthal_D/0/1/0/all/0/1\">David B. Blumenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumbach_J/0/1/0/all/0/1\">Jan Baumbach</a>",
          "description": "Federated learning has faced performance and network communication\nchallenges, especially in the environments where the data is not independent\nand identically distributed (IID) across the clients. To address the former\nchallenge, we introduce the federated-centralized concordance property and show\nthat the federated single-mini-batch training approach can achieve comparable\nperformance as the corresponding centralized training in the Non-IID\nenvironments. To deal with the latter, we present the federated\nmulti-mini-batch approach and illustrate that it can establish a trade-off\nbetween the performance and communication efficiency and outperforms federated\naveraging in the Non-IID settings.",
          "link": "http://arxiv.org/abs/2011.07006",
          "publishedOn": "2021-07-06T01:58:10.115Z",
          "wordCount": 568,
          "title": "Federated Multi-Mini-Batch: An Efficient Training Approach to Federated Learning in Non-IID Environments. (arXiv:2011.07006v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhicheng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>",
          "description": "Model-based deep reinforcement learning has achieved success in various\ndomains that require high sample efficiencies, such as Go and robotics.\nHowever, there are some remaining issues, such as planning efficient\nexplorations to learn more accurate dynamic models, evaluating the uncertainty\nof the learned models, and more rational utilization of models. To mitigate\nthese issues, we present MEEE, a model-ensemble method that consists of\noptimistic exploration and weighted exploitation. During exploration, unlike\nprior methods directly selecting the optimal action that maximizes the expected\naccumulative return, our agent first generates a set of action candidates and\nthen seeks out the optimal action that takes both expected return and future\nobservation novelty into account. During exploitation, different discounted\nweights are assigned to imagined transition tuples according to their model\nuncertainty respectively, which will prevent model predictive error propagation\nin agent training. Experiments on several challenging continuous control\nbenchmark tasks demonstrated that our approach outperforms other model-free and\nmodel-based state-of-the-art methods, especially in sample complexity.",
          "link": "http://arxiv.org/abs/2107.01825",
          "publishedOn": "2021-07-06T01:58:10.106Z",
          "wordCount": 617,
          "title": "Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation. (arXiv:2107.01825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.08603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitesh_K/0/1/0/all/0/1\">Kumar Nitesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Kuzelka Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luc_D/0/1/0/all/0/1\">De Raedt Luc</a>",
          "description": "Relational autocompletion is the problem of automatically filling out some\nmissing values in multi-relational data. We tackle this problem within the\nprobabilistic logic programming framework of Distributional Clauses (DC), which\nsupports both discrete and continuous probability distributions. Within this\nframework, we introduce DiceML { an approach to learn both the structure and\nthe parameters of DC programs from relational data (with possibly missing\ndata). To realize this, DiceML integrates statistical modeling and\ndistributional clauses with rule learning. The distinguishing features of\nDiceML are that it 1) tackles autocompletion in relational data, 2) learns\ndistributional clauses extended with statistical models, 3) deals with both\ndiscrete and continuous distributions, 4) can exploit background knowledge, and\n5) uses an expectation-maximization based algorithm to cope with missing data.\nThe empirical results show the promise of the approach, even when there is\nmissing data.",
          "link": "http://arxiv.org/abs/2001.08603",
          "publishedOn": "2021-07-06T01:58:10.094Z",
          "wordCount": 628,
          "title": "Learning Distributional Programs for Relational Autocompletion. (arXiv:2001.08603v5 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ailon_N/0/1/0/all/0/1\">Nir Ailon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibovich_O/0/1/0/all/0/1\">Omer Leibovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vineet Nair</a>",
          "description": "A butterfly network consists of logarithmically many layers, each with a\nlinear number of non-zero weights (pre-specified). The fast\nJohnson-Lindenstrauss transform (FJLT) can be represented as a butterfly\nnetwork followed by a projection onto a random subset of the coordinates.\nMoreover, a random matrix based on FJLT with high probability approximates the\naction of any matrix on a vector. Motivated by these facts, we propose to\nreplace a dense linear layer in any neural network by an architecture based on\nthe butterfly network. The proposed architecture significantly improves upon\nthe quadratic number of weights required in a standard dense layer to nearly\nlinear with little compromise in expressibility of the resulting operator. In a\ncollection of wide variety of experiments, including supervised prediction on\nboth the NLP and vision data, we show that this not only produces results that\nmatch and at times outperform existing well-known architectures, but it also\noffers faster training and prediction in deployment. To understand the\noptimization problems posed by neural networks with a butterfly network, we\nalso study the optimization landscape of the encoder-decoder network, where the\nencoder is replaced by a butterfly network followed by a dense linear layer in\nsmaller dimension. Theoretical result presented in the paper explains why the\ntraining speed and outcome are not compromised by our proposed approach.",
          "link": "http://arxiv.org/abs/2007.08864",
          "publishedOn": "2021-07-06T01:58:10.081Z",
          "wordCount": 689,
          "title": "Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice. (arXiv:2007.08864v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foorthuis_R/0/1/0/all/0/1\">Ralph Foorthuis</a>",
          "description": "Anomalies are cases that are in some way unusual and do not appear to fit the\ngeneral patterns present in the dataset. Several conceptualizations exist to\ndistinguish between different types of anomalies. However, these are either too\nspecific to be generally applicable or so abstract that they neither provide\nconcrete insight into the nature of anomaly types nor facilitate the functional\nevaluation of anomaly detection algorithms. With the recent criticism on 'black\nbox' algorithms and analytics it has become clear that this is an undesirable\nsituation. This paper therefore introduces a general typology of anomalies that\noffers a clear and tangible definition of the different types of anomalies in\ndatasets. The typology also facilitates the evaluation of the functional\ncapabilities of anomaly detection algorithms and as a framework assists in\nanalyzing the conceptual levels of data, patterns and anomalies. Finally, it\nserves as an analytical tool for studying anomaly types from other typologies.",
          "link": "http://arxiv.org/abs/2107.01615",
          "publishedOn": "2021-07-06T01:58:10.060Z",
          "wordCount": 645,
          "title": "A Typology of Data Anomalies. (arXiv:2107.01615v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1902.09434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1\">Hugo Caselles-Dupr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ortiz_M/0/1/0/all/0/1\">Michael Garcia-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>",
          "description": "We consider the problem of building a state representation model for control,\nin a continual learning setting. As the environment changes, the aim is to\nefficiently compress the sensory state's information without losing past\nknowledge, and then use Reinforcement Learning on the resulting features for\nefficient policy learning. To this end, we propose S-TRIGGER, a general method\nfor Continual State Representation Learning applicable to Variational\nAuto-Encoders and its many variants. The method is based on Generative Replay,\ni.e. the use of generated samples to maintain past knowledge. It comes along\nwith a statistically sound method for environment change detection, which\nself-triggers the Generative Replay. Our experiments on VAEs show that\nS-TRIGGER learns state representations that allows fast and high-performing\nReinforcement Learning, while avoiding catastrophic forgetting. The resulting\nsystem is capable of autonomously learning new information without using past\ndata and with a bounded system size. Code for our experiments is attached in\nAppendix.",
          "link": "http://arxiv.org/abs/1902.09434",
          "publishedOn": "2021-07-06T01:58:10.045Z",
          "wordCount": 629,
          "title": "S-TRIGGER: Continual State Representation Learning via Self-Triggered Generative Replay. (arXiv:1902.09434v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pricope_T/0/1/0/all/0/1\">Tidor-Vlad Pricope</a>",
          "description": "Classifying hand-written digits and letters has taken a big leap with the\nintroduction of ConvNets. However, on very constrained hardware the time\nnecessary to train such models would be high. Our main contribution is twofold.\nFirst, we extensively test an end-to-end vanilla neural network (MLP) approach\nin pure numpy without any pre-processing or feature extraction done beforehand.\nSecond, we show that basic data mining operations can significantly improve the\nperformance of the models in terms of computational time, without sacrificing\nmuch accuracy. We illustrate our claims on a simpler variant of the Extended\nMNIST dataset, called Balanced EMNIST dataset. Our experiments show that,\nwithout any data mining, we get increased generalization performance when using\nmore hidden layers and regularization techniques, the best model achieving\n84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA\nwe were able to increase that figure to 85.08% with only 10% of the original\nfeature space, reducing the memory size needed by 64%. Finally, adding methods\nto remove possibly harmful training samples like deviation from the mean helped\nus to still achieve over 84% test accuracy but with only 32.8% of the original\nmemory size for the training set. This compares favorably to the majority of\nliterature results obtained through similar architectures. Although this\napproach gets outshined by state-of-the-art models, it does scale to some\n(AlexNet, VGGNet) trained on 50% of the same dataset.",
          "link": "http://arxiv.org/abs/2107.01782",
          "publishedOn": "2021-07-06T01:58:10.013Z",
          "wordCount": 675,
          "title": "A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources. (arXiv:2107.01782v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_R/0/1/0/all/0/1\">Ron Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maman_N/0/1/0/all/0/1\">Nadav Maman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Inderjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momiyama_S/0/1/0/all/0/1\">Satoru Momiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>",
          "description": "Although cyberattacks on machine learning (ML) production systems can be\ndestructive, many industry practitioners are ill equipped, lacking tactical and\nstrategic tools that would allow them to analyze, detect, protect against, and\nrespond to cyberattacks targeting their ML-based systems. In this paper, we\ntake a significant step toward securing ML production systems by integrating\nthese systems and their vulnerabilities into cybersecurity risk assessment\nframeworks. Specifically, we performed a comprehensive threat analysis of ML\nproduction systems and developed an extension to the MulVAL attack graph\ngeneration and analysis framework to incorporate cyberattacks on ML production\nsystems. Using the proposed extension, security practitioners can apply attack\ngraph analysis methods in environments that include ML components, thus\nproviding security experts with a practical tool for evaluating the impact and\nquantifying the risk of a cyberattack targeting an ML production system.",
          "link": "http://arxiv.org/abs/2107.01806",
          "publishedOn": "2021-07-06T01:58:09.984Z",
          "wordCount": 589,
          "title": "A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems. (arXiv:2107.01806v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1\">Ahmed H. Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiangeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_A/0/1/0/all/0/1\">Asfiya Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael C. Yip</a>",
          "description": "Constrained motion planning is a challenging field of research, aiming for\ncomputationally efficient methods that can find a collision-free path on the\nconstraint manifolds between a given start and goal configuration. These\nplanning problems come up surprisingly frequently, such as in robot\nmanipulation for performing daily life assistive tasks. However, few solutions\nto constrained motion planning are available, and those that exist struggle\nwith high computational time complexity in finding a path solution on the\nmanifolds. To address this challenge, we present Constrained Motion Planning\nNetworks X (CoMPNetX). It is a neural planning approach, comprising a\nconditional deep neural generator and discriminator with neural gradients-based\nfast projection operator. We also introduce neural task and scene\nrepresentations conditioned on which the CoMPNetX generates implicit manifold\nconfigurations to turbo-charge any underlying classical planner such as\nSampling-based Motion Planning methods for quickly solving complex constrained\nplanning tasks. We show that our method finds path solutions with high success\nrates and lower computation times than state-of-the-art traditional\npath-finding tools on various challenging scenarios.",
          "link": "http://arxiv.org/abs/2010.08707",
          "publishedOn": "2021-07-06T01:58:09.968Z",
          "wordCount": null,
          "title": "Constrained Motion Planning Networks X. (arXiv:2010.08707v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hai Li</a>",
          "description": "Learning with graphs has attracted significant attention recently. Existing\nrepresentation learning methods on graphs have achieved state-of-the-art\nperformance on various graph-related tasks such as node classification, link\nprediction, etc. However, we observe that these methods could leak serious\nprivate information. For instance, one can accurately infer the links (or node\nidentity) in a graph from a node classifier (or link predictor) trained on the\nlearnt node representations by existing methods. To address the issue, we\npropose a privacy-preserving representation learning framework on graphs from\nthe \\emph{mutual information} perspective. Specifically, our framework includes\na primary learning task and a privacy protection task, and we consider node\nclassification and link prediction as the two tasks of interest. Our goal is to\nlearn node representations such that they can be used to achieve high\nperformance for the primary learning task, while obtaining performance for the\nprivacy protection task close to random guessing. We formally formulate our\ngoal via mutual information objectives. However, it is intractable to compute\nmutual information in practice. Then, we derive tractable variational bounds\nfor the mutual information terms, where each bound can be parameterized via a\nneural network. Next, we train these parameterized neural networks to\napproximate the true mutual information and learn privacy-preserving node\nrepresentations. We finally evaluate our framework on various graph datasets.",
          "link": "http://arxiv.org/abs/2107.01475",
          "publishedOn": "2021-07-06T01:58:09.963Z",
          "wordCount": null,
          "title": "Privacy-Preserving Representation Learning on Graphs: A Mutual Information Perspective. (arXiv:2107.01475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10558",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Webber_R/0/1/0/all/0/1\">Robert J. Webber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindsey_M/0/1/0/all/0/1\">Michael Lindsey</a>",
          "description": "Variational Monte Carlo (VMC) is an approach for computing ground-state\nwavefunctions that has recently become more powerful due to the introduction of\nneural network-based wavefunction parametrizations. However, efficiently\ntraining neural wavefunctions to converge to an energy minimum remains a\ndifficult problem. In this work, we analyze optimization and sampling methods\nused in VMC and introduce alterations to improve their performance. First,\nbased on theoretical convergence analysis in a noiseless setting, we motivate a\nnew optimizer that we call the Rayleigh-Gauss-Newton method, which can improve\nupon gradient descent and natural gradient descent to achieve superlinear\nconvergence with little added computational cost. Second, in order to realize\nthis favorable comparison in the presence of stochastic noise, we analyze the\neffect of sampling error on VMC parameter updates and experimentally\ndemonstrate that it can be reduced by the parallel tempering method. In\nparticular, we demonstrate that RGN can be made robust to energy spikes that\noccur when new regions of configuration space become available to the sampler\nover the course of optimization. Finally, putting theory into practice, we\napply our enhanced optimization and sampling methods to the transverse-field\nIsing and XXZ models on large lattices, yielding ground-state energy estimates\nwith remarkably high accuracy after just 200-500 parameter updates.",
          "link": "http://arxiv.org/abs/2106.10558",
          "publishedOn": "2021-07-06T01:58:09.961Z",
          "wordCount": 671,
          "title": "Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo. (arXiv:2106.10558v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>",
          "description": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.",
          "link": "http://arxiv.org/abs/2106.07250",
          "publishedOn": "2021-07-06T01:58:09.954Z",
          "wordCount": 570,
          "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Sunny Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_P/0/1/0/all/0/1\">Pranav Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakuwal_I/0/1/0/all/0/1\">Ishan Pakuwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_P/0/1/0/all/0/1\">Prabhakar Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_J/0/1/0/all/0/1\">Jayson Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "Can a machine learn Machine Learning? This work trains a machine learning\nmodel to solve machine learning problems from a University undergraduate level\ncourse. We generate a new training set of questions and answers consisting of\ncourse exercises, homework, and quiz questions from MIT's 6.036 Introduction to\nMachine Learning course and train a machine learning model to answer these\nquestions. Our system demonstrates an overall accuracy of 96% for open-response\nquestions and 97% for multiple-choice questions, compared with MIT students'\naverage of 93%, achieving grade A performance in the course, all in real-time.\nQuestions cover all 12 topics taught in the course, excluding coding questions\nor questions with images. Topics include: (i) basic machine learning\nprinciples; (ii) perceptrons; (iii) feature extraction and selection; (iv)\nlogistic regression; (v) regression; (vi) neural networks; (vii) advanced\nneural networks; (viii) convolutional neural networks; (ix) recurrent neural\nnetworks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii)\ndecision trees. Our system uses Transformer models within an encoder-decoder\narchitecture with graph and tree representations. An important aspect of our\napproach is a data-augmentation scheme for generating new example problems. We\nalso train a machine learning model to generate problem hints. Thus, our system\nautomatically generates new questions across topics, answers both open-response\nquestions and multiple-choice questions, classifies problems, and generates\nproblem hints, pushing the envelope of AI for STEM education.",
          "link": "http://arxiv.org/abs/2107.01238",
          "publishedOn": "2021-07-06T01:58:09.945Z",
          "wordCount": null,
          "title": "Solving Machine Learning Problems. (arXiv:2107.01238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongmei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tengxu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiawei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>",
          "description": "Recently years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\ncommunication framework whose design goal is to provide a system abstraction\nthat is both flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by the new system design, BAGUA has\na great ability to implement and extend various state-of-the-art distributed\nlearning algorithms. In a production cluster with up to 16 machines (128 GPUs),\nBAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training\ntime by a significant margin (up to 1.95 times) across a diverse range of\ntasks. Moreover, we conduct a rigorous tradeoff exploration showing that\ndifferent algorithms and system relaxations achieve the best performance over\ndifferent network conditions.",
          "link": "http://arxiv.org/abs/2107.01499",
          "publishedOn": "2021-07-06T01:58:09.942Z",
          "wordCount": 664,
          "title": "BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01214",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miller_B/0/1/0/all/0/1\">Benjamin Kurt Miller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cole_A/0/1/0/all/0/1\">Alex Cole</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Forre_P/0/1/0/all/0/1\">Patrick Forr&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weniger_C/0/1/0/all/0/1\">Christoph Weniger</a>",
          "description": "Parametric stochastic simulators are ubiquitous in science, often featuring\nhigh-dimensional input parameters and/or an intractable likelihood. Performing\nBayesian parameter inference in this context can be challenging. We present a\nneural simulator-based inference algorithm which simultaneously offers\nsimulation efficiency and fast empirical posterior testability, which is unique\namong modern algorithms. Our approach is simulation efficient by simultaneously\nestimating low-dimensional marginal posteriors instead of the joint posterior\nand by proposing simulations targeted to an observation of interest via a prior\nsuitably truncated by an indicator function. Furthermore, by estimating a\nlocally amortized posterior our algorithm enables efficient empirical tests of\nthe robustness of the inference results. Such tests are important for\nsanity-checking inference in real-world applications, which do not feature a\nknown ground truth. We perform experiments on a marginalized version of the\nsimulation-based inference benchmark and two complex and narrow posteriors,\nhighlighting the simulator efficiency of our algorithm as well as the quality\nof the estimated marginal posteriors. Implementation on GitHub.",
          "link": "http://arxiv.org/abs/2107.01214",
          "publishedOn": "2021-07-06T01:58:09.899Z",
          "wordCount": null,
          "title": "Truncated Marginal Neural Ratio Estimation. (arXiv:2107.01214v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07279",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1\">Mohammad Saber Iraji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1\">Jafar Tanha</a>",
          "description": "The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.",
          "link": "http://arxiv.org/abs/2104.07279",
          "publishedOn": "2021-07-06T01:58:09.898Z",
          "wordCount": 679,
          "title": "COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images. (arXiv:2104.07279v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
          "link": "http://arxiv.org/abs/2105.03075",
          "publishedOn": "2021-07-06T01:58:09.890Z",
          "wordCount": 672,
          "title": "A Survey of Data Augmentation Approaches for NLP. (arXiv:2105.03075v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drechsler_J/0/1/0/all/0/1\">Joerg Drechsler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1\">Ira Globus-Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1\">Audra McMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarathy_J/0/1/0/all/0/1\">Jayshree Sarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>",
          "description": "Differential privacy is a restriction on data processing algorithms that\nprovides strong confidentiality guarantees for individual records in the data.\nHowever, research on proper statistical inference, that is, research on\nproperly quantifying the uncertainty of the (noisy) sample estimate regarding\nthe true value in the population, is currently still limited. This paper\nproposes and evaluates several strategies to compute valid differentially\nprivate confidence intervals for the median. Instead of computing a\ndifferentially private point estimate and deriving its uncertainty, we directly\nestimate the interval bounds and discuss why this approach is superior if\nensuring privacy is important. We also illustrate that addressing both sources\nof uncertainty--the error from sampling and the error from protecting the\noutput--simultaneously should be preferred over simpler approaches that\nincorporate the uncertainty in a sequential fashion. We evaluate the\nperformance of the different algorithms under various parameter settings in\nextensive simulation studies and demonstrate how the findings could be applied\nin practical settings using data from the 1940 Decennial Census.",
          "link": "http://arxiv.org/abs/2106.10333",
          "publishedOn": "2021-07-06T01:58:09.883Z",
          "wordCount": 644,
          "title": "Non-parametric Differentially Private Confidence Intervals for the Median. (arXiv:2106.10333v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-07-06T01:58:09.807Z",
          "wordCount": 619,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zehao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "Domain generalization is challenging due to the domain shift and the\nuncertainty caused by the inaccessibility of target domain data. In this paper,\nwe address both challenges with a probabilistic framework based on variational\nBayesian inference, by incorporating uncertainty into neural network weights.\nWe couple domain invariance in a probabilistic formula with the variational\nBayesian inference. This enables us to explore domain-invariant learning in a\nprincipled way. Specifically, we derive domain-invariant representations and\nclassifiers, which are jointly established in a two-layer Bayesian neural\nnetwork. We empirically demonstrate the effectiveness of our proposal on four\nwidely used cross-domain visual recognition benchmarks. Ablation studies\nvalidate the synergistic benefits of our Bayesian treatment when jointly\nlearning domain-invariant representations and classifiers for domain\ngeneralization. Further, our method consistently delivers state-of-the-art mean\naccuracy on all benchmarks.",
          "link": "http://arxiv.org/abs/2105.04030",
          "publishedOn": "2021-07-06T01:58:09.785Z",
          "wordCount": 600,
          "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryner_D/0/1/0/all/0/1\">Darshan W. Bryner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>",
          "description": "Reinforcement learning in large-scale environments is challenging due to the\nmany possible actions that can be taken in specific situations. We have\npreviously developed a means of constraining, and hence speeding up, the search\nprocess through the use of motion primitives; motion primitives are sequences\nof pre-specified actions taken across a state series. As a byproduct of this\nwork, we have found that if the motion primitives' motions and actions are\nlabeled, then the search can be sped up further. Since motion primitives may\ninitially lack such details, we propose a theoretically viewpoint-insensitive\nand speed-insensitive means of automatically annotating the underlying motions\nand actions. We do this through a differential-geometric, spatio-temporal\nkinematics descriptor, which analyzes how the poses of entities in two motion\nsequences change over time. We use this descriptor in conjunction with a\nweighted-nearest-neighbor classifier to label the primitives using a limited\nset of training examples. In our experiments, we achieve high motion and action\nannotation rates for human-action-derived primitives with as few as one\ntraining sample. We also demonstrate that reinforcement learning using\naccurately labeled trajectories leads to high-performing policies more quickly\nthan standard reinforcement learning techniques. This is partly because motion\nprimitives encode prior domain knowledge and preempt the need to re-discover\nthat knowledge during training. It is also because agents can leverage the\nlabels to systematically ignore action classes that do not facilitate task\nobjectives, thereby reducing the action space.",
          "link": "http://arxiv.org/abs/2102.12017",
          "publishedOn": "2021-07-06T01:58:09.779Z",
          "wordCount": 725,
          "title": "Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning. (arXiv:2102.12017v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "In sequence-to-sequence learning, the decoder relies on the attention\nmechanism to efficiently extract information from the encoder. While it is\ncommon practice to draw information from only the last encoder layer, recent\nwork has proposed to use representations from different encoder layers for\ndiversified levels of information. Nonetheless, the decoder still obtains only\na single view of the source sequences, which might lead to insufficient\ntraining of the encoder layer stack due to the hierarchy bypassing problem. In\nthis work, we propose layer-wise cross-view decoding, where for each decoder\nlayer, together with the representations from the last encoder layer, which\nserve as a global view, those from other encoder layers are supplemented for a\nstereoscopic view of the source sequences. Systematic experiments show that we\nsuccessfully address the hierarchy bypassing problem and substantially improve\nthe performance of sequence-to-sequence learning with deep representations on\ndiverse tasks.",
          "link": "http://arxiv.org/abs/2005.08081",
          "publishedOn": "2021-07-06T01:58:09.770Z",
          "wordCount": 637,
          "title": "Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning. (arXiv:2005.08081v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1\">Kaitlin Maile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1\">Erwan Lecarpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1\">Herv&#xe9; Luga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Dennis G. Wilson</a>",
          "description": "Differentiable Architecture Search (DARTS) is a recently proposed neural\narchitecture search (NAS) method based on a differentiable relaxation. Due to\nits success, numerous variants analyzing and improving parts of the DARTS\nframework have recently been proposed. By considering the problem as a\nconstrained bilevel optimization, we propose and analyze three improvements to\narchitectural weight competition, update scheduling, and regularization towards\ndiscretization. First, we introduce a new approach to the activation of\narchitecture weights, which prevents confounding competition within an edge and\nallows for fair comparison across edges to aid in discretization. Next, we\npropose a dynamic schedule based on per-minibatch network information to make\narchitecture updates more informed. Finally, we consider two regularizations,\nbased on proximity to discretization and the Alternating Directions Method of\nMultipliers (ADMM) algorithm, to promote early discretization. Our results show\nthat this new activation scheme reduces final architecture size and the\nregularizations improve reliability in search results while maintaining\ncomparable performance to state-of-the-art in NAS, especially when used with\nour new dynamic informed schedule.",
          "link": "http://arxiv.org/abs/2106.11655",
          "publishedOn": "2021-07-06T01:58:09.752Z",
          "wordCount": 631,
          "title": "On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01408",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lee_H/0/1/0/all/0/1\">Hyungi Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yun_E/0/1/0/all/0/1\">Eunggu Yun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1\">Hongseok Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>",
          "description": "Recent works have revealed that infinitely-wide feed-forward or recurrent\nneural networks of any architecture correspond to Gaussian processes referred\nto as $\\mathrm{NNGP}$. While these works have extended the class of neural\nnetworks converging to Gaussian processes significantly, however, there has\nbeen little focus on broadening the class of stochastic processes that such\nneural networks converge to. In this work, inspired by the scale mixture of\nGaussian random variables, we propose the scale mixture of $\\mathrm{NNGP}$ for\nwhich we introduce a prior distribution on the scale of the last-layer\nparameters. We show that simply introducing a scale prior on the last-layer\nparameters can turn infinitely-wide neural networks of any architecture into a\nricher class of stochastic processes. Especially, with certain scale priors, we\nobtain heavy-tailed stochastic processes, and we recover Student's $t$\nprocesses in the case of inverse gamma priors. We further analyze the\ndistributions of the neural networks initialized with our prior setting and\ntrained with gradient descents and obtain similar results as for\n$\\mathrm{NNGP}$. We present a practical posterior-inference algorithm for the\nscale mixture of $\\mathrm{NNGP}$ and empirically demonstrate its usefulness on\nregression and classification tasks.",
          "link": "http://arxiv.org/abs/2107.01408",
          "publishedOn": "2021-07-06T01:58:09.745Z",
          "wordCount": 618,
          "title": "Scale Mixtures of Neural Network Gaussian Processes. (arXiv:2107.01408v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.00492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>",
          "description": "Sentiment analysis in conversations has gained increasing attention in recent\nyears for the growing amount of applications it can serve, e.g., sentiment\nanalysis, recommender systems, and human-robot interaction. The main difference\nbetween conversational sentiment analysis and single sentence sentiment\nanalysis is the existence of context information which may influence the\nsentiment of an utterance in a dialogue. How to effectively encode contextual\ninformation in dialogues, however, remains a challenge. Existing approaches\nemploy complicated deep learning structures to distinguish different parties in\na conversation and then model the context information. In this paper, we\npropose a fast, compact and parameter-efficient party-ignorant framework named\nbidirectional emotional recurrent unit for conversational sentiment analysis.\nIn our system, a generalized neural tensor block followed by a two-channel\nclassifier is designed to perform context compositionality and sentiment\nclassification, respectively. Extensive experiments on three standard datasets\ndemonstrate that our model outperforms the state of the art in most cases.",
          "link": "http://arxiv.org/abs/2006.00492",
          "publishedOn": "2021-07-06T01:58:09.738Z",
          "wordCount": 633,
          "title": "BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis. (arXiv:2006.00492v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1\">Vektor Dewanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1\">Marcus Gallagher</a>",
          "description": "In reinforcement learning (RL), the goal is to obtain an optimal policy, for\nwhich the optimality criterion is fundamentally important. Two major optimality\ncriteria are average and discounted rewards, where the later is typically\nconsidered as an approximation to the former. While the discounted reward is\nmore popular, it is problematic to apply in environments that have no natural\nnotion of discounting. This motivates us to revisit a) the progression of\noptimality criteria in dynamic programming, b) justification for and\ncomplication of an artificial discount factor, and c) benefits of directly\nmaximizing the average reward. Our contributions include a thorough examination\nof the relationship between average and discounted rewards, as well as a\ndiscussion of their pros and cons in RL. We emphasize that average-reward RL\nmethods possess the ingredient and mechanism for developing the general\ndiscounting-free optimality criterion (Veinott, 1969) in RL.",
          "link": "http://arxiv.org/abs/2107.01348",
          "publishedOn": "2021-07-06T01:58:09.731Z",
          "wordCount": 594,
          "title": "Examining average and discounted reward optimality criteria in reinforcement learning. (arXiv:2107.01348v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>",
          "description": "In problem-solving, we humans can come up with multiple novel solutions to\nthe same problem. However, reinforcement learning algorithms can only produce a\nset of monotonous policies that maximize the cumulative reward but lack\ndiversity and novelty. In this work, we address the problem of generating novel\npolicies in reinforcement learning tasks. Instead of following the\nmulti-objective framework used in existing methods, we propose to rethink the\nproblem under a novel perspective of constrained optimization. We first\nintroduce a new metric to evaluate the difference between policies and then\ndesign two practical novel policy generation methods following the new\nperspective. The two proposed methods, namely the Constrained Task Novel\nBisector (CTNB) and the Interior Policy Differentiation (IPD), are derived from\nthe feasible direction method and the interior point method commonly known in\nthe constrained optimization literature. Experimental comparisons on the MuJoCo\ncontrol suite show our methods can achieve substantial improvement over\nprevious novelty-seeking methods in terms of both the novelty of policies and\ntheir performances in the primal task.",
          "link": "http://arxiv.org/abs/2005.10696",
          "publishedOn": "2021-07-06T01:58:09.724Z",
          "wordCount": 639,
          "title": "Novel Policy Seeking with Constrained Optimization. (arXiv:2005.10696v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "Feedforward neural networks are widely used as universal predictive models to\nfit data distribution. Common gradient-based learning, however, suffers from\nmany drawbacks making the training process ineffective and time-consuming.\nAlternative randomized learning does not use gradients but selects hidden node\nparameters randomly. This makes the training process extremely fast. However,\nthe problem in randomized learning is how to determine the random parameters. A\nrecently proposed method uses autoencoders for unsupervised parameter learning.\nThis method showed superior performance on classification tasks. In this work,\nwe apply this method to regression problems, and, finding that it has some\ndrawbacks, we show how to improve it. We propose a learning method of\nautoencoders that controls the produced random weights. We also propose how to\ndetermine the biases of hidden nodes. We empirically compare autoencoder based\nlearning with other randomized learning methods proposed recently for\nregression and find that despite the proposed improvement of the autoencoder\nbased learning, it does not outperform its competitors in fitting accuracy.\nMoreover, the method is much more complex than its competitors.",
          "link": "http://arxiv.org/abs/2107.01711",
          "publishedOn": "2021-07-06T01:58:09.717Z",
          "wordCount": 616,
          "title": "Autoencoder based Randomized Learning of Feedforward Neural Networks for Regression. (arXiv:2107.01711v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christoph Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinov_T/0/1/0/all/0/1\">Teodor V. Marinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1\">Julian Zimmert</a>",
          "description": "We provide improved gap-dependent regret bounds for reinforcement learning in\nfinite episodic Markov decision processes. Compared to prior work, our bounds\ndepend on alternative definitions of gaps. These definitions are based on the\ninsight that, in order to achieve a favorable regret, an algorithm does not\nneed to learn how to behave optimally in states that are not reached by an\noptimal policy. We prove tighter upper regret bounds for optimistic algorithms\nand accompany them with new information-theoretic lower bounds for a large\nclass of MDPs. Our results show that optimistic algorithms can not achieve the\ninformation-theoretic lower bounds even in deterministic MDPs unless there is a\nunique optimal policy.",
          "link": "http://arxiv.org/abs/2107.01264",
          "publishedOn": "2021-07-06T01:58:09.699Z",
          "wordCount": 548,
          "title": "Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning. (arXiv:2107.01264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>",
          "description": "Academia and industry have developed several platforms to support the popular\nprivacy-preserving distributed learning method -- Federated Learning (FL).\nHowever, these platforms are complex to use and require a deep understanding of\nFL, which imposes high barriers to entry for beginners, limits the productivity\nof researchers, and compromises deployment efficiency. In this paper, we\npropose the first low-code FL platform, EasyFL, to enable users with various\nlevels of expertise to experiment and prototype FL applications with little\ncoding. We achieve this goal while ensuring great flexibility and extensibility\nfor customization by unifying simple API design, modular design, and granular\ntraining flow abstraction. With only a few lines of code, EasyFL empowers them\nwith many out-of-the-box functionalities to accelerate experimentation and\ndeployment. These practical functionalities are heterogeneity simulation,\ncomprehensive tracking, distributed training optimization, and seamless\ndeployment. They are proposed based on challenges identified in the proposed FL\nlife cycle. Compared with other platforms, EasyFL not only requires just three\nlines of code (at least 10x lesser) to build a vanilla FL application but also\nincurs lower training overhead. Besides, our evaluations demonstrate that\nEasyFL expedites distributed training by 1.5x. It also improves the efficiency\nof deployment. We believe that EasyFL will increase the productivity of\nresearchers and democratize FL to wider audiences.",
          "link": "http://arxiv.org/abs/2105.07603",
          "publishedOn": "2021-07-06T01:58:09.690Z",
          "wordCount": 682,
          "title": "EasyFL: A Low-code Federated Learning Platform For Dummies. (arXiv:2105.07603v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiping Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1\">H. V. Jagadish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>",
          "description": "Relational databases are the de facto standard for storing and querying\nstructured data, and extracting insights from structured data requires advanced\nanalytics. Deep neural networks (DNNs) have achieved super-human prediction\nperformance in particular data types, e.g., images. However, existing DNNs may\nnot produce meaningful results when applied to structured data. The reason is\nthat there are correlations and dependencies across combinations of attribute\nvalues in a table, and these do not follow simple additive patterns that can be\neasily mimicked by a DNN. The number of possible such cross features is\ncombinatorial, making them computationally prohibitive to model. Furthermore,\nthe deployment of learning models in real-world applications has also\nhighlighted the need for interpretability, especially for high-stakes\napplications, which remains another issue of concern to DNNs.\n\nIn this paper, we present ARM-Net, an adaptive relation modeling network\ntailored for structured data, and a lightweight framework ARMOR based on\nARM-Net for relational data analytics. The key idea is to model feature\ninteractions with cross features selectively and dynamically, by first\ntransforming the input features into exponential space, and then determining\nthe interaction order and interaction weights adaptively for each cross\nfeature. We propose a novel sparse attention mechanism to dynamically generate\nthe interaction weights given the input tuple, so that we can explicitly model\ncross features of arbitrary orders with noisy features filtered selectively.\nThen during model inference, ARM-Net can specify the cross features being used\nfor each prediction for higher accuracy and better interpretability. Our\nextensive experiments on real-world datasets demonstrate that ARM-Net\nconsistently outperforms existing models and provides more interpretable\npredictions for data-driven decision making.",
          "link": "http://arxiv.org/abs/2107.01830",
          "publishedOn": "2021-07-06T01:58:09.684Z",
          "wordCount": 724,
          "title": "ARM-Net: Adaptive Relation Modeling Network for Structured Data. (arXiv:2107.01830v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02130",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1\">Tianfang Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bokrantz_R/0/1/0/all/0/1\">Rasmus Bokrantz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Olsson_J/0/1/0/all/0/1\">Jimmy Olsson</a>",
          "description": "We present a new nonparametric mixture-of-experts model for multivariate\nregression problems, inspired by the probabilistic $k$-nearest neighbors\nalgorithm. Using a conditionally specified model, predictions for out-of-sample\ninputs are based on similarities to each observed data point, yielding\npredictive distributions represented by Gaussian mixtures. Posterior inference\nis performed on the parameters of the mixture components as well as the\ndistance metric using a mean-field variational Bayes algorithm accompanied with\na stochastic gradient-based optimization procedure. The proposed method is\nespecially advantageous in settings where inputs are of relatively high\ndimension in comparison to the data size, where input--output relationships are\ncomplex, and where predictive distributions may be skewed or multimodal.\nComputational studies on two synthetic datasets and one dataset comprising dose\nstatistics of radiation therapy treatment plans show that our\nmixture-of-experts method performs similarly or better than a conditional\nDirichlet process mixture model both in terms of validation metrics and visual\ninspection.",
          "link": "http://arxiv.org/abs/2012.02130",
          "publishedOn": "2021-07-06T01:58:09.677Z",
          "wordCount": 605,
          "title": "A similarity-based Bayesian mixture-of-experts model. (arXiv:2012.02130v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zewei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_L/0/1/0/all/0/1\">Liwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Muchao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Houping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>",
          "description": "Federated learning (FL) has emerged as an effective technique to co-training\nmachine learning models without actually sharing data and leaking privacy.\nHowever, most existing FL methods focus on the supervised setting and ignore\nthe utilization of unlabeled data. Although there are a few existing studies\ntrying to incorporate unlabeled data into FL, they all fail to maintain\nperformance guarantees or generalization ability in various real-world\nsettings. In this paper, we focus on designing a general framework FedSiam to\ntackle different scenarios of federated semi-supervised learning, including\nfour settings in the labels-at-client scenario and two setting in the\nlabels-at-server scenario. FedSiam is built upon a siamese network into FL with\na momentum update to handle the non-IID challenges introduced by unlabeled\ndata. We further propose a new metric to measure the divergence of local model\nlayers within the siamese network. Based on the divergence, FedSiam can\nautomatically select layer-level parameters to be uploaded to the server in an\nadaptive manner. Experimental results on three datasets under two scenarios\nwith different data distribution settings demonstrate that the proposed FedSiam\nframework outperforms state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2012.03292",
          "publishedOn": "2021-07-06T01:58:09.670Z",
          "wordCount": 644,
          "title": "FedSiam: Towards Adaptive Federated Semi-Supervised Learning. (arXiv:2012.03292v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01876",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zheng_X/0/1/0/all/0/1\">Xiangyu Zheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "This paper proposes an invariant causal predictor that is robust to\ndistribution shift across domains and maximally reserves the transferable\ninvariant information. Based on a disentangled causal factorization, we\nformulate the distribution shift as soft interventions in the system, which\ncovers a wide range of cases for distribution shift as we do not make prior\nspecifications on the causal structure or the intervened variables. Instead of\nimposing regularizations to constrain the invariance of the predictor, we\npropose to predict by the intervened conditional expectation based on the\ndo-operator and then prove that it is invariant across domains. More\nimportantly, we prove that the proposed predictor is the robust predictor that\nminimizes the worst-case quadratic loss among the distributions of all domains.\nFor empirical learning, we propose an intuitive and flexible estimating method\nbased on data regeneration and present a local causal discovery procedure to\nguide the regeneration step. The key idea is to regenerate data such that the\nregenerated distribution is compatible with the intervened graph, which allows\nus to incorporate standard supervised learning methods with the regenerated\ndata. Experimental results on both synthetic and real data demonstrate the\nefficacy of our predictor in improving the predictive accuracy and robustness\nacross domains.",
          "link": "http://arxiv.org/abs/2107.01876",
          "publishedOn": "2021-07-06T01:58:09.652Z",
          "wordCount": 630,
          "title": "Causally Invariant Predictor with Shift-Robustness. (arXiv:2107.01876v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Naftali Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_S/0/1/0/all/0/1\">Srijan Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1\">Tucker Balch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>",
          "description": "In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.",
          "link": "http://arxiv.org/abs/2107.01273",
          "publishedOn": "2021-07-06T01:58:09.646Z",
          "wordCount": 554,
          "title": "Visual Time Series Forecasting: An Image-driven Approach. (arXiv:2107.01273v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammadi Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1\">Avinash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1\">Aditya Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "We consider an improper reinforcement learning setting where a learner is\ngiven $M$ base controllers for an unknown Markov decision process, and wishes\nto combine them optimally to produce a potentially new controller that can\noutperform each of the base ones. This can be useful in tuning across\ncontrollers, learnt possibly in mismatched or simulated environments, to obtain\na good controller for a given target environment with relatively few trials.\n\n\\par We propose a gradient-based approach that operates over a class of\nimproper mixtures of the controllers. We derive convergence rate guarantees for\nthe approach assuming access to a gradient oracle. The value function of the\nmixture and its gradient may not be available in closed-form; however, we show\nthat we can employ rollouts and simultaneous perturbation stochastic\napproximation (SPSA) for explicit gradient descent optimization. Numerical\nresults on (i) the standard control theoretic benchmark of stabilizing an\ninverted pendulum and (ii) a constrained queueing task show that our improper\npolicy optimization algorithm can stabilize the system even when the base\npolicies at its disposal are unstable\\footnote{Under review. Please do not\ndistribute.}.",
          "link": "http://arxiv.org/abs/2102.08201",
          "publishedOn": "2021-07-06T01:58:09.637Z",
          "wordCount": 659,
          "title": "Improper Reinforcement Learning with Gradient-based Policy Optimization. (arXiv:2102.08201v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Titsias_M/0/1/0/all/0/1\">Michalis K. Titsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_F/0/1/0/all/0/1\">Francisco J. R. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoloutsopoulos_S/0/1/0/all/0/1\">Sotirios Nikoloutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1\">Alexandre Galashov</a>",
          "description": "We formulate meta learning using information theoretic concepts; namely,\nmutual information and the information bottleneck. The idea is to learn a\nstochastic representation or encoding of the task description, given by a\ntraining set, that is highly informative about predicting the validation set.\nBy making use of variational approximations to the mutual information, we\nderive a general and tractable framework for meta learning. This framework\nunifies existing gradient-based algorithms and also allows us to derive new\nalgorithms. In particular, we develop a memory-based algorithm that uses\nGaussian processes to obtain non-parametric encoding representations. We\ndemonstrate our method on a few-shot regression problem and on four few-shot\nclassification problems, obtaining competitive accuracy when compared to\nexisting baselines.",
          "link": "http://arxiv.org/abs/2009.03228",
          "publishedOn": "2021-07-06T01:58:09.629Z",
          "wordCount": 599,
          "title": "Information Theoretic Meta Learning with Gaussian Processes. (arXiv:2009.03228v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.11918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Adaptive gradient methods such as RMSProp and Adam use exponential moving\nestimate of the squared gradient to compute adaptive step sizes, achieving\nbetter convergence than SGD in face of noisy objectives. However, Adam can have\nundesirable convergence behaviors due to unstable or extreme adaptive learning\nrates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the\nadaptive learning rates of Adam in the later stage of training, but they do not\noutperform Adam in some practical tasks such as training Transformers\n\\cite{transformer}. In this paper, we propose an adaptive learning rate\nprinciple, in which the running mean of squared gradient in Adam is replaced by\na weighted mean, with weights chosen to maximize the estimated variance of each\ncoordinate. This results in a faster adaptation to the local gradient variance,\nwhich leads to more desirable empirical convergence behaviors than Adam. We\nprove the proposed algorithm converges under mild assumptions for nonconvex\nstochastic optimization problems, and demonstrate the improved efficacy of our\nadaptive averaging approach on machine translation, natural language\nunderstanding and large-batch pretraining of BERT. The code is available at\nhttps://github.com/zhuchen03/MaxVA.",
          "link": "http://arxiv.org/abs/2006.11918",
          "publishedOn": "2021-07-06T01:58:09.623Z",
          "wordCount": 686,
          "title": "MaxVA: Fast Adaptation of Step Sizes by Maximizing Observed Variance of Gradients. (arXiv:2006.11918v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.02325",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">David D. Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1\">Jennifer Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thakker_R/0/1/0/all/0/1\">Rohan Thakker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alatur_N/0/1/0/all/0/1\">Nikhilesh Alatur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agha_mohammadi_A/0/1/0/all/0/1\">Ali-akbar Agha-mohammadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos A. Theodorou</a>",
          "description": "Deep learning has enjoyed much recent success, and applying state-of-the-art\nmodel learning methods to controls is an exciting prospect. However, there is a\nstrong reluctance to use these methods on safety-critical systems, which have\nconstraints on safety, stability, and real-time performance. We propose a\nframework which satisfies these constraints while allowing the use of deep\nneural networks for learning model uncertainties. Central to our method is the\nuse of Bayesian model learning, which provides an avenue for maintaining\nappropriate degrees of caution in the face of the unknown. In the proposed\napproach, we develop an adaptive control framework leveraging the theory of\nstochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control\nBarrier Functions) along with tractable Bayesian model learning via Gaussian\nProcesses or Bayesian neural networks. Under reasonable assumptions, we\nguarantee stability and safety while adapting to unknown dynamics with\nprobability 1. We demonstrate this architecture for high-speed terrestrial\nmobility targeting potential applications in safety-critical high-speed Mars\nrover missions.",
          "link": "http://arxiv.org/abs/1910.02325",
          "publishedOn": "2021-07-06T01:58:09.605Z",
          "wordCount": 712,
          "title": "Bayesian Learning-Based Adaptive Control for Safety Critical Systems. (arXiv:1910.02325v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yujun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1\">Milad Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1\">Kevin Swersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Most graph convolutional neural networks (GCNs) perform poorly in graphs\nwhere neighbors typically have different features/classes (heterophily) and\nwhen stacking multiple layers (oversmoothing). These two seemingly unrelated\nproblems have been studied independently, but there is recent empirical\nevidence that solving one problem may benefit the other. In this work, going\nbeyond empirical observations, we aim to: (1) propose a new perspective to\nanalyze the heterophily and oversmoothing problems under a unified theoretical\nframework, (2) identify the common causes of the two problems based on the\nproposed framework, and (3) propose simple yet effective strategies that\naddress the common causes. Focusing on the node classification task, we use\nlinear separability of node representations as an indicator to reflect the\nperformance of GCNs and we propose to study the linear separability by\nanalyzing the statistical change of the node representations in the graph\nconvolution. We find that the relative degree of a node (compared to its\nneighbors) and the heterophily level of a node's neighborhood are the root\ncauses that influence the separability of node representations. Our analysis\nsuggests that: (1) Nodes with high heterophily always produce less separable\nrepresentations after graph convolution; (2) Even with low heterophily, degree\ndisparity between nodes can influence the network dynamics and result in a\npseudo-heterophily situation, which helps to explain oversmoothing. Based on\nour insights, we propose simple modifications to the GCN architecture -- i.e.,\ndegree corrections and signed messages -- which alleviate the root causes of\nthese issues, and also show this empirically on 9 real networks. Compared to\nother approaches, which tend to work well in one regime but fail in others, our\nmodified GCN model consistently performs well across all settings.",
          "link": "http://arxiv.org/abs/2102.06462",
          "publishedOn": "2021-07-06T01:58:09.598Z",
          "wordCount": 766,
          "title": "Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks. (arXiv:2102.06462v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tobing_P/0/1/0/all/0/1\">Patrick Lumban Tobing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.",
          "link": "http://arxiv.org/abs/2105.09858",
          "publishedOn": "2021-07-06T01:58:09.591Z",
          "wordCount": 681,
          "title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction. (arXiv:2105.09858v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Sandeep Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufraisse_M/0/1/0/all/0/1\">Marius Dufraisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_G/0/1/0/all/0/1\">Girish Varma</a>",
          "description": "Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.",
          "link": "http://arxiv.org/abs/2107.01358",
          "publishedOn": "2021-07-06T01:58:09.584Z",
          "wordCount": 623,
          "title": "CInC Flow: Characterizable Invertible 3x3 Convolution. (arXiv:2107.01358v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minkyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>",
          "description": "This paper studies probability distributions ofpenultimate activations of\nclassification networks.We show that, when a classification network istrained\nwith the cross-entropy loss, its final classi-fication layer forms\naGenerative-Discriminativepairwith a generative classifier based on a\nspecificdistribution of penultimate activations. More im-portantly, the\ndistribution is parameterized by theweights of the final fully-connected layer,\nand canbe considered as a generative model that synthe-sizes the penultimate\nactivations without feedinginput data. We empirically demonstrate that\nthisgenerative model enables stable knowledge dis-tillation in the presence of\ndomain shift, and cantransfer knowledge from a classifier to\nvariationalautoencoders and generative adversarial networksfor\nclass-conditional image generation.",
          "link": "http://arxiv.org/abs/2107.01900",
          "publishedOn": "2021-07-06T01:58:09.567Z",
          "wordCount": 538,
          "title": "On The Distribution of Penultimate Activations of Classification Networks. (arXiv:2107.01900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aditya Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "Despite the fact that deep reinforcement learning (RL) has surpassed\nhuman-level performances in various tasks, it still has several fundamental\nchallenges. First, most RL methods require intensive data from the exploration\nof the environment to achieve satisfactory performance. Second, the use of\nneural networks in RL renders it hard to interpret the internals of the system\nin a way that humans can understand. To address these two challenges, we\npropose a framework that enables an RL agent to reason over its exploration\nprocess and distill high-level knowledge for effectively guiding its future\nexplorations. Specifically, we propose a novel RL algorithm that learns\nhigh-level knowledge in the form of a finite reward automaton by using the L*\nlearning algorithm. We prove that in episodic RL, a finite reward automaton can\nexpress any non-Markovian bounded reward functions with finitely many reward\nvalues and approximate any non-Markovian bounded reward function (with\ninfinitely many reward values) with arbitrary precision. We also provide a\nlower bound for the episode length such that the proposed RL approach almost\nsurely converges to an optimal policy in the limit. We test this approach on\ntwo RL environments with non-Markovian reward functions, choosing a variety of\ntasks with increasing complexity for each environment. We compare our algorithm\nwith the state-of-the-art RL algorithms for non-Markovian reward functions,\nsuch as Joint Inference of Reward machines and Policies for RL (JIRP), Learning\nReward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show\nthat our algorithm converges to an optimal policy faster than other baseline\nmethods.",
          "link": "http://arxiv.org/abs/2006.15714",
          "publishedOn": "2021-07-06T01:58:09.560Z",
          "wordCount": 745,
          "title": "Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.04675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "Pruning unimportant parameters can allow deep neural networks (DNNs) to\nreduce their heavy computation and memory requirements. A saliency metric\nestimates which parameters can be safely pruned with little impact on the\nclassification performance of the DNN. Many saliency metrics have been\nproposed, each within the context of a wider pruning algorithm. The result is\nthat it is difficult to separate the effectiveness of the saliency metric from\nthe wider pruning algorithm that surrounds it. Similar-looking saliency metrics\ncan yield very different results because of apparently minor design choices. We\npropose a taxonomy of saliency metrics based on four mostly-orthogonal\nprincipal components. We show that a broad range of metrics from the pruning\nliterature can be grouped according to these components. Our taxonomy not only\nserves as a guide to prior work, but allows us to construct new saliency\nmetrics by exploring novel combinations of our taxonomic components. We perform\nan in-depth experimental investigation of more than 300 saliency metrics. Our\nresults provide decisive answers to open research questions, and demonstrate\nthe importance of reduction and scaling when pruning groups of weights. We find\nthat some of our constructed metrics can outperform the best existing\nstate-of-the-art metrics for convolutional neural network channel pruning.",
          "link": "http://arxiv.org/abs/1906.04675",
          "publishedOn": "2021-07-06T01:58:09.553Z",
          "wordCount": 662,
          "title": "Taxonomy of Saliency Metrics for Channel Pruning. (arXiv:1906.04675v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yue Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Off-policy evaluation (OPE) leverages data generated by other policies to\nevaluate a target policy. Previous OPE methods mainly focus on precisely\nestimating the true performance of a policy. We observe that in many\napplications, (1) the end goal of OPE is to compare two or multiple candidate\npolicies and choose a good one, which is actually a much simpler task than\nevaluating their true performance; and (2) there are usually multiple policies\nthat have been deployed in real-world systems and thus whose true performance\nis known through serving real users. Inspired by the two observations, in this\nwork, we define a new problem, supervised off-policy ranking (SOPR), which aims\nto rank a set of new/target policies based on supervised learning by leveraging\noff-policy data and policies with known performance. We further propose a\nmethod for supervised off-policy ranking that learns a policy scoring model by\ncorrectly ranking training policies with known performance rather than\nestimating their precise performance. Our method leverages logged states and\npolicies to learn a Transformer based model that maps offline interaction data\nincluding logged states and the actions taken by a target policy on these\nstates to a score. Experiments on different games, datasets, training policy\nsets, and test policy sets show that our method outperforms strong baseline OPE\nmethods in terms of both rank correlation and performance gap between the truly\nbest and the best of the ranked top three policies. Furthermore, our method is\nmore stable than baseline methods.",
          "link": "http://arxiv.org/abs/2107.01360",
          "publishedOn": "2021-07-06T01:58:09.545Z",
          "wordCount": 671,
          "title": "Supervised Off-Policy Ranking. (arXiv:2107.01360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1\">Reza Esfandiarpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajabdollahi_M/0/1/0/all/0/1\">Mohsen Hajabdollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>",
          "description": "In many practical few-shot learning problems, even though labeled examples\nare scarce, there are abundant auxiliary datasets that potentially contain\nuseful information. We propose the problem of extended few-shot learning to\nstudy these scenarios. We then introduce a framework to address the challenges\nof efficiently selecting and effectively using auxiliary data in few-shot image\nclassification. Given a large auxiliary dataset and a notion of semantic\nsimilarity among classes, we automatically select pseudo shots, which are\nlabeled examples from other classes related to the target task. We show that\nnaive approaches, such as (1) modeling these additional examples the same as\nthe target task examples or (2) using them to learn features via transfer\nlearning, only increase accuracy by a modest amount. Instead, we propose a\nmasking module that adjusts the features of auxiliary data to be more similar\nto those of the target classes. We show that this masking module performs\nbetter than naively modeling the support examples and transfer learning by 4.68\nand 6.03 percentage points, respectively.",
          "link": "http://arxiv.org/abs/2012.07176",
          "publishedOn": "2021-07-06T01:58:09.538Z",
          "wordCount": 656,
          "title": "Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks. (arXiv:2012.07176v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Modelling mix-and-match relationships among fashion items has become\nincreasingly demanding yet challenging for modern E-commerce recommender\nsystems. When performing clothes matching, most existing approaches leverage\nthe latent visual features extracted from fashion item images for compatibility\nmodelling, which lacks explainability of generated matching results and can\nhardly convince users of the recommendations. Though recent methods start to\nincorporate pre-defined attribute information (e.g., colour, style, length,\netc.) for learning item representations and improving the model\ninterpretability, their utilisation of attribute information is still mainly\nreserved for enhancing the learned item representations and generating\nexplanations via post-processing. As a result, this creates a severe bottleneck\nwhen we are trying to advance the recommendation accuracy and generating\nfine-grained explanations since the explicit attributes have only loose\nconnections to the actual recommendation process. This work aims to tackle the\nexplainability challenge in fashion recommendation tasks by proposing a novel\nAttribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender\nassesses the outfit compatibility by explicitly leveraging the extracted\nattribute-level representations from each item's visual feature. The attributes\nserve as the bridge between two fashion items, where we quantify the affinity\nof a pair of items through the learned compatibility between their attributes.\nExtensive experiments have demonstrated that, by making full use of the\nexplicit attributes in the recommendation process, AFRec is able to achieve\nstate-of-the-art recommendation accuracy and generate intuitive explanations at\nthe same time.",
          "link": "http://arxiv.org/abs/2107.01655",
          "publishedOn": "2021-07-06T01:58:09.518Z",
          "wordCount": 654,
          "title": "Attribute-aware Explainable Complementary Clothing Recommendation. (arXiv:2107.01655v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elesedy_B/0/1/0/all/0/1\">Bryn Elesedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1\">Varun Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "We analyse the pruning procedure behind the lottery ticket hypothesis\narXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear\nmodels trained by gradient flow. We begin by presenting sufficient conditions\non the statistical structure of the features under which IMP prunes those\nfeatures that have smallest projection onto the data. Following this, we\nexplore IMP as a method for sparse estimation.",
          "link": "http://arxiv.org/abs/2007.08243",
          "publishedOn": "2021-07-06T01:58:09.511Z",
          "wordCount": 549,
          "title": "Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning. (arXiv:2007.08243v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagstaff_E/0/1/0/all/0/1\">Edward Wagstaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_F/0/1/0/all/0/1\">Fabian B. Fuchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelcke_M/0/1/0/all/0/1\">Martin Engelcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1\">Michael A. Osborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>",
          "description": "Modelling functions of sets, or equivalently, permutation-invariant\nfunctions, is a long-standing challenge in machine learning. Deep Sets is a\npopular method which is known to be a universal approximator for continuous set\nfunctions. We provide a theoretical analysis of Deep Sets which shows that this\nuniversal approximation property is only guaranteed if the model's latent space\nis sufficiently high-dimensional. If the latent space is even one dimension\nlower than necessary, there exist piecewise-affine functions for which Deep\nSets performs no better than a na\\\"ive constant baseline, as judged by\nworst-case error. Deep Sets may be viewed as the most efficient incarnation of\nthe Janossy pooling paradigm. We identify this paradigm as encompassing most\ncurrently popular set-learning methods. Based on this connection, we discuss\nthe implications of our results for set learning more broadly, and identify\nsome open questions on the universality of Janossy pooling in general.",
          "link": "http://arxiv.org/abs/2107.01959",
          "publishedOn": "2021-07-06T01:58:09.504Z",
          "wordCount": 585,
          "title": "Universal Approximation of Functions on Sets. (arXiv:2107.01959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vadillo_J/0/1/0/all/0/1\">Jon Vadillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1\">Roberto Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_J/0/1/0/all/0/1\">Jose A. Lozano</a>",
          "description": "Reliable deployment of machine learning models such as neural networks\ncontinues to be challenging due to several limitations. Some of the main\nshortcomings are the lack of interpretability and the lack of robustness\nagainst adversarial examples or out-of-distribution inputs. In this paper, we\nexplore the possibilities and limits of adversarial attacks for explainable\nmachine learning models. First, we extend the notion of adversarial examples to\nfit in explainable machine learning scenarios, in which the inputs, the output\nclassifications and the explanations of the model's decisions are assessed by\nhumans. Next, we propose a comprehensive framework to study whether (and how)\nadversarial examples can be generated for explainable models under human\nassessment, introducing novel attack paradigms. In particular, our framework\nconsiders a wide range of relevant (yet often ignored) factors such as the type\nof problem, the user expertise or the objective of the explanations in order to\nidentify the attack strategies that should be adopted in each scenario to\nsuccessfully deceive the model (and the human). These contributions intend to\nserve as a basis for a more rigorous and realistic study of adversarial\nexamples in the field of explainable machine learning.",
          "link": "http://arxiv.org/abs/2107.01943",
          "publishedOn": "2021-07-06T01:58:09.498Z",
          "wordCount": 639,
          "title": "When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1\">Eduard Gorbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burlachenko_K/0/1/0/all/0/1\">Konstantin Burlachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>",
          "description": "We develop and analyze MARINA: a new communication efficient method for\nnon-convex distributed learning over heterogeneous datasets. MARINA employs a\nnovel communication compression strategy based on the compression of gradient\ndifferences that is reminiscent of but different from the strategy employed in\nthe DIANA method of Mishchenko et al. (2019). Unlike virtually all competing\ndistributed first-order methods, including DIANA, ours is based on a carefully\ndesigned biased gradient estimator, which is the key to its superior\ntheoretical and practical performance. The communication complexity bounds we\nprove for MARINA are evidently better than those of all previous first-order\nmethods. Further, we develop and analyze two variants of MARINA: VR-MARINA and\nPP-MARINA. The first method is designed for the case when the local loss\nfunctions owned by clients are either of a finite sum or of an expectation\nform, and the second method allows for a partial participation of clients -- a\nfeature important in federated learning. All our methods are superior to\nprevious state-of-the-art methods in terms of oracle/communication complexity.\nFinally, we provide a convergence analysis of all methods for problems\nsatisfying the Polyak-Lojasiewicz condition.",
          "link": "http://arxiv.org/abs/2102.07845",
          "publishedOn": "2021-07-06T01:58:09.487Z",
          "wordCount": 658,
          "title": "MARINA: Faster Non-Convex Distributed Learning with Compression. (arXiv:2102.07845v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Kangle Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>",
          "description": "We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.",
          "link": "http://arxiv.org/abs/2001.04463",
          "publishedOn": "2021-07-06T01:58:09.480Z",
          "wordCount": 626,
          "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders. (arXiv:2001.04463v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacotte_J/0/1/0/all/0/1\">Jonathan Lacotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>",
          "description": "We prove that finding all globally optimal two-layer ReLU neural networks can\nbe performed by solving a convex optimization program with cone constraints.\nOur analysis is novel, characterizes all optimal solutions, and does not\nleverage duality-based analysis which was recently used to lift neural network\ntraining into convex spaces. Given the set of solutions of our convex\noptimization program, we show how to construct exactly the entire set of\noptimal neural networks. We provide a detailed characterization of this optimal\nset and its invariant transformations. As additional consequences of our convex\nperspective, (i) we establish that Clarke stationary points found by stochastic\ngradient descent correspond to the global optimum of a subsampled convex\nproblem (ii) we provide a polynomial-time algorithm for checking if a neural\nnetwork is a global minimum of the training loss (iii) we provide an explicit\nconstruction of a continuous path between any neural network and the global\nminimum of its sublevel set and (iv) characterize the minimal size of the\nhidden layer so that the neural network optimization landscape has no spurious\nvalleys. Overall, we provide a rich framework for studying the landscape of\nneural network training loss through convexity.",
          "link": "http://arxiv.org/abs/2006.05900",
          "publishedOn": "2021-07-06T01:58:09.463Z",
          "wordCount": 668,
          "title": "All Local Minima are Global for Two-Layer ReLU Neural Networks: The Hidden Convex Optimization Landscape. (arXiv:2006.05900v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01850",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1\">Chandler Squires</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Transforming a causal system from a given initial state to a desired target\nstate is an important task permeating multiple fields including control theory,\nbiology, and materials science. In causal models, such transformations can be\nachieved by performing a set of interventions. In this paper, we consider the\nproblem of identifying a shift intervention that matches the desired mean of a\nsystem through active learning. We define the Markov equivalence class that is\nidentifiable from shift interventions and propose two active learning\nstrategies that are guaranteed to exactly match a desired mean. We then derive\na worst-case lower bound for the number of interventions required and show that\nthese strategies are optimal for certain classes of graphs. In particular, we\nshow that our strategies may require exponentially fewer interventions than the\npreviously considered approaches, which optimize for structure learning in the\nunderlying causal graph. In line with our theoretical results, we also\ndemonstrate experimentally that our proposed active learning strategies require\nfewer interventions compared to several baselines.",
          "link": "http://arxiv.org/abs/2107.01850",
          "publishedOn": "2021-07-06T01:58:09.456Z",
          "wordCount": 601,
          "title": "Matching a Desired Causal State via Shift Interventions. (arXiv:2107.01850v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2006.16785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasser_P/0/1/0/all/0/1\">Pablo Strasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "Despite the recent success of reinforcement learning in various domains,\nthese approaches remain, for the most part, deterringly sensitive to\nhyper-parameters and are often riddled with essential engineering feats\nallowing their success. We consider the case of off-policy generative\nadversarial imitation learning, and perform an in-depth review, qualitative and\nquantitative, of the method. We show that forcing the learned reward function\nto be local Lipschitz-continuous is a sine qua non condition for the method to\nperform well. We then study the effects of this necessary condition and provide\nseveral theoretical results involving the local Lipschitzness of the\nstate-value function. We complement these guarantees with empirical evidence\nattesting to the strong positive effect that the consistent satisfaction of the\nLipschitzness constraint on the reward has on imitation performance. Finally,\nwe tackle a generic pessimistic reward preconditioning add-on spawning a large\nclass of reward shaping methods, which makes the base method it is plugged into\nprovably more robust, as shown in several additional theoretical guarantees. We\nthen discuss these through a fine-grained lens and share our insights.\nCrucially, the guarantees derived and reported in this work are valid for any\nreward satisfying the Lipschitzness condition, nothing is specific to\nimitation. As such, these may be of independent interest.",
          "link": "http://arxiv.org/abs/2006.16785",
          "publishedOn": "2021-07-06T01:58:09.449Z",
          "wordCount": 674,
          "title": "Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhisong Pan</a>",
          "description": "Despite the empirical success of deep learning, it still lacks theoretical\nunderstandings to explain why randomly initialized neural network trained by\nfirst-order optimization methods is able to achieve zero training loss, even\nthough its landscape is non-convex and non-smooth. Recently, there are some\nworks to demystifies this phenomenon under over-parameterized regime. In this\nwork, we make further progress on this area by considering a commonly used\nmomentum optimization algorithm: Nesterov accelerated method (NAG). We analyze\nthe convergence of NAG for two-layer fully connected neural network with ReLU\nactivation. Specifically, we prove that the error of NAG converges to zero at a\nlinear convergence rate $1-\\Theta(1/\\sqrt{\\kappa})$, where $\\kappa > 1$ is\ndetermined by the initialization and the architecture of neural network.\nComparing to the rate $1-\\Theta(1/\\kappa)$ of gradient descent, NAG achieves an\nacceleration. Besides, it also validates NAG and Heavy-ball method can achieve\na similar convergence rate.",
          "link": "http://arxiv.org/abs/2107.01832",
          "publishedOn": "2021-07-06T01:58:09.443Z",
          "wordCount": 583,
          "title": "Provable Convergence of Nesterov Accelerated Method for Over-Parameterized Neural Networks. (arXiv:2107.01832v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szelogowski_D/0/1/0/all/0/1\">Daniel Szelogowski</a>",
          "description": "Current computational-emotion research has focused on applying acoustic\nproperties to analyze how emotions are perceived mathematically or used in\nnatural language processing machine learning models. While recent interest has\nfocused on analyzing emotions from the spoken voice, little experimentation has\nbeen performed to discover how emotions are recognized in the singing voice --\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\ndifficult to interpret, has corrupted/distorted/nonsense information like\nactual noise sounds in this case, or has a low ratio of usable/unusable\ninformation). Not only does this ignore the challenges of training machine\nlearning models on more subjective data and testing them with much noisier\ndata, but there is also a clear disconnect in progress between advancing the\ndevelopment of convolutional neural networks and the goal of emotionally\ncognizant artificial intelligence. By training a new model to include this type\nof information with a rich comprehension of psycho-acoustic properties, not\nonly can models be trained to recognize information within extremely noisy\ndata, but advancement can be made toward more complex biofeedback applications\n-- including creating a model which could recognize emotions given any human\ninformation (language, breath, voice, body, posture) and be used in any\nperformance medium (music, speech, acting) or psychological assistance for\npatients with disorders such as BPD, alexithymia, autism, among others. This\npaper seeks to reflect and expand upon the findings of related research and\npresent a stepping-stone toward this end goal.",
          "link": "http://arxiv.org/abs/2105.00173",
          "publishedOn": "2021-07-06T01:58:09.425Z",
          "wordCount": 733,
          "title": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers. (arXiv:2105.00173v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asadi_R/0/1/0/all/0/1\">Reza Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regan_A/0/1/0/all/0/1\">Amelia Regan</a>",
          "description": "Time Series data are broadly studied in various domains of transportation\nsystems. Traffic data area challenging example of spatio-temporal data, as it\nis multi-variate time series with high correlations in spatial and temporal\nneighborhoods. Spatio-temporal clustering of traffic flow data find similar\npatterns in both spatial and temporal domain, where it provides better\ncapability for analyzing a transportation network, and improving related\nmachine learning models, such as traffic flow prediction and anomaly detection.\nIn this paper, we propose a spatio-temporal clustering model, where it clusters\ntime series data based on spatial and temporal contexts. We propose a variation\nof a Deep Embedded Clustering(DEC) model for finding spatio-temporal clusters.\nThe proposed model Spatial-DEC (S-DEC) use prior geographical information in\nbuilding latent feature representations. We also define evaluation metrics for\nspatio-temporal clusters. Not only do the obtained clusters have better\ntemporal similarity when evaluated using DTW distance, but also the clusters\nbetter represents spatial connectivity and dis-connectivity. We use traffic\nflow data obtained by PeMS in our analysis. The results show that the proposed\nSpatial-DEC can find more desired spatio-temporal clusters.",
          "link": "http://arxiv.org/abs/2107.01310",
          "publishedOn": "2021-07-06T01:58:09.419Z",
          "wordCount": 607,
          "title": "Clustering of Time Series Data with Prior Geographical Information. (arXiv:2107.01310v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.07248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ustimenko_A/0/1/0/all/0/1\">Aleksei Ustimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1\">Liudmila Prokhorenkova</a>",
          "description": "This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a\npowerful and efficient machine learning framework that may deal with a wide\nrange of loss functions and has provable generalization guarantees. The method\nis based on a special form of the Langevin diffusion equation specifically\ndesigned for gradient boosting. This allows us to theoretically guarantee the\nglobal convergence even for multimodal loss functions, while standard gradient\nboosting algorithms can guarantee only local optimum. We also empirically show\nthat SGLB outperforms classic gradient boosting when applied to classification\ntasks with 0-1 loss function, which is known to be multimodal.",
          "link": "http://arxiv.org/abs/2001.07248",
          "publishedOn": "2021-07-06T01:58:09.408Z",
          "wordCount": 568,
          "title": "SGLB: Stochastic Gradient Langevin Boosting. (arXiv:2001.07248v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1807.04209",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Dwork_C/0/1/0/all/0/1\">Cynthia Dwork</a>, <a href=\"http://arxiv.org/find/math/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Differential privacy provides a rigorous framework for privacy-preserving\ndata analysis. This paper proposes the first differentially private procedure\nfor controlling the false discovery rate (FDR) in multiple hypothesis testing.\nInspired by the Benjamini-Hochberg procedure (BHq), our approach is to first\nrepeatedly add noise to the logarithms of the $p$-values to ensure differential\nprivacy and to select an approximately smallest $p$-value serving as a\npromising candidate at each iteration; the selected $p$-values are further\nsupplied to the BHq and our private procedure releases only the rejected ones.\nMoreover, we develop a new technique that is based on a backward submartingale\nfor proving FDR control of a broad class of multiple testing procedures,\nincluding our private procedure, and both the BHq step-up and step-down\nprocedures. As a novel aspect, the proof works for arbitrary dependence between\nthe true null and false null test statistics, while FDR control is maintained\nup to a small multiplicative factor.",
          "link": "http://arxiv.org/abs/1807.04209",
          "publishedOn": "2021-07-06T01:58:09.400Z",
          "wordCount": 612,
          "title": "Differentially Private False Discovery Rate Control. (arXiv:1807.04209v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1\">Felix Leibfried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1\">Vincent Dutordoir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1\">ST John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1\">Nicolas Durrande</a>",
          "description": "Gaussian processes (GPs) provide a framework for Bayesian inference that can\noffer principled uncertainty estimates for a large range of problems. For\nexample, if we consider regression problems with Gaussian likelihoods, a GP\nmodel enjoys a posterior in closed form. However, identifying the posterior GP\nscales cubically with the number of training examples and requires to store all\nexamples in memory. In order to overcome these obstacles, sparse GPs have been\nproposed that approximate the true posterior GP with pseudo-training examples.\nImportantly, the number of pseudo-training examples is user-defined and enables\ncontrol over computational and memory complexity. In the general case, sparse\nGPs do not enjoy closed-form solutions and one has to resort to approximate\ninference. In this context, a convenient choice for approximate inference is\nvariational inference (VI), where the problem of Bayesian inference is cast as\nan optimization problem -- namely, to maximize a lower bound of the log\nmarginal likelihood. This paves the way for a powerful and versatile framework,\nwhere pseudo-training examples are treated as optimization arguments of the\napproximate posterior that are jointly identified together with hyperparameters\nof the generative model (i.e. prior and likelihood). The framework can\nnaturally handle a wide scope of supervised learning problems, ranging from\nregression with heteroscedastic and non-Gaussian likelihoods to classification\nproblems with discrete labels, but also multilabel problems. The purpose of\nthis tutorial is to provide access to the basic matter for readers without\nprior knowledge in both GPs and VI. A proper exposition to the subject enables\nalso access to more recent advances (like importance-weighted VI as well as\ninterdomain, multioutput and deep GPs) that can serve as an inspiration for new\nresearch ideas.",
          "link": "http://arxiv.org/abs/2012.13962",
          "publishedOn": "2021-07-06T01:58:09.393Z",
          "wordCount": 825,
          "title": "A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v11 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vovk_V/0/1/0/all/0/1\">Vladimir Vovk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petej_I/0/1/0/all/0/1\">Ivan Petej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gammerman_A/0/1/0/all/0/1\">Alex Gammerman</a>",
          "description": "This note proposes a way of making probability forecasting rules less\nsensitive to changes in data distribution, concentrating on the simple case of\nbinary classification. This is important in applications of machine learning,\nwhere the quality of a trained predictor may drop significantly in the process\nof its exploitation. Our techniques are based on recent work on conformal test\nmartingales and older work on prediction with expert advice, namely tracking\nthe best expert.",
          "link": "http://arxiv.org/abs/2107.01726",
          "publishedOn": "2021-07-06T01:58:09.374Z",
          "wordCount": 507,
          "title": "Adaptive calibration for binary classification. (arXiv:2107.01726v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1\">Antonio Alguacil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_W/0/1/0/all/0/1\">Wagner Gon&#xe7;alves Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1\">Michael Bauerheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1\">Marc C. Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1\">St&#xe9;phane Moreau</a>",
          "description": "Accurate modeling of boundary conditions is crucial in computational physics.\nThe ever increasing use of neural networks as surrogates for physics-related\nproblems calls for an improved understanding of boundary condition treatment,\nand its influence on the network accuracy. In this paper, several strategies to\nimpose boundary conditions (namely padding, improved spatial context, and\nexplicit encoding of physical boundaries) are investigated in the context of\nfully convolutional networks applied to recurrent tasks. These strategies are\nevaluated on two spatio-temporal evolving problems modeled by partial\ndifferential equations: the 2D propagation of acoustic waves (hyperbolic PDE)\nand the heat equation (parabolic PDE). Results reveal a high sensitivity of\nboth accuracy and stability on the boundary implementation in such recurrent\ntasks. It is then demonstrated that the choice of the optimal padding strategy\nis directly linked to the data semantics. Furthermore, the inclusion of\nadditional input spatial context or explicit physics-based rules allows a\nbetter handling of boundaries in particular for large number of recurrences,\nresulting in more robust and stable neural networks, while facilitating the\ndesign and versatility of such networks.",
          "link": "http://arxiv.org/abs/2106.11160",
          "publishedOn": "2021-07-06T01:58:09.330Z",
          "wordCount": 668,
          "title": "Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.10190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "Despite the empirical success of using Adversarial Training to defend deep\nlearning models against adversarial perturbations, so far, it still remains\nrather unclear what the principles are behind the existence of adversarial\nperturbations, and what adversarial training does to the neural network to\nremove them.\n\nIn this paper, we present a principle that we call Feature Purification,\nwhere we show one of the causes of the existence of adversarial examples is the\naccumulation of certain small dense mixtures in the hidden weights during the\ntraining process of a neural network; and more importantly, one of the goals of\nadversarial training is to remove such mixtures to purify hidden weights. We\npresent both experiments on the CIFAR-10 dataset to illustrate this principle,\nand a theoretical result proving that for certain natural classification tasks,\ntraining a two-layer neural network with ReLU activation using randomly\ninitialized gradient descent indeed satisfies this principle.\n\nTechnically, we give, to the best of our knowledge, the first result proving\nthat the following two can hold simultaneously for training a neural network\nwith ReLU activation. (1) Training over the original data is indeed non-robust\nto small adversarial perturbations of some radius. (2) Adversarial training,\neven with an empirical perturbation algorithm such as FGM, can in fact be\nprovably robust against ANY perturbations of the same radius. Finally, we also\nprove a complexity lower bound, showing that low complexity models such as\nlinear classifiers, low-degree polynomials, or even the neural tangent kernel\nfor this network, CANNOT defend against perturbations of this same radius, no\nmatter what algorithms are used to train them.",
          "link": "http://arxiv.org/abs/2005.10190",
          "publishedOn": "2021-07-06T01:58:09.317Z",
          "wordCount": 752,
          "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning. (arXiv:2005.10190v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternative\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-07-06T01:58:09.306Z",
          "wordCount": 744,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+el_Bouri_R/0/1/0/all/0/1\">Rasheed el-Bouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tingting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>",
          "description": "Given the abundance and ease of access of personal data today, individual\nprivacy has become of paramount importance, particularly in the healthcare\ndomain. In this work, we aim to utilise patient data extracted from multiple\nhospital data centres to train a machine learning model without sacrificing\npatient privacy. We develop a scheduling algorithm in conjunction with a\nstudent-teacher algorithm that is deployed in a federated manner. This allows a\ncentral model to learn from batches of data at each federal node. The teacher\nacts between data centres to update the main task (student) algorithm using the\ndata that is stored in the various data centres. We show that the scheduler,\ntrained using meta-gradients, can effectively organise training and as a result\ntrain a machine learning model on a diverse dataset without needing explicit\naccess to the patient data. We achieve state-of-the-art performance and show\nhow our method overcomes some of the problems faced in the federated learning\nsuch as node poisoning. We further show how the scheduler can be used as a\nmechanism for transfer learning, allowing different teachers to work together\nin training a student for state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2107.01707",
          "publishedOn": "2021-07-06T01:58:09.281Z",
          "wordCount": 638,
          "title": "Towards Scheduling Federated Deep Learning using Meta-Gradients for Inter-Hospital Learning. (arXiv:2107.01707v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01562",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>",
          "description": "This article gives a new proof that fully connected neural networks with\nrandom weights and biases converge to Gaussian processes in the regime where\nthe input dimension, output dimension, and depth are kept fixed, while the\nhidden layer widths tend to infinity. Unlike prior work, convergence is shown\nassuming only moment conditions for the distribution of weights and for quite\ngeneral non-linearities.",
          "link": "http://arxiv.org/abs/2107.01562",
          "publishedOn": "2021-07-06T01:58:09.273Z",
          "wordCount": 502,
          "title": "Random Neural Networks in the Infinite Width Limit as Gaussian Processes. (arXiv:2107.01562v1 [math.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mougan_C/0/1/0/all/0/1\">Carlos Mougan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nin_J/0/1/0/all/0/1\">Jordi Nin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujol_O/0/1/0/all/0/1\">Oriol Pujol</a>",
          "description": "Regression problems have been widely studied in machinelearning literature\nresulting in a plethora of regression models and performance measures. However,\nthere are few techniques specially dedicated to solve the problem of how to\nincorporate categorical features to regression problems. Usually, categorical\nfeature encoders are general enough to cover both classification and regression\nproblems. This lack of specificity results in underperforming regression\nmodels. In this paper,we provide an in-depth analysis of how to tackle high\ncardinality categor-ical features with the quantile. Our proposal outperforms\nstate-of-the-encoders, including the traditional statistical mean target\nencoder, when considering the Mean Absolute Error, especially in the presence\nof long-tailed or skewed distributions. Besides, to deal with possible\noverfitting when there are categories with small support, our encoder benefits\nfrom additive smoothing. Finally, we describe how to expand the encoded values\nby creating a set of features with different quantiles. This expanded encoder\nprovides a more informative output about the categorical feature in question,\nfurther boosting the performance of the regression model.",
          "link": "http://arxiv.org/abs/2105.13783",
          "publishedOn": "2021-07-06T01:58:09.267Z",
          "wordCount": 647,
          "title": "Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems. (arXiv:2105.13783v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1\">Uday Singh Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devineni_P/0/1/0/all/0/1\">Pravallika Devineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1\">Evangelos E. Papalexakis</a>",
          "description": "Tools to analyze the latent space of deep neural networks provide a step\ntowards better understanding them. In this work, we motivate sparse subspace\nclustering (SSC) with an aim to learn affinity graphs from the latent structure\nof a given neural network layer trained over a set of inputs. We then use tools\nfrom Community Detection to quantify structures present in the input. These\nexperiments reveal that as we go deeper in a network, inputs tend to have an\nincreasing affinity to other inputs of the same class. Subsequently, we utilise\nmatrix similarity measures to perform layer-wise comparisons between affinity\ngraphs. In doing so we first demonstrate that when comparing a given layer\ncurrently under training to its final state, the shallower the layer of the\nnetwork, the quicker it is to converge than the deeper layers. When performing\na pairwise analysis of the entire network architecture, we observe that, as the\nnetwork increases in size, it reorganises from a state where each layer is\nmoderately similar to its neighbours, to a state where layers within a block\nhave high similarity than to layers in other blocks. Finally, we analyze the\nlearned affinity graphs of the final convolutional layer of the network and\ndemonstrate how an input's local neighbourhood affects its classification by\nthe network.",
          "link": "http://arxiv.org/abs/2107.01296",
          "publishedOn": "2021-07-06T01:58:09.260Z",
          "wordCount": 643,
          "title": "Subspace Clustering Based Analysis of Neural Networks. (arXiv:2107.01296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sedova_A/0/1/0/all/0/1\">Anastasiia Sedova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_A/0/1/0/all/0/1\">Andreas Stephan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speranskaya_M/0/1/0/all/0/1\">Marina Speranskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>",
          "description": "Strategies for improving the training and prediction quality of weakly\nsupervised machine learning models vary in how much they are tailored to a\nspecific task or integrated with a specific model architecture. In this work,\nwe introduce Knodle, a software framework that treats weak data annotations,\ndeep learning models, and methods for improving weakly supervised training as\nseparate, modular components. This modularization gives the training process\naccess to fine-grained information such as data set characteristics, matches of\nheuristic rules, or elements of the deep learning model ultimately used for\nprediction. Hence, our framework can encompass a wide range of training methods\nfor improving weak supervision, ranging from methods that only look at\ncorrelations of rules and output classes (independently of the machine learning\nmodel trained with the resulting labels), to those that harness the interplay\nof neural networks and weakly labeled data. We illustrate the benchmarking\npotential of the framework with a performance comparison of several reference\nimplementations on a selection of datasets that are already available in\nKnodle.\n\nThe framework is published as an open-source Python package knodle and\navailable at https://github.com/knodle/knodle.",
          "link": "http://arxiv.org/abs/2104.11557",
          "publishedOn": "2021-07-06T01:58:09.244Z",
          "wordCount": 652,
          "title": "Knodle: Modular Weakly Supervised Learning with PyTorch. (arXiv:2104.11557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xueying Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>",
          "description": "Active learning aims to achieve greater accuracy with less training data by\nselecting the most useful data samples from which it learns. Single-criterion\nbased methods (i.e., informativeness and representativeness based methods) are\nsimple and efficient; however, they lack adaptability to different real-world\nscenarios. In this paper, we introduce a multiple-criteria based active\nlearning algorithm, which incorporates three complementary criteria, i.e.,\ninformativeness, representativeness and diversity, to make appropriate\nselections in the active learning rounds under different data types. We\nconsider the selection process as a Determinantal Point Process, which good\nbalance among these criteria. We refine the query selection strategy by both\nselecting the hardest unlabeled data sample and biasing towards the classifiers\nthat are more suitable for the current data distribution. In addition, we also\nconsider the dependencies and relationships between these data points in data\nselection by means of centroidbased clustering approaches. Through evaluations\non synthetic and real-world datasets, we show that our method performs\nsignificantly better and is more stable than other multiple-criteria based AL\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.01622",
          "publishedOn": "2021-07-06T01:58:09.235Z",
          "wordCount": 599,
          "title": "Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes. (arXiv:2107.01622v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Botteghi_N/0/1/0/all/0/1\">Nicol&#xf2; Botteghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poel_M/0/1/0/all/0/1\">Mannes Poel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirmacek_B/0/1/0/all/0/1\">Beril Sirmacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>",
          "description": "Deep Reinforcement Learning has shown its ability in solving complicated\nproblems directly from high-dimensional observations. However, in end-to-end\nsettings, Reinforcement Learning algorithms are not sample-efficient and\nrequires long training times and quantities of data. In this work, we proposed\na framework for sample-efficient Reinforcement Learning that take advantage of\nstate and action representations to transform a high-dimensional problem into a\nlow-dimensional one. Moreover, we seek to find the optimal policy mapping\nlatent states to latent actions. Because now the policy is learned on abstract\nrepresentations, we enforce, using auxiliary loss functions, the lifting of\nsuch policy to the original problem domain. Results show that the novel\nframework can efficiently learn low-dimensional and interpretable state and\naction representations and the optimal latent policy.",
          "link": "http://arxiv.org/abs/2107.01677",
          "publishedOn": "2021-07-06T01:58:09.216Z",
          "wordCount": 560,
          "title": "Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics. (arXiv:2107.01677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ultsch_A/0/1/0/all/0/1\">Alfred Ultsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">J&#xf6;rg Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohnert_M/0/1/0/all/0/1\">Maximilian R&#xf6;hnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonin_M/0/1/0/all/0/1\">Malte Von Bonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oelschlagel_U/0/1/0/all/0/1\">Uta Oelschl&#xe4;gel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_C/0/1/0/all/0/1\">Cornelia Brendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrun_M/0/1/0/all/0/1\">Michael C. Thrun</a>",
          "description": "Typical state of the art flow cytometry data samples consists of measures of\nmore than 100.000 cells in 10 or more features. AI systems are able to diagnose\nsuch data with almost the same accuracy as human experts. However, there is one\ncentral challenge in such systems: their decisions have far-reaching\nconsequences for the health and life of people, and therefore, the decisions of\nAI systems need to be understandable and justifiable by humans. In this work,\nwe present a novel explainable AI method, called ALPODS, which is able to\nclassify (diagnose) cases based on clusters, i.e., subpopulations, in the\nhigh-dimensional data. ALPODS is able to explain its decisions in a form that\nis understandable for human experts. For the identified subpopulations, fuzzy\nreasoning rules expressed in the typical language of domain experts are\ngenerated. A visualization method based on these rules allows human experts to\nunderstand the reasoning used by the AI system. A comparison to a selection of\nstate of the art explainable AI systems shows that ALPODS operates efficiently\non known benchmark data and also on everyday routine case data.",
          "link": "http://arxiv.org/abs/2107.01820",
          "publishedOn": "2021-07-06T01:58:09.209Z",
          "wordCount": 657,
          "title": "An Explainable AI System for the Diagnosis of High Dimensional Biomedical Data. (arXiv:2107.01820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Putra_R/0/1/0/all/0/1\">Rachmad Vidya Wicaksana Putra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "A prominent technique for reducing the memory footprint of Spiking Neural\nNetworks (SNNs) without decreasing the accuracy significantly is quantization.\nHowever, the state-of-the-art only focus on employing the weight quantization\ndirectly from a specific quantization scheme, i.e., either the post-training\nquantization (PTQ) or the in-training quantization (ITQ), and do not consider\n(1) quantizing other SNN parameters (e.g., neuron membrane potential), (2)\nexploring different combinations of quantization approaches (i.e., quantization\nschemes, precision levels, and rounding schemes), and (3) selecting the SNN\nmodel with a good memory-accuracy trade-off at the end. Therefore, the memory\nsaving offered by these state-of-the-art to meet the targeted accuracy is\nlimited, thereby hindering processing SNNs on the resource-constrained systems\n(e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel\nquantization framework for memory-efficient SNNs. The key mechanisms of the\nQ-SpiNN are: (1) employing quantization for different SNN parameters based on\ntheir significance to the accuracy, (2) exploring different combinations of\nquantization schemes, precision levels, and rounding schemes to find efficient\nSNN model candidates, and (3) developing an algorithm that quantifies the\nbenefit of the memory-accuracy trade-off obtained by the candidates, and\nselects the Pareto-optimal one. The experimental results show that, for the\nunsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while\nmaintaining the accuracy within 1% from the baseline on the MNIST dataset. For\nthe supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping\nthe accuracy within 2% from the baseline on the DVS-Gesture dataset.",
          "link": "http://arxiv.org/abs/2107.01807",
          "publishedOn": "2021-07-06T01:58:09.202Z",
          "wordCount": 705,
          "title": "Q-SpiNN: A Framework for Quantizing Spiking Neural Networks. (arXiv:2107.01807v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shachaf_G/0/1/0/all/0/1\">Gal Shachaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brutzkus_A/0/1/0/all/0/1\">Alon Brutzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>",
          "description": "Fine-tuning is a common practice in deep learning, achieving excellent\ngeneralization results on downstream tasks using relatively little training\ndata. Although widely used in practice, it is lacking strong theoretical\nunderstanding. We analyze the sample complexity of this scheme for regression\nwith linear teachers in several architectures. Intuitively, the success of\nfine-tuning depends on the similarity between the source tasks and the target\ntask, however measuring it is non trivial. We show that a relevant measure\nconsiders the relation between the source task, the target task and the\ncovariance structure of the target data. In the setting of linear regression,\nwe show that under realistic settings a substantial sample complexity reduction\nis plausible when the above measure is low. For deep linear regression, we\npresent a novel result regarding the inductive bias of gradient-based training\nwhen the network is initialized with pretrained weights. Using this result we\nshow that the similarity measure for this setting is also affected by the depth\nof the network. We further present results on shallow ReLU models, and analyze\nthe dependence of sample complexity there on source and target tasks. We\nempirically demonstrate our results for both synthetic and realistic data.",
          "link": "http://arxiv.org/abs/2107.01641",
          "publishedOn": "2021-07-06T01:58:09.176Z",
          "wordCount": 624,
          "title": "A Theoretical Analysis of Fine-tuning with Linear Teachers. (arXiv:2107.01641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nilsen_G/0/1/0/all/0/1\">Geir K. Nilsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munthe_Kaas_A/0/1/0/all/0/1\">Antonella Z. Munthe-Kaas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skaug_H/0/1/0/all/0/1\">Hans J. Skaug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_M/0/1/0/all/0/1\">Morten Brun</a>",
          "description": "We validate the recently introduced deep learning classification adapted\nDelta method by a comparison with the classical Bootstrap. We show that there\nis a strong linear relationship between the quantified predictive epistemic\nuncertainty levels obtained from the two methods when applied on two\nLeNet-based neural network classifiers using the MNIST and CIFAR-10 datasets.\nFurthermore, we demonstrate that the Delta method offers a five times\ncomputation time reduction compared to the Bootstrap.",
          "link": "http://arxiv.org/abs/2107.01606",
          "publishedOn": "2021-07-06T01:58:09.169Z",
          "wordCount": 521,
          "title": "A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification. (arXiv:2107.01606v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "We formally study how ensemble of deep learning models can improve test\naccuracy, and how the superior performance of ensemble can be distilled into a\nsingle model using knowledge distillation. We consider the challenging case\nwhere the ensemble is simply an average of the outputs of a few independently\ntrained neural networks with the SAME architecture, trained using the SAME\nalgorithm on the SAME data set, and they only differ by the random seeds used\nin the initialization.\n\nWe empirically show that ensemble/knowledge distillation in deep learning\nworks very differently from traditional learning theory, especially differently\nfrom ensemble of random feature mappings or the neural-tangent-kernel feature\nmappings, and is potentially out of the scope of existing theorems. Thus, to\nproperly understand ensemble and knowledge distillation in deep learning, we\ndevelop a theory showing that when data has a structure we refer to as\n\"multi-view\", then ensemble of independently trained neural networks can\nprovably improve test accuracy, and such superior test accuracy can also be\nprovably distilled into a single model by training a single model to match the\noutput of the ensemble instead of the true label. Our result sheds light on how\nensemble works in deep learning in a way that is completely different from\ntraditional theorems, and how the \"dark knowledge\" is hidden in the outputs of\nthe ensemble -- that can be used in knowledge distillation -- comparing to the\ntrue data labels. In the end, we prove that self-distillation can also be\nviewed as implicitly combining ensemble and knowledge distillation to improve\ntest accuracy.",
          "link": "http://arxiv.org/abs/2012.09816",
          "publishedOn": "2021-07-06T01:58:09.105Z",
          "wordCount": 736,
          "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning. (arXiv:2012.09816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1\">Alain Rakotomamonjy</a> (DocApp - LITIS), <a href=\"http://arxiv.org/find/cs/1/au:+Ralaivola_L/0/1/0/all/0/1\">Liva Ralaivola</a>",
          "description": "Developing machine learning methods that are privacy preserving is today a\ncentral topic of research, with huge practical impacts. Among the numerous ways\nto address privacy-preserving learning, we here take the perspective of\ncomputing the divergences between distributions under the Differential Privacy\n(DP) framework -- being able to compute divergences between distributions is\npivotal for many machine learning problems, such as learning generative models\nor domain adaptation problems. Instead of resorting to the popular\ngradient-based sanitization method for DP, we tackle the problem at its roots\nby focusing on the Sliced Wasserstein Distance and seamlessly making it\ndifferentially private. Our main contribution is as follows: we analyze the\nproperty of adding a Gaussian perturbation to the intrinsic randomized\nmechanism of the Sliced Wasserstein Distance, and we establish the\nsensitivityof the resulting differentially private mechanism. One of our\nimportant findings is that this DP mechanism transforms the Sliced Wasserstein\ndistance into another distance, that we call the Smoothed Sliced Wasserstein\nDistance. This new differentially private distribution distance can be plugged\ninto generative models and domain adaptation algorithms in a transparent way,\nand we empirically show that it yields highly competitive performance compared\nwith gradient-based DP approaches from the literature, with almost no loss in\naccuracy for the domain adaptation problems that we consider.",
          "link": "http://arxiv.org/abs/2107.01848",
          "publishedOn": "2021-07-06T01:58:09.091Z",
          "wordCount": 654,
          "title": "Differentially Private Sliced Wasserstein Distance. (arXiv:2107.01848v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1\">Jakob Heiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1\">Josef Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1\">Hanna Wutte</a>",
          "description": "Today, various forms of neural networks are trained to perform approximation\ntasks in many fields. However, the estimates obtained are not fully understood\non function space. Empirical results suggest that typical training algorithms\nfavor regularized solutions. These observations motivate us to analyze\nproperties of the neural networks found by gradient descent initialized close\nto zero, that is frequently employed to perform the training task. As a\nstarting point, we consider one dimensional (shallow) ReLU neural networks in\nwhich weights are chosen randomly and only the terminal layer is trained.\nFirst, we rigorously show that for such networks ridge regularized regression\ncorresponds in function space to regularizing the estimate's second derivative\nfor fairly general loss functionals. For least squares regression, we show that\nthe trained network converges to the smooth spline interpolation of the\ntraining data as the number of hidden nodes tends to infinity. Moreover, we\nderive a correspondence between the early stopped gradient descent and the\nsmoothing spline regression. Our analysis might give valuable insight on the\nproperties of the solutions obtained using gradient descent methods in general\nsettings.",
          "link": "http://arxiv.org/abs/1911.02903",
          "publishedOn": "2021-07-06T01:58:09.074Z",
          "wordCount": 704,
          "title": "How Implicit Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part I: the 1-D Case of Two Layers with Random First Layer. (arXiv:1911.02903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Transfer-based adversarial attacks can effectively evaluate model robustness\nin the black-box setting. Though several methods have demonstrated impressive\ntransferability of untargeted adversarial examples, targeted adversarial\ntransferability is still challenging. The existing methods either have low\ntargeted transferability or sacrifice computational efficiency. In this paper,\nwe develop a simple yet practical framework to efficiently craft targeted\ntransfer-based adversarial examples. Specifically, we propose a conditional\ngenerative attacking model, which can generate the adversarial examples\ntargeted at different classes by simply altering the class embedding and share\na single backbone. Extensive experiments demonstrate that our method improves\nthe success rates of targeted black-box attacks by a significant margin over\nthe existing methods -- it reaches an average success rate of 29.6\\% against\nsix diverse models based only on one substitute white-box model in the standard\ntesting of NeurIPS 2017 competition, which outperforms the state-of-the-art\ngradient-based attack methods (with an average success rate of $<$2\\%) by a\nlarge margin. Moreover, the proposed method is also more efficient beyond an\norder of magnitude than gradient-based methods.",
          "link": "http://arxiv.org/abs/2107.01809",
          "publishedOn": "2021-07-06T01:58:09.061Z",
          "wordCount": 611,
          "title": "Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks. (arXiv:2107.01809v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lawless_C/0/1/0/all/0/1\">Connor Lawless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>",
          "description": "In recent years, machine learning has begun automating decision making in\nfields as varied as college admissions, credit lending, and criminal\nsentencing. The socially sensitive nature of some of these applications\ntogether with increasing regulatory constraints has necessitated the need for\nalgorithms that are both fair and interpretable. In this paper we consider the\nproblem of building Boolean rule sets in disjunctive normal form (DNF), an\ninterpretable model for binary classification, subject to fairness constraints.\nWe formulate the problem as an integer program that maximizes classification\naccuracy with explicit constraints on two different measures of classification\nparity: equality of opportunity and equalized odds. Column generation\nframework, with a novel formulation, is used to efficiently search over\nexponentially many possible rules. When combined with faster heuristics, our\nmethod can deal with large data-sets. Compared to other fair and interpretable\nclassifiers, our method is able to find rule sets that meet stricter notions of\nfairness with a modest trade-off in accuracy.",
          "link": "http://arxiv.org/abs/2107.01325",
          "publishedOn": "2021-07-06T01:58:08.884Z",
          "wordCount": 590,
          "title": "Fair Decision Rules for Binary Classification. (arXiv:2107.01325v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pauloski_J/0/1/0/all/0/1\">J. Gregory Pauloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chard_K/0/1/0/all/0/1\">Kyle Chard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>",
          "description": "Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to\nconverge faster in deep neural network (DNN) training than stochastic gradient\ndescent (SGD); however, K-FAC's larger memory footprint hinders its\napplicability to large models. We present KAISA, a K-FAC-enabled, Adaptable,\nImproved, and ScAlable second-order optimizer framework that adapts the memory\nfootprint, communication, and computation given specific models and hardware to\nachieve maximized performance and enhanced scalability. We quantify the\ntradeoffs between memory and communication cost and evaluate KAISA on large\nmodels, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA\nA100 GPUs. Compared to the original optimizers, KAISA converges 18.1-36.3%\nfaster across applications with the same global batch size. Under a fixed\nmemory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and\nBERT-Large, respectively. KAISA can balance memory and communication to achieve\nscaling efficiency equal to or better than the baseline optimizers.",
          "link": "http://arxiv.org/abs/2107.01739",
          "publishedOn": "2021-07-06T01:58:08.876Z",
          "wordCount": 613,
          "title": "KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural Networks. (arXiv:2107.01739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong-Yeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Wan Choi</a>",
          "description": "Continual learning has been a major problem in the deep learning community,\nwhere the main challenge is how to effectively learn a series of newly arriving\ntasks without forgetting the knowledge of previous tasks. Initiated by Learning\nwithout Forgetting (LwF), many of the existing works report that knowledge\ndistillation is effective to preserve the previous knowledge, and hence they\ncommonly use a soft label for the old task, namely a knowledge distillation\n(KD) loss, together with a class label for the new task, namely a cross entropy\n(CE) loss, to form a composite loss for a single neural network. However, this\napproach suffers from learning the knowledge by a CE loss as a KD loss often\nmore strongly influences the objective function when they are in a competitive\nsituation within a single network. This could be a critical problem\nparticularly in a class incremental scenario, where the knowledge across tasks\nas well as within the new task, both of which can only be acquired by a CE\nloss, is essentially learned due to the existence of a unified classifier. In\nthis paper, we propose a novel continual learning method, called\nSplit-and-Bridge, which can successfully address the above problem by partially\nsplitting a neural network into two partitions for training the new task\nseparated from the old task and re-connecting them for learning the knowledge\nacross tasks. In our thorough experimental analysis, our Split-and-Bridge\nmethod outperforms the state-of-the-art competitors in KD-based continual\nlearning.",
          "link": "http://arxiv.org/abs/2107.01349",
          "publishedOn": "2021-07-06T01:58:08.870Z",
          "wordCount": 704,
          "title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network. (arXiv:2107.01349v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "Modeling complex physical dynamics is a fundamental task in science and\nengineering. Traditional physics-based models are interpretable but rely on\nrigid assumptions. And the direct numerical approximation is usually\ncomputationally intensive, requiring significant computational resources and\nexpertise. While deep learning (DL) provides novel alternatives for efficiently\nrecognizing complex patterns and emulating nonlinear dynamics, it does not\nnecessarily obey the governing laws of physical systems, nor do they generalize\nwell across different systems. Thus, the study of physics-guided DL emerged and\nhas gained great progress. It aims to take the best from both physics-based\nmodeling and state-of-the-art DL models to better solve scientific problems. In\nthis paper, we provide a structured overview of existing methodologies of\nintegrating prior physical knowledge or physics-based modeling into DL and\ndiscuss the emerging opportunities.",
          "link": "http://arxiv.org/abs/2107.01272",
          "publishedOn": "2021-07-06T01:58:08.863Z",
          "wordCount": 553,
          "title": "Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Telukunta_M/0/1/0/all/0/1\">Mukund Telukunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadendla_V/0/1/0/all/0/1\">Venkata Sriram Siddhardh Nadendla</a>",
          "description": "Bias evaluation in machine-learning based services (MLS) based on traditional\nalgorithmic fairness notions that rely on comparative principles is practically\ndifficult, making it necessary to rely on human auditor feedback. However, in\nspite of taking rigorous training on various comparative fairness notions,\nhuman auditors are known to disagree on various aspects of fairness notions in\npractice, making it difficult to collect reliable feedback. This paper offers a\nparadigm shift to the domain of algorithmic fairness via proposing a new\nfairness notion based on the principle of non-comparative justice. In contrary\nto traditional fairness notions where the outcomes of two individuals/groups\nare compared, our proposed notion compares the MLS' outcome with a desired\noutcome for each input. This desired outcome naturally describes a human\nauditor's expectation, and can be easily used to evaluate MLS on crowd-auditing\nplatforms. We show that any MLS can be deemed fair from the perspective of\ncomparative fairness (be it in terms of individual fairness, statistical\nparity, equal opportunity or calibration) if it is non-comparatively fair with\nrespect to a fair auditor. We also show that the converse holds true in the\ncontext of individual fairness. Given that such an evaluation relies on the\ntrustworthiness of the auditor, we also present an approach to identify fair\nand reliable auditors by estimating their biases with respect to a given set of\nsensitive attributes, as well as quantify the uncertainty in the estimation of\nbiases within a given MLS. Furthermore, all of the above results are also\nvalidated on COMPAS, German credit and Adult Census Income datasets.",
          "link": "http://arxiv.org/abs/2107.01277",
          "publishedOn": "2021-07-06T01:58:08.833Z",
          "wordCount": 709,
          "title": "Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions. (arXiv:2107.01277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hlavac_J/0/1/0/all/0/1\">Jaroslav Hlav&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Martin Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1\">Jan Kohout</a>",
          "description": "The nearest prototype classification is a less computationally intensive\nreplacement for the $k$-NN method, especially when large datasets are\nconsidered. In metric spaces, centroids are often used as prototypes to\nrepresent whole clusters. The selection of cluster prototypes in non-metric\nspaces is more challenging as the idea of computing centroids is not directly\napplicable.\n\nIn this paper, we present CRS, a novel method for selecting a small yet\nrepresentative subset of objects as a cluster prototype. Memory and\ncomputationally efficient selection of representatives is enabled by leveraging\nthe similarity graph representation of each cluster created by the NN-Descent\nalgorithm. CRS can be used in an arbitrary metric or non-metric space because\nof the graph-based approach, which requires only a pairwise similarity measure.\nAs we demonstrate in the experimental evaluation, our method outperforms the\nstate of the art techniques on multiple datasets from different domains.",
          "link": "http://arxiv.org/abs/2107.01345",
          "publishedOn": "2021-07-06T01:58:08.827Z",
          "wordCount": 575,
          "title": "Cluster Representatives Selection in Non-Metric Spaces for Nearest Prototype Classification. (arXiv:2107.01345v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">M. A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corvonato_A/0/1/0/all/0/1\">A. Corvonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">C. Heumann</a>",
          "description": "The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.",
          "link": "http://arxiv.org/abs/2102.12330",
          "publishedOn": "2021-07-06T01:58:08.819Z",
          "wordCount": 642,
          "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models. (arXiv:2102.12330v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiexia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Furong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Juanjuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kejiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>",
          "description": "Accurate traffic state prediction is the foundation of transportation control\nand guidance. It is very challenging due to the complex spatiotemporal\ndependencies in traffic data. Existing works cannot perform well for multi-step\ntraffic prediction that involves long future time period. The spatiotemporal\ninformation dilution becomes serve when the time gap between input step and\npredicted step is large, especially when traffic data is not sufficient or\nnoisy. To address this issue, we propose a multi-spatial graph convolution\nbased Seq2Seq model. Our main novelties are three aspects: (1) We enrich the\nspatiotemporal information of model inputs by fusing multi-view features (time,\nlocation and traffic states) (2) We build multiple kinds of spatial\ncorrelations based on both prior knowledge and data-driven knowledge to improve\nmodel performance especially in insufficient or noisy data cases. (3) A\nspatiotemporal attention mechanism based on reachability knowledge is novelly\ndesigned to produce high-level features fed into decoder of Seq2Seq directly to\nease information dilution. Our model is evaluated on two real world traffic\ndatasets and achieves better performance than other competitors.",
          "link": "http://arxiv.org/abs/2107.01528",
          "publishedOn": "2021-07-06T01:58:08.812Z",
          "wordCount": 626,
          "title": "Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting. (arXiv:2107.01528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1\">Paulito P. Palmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1\">Akihiro Kishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1\">Radu Marinescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1\">Elizabeth Daly</a>",
          "description": "The pipeline optimization problem in machine learning requires simultaneous\noptimization of pipeline structures and parameter adaptation of their elements.\nHaving an elegant way to express these structures can help lessen the\ncomplexity in the management and analysis of their performances together with\nthe different choices of optimization strategies. With these issues in mind, we\ncreated the AMLP toolkit which facilitates the creation and evaluation of\ncomplex machine learning pipeline structures using simple expressions. We use\nAMLP to find optimal pipeline signatures, datamine them, and use these\ndatamined features to speed-up learning and prediction. We formulated a\ntwo-stage pipeline optimization with surrogate modeling in AMLP which\noutperforms other AutoML approaches with a 4-hour time budget in less than 5\nminutes of AMLP computation time.",
          "link": "http://arxiv.org/abs/2107.01253",
          "publishedOn": "2021-07-06T01:58:08.806Z",
          "wordCount": 564,
          "title": "Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01777",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Singh_S/0/1/0/all/0/1\">Shashank Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Khim_J/0/1/0/all/0/1\">Justin Khim</a>",
          "description": "Within the vast body of statistical theory developed for binary\nclassification, few meaningful results exist for imbalanced classification, in\nwhich data are dominated by samples from one of the two classes. Existing\ntheory faces at least two main challenges. First, meaningful results must\nconsider more complex performance measures than classification accuracy. To\naddress this, we characterize a novel generalization of the Bayes-optimal\nclassifier to any performance metric computed from the confusion matrix, and we\nuse this to show how relative performance guarantees can be obtained in terms\nof the error of estimating the class probability function under uniform\n($\\mathcal{L}_\\infty$) loss. Second, as we show, optimal classification\nperformance depends on certain properties of class imbalance that have not\npreviously been formalized. Specifically, we propose a novel sub-type of class\nimbalance, which we call Uniform Class Imbalance. We analyze how Uniform Class\nImbalance influences optimal classifier performance and show that it\nnecessitates different classifier behavior than other types of class imbalance.\nWe further illustrate these two contributions in the case of $k$-nearest\nneighbor classification, for which we develop novel guarantees. Together, these\nresults provide some of the first meaningful finite-sample statistical theory\nfor imbalanced binary classification.",
          "link": "http://arxiv.org/abs/2107.01777",
          "publishedOn": "2021-07-06T01:58:08.777Z",
          "wordCount": 637,
          "title": "Statistical Theory for Imbalanced Binary Classification. (arXiv:2107.01777v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shih-Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilak_V/0/1/0/all/0/1\">Vimal Thilak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1\">Etai Littwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saremi_O/0/1/0/all/0/1\">Omid Saremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>",
          "description": "Deep linear networks trained with gradient descent yield low rank solutions,\nas is typically studied in matrix factorization. In this paper, we take a step\nfurther and analyze implicit rank regularization in autoencoders. We show\ngreedy learning of low-rank latent codes induced by a linear sub-network at the\nautoencoder bottleneck. We further propose orthogonal initialization and\nprincipled learning rate adjustment to mitigate sensitivity of training\ndynamics to spectral prior and linear depth. With linear autoencoders on\nsynthetic data, our method converges stably to ground-truth latent code rank.\nWith nonlinear autoencoders, our method converges to latent ranks optimal for\ndownstream classification and image sampling.",
          "link": "http://arxiv.org/abs/2107.01301",
          "publishedOn": "2021-07-06T01:58:08.770Z",
          "wordCount": 540,
          "title": "Implicit Greedy Rank Learning in Autoencoders via Overparameterized Linear Networks. (arXiv:2107.01301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02931",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Giampouras_P/0/1/0/all/0/1\">Paris V. Giampouras</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rontogiannis_A/0/1/0/all/0/1\">Athanasios A. Rontogiannis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kofidis_E/0/1/0/all/0/1\">Eleftherios Kofidis</a>",
          "description": "The so-called block-term decomposition (BTD) tensor model, especially in its\nrank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention\ndue to its enhanced ability of representing systems and signals that are\ncomposed of \\emph{blocks} of rank higher than one, a scenario encountered in\nnumerous and diverse applications. Uniqueness conditions and fitting methods\nhave thus been thoroughly studied. Nevertheless, the challenging problem of\nestimating the BTD model structure, namely the number of block terms, $R$, and\ntheir individual ranks, $L_r$, has only recently started to attract significant\nattention, mainly through regularization-based approaches which entail the need\nto tune the regularization parameter(s). In this work, we build on ideas of\nsparse Bayesian learning (SBL) and put forward a fully automated Bayesian\napproach. Through a suitably crafted multi-level \\emph{hierarchical}\nprobabilistic model, which gives rise to heavy-tailed prior distributions for\nthe BTD factors, structured sparsity is \\emph{jointly} imposed. Ranks are then\nestimated from the numbers of blocks ($R$) and columns ($L_r$) of\nnon-negligible energy. Approximate posterior inference is implemented, within\nthe variational inference framework. The resulting iterative algorithm\ncompletely avoids hyperparameter tuning, which is a significant defect of\nregularization-based methods. Alternative probabilistic models are also\nexplored and the connections with their regularization-based counterparts are\nbrought to light with the aid of the associated maximum a-posteriori (MAP)\nestimators. We report simulation results with both synthetic and real-word\ndata, which demonstrate the merits of the proposed method in terms of both rank\nestimation and model fitting as compared to state-of-the-art relevant methods.",
          "link": "http://arxiv.org/abs/2101.02931",
          "publishedOn": "2021-07-06T01:58:08.764Z",
          "wordCount": 703,
          "title": "Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way. (arXiv:2101.02931v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "This work contributes to the development of a new data-driven method (D-DM)\nof feedforward neural networks (FNNs) learning. This method was proposed\nrecently as a way of improving randomized learning of FNNs by adjusting the\nnetwork parameters to the target function fluctuations. The method employs\nlogistic sigmoid activation functions for hidden nodes. In this study, we\nintroduce other activation functions, such as bipolar sigmoid, sine function,\nsaturating linear functions, reLU, and softplus. We derive formulas for their\nparameters, i.e. weights and biases. In the simulation study, we evaluate the\nperformance of FNN data-driven learning with different activation functions.\nThe results indicate that the sigmoid activation functions perform much better\nthan others in the approximation of complex, fluctuated target functions.",
          "link": "http://arxiv.org/abs/2107.01702",
          "publishedOn": "2021-07-06T01:58:08.753Z",
          "wordCount": 565,
          "title": "Data-Driven Learning of Feedforward Neural Networks with Different Activation Functions. (arXiv:2107.01702v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01502",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinglong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_N/0/1/0/all/0/1\">Ning Huang</a>",
          "description": "Pulmonary vessel segmentation is important for clinical diagnosis of\npulmonary diseases, while is also challenging due to the complicated structure.\nIn this work, we present an effective framework and refinement process of\npulmonary vessel segmentation from chest computed tomographic (CT) images. The\nkey to our approach is a 2.5D segmentation network applied from three\northogonal axes, which presents a robust and fully automated pulmonary vessel\nsegmentation result with lower network complexity and memory usage compared to\n3D networks. The slice radius is introduced to convolve the adjacent\ninformation of the center slice and the multi-planar fusion optimizes the\npresentation of intra- and inter- slice features. Besides, the tree-like\nstructure of the pulmonary vessel is extracted in the post-processing process,\nwhich is used for segmentation refining and pruning. In the evaluation\nexperiments, three fusion methods are tested and the most promising one is\ncompared with the state-of-the-art 2D and 3D structures on 300 cases of lung\nimages randomly selected from LIDC dataset. Our method outperforms other\nnetwork structures by a large margin and achieves by far the highest average\nDICE score of 0.9272 and precision of 0.9310, as per our knowledge from the\npulmonary vessel segmentation models available in the literature.",
          "link": "http://arxiv.org/abs/2107.01502",
          "publishedOn": "2021-07-06T01:58:08.746Z",
          "wordCount": 677,
          "title": "Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images. (arXiv:2107.01502v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1\">Weiming Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1\">Lionel Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonilla_E/0/1/0/all/0/1\">Edwin V. Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1\">Fabio Ramos</a>",
          "description": "Advances in differentiable numerical integrators have enabled the use of\ngradient descent techniques to learn ordinary differential equations (ODEs). In\nthe context of machine learning, differentiable solvers are central for Neural\nODEs (NODEs), a class of deep learning models with continuous depth, rather\nthan discrete layers. However, these integrators can be unsatisfactorily slow\nand inaccurate when learning systems of ODEs from long sequences, or when\nsolutions of the system vary at widely different timescales in each dimension.\nIn this paper we propose an alternative approach to learning ODEs from data: we\nrepresent the underlying ODE as a vector field that is related to another base\nvector field by a differentiable bijection, modelled by an invertible neural\nnetwork. By restricting the base ODE to be amenable to integration, we can\ndrastically speed up and improve the robustness of integration. We demonstrate\nthe efficacy of our method in training and evaluating continuous neural\nnetworks models, as well as in learning benchmark ODE systems. We observe\nimprovements of up to two orders of magnitude when integrating learned ODEs\nwith GPUs computation.",
          "link": "http://arxiv.org/abs/2107.01650",
          "publishedOn": "2021-07-06T01:58:08.739Z",
          "wordCount": 612,
          "title": "Learning ODEs via Diffeomorphisms for Fast and Robust Integration. (arXiv:2107.01650v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1\">Lucas Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlor_T/0/1/0/all/0/1\">Tim Schl&#xf6;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1\">Jakob Sch&#xf6;ffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>",
          "description": "Deployed machine learning models are confronted with the problem of changing\ndata over time, a phenomenon also called concept drift. While existing\napproaches of concept drift detection already show convincing results, they\nrequire true labels as a prerequisite for successful drift detection.\nEspecially in many real-world application scenarios-like the ones covered in\nthis work-true labels are scarce, and their acquisition is expensive.\nTherefore, we introduce a new algorithm for drift detection, Uncertainty Drift\nDetection (UDD), which is able to detect drifts without access to true labels.\nOur approach is based on the uncertainty estimates provided by a deep neural\nnetwork in combination with Monte Carlo Dropout. Structural changes over time\nare detected by applying the ADWIN technique on the uncertainty estimates, and\ndetected drifts trigger a retraining of the prediction model. In contrast to\ninput data-based drift detection, our approach considers the effects of the\ncurrent input data on the properties of the prediction model rather than\ndetecting change on the input data only (which can lead to unnecessary\nretrainings). We show that UDD outperforms other state-of-the-art strategies on\ntwo synthetic as well as ten real-world data sets for both regression and\nclassification tasks.",
          "link": "http://arxiv.org/abs/2107.01873",
          "publishedOn": "2021-07-06T01:58:08.714Z",
          "wordCount": 627,
          "title": "Detecting Concept Drift With Neural Network Model Uncertainty. (arXiv:2107.01873v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "Sampling from a log-concave distribution function on $\\mathbb{R}^d$ (with\n$d\\gg 1$) is a popular problem that has wide applications. In this paper we\nstudy the application of random coordinate descent method (RCD) on the Langevin\nMonte Carlo (LMC) sampling method, and we find two sides of the theory:\n\n1. The direct application of RCD on LMC does reduce the number of finite\ndifferencing approximations per iteration, but it induces a large variance\nerror term. More iterations are then needed, and ultimately the method gains no\ncomputational advantage;\n\n2. When variance reduction techniques (such as SAGA and SVRG) are\nincorporated in RCD-LMC, the variance error term is reduced. The new methods,\ncompared to the vanilla LMC, reduce the total computational cost by $d$ folds,\nand achieve the optimal cost rate.\n\nWe perform our investigations in both overdamped and underdamped settings.",
          "link": "http://arxiv.org/abs/2007.14209",
          "publishedOn": "2021-07-06T01:58:08.699Z",
          "wordCount": 623,
          "title": "Langevin Monte Carlo: random coordinate descent and variance reduction. (arXiv:2007.14209v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zeyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siming Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>",
          "description": "WikiKG90M in KDD Cup 2021 is a large encyclopedic knowledge graph, which\ncould benefit various downstream applications such as question answering and\nrecommender systems. Participants are invited to complete the knowledge graph\nby predicting missing triplets. Recent representation learning methods have\nachieved great success on standard datasets like FB15k-237. Thus, we train the\nadvanced algorithms in different domains to learn the triplets, including OTE,\nQuatE, RotatE and TransE. Significantly, we modified OTE into NOTE (short for\nNorm-OTE) for better performance. Besides, we use both the DeepWalk and the\npost-smoothing technique to capture the graph structure for supplementation. In\naddition to the representations, we also use various statistical probabilities\namong the head entities, the relations and the tail entities for the final\nprediction. Experimental results show that the ensemble of state-of-the-art\nrepresentation learning methods could draw on each others strengths. And we\ndevelop feature engineering from validation candidates for further\nimprovements. Please note that we apply the same strategy on the test set for\nfinal inference. And these features may not be practical in the real world when\nconsidering ranking against all the entities.",
          "link": "http://arxiv.org/abs/2107.01892",
          "publishedOn": "2021-07-06T01:58:08.690Z",
          "wordCount": 637,
          "title": "NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC. (arXiv:2107.01892v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jegorova_M/0/1/0/all/0/1\">Marija Jegorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_C/0/1/0/all/0/1\">Chaitanya Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayor_C/0/1/0/all/0/1\">Charlie Mayor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_A/0/1/0/all/0/1\">Alexander Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Leakage of data from publicly available Machine Learning (ML) models is an\narea of growing significance as commercial and government applications of ML\ncan draw on multiple sources of data, potentially including users' and clients'\nsensitive data. We provide a comprehensive survey of contemporary advances on\nseveral fronts, covering involuntary data leakage which is natural to ML\nmodels, potential malevolent leakage which is caused by privacy attacks, and\ncurrently available defence mechanisms. We focus on inference-time leakage, as\nthe most likely scenario for publicly available models. We first discuss what\nleakage is in the context of different data, tasks, and model architectures. We\nthen propose a taxonomy across involuntary and malevolent leakage, available\ndefences, followed by the currently available assessment metrics and\napplications. We conclude with outstanding challenges and open questions,\noutlining some promising directions for future research.",
          "link": "http://arxiv.org/abs/2107.01614",
          "publishedOn": "2021-07-06T01:58:08.682Z",
          "wordCount": 574,
          "title": "Survey: Leakage and Privacy at Inference Time. (arXiv:2107.01614v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>",
          "description": "When a population exhibits heterogeneity, we often model it via a finite\nmixture: decompose it into several different but homogeneous subpopulations.\nContemporary practice favors learning the mixtures by maximizing the likelihood\nfor statistical efficiency and the convenient EM-algorithm for numerical\ncomputation. Yet the maximum likelihood estimate (MLE) is not well defined for\nthe most widely used finite normal mixture in particular and for finite\nlocation-scale mixture in general. We hence investigate feasible alternatives\nto MLE such as minimum distance estimators. Recently, the Wasserstein distance\nhas drawn increased attention in the machine learning community. It has\nintuitive geometric interpretation and is successfully employed in many new\napplications. Do we gain anything by learning finite location-scale mixtures\nvia a minimum Wasserstein distance estimator (MWDE)? This paper investigates\nthis possibility in several respects. We find that the MWDE is consistent and\nderive a numerical solution under finite location-scale mixtures. We study its\nrobustness against outliers and mild model mis-specifications. Our moderate\nscaled simulation study shows the MWDE suffers some efficiency loss against a\npenalized version of MLE in general without noticeable gain in robustness. We\nreaffirm the general superiority of the likelihood based learning strategies\neven for the non-regular finite location-scale mixtures.",
          "link": "http://arxiv.org/abs/2107.01323",
          "publishedOn": "2021-07-06T01:58:08.676Z",
          "wordCount": 628,
          "title": "Minimum Wasserstein Distance Estimator under Finite Location-scale Mixtures. (arXiv:2107.01323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01407",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "The performance of state-of-the-art baselines in the offline RL regime varies\nwidely over the spectrum of dataset qualities, ranging from \"far-from-optimal\"\nrandom data to \"close-to-optimal\" expert demonstrations. We re-implement these\nunder a fair, unified, and highly factorized framework, and show that when a\ngiven baseline outperforms its competing counterparts on one end of the\nspectrum, it never does on the other end. This consistent trend prevents us\nfrom naming a victor that outperforms the rest across the board. We attribute\nthe asymmetry in performance between the two ends of the quality spectrum to\nthe amount of inductive bias injected into the agent to entice it to posit that\nthe behavior underlying the offline dataset is optimal for the task. The more\nbias is injected, the higher the agent performs, provided the dataset is\nclose-to-optimal. Otherwise, its effect is brutally detrimental. Adopting an\nadvantage-weighted regression template as base, we conduct an investigation\nwhich corroborates that injections of such optimality inductive bias, when not\ndone parsimoniously, makes the agent subpar in the datasets it was dominant as\nsoon as the offline policy is sub-optimal. In an effort to design methods that\nperform well across the whole spectrum, we revisit the generalized policy\niteration scheme for the offline regime, and study the impact of nine distinct\nnewly-introduced proposal distributions over actions, involved in proposed\ngeneralization of the policy evaluation and policy improvement update rules. We\nshow that certain orchestrations strike the right balance and can improve the\nperformance on one end of the spectrum without harming it on the other end.",
          "link": "http://arxiv.org/abs/2107.01407",
          "publishedOn": "2021-07-06T01:58:08.658Z",
          "wordCount": 698,
          "title": "Where is the Grass Greener? Revisiting Generalized Policy Iteration for Offline Reinforcement Learning. (arXiv:2107.01407v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zixin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n\nIn this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.",
          "link": "http://arxiv.org/abs/2105.15134",
          "publishedOn": "2021-07-06T01:58:08.652Z",
          "wordCount": 725,
          "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01590",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ming_D/0/1/0/all/0/1\">Deyu Ming</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Williamson_D/0/1/0/all/0/1\">Daniel Williamson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guillas_S/0/1/0/all/0/1\">Serge Guillas</a>",
          "description": "We propose a novel deep Gaussian process (DGP) inference method for computer\nmodel emulation using stochastic imputation. By stochastically imputing the\nlatent layers, the approach transforms the DGP into the linked GP, a\nstate-of-the-art surrogate model formed by linking a system of feed-forward\ncoupled GPs. This transformation renders a simple while efficient DGP training\nprocedure that only involves optimizations of conventional stationary GPs. In\naddition, the analytically tractable mean and variance of the linked GP allows\none to implement predictions from DGP emulators in a fast and accurate manner.\nWe demonstrate the method in a series of synthetic examples and real-world\napplications, and show that it is a competitive candidate for efficient DGP\nsurrogate modeling in comparison to the variational inference and the\nfully-Bayesian approach. A $\\texttt{Python}$ package $\\texttt{dgpsi}$\nimplementing the method is also produced and available at\nhttps://github.com/mingdeyu/DGP.",
          "link": "http://arxiv.org/abs/2107.01590",
          "publishedOn": "2021-07-06T01:58:08.644Z",
          "wordCount": 577,
          "title": "Deep Gaussian Process Emulation using Stochastic Imputation. (arXiv:2107.01590v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1\">Sayyed Ahmad Naghavi Nozad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1\">Maryam Amir Haeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1\">Gianluigi Folino</a>",
          "description": "This paper presents a batch-wise density-based clustering approach for local\noutlier detection in massive-scale datasets. Unlike the well-known traditional\nalgorithms, which assume that all the data is memory-resident, our proposed\nmethod is scalable and processes the input data chunk-by-chunk within the\nconfines of a limited memory buffer. A temporary clustering model is built at\nthe first phase; then, it is gradually updated by analyzing consecutive memory\nloads of points. Subsequently, at the end of scalable clustering, the\napproximate structure of the original clusters is obtained. Finally, by another\nscan of the entire dataset and using a suitable criterion, an outlying score is\nassigned to each object called SDCOR (Scalable Density-based Clustering\nOutlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate\nthat the proposed method has a low linear time complexity and is more effective\nand efficient compared to best-known conventional density-based methods, which\nneed to load all data into the memory; and also, to some fast distance-based\nmethods, which can perform on data resident in the disk.",
          "link": "http://arxiv.org/abs/2006.07616",
          "publishedOn": "2021-07-06T01:58:08.636Z",
          "wordCount": 755,
          "title": "SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v11 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maulana_M/0/1/0/all/0/1\">Muhammad Rizki Maulana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Sun Lee</a>",
          "description": "Ensemble and auxiliary tasks are both well known to improve the performance\nof machine learning models when data is limited. However, the interaction\nbetween these two methods is not well studied, particularly in the context of\ndeep reinforcement learning. In this paper, we study the effects of ensemble\nand auxiliary tasks when combined with the deep Q-learning algorithm. We\nperform a case study on ATARI games under limited data constraint. Moreover, we\nderive a refined bias-variance-covariance decomposition to analyze the\ndifferent ways of learning ensembles and using auxiliary tasks, and use the\nanalysis to help provide some understanding of the case study. Our code is open\nsource and available at https://github.com/NUS-LID/RENAULT.",
          "link": "http://arxiv.org/abs/2107.01904",
          "publishedOn": "2021-07-06T01:58:08.629Z",
          "wordCount": 556,
          "title": "Ensemble and Auxiliary Tasks for Data-Efficient Deep Reinforcement Learning. (arXiv:2107.01904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Grzegorz Dudek</a>",
          "description": "This work contributes to the development of neural forecasting models with\nnovel randomization-based learning methods. These methods improve the fitting\nabilities of the neural model, in comparison to the standard method, by\ngenerating network parameters in accordance with the data and target function\nfeatures. A pattern-based representation of time series makes the proposed\napproach useful for forecasting time series with multiple seasonality. In the\nsimulation study, we evaluate the performance of the proposed models and find\nthat they can compete in terms of forecasting accuracy with fully-trained\nnetworks. Extremely fast and easy training, simple architecture, ease of\nimplementation, high accuracy as well as dealing with nonstationarity and\nmultiple seasonality in time series make the proposed model very attractive for\na wide range of complex time series forecasting problems.",
          "link": "http://arxiv.org/abs/2107.01705",
          "publishedOn": "2021-07-06T01:58:08.608Z",
          "wordCount": 572,
          "title": "Randomized Neural Networks for Forecasting Time Series with Multiple Seasonality. (arXiv:2107.01705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Temperature monitoring during the life time of heat source components in\nengineering systems becomes essential to guarantee the normal work and the\nworking life of these components. However, prior methods, which mainly use the\ninterpolate estimation to reconstruct the temperature field from limited\nmonitoring points, require large amounts of temperature tensors for an accurate\nestimation. This may decrease the availability and reliability of the system\nand sharply increase the monitoring cost. To solve this problem, this work\ndevelops a novel physics-informed deep reversible regression models for\ntemperature field reconstruction of heat-source systems (TFR-HSS), which can\nbetter reconstruct the temperature field with limited monitoring points\nunsupervisedly. First, we define the TFR-HSS task mathematically, and\nnumerically model the task, and hence transform the task as an image-to-image\nregression problem. Then this work develops the deep reversible regression\nmodel which can better learn the physical information, especially over the\nboundary. Finally, considering the physical characteristics of heat conduction\nas well as the boundary conditions, this work proposes the physics-informed\nreconstruction loss including four training losses and jointly learns the deep\nsurrogate model with these losses unsupervisedly. Experimental studies have\nconducted over typical two-dimensional heat-source systems to demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2106.11929",
          "publishedOn": "2021-07-06T01:58:08.602Z",
          "wordCount": 686,
          "title": "Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zijie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Graph neural networks (GNNs) have been widely used in various graph-related\nproblems such as node classification and graph classification, where the\nsuperior performance is mainly established when natural node features are\navailable. However, it is not well understood how GNNs work without natural\nnode features, especially regarding the various ways to construct artificial\nones. In this paper, we point out the two types of artificial node\nfeatures,i.e., positional and structural node features, and provide insights on\nwhy each of them is more appropriate for certain tasks,i.e., positional node\nclassification, structural node classification, and graph classification.\nExtensive experimental results on 10 benchmark datasets validate our insights,\nthus leading to a practical guideline on the choices between different\nartificial node features for GNNs on non-attributed graphs. The code is\navailable at https://github.com/zjzijielu/gnn-exp/.",
          "link": "http://arxiv.org/abs/2107.01495",
          "publishedOn": "2021-07-06T01:58:08.594Z",
          "wordCount": 611,
          "title": "On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs. (arXiv:2107.01495v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Ajay Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitlangia_S/0/1/0/all/0/1\">Sharad Chitlangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>",
          "description": "Reinforcement learning methods have recently been very successful at\nperforming complex sequential tasks like playing Atari games, Go and Poker.\nThese algorithms have outperformed humans in several tasks by learning from\nscratch, using only scalar rewards obtained through interaction with their\nenvironment. While there certainly has been considerable independent innovation\nto produce such results, many core ideas in reinforcement learning are inspired\nby phenomena in animal learning, psychology and neuroscience. In this paper, we\ncomprehensively review a large number of findings in both neuroscience and\npsychology that evidence reinforcement learning as a promising candidate for\nmodeling learning and decision making in the brain. In doing so, we construct a\nmapping between various classes of modern RL algorithms and specific findings\nin both neurophysiological and behavioral literature. We then discuss the\nimplications of this observed relationship between RL, neuroscience and\npsychology and its role in advancing research in both AI and brain science.",
          "link": "http://arxiv.org/abs/2007.01099",
          "publishedOn": "2021-07-06T01:58:08.581Z",
          "wordCount": 639,
          "title": "Reinforcement Learning and its Connections with Neuroscience and Psychology. (arXiv:2007.01099v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.",
          "link": "http://arxiv.org/abs/2107.01330",
          "publishedOn": "2021-07-06T01:58:08.574Z",
          "wordCount": 611,
          "title": "SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network. (arXiv:2107.01330v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01337",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Selim_M/0/1/0/all/0/1\">Md Selim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guo-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jin Chen</a>",
          "description": "While remarkable advances have been made in Computed Tomography (CT),\ncapturing CT images with non-standardized protocols causes low reproducibility\nregarding radiomic features, forming a barrier on CT image analysis in a large\nscale. RadiomicGAN is developed to effectively mitigate the discrepancy caused\nby using non-standard reconstruction kernels. RadiomicGAN consists of hybrid\nneural blocks including both pre-trained and trainable layers adopted to learn\nradiomic feature distributions efficiently. A novel training approach, called\nDynamic Window-based Training, has been developed to smoothly transform the\npre-trained model to the medical imaging domain. Model performance evaluated\nusing 1401 radiomic features show that RadiomicGAN clearly outperforms the\nstate-of-art image standardization models.",
          "link": "http://arxiv.org/abs/2107.01337",
          "publishedOn": "2021-07-06T01:58:08.555Z",
          "wordCount": 553,
          "title": "CT Image Harmonization for Enhancing Radiomics Studies. (arXiv:2107.01337v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xianjun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuanjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuzhong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose a novel neural model compression strategy combining data\naugmentation, knowledge transfer, pruning, and quantization for device-robust\nacoustic scene classification (ASC). Specifically, we tackle the ASC task in a\nlow-resource environment leveraging a recently proposed advanced neural network\npruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a\nsub-network neural model associated with a small amount non-zero model\nparameters. The effectiveness of LTH for low-complexity acoustic modeling is\nassessed by investigating various data augmentation and compression schemes,\nand we report an efficient joint framework for low-complexity multi-device ASC,\ncalled Acoustic Lottery. Acoustic Lottery could compress an ASC model over\n$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and\nLog loss of 0.76) compared to its not compressed seed model. All results\nreported in this work are based on a joint effort of four groups, namely\nGT-USTC-UKE-Tencent, aiming to address the \"Low-Complexity Acoustic Scene\nClassification (ASC) with Multiple Devices\" in the DCASE 2021 Challenge Task\n1a.",
          "link": "http://arxiv.org/abs/2107.01461",
          "publishedOn": "2021-07-06T01:58:08.548Z",
          "wordCount": 647,
          "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification. (arXiv:2107.01461v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hufei Zhu</a>",
          "description": "The original Broad Learning System (BLS) on new added nodes and its existing\nefficient implementation both assume the ridge parameter is near 0 in the ridge\ninverse to approximate the generalized inverse, and compute the generalized\ninverse solution for the output weights. In this paper, we propose two ridge\nsolutions for the output weights in the BLS on added nodes, where the ridge\nparameter can be any positive real number. One of the proposed ridge solutions\ncomputes the output weights from the inverse Cholesky factor, which is updated\nby extending the existing inverse Cholesky factorization. The other proposed\nridge solution computes the output weights from the ridge inverse, and updates\nthe ridge inverse by extending the Greville method that can only computes the\ngeneralized inverse of a partitioned matrix. The proposed BLS algorithm based\non the ridge inverse requires the same complexity as the original BLS\nalgorithm, while the proposed BLS algorithm based on the inverse Cholesky\nfactor requires less complexity and training time than the original BLS and the\nexisting efficient BLS. Both the proposed ridge solutions for BLS achieve the\nsame testing accuracy as the standard ridge solution in the numerical\nexperiments. The difference between the testing accuracy of the proposed ridge\nsolutions and that of the existing generalized inverse solutions is negligible\nwhen the ridge parameter is very small, and becomes too big to be ignored when\nthe ridge parameter is not very small. When the ridge parameter is not near 0,\nusually the proposed two ridge solutions for BLS achieve better testing\naccuracy than the existing generalized inverse solutions for BLS, and then the\nformer are more preferred than the latter.",
          "link": "http://arxiv.org/abs/1911.04872",
          "publishedOn": "2021-07-06T01:58:08.539Z",
          "wordCount": 737,
          "title": "Two Ridge Solutions for the Incremental Broad Learning System on Added Nodes. (arXiv:1911.04872v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01629",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cong_Z/0/1/0/all/0/1\">Ziwei Cong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Manchanda_P/0/1/0/all/0/1\">Puneet Manchanda</a>",
          "description": "The common belief about the growing medium of livestreaming is that its value\nlies in its \"live\" component. In this paper, we leverage data from a large\nlivestreaming platform to examine this belief. We are able to do this as this\nplatform also allows viewers to purchase the recorded version of the\nlivestream. We summarize the value of livestreaming content by estimating how\ndemand responds to price before, on the day of, and after the livestream. We do\nthis by proposing a generalized Orthogonal Random Forest framework. This\nframework allows us to estimate heterogeneous treatment effects in the presence\nof high-dimensional confounders whose relationships with the treatment policy\n(i.e., price) are complex but partially known. We find significant dynamics in\nthe price elasticity of demand over the temporal distance to the scheduled\nlivestreaming day and after. Specifically, demand gradually becomes less price\nsensitive over time to the livestreaming day and is inelastic on the\nlivestreaming day. Over the post-livestream period, demand is still sensitive\nto price, but much less than the pre-livestream period. This indicates that the\nvlaue of livestreaming persists beyond the live component. Finally, we provide\nsuggestive evidence for the likely mechanisms driving our results. These are\nquality uncertainty reduction for the patterns pre- and post-livestream and the\npotential of real-time interaction with the creator on the day of the\nlivestream.",
          "link": "http://arxiv.org/abs/2107.01629",
          "publishedOn": "2021-07-06T01:58:08.532Z",
          "wordCount": 675,
          "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal Random Forest. (arXiv:2107.01629v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kage_P/0/1/0/all/0/1\">Patrick Kage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreadis_P/0/1/0/all/0/1\">Pavlos Andreadis</a>",
          "description": "Detecting latent structure within a dataset is a crucial step in performing\nanalysis of a dataset. However, existing state-of-the-art techniques for\nsubclass discovery are limited: either they are limited to detecting very small\nnumbers of outliers or they lack the statistical power to deal with complex\ndata such as image or audio. This paper proposes a solution to this subclass\ndiscovery problem: by leveraging instance explanation methods, an existing\nclassifier can be extended to detect latent classes via differences in the\nclassifier's internal decisions about each instance. This works not only with\nsimple classification techniques but also with deep neural networks, allowing\nfor a powerful and flexible approach to detecting latent structure within\ndatasets. Effectively, this represents a projection of the dataset into the\nclassifier's \"explanation space,\" and preliminary results show that this\ntechnique outperforms the baseline for the detection of latent classes even\nwith limited processing. This paper also contains a pipeline for analyzing\nclassifiers automatically, and a web application for interactively exploring\nthe results from this technique.",
          "link": "http://arxiv.org/abs/2107.01657",
          "publishedOn": "2021-07-06T01:58:08.517Z",
          "wordCount": 606,
          "title": "Class Introspection: A Novel Technique for Detecting Unlabeled Subclasses by Leveraging Classifier Explainability Methods. (arXiv:2107.01657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>",
          "description": "Federated Learning (FL) enables multiple distributed clients (e.g., mobile\ndevices) to collaboratively train a centralized model while keeping the\ntraining data locally on the client. Compared to traditional centralized\nmachine learning, FL offers many favorable features such as offloading\noperations which would usually be performed by a central server and reducing\nrisks of serious privacy leakage. However, Byzantine clients that send\nincorrect or disruptive updates due to system failures or adversarial attacks\nmay disturb the joint learning process, consequently degrading the performance\nof the resulting model. In this paper, we propose to mitigate these failures\nand attacks from a spatial-temporal perspective. Specifically, we use a\nclustering-based method to detect and exclude incorrect updates by leveraging\ntheir geometric properties in the parameter space. Moreover, to further handle\nmalicious clients with time-varying behaviors, we propose to adaptively adjust\nthe learning rate according to momentum-based update speculation. Extensive\nexperiments on 4 public datasets demonstrate that our algorithm achieves\nenhanced robustness comparing to existing methods under both cross-silo and\ncross-device FL settings with faulty/malicious clients.",
          "link": "http://arxiv.org/abs/2107.01477",
          "publishedOn": "2021-07-06T01:58:08.500Z",
          "wordCount": 605,
          "title": "Byzantine-robust Federated Learning through Spatial-temporal Analysis of Local Model Updates. (arXiv:2107.01477v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazzini_P/0/1/0/all/0/1\">Paolo Fazzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_I/0/1/0/all/0/1\">Isaac Wheeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petracchini_F/0/1/0/all/0/1\">Francesco Petracchini</a>",
          "description": "In this work we theoretically and experimentally analyze Multi-Agent\nAdvantage Actor-Critic (MA2C) and Independent Advantage Actor-Critic (IA2C),\ntwo recently proposed multi-agent reinforcement learning methods that can be\napplied to control traffic signals in urban areas. The two methods differ in\ntheir use of a reward calculated locally or globally and in the management of\nagents' communication. We analyze the methods theoretically with the framework\nprovided by non-Markov decision processes, which provides useful insights in\nthe analysis of the algorithms. Moreover, we analyze the efficacy and the\nrobustness of the methods experimentally by testing them in two traffic areas\nin the Bologna (Italy) area, simulated by SUMO, a software tool. The\nexperimental results indicate that MA2C achieves the best performance in the\nmajority of cases, outperforms the alternative method considered, and displays\nsufficient stability during the learning process.",
          "link": "http://arxiv.org/abs/2107.01347",
          "publishedOn": "2021-07-06T01:58:08.493Z",
          "wordCount": 585,
          "title": "Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study. (arXiv:2107.01347v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuezheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shui Yu</a>",
          "description": "Federated learning (FL) empowers distributed clients to collaboratively train\na shared machine learning model through exchanging parameter information.\nDespite the fact that FL can protect clients' raw data, malicious users can\nstill crack original data with disclosed parameters. To amend this flaw,\ndifferential privacy (DP) is incorporated into FL clients to disturb original\nparameters, which however can significantly impair the accuracy of the trained\nmodel. In this work, we study a crucial question which has been vastly\noverlooked by existing works: what are the optimal numbers of queries and\nreplies in FL with DP so that the final model accuracy is maximized. In FL, the\nparameter server (PS) needs to query participating clients for multiple global\niterations to complete training. Each client responds a query from the PS by\nconducting a local iteration. Our work investigates how many times the PS\nshould query clients and how many times each client should reply the PS. We\ninvestigate two most extensively used DP mechanisms (i.e., the Laplace\nmechanism and Gaussian mechanisms). Through conducting convergence rate\nanalysis, we can determine the optimal numbers of queries and replies in FL\nwith DP so that the final model accuracy can be maximized. Finally, extensive\nexperiments are conducted with publicly available datasets: MNIST and FEMNIST,\nto verify our analysis and the results demonstrate that properly setting the\nnumbers of queries and replies can significantly improve the final model\naccuracy in FL with DP.",
          "link": "http://arxiv.org/abs/2107.01895",
          "publishedOn": "2021-07-06T01:58:08.420Z",
          "wordCount": 694,
          "title": "Optimizing the Numbers of Queries and Replies in Federated Learning with Differential Privacy. (arXiv:2107.01895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yifan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tianjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">David Wipf Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.",
          "link": "http://arxiv.org/abs/2107.01319",
          "publishedOn": "2021-07-06T01:58:08.413Z",
          "wordCount": 608,
          "title": "Learning Hierarchical Graph Neural Networks for Image Clustering. (arXiv:2107.01319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sandeep Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_O/0/1/0/all/0/1\">Or Zamir</a>",
          "description": "Random dimensionality reduction is a versatile tool for speeding up\nalgorithms for high-dimensional problems. We study its application to two\nclustering problems: the facility location problem, and the single-linkage\nhierarchical clustering problem, which is equivalent to computing the minimum\nspanning tree. We show that if we project the input pointset $X$ onto a random\n$d = O(d_X)$-dimensional subspace (where $d_X$ is the doubling dimension of\n$X$), then the optimum facility location cost in the projected space\napproximates the original cost up to a constant factor. We show an analogous\nstatement for minimum spanning tree, but with the dimension $d$ having an extra\n$\\log \\log n$ term and the approximation factor being arbitrarily close to $1$.\nFurthermore, we extend these results to approximating solutions instead of just\ntheir costs. Lastly, we provide experimental results to validate the quality of\nsolutions and the speedup due to the dimensionality reduction. Unlike several\nprevious papers studying this approach in the context of $k$-means and\n$k$-medians, our dimension bound does not depend on the number of clusters but\nonly on the intrinsic dimensionality of $X$.",
          "link": "http://arxiv.org/abs/2107.01804",
          "publishedOn": "2021-07-06T01:58:08.393Z",
          "wordCount": 635,
          "title": "Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering. (arXiv:2107.01804v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yakhchi_S/0/1/0/all/0/1\">Shahpar Yakhchi</a>",
          "description": "Recommender systems (RSs) have emerged as very useful tools to help customers\nwith their decision-making process, find items of their interest, and alleviate\nthe information overload problem. There are two different lines of approaches\nin RSs: (1) general recommenders with the main goal of discovering long-term\nusers' preferences, and (2) sequential recommenders with the main focus of\ncapturing short-term users' preferences in a session of user-item interaction\n(here, a session refers to a record of purchasing multiple items in one\nshopping event). While considering short-term users' preferences may satisfy\ntheir current needs and interests, long-term users' preferences provide users\nwith the items that they may interact with, eventually. In this thesis, we\nfirst focus on improving the performance of general RSs. Most of the existing\ngeneral RSs tend to exploit the users' rating patterns on common items to\ndetect similar users. The data sparsity problem (i.e. the lack of available\ninformation) is one of the major challenges for the current general RSs, and\nthey may fail to have any recommendations when there are no common items of\ninterest among users. We call this problem data sparsity with no feedback on\ncommon items (DSW-n-FCI). To overcome this problem, we propose a\npersonality-based RS in which similar users are identified based on the\nsimilarity of their personality traits.",
          "link": "http://arxiv.org/abs/2107.01529",
          "publishedOn": "2021-07-06T01:58:08.304Z",
          "wordCount": 655,
          "title": "Learning Complex Users' Preferences for Recommender Systems. (arXiv:2107.01529v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01303",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dadashkarimi_J/0/1/0/all/0/1\">Javid Dadashkarimi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Scheinost_D/0/1/0/all/0/1\">Dustin Scheinost</a>",
          "description": "Functional connectomes derived from functional magnetic resonance imaging\nhave long been used to understand the functional organization of the brain.\nNevertheless, a connectome is intrinsically linked to the atlas used to create\nit. In other words, a connectome generated from one atlas is different in scale\nand resolution compared to a connectome generated from another atlas. Being\nable to map connectomes and derived results between different atlases without\nadditional pre-processing is a crucial step in improving interpretation and\ngeneralization between studies that use different atlases. Here, we use optimal\ntransport, a powerful mathematical technique, to find an optimum mapping\nbetween two atlases. This mapping is then used to transform time series from\none atlas to another in order to reconstruct a connectome. We validate our\napproach by comparing transformed connectomes against their \"gold-standard\"\ncounterparts (i.e., connectomes generated directly from an atlas) and\ndemonstrate the utility of transformed connectomes by applying these\nconnectomes to predictive models based on a different atlas. We show that these\ntransformed connectomes are significantly similar to their \"gold-standard\"\ncounterparts and maintain individual differences in brain-behavior\nassociations, demonstrating both the validity of our approach and its utility\nin downstream analyses. Overall, our approach is a promising avenue to increase\nthe generalization of connectome-based results across different atlases.",
          "link": "http://arxiv.org/abs/2107.01303",
          "publishedOn": "2021-07-06T01:58:08.269Z",
          "wordCount": 645,
          "title": "Data-driven mapping between functional connectomes using optimal transport. (arXiv:2107.01303v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01338",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shankar_S/0/1/0/all/0/1\">Shiv Shankar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sheldon_D/0/1/0/all/0/1\">Daniel Sheldon</a>",
          "description": "Field observations form the basis of many scientific studies, especially in\necological and social sciences. Despite efforts to conduct such surveys in a\nstandardized way, observations can be prone to systematic measurement errors.\nThe removal of systematic variability introduced by the observation process, if\npossible, can greatly increase the value of this data. Existing non-parametric\ntechniques for correcting such errors assume linear additive noise models. This\nleads to biased estimates when applied to generalized linear models (GLM). We\npresent an approach based on residual functions to address this limitation. We\nthen demonstrate its effectiveness on synthetic data and show it reduces\nsystematic detection variability in moth surveys.",
          "link": "http://arxiv.org/abs/2107.01338",
          "publishedOn": "2021-07-06T01:58:08.180Z",
          "wordCount": 530,
          "title": "Sibling Regression for Generalized Linear Models. (arXiv:2107.01338v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sean Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Graph neural networks (GNNs) have been extensively studied for prediction\ntasks on graphs. As pointed out by recent studies, most GNNs assume local\nhomophily, i.e., strong similarities in local neighborhoods. This assumption\nhowever limits the generalizability power of GNNs. To address this limitation,\nwe propose a flexible GNN model, which is capable of handling any graphs\nwithout being restricted by their underlying homophily. At its core, this model\nadopts a node attention mechanism based on multiple learnable spectral filters;\ntherefore, the aggregation scheme is learned adaptively for each graph in the\nspectral domain. We evaluated the proposed model on node classification tasks\nover eight benchmark datasets. The proposed model is shown to generalize well\nto both homophilic and heterophilic graphs. Further, it outperforms all\nstate-of-the-art baselines on heterophilic graphs and performs comparably with\nthem on homophilic graphs.",
          "link": "http://arxiv.org/abs/2103.14187",
          "publishedOn": "2021-07-05T01:55:00.439Z",
          "wordCount": 614,
          "title": "Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. (arXiv:2103.14187v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:55:00.423Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1\">Thomas Lavastida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1\">Benjamin Moseley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_R/0/1/0/all/0/1\">R. Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenyang Xu</a>",
          "description": "We propose a new model for augmenting algorithms with predictions by\nrequiring that they are formally learnable and instance robust. Learnability\nensures that predictions can be efficiently constructed from a reasonable\namount of past data. Instance robustness ensures that the prediction is robust\nto modest changes in the problem input, where the measure of the change may be\nproblem specific. Instance robustness insists on a smooth degradation in\nperformance as a function of the change. Ideally, the performance is never\nworse than worst-case bounds. This also allows predictions to be objectively\ncompared.\n\nWe design online algorithms with predictions for a network flow allocation\nproblem and restricted assignment makespan minimization. For both problems, two\nkey properties are established: high quality predictions can be learned from a\nsmall sample of prior instances and these predictions are robust to errors that\nsmoothly degrade as the underlying problem instance changes.",
          "link": "http://arxiv.org/abs/2011.11743",
          "publishedOn": "2021-07-05T01:55:00.416Z",
          "wordCount": 622,
          "title": "Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing. (arXiv:2011.11743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04579",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Chabaud_U/0/1/0/all/0/1\">Ulysse Chabaud</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Markham_D/0/1/0/all/0/1\">Damian Markham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sohbi_A/0/1/0/all/0/1\">Adel Sohbi</a>",
          "description": "We study supervised learning algorithms in which a quantum device is used to\nperform a computational subroutine - either for prediction via probability\nestimation, or to compute a kernel via estimation of quantum states overlap. We\ndesign implementations of these quantum subroutines using Boson Sampling\narchitectures in linear optics, supplemented by adaptive measurements. We then\nchallenge these quantum algorithms by deriving classical simulation algorithms\nfor the tasks of output probability estimation and overlap estimation. We\nobtain different classical simulability regimes for these two computational\ntasks in terms of the number of adaptive measurements and input photons. In\nboth cases, our results set explicit limits to the range of parameters for\nwhich a quantum advantage can be envisaged with adaptive linear optics compared\nto classical machine learning algorithms: we show that the number of input\nphotons and the number of adaptive measurements cannot be simultaneously small\ncompared to the number of modes. Interestingly, our analysis leaves open the\npossibility of a near-term quantum advantage with a single adaptive\nmeasurement.",
          "link": "http://arxiv.org/abs/2102.04579",
          "publishedOn": "2021-07-05T01:55:00.393Z",
          "wordCount": 624,
          "title": "Quantum machine learning with adaptive linear optics. (arXiv:2102.04579v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisman_D/0/1/0/all/0/1\">Dana Fisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frenkel_H/0/1/0/all/0/1\">Hadar Frenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1\">Sandra Zilles</a>",
          "description": "We revisit the complexity of procedures on SFAs (such as intersection,\nemptiness, etc.) and analyze them according to the measures we find suitable\nfor symbolic automata: the number of states, the maximal number of transitions\nexiting a state, and the size of the most complex transition predicate. We pay\nattention to the special forms of SFAs: {normalized SFAs} and {neat SFAs}, as\nwell as to SFAs over a {monotonic} effective Boolean algebra.",
          "link": "http://arxiv.org/abs/2011.05389",
          "publishedOn": "2021-07-05T01:55:00.384Z",
          "wordCount": 544,
          "title": "On the Complexity of Symbolic Finite-State Automata. (arXiv:2011.05389v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:55:00.377Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood and we study its performance using simulated data. We find\nthat, when data comprise multiple related data sets, BHD outperforms the\nBayesian Dirichlet equivalent uniform (BDeu) score in terms of reconstruction\naccuracy as measured by the Structural Hamming distance, and that it is as\naccurate as BDeu when data are homogeneous. Moreover, the estimated networks\nare sparser and therefore more interpretable than those obtained with BDeu,\nthanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-05T01:55:00.359Z",
          "wordCount": 646,
          "title": "Structure Learning from Related Data Sets with a Hierarchical Bayesian Score. (arXiv:2008.01683v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05138",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Emmenegger_N/0/1/0/all/0/1\">Nicolas Emmenegger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kyng_R/0/1/0/all/0/1\">Rasmus Kyng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zehmakan_A/0/1/0/all/0/1\">Ahad N. Zehmakan</a>",
          "description": "We prove lower bounds for higher-order methods in smooth non-convex\nfinite-sum optimization. Our contribution is threefold: We first show that a\ndeterministic algorithm cannot profit from the finite-sum structure of the\nobjective, and that simulating a pth-order regularized method on the whole\nfunction by constructing exact gradient information is optimal up to constant\nfactors. We further show lower bounds for randomized algorithms and compare\nthem with the best known upper bounds. To address some gaps between the bounds,\nwe propose a new second-order smoothness assumption that can be seen as an\nanalogue of the first-order mean-squared smoothness assumption. We prove that\nit is sufficient to ensure state-of-the-art convergence guarantees, while\nallowing for a sharper lower bound.",
          "link": "http://arxiv.org/abs/2103.05138",
          "publishedOn": "2021-07-05T01:55:00.332Z",
          "wordCount": 596,
          "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization. (arXiv:2103.05138v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_F/0/1/0/all/0/1\">Faraz Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_A/0/1/0/all/0/1\">Ali Motie Nasrabadi</a>",
          "description": "A conventional subject-dependent (SD) brain-computer interface (BCI) requires\na complete data-gathering, training, and calibration phase for each user before\nit can be used. In recent years, a number of subject-independent (SI) BCIs have\nbeen developed. However, there are many problems preventing them from being\nused in real-world BCI applications. A weaker performance compared to the\nsubject-dependent (SD) approach, and a relatively large model requiring high\ncomputational power are the most important ones. Therefore, a potential\nreal-world BCI would greatly benefit from a compact low-power\nsubject-independent BCI framework, ready to be used immediately after the user\nputs it on. To move towards this goal, we propose a novel subject-independent\nBCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)\ntrained on the motor imagery (MI) paradigm of a large-scale\nelectroencephalography (EEG) signals database consisting of 21600 trials for 54\nsubjects performing two-class hand-movement MI tasks. The proposed framework\napplies a wavelet kernel convolutional neural network (WKCNN) and a temporal\nconvolutional neural network (TCNN) in order to represent and extract the\ndiverse spectral features of EEG signals. The outputs of the convolutional\nlayers go through a common spatial pattern (CSP) algorithm for spatial feature\nextraction. The number of CSP features is reduced by a dense neural network,\nand the final class label is determined by a linear discriminative analysis\n(LDA) classifier. The CCSPNet framework evaluation results show that it is\npossible to have a low-power compact BCI that achieves both SD and SI\nperformance comparable to complex and computationally expensive.",
          "link": "http://arxiv.org/abs/2012.13567",
          "publishedOn": "2021-07-05T01:55:00.326Z",
          "wordCount": 750,
          "title": "Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:55:00.319Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "A crucial ability of human intelligence is to build up models of individual\n3D objects from partial scene observations. Recent works achieve object-centric\ngeneration but without the ability to infer the representation, or achieve 3D\nscene representation learning but without object-centric compositionality.\nTherefore, learning to represent and render 3D scenes with object-centric\ncompositionality remains elusive. In this paper, we propose a probabilistic\ngenerative model for learning to build modular and compositional 3D object\nmodels from partial observations of a multi-object scene. The proposed model\ncan (i) infer the 3D object representations by learning to search and group\nobject areas and also (ii) render from an arbitrary viewpoint not only\nindividual objects but also the full scene by compositing the objects. The\nentire learning process is unsupervised and end-to-end. In experiments, in\naddition to generation quality, we also demonstrate that the learned\nrepresentation permits object-wise manipulation and novel scene generation, and\ngeneralizes to various settings. Results can be found on our project website:\nhttps://sites.google.com/view/roots3d",
          "link": "http://arxiv.org/abs/2006.06130",
          "publishedOn": "2021-07-05T01:55:00.310Z",
          "wordCount": 644,
          "title": "ROOTS: Object-Centric Representation and Rendering of 3D Scenes. (arXiv:2006.06130v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a> (ANITI), <a href=\"http://arxiv.org/find/cs/1/au:+Vigouroux_D/0/1/0/all/0/1\">David Vigouroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">R&#xe9;mi Cad&#xe8;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a> (ANITI)",
          "description": "A plethora of methods have been proposed to explain howdeep neural networks\nreach a decision but comparativelylittle effort has been made to ensure that\nthe explanationsproduced by these methods are objectively relevant.\nWhiledesirable properties for a good explanation are easy to come,objective\nmeasures have been harder to derive. Here, we pro-pose two new measures to\nevaluate explanations borrowedfrom the field of algorithmic stability: relative\nconsistencyReCo and mean generalizability MeGe. We conduct severalexperiments\non multiple image datasets and network archi-tectures to demonstrate the\nbenefits of the proposed measuresover representative methods. We show that\npopular fidelitymeasures are not sufficient to guarantee good\nexplanations.Finally, we show empirically that 1-Lipschitz networks pro-vide\ngeneral and consistent explanations, regardless of theexplanation method used,\nmaking them a relevant directionfor explainability.",
          "link": "http://arxiv.org/abs/2009.04521",
          "publishedOn": "2021-07-05T01:55:00.293Z",
          "wordCount": 606,
          "title": "How good is your explanation? Algorithmic stability measures to assess the qualityof explanations for deep neural networks. (arXiv:2009.04521v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:55:00.265Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We propose a taxonomy and outline the four\nfamilies in time series data augmentation, including transformation-based\nmethods, pattern mixing, generative models, and decomposition methods.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with six different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.",
          "link": "http://arxiv.org/abs/2007.15951",
          "publishedOn": "2021-07-05T01:55:00.255Z",
          "wordCount": 651,
          "title": "An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02898",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghalamkari_K/0/1/0/all/0/1\">Kazu Ghalamkari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "We present an efficient low-rank approximation algorithm for non-negative\ntensors. The algorithm is derived from our two findings: First, we show that\nrank-1 approximation for tensors can be viewed as a mean-field approximation by\ntreating each tensor as a probability distribution. Second, we theoretically\nprovide a sufficient condition for distribution parameters to reduce Tucker\nranks of tensors and, interestingly, this sufficient condition can be achieved\nby iterative application of the mean-field approximation. Since the mean-field\napproximation is always given as a closed formula, our findings lead to a fast\nlow-rank approximation algorithm without using a gradient method. We\nempirically demonstrate that our algorithm is faster than the existing\nnon-negative Tucker rank reduction methods with achieving competitive or better\napproximation of given tensors.",
          "link": "http://arxiv.org/abs/2103.02898",
          "publishedOn": "2021-07-05T01:55:00.248Z",
          "wordCount": 581,
          "title": "Fast Tucker Rank Reduction for Non-Negative Tensors Using Mean-Field Approximation. (arXiv:2103.02898v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.02725",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sebbouh_O/0/1/0/all/0/1\">Othmane Sebbouh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gazagnadou_N/0/1/0/all/0/1\">Nidham Gazagnadou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jelassi_S/0/1/0/all/0/1\">Samy Jelassi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>",
          "description": "Among the very first variance reduced stochastic methods for solving the\nempirical risk minimization problem was the SVRG method (Johnson & Zhang 2013).\nSVRG is an inner-outer loop based method, where in the outer loop a reference\nfull gradient is evaluated, after which $m \\in \\mathbb{N}$ steps of an inner\nloop are executed where the reference gradient is used to build a variance\nreduced estimate of the current gradient. The simplicity of the SVRG method and\nits analysis have led to multiple extensions and variants for even non-convex\noptimization. We provide a more general analysis of SVRG than had been\npreviously done by using arbitrary sampling, which allows us to analyse\nvirtually all forms of mini-batching through a single theorem. Furthermore, our\nanalysis is focused on more practical variants of SVRG including a new variant\nof the loopless SVRG (Hofman et al 2015, Kovalev et al 2019, Kulunchakov and\nMairal 2019) and a variant of k-SVRG (Raj and Stich 2018) where $m=n$ and where\n$n$ is the number of data points. Since our setup and analysis reflect what is\ndone in practice, we are able to set the parameters such as the mini-batch size\nand step size using our theory in such a way that produces a more efficient\nalgorithm in practice, as we show in extensive numerical experiments.",
          "link": "http://arxiv.org/abs/1908.02725",
          "publishedOn": "2021-07-05T01:55:00.242Z",
          "wordCount": 692,
          "title": "Towards closing the gap between the theory and practice of SVRG. (arXiv:1908.02725v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Randomized smoothing is a recent technique that achieves state-of-art\nperformance in training certifiably robust deep neural networks. While the\nsmoothing family of distributions is often connected to the choice of the norm\nused for certification, the parameters of these distributions are always set as\nglobal hyper parameters independent of the input data on which a network is\ncertified. In this work, we revisit Gaussian randomized smoothing and show that\nthe variance of the Gaussian distribution can be optimized at each input so as\nto maximize the certification radius for the construction of the smoothed\nclassifier. This new approach is generic, parameter-free, and easy to\nimplement. In fact, we show that our data dependent framework can be seamlessly\nincorporated into 3 randomized smoothing approaches, leading to consistent\nimproved certified accuracy. When this framework is used in the training\nroutine of these approaches followed by a data dependent certification, we\nachieve 9\\% and 6\\% improvement over the certified accuracy of the strongest\nbaseline for a radius of 0.5 on CIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.04351",
          "publishedOn": "2021-07-05T01:55:00.224Z",
          "wordCount": 633,
          "title": "Data Dependent Randomized Smoothing. (arXiv:2012.04351v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01105",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Patacchiola_M/0/1/0/all/0/1\">Massimiliano Patacchiola</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowozin_S/0/1/0/all/0/1\">Sebastian Nowozin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Meta learning approaches to few-shot classification are computationally\nefficient at test time requiring just a few optimization steps or single\nforward pass to learn a new task, but they remain highly memory-intensive to\ntrain. This limitation arises because a task's entire support set, which can\ncontain up to 1000 images, must be processed before an optimization step can be\ntaken. Harnessing the performance gains offered by large images thus requires\neither parallelizing the meta-learner across multiple GPUs, which may not be\navailable, or trade-offs between task and image size when memory constraints\napply. We improve on both options by proposing LITE, a general and memory\nefficient episodic training scheme that enables meta-training on large tasks\ncomposed of large images on a single GPU. We achieve this by observing that the\ngradients for a task can be decomposed into a sum of gradients over the task's\ntraining images. This enables us to perform a forward pass on a task's entire\ntraining set but realize significant memory savings by back-propagating only a\nrandom subset of these images which we show is an unbiased approximation of the\nfull gradient. We use LITE to train meta-learners and demonstrate new\nstate-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4\nparts of the challenging VTAB+MD benchmark relative to leading meta-learners.\nLITE also enables meta-learners to be competitive with transfer learning\napproaches but at a fraction of the test-time computational cost, thus serving\nas a counterpoint to the recent narrative that transfer learning is all you\nneed for few-shot classification.",
          "link": "http://arxiv.org/abs/2107.01105",
          "publishedOn": "2021-07-05T01:55:00.209Z",
          "wordCount": 691,
          "title": "Memory Efficient Meta-Learning with Large Images. (arXiv:2107.01105v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tann_W/0/1/0/all/0/1\">Wesley Joon-Wie Tann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ee-Chien Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>",
          "description": "We introduce the controllable graph generation problem, formulated as\ncontrolling graph attributes during the generative process to produce desired\ngraphs with understandable structures. Using a transparent and straightforward\nMarkov model to guide this generative process, practitioners can shape and\nunderstand the generated graphs. We propose ${\\rm S{\\small HADOW}C{\\small\nAST}}$, a generative model capable of controlling graph generation while\nretaining the original graph's intrinsic properties. The proposed model is\nbased on a conditional generative adversarial network. Given an observed graph\nand some user-specified Markov model parameters, ${\\rm S{\\small HADOW}C{\\small\nAST}}$ controls the conditions to generate desired graphs. Comprehensive\nexperiments on three real-world network datasets demonstrate our model's\ncompetitive performance in the graph generation task. Furthermore, we show its\neffective controllability by directing ${\\rm S{\\small HADOW}C{\\small AST}}$ to\ngenerate hypothetical scenarios with different graph structures.",
          "link": "http://arxiv.org/abs/2006.03774",
          "publishedOn": "2021-07-05T01:55:00.202Z",
          "wordCount": 610,
          "title": "SHADOWCAST: Controllable Graph Generation. (arXiv:2006.03774v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We present a primal only derivation of Mirror Descent as a \"partial\"\ndiscretization of gradient flow on a Riemannian manifold where the metric\ntensor is the Hessian of the Mirror Descent potential. We contrast this\ndiscretization to Natural Gradient Descent, which is obtained by a \"full\"\nforward Euler discretization. This view helps shed light on the relationship\nbetween the methods and allows generalizing Mirror Descent to general\nRiemannian geometries, even when the metric tensor is {\\em not} a Hessian, and\nthus there is no \"dual.\"",
          "link": "http://arxiv.org/abs/2004.01025",
          "publishedOn": "2021-07-05T01:55:00.195Z",
          "wordCount": 567,
          "title": "Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent. (arXiv:2004.01025v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zweig_A/0/1/0/all/0/1\">Aaron Zweig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Symmetric functions, which take as input an unordered, fixed-size set, are\nknown to be universally representable by neural networks that enforce\npermutation invariance. These architectures only give guarantees for fixed\ninput sizes, yet in many practical applications, including point clouds and\nparticle physics, a relevant notion of generalization should include varying\nthe input size. In this work we treat symmetric functions (of any size) as\nfunctions over probability measures, and study the learning and representation\nof neural networks defined on measures. By focusing on shallow architectures,\nwe establish approximation and generalization bounds under different choices of\nregularization (such as RKHS and variation norms), that capture a hierarchy of\nfunctional spaces with increasing degree of non-linear learning. The resulting\nmodels can be learned efficiently and enjoy generalization guarantees that\nextend across input sizes, as we verify empirically.",
          "link": "http://arxiv.org/abs/2008.06952",
          "publishedOn": "2021-07-05T01:55:00.161Z",
          "wordCount": 612,
          "title": "A Functional Perspective on Learning Symmetric Functions with Neural Networks. (arXiv:2008.06952v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peterson_J/0/1/0/all/0/1\">J. Luc Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_B/0/1/0/all/0/1\">Ben Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koning_J/0/1/0/all/0/1\">Joe Koning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1\">Peter Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semler_J/0/1/0/all/0/1\">Jessica Semler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jeremy White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_K/0/1/0/all/0/1\">Kevin Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremer_P/0/1/0/all/0/1\">Peer-Timo Bremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1\">Francesco Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">David Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_J/0/1/0/all/0/1\">Jim A. Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1\">Sam A. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kustowski_B/0/1/0/all/0/1\">Bogdan Kustowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Steven Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spears_B/0/1/0/all/0/1\">Brian Spears</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essen_B/0/1/0/all/0/1\">Brian Van Essen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1\">Jae-Seung Yeom</a>",
          "description": "With the growing complexity of computational and experimental facilities,\nmany scientific researchers are turning to machine learning (ML) techniques to\nanalyze large scale ensemble data. With complexities such as multi-component\nworkflows, heterogeneous machine architectures, parallel file systems, and\nbatch scheduling, care must be taken to facilitate this analysis in a high\nperformance computing (HPC) environment. In this paper, we present Merlin, a\nworkflow framework to enable large ML-friendly ensembles of scientific HPC\nsimulations. By augmenting traditional HPC with distributed compute\ntechnologies, Merlin aims to lower the barrier for scientific subject matter\nexperts to incorporate ML into their analysis. In addition to its design, we\ndescribe some example applications that Merlin has enabled on leadership-class\nHPC resources, such as the ML-augmented optimization of nuclear fusion\nexperiments and the calibration of infectious disease models to study the\nprogression of and possible mitigation strategies for COVID-19.",
          "link": "http://arxiv.org/abs/1912.02892",
          "publishedOn": "2021-07-05T01:55:00.147Z",
          "wordCount": 717,
          "title": "Enabling Machine Learning-Ready HPC Ensembles with Merlin. (arXiv:1912.02892v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajic_M/0/1/0/all/0/1\">Milena Bajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pour_S/0/1/0/all/0/1\">Shahrzad M. Pour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skar_A/0/1/0/all/0/1\">Asmus Skar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettinari_M/0/1/0/all/0/1\">Matteo Pettinari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levenberg_E/0/1/0/all/0/1\">Eyal Levenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alstrom_T/0/1/0/all/0/1\">Tommy Sonne Alstr&#xf8;m</a>",
          "description": "Road roughness is a very important road condition for the infrastructure, as\nthe roughness affects both the safety and ride comfort of passengers. The roads\ndeteriorate over time which means the road roughness must be continuously\nmonitored in order to have an accurate understand of the condition of the road\ninfrastructure. In this paper, we propose a machine learning pipeline for road\nroughness prediction using the vertical acceleration of the car and the car\nspeed. We compared well-known supervised machine learning models such as linear\nregression, naive Bayes, k-nearest neighbor, random forest, support vector\nmachine, and the multi-layer perceptron neural network. The models are trained\non an optimally selected set of features computed in the temporal and\nstatistical domain. The results demonstrate that machine learning methods can\naccurately predict road roughness, using the recordings of the cost\napproachable in-vehicle sensors installed in conventional passenger cars. Our\nfindings demonstrate that the technology is well suited to meet future pavement\ncondition monitoring, by enabling continuous monitoring of a wide road network.",
          "link": "http://arxiv.org/abs/2107.01199",
          "publishedOn": "2021-07-05T01:55:00.140Z",
          "wordCount": 606,
          "title": "Road Roughness Estimation Using Machine Learning. (arXiv:2107.01199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:55:00.126Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huixin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yongcan Cao</a>",
          "description": "Besides accuracy, the model size of convolutional neural networks (CNN)\nmodels is another important factor considering limited hardware resources in\npractical applications. For example, employing deep neural networks on mobile\nsystems requires the design of accurate yet fast CNN for low latency in\nclassification and object detection. To fulfill the need, we aim at obtaining\nCNN models with both high testing accuracy and small size to address resource\nconstraints in many embedded devices. In particular, this paper focuses on\nproposing a generic reinforcement learning-based model compression approach in\na two-stage compression pipeline: pruning and quantization. The first stage of\ncompression, i.e., pruning, is achieved via exploiting deep reinforcement\nlearning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise\nchannel pruning and element-wise variational pruning via information dropout.\nThe second stage, i.e., quantization, is achieved via a similar DRL approach\nbut focuses on obtaining the optimal bits representation for individual layers.\nWe further conduct experimental results on CIFAR-10 and ImageNet datasets. For\nthe CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x\nfrom 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet\ndataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to\n4.14MB with no accuracy loss.",
          "link": "http://arxiv.org/abs/1912.02254",
          "publishedOn": "2021-07-05T01:55:00.113Z",
          "wordCount": 674,
          "title": "Deep Model Compression Via Two-Stage Deep Reinforcement Learning. (arXiv:1912.02254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01126",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Melkas_L/0/1/0/all/0/1\">Laila Melkas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Savvides_R/0/1/0/all/0/1\">Rafael Savvides</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chandramouli_S/0/1/0/all/0/1\">Suyog Chandramouli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Makela_J/0/1/0/all/0/1\">Jarmo M&#xe4;kel&#xe4;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nieminen_T/0/1/0/all/0/1\">Tuomo Nieminen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mammarella_I/0/1/0/all/0/1\">Ivan Mammarella</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>",
          "description": "Causal structure discovery (CSD) models are making inroads into several\ndomains, including Earth system sciences. Their widespread adaptation is\nhowever hampered by the fact that the resulting models often do not take into\naccount the domain knowledge of the experts and that it is often necessary to\nmodify the resulting models iteratively. We present a workflow that is required\nto take this knowledge into account and to apply CSD algorithms in Earth system\nsciences. At the same time, we describe open research questions that still need\nto be addressed. We present a way to interactively modify the outputs of the\nCSD algorithms and argue that the user interaction can be modelled as a greedy\nfinding of the local maximum-a-posteriori solution of the likelihood function,\nwhich is composed of the likelihood of the causal model and the prior\ndistribution representing the knowledge of the expert user. We use a real-world\ndata set for examples constructed in collaboration with our co-authors, who are\nthe domain area experts. We show that finding maximally usable causal models in\nthe Earth system sciences or other similar domains is a difficult task which\ncontains many interesting open research questions. We argue that taking the\ndomain knowledge into account has a substantial effect on the final causal\nmodels discovered.",
          "link": "http://arxiv.org/abs/2107.01126",
          "publishedOn": "2021-07-05T01:55:00.095Z",
          "wordCount": 681,
          "title": "Interactive Causal Structure Discovery in Earth System Sciences. (arXiv:2107.01126v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yuewei Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deng_X/0/1/0/all/0/1\">Xinwei Deng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "Successful applications of InfoNCE and its variants have popularized the use\nof contrastive variational mutual information (MI) estimators in machine\nlearning. While featuring superior stability, these estimators crucially depend\non costly large-batch training, and they sacrifice bound tightness for variance\nreduction. To overcome these limitations, we revisit the mathematics of popular\nvariational MI bounds from the lens of unnormalized statistical modeling and\nconvex optimization. Our investigation not only yields a new unified\ntheoretical framework encompassing popular variational MI bounds but also leads\nto a novel, simple, and powerful contrastive MI estimator named as FLO.\nTheoretically, we show that the FLO estimator is tight, and it provably\nconverges under stochastic gradient descent. Empirically, our FLO estimator\novercomes the limitations of its predecessors and learns more efficiently. The\nutility of FLO is verified using an extensive set of benchmarks, which also\nreveals the trade-offs in practical MI estimation.",
          "link": "http://arxiv.org/abs/2107.01131",
          "publishedOn": "2021-07-05T01:55:00.088Z",
          "wordCount": 597,
          "title": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization. (arXiv:2107.01131v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_P/0/1/0/all/0/1\">Pinto Kumar Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_U/0/1/0/all/0/1\">Utku Kose</a>",
          "description": "Cybersecurity is a very emerging field that protects systems, networks, and\ndata from digital attacks. With the increase in the scale of the Internet and\nthe evolution of cyber attacks, developing novel cybersecurity tools has become\nimportant, particularly for Internet of things (IoT) networks. This paper\nprovides a systematic review of the application of deep learning (DL)\napproaches for cybersecurity. This paper provides a short description of DL\nmethods which is used in cybersecurity, including deep belief networks,\ngenerative adversarial networks, recurrent neural networks, and others. Next,\nwe illustrate the differences between shallow learning and DL. Moreover, a\ndiscussion is provided on the currently prevailing cyber-attacks in IoT and\nother networks, and the effectiveness of DL methods to manage these attacks.\nBesides, this paper describes studies that highlight the DL technique,\ncybersecurity applications, and the source of datasets. Next, a discussion is\nprovided on the feasibility of DL systems for malware detection and\nclassification, intrusion detection, and other frequent cyber-attacks,\nincluding identifying file type, spam, and network traffic. Our review\nindicates that high classification accuracy of 99.72% is obtained by restricted\nBoltzmann machine (RBM) when applied to a custom dataset, while long short-term\nmemory (LSTM) achieves an accuracy of 99.80% for KDD Cup 99 dataset. Finally,\nthis article discusses the importance of cybersecurity for reliable and\npracticable IoT-driven healthcare systems.",
          "link": "http://arxiv.org/abs/2107.01185",
          "publishedOn": "2021-07-05T01:55:00.082Z",
          "wordCount": 683,
          "title": "Artificial Neural Network for Cybersecurity: A Comprehensive Review. (arXiv:2107.01185v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiku_S/0/1/0/all/0/1\">Saideep Tiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">Sudeep Pasricha</a>",
          "description": "GPS technology has revolutionized the way we localize and navigate outdoors.\nHowever, the poor reception of GPS signals in buildings makes it unsuitable for\nindoor localization. WiFi fingerprinting-based indoor localization is one of\nthe most promising ways to meet this demand. Unfortunately, most work in the\ndomain fails to resolve challenges associated with deployability on\nresource-limited embedded devices. In this work, we propose a compression-aware\nand high-accuracy deep learning framework called CHISEL that outperforms the\nbest-known works in the area while maintaining localization robustness on\nembedded devices.",
          "link": "http://arxiv.org/abs/2107.01192",
          "publishedOn": "2021-07-05T01:55:00.074Z",
          "wordCount": 519,
          "title": "CHISEL: Compression-Aware High-Accuracy Embedded Indoor Localization with Deep Learning. (arXiv:2107.01192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01201",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1\">Rajeev Rikhye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1\">Ian McGraw</a>",
          "description": "In this paper, we propose a solution to allow speaker conditioned speech\nmodels, such as VoiceFilter-Lite, to support an arbitrary number of enrolled\nusers in a single pass. This is achieved by using an attention mechanism on\nmultiple speaker embeddings to compute a single attentive embedding, which is\nthen used as a side input to the model. We implemented multi-user\nVoiceFilter-Lite and evaluated it for three tasks: (1) a streaming automatic\nspeech recognition (ASR) task; (2) a text-independent speaker verification\ntask; and (3) a personalized keyphrase detection task, where ASR has to detect\nkeyphrases from multiple enrolled users in a noisy environment. Our experiments\nshow that, with up to four enrolled users, multi-user VoiceFilter-Lite is able\nto significantly reduce speech recognition and speaker verification errors when\nthere is overlapping speech, without affecting performance under other acoustic\nconditions. This attentive speaker embedding approach can also be easily\napplied to other speaker-conditioned models such as personal VAD and\npersonalized ASR.",
          "link": "http://arxiv.org/abs/2107.01201",
          "publishedOn": "2021-07-05T01:55:00.068Z",
          "wordCount": 601,
          "title": "Multi-user VoiceFilter-Lite via Attentive Speaker Embedding. (arXiv:2107.01201v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:59.465Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1809.04091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sepehry_B/0/1/0/all/0/1\">Behrooz Sepehry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranmanesh_E/0/1/0/all/0/1\">Ehsan Iranmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronagh_P/0/1/0/all/0/1\">Pooya Ronagh</a>",
          "description": "We introduce two quantum algorithms for solving structured prediction\nproblems. We first show that a stochastic gradient descent that uses the\nquantum minimum finding algorithm and takes its probabilistic failure into\naccount solves the structured prediction problem with a runtime that scales\nwith the square root of the size of the label space, and in $\\widetilde\nO\\left(1/\\epsilon\\right)$ with respect to the precision, $\\epsilon$, of the\nsolution. Motivated by robust inference techniques in machine learning, we then\nintroduce another quantum algorithm that solves a smooth approximation of the\nstructured prediction problem with a similar quantum speedup in the size of the\nlabel space and a similar scaling in the precision parameter. In doing so, we\nanalyze a variant of stochastic gradient descent for convex optimization in the\npresence of an additive error in the calculation of the gradients, and show\nthat its convergence rate does not deteriorate if the additive errors are of\nthe order $O(\\sqrt\\epsilon)$. This algorithm uses quantum Gibbs sampling at\ntemperature $\\Omega (\\epsilon)$ as a subroutine. Based on these theoretical\nobservations, we propose a method for using quantum Gibbs samplers to combine\nfeedforward neural networks with probabilistic graphical models for quantum\nmachine learning. Our numerical results using Monte Carlo simulations on an\nimage tagging task demonstrate the benefit of the approach.",
          "link": "http://arxiv.org/abs/1809.04091",
          "publishedOn": "2021-07-05T01:54:59.459Z",
          "wordCount": 716,
          "title": "Quantum Algorithms for Structured Prediction. (arXiv:1809.04091v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naeemullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Deep neural networks are vulnerable to input deformations in the form of\nvector fields of pixel displacements and to other parameterized geometric\ndeformations e.g. translations, rotations, etc. Current input deformation\ncertification methods either (i) do not scale to deep networks on large input\ndatasets, or (ii) can only certify a specific class of deformations, e.g. only\nrotations. We reformulate certification in randomized smoothing setting for\nboth general vector field and parameterized deformations and propose\nDeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large\nnetworks on large input datasets. For instance, DeformRS-Par certifies rich\ndeformations, covering translations, rotations, scaling, affine deformations,\nand other visually aligned deformations such as ones parameterized by\nDiscrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10 and\nImageNet show that DeformRS-Par outperforms existing state-of-the-art in\ncertified accuracy, e.g. improved certified accuracy of 6% against perturbed\nrotations in the set [-10,10] degrees on ImageNet.",
          "link": "http://arxiv.org/abs/2107.00996",
          "publishedOn": "2021-07-05T01:54:59.453Z",
          "wordCount": 591,
          "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing. (arXiv:2107.00996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:59.446Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Metro origin-destination prediction is a crucial yet challenging task for\nintelligent transportation management, which aims to accurately forecast two\nspecific types of cross-station ridership, i.e., Origin-Destination (OD) one\nand Destination-Origin (DO) one. However, complete OD matrices of previous time\nintervals can not be obtained immediately in online metro systems, and\nconventional methods only used limited information to forecast the future OD\nand DO ridership separately.In this work, we proposed a novel neural network\nmodule termed Heterogeneous Information Aggregation Machine (HIAM), which fully\nexploits heterogeneous information of historical data (e.g., incomplete OD\nmatrices, unfinished order vectors, and DO matrices) to jointly learn the\nevolutionary patterns of OD and DO ridership. Specifically, an OD modeling\nbranch estimates the potential destinations of unfinished orders explicitly to\ncomplement the information of incomplete OD matrices, while a DO modeling\nbranch takes DO matrices as input to capture the spatial-temporal distribution\nof DO ridership. Moreover, a Dual Information Transformer is introduced to\npropagate the mutual information among OD features and DO features for modeling\nthe OD-DO causality and correlation. Based on the proposed HIAM, we develop a\nunified Seq2Seq network to forecast the future OD and DO ridership\nsimultaneously. Extensive experiments conducted on two large-scale benchmarks\ndemonstrate the effectiveness of our method for online metro origin-destination\nprediction.",
          "link": "http://arxiv.org/abs/2107.00946",
          "publishedOn": "2021-07-05T01:54:59.440Z",
          "wordCount": 651,
          "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:59.424Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleindessner_M/0/1/0/all/0/1\">Matth&#xe4;us Kleindessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>",
          "description": "When machine learning systems meet real world applications, accuracy is only\none of several requirements. In this paper, we assay a complementary\nperspective originating from the increasing availability of pre-trained and\nregularly improving state-of-the-art models. While new improved models develop\nat a fast pace, downstream tasks vary more slowly or stay constant. Assume that\nwe have a large unlabelled data set for which we want to maintain accurate\npredictions. Whenever a new and presumably better ML models becomes available,\nwe encounter two problems: (i) given a limited budget, which data points should\nbe re-evaluated using the new model?; and (ii) if the new predictions differ\nfrom the current ones, should we update? Problem (i) is about compute cost,\nwhich matters for very large data sets and models. Problem (ii) is about\nmaintaining consistency of the predictions, which can be highly relevant for\ndownstream applications; our demand is to avoid negative flips, i.e., changing\ncorrect to incorrect predictions. In this paper, we formalize the Prediction\nUpdate Problem and present an efficient probabilistic approach as answer to the\nabove questions. In extensive experiments on standard classification benchmark\ndata sets, we show that our method outperforms alternative strategies along key\nmetrics for backward-compatible prediction updates.",
          "link": "http://arxiv.org/abs/2107.01057",
          "publishedOn": "2021-07-05T01:54:59.417Z",
          "wordCount": 637,
          "title": "Backward-Compatible Prediction Updates: A Probabilistic Approach. (arXiv:2107.01057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "In this paper, we study stochastic optimization of areas under\nprecision-recall curves (AUPRC), which is widely used for combating imbalanced\nclassification tasks. Although a few methods have been proposed for maximizing\nAUPRC, stochastic optimization of AUPRC with convergence guarantee remains an\nundeveloped territory. A recent work [42] has proposed a promising approach\ntowards AUPRC based on maximizing a surrogate loss for the average precision,\nand proved an $O(1/\\epsilon^5)$ complexity for finding an $\\epsilon$-stationary\nsolution of the non-convex objective. In this paper, we further improve the\nstochastic optimization of AURPC by (i) developing novel stochastic momentum\nmethods with a better iteration complexity of $O(1/\\epsilon^4)$ for finding an\n$\\epsilon$-stationary solution; and (ii) designing a novel family of stochastic\nadaptive methods with the same iteration complexity of $O(1/\\epsilon^4)$, which\nenjoy faster convergence in practice. To this end, we propose two innovative\ntechniques that are critical for improving the convergence: (i) the biased\nestimators for tracking individual ranking scores are updated in a randomized\ncoordinate-wise manner; and (ii) a momentum update is used on top of the\nstochastic gradient estimator for tracking the gradient of the objective.\nExtensive experiments on various data sets demonstrate the effectiveness of the\nproposed algorithms. Of independent interest, the proposed stochastic momentum\nand adaptive algorithms are also applicable to a class of two-level stochastic\ndependent compositional optimization problems.",
          "link": "http://arxiv.org/abs/2107.01173",
          "publishedOn": "2021-07-05T01:54:59.410Z",
          "wordCount": 655,
          "title": "Momentum Accelerates the Convergence of Stochastic AUPRC Maximization. (arXiv:2107.01173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1\">Nikita Pavlichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stelmakh_I/0/1/0/all/0/1\">Ivan Stelmakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1\">Dmitry Ustalov</a>",
          "description": "Domain-specific data is the crux of the successful transfer of machine\nlearning systems from benchmarks to real life. Crowdsourcing has become one of\nthe standard tools for cheap and time-efficient data collection for simple\nproblems such as image classification: thanks in large part to advances in\nresearch on aggregation methods. However, the applicability of crowdsourcing to\nmore complex tasks (e.g., speech recognition) remains limited due to the lack\nof principled aggregation methods for these modalities. The main obstacle\ntowards designing advanced aggregation methods is the absence of training data,\nand in this work, we focus on bridging this gap in speech recognition. For\nthis, we collect and release CrowdSpeech -- the first publicly available\nlarge-scale dataset of crowdsourced audio transcriptions. Evaluation of\nexisting aggregation methods on our data shows room for improvement, suggesting\nthat our work may entail the design of better algorithms. At a higher level, we\nalso contribute to the more general challenge of collecting high-quality\ndatasets using crowdsourcing: we develop a principled pipeline for constructing\ndatasets of crowdsourced audio transcriptions in any novel domain. We show its\napplicability on an under-resourced language by constructing VoxDIY -- a\ncounterpart of CrowdSpeech for the Russian language. We also release the code\nthat allows a full replication of our data collection pipeline and share\nvarious insights on best practices of data collection via crowdsourcing.",
          "link": "http://arxiv.org/abs/2107.01091",
          "publishedOn": "2021-07-05T01:54:59.403Z",
          "wordCount": 668,
          "title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription. (arXiv:2107.01091v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis Banerjee</a>",
          "description": "We present a new framework to measure the intrinsic properties of (deep)\nneural networks. While we focus on convolutional networks, our framework can be\nextrapolated to any network architecture. In particular, we evaluate two\nnetwork properties, namely, capacity (related to expressivity) and compression,\nboth of which depend only on the network structure and are independent of the\ntraining and test data. To this end, we propose two metrics: the first one,\ncalled layer complexity, captures the architectural complexity of any network\nlayer; and, the second one, called layer intrinsic power, encodes how data is\ncompressed along the network. The metrics are based on the concept of layer\nalgebra, which is also introduced in this paper. This concept is based on the\nidea that the global properties depend on the network topology, and the leaf\nnodes of any neural network can be approximated using local transfer functions,\nthereby, allowing a simple computation of the global metrics. We also compare\nthe properties of the state-of-the art architectures using our metrics and use\nthe properties to analyze the classification accuracy on benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.01081",
          "publishedOn": "2021-07-05T01:54:59.372Z",
          "wordCount": 617,
          "title": "Neural Network Layer Algebra: A Framework to Measure Capacity and Compression in Deep Learning. (arXiv:2107.01081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:59.347Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1\">Jose M. Pe&#xf1;a</a>",
          "description": "We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to specify two\nintuitive parameters. Otherwise, the method is assumption-free. The method\nreturns an interval that contains the true causal effect. Moreover, the bounds\nof the interval are sharp, i.e. attainable. We show experimentally that our\nbounds can be sharper than those obtained by the method of Ding and VanderWeele\n(2016). Finally, we extend our method to bound the natural direct and indirect\neffects when there are measured mediators and unmeasured exposure-outcome\nconfounding.",
          "link": "http://arxiv.org/abs/2104.13020",
          "publishedOn": "2021-07-05T01:54:59.325Z",
          "wordCount": 543,
          "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding. (arXiv:2104.13020v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:59.318Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:59.311Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuetz_M/0/1/0/all/0/1\">Martin J. A. Schuetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_J/0/1/0/all/0/1\">J. Kyle Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzgraber_H/0/1/0/all/0/1\">Helmut G. Katzgraber</a>",
          "description": "We demonstrate how graph neural networks can be used to solve combinatorial\noptimization problems. Our approach is broadly applicable to canonical NP-hard\nproblems in the form of quadratic unconstrained binary optimization problems,\nsuch as maximum cut, minimum vertex cover, maximum independent set, as well as\nIsing spin glasses and higher-order generalizations thereof in the form of\npolynomial unconstrained binary optimization problems. We apply a relaxation\nstrategy to the problem Hamiltonian to generate a differentiable loss function\nwith which we train the graph neural network and apply a simple projection to\ninteger variables once the unsupervised training process has completed. We\nshowcase our approach with numerical results for the canonical maximum cut and\nmaximum independent set problems. We find that the graph neural network\noptimizer performs on par or outperforms existing solvers, with the ability to\nscale beyond the state of the art to problems with millions of variables.",
          "link": "http://arxiv.org/abs/2107.01188",
          "publishedOn": "2021-07-05T01:54:59.305Z",
          "wordCount": 616,
          "title": "Combinatorial Optimization with Physics-Inspired Graph Neural Networks. (arXiv:2107.01188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Stephen Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Classical machine learning approaches are sensitive to non-stationarity.\nTransfer learning can address non-stationarity by sharing knowledge from one\nsystem to another, however, in areas like machine prognostics and defense, data\nis fundamentally limited. Therefore, transfer learning algorithms have little,\nif any, examples from which to learn. Herein, we suggest that these constraints\non algorithmic learning can be addressed by systems engineering. We formally\ndefine transfer distance in general terms and demonstrate its use in\nempirically quantifying the transferability of models. We consider the use of\ntransfer distance in the design of machine rebuild procedures to allow for\ntransferable prognostic models. We also consider the use of transfer distance\nin predicting operational performance in computer vision. Practitioners can use\nthe presented methodology to design and operate systems with consideration for\nthe learning theoretic challenges faced by component learning systems.",
          "link": "http://arxiv.org/abs/2107.01184",
          "publishedOn": "2021-07-05T01:54:59.298Z",
          "wordCount": 576,
          "title": "Empirically Measuring Transfer Distance for System Design and Operation. (arXiv:2107.01184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1\">Paul Mineiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "The use of pessimism, when reasoning about datasets lacking exhaustive\nexploration has recently gained prominence in offline reinforcement learning.\nDespite the robustness it adds to the algorithm, overly pessimistic reasoning\ncan be equally damaging in precluding the discovery of good policies, which is\nan issue for the popular bonus-based pessimism. In this paper, we introduce the\nnotion of Bellman-consistent pessimism for general function approximation:\ninstead of calculating a point-wise lower bound for the value function, we\nimplement pessimism at the initial state over the set of functions consistent\nwith the Bellman equations. Our theoretical guarantees only require Bellman\nclosedness as standard in the exploratory setting, in which case bonus-based\npessimism fails to provide guarantees. Even in the special case of linear MDPs\nwhere stronger function-approximation assumptions hold, our result improves\nupon a recent bonus-based approach by $\\mathcal{O}(d)$ in its sample complexity\nwhen the action space is finite. Remarkably, our algorithms automatically adapt\nto the best bias-variance tradeoff in the hindsight, whereas most prior\napproaches require tuning extra hyperparameters a priori.",
          "link": "http://arxiv.org/abs/2106.06926",
          "publishedOn": "2021-07-05T01:54:59.261Z",
          "wordCount": 637,
          "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haike Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "This paper presents a new model-free algorithm for episodic finite-horizon\nMarkov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which\nenjoys a stronger gap-dependent regret bound. The first innovation is to\nestimate the optimal $Q$-function by combining an optimistic bootstrap with an\nadaptive multi-step Monte Carlo rollout. The second innovation is to select the\naction with the largest confidence interval length among admissible actions\nthat are not dominated by any other actions. We show when each state has a\nunique optimal action, AMB achieves a gap-dependent regret bound that only\nscales with the sum of the inverse of the sub-optimality gaps. In contrast,\nSimchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB)\nalgorithms suffer an additional $\\Omega\\left(\\frac{S}{\\Delta_{min}}\\right)$\nregret due to over-exploration where $\\Delta_{min}$ is the minimum\nsub-optimality gap and $S$ is the number of states. We further show that for\ngeneral MDPs, AMB suffers an additional $\\frac{|Z_{mul}|}{\\Delta_{min}}$\nregret, where $Z_{mul}$ is the set of state-action pairs $(s,a)$'s satisfying\n$a$ is a non-unique optimal action for $s$. We complement our upper bound with\na lower bound showing the dependency on $\\frac{|Z_{mul}|}{\\Delta_{min}}$ is\nunavoidable for any consistent algorithm. This lower bound also implies a\nseparation between reinforcement learning and contextual bandits.",
          "link": "http://arxiv.org/abs/2102.04692",
          "publishedOn": "2021-07-05T01:54:59.250Z",
          "wordCount": 670,
          "title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap. (arXiv:2102.04692v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanoh_R/0/1/0/all/0/1\">Ryuichi Kanoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "A multiplicative constant scaling factor is often applied to the model output\nto adjust the dynamics of neural network parameters. This has been used as one\nof the key interventions in an empirical study of lazy and active behavior.\nHowever, we show that the combination of such scaling and a commonly used\nadaptive learning rate optimizer strongly affects the training behavior of the\nneural network. This is problematic as it can cause \\emph{unintended behavior}\nof neural networks, resulting in the misinterpretation of experimental results.\nSpecifically, for some scaling settings, the effect of the adaptive learning\nrate disappears or is strongly influenced by the scaling factor. To avoid the\nunintended effect, we present a modification of an optimization algorithm and\ndemonstrate remarkable differences between adaptive learning rate optimization\nand simple gradient descent, especially with a small ($<1.0$) scaling factor.",
          "link": "http://arxiv.org/abs/2103.03466",
          "publishedOn": "2021-07-05T01:54:59.244Z",
          "wordCount": 607,
          "title": "Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change. (arXiv:2103.03466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>",
          "description": "Inductive Logic Programming (ILP) aims to learn generalised, interpretable\nhypotheses in a data-efficient manner. However, current ILP systems require\ntraining examples to be specified in a structured logical form. To address this\nproblem, this paper proposes a neural-symbolic learning framework, called\nFeed-Forward Neural-Symbolic Learner (FF-NSL), that integrates state-of-the-art\nILP systems, based on the Answer Set semantics, with Neural Networks (NNs), in\norder to learn interpretable hypotheses from labelled unstructured data. To\ndemonstrate the generality and robustness of FF-NSL, we use two datasets\nsubject to distributional shifts, for which pre-trained NNs may give incorrect\npredictions with high confidence. Experimental results show that FF-NSL\noutperforms tree-based and neural-based approaches by learning more accurate\nand interpretable hypotheses with fewer examples.",
          "link": "http://arxiv.org/abs/2106.13103",
          "publishedOn": "2021-07-05T01:54:59.237Z",
          "wordCount": 572,
          "title": "FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-07-05T01:54:59.220Z",
          "wordCount": 582,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03361",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Zotov_M/0/1/0/all/0/1\">Mikhail Zotov</a>",
          "description": "We employ neural networks for classification of data of the TUS fluorescence\ntelescope, the world's first orbital detector of ultra-high energy cosmic rays.\nWe focus on two particular types of signals in the TUS data: track-like flashes\nproduced by cosmic ray hits of the photodetector and flashes that originated\nfrom distant lightnings. We demonstrate that even simple neural networks\ncombined with certain conventional methods of data analysis can be highly\neffective in tasks of classification of data of fluorescence telescopes.",
          "link": "http://arxiv.org/abs/2106.03361",
          "publishedOn": "2021-07-05T01:54:59.214Z",
          "wordCount": 561,
          "title": "Application of neural networks to classification of data of the TUS orbital telescope. (arXiv:2106.03361v2 [astro-ph.IM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01034",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>",
          "description": "The Intergovernmental Panel on Climate Change proposes different mitigation\nstrategies to achieve the net emissions reductions that would be required to\nfollow a pathway that limits global warming to 1.5{\\deg}C with no or limited\novershoot. The transition towards a carbon-free society goes through an\ninevitable increase of the share of renewable generation in the energy mix and\na drastic decrease in terms of the total consumption of fossil fuels.\nTherefore, this thesis studies the integration of renewables in power systems\nby investigating forecasting and decision-making tools. Indeed, in contrast to\nconventional power plants, renewable energy is subject to uncertainty. Most of\nthe generation technologies based on renewable sources are non-dispatchable,\nand their production is stochastic and hard to predict in advance. A high share\nof renewables is a great challenge for power systems that have been designed\nand sized for dispatchable units. In this context, probabilistic forecasts,\nwhich aim at modeling the distribution of all possible future realizations,\nhave become an important tool to equip decision-makers, hopefully leading to\nbetter decisions in energy applications. This thesis focus on two main research\nquestions: (1) How to produce reliable probabilistic forecasts of renewable\ngeneration, consumption, and electricity prices? (2) How to make decisions with\nuncertainty using probabilistic forecasts? The thesis perimeter is the energy\nmanagement of \"small\" systems such as microgrids at a residential scale on a\nday-ahead basis. It is divided into two main parts to propose directions to\naddress both research questions (1) a forecasting part; (2) a planning and\ncontrol part.",
          "link": "http://arxiv.org/abs/2107.01034",
          "publishedOn": "2021-07-05T01:54:59.206Z",
          "wordCount": 724,
          "title": "Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12828",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Heaton_H/0/1/0/all/0/1\">Howard Heaton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1\">Jialin Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Learning to optimize (L2O) is an emerging approach that leverages machine\nlearning to develop optimization methods, aiming at reducing the laborious\niterations of hand engineering. It automates the design of an optimization\nmethod based on its performance on a set of training problems. This data-driven\nprocedure generates methods that can efficiently solve problems similar to\nthose in the training. In sharp contrast, the typical and traditional designs\nof optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems specified by the theory. The difference\nmakes L2O suitable for repeatedly solving a certain type of optimization\nproblems over a specific distribution of data, while it typically fails on\nout-of-distribution problems. The practicality of L2O depends on the type of\ntarget optimization, the chosen architecture of the method to learn, and the\ntraining procedure. This new paradigm has motivated a community of researchers\nto explore L2O and report their findings.\n\nThis article is poised to be the first comprehensive survey and benchmark of\nL2O for continuous optimization. We set up taxonomies, categorize existing\nworks and research directions, present insights, and identify open challenges.\nWe also benchmarked many existing L2O approaches on a few but representative\noptimization problems. For reproducible research and fair benchmarking\npurposes, we released our software implementation and data in the package\nOpen-L2O at https://github.com/VITA-Group/Open-L2O.",
          "link": "http://arxiv.org/abs/2103.12828",
          "publishedOn": "2021-07-05T01:54:59.199Z",
          "wordCount": 685,
          "title": "Learning to Optimize: A Primer and A Benchmark. (arXiv:2103.12828v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05842",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1\">Raaz Dwivedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.",
          "link": "http://arxiv.org/abs/2105.05842",
          "publishedOn": "2021-07-05T01:54:59.191Z",
          "wordCount": 630,
          "title": "Kernel Thinning. (arXiv:2105.05842v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shingi_G/0/1/0/all/0/1\">Geet Shingi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saglani_H/0/1/0/all/0/1\">Harsh Saglani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Preeti Jain</a>",
          "description": "Cyberattacks are a major issues and it causes organizations great financial,\nand reputation harm. However, due to various factors, the current network\nintrusion detection systems (NIDS) seem to be insufficent. Predominant NIDS\nidentifies Cyberattacks through a handcrafted dataset of rules. Although the\nrecent applications of machine learning and deep learning have alleviated the\nenormous effort in NIDS, the security of network data has always been a prime\nconcern. However, to encounter the security problem and enable sharing among\norganizations, Federated Learning (FL) scheme is employed. Although the current\nFL systems have been successful, a network's data distribution does not always\nfit into a single global model as in FL. Thus, in such cases, having a single\nglobal model in FL is no feasible. In this paper, we propose a\nSegmented-Federated Learning (Segmented-FL) learning scheme for a more\nefficient NIDS. The Segmented-FL approach employs periodic local model\nevaluation based on which the segmentation occurs. We aim to bring similar\nnetwork environments to the same group. Further, the Segmented-FL system is\ncoupled with a weighted aggregation of local model parameters based on the\nnumber of data samples a worker possesses to further augment the performance.\nThe improved performance by our system as compared to the FL and centralized\nsystems on standard dataset further validates our system and makes a strong\ncase for extending our technique across various tasks. The solution finds its\napplication in organizations that want to collaboratively learn on diverse\nnetwork environments and protect the privacy of individual datasets.",
          "link": "http://arxiv.org/abs/2107.00881",
          "publishedOn": "2021-07-05T01:54:59.184Z",
          "wordCount": 712,
          "title": "Segmented Federated Learning for Adaptive Intrusion Detection System. (arXiv:2107.00881v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:59.164Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1\">Shaked Dovrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Single channel speech separation has experienced great progress in the last\nfew years. However, training neural speech separation for a large number of\nspeakers (e.g., more than 10 speakers) is out of reach for the current methods,\nwhich rely on the Permutation Invariant Loss (PIT). In this work, we present a\npermutation invariant training that employs the Hungarian algorithm in order to\ntrain with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in\ncomparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified\narchitecture that can handle the increased number of speakers. Our approach\nseparates up to $20$ speakers and improves the previous results for large $C$\nby a wide margin.",
          "link": "http://arxiv.org/abs/2104.08955",
          "publishedOn": "2021-07-05T01:54:59.156Z",
          "wordCount": 599,
          "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training. (arXiv:2104.08955v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xucheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Ce Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>",
          "description": "Weakly-supervised anomaly detection aims at learning an anomaly detector from\na limited amount of labeled data and abundant unlabeled data. Recent works\nbuild deep neural networks for anomaly detection by discriminatively mapping\nthe normal samples and abnormal samples to different regions in the feature\nspace or fitting different distributions. However, due to the limited number of\nannotated anomaly samples, directly training networks with the discriminative\nloss may not be sufficient. To overcome this issue, this paper proposes a novel\nstrategy to transform the input data into a more meaningful representation that\ncould be used for anomaly detection. Specifically, we leverage an autoencoder\nto encode the input data and utilize three factors, hidden representation,\nreconstruction residual vector, and reconstruction error, as the new\nrepresentation for the input data. This representation amounts to encode a test\nsample with its projection on the training data manifold, its direction to its\nprojection and its distance to its projection. In addition to this encoding, we\nalso propose a novel network architecture to seamlessly incorporate those three\nfactors. From our extensive experiments, the benefits of the proposed strategy\nare clearly demonstrated by its superior performance over the competitive\nmethods.",
          "link": "http://arxiv.org/abs/2105.10500",
          "publishedOn": "2021-07-05T01:54:59.149Z",
          "wordCount": 701,
          "title": "Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection. (arXiv:2105.10500v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Joao A. Candido Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armand_S/0/1/0/all/0/1\">St&#xe9;phane Armand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "In this work, we want to learn to model the dynamics of similar yet distinct\ngroups of interacting objects. These groups follow some common physical laws\nthat exhibit specificities that are captured through some vectorial\ndescription. We develop a model that allows us to do conditional generation\nfrom any such group given its vectorial description. Unlike previous work on\nlearning dynamical systems that can only do trajectory completion and require a\npart of the trajectory dynamics to be provided as input in generation time, we\ndo generation using only the conditioning vector with no access to generation\ntime's trajectories. We evaluate our model in the setting of modeling human\ngait and, in particular pathological human gait.",
          "link": "http://arxiv.org/abs/2106.11083",
          "publishedOn": "2021-07-05T01:54:59.142Z",
          "wordCount": 576,
          "title": "Conditional Neural Relational Inference for Interacting Systems. (arXiv:2106.11083v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11713",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it",
          "link": "http://arxiv.org/abs/2101.11713",
          "publishedOn": "2021-07-05T01:54:59.135Z",
          "wordCount": 571,
          "title": "Neural networks for Anatomical Therapeutic Chemical (ATC). (arXiv:2101.11713v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1\">Andrea Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1\">Sebastian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "In complex industrial settings, it is common practice to monitor the\noperation of machines in order to detect undesired states, adjust maintenance\nschedules, optimize system performance or collect usage statistics of\nindividual machines. In this work, we focus on estimating the power output of a\nCombined Heat and Power (CHP) machine of a medium-sized company facility by\nanalyzing the total facility power consumption. We formulate the problem as a\ntime-series classification problem where the class label represents the CHP\npower output. As the facility is fully instrumented and sensor measurements\nfrom the CHP are available, we generate the training labels in an automated\nfashion from the CHP sensor readings. However, sensor failures result in\nmislabeled training data samples which are hard to detect and remove from the\ndataset. Therefore, we propose a novel multi-task deep learning approach that\njointly trains a classifier and an autoencoder with a shared embedding\nrepresentation. The proposed approach targets to gradually correct the\nmislabelled data samples during training in a self-supervised fashion, without\nany prior assumption on the amount of label noise. We benchmark our approach on\nseveral time-series classification datasets and find it to be comparable and\nsometimes better than state-of-the-art methods. On the real-world use-case of\npredicting the CHP power output, we thoroughly evaluate the architectural\ndesign choices and show that the final architecture considerably increases the\nrobustness of the learning process and consistently beats other recent\nstate-of-the-art algorithms in the presence of unstructured as well as\nstructured label noise.",
          "link": "http://arxiv.org/abs/2105.00349",
          "publishedOn": "2021-07-05T01:54:59.116Z",
          "wordCount": 742,
          "title": "Estimating the electrical power output of industrial devices with end-to-end time-series classification in the presence of label noise. (arXiv:2105.00349v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06006",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>",
          "description": "In this paper, we propose Textual Echo Cancellation (TEC) - a framework for\ncancelling the text-to-speech (TTS) playback echo from overlapping speech\nrecordings. Such a system can largely improve speech recognition performance\nand user experience for intelligent devices such as smart speakers, as the user\ncan talk to the device while the device is still playing the TTS signal\nresponding to the previous query. We implement this system by using a novel\nsequence-to-sequence model with multi-source attention that takes both the\nmicrophone mixture signal and source text of the TTS playback as inputs, and\npredicts the enhanced audio. Experiments show that the textual information of\nthe TTS playback is critical to enhancement performance. Besides, the text\nsequence is much smaller in size compared with the raw acoustic signal of the\nTTS playback, and can be immediately transmitted to the device or ASR server\neven before the playback is synthesized. Therefore, our proposed approach\neffectively reduces Internet communication and latency compared with\nalternative approaches such as acoustic echo cancellation (AEC).",
          "link": "http://arxiv.org/abs/2008.06006",
          "publishedOn": "2021-07-05T01:54:59.108Z",
          "wordCount": 632,
          "title": "Textual Echo Cancellation. (arXiv:2008.06006v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01106",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "The conditional gradient method (CGM) is widely used in large-scale sparse\nconvex optimization, having a low per iteration computational cost for\nstructured sparse regularizers and a greedy approach to collecting nonzeros. We\nexplore the sparsity acquiring properties of a general penalized CGM (P-CGM)\nfor convex regularizers and a reweighted penalized CGM (RP-CGM) for nonconvex\nregularizers, replacing the usual convex constraints with gauge-inspired\npenalties. This generalization does not increase the per-iteration complexity\nnoticeably. Without assuming bounded iterates or using line search, we show\n$O(1/t)$ convergence of the gap of each subproblem, which measures distance to\na stationary point. We couple this with a screening rule which is safe in the\nconvex case, converging to the true support at a rate $O(1/(\\delta^2))$ where\n$\\delta \\geq 0$ measures how close the problem is to degeneracy. In the\nnonconvex case the screening rule converges to the true support in a finite\nnumber of iterations, but is not necessarily safe in the intermediate iterates.\nIn our experiments, we verify the consistency of the method and adjust the\naggressiveness of the screening rule by tuning the concavity of the\nregularizer.",
          "link": "http://arxiv.org/abs/2107.01106",
          "publishedOn": "2021-07-05T01:54:59.096Z",
          "wordCount": 617,
          "title": "Screening for a Reweighted Penalized Conditional Gradient Method. (arXiv:2107.01106v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:59.089Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01103",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Majumdar_S/0/1/0/all/0/1\">Subhabrata Majumdar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1\">Snigdhansu Chatterjee</a>",
          "description": "High-dimensional data, where the dimension of the feature space is much\nlarger than sample size, arise in a number of statistical applications. In this\ncontext, we construct the generalized multivariate sign transformation, defined\nas a vector divided by its norm. For different choices of the norm function,\nthe resulting transformed vector adapts to certain geometrical features of the\ndata distribution. Building up on this idea, we obtain one-sample and\ntwo-sample testing procedures for mean vectors of high-dimensional data using\nthese generalized sign vectors. These tests are based on U-statistics using\nkernel inner products, do not require prohibitive assumptions, and are amenable\nto a fast randomization-based implementation. Through experiments in a number\nof data settings, we show that tests using generalized signs display higher\npower than existing tests, while maintaining nominal type-I error rates.\nFinally, we provide example applications on the MNIST and Minnesota Twin\nStudies genomic data.",
          "link": "http://arxiv.org/abs/2107.01103",
          "publishedOn": "2021-07-05T01:54:59.079Z",
          "wordCount": 583,
          "title": "Generalized Multivariate Signs for Nonparametric Hypothesis Testing in High Dimensions. (arXiv:2107.01103v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01163",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Baldassi_C/0/1/0/all/0/1\">Carlo Baldassi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lauditi_C/0/1/0/all/0/1\">Clarissa Lauditi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Malatesta_E/0/1/0/all/0/1\">Enrico M. Malatesta</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Perugini_G/0/1/0/all/0/1\">Gabriele Perugini</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zecchina_R/0/1/0/all/0/1\">Riccardo Zecchina</a>",
          "description": "The success of deep learning has revealed the application potential of neural\nnetworks across the sciences and opened up fundamental theoretical problems. In\nparticular, the fact that learning algorithms based on simple variants of\ngradient methods are able to find near-optimal minima of highly nonconvex loss\nfunctions is an unexpected feature of neural networks which needs to be\nunderstood in depth. Such algorithms are able to fit the data almost perfectly,\neven in the presence of noise, and yet they have excellent predictive\ncapabilities. Several empirical results have shown a reproducible correlation\nbetween the so-called flatness of the minima achieved by the algorithms and the\ngeneralization performance. At the same time, statistical physics results have\nshown that in nonconvex networks a multitude of narrow minima may coexist with\na much smaller number of wide flat minima, which generalize well. Here we show\nthat wide flat minima arise from the coalescence of minima that correspond to\nhigh-margin classifications. Despite being exponentially rare compared to\nzero-margin solutions, high-margin minima tend to concentrate in particular\nregions. These minima are in turn surrounded by other solutions of smaller and\nsmaller margin, leading to dense regions of solutions over long distances. Our\nanalysis also provides an alternative analytical method for estimating when\nflat minima appear and when algorithms begin to find solutions, as the number\nof model parameters varies.",
          "link": "http://arxiv.org/abs/2107.01163",
          "publishedOn": "2021-07-05T01:54:59.061Z",
          "wordCount": 687,
          "title": "Unveiling the structure of wide flat minima in neural networks. (arXiv:2107.01163v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:59.047Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13922",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1\">Mihaela Rosca</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1\">Benoit Dherin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrett_D/0/1/0/all/0/1\">David G. T. Barrett</a>",
          "description": "Gradient-based methods for two-player games produce rich dynamics that can\nsolve challenging problems, yet can be difficult to stabilize and understand.\nPart of this complexity originates from the discrete update steps given by\nsimultaneous or alternating gradient descent, which causes each player to drift\naway from the continuous gradient flow -- a phenomenon we call discretization\ndrift. Using backward error analysis, we derive modified continuous dynamical\nsystems that closely follow the discrete dynamics. These modified dynamics\nprovide an insight into the notorious challenges associated with zero-sum\ngames, including Generative Adversarial Networks. In particular, we identify\ndistinct components of the discretization drift that can alter performance and\nin some cases destabilize the game. Finally, quantifying discretization drift\nallows us to identify regularizers that explicitly cancel harmful forms of\ndrift or strengthen beneficial forms of drift, and thus improve performance of\nGAN training.",
          "link": "http://arxiv.org/abs/2105.13922",
          "publishedOn": "2021-07-05T01:54:59.041Z",
          "wordCount": 587,
          "title": "Discretization Drift in Two-Player Games. (arXiv:2105.13922v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01017",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Menezes_A/0/1/0/all/0/1\">Angelo Garangau Menezes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mastelini_S/0/1/0/all/0/1\">Saulo Martiello Mastelini</a>",
          "description": "Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.",
          "link": "http://arxiv.org/abs/2107.01017",
          "publishedOn": "2021-07-05T01:54:59.002Z",
          "wordCount": 598,
          "title": "MegazordNet: combining statistical and machine learning standpoints for time series forecasting. (arXiv:2107.01017v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:58.985Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhechba_M/0/1/0/all/0/1\">Mehdi Boukhechba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1\">Laura E. Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daqing Zhang</a>",
          "description": "Mobile Sensing Apps have been widely used as a practical approach to collect\nbehavioral and health-related information from individuals and provide timely\nintervention to promote health and well-beings, such as mental health and\nchronic cares. As the objectives of mobile sensing could be either \\emph{(a)\npersonalized medicine for individuals} or \\emph{(b) public health for\npopulations}, in this work we review the design of these mobile sensing apps,\nand propose to categorize the design of these apps/systems in two paradigms --\n\\emph{(i) Personal Sensing} and \\emph{(ii) Crowd Sensing} paradigms. While both\nsensing paradigms might incorporate with common ubiquitous sensing\ntechnologies, such as wearable sensors, mobility monitoring, mobile data\noffloading, and/or cloud-based data analytics to collect and process sensing\ndata from individuals, we present a novel taxonomy system with two major\ncomponents that can specify and classify apps/systems from aspects of the\nlife-cycle of mHealth Sensing: \\emph{(1) Sensing Task Creation \\&\nParticipation}, \\emph{(2) Health Surveillance \\& Data Collection}, and\n\\emph{(3) Data Analysis \\& Knowledge Discovery}. With respect to different\ngoals of the two paradigms, this work systematically reviews this field, and\nsummarizes the design of typical apps/systems in the view of the configurations\nand interactions between these two components. In addition to summarization,\nthe proposed taxonomy system also helps figure out the potential directions of\nmobile sensing for health from both personalized medicines and population\nhealth perspectives.",
          "link": "http://arxiv.org/abs/2107.00948",
          "publishedOn": "2021-07-05T01:54:58.978Z",
          "wordCount": 685,
          "title": "From Personalized Medicine to Population Health: A Survey of mHealth Sensing Techniques. (arXiv:2107.00948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01032",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shezan_S/0/1/0/all/0/1\">SK. A. Shezan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rawdah_S/0/1/0/all/0/1\">S. Rawdah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shafin Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_Z/0/1/0/all/0/1\">Ziaur Rahman</a>",
          "description": "The energy demand is growing daily at an accelerated pace due to the\ninternationalization and development of civilization. Yet proper economic\nutilization of additional energy generated by the Islanded Hybrid Microgrid\nSystem (IHMS) that was not consumed by the load is a major global challenge. To\nresolve the above-stated summons, this research focuses on a multi-optimal\ncombination of IHMS for the Penang Hill Resort located on Penang Island,\nMalaysia, with effective use of redundant energy. To avail this excess energy\nefficiently, an electrical heater along with a storage tank has been designed\nconcerning diversion load having proper energy management. Furthermore, the\nsystem design has adopted the HOMER Pro software for profitable and practical\nanalysis. Alongside, MATLAB Simulink had stabilized the whole system by\nrepresenting the values of 2068 and 19,072 kW that have been determined as the\napproximated peak and average load per day for the resort. Moreover, the\noptimized IHMS is comprehended of Photovoltaic (PV) cells, Diesel Generator,\nWind Turbine, Battery, and Converter. Adjacent to this, the optimized system\nensued in having a Net Present Cost (NPC) of $21.66 million, Renewable Fraction\n(RF) of 27.8%, Cost of Energy (COE) of $0.165/kWh, CO2 of 1,735,836 kg/year,\nand excess energy of 517.29MWh per annum. Since the diesel generator lead\nsystem was included in the scheme, a COE of $0.217/kWh, CO2 of 5,124,879\nkg/year, and NPC of $23.25 million were attained. The amount of excess energy\nis effectively utilized with an electrical heater as a diversion load.",
          "link": "http://arxiv.org/abs/2107.01032",
          "publishedOn": "2021-07-05T01:54:58.971Z",
          "wordCount": 736,
          "title": "Design and implementation of an islanded hybrid microgrid system for a large resort center for Penang Island with the proper application of excess energy. (arXiv:2107.01032v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:58.965Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:58.957Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:58.940Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jitani_A/0/1/0/all/0/1\">Anirudha Jitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Aditya Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhongwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_zeid_H/0/1/0/all/0/1\">Hatem Abou-zeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fapi_E/0/1/0/all/0/1\">Emmanuel T. Fapi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purmehdi_H/0/1/0/all/0/1\">Hakimeh Purmehdi</a>",
          "description": "Mobile Edge Computing (MEC) refers to the concept of placing computational\ncapability and applications at the edge of the network, providing benefits such\nas reduced latency in handling client requests, reduced network congestion, and\nimproved performance of applications. The performance and reliability of MEC\nare degraded significantly when one or several edge servers in the cluster are\noverloaded. Especially when a server crashes due to the overload, it causes\nservice failures in MEC. In this work, an adaptive admission control policy to\nprevent edge node from getting overloaded is presented. This approach is based\non a recently-proposed low complexity RL (Reinforcement Learning) algorithm\ncalled SALMUT (Structure-Aware Learning for Multiple Thresholds), which\nexploits the structure of the optimal admission control policy in multi-class\nqueues for an average-cost setting. We extend the framework to work for node\noverload-protection problem in a discounted-cost setting. The proposed solution\nis validated using several scenarios mimicking real-world deployments in two\ndifferent settings - computer simulations and a docker testbed. Our empirical\nevaluations show that the total discounted cost incurred by SALMUT is similar\nto state-of-the-art deep RL algorithms such as PPO (Proximal Policy\nOptimization) and A2C (Advantage Actor Critic) but requires an order of\nmagnitude less time to train, outputs easily interpretable policy, and can be\ndeployed in an online manner.",
          "link": "http://arxiv.org/abs/2107.01025",
          "publishedOn": "2021-07-05T01:54:58.932Z",
          "wordCount": 671,
          "title": "Structure-aware reinforcement learning for node-overload protection in mobile edge computing. (arXiv:2107.01025v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaoqun You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xianbin Cao</a>",
          "description": "This paper investigates the problem of providing ultra-reliable and\nenergy-efficient virtual reality (VR) experiences for wireless mobile users. To\nensure reliable ultra-high-definition (UHD) video frame delivery to mobile\nusers and enhance their immersive visual experiences, a coordinated multipoint\n(CoMP) transmission technique and millimeter wave (mmWave) communications are\nexploited. Owing to user movement and time-varying wireless channels, the\nwireless VR experience enhancement problem is formulated as a\nsequence-dependent and mixed-integer problem with a goal of maximizing users'\nfeeling of presence (FoP) in the virtual world, subject to power consumption\nconstraints on access points (APs) and users' head-mounted displays (HMDs). The\nproblem, however, is hard to be directly solved due to the lack of users'\naccurate tracking information and the sequence-dependent and mixed-integer\ncharacteristics. To overcome this challenge, we develop a parallel echo state\nnetwork (ESN) learning method to predict users' tracking information by\ntraining fresh and historical tracking samples separately collected by APs.\nWith the learnt results, we propose a deep reinforcement learning (DRL) based\noptimization algorithm to solve the formulated problem. In this algorithm, we\nimplement deep neural networks (DNNs) as a scalable solution to produce integer\ndecision variables and solving a continuous power control problem to criticize\nthe integer decision variables. Finally, the performance of the proposed\nalgorithm is compared with various benchmark algorithms, and the impact of\ndifferent design parameters is also discussed. Simulation results demonstrate\nthat the proposed algorithm is more 4.14% energy-efficient than the benchmark\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.01001",
          "publishedOn": "2021-07-05T01:54:58.924Z",
          "wordCount": 696,
          "title": "Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets Deep Reinforcement Learning. (arXiv:2107.01001v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Albert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chun Yin Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hains_G/0/1/0/all/0/1\">Ga&#xe9;tan Hains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humphrey_J/0/1/0/all/0/1\">Jack Humphrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhrmann_H/0/1/0/all/0/1\">Hans Fuhrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1\">Youry Khmelevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_C/0/1/0/all/0/1\">Chris Mazur</a>",
          "description": "Gamers Private Network (GPN) is a client/server technology that guarantees a\nconnection for online video games that is more reliable and lower latency than\na standard internet connection. Users of the GPN technology benefit from a\nstable and high-quality gaming experience for online games, which are hosted\nand played across the world. After transforming a massive volume of raw\nnetworking data collected by WTFast, we have structured the cleaned data into a\nspecial-purpose data warehouse and completed the extensive analysis using\nmachine learning and neural nets technologies, and business intelligence tools.\nThese analyses demonstrate the ability to predict and quantify changes in the\nnetwork and demonstrate the benefits gained from the use of a GPN for users\nwhen connected to an online game session.",
          "link": "http://arxiv.org/abs/2107.00998",
          "publishedOn": "2021-07-05T01:54:58.916Z",
          "wordCount": 595,
          "title": "Gamers Private Network Performance Forecasting. From Raw Data to the Data Warehouse with Machine Learning and Neural Nets. (arXiv:2107.00998v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of neural\ncode search. Last, we define two intuitive quantification metrics oriented to\nthe completeness of semantic and syntactic information of the code data.",
          "link": "http://arxiv.org/abs/2107.00992",
          "publishedOn": "2021-07-05T01:54:58.901Z",
          "wordCount": 556,
          "title": "Multimodal Representation for Neural Code Search. (arXiv:2107.00992v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:58.894Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1\">Alexander Gepperth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfulb_B/0/1/0/all/0/1\">Benedikt Pf&#xfc;lb</a>",
          "description": "We present an approach for efficiently training Gaussian Mixture Model (GMM)\nby Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional\nstreaming data. Our training scheme does not require data-driven parameter\ninitialization (e.g., k-means) and can thus be trained based on a random\ninitialization. Furthermore, the approach allows mini-batch sizes as low as 1,\nwhich are typical for streaming-data settings. Major problems in such settings\nare undesirable local optima during early training phases and numerical\ninstabilities due to high data dimensionalities. We introduce an adaptive\nannealing procedure to address the first problem, whereas numerical\ninstabilities are eliminated by using an exponential-free approximation to the\nstandard GMM log-likelihood. Experiments on a variety of visual and non-visual\nbenchmarks show that our SGD approach can be trained completely without, for\ninstance, k-means based centroid initialization. It also compares favorably to\nan online variant of Expectation-Maximization (EM) - stochastic EM (sEM), which\nit outperforms by a large margin for very high-dimensional data.",
          "link": "http://arxiv.org/abs/1912.09379",
          "publishedOn": "2021-07-05T01:54:58.854Z",
          "wordCount": 635,
          "title": "Gradient-based training of Gaussian Mixture Models for High-Dimensional Streaming Data. (arXiv:1912.09379v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+dEon_G/0/1/0/all/0/1\">Greg d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dEon_J/0/1/0/all/0/1\">Jason d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">James R. Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>",
          "description": "Supervised learning models often make systematic errors on rare subsets of\nthe data. However, such systematic errors can be difficult to identify, as\nmodel performance can only be broken down across sensitive groups when these\ngroups are known and explicitly labelled. This paper introduces a method for\ndiscovering systematic errors, which we call the spotlight. The key idea is\nthat similar inputs tend to have similar representations in the final hidden\nlayer of a neural network. We leverage this structure by \"shining a spotlight\"\non this representation space to find contiguous regions where the model\nperforms poorly. We show that the spotlight surfaces semantically meaningful\nareas of weakness in a wide variety of model architectures, including image\nclassifiers, language models, and recommender systems.",
          "link": "http://arxiv.org/abs/2107.00758",
          "publishedOn": "2021-07-05T01:54:58.836Z",
          "wordCount": 569,
          "title": "The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models. (arXiv:2107.00758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1\">Dongjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_S/0/1/0/all/0/1\">Soonsang Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Sunbeom Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junyoung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hanyoung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jongkeun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyung_W/0/1/0/all/0/1\">Wonshik Kyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1\">Byungmin Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jounghoon Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeongkwan Kimand Changyoung Kim</a>",
          "description": "In spectroscopic experiments, data acquisition in multi-dimensional phase\nspace may require long acquisition time, owing to the large phase space volume\nto be covered. In such case, the limited time available for data acquisition\ncan be a serious constraint for experiments in which multidimensional spectral\ndata are acquired. Here, taking angle-resolved photoemission spectroscopy\n(ARPES) as an example, we demonstrate a denoising method that utilizes deep\nlearning as an intelligent way to overcome the constraint. With readily\navailable ARPES data and random generation of training data set, we\nsuccessfully trained the denoising neural network without overfitting. The\ndenoising neural network can remove the noise in the data while preserving its\nintrinsic information. We show that the denoising neural network allows us to\nperform similar level of second-derivative and line shape analysis on data\ntaken with two orders of magnitude less acquisition time. The importance of our\nmethod lies in its applicability to any multidimensional spectral data that are\nsusceptible to statistical noise.",
          "link": "http://arxiv.org/abs/2107.00844",
          "publishedOn": "2021-07-05T01:54:58.830Z",
          "wordCount": 645,
          "title": "Deep learning-based statistical noise reduction for multidimensional spectral data. (arXiv:2107.00844v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:58.815Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.780Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.774Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:58.767Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:58.752Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Quanyan Zhu</a>",
          "description": "The rapid growth in the number of devices and their connectivity has enlarged\nthe attack surface and weakened cyber systems. As attackers become increasingly\nsophisticated and resourceful, mere reliance on traditional cyber protection,\nsuch as intrusion detection, firewalls, and encryption, is insufficient to\nsecure cyber systems. Cyber resilience provides a new security paradigm that\ncomplements inadequate protection with resilience mechanisms. A Cyber-Resilient\nMechanism (CRM) adapts to the known or zero-day threats and uncertainties in\nreal-time and strategically responds to them to maintain the critical functions\nof the cyber systems. Feedback architectures play a pivotal role in enabling\nthe online sensing, reasoning, and actuation of the CRM. Reinforcement Learning\n(RL) is an important class of algorithms that epitomize the feedback\narchitectures for cyber resiliency, allowing the CRM to provide dynamic and\nsequential responses to attacks with limited prior knowledge of the attacker.\nIn this work, we review the literature on RL for cyber resiliency and discuss\nthe cyber-resilient defenses against three major types of vulnerabilities,\ni.e., posture-related, information-related, and human-related vulnerabilities.\nWe introduce moving target defense, defensive cyber deception, and assistive\nhuman security technologies as three application domains of CRMs to elaborate\non their designs. The RL technique also has vulnerabilities itself. We explain\nthe major vulnerabilities of RL and present several attack models in which the\nattacks target the rewards, the measurements, and the actuators. We show that\nthe attacker can trick the RL agent into learning a nefarious policy with\nminimum attacking effort, which shows serious security concerns for RL-enabled\nsystems. Finally, we discuss the future challenges of RL for cyber security and\nresiliency and emerging applications of RL-based CRMs.",
          "link": "http://arxiv.org/abs/2107.00783",
          "publishedOn": "2021-07-05T01:54:58.746Z",
          "wordCount": 708,
          "title": "Reinforcement Learning for Feedback-Enabled Cyber Resilience. (arXiv:2107.00783v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1\">Tareq Si Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1\">Giovanni Neglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carra_D/0/1/0/all/0/1\">Damiano Carra</a>",
          "description": "Similarity search is a key operation in multimedia retrieval systems and\nrecommender systems, and it will play an important role also for future machine\nlearning and augmented reality applications. When these systems need to serve\nlarge objects with tight delay constraints, edge servers close to the end-user\ncan operate as similarity caches to speed up the retrieval. In this paper we\npresent A\\c{C}AI, a new similarity caching policy which improves on the state\nof the art by using (i) an (approximate) index for the whole catalog to decide\nwhich objects to serve locally and which to retrieve from the remote server,\nand (ii) a mirror ascent algorithm to update the set of local objects with\nstrong guarantees even when the request process does not exhibit any\nstatistical regularity.",
          "link": "http://arxiv.org/abs/2107.00957",
          "publishedOn": "2021-07-05T01:54:58.739Z",
          "wordCount": 566,
          "title": "A\\c{C}AI: Ascent Similarity Caching with Approximate Indexes. (arXiv:2107.00957v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naumann_P/0/1/0/all/0/1\">Philip Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Counterfactuals have become a popular technique nowadays for interacting with\nblack-box machine learning models and understanding how to change a particular\ninstance to obtain a desired outcome from the model. However, most existing\napproaches assume instant materialization of these changes, ignoring that they\nmay require effort and a specific order of application. Recently, methods have\nbeen proposed that also consider the order in which actions are applied,\nleading to the so-called sequential counterfactual generation problem.\n\nIn this work, we propose a model-agnostic method for sequential\ncounterfactual generation. We formulate the task as a multi-objective\noptimization problem and present a genetic algorithm approach to find optimal\nsequences of actions leading to the counterfactuals. Our cost model considers\nnot only the direct effect of an action, but also its consequences.\nExperimental results show that compared to state-of-the-art, our approach\ngenerates less costly solutions, is more efficient and provides the user with a\ndiverse set of solutions to choose from.",
          "link": "http://arxiv.org/abs/2104.05592",
          "publishedOn": "2021-07-05T01:54:58.734Z",
          "wordCount": 620,
          "title": "Consequence-aware Sequential Counterfactual Generation. (arXiv:2104.05592v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.05686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Deep learning models are full of hyperparameters, which are set manually\nbefore the learning process can start. To find the best configuration for these\nhyperparameters in such a high dimensional space, with time-consuming and\nexpensive model training / validation, is not a trivial challenge. Bayesian\noptimization is a powerful tool for the joint optimization of hyperparameters,\nefficiently trading off exploration and exploitation of the hyperparameter\nspace. In this paper, we discuss Bayesian hyperparameter optimization,\nincluding hyperparameter optimization, Bayesian optimization, and Gaussian\nprocesses. We also review BoTorch, GPyTorch and Ax, the new open-source\nframeworks that we use for Bayesian optimization, Gaussian process inference\nand adaptive experimentation, respectively. For experimentation, we apply\nBayesian hyperparameter optimization, for optimizing group weights, to weighted\ngroup pooling, which couples unsupervised tiered graph autoencoders learning\nand supervised graph prediction learning for molecular graphs. We find that Ax,\nBoTorch and GPyTorch together provide a simple-to-use but powerful framework\nfor Bayesian hyperparameter optimization, using Ax's high-level API that\nconstructs and runs a full optimization loop and returns the best\nhyperparameter configuration.",
          "link": "http://arxiv.org/abs/1912.05686",
          "publishedOn": "2021-07-05T01:54:58.726Z",
          "wordCount": 629,
          "title": "Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax. (arXiv:1912.05686v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>",
          "description": "The last few years have seen gigantic leaps in algorithms and systems to\nsupport efficient deep learning inference. Pruning and quantization algorithms\ncan now consistently compress neural networks by an order of magnitude. For a\ncompressed neural network, a multitude of inference frameworks have been\ndesigned to maximize the performance of the target hardware. While we find\nmature support for quantized neural networks in production frameworks such as\nOpenVINO and MNN, support for pruned sparse neural networks is still lacking.\nTo tackle this challenge, we present SparseDNN, a sparse deep learning\ninference engine targeting CPUs. We present both kernel-level optimizations\nwith a sparse code generator to accelerate sparse operators and novel\nnetwork-level optimizations catering to sparse networks. We show that our\nsparse code generator can achieve significant speedups over state-of-the-art\nsparse and dense libraries. On end-to-end benchmarks such as Huggingface\npruneBERT, SparseDNN achieves up to 5x throughput improvement over dense\ninference with state-of-the-art OpenVINO.",
          "link": "http://arxiv.org/abs/2101.07948",
          "publishedOn": "2021-07-05T01:54:58.721Z",
          "wordCount": 612,
          "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs. (arXiv:2101.07948v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ke_N/0/1/0/all/0/1\">Nan Rosemary Ke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Didolkar_A/0/1/0/all/0/1\">Aniket Didolkar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mittal_S/0/1/0/all/0/1\">Sarthak Mittal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lajoie_G/0/1/0/all/0/1\">Guillaume Lajoie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Rezende</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mozer_M/0/1/0/all/0/1\">Michael Mozer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Inducing causal relationships from observations is a classic problem in\nmachine learning. Most work in causality starts from the premise that the\ncausal variables themselves are observed. However, for AI agents such as robots\ntrying to make sense of their environment, the only observables are low-level\nvariables like pixels in images. To generalize well, an agent must induce\nhigh-level variables, particularly those which are causal or are affected by\ncausal variables. A central goal for AI and causality is thus the joint\ndiscovery of abstract representations and causal structure. However, we note\nthat existing environments for studying causal induction are poorly suited for\nthis objective because they have complicated task-specific causal graphs which\nare impossible to manipulate parametrically (e.g., number of nodes, sparsity,\ncausal chain length, etc.). In this work, our goal is to facilitate research in\nlearning representations of high-level variables as well as causal structures\namong them. In order to systematically probe the ability of methods to identify\nthese variables and structures, we design a suite of benchmarking RL\nenvironments. We evaluate various representation learning algorithms from the\nliterature and find that explicitly incorporating structure and modularity in\nmodels can help causal induction in model-based reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.00848",
          "publishedOn": "2021-07-05T01:54:58.704Z",
          "wordCount": 653,
          "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning. (arXiv:2107.00848v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03164",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Batzner_S/0/1/0/all/0/1\">Simon Batzner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Musaelian_A/0/1/0/all/0/1\">Albert Musaelian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_L/0/1/0/all/0/1\">Lixin Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Geiger_M/0/1/0/all/0/1\">Mario Geiger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mailoa_J/0/1/0/all/0/1\">Jonathan P. Mailoa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kornbluth_M/0/1/0/all/0/1\">Mordechai Kornbluth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Molinari_N/0/1/0/all/0/1\">Nicola Molinari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Smidt_T/0/1/0/all/0/1\">Tess E. Smidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kozinsky_B/0/1/0/all/0/1\">Boris Kozinsky</a>",
          "description": "This work presents Neural Equivariant Interatomic Potentials (NequIP), a\nSE(3)-equivariant neural network approach for learning interatomic potentials\nfrom ab-initio calculations for molecular dynamics simulations. While most\ncontemporary symmetry-aware models use invariant convolutions and only act on\nscalars, NequIP employs SE(3)-equivariant convolutions for interactions of\ngeometric tensors, resulting in a more information-rich and faithful\nrepresentation of atomic environments. The method achieves state-of-the-art\naccuracy on a challenging set of diverse molecules and materials while\nexhibiting remarkable data efficiency. NequIP outperforms existing models with\nup to three orders of magnitude fewer training data, challenging the widely\nheld belief that deep neural networks require massive training sets. The high\ndata efficiency of the method allows for the construction of accurate\npotentials using high-order quantum chemical level of theory as reference and\nenables high-fidelity molecular dynamics simulations over long time scales.",
          "link": "http://arxiv.org/abs/2101.03164",
          "publishedOn": "2021-07-05T01:54:58.698Z",
          "wordCount": 614,
          "title": "SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials. (arXiv:2101.03164v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Existing frameworks for transfer learning are incomplete from a systems\ntheoretic perspective. They place emphasis on notions of domain and task, and\nneglect notions of structure and behavior. In doing so, they limit the extent\nto which formalism can be carried through into the elaboration of their\nframeworks. Herein, we use Mesarovician systems theory to define transfer\nlearning as a relation on sets and subsequently characterize the general nature\nof transfer learning as a mathematical construct. We interpret existing\nframeworks in terms of ours and go beyond existing frameworks to define notions\nof transferability, transfer roughness, and transfer distance. Importantly,\ndespite its formalism, our framework avoids the detailed mathematics of\nlearning theory or machine learning solution methods without excluding their\nconsideration. As such, we provide a formal, general systems framework for\nmodeling transfer learning that offers a rigorous foundation for system design\nand analysis.",
          "link": "http://arxiv.org/abs/2107.01196",
          "publishedOn": "2021-07-05T01:54:58.691Z",
          "wordCount": 580,
          "title": "A Systems Theory of Transfer Learning. (arXiv:2107.01196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00877",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Amakasu_T/0/1/0/all/0/1\">Takashi Amakasu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chauvet_N/0/1/0/all/0/1\">Nicolas Chauvet</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bachelier_G/0/1/0/all/0/1\">Guillaume Bachelier</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Huant_S/0/1/0/all/0/1\">Serge Huant</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horisaki_R/0/1/0/all/0/1\">Ryoichi Horisaki</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Naruse_M/0/1/0/all/0/1\">Makoto Naruse</a>",
          "description": "In recent cross-disciplinary studies involving both optics and computing,\nsingle-photon-based decision-making has been demonstrated by utilizing the\nwave-particle duality of light to solve multi-armed bandit problems.\nFurthermore, entangled-photon-based decision-making has managed to solve a\ncompetitive multi-armed bandit problem in such a way that conflicts of\ndecisions among players are avoided while ensuring equality. However, as these\nstudies are based on the polarization of light, the number of available choices\nis limited to two, corresponding to two orthogonal polarization states. Here we\npropose a scalable principle to solve competitive decision-making situations by\nusing the orbital angular momentum as the tunable degree of freedom of photons,\nwhich theoretically allows an unlimited number of arms. Moreover, by extending\nthe Hong-Ou-Mandel effect to more than two states, we theoretically establish\nan experimental configuration able to generate entangled photon states with\norbital angular momentum and conditions that provide conflict-free selections\nat every turn. We numerically examine total rewards regarding three-armed\nbandit problems, for which the proposed strategy accomplishes almost the\ntheoretical maximum, which is greater than a conventional mixed strategy\nintending to realize Nash equilibrium. This is thanks to the entanglement\nproperty that achieves no-conflict selections, even in the exploring phase to\nfind the best arms.",
          "link": "http://arxiv.org/abs/2107.00877",
          "publishedOn": "2021-07-05T01:54:58.675Z",
          "wordCount": 651,
          "title": "Conflict-free collective stochastic decision making by orbital angular momentum entangled photons. (arXiv:2107.00877v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takabatake_K/0/1/0/all/0/1\">Kazuya Takabatake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "Dependency networks (Heckerman et al., 2000) are potential probabilistic\ngraphical models for systems comprising a large number of variables. Like\nBayesian networks, the structure of a dependency network is represented by a\ndirected graph, and each node has a conditional probability table. Learning and\ninference are realized locally on individual nodes; therefore, computation\nremains tractable even with a large number of variables. However, the\ndependency network's learned distribution is the stationary distribution of a\nMarkov chain called pseudo-Gibbs sampling and has no closed-form expressions.\nThis technical disadvantage has impeded the development of dependency networks.\nIn this paper, we consider a certain manifold for each node. Then, we can\ninterpret pseudo-Gibbs sampling as iterative m-projections onto these\nmanifolds. This interpretation provides a theoretical bound for the location\nwhere the stationary distribution of pseudo-Gibbs sampling exists in\ndistribution space. Furthermore, this interpretation involves structure and\nparameter learning algorithms as optimization problems. In addition, we compare\ndependency and Bayesian networks experimentally. The results demonstrate that\nthe dependency network and the Bayesian network have roughly the same\nperformance in terms of the accuracy of their learned distributions. The\nresults also show that the dependency network can learn much faster than the\nBayesian network.",
          "link": "http://arxiv.org/abs/2107.00871",
          "publishedOn": "2021-07-05T01:54:58.667Z",
          "wordCount": 632,
          "title": "Reconsidering Dependency Networks from an Information Geometry Perspective. (arXiv:2107.00871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wenqi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Gong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_A/0/1/0/all/0/1\">Arun Iyengar</a>",
          "description": "Federated learning(FL) is an emerging distributed learning paradigm with\ndefault client privacy because clients can keep sensitive data on their devices\nand only share local training parameter updates with the federated server.\nHowever, recent studies reveal that gradient leakages in FL may compromise the\nprivacy of client training data. This paper presents a gradient leakage\nresilient approach to privacy-preserving federated learning with per training\nexample-based client differential privacy, coined as Fed-CDP. It makes three\noriginal contributions. First, we identify three types of client gradient\nleakage threats in federated learning even with encrypted client-server\ncommunications. We articulate when and why the conventional server coordinated\ndifferential privacy approach, coined as Fed-SDP, is insufficient to protect\nthe privacy of the training data. Second, we introduce Fed-CDP, the per\nexample-based client differential privacy algorithm, and provide a formal\nanalysis of Fed-CDP with the $(\\epsilon, \\delta)$ differential privacy\nguarantee, and a formal comparison between Fed-CDP and Fed-SDP in terms of\nprivacy accounting. Third, we formally analyze the privacy-utility trade-off\nfor providing differential privacy guarantee by Fed-CDP and present a dynamic\ndecay noise-injection policy to further improve the accuracy and resiliency of\nFed-CDP. We evaluate and compare Fed-CDP and Fed-CDP(decay) with Fed-SDP in\nterms of differential privacy guarantee and gradient leakage resilience over\nfive benchmark datasets. The results show that the Fed-CDP approach outperforms\nconventional Fed-SDP in terms of resilience to client gradient leakages while\noffering competitive accuracy performance in federated learning.",
          "link": "http://arxiv.org/abs/2107.01154",
          "publishedOn": "2021-07-05T01:54:58.660Z",
          "wordCount": 664,
          "title": "Gradient-Leakage Resilient Federated Learning. (arXiv:2107.01154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongjie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Ding-Xuan Zhou</a>",
          "description": "We consider a family of deep neural networks consisting of two groups of\nconvolutional layers, a downsampling operator, and a fully connected layer. The\nnetwork structure depends on two structural parameters which determine the\nnumbers of convolutional layers and the width of the fully connected layer. We\nestablish an approximation theory with explicit approximation rates when the\napproximated function takes a composite form $f\\circ Q$ with a feature\npolynomial $Q$ and a univariate function $f$. In particular, we prove that such\na network can outperform fully connected shallow networks in approximating\nradial functions with $Q(x) =|x|^2$, when the dimension $d$ of data from\n$\\mathbb{R}^d$ is large. This gives the first rigorous proof for the\nsuperiority of deep convolutional neural networks in approximating functions\nwith special structures. Then we carry out generalization analysis for\nempirical risk minimization with such a deep network in a regression framework\nwith the regression function of the form $f\\circ Q$. Our network structure\nwhich does not use any composite information or the functions $Q$ and $f$ can\nautomatically extract features and make use of the composite nature of the\nregression function via tuning the structural parameters. Our analysis provides\nan error bound which decreases with the network depth to a minimum and then\nincreases, verifying theoretically a trade-off phenomenon observed for network\ndepths in many practical applications.",
          "link": "http://arxiv.org/abs/2107.00896",
          "publishedOn": "2021-07-05T01:54:58.639Z",
          "wordCount": 653,
          "title": "Theory of Deep Convolutional Neural Networks III: Approximating Radial Functions. (arXiv:2107.00896v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The double descent curve is one of the most intriguing properties of deep\nneural networks. It contrasts the classical bias-variance curve with the\nbehavior of modern neural networks, occurring where the number of samples nears\nthe number of parameters. In this work, we explore the connection between the\ndouble descent phenomena and the number of samples in the deep neural network\nsetting. In particular, we propose a construction which augments the existing\ndataset by artificially increasing the number of samples. This construction\nempirically mitigates the double descent curve in this setting. We reproduce\nexisting work on deep double descent, and observe a smooth descent into the\noverparameterized region for our construction. This occurs both with respect to\nthe model size, and with respect to the number epochs.",
          "link": "http://arxiv.org/abs/2107.00797",
          "publishedOn": "2021-07-05T01:54:58.620Z",
          "wordCount": 553,
          "title": "Mitigating deep double descent by concatenating inputs. (arXiv:2107.00797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1\">Liren Shan</a>",
          "description": "We consider the problem of explainable $k$-medians and $k$-means introduced\nby Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,\nour goal is to find a \\emph{threshold decision tree} that partitions data into\n$k$ clusters and minimizes the $k$-medians or $k$-means objective. The obtained\nclustering is easy to interpret because every decision node of a threshold tree\nsplits data based on a single feature into two groups. We propose a new\nalgorithm for this problem which is $\\tilde O(\\log k)$ competitive with\n$k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means.\nThis is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by\nDasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{3/2}\nk)$ competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is\nnear-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$\nfor $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for\n$k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians\nwith $\\ell_2$ norm.",
          "link": "http://arxiv.org/abs/2107.00798",
          "publishedOn": "2021-07-05T01:54:58.600Z",
          "wordCount": 608,
          "title": "Near-optimal Algorithms for Explainable k-Medians and k-Means. (arXiv:2107.00798v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benato_L/0/1/0/all/0/1\">Lisa Benato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_E/0/1/0/all/0/1\">Erik Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdmann_M/0/1/0/all/0/1\">Martin Erdmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fackeldey_P/0/1/0/all/0/1\">Peter Fackeldey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glombitza_J/0/1/0/all/0/1\">Jonas Glombitza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_N/0/1/0/all/0/1\">Nikolai Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasieczka_G/0/1/0/all/0/1\">Gregor Kasieczka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korcari_W/0/1/0/all/0/1\">William Korcari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhr_T/0/1/0/all/0/1\">Thomas Kuhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinheimer_J/0/1/0/all/0/1\">Jan Steinheimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_H/0/1/0/all/0/1\">Horst St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plehn_T/0/1/0/all/0/1\">Tilman Plehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "We introduce a collection of datasets from fundamental physics research --\nincluding particle physics, astroparticle physics, and hadron- and nuclear\nphysics -- for supervised machine learning studies. These datasets, containing\nhadronic top quarks, cosmic-ray induced air showers, phase transitions in\nhadronic matter, and generator-level histories, are made public to simplify\nfuture work on cross-disciplinary machine learning and transfer learning in\nfundamental physics. Based on these data, we present a simple yet flexible\ngraph-based neural network architecture that can easily be applied to a wide\nrange of supervised learning tasks in these domains. We show that our approach\nreaches performance close to state-of-the-art dedicated methods on all\ndatasets. To simplify adaptation for various problems, we provide\neasy-to-follow instructions on how graph-based representations of data\nstructures, relevant for fundamental physics, can be constructed and provide\ncode implementations for several of them. Implementations are also provided for\nour proposed method and all reference algorithms.",
          "link": "http://arxiv.org/abs/2107.00656",
          "publishedOn": "2021-07-05T01:54:58.587Z",
          "wordCount": 645,
          "title": "Shared Data and Algorithms for Deep Learning in Fundamental Physics. (arXiv:2107.00656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00813",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Qiu_C/0/1/0/all/0/1\">Changxin Qiu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yan_J/0/1/0/all/0/1\">Jue Yan</a>",
          "description": "Motivated by finite volume scheme, a cell-average based neural network method\nis proposed. The method is based on the integral or weak formulation of partial\ndifferential equations. A simple feed forward network is forced to learn the\nsolution average evolution between two neighboring time steps. Offline\nsupervised training is carried out to obtain the optimal network parameter set,\nwhich uniquely identifies one finite volume like neural network method. Once\nwell trained, the network method is implemented as a finite volume scheme, thus\nis mesh dependent. Different to traditional numerical methods, our method can\nbe relieved from the explicit scheme CFL restriction and can adapt to any time\nstep size for solution evolution. For Heat equation, first order of convergence\nis observed and the errors are related to the spatial mesh size but are\nobserved independent of the mesh size in time. The cell-average based neural\nnetwork method can sharply evolve contact discontinuity with almost zero\nnumerical diffusion introduced. Shock and rarefaction waves are well captured\nfor nonlinear hyperbolic conservation laws.",
          "link": "http://arxiv.org/abs/2107.00813",
          "publishedOn": "2021-07-05T01:54:58.580Z",
          "wordCount": 608,
          "title": "Cell-average based neural network method for hyperbolic and parabolic partial differential equations. (arXiv:2107.00813v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:58.559Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Ani Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-05T01:54:58.546Z",
          "wordCount": 688,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zehao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S.Du</a>",
          "description": "As one of the most popular methods in the field of reinforcement learning,\nQ-learning has received increasing attention. Recently, there have been more\ntheoretical works on the regret bound of algorithms that belong to the\nQ-learning class in different settings. In this paper, we analyze the\ncumulative regret when conducting Nash Q-learning algorithm on 2-player\nturn-based stochastic Markov games (2-TBSG), and propose the very first gap\ndependent logarithmic upper bounds in the episodic tabular setting. This bound\nmatches the theoretical lower bound only up to a logarithmic term. Furthermore,\nwe extend the conclusion to the discounted game setting with infinite horizon\nand propose a similar gap dependent logarithmic regret bound. Also, under the\nlinear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both\ncentralized and independent settings.",
          "link": "http://arxiv.org/abs/2107.00685",
          "publishedOn": "2021-07-05T01:54:58.513Z",
          "wordCount": 564,
          "title": "Gap-Dependent Bounds for Two-Player Markov Games. (arXiv:2107.00685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>",
          "description": "The rich body of Bandit literature not only offers a diverse toolbox of\nalgorithms, but also makes it hard for a practitioner to find the right\nsolution to solve the problem at hand. Typical textbooks on Bandits focus on\ndesigning and analyzing algorithms, and surveys on applications often present a\nlist of individual applications. While these are valuable resources, there\nexists a gap in mapping applications to appropriate Bandit algorithms. In this\npaper, we aim to reduce this gap with a structured map of Bandits to help\npractitioners navigate to find relevant and practical Bandit algorithms.\nInstead of providing a comprehensive overview, we focus on a small number of\nkey decision points related to reward, action, and features, which often affect\nhow Bandit algorithms are chosen in practice.",
          "link": "http://arxiv.org/abs/2107.00680",
          "publishedOn": "2021-07-05T01:54:58.507Z",
          "wordCount": 560,
          "title": "A Map of Bandits for E-commerce. (arXiv:2107.00680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:58.500Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00839",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Delarue_F/0/1/0/all/0/1\">Fran&#xe7;ois Delarue</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vasileiadis_A/0/1/0/all/0/1\">Athanasios Vasileiadis</a>",
          "description": "The goal of this paper is to demonstrate that common noise may serve as an\nexploration noise for learning the solution of a mean field game. This concept\nis here exemplified through a toy linear-quadratic model, for which a suitable\nform of common noise has already been proven to restore existence and\nuniqueness. We here go one step further and prove that the same form of common\nnoise may force the convergence of the learning algorithm called `fictitious\nplay', and this without any further potential or monotone structure. Several\nnumerical examples are provided in order to support our theoretical analysis.",
          "link": "http://arxiv.org/abs/2107.00839",
          "publishedOn": "2021-07-05T01:54:58.491Z",
          "wordCount": 539,
          "title": "Exploration noise for learning linear-quadratic mean field games. (arXiv:2107.00839v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for robust DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach reduces the\nchance of overfitting, yields an unbiased prediction, and achieves\nstate-of-the-art performance in Davis and KIBA datasets. EnsembleDLM also\nreaches state-of-the-art performance in cross-domain applications and decent\ncross-domain performance (Pearson correlation coefficient and concordance index\n> 0.8) with transfer learning using approximately twice the amount of test data\nin the new domain.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-05T01:54:58.483Z",
          "wordCount": 560,
          "title": "Toward Robust Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddu_S/0/1/0/all/0/1\">Suryanarayana Maddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturm_D/0/1/0/all/0/1\">Dominik Sturm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%7Fuller_C/0/1/0/all/0/1\">Christian L. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbalzarini_I/0/1/0/all/0/1\">Ivo F. Sbalzarini</a>",
          "description": "We characterize and remedy a failure mode that may arise from multi-scale\ndynamics with scale imbalances during training of deep neural networks, such as\nPhysics Informed Neural Networks (PINNs). PINNs are popular machine-learning\ntemplates that allow for seamless integration of physical equation models with\ndata. Their training amounts to solving an optimization problem over a weighted\nsum of data-fidelity and equation-fidelity objectives. Conflicts between\nobjectives can arise from scale imbalances, heteroscedasticity in the data,\nstiffness of the physical equation, or from catastrophic interference during\nsequential training. We explain the training pathology arising from this and\npropose a simple yet effective inverse-Dirichlet weighting strategy to\nalleviate the issue. We compare with Sobolev training of neural networks,\nproviding the baseline of analytically $\\boldsymbol{\\epsilon}$-optimal\ntraining. We demonstrate the effectiveness of inverse-Dirichlet weighting in\nvarious applications, including a multi-scale model of active turbulence, where\nwe show orders of magnitude improvement in accuracy and convergence over\nconventional PINN training. For inverse modeling using sequential training, we\nfind that inverse-Dirichlet weighting protects a PINN against catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2107.00940",
          "publishedOn": "2021-07-05T01:54:58.464Z",
          "wordCount": 623,
          "title": "Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks. (arXiv:2107.00940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:58.452Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:58.445Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1\">Guy Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1\">Jane Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li-Yang Tan</a>",
          "description": "Greedy decision tree learning heuristics are mainstays of machine learning\npractice, but theoretical justification for their empirical success remains\nelusive. In fact, it has long been known that there are simple target functions\nfor which they fail badly (Kearns and Mansour, STOC 1996).\n\nRecent work of Brutzkus, Daniely, and Malach (COLT 2020) considered the\nsmoothed analysis model as a possible avenue towards resolving this disconnect.\nWithin the smoothed setting and for targets $f$ that are $k$-juntas, they\nshowed that these heuristics successfully learn $f$ with depth-$k$ decision\ntree hypotheses. They conjectured that the same guarantee holds more generally\nfor targets that are depth-$k$ decision trees.\n\nWe provide a counterexample to this conjecture: we construct targets that are\ndepth-$k$ decision trees and show that even in the smoothed setting, these\nheuristics build trees of depth $2^{\\Omega(k)}$ before achieving high accuracy.\nWe also show that the guarantees of Brutzkus et al. cannot extend to the\nagnostic setting: there are targets that are very close to $k$-juntas, for\nwhich these heuristics build trees of depth $2^{\\Omega(k)}$ before achieving\nhigh accuracy.",
          "link": "http://arxiv.org/abs/2107.00819",
          "publishedOn": "2021-07-05T01:54:58.439Z",
          "wordCount": 628,
          "title": "Decision tree heuristics can fail, even in the smoothed setting. (arXiv:2107.00819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-05T01:54:58.431Z",
          "wordCount": 710,
          "title": "The Causal Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Federated learning is promising for its ability to collaboratively train\nmodels with multiple clients without accessing their data, but vulnerable when\nclients' data distributions diverge from each other. This divergence further\nleads to a dilemma: \"Should we prioritize the learned model's generic\nperformance (for future use at the server) or its personalized performance (for\neach client)?\" These two, seemingly competing goals have divided the community\nto focus on one or the other, yet in this paper we show that it is possible to\napproach both at the same time. Concretely, we propose a novel federated\nlearning framework that explicitly decouples a model's dual duties with two\nprediction tasks. On the one hand, we introduce a family of losses that are\nrobust to non-identical class distributions, enabling clients to train a\ngeneric predictor with a consistent objective across them. On the other hand,\nwe formulate the personalized predictor as a lightweight adaptive module that\nis learned to minimize each client's empirical risk on top of the generic\npredictor. With this two-loss, two-predictor framework which we name Federated\nRobust Decoupling Fed-RoD, the learned model can simultaneously achieve\nstate-of-the-art generic and personalized performance, essentially bridging the\ntwo tasks.",
          "link": "http://arxiv.org/abs/2107.00778",
          "publishedOn": "2021-07-05T01:54:58.415Z",
          "wordCount": 622,
          "title": "On Bridging Generic and Personalized Federated Learning. (arXiv:2107.00778v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:58.409Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunzhuang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_A/0/1/0/all/0/1\">Andrew Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>",
          "description": "This paper proposes a novel primal heuristic for Mixed Integer Programs, by\nemploying machine learning techniques. Mixed Integer Programming is a general\ntechnique for formulating combinatorial optimization problems. Inside a solver,\nprimal heuristics play a critical role in finding good feasible solutions that\nenable one to tighten the duality gap from the outset of the Branch-and-Bound\nalgorithm (B&B), greatly improving its performance by pruning the B&B tree\naggressively. In this paper, we investigate whether effective primal heuristics\ncan be automatically learned via machine learning. We propose a new method to\nrepresent an optimization problem as a graph, and train a Graph Convolutional\nNetwork on solved problem instances with known optimal solutions. This in turn\ncan predict the values of decision variables in the optimal solution for an\nunseen problem instance of a similar type. The prediction of variable solutions\nis then leveraged by a novel configuration of the B&B method, Probabilistic\nBranching with guided Depth-first Search (PB-DFS) approach, aiming to find\n(near-)optimal solutions quickly. The experimental results show that this new\nheuristic can find better primal solutions at a much earlier stage of the\nsolving process, compared to other state-of-the-art primal heuristics.",
          "link": "http://arxiv.org/abs/2107.00866",
          "publishedOn": "2021-07-05T01:54:58.402Z",
          "wordCount": 633,
          "title": "Learning Primal Heuristics for Mixed Integer Programs. (arXiv:2107.00866v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hantao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>",
          "description": "Neural network based speech recognition systems suffer from performance\ndegradation due to accented speech, especially unfamiliar accents. In this\npaper, we study the supervised contrastive learning framework for accented\nspeech recognition. To build different views (similar \"positive\" data samples)\nfor contrastive learning, three data augmentation techniques including noise\ninjection, spectrogram augmentation and TTS-same-sentence generation are\nfurther investigated. From the experiments on the Common Voice dataset, we have\nshown that contrastive learning helps to build data-augmentation invariant and\npronunciation invariant representations, which significantly outperforms\ntraditional joint training methods in both zero-shot and full-shot settings.\nExperiments show that contrastive learning can improve accuracy by 3.66%\n(zero-shot) and 3.78% (full-shot) on average, comparing to the joint training\nmethod.",
          "link": "http://arxiv.org/abs/2107.00921",
          "publishedOn": "2021-07-05T01:54:58.391Z",
          "wordCount": 564,
          "title": "Supervised Contrastive Learning for Accented Speech Recognition. (arXiv:2107.00921v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaeemzadeh_A/0/1/0/all/0/1\">Alireza Zaeemzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "A reinforcement-learning-based non-uniform compressed sensing (NCS) framework\nfor time-varying signals is introduced. The proposed scheme, referred to as\nRL-NCS, aims to boost the performance of signal recovery through an optimal and\nadaptive distribution of sensing energy among two groups of coefficients of the\nsignal, referred to as the region of interest (ROI) coefficients and non-ROI\ncoefficients. The coefficients in ROI usually have greater importance and need\nto be reconstructed with higher accuracy compared to non-ROI coefficients. In\norder to accomplish this task, the ROI is predicted at each time step using two\nspecific approaches. One of these approaches incorporates a long short-term\nmemory (LSTM) network for the prediction. The other approach employs the\nprevious ROI information for predicting the next step ROI. Using the\nexploration-exploitation technique, a Q-network learns to choose the best\napproach for designing the measurement matrix. Furthermore, a joint loss\nfunction is introduced for the efficient training of the Q-network as well as\nthe LSTM network. The result indicates a significant performance gain for our\nproposed method, even for rapidly varying signals and a reduced number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.00838",
          "publishedOn": "2021-07-05T01:54:58.385Z",
          "wordCount": 625,
          "title": "RL-NCS: Reinforcement learning based data-driven approach for nonuniform compressed sensing. (arXiv:2107.00838v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:58.367Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00801",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "The ratio of two probability densities, called a density-ratio, is a vital\nquantity in machine learning. In particular, a relative density-ratio, which is\na bounded extension of the density-ratio, has received much attention due to\nits stability and has been used in various applications such as outlier\ndetection and dataset comparison. Existing methods for (relative) density-ratio\nestimation (DRE) require many instances from both densities. However,\nsufficient instances are often unavailable in practice. In this paper, we\npropose a meta-learning method for relative DRE, which estimates the relative\ndensity-ratio from a few instances by using knowledge in related datasets.\nSpecifically, given two datasets that consist of a few instances, our model\nextracts the datasets' information by using neural networks and uses it to\nobtain instance embeddings appropriate for the relative DRE. We model the\nrelative density-ratio by a linear model on the embedded space, whose global\noptimum solution can be obtained as a closed-form solution. The closed-form\nsolution enables fast and effective adaptation to a few instances, and its\ndifferentiability enables us to train our model such that the expected test\nerror for relative DRE can be explicitly minimized after adapting to a few\ninstances. We empirically demonstrate the effectiveness of the proposed method\nby using three problems: relative DRE, dataset comparison, and outlier\ndetection.",
          "link": "http://arxiv.org/abs/2107.00801",
          "publishedOn": "2021-07-05T01:54:58.361Z",
          "wordCount": 643,
          "title": "Meta-Learning for Relative Density-Ratio Estimation. (arXiv:2107.00801v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masrani_V/0/1/0/all/0/1\">Vaden Masrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1\">Rob Brekelmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Thang Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "Many common machine learning methods involve the geometric annealing path, a\nsequence of intermediate densities between two distributions of interest\nconstructed using the geometric average. While alternatives such as the\nmoment-averaging path have demonstrated performance gains in some settings,\ntheir practical applicability remains limited by exponential family endpoint\nassumptions and a lack of closed form energy function. In this work, we\nintroduce $q$-paths, a family of paths which is derived from a generalized\nnotion of the mean, includes the geometric and arithmetic mixtures as special\ncases, and admits a simple closed form involving the deformed logarithm\nfunction from nonextensive thermodynamics. Following previous analysis of the\ngeometric path, we interpret our $q$-paths as corresponding to a\n$q$-exponential family of distributions, and provide a variational\nrepresentation of intermediate densities as minimizing a mixture of\n$\\alpha$-divergences to the endpoints. We show that small deviations away from\nthe geometric path yield empirical gains for Bayesian inference using\nSequential Monte Carlo and generative model evaluation using Annealed\nImportance Sampling.",
          "link": "http://arxiv.org/abs/2107.00745",
          "publishedOn": "2021-07-05T01:54:58.355Z",
          "wordCount": 619,
          "title": "q-Paths: Generalizing the Geometric Annealing Path using Power Means. (arXiv:2107.00745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "We propose a few-shot learning method for unsupervised feature selection,\nwhich is a task to select a subset of relevant features in unlabeled data.\nExisting methods usually require many instances for feature selection. However,\nsufficient instances are often unavailable in practice. The proposed method can\nselect a subset of relevant features in a target task given a few unlabeled\ntarget instances by training with unlabeled instances in multiple source tasks.\nOur model consists of a feature selector and decoder. The feature selector\noutputs a subset of relevant features taking a few unlabeled instances as input\nsuch that the decoder can reconstruct the original features of unseen instances\nfrom the selected ones. The feature selector uses the Concrete random variables\nto select features via gradient descent. To encode task-specific properties\nfrom a few unlabeled instances to the model, the Concrete random variables and\ndecoder are modeled using permutation-invariant neural networks that take a few\nunlabeled instances as input. Our model is trained by minimizing the expected\ntest reconstruction error given a few unlabeled instances that is calculated\nwith datasets in source tasks. We experimentally demonstrate that the proposed\nmethod outperforms existing feature selection methods.",
          "link": "http://arxiv.org/abs/2107.00816",
          "publishedOn": "2021-07-05T01:54:58.349Z",
          "wordCount": 625,
          "title": "Few-shot Learning for Unsupervised Feature Selection. (arXiv:2107.00816v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00693",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_A/0/1/0/all/0/1\">Asiful Arefeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_A/0/1/0/all/0/1\">Ali Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafari_R/0/1/0/all/0/1\">Roozbeh Jafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirazi_B/0/1/0/all/0/1\">Behrooz A. Shirazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Inter-beat interval (IBI) measurement enables estimation of heart-rate\nvariability (HRV) which, in turns, can provide early indication of potential\ncardiovascular diseases. However, extracting IBIs from noisy signals is\nchallenging since the morphology of the signal is distorted in the presence of\nthe noise. Electrocardiogram (ECG) of a person in heavy motion is highly\ncorrupted with noise, known as motion-artifact, and IBI extracted from it is\ninaccurate. As a part of remote health monitoring and wearable system\ndevelopment, denoising ECG signals and estimating IBIs correctly from them have\nbecome an emerging topic among signal-processing researchers. Apart from\nconventional methods, deep-learning techniques have been successfully used in\nsignal denoising recently, and diagnosis process has become easier, leading to\naccuracy levels that were previously unachievable. We propose a deep-learning\napproach leveraging tiramisu autoencoder model to suppress motion-artifact\nnoise and make the R-peaks of the ECG signal prominent even in the presence of\nhigh-intensity motion. After denoising, IBIs are estimated more accurately\nexpediting diagnosis tasks. Results illustrate that our method enables IBI\nestimation from noisy ECG signals with SNR up to -30dB with average root mean\nsquare error (RMSE) of 13 milliseconds for estimated IBIs. At this noise level,\nour error percentage remains below 8% and outperforms other state of the art\ntechniques.",
          "link": "http://arxiv.org/abs/2107.00693",
          "publishedOn": "2021-07-05T01:54:58.342Z",
          "wordCount": 665,
          "title": "Inter-Beat Interval Estimation with Tiramisu Model: A Novel Approach with Reduced Error. (arXiv:2107.00693v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:58.325Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E%2E_A/0/1/0/all/0/1\">Amir Ghalamzan E.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellotto_N/0/1/0/all/0/1\">Nicola Bellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuay&#xe1;huitl</a>",
          "description": "Robots learning a new manipulation task from a small amount of demonstrations\nare increasingly demanded in different workspaces. A classifier model assessing\nthe quality of actions can predict the successful completion of a task, which\ncan be used by intelligent agents for action-selection. This paper presents a\nnovel classifier that learns to classify task completion only from a few\ndemonstrations. We carry out a comprehensive comparison of different neural\nclassifiers, e.g. fully connected-based, fully convolutional-based,\nsequence2sequence-based, and domain adaptation-based classification. We also\npresent a new dataset including five robot manipulation tasks, which is\npublicly available. We compared the performances of our novel classifier and\nthe existing models using our dataset and the MIME dataset. The results suggest\ndomain adaptation and timing-based features improve success prediction. Our\nnovel model, i.e. fully convolutional neural network with domain adaptation and\ntiming features, achieves an average classification accuracy of 97.3\\% and\n95.5\\% across tasks in both datasets whereas state-of-the-art classifiers\nwithout domain adaptation and timing-features only achieve 82.4\\% and 90.3\\%,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.00722",
          "publishedOn": "2021-07-05T01:54:58.319Z",
          "wordCount": 613,
          "title": "Neural Task Success Classifiers for Robotic Manipulation from Few Real Demonstrations. (arXiv:2107.00722v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautam&#xe4;ki</a>",
          "description": "Reinforcement learning (RL) research focuses on general solutions that can be\napplied across different domains. This results in methods that RL practitioners\ncan use in almost any domain. However, recent studies often lack the\nengineering steps (\"tricks\") which may be needed to effectively use RL, such as\nreward shaping, curriculum learning, and splitting a large task into smaller\nchunks. Such tricks are common, if not necessary, to achieve state-of-the-art\nresults and win RL competitions. To ease the engineering efforts, we distill\ndescriptions of tricks from state-of-the-art results and study how well these\ntricks can improve a standard deep Q-learning agent. The long-term goal of this\nwork is to enable combining proven RL methods with domain-specific tricks by\nproviding a unified software framework and accompanying insights in multiple\ndomains.",
          "link": "http://arxiv.org/abs/2107.00703",
          "publishedOn": "2021-07-05T01:54:58.308Z",
          "wordCount": 577,
          "title": "Distilling Reinforcement Learning Tricks for Video Games. (arXiv:2107.00703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Elia Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Francesco Silvestri</a>",
          "description": "A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it everywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these areas is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.",
          "link": "http://arxiv.org/abs/2107.00761",
          "publishedOn": "2021-07-05T01:54:58.297Z",
          "wordCount": 654,
          "title": "On the Bike Spreading Problem. (arXiv:2107.00761v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. All our algorithms run in near linear time in the number of\npoints and the dimension.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-05T01:54:58.290Z",
          "wordCount": 662,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altuner_A/0/1/0/all/0/1\">Anil Berk Altuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1\">Zeynep Hilal Kilimci</a>",
          "description": "Stock market prediction has been an important topic for investors,\nresearchers, and analysts. Because it is affected by too many factors, stock\nmarket prediction is a difficult task to handle. In this study, we propose a\nnovel method that is based on deep reinforcement learning methodologies for the\ndirection prediction of stocks using sentiments of community and knowledge\ngraph. For this purpose, we firstly construct a social knowledge graph of users\nby analyzing relations between connections. After that, time series analysis of\nrelated stock and sentiment analysis is blended with deep reinforcement\nmethodology. Turkish version of Bidirectional Encoder Representations from\nTransformers (BerTurk) is employed to analyze the sentiments of the users while\ndeep Q-learning methodology is used for the deep reinforcement learning side of\nthe proposed model to construct the deep Q network. In order to demonstrate the\neffectiveness of the proposed model, Garanti Bank (GARAN), Akbank (AKBNK),\nT\\\"urkiye \\.I\\c{s} Bankas{\\i} (ISCTR) stocks in Istanbul Stock Exchange are\nused as a case study. Experiment results show that the proposed novel model\nachieves remarkable results for stock market prediction task.",
          "link": "http://arxiv.org/abs/2107.00931",
          "publishedOn": "2021-07-05T01:54:58.273Z",
          "wordCount": 642,
          "title": "A Novel Deep Reinforcement Learning Based Stock Direction Prediction using Knowledge Graph and Community Aware Sentiments. (arXiv:2107.00931v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:58.262Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00734",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hsieh_C/0/1/0/all/0/1\">Chung-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_J/0/1/0/all/0/1\">Jiunn-Wei Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_K/0/1/0/all/0/1\">Kai-Feng Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>",
          "description": "Recent results have demonstrated that samplers constructed with flow-based\ngenerative models are a promising new approach for configuration generation in\nlattice field theory. In this paper, we present a set of methods to construct\nflow models for targets with multiple separated modes (i.e. theories with\nmultiple vacua). We demonstrate the application of these methods to modeling\ntwo-dimensional real scalar field theory in its symmetry-broken phase. In this\ncontext we investigate the performance of different flow-based sampling\nalgorithms, including a composite sampling algorithm where flow-based proposals\nare occasionally augmented by applying updates using traditional algorithms\nlike HMC.",
          "link": "http://arxiv.org/abs/2107.00734",
          "publishedOn": "2021-07-05T01:54:58.256Z",
          "wordCount": 563,
          "title": "Flow-based sampling for multimodal distributions in lattice field theory. (arXiv:2107.00734v1 [hep-lat])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1\">Tadayoshi Kohno</a>",
          "description": "When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.",
          "link": "http://arxiv.org/abs/2106.06654",
          "publishedOn": "2021-07-02T01:58:03.349Z",
          "wordCount": 544,
          "title": "Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-07-02T01:58:03.343Z",
          "wordCount": 668,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Deep learning-based techniques have been widely applied to the program\nanalysis tasks, in fields such as type inference, fault localization, and code\nsummarization. Hitherto deep learning-based software engineering systems rely\nthoroughly on supervised learning approaches, which require laborious manual\neffort to collect and label a prohibitively large amount of data. However, most\nTuring-complete imperative languages share similar control- and data-flow\nstructures, which make it possible to transfer knowledge learned from one\nlanguage to another. In this paper, we propose cross-lingual adaptation of\nprogram analysis, which allows us to leverage prior knowledge learned from the\nlabeled dataset of one language and transfer it to the others. Specifically, we\nimplemented a cross-lingual adaptation framework, PLATO, to transfer a deep\nlearning-based type inference procedure across weakly typed languages, e.g.,\nPython to JavaScript and vice versa. PLATO incorporates a novel joint graph\nkernelized attention based on abstract syntax tree and control flow graph, and\napplies anchor word augmentation across different languages. Besides, by\nleveraging data from strongly typed languages, PLATO improves the perplexity of\nthe backbone cross-programming-language model and the performance of downstream\ncross-lingual transfer for type inference. Experimental results illustrate that\nour framework significantly improves the transferability over the baseline\nmethod by a large margin.",
          "link": "http://arxiv.org/abs/2107.00157",
          "publishedOn": "2021-07-02T01:58:03.336Z",
          "wordCount": 634,
          "title": "Cross-Lingual Adaptation for Type Inference. (arXiv:2107.00157v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-07-02T01:58:03.328Z",
          "wordCount": 613,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-07-02T01:58:03.322Z",
          "wordCount": 617,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-07-02T01:58:03.314Z",
          "wordCount": 648,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.06605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halvani_O/0/1/0/all/0/1\">Oren Halvani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graner_L/0/1/0/all/0/1\">Lukas Graner</a>",
          "description": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.",
          "link": "http://arxiv.org/abs/2005.06605",
          "publishedOn": "2021-07-02T01:58:02.899Z",
          "wordCount": 724,
          "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis. (arXiv:2005.06605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:58:02.883Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07388",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1\">Slim Essid</a>",
          "description": "Through solving pretext tasks, self-supervised learning (SSL) leverages\nunlabeled data to extract useful latent representations replacing traditional\ninput features in the downstream task. A common pretext task consists in\npretraining a SSL model on pseudo-labels derived from the original signal. This\ntechnique is particularly relevant for speech data where various meaningful\nsignal processing features may serve as pseudo-labels. However, the process of\nselecting pseudo-labels, for speech or other types of data, remains mostly\nunexplored and currently relies on observing the results on the final\ndownstream task. Nevertheless, this methodology is not sustainable at scale due\nto substantial computational (hence carbon) costs. Thus, this paper introduces\na practical and theoretical framework to select relevant pseudo-labels with\nrespect to a given downstream task. More precisely, we propose a functional\nestimator of the pseudo-label utility grounded in the conditional independence\ntheory, which does not require any training. The experiments conducted on\nspeaker recognition and automatic speech recognition validate our estimator,\nshowing a significant correlation between the performance observed on the\ndownstream task and the utility estimates obtained with our approach,\nfacilitating the prospection of relevant pseudo-labels for self-supervised\nspeech representation learning.",
          "link": "http://arxiv.org/abs/2104.07388",
          "publishedOn": "2021-07-02T01:58:02.854Z",
          "wordCount": 665,
          "title": "Conditional independence for pretext task selection in Self-supervised speech representation learning. (arXiv:2104.07388v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansizoglou_I/0/1/0/all/0/1\">Ioannis Kansizoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bampis_L/0/1/0/all/0/1\">Loukas Bampis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasteratos_A/0/1/0/all/0/1\">Antonios Gasteratos</a>",
          "description": "One of the most prominent attributes of Neural Networks (NNs) constitutes\ntheir capability of learning to extract robust and descriptive features from\nhigh dimensional data, like images. Hence, such an ability renders their\nexploitation as feature extractors particularly frequent in an abundant of\nmodern reasoning systems. Their application scope mainly includes complex\ncascade tasks, like multi-modal recognition and deep Reinforcement Learning\n(RL). However, NNs induce implicit biases that are difficult to avoid or to\ndeal with and are not met in traditional image descriptors. Moreover, the lack\nof knowledge for describing the intra-layer properties -- and thus their\ngeneral behavior -- restricts the further applicability of the extracted\nfeatures. With the paper at hand, a novel way of visualizing and understanding\nthe vector space before the NNs' output layer is presented, aiming to enlighten\nthe deep feature vectors' properties under classification tasks. Main attention\nis paid to the nature of overfitting in the feature space and its adverse\neffect on further exploitation. We present the findings that can be derived\nfrom our model's formulation, and we evaluate them on realistic recognition\nscenarios, proving its prominence by improving the obtained results.",
          "link": "http://arxiv.org/abs/2007.00062",
          "publishedOn": "2021-07-02T01:58:02.845Z",
          "wordCount": 671,
          "title": "Deep Feature Space: A Geometrical Perspective. (arXiv:2007.00062v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-07-02T01:58:02.832Z",
          "wordCount": 626,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khuat_T/0/1/0/all/0/1\">Thanh Tung Khuat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrys_B/0/1/0/all/0/1\">Bogdan Gabrys</a>",
          "description": "This paper proposes a simple yet powerful ensemble classifier, called Random\nHyperboxes, constructed from individual hyperbox-based classifiers trained on\nthe random subsets of sample and feature spaces of the training set. We also\nshow a generalization error bound of the proposed classifier based on the\nstrength of the individual hyperbox-based classifiers as well as the\ncorrelation among them. The effectiveness of the proposed classifier is\nanalyzed using a carefully selected illustrative example and compared\nempirically with other popular single and ensemble classifiers via 20 datasets\nusing statistical testing methods. The experimental results confirmed that our\nproposed method outperformed other fuzzy min-max neural networks, popular\nlearning algorithms, and is competitive with other ensemble methods. Finally,\nwe identify the existing issues related to the generalization error bounds of\nthe real datasets and inform the potential research directions.",
          "link": "http://arxiv.org/abs/2006.00695",
          "publishedOn": "2021-07-02T01:58:02.817Z",
          "wordCount": 608,
          "title": "Random Hyperboxes. (arXiv:2006.00695v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almulla_H/0/1/0/all/0/1\">Hussein Almulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gay_G/0/1/0/all/0/1\">Gregory Gay</a>",
          "description": "Search-based test generation is guided by feedback from one or more fitness\nfunctions -- scoring functions that judge solution optimality. Choosing\ninformative fitness functions is crucial to meeting the goals of a tester.\nUnfortunately, many goals - such as forcing the class-under-test to throw\nexceptions, increasing test suite diversity, and attaining Strong Mutation\nCoverage - do not have effective fitness function formulations. We propose that\nmeeting such goals requires treating fitness function identification as a\nsecondary optimization step. An adaptive algorithm that can vary the selection\nof fitness functions could adjust its selection throughout the generation\nprocess to maximize goal attainment, based on the current population of test\nsuites. To test this hypothesis, we have implemented two reinforcement learning\nalgorithms in the EvoSuite unit test generation framework, and used these\nalgorithms to dynamically set the fitness functions used during generation for\nthe three goals identified above.\n\nWe have evaluated our framework, EvoSuiteFIT, on a set of Java case examples.\nEvoSuiteFIT techniques attain significant improvements for two of the three\ngoals, and show limited improvements on the third when the number of\ngenerations of evolution is fixed. Additionally, for two of the three goals,\nEvoSuiteFIT detects faults missed by the other techniques. The ability to\nadjust fitness functions allows strategic choices that efficiently produce more\neffective test suites, and examining these choices offers insight into how to\nattain our testing goals. We find that adaptive fitness function selection is a\npowerful technique to apply when an effective fitness function does not already\nexist for achieving a testing goal.",
          "link": "http://arxiv.org/abs/2102.04822",
          "publishedOn": "2021-07-02T01:58:02.797Z",
          "wordCount": 731,
          "title": "Learning How to Search: Generating Effective Test Cases Through Adaptive Fitness Function Selection. (arXiv:2102.04822v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_U/0/1/0/all/0/1\">Ujjal Kr Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repakula_S/0/1/0/all/0/1\">Sandeep Repakula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1\">Abhinav Ravi</a>",
          "description": "In this paper, we utilize deep visual Representation Learning to address an\nimportant problem in fashion e-commerce: color variants identification, i.e.,\nidentifying fashion products that match exactly in their design (or style), but\nonly to differ in their color. At first we attempt to tackle the problem by\nobtaining manual annotations (depicting whether two products are color\nvariants), and train a supervised triplet loss based neural network model to\nlearn representations of fashion products. However, for large scale real-world\nindustrial datasets such as addressed in our paper, it is infeasible to obtain\nannotations for the entire dataset, while capturing all the difficult corner\ncases. Interestingly, we observed that color variants are essentially\nmanifestations of color jitter based augmentations. Thus, we instead explore\nSelf-Supervised Learning (SSL) to solve this problem. We observed that existing\nstate-of-the-art SSL methods perform poor, for our problem. To address this, we\npropose a novel SSL based color variants model that simultaneously focuses on\ndifferent parts of an apparel. Quantitative and qualitative evaluation shows\nthat our method outperforms existing SSL methods, and at times, the supervised\nmodel.",
          "link": "http://arxiv.org/abs/2104.08581",
          "publishedOn": "2021-07-02T01:58:02.777Z",
          "wordCount": 663,
          "title": "Color Variants Identification in Fashion e-commerce via Contrastive Self-Supervised Representation Learning. (arXiv:2104.08581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haoyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Z/0/1/0/all/0/1\">Zhihao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Convolutional neural networks (CNNs) have been successfully used in a range\nof tasks. However, CNNs are often viewed as \"black-box\" and lack of\ninterpretability. One main reason is due to the filter-class entanglement -- an\nintricate many-to-many correspondence between filters and classes. Most\nexisting works attempt post-hoc interpretation on a pre-trained model, while\nneglecting to reduce the entanglement underlying the model. In contrast, we\nfocus on alleviating filter-class entanglement during training. Inspired by\ncellular differentiation, we propose a novel strategy to train interpretable\nCNNs by encouraging class-specific filters, among which each filter responds to\nonly one (or few) class. Concretely, we design a learnable sparse\nClass-Specific Gate (CSG) structure to assign each filter with one (or few)\nclass in a flexible way. The gate allows a filter's activation to pass only\nwhen the input samples come from the specific class. Extensive experiments\ndemonstrate the fabulous performance of our method in generating a sparse and\nhighly class-related representation of the input, which leads to stronger\ninterpretability. Moreover, comparing with the standard training strategy, our\nmodel displays benefits in applications like object localization and\nadversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.",
          "link": "http://arxiv.org/abs/2007.08194",
          "publishedOn": "2021-07-02T01:58:02.760Z",
          "wordCount": 693,
          "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters. (arXiv:2007.08194v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1\">Philippe Mary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1\">Visa Koivunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1\">Christophe Moy</a>",
          "description": "In this chapter, we will give comprehensive examples of applying RL in\noptimizing the physical layer of wireless communications by defining different\nclass of problems and the possible solutions to handle them. In Section 9.2, we\npresent all the basic theory needed to address a RL problem, i.e. Markov\ndecision process (MDP), Partially observable Markov decision process (POMDP),\nbut also two very important and widely used algorithms for RL, i.e. the\nQ-learning and SARSA algorithms. We also introduce the deep reinforcement\nlearning (DRL) paradigm and the section ends with an introduction to the\nmulti-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples\nto illustrate how the basic concepts of RL are employed in communication\nsystems. We present applications extracted from literature with simplified\nsystem models using similar notation as in Section 9.2 of this Chapter. In\nSection 9.3, we also focus on modeling RL problems, i.e. how action and state\nspaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a\nprospective thought on RL trends and it ends with a review of a broader state\nof the art in Section 9.5.",
          "link": "http://arxiv.org/abs/2106.11595",
          "publishedOn": "2021-07-02T01:58:02.753Z",
          "wordCount": 654,
          "title": "Reinforcement Learning for Physical Layer Communications. (arXiv:2106.11595v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07621",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Borghini_E/0/1/0/all/0/1\">Eugenio Borghini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fernandez_X/0/1/0/all/0/1\">Ximena Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Groisman_P/0/1/0/all/0/1\">Pablo Groisman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mindlin_G/0/1/0/all/0/1\">Gabriel Mindlin</a>",
          "description": "We address the problem of estimating intrinsic distances in a manifold from a\nfinite sample. We prove that the metric space defined by the sample endowed\nwith a computable metric known as sample Fermat distance converges a.s. in the\nsense of Gromov-Hausdorff. The limiting object is the manifold itself endowed\nwith the population Fermat distance, an intrinsic metric that accounts for both\nthe geometry of the manifold and the density that produces the sample. This\nresult is applied to obtain intrinsic persistence diagrams, which are less\nsensitive to the particular embedding of the manifold in the Euclidean space.\nWe show that this approach is robust to outliers and deduce a method for\npattern recognition in signals, with applications in real data.",
          "link": "http://arxiv.org/abs/2012.07621",
          "publishedOn": "2021-07-02T01:58:02.745Z",
          "wordCount": 606,
          "title": "Intrinsic persistent homology via density-based metric learning. (arXiv:2012.07621v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloesch_M/0/1/0/all/0/1\">Michael Bloesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1\">Jost Tobias Springenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1\">Abbas Abdolmaleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1\">Martin Riedmiller</a>",
          "description": "Despite the close connection between exploration and sample efficiency, most\nstate of the art reinforcement learning algorithms include no considerations\nfor exploration beyond maximizing the entropy of the policy. In this work we\naddress this seeming missed opportunity. We observe that the most common\nformulation of directed exploration in deep RL, known as bonus-based\nexploration (BBE), suffers from bias and slow coverage in the few-sample\nregime. This causes BBE to be actively detrimental to policy learning in many\ncontrol tasks. We show that by decoupling the task policy from the exploration\npolicy, directed exploration can be highly effective for sample-efficient\ncontinuous control. Our method, Decoupled Exploration and Exploitation Policies\n(DEEP), can be combined with any off-policy RL algorithm without modification.\nWhen used in conjunction with soft actor-critic, DEEP incurs no performance\npenalty in densely-rewarding environments. On sparse environments, DEEP gives a\nseveral-fold improvement in data efficiency due to better exploration.",
          "link": "http://arxiv.org/abs/2101.09458",
          "publishedOn": "2021-07-02T01:58:02.738Z",
          "wordCount": 621,
          "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. (arXiv:2101.09458v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1\">Andrea Apicella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1\">Francesco Isgr&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1\">Roberto Prevete</a>",
          "description": "Nowadays, it is growing interest to make Machine Learning (ML) systems more\nunderstandable and trusting to general users. Thus, generating explanations for\nML system behaviours that are understandable to human beings is a central\nscientific and technological issue addressed by the rapidly growing research\narea of eXplainable Artificial Intelligence (XAI). Recently, it is becoming\nmore and more evident that new directions to create better explanations should\ntake into account what a good explanation is to a human user, and consequently,\ndevelop XAI solutions able to provide user-centred explanations. This paper\nsuggests taking advantage of developing an XAI general approach that allows\nproducing explanations for an ML system behaviour in terms of different and\nuser-selected input features, i.e., explanations composed of input properties\nthat the human user can select according to his background knowledge and goals.\nTo this end, we propose an XAI general approach which is able: 1) to construct\nexplanations in terms of input features that represent more salient and\nunderstandable input properties for a user, which we call here Middle-Level\ninput Features (MLFs), 2) to be applied to different types of MLFs. We\nexperimentally tested our approach on two different datasets and using three\ndifferent types of MLFs. The results seem encouraging.",
          "link": "http://arxiv.org/abs/2106.05037",
          "publishedOn": "2021-07-02T01:58:02.718Z",
          "wordCount": 659,
          "title": "A general approach for Explanations in terms of Middle Level Features. (arXiv:2106.05037v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andersson_C/0/1/0/all/0/1\">Carl R. Andersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahlstrom_N/0/1/0/all/0/1\">Niklas Wahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>",
          "description": "We propose a model for hierarchical structured data as an extension to the\nstochastic temporal convolutional network. The proposed model combines an\nautoregressive model with a hierarchical variational autoencoder and\ndownsampling to achieve superior computational complexity. We evaluate the\nproposed model on two different types of sequential data: speech and\nhandwritten text. The results are promising with the proposed model achieving\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2104.13853",
          "publishedOn": "2021-07-02T01:58:02.712Z",
          "wordCount": 536,
          "title": "Learning deep autoregressive models for hierarchical data. (arXiv:2104.13853v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent, and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time\nshort-time human action recognition. Extensive experimentation on MPOSE2021\nwith our proposed methodology and several previous architectural solutions\nproves the effectiveness of the AcT model and poses the base for future work on\nHAR.",
          "link": "http://arxiv.org/abs/2107.00606",
          "publishedOn": "2021-07-02T01:58:02.704Z",
          "wordCount": 607,
          "title": "Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition. (arXiv:2107.00606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1\">Dominik D&#xfc;rrschnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyda_M/0/1/0/all/0/1\">Maren Koyda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1\">Gerd Stumme</a>",
          "description": "Formal Concept Analysis (FCA) allows to analyze binary data by deriving\nconcepts and ordering them in lattices. One of the main goals of FCA is to\nenable humans to comprehend the information that is encapsulated in the data;\nhowever, the large size of concept lattices is a limiting factor for the\nfeasibility of understanding the underlying structural properties. The size of\nsuch a lattice depends on the number of subcontexts in the corresponding formal\ncontext that are isomorphic to a contranominal scale of high dimension. In this\nwork, we propose the algorithm ContraFinder that enables the computation of all\ncontranominal scales of a given formal context. Leveraging this algorithm, we\nintroduce delta-adjusting, a novel approach in order to decrease the number of\ncontranominal scales in a formal context by the selection of an appropriate\nattribute subset. We demonstrate that delta-adjusting a context reduces the\nsize of the hereby emerging sub-semilattice and that the implication set is\nrestricted to meaningful implications. This is evaluated with respect to its\nassociated knowledge by means of a classification task. Hence, our proposed\ntechnique strongly improves understandability while preserving important\nconceptual structures.",
          "link": "http://arxiv.org/abs/2106.10978",
          "publishedOn": "2021-07-02T01:58:02.697Z",
          "wordCount": 652,
          "title": "Attribute Selection using Contranominal Scales. (arXiv:2106.10978v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>",
          "description": "Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}",
          "link": "http://arxiv.org/abs/2101.03988",
          "publishedOn": "2021-07-02T01:58:02.691Z",
          "wordCount": 619,
          "title": "Identification of COVID-19 related Fake News via Neural Stacking. (arXiv:2101.03988v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>",
          "description": "Many complex multi-robot systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.",
          "link": "http://arxiv.org/abs/2102.03479",
          "publishedOn": "2021-07-02T01:58:02.672Z",
          "wordCount": 722,
          "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v12 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_K/0/1/0/all/0/1\">Kumar Vijay Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Ashok Kumar</a>",
          "description": "We examine the role of information geometry in the context of classical\nCram\\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory\nof obtaining dualistic geometric structures from a divergence function and then\napplying Amari-Nagoaka's theory to obtain a CR type inequality. The classical\ndeterministic CR inequality is derived from Kullback-Leibler (KL)-divergence.\nWe show that this framework could be generalized to other CR type inequalities\nthrough four examples: $\\alpha$-version of CR inequality, generalized CR\ninequality, Bayesian CR inequality, and Bayesian $\\alpha$-CR inequality. These\nare obtained from, respectively, $I_\\alpha$-divergence (or relative\n$\\alpha$-entropy), generalized Csisz\\'ar divergence, Bayesian KL divergence,\nand Bayesian $I_\\alpha$-divergence.",
          "link": "http://arxiv.org/abs/2104.01061",
          "publishedOn": "2021-07-02T01:58:02.664Z",
          "wordCount": 591,
          "title": "Information Geometry and Classical Cram\\'{e}r-Rao Type Inequalities. (arXiv:2104.01061v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11918",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ren_Z/0/1/0/all/0/1\">Zhimei Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "We study the problem of dynamic batch learning in high-dimensional sparse\nlinear contextual bandits, where a decision maker can only adapt decisions at a\nbatch level. In particular, the decision maker, only observing rewards at the\nend of each batch, dynamically decides how many individuals to include in the\nnext batch (at the current batch's end) and what personalized action-selection\nscheme to adopt within the batch. Such batch constraints are ubiquitous in a\nvariety of practical contexts, including personalized product offerings in\nmarketing and medical treatment selection in clinical trials. We characterize\nthe fundamental learning limit in this problem via a novel lower bound analysis\nand provide a simple, exploration-free algorithm that uses the LASSO estimator,\nwhich achieves the minimax optimal performance characterized by the lower bound\n(up to log factors). To our best knowledge, our work provides the first inroad\ninto a rigorous understanding of dynamic batch learning with high-dimensional\ncovariates. We also demonstrate the efficacy of our algorithm on both synthetic\ndata and the Warfarin medical dosing data. The empirical results show that with\nthree batches (hence only two opportunities to adapt), our algorithm already\nperforms comparably (in terms of statistical performance) to the\nstate-of-the-art fully online high-dimensional linear contextual bandits\nalgorithm. As an added bonus, since our algorithm operates in batches, it is\norders of magnitudes faster than fully online learning algorithms. As such, our\nalgorithm provides a desirable candidate for practical data-driven personalized\ndecision making problems, where limited adaptivity is often a hard constraint.",
          "link": "http://arxiv.org/abs/2008.11918",
          "publishedOn": "2021-07-02T01:58:02.654Z",
          "wordCount": 717,
          "title": "Dynamic Batch Learning in High-Dimensional Sparse Linear Contextual Bandits. (arXiv:2008.11918v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_J/0/1/0/all/0/1\">Janne H. Korhonen</a>",
          "description": "We consider a standard distributed optimisation setting where $N$ machines,\neach holding a $d$-dimensional function $f_i$, aim to jointly minimise the sum\nof the functions $\\sum_{i = 1}^N f_i (x)$. This problem arises naturally in\nlarge-scale distributed optimisation, where a standard solution is to apply\nvariants of (stochastic) gradient descent. We focus on the communication\ncomplexity of this problem: our main result provides the first fully\nunconditional bounds on total number of bits which need to be sent and received\nby the $N$ machines to solve this problem under point-to-point communication,\nwithin a given error-tolerance. Specifically, we show that $\\Omega( Nd \\log d /\nN\\varepsilon)$ total bits need to be communicated between the machines to find\nan additive $\\epsilon$-approximation to the minimum of $\\sum_{i = 1}^N f_i\n(x)$. The result holds for both deterministic and randomised algorithms, and,\nimportantly, requires no assumptions on the algorithm structure. The lower\nbound is tight under certain restrictions on parameter values, and is matched\nwithin constant factors for quadratic objectives by a new variant of quantised\ngradient descent, which we describe and analyse. Our results bring over tools\nfrom communication complexity to distributed optimisation, which has potential\nfor further applications.",
          "link": "http://arxiv.org/abs/2010.08222",
          "publishedOn": "2021-07-02T01:58:02.647Z",
          "wordCount": 665,
          "title": "Towards Tight Communication Lower Bounds for Distributed Optimisation. (arXiv:2010.08222v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinchi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1\">Javier Fernandez-Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro Porto Buarque de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1\">Daniel J. Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topal_T/0/1/0/all/0/1\">Taner Topal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Akhil Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>",
          "description": "Despite impressive results, deep learning-based technologies also raise\nsevere privacy and environmental concerns induced by the training procedure\noften conducted in datacenters. In response, alternatives to centralized\ntraining such as Federated Learning (FL) have emerged. Perhaps unexpectedly,\nFL, in particular, is starting to be deployed at a global scale by companies\nthat must adhere to new legal demands and policies originating from governments\nand civil society for privacy protection. However, the potential environmental\nimpact related to FL remains unclear and unexplored. This paper offers the\nfirst-ever systematic study of the carbon footprint of FL. First, we propose a\nrigorous model to quantify the carbon footprint, hence facilitating the\ninvestigation of the relationship between FL design and carbon emissions. Then,\nwe compare the carbon footprint of FL to traditional centralized learning. Our\nfindings show that FL, despite being slower to converge in some cases, may\nresult in a comparatively greener impact than a centralized equivalent setup.\nWe performed extensive experiments across different types of datasets,\nsettings, and various deep learning models with FL. Finally, we highlight and\nconnect the reported results to the future challenges and trends in FL to\nreduce its environmental impact, including algorithms efficiency, hardware\ncapabilities, and stronger industry transparency.",
          "link": "http://arxiv.org/abs/2102.07627",
          "publishedOn": "2021-07-02T01:58:02.640Z",
          "wordCount": 707,
          "title": "A first look into the carbon footprint of federated learning. (arXiv:2102.07627v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_M/0/1/0/all/0/1\">Mudit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Shreyans Mehta</a>",
          "description": "Social media platforms such as Twitter, Facebook etc can be utilised as an\nimportant source of information during disaster events. This information can be\nused for disaster response and crisis management if processed accurately and\nquickly. However, the data present in such situations is ever-changing, and\nusing considerable resources during such a crisis is not feasible. Therefore,\nwe have to develop a low resource and continually learning system that\nincorporates text classification models which are robust against noisy and\nunordered data. We utilised Distributed learning which enabled us to learn on\nresource-constrained devices, then to alleviate catastrophic forgetting in our\ntarget neural networks we utilized regularization. We then applied federated\naveraging for distributed learning and to aggregate the central model for\ncontinual learning.",
          "link": "http://arxiv.org/abs/2104.12876",
          "publishedOn": "2021-07-02T01:58:02.620Z",
          "wordCount": 604,
          "title": "Continual Distributed Learning for Crisis Management. (arXiv:2104.12876v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuxiong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Hongming Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>",
          "description": "It is hard to directly implement Graph Neural Networks (GNNs) on large scaled\ngraphs. Besides of existed neighbor sampling techniques, scalable methods\ndecoupling graph convolutions and other learnable transformations into\npreprocessing and post classifier allow normal minibatch training. By replacing\nredundant concatenation operation with attention mechanism in SIGN, we propose\nScalable and Adaptive Graph Neural Networks (SAGN). SAGN can adaptively gather\nneighborhood information among different hops. To further improve scalable\nmodels on semi-supervised learning tasks, we propose Self-Label-Enhance (SLE)\nframework combining self-training approach and label propagation in depth. We\nadd base model with a scalable node label module. Then we iteratively train\nmodels and enhance train set in several stages. To generate input of node label\nmodule, we directly apply label propagation based on one-hot encoded label\nvectors without inner random masking. We find out that empirically the label\nleakage has been effectively alleviated after graph convolutions. The hard\npseudo labels in enhanced train set participate in label propagation with true\nlabels. Experiments on both inductive and transductive datasets demonstrate\nthat, compared with other sampling-based and sampling-free methods, SAGN\nachieves better or comparable results and SLE can further improve performance.",
          "link": "http://arxiv.org/abs/2104.09376",
          "publishedOn": "2021-07-02T01:58:02.614Z",
          "wordCount": 666,
          "title": "Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced training. (arXiv:2104.09376v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nellikkath_R/0/1/0/all/0/1\">Rahul Nellikkath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1\">Spyros Chatzivasileiadis</a>",
          "description": "Physics-informed neural networks exploit the existing models of the\nunderlying physical systems to generate higher accuracy results with fewer\ndata. Such approaches can help drastically reduce the computation time and\ngenerate a good estimate of computationally intensive processes in power\nsystems, such as dynamic security assessment or optimal power flow. Combined\nwith the extraction of worst-case guarantees for the neural network\nperformance, such neural networks can be applied in safety-critical\napplications in power systems and build a high level of trust among power\nsystem operators. This paper takes the first step and applies, for the first\ntime to our knowledge, Physics-Informed Neural Networks with Worst-Case\nGuarantees for the DC Optimal Power Flow problem. We look for guarantees\nrelated to (i) maximum constraint violations, (ii) maximum distance between\npredicted and optimal decision variables, and (iii) maximum sub-optimality in\nthe entire input domain. In a range of PGLib-OPF networks, we demonstrate how\nphysics-informed neural networks can be supplied with worst-case guarantees and\nhow they can lead to reduced worst-case violations compared with conventional\nneural networks.",
          "link": "http://arxiv.org/abs/2107.00465",
          "publishedOn": "2021-07-02T01:58:02.607Z",
          "wordCount": 632,
          "title": "Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC Optimal Power Flow. (arXiv:2107.00465v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14196",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1\">Seyed Mohsen Hosseini</a>",
          "description": "A novel method for feature fusion in convolutional neural networks is\nproposed in this paper. Different feature fusion techniques are suggested to\nfacilitate the flow of information and improve the training of deep neural\nnetworks. Some of these techniques as well as the proposed network can be\nconsidered a type of Directed Acyclic Graph (DAG) Network, where a layer can\nreceive inputs from other layers and have outputs to other layers. In the\nproposed general framework of Lattice Fusion Network (LFNet), feature maps of\neach convolutional layer are passed to other layers based on a lattice graph\nstructure, where nodes are convolutional layers. To evaluate the performance of\nthe proposed architecture, different designs based on the general framework of\nLFNet are implemented for the task of image denoising. This task is used as an\nexample where training deep convolutional networks is needed. Results are\ncompared with state of the art methods. The proposed network is able to achieve\nbetter results with far fewer learnable parameters, which shows the\neffectiveness of LFNets for training of deep neural networks.",
          "link": "http://arxiv.org/abs/2011.14196",
          "publishedOn": "2021-07-02T01:58:02.601Z",
          "wordCount": 643,
          "title": "Lattice Fusion Networks for Image Denoising. (arXiv:2011.14196v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arindam Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayer_A/0/1/0/all/0/1\">Artun Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1\">Santiago Segarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The graph convolutional network (GCN) is a go-to solution for machine\nlearning on graphs, but its training is notoriously difficult to scale both in\nterms of graph size and the number of model parameters. Although some work has\nexplored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we\npioneer efficient training of large-scale GCN models (i.e., ultra-wide,\noverparameterized models) with the proposal of a novel, distributed training\nframework. Our proposed training methodology, called GIST, disjointly\npartitions the parameters of a GCN model into several, smaller sub-GCNs that\nare trained independently and in parallel. In addition to being compatible with\nany GCN architecture, GIST improves model performance, scales to training on\narbitrarily large graphs, significantly decreases wall-clock training time, and\nenables the training of markedly overparameterized GCN models. Remarkably, with\nGIST, we train an astonishgly-wide 32,768-dimensional GraphSAGE model, which\nexceeds the capacity of a single GPU by a factor of 8X, to SOTA performance on\nthe Amazon2M dataset.",
          "link": "http://arxiv.org/abs/2102.10424",
          "publishedOn": "2021-07-02T01:58:02.595Z",
          "wordCount": 662,
          "title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks. (arXiv:2102.10424v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.2 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-02T01:58:02.576Z",
          "wordCount": 721,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:58:02.558Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bynum_L/0/1/0/all/0/1\">Lucius E.J. Bynum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1\">Joshua R. Loftus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1\">Julia Stoyanovich</a>",
          "description": "A significant body of research in the data sciences considers unfair\ndiscrimination against social categories such as race or gender that could\noccur or be amplified as a result of algorithmic decisions. Simultaneously,\nreal-world disparities continue to exist, even before algorithmic decisions are\nmade. In this work, we draw on insights from the social sciences and humanistic\nstudies brought into the realm of causal modeling and constrained optimization,\nand develop a novel algorithmic framework for tackling pre-existing real-world\ndisparities. The purpose of our framework, which we call the \"impact\nremediation framework,\" is to measure real-world disparities and discover the\noptimal intervention policies that could help improve equity or access to\nopportunity for those who are underserved with respect to an outcome of\ninterest. We develop a disaggregated approach to tackling pre-existing\ndisparities that relaxes the typical set of assumptions required for the use of\nsocial categories in structural causal models. Our approach flexibly\nincorporates counterfactuals and is compatible with various ontological\nassumptions about the nature of social categories. We demonstrate impact\nremediation with a real-world case study and compare our disaggregated approach\nto an existing state-of-the-art approach, comparing its structure and resulting\npolicy recommendations. In contrast to most work on optimal policy learning, we\nexplore disparity reduction itself as an objective, explicitly focusing the\npower of algorithms on reducing inequality.",
          "link": "http://arxiv.org/abs/2107.00593",
          "publishedOn": "2021-07-02T01:58:02.552Z",
          "wordCount": 664,
          "title": "Impact Remediation: Optimal Interventions to Reduce Inequality. (arXiv:2107.00593v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03534",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gautier_R/0/1/0/all/0/1\">Raphael Gautier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pandita_P/0/1/0/all/0/1\">Piyush Pandita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mavris_D/0/1/0/all/0/1\">Dimitri Mavris</a>",
          "description": "Modern day engineering problems are ubiquitously characterized by\nsophisticated computer codes that map parameters or inputs to an underlying\nphysical process. In other situations, experimental setups are used to model\nthe physical process in a laboratory, ensuring high precision while being\ncostly in materials and logistics. In both scenarios, only limited amount of\ndata can be generated by querying the expensive information source at a finite\nnumber of inputs or designs. This problem is compounded further in the presence\nof a high-dimensional input space. State-of-the-art parameter space dimension\nreduction methods, such as active subspace, aim to identify a subspace of the\noriginal input space that is sufficient to explain the output response. These\nmethods are restricted by their reliance on gradient evaluations or copious\ndata, making them inadequate to expensive problems without direct access to\ngradients. The proposed methodology is gradient-free and fully Bayesian, as it\nquantifies uncertainty in both the low-dimensional subspace and the surrogate\nmodel parameters. This enables a full quantification of epistemic uncertainty\nand robustness to limited data availability. It is validated on multiple\ndatasets from engineering and science and compared to two other\nstate-of-the-art methods based on four aspects: a) recovery of the active\nsubspace, b) deterministic prediction accuracy, c) probabilistic prediction\naccuracy, and d) training time. The comparison shows that the proposed method\nimproves the active subspace recovery and predictive accuracy, in both the\ndeterministic and probabilistic sense, when only few model observations are\navailable for training, at the cost of increased training time.",
          "link": "http://arxiv.org/abs/2008.03534",
          "publishedOn": "2021-07-02T01:58:02.543Z",
          "wordCount": 706,
          "title": "A Fully Bayesian Gradient-Free Supervised Dimension Reduction Method using Gaussian Processes. (arXiv:2008.03534v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>",
          "description": "Differentiable particle filters provide a flexible mechanism to adaptively\ntrain dynamic and measurement models by learning from observed data. However,\nmost existing differentiable particle filters are within the bootstrap particle\nfiltering framework and fail to incorporate the information from latest\nobservations to construct better proposals. In this paper, we utilize\nconditional normalizing flows to construct proposal distributions for\ndifferentiable particle filters, enriching the distribution families that the\nproposal distributions can represent. In addition, normalizing flows are\nincorporated in the construction of the dynamic model, resulting in a more\nexpressive dynamic model. We demonstrate the performance of the proposed\nconditional normalizing flow-based differentiable particle filters in a visual\ntracking task.",
          "link": "http://arxiv.org/abs/2107.00488",
          "publishedOn": "2021-07-02T01:58:02.535Z",
          "wordCount": 544,
          "title": "Differentiable Particle Filters through Conditional Normalizing Flow. (arXiv:2107.00488v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00472",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhou_B/0/1/0/all/0/1\">Baojian Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>",
          "description": "In this paper, we propose approximate Frank-Wolfe (FW) algorithms to solve\nconvex optimization problems over graph-structured support sets where the\n\\textit{linear minimization oracle} (LMO) cannot be efficiently obtained in\ngeneral. We first demonstrate that two popular approximation assumptions\n(\\textit{additive} and \\textit{multiplicative gap errors)}, are not valid for\nour problem, in that no cheap gap-approximate LMO oracle exists in general.\nInstead, a new \\textit{approximate dual maximization oracle} (DMO) is proposed,\nwhich approximates the inner product rather than the gap. When the objective is\n$L$-smooth, we prove that the standard FW method using a $\\delta$-approximate\nDMO converges as $\\mathcal{O}(L / \\delta t + (1-\\delta)(\\delta^{-1} +\n\\delta^{-2}))$ in general, and as $\\mathcal{O}(L/(\\delta^2(t+2)))$ over a\n$\\delta$-relaxation of the constraint set. Additionally, when the objective is\n$\\mu$-strongly convex and the solution is unique, a variant of FW converges to\n$\\mathcal{O}(L^2\\log(t)/(\\mu \\delta^6 t^2))$ with the same per-iteration\ncomplexity. Our empirical results suggest that even these improved bounds are\npessimistic, with significant improvement in recovering real-world images with\ngraph-structured sparsity.",
          "link": "http://arxiv.org/abs/2107.00472",
          "publishedOn": "2021-07-02T01:58:02.517Z",
          "wordCount": 598,
          "title": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets. (arXiv:2107.00472v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natali_L/0/1/0/all/0/1\">Laura Natali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamialahmadi_O/0/1/0/all/0/1\">Oveis Jamialahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Stefano Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1\">Joana B. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpe_G/0/1/0/all/0/1\">Giovanni Volpe</a>",
          "description": "Neural network training and validation rely on the availability of large\nhigh-quality datasets. However, in many cases only incomplete datasets are\navailable, particularly in health care applications, where each patient\ntypically undergoes different clinical procedures or can drop out of a study.\nSince the data to train the neural networks need to be complete, most studies\ndiscard the incomplete datapoints, which reduces the size of the training data,\nor impute the missing features, which can lead to artefacts. Alas, both\napproaches are inadequate when a large portion of the data is missing. Here, we\nintroduce GapNet, an alternative deep-learning training approach that can use\nhighly incomplete datasets. First, the dataset is split into subsets of samples\ncontaining all values for a certain cluster of features. Then, these subsets\nare used to train individual neural networks. Finally, this ensemble of neural\nnetworks is combined into a single neural network whose training is fine-tuned\nusing all complete datapoints. Using two highly incomplete real-world medical\ndatasets, we show that GapNet improves the identification of patients with\nunderlying Alzheimer's disease pathology and of patients at risk of\nhospitalization due to Covid-19. By distilling the information available in\nincomplete datasets without having to reduce their size or to impute missing\nvalues, GapNet will permit to extract valuable information from a wide range of\ndatasets, benefiting diverse fields from medicine to engineering.",
          "link": "http://arxiv.org/abs/2107.00429",
          "publishedOn": "2021-07-02T01:58:02.509Z",
          "wordCount": 721,
          "title": "Neural Network Training with Highly Incomplete Datasets. (arXiv:2107.00429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>",
          "description": "The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and learning\nefficiency of downstream tasks in scenes with multiple objects. In this paper,\nwe train state-of-the-art unsupervised models on five common multi-object\ndatasets and evaluate segmentation accuracy and downstream object property\nprediction. In addition, we study systematic generalization and robustness by\ninvestigating the settings where either single objects are out-of-distribution\n-- e.g., having unseen colors, textures, and shapes -- or global properties of\nthe scene are altered -- e.g., by occlusions, cropping, or increasing the\nnumber of objects. From our experimental study, we find object-centric\nrepresentations to be generally useful for downstream tasks and robust to\nshifts in the data distribution, especially if shifts affect single objects.",
          "link": "http://arxiv.org/abs/2107.00637",
          "publishedOn": "2021-07-02T01:58:02.501Z",
          "wordCount": 593,
          "title": "Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00352",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Recently, sampling methods have been successfully applied to enhance the\nsample quality of Generative Adversarial Networks (GANs). However, in practice,\nthey typically have poor sample efficiency because of the independent proposal\nsampling from the generator. In this work, we propose REP-GAN, a novel sampling\nmethod that allows general dependent proposals by REParameterizing the Markov\nchains into the latent space of the generator. Theoretically, we show that our\nreparameterized proposal admits a closed-form Metropolis-Hastings acceptance\nratio. Empirically, extensive experiments on synthetic and real datasets\ndemonstrate that our REP-GAN largely improves the sample efficiency and obtains\nbetter sample quality simultaneously.",
          "link": "http://arxiv.org/abs/2107.00352",
          "publishedOn": "2021-07-02T01:58:02.494Z",
          "wordCount": 532,
          "title": "Reparameterized Sampling for Generative Adversarial Networks. (arXiv:2107.00352v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00421",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Charalambous_T/0/1/0/all/0/1\">Themistoklis Charalambous</a>",
          "description": "The repetitive tracking task for time-varying systems (TVSs) with\nnon-repetitive time-varying parameters, which is also called non-repetitive\nTVSs, is realized in this paper using iterative learning control (ILC). A\nmachine learning (ML) based nominal model update mechanism, which utilizes the\nlinear regression technique to update the nominal model at each ILC trial only\nusing the current trial information, is proposed for non-repetitive TVSs in\norder to enhance the ILC performance. Given that the ML mechanism forces the\nmodel uncertainties to remain within the ILC robust tolerance, an ILC update\nlaw is proposed to deal with non-repetitive TVSs. How to tune parameters inside\nML and ILC algorithms to achieve the desired aggregate performance is also\nprovided. The robustness and reliability of the proposed method are verified by\nsimulations. Comparison with current state-of-the-art demonstrates its superior\ncontrol performance in terms of controlling precision. This paper broadens ILC\napplications from time-invariant systems to non-repetitive TVSs, adopts ML\nregression technique to estimate non-repetitive time-varying parameters between\ntwo ILC trials and proposes a detailed parameter tuning mechanism to achieve\ndesired performance, which are the main contributions.",
          "link": "http://arxiv.org/abs/2107.00421",
          "publishedOn": "2021-07-02T01:58:02.483Z",
          "wordCount": 624,
          "title": "Machine learning based iterative learning control for non-repetitive time-varying systems. (arXiv:2107.00421v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wanlu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Ming Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1\">Mikael Skoglund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>",
          "description": "Edge computing provides a promising paradigm to support the implementation of\nIndustrial Internet of Things (IIoT) by offloading tasks to nearby edge nodes.\nMeanwhile, the increasing network size makes it impractical for centralized\ndata processing due to limited bandwidth, and consequently a decentralized\nlearning scheme is preferable. Reinforcement learning (RL) has been widely\ninvestigated and shown to be a promising solution for decision-making and\noptimal control processes. For RL in a decentralized setup, edge nodes (agents)\nconnected through a communication network aim to work collaboratively to find a\npolicy to optimize the global reward as the sum of local rewards. However,\ncommunication costs, scalability and adaptation in complex environments with\nheterogeneous agents may significantly limit the performance of decentralized\nRL. Alternating direction method of multipliers (ADMM) has a structure that\nallows for decentralized implementation, and has shown faster convergence than\ngradient descent based methods. Therefore, we propose an adaptive stochastic\nincremental ADMM (asI-ADMM) algorithm and apply the asI-ADMM to decentralized\nRL with edge-computing-empowered IIoT networks. We provide convergence\nproperties for proposed algorithms by designing a Lyapunov function and prove\nthat the asI-ADMM has $O(\\frac{1}{k}) +O(\\frac{1}{M})$ convergence rate where\n$k$ and $ M$ are the number of iterations and batch samples, respectively.\nThen, we test our algorithm with two supervised learning problems. For\nperformance evaluation, we simulate two applications in decentralized RL\nsettings with homogeneous and heterogeneous agents. The experiment results show\nthat our proposed algorithms outperform the state of the art in terms of\ncommunication costs and scalability, and can well adapt to complex IoT\nenvironments.",
          "link": "http://arxiv.org/abs/2107.00481",
          "publishedOn": "2021-07-02T01:58:02.466Z",
          "wordCount": 701,
          "title": "Adaptive Stochastic ADMM for Decentralized Reinforcement Learning in Edge Industrial IoT. (arXiv:2107.00481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kingma_D/0/1/0/all/0/1\">Diederik P. Kingma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Ben Poole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>",
          "description": "Diffusion-based generative models have demonstrated a capacity for\nperceptually impressive synthesis, but can they also be great likelihood-based\nmodels? We answer this in the affirmative, and introduce a family of\ndiffusion-based generative models that obtain state-of-the-art likelihoods on\nstandard image density estimation benchmarks. Unlike other diffusion-based\nmodels, our method allows for efficient optimization of the noise schedule\njointly with the rest of the model. We show that the variational lower bound\n(VLB) simplifies to a remarkably short expression in terms of the\nsignal-to-noise ratio of the diffused data, thereby improving our theoretical\nunderstanding of this model class. Using this insight, we prove an equivalence\nbetween several models proposed in the literature. In addition, we show that\nthe continuous-time VLB is invariant to the noise schedule, except for the\nsignal-to-noise ratio at its endpoints. This enables us to learn a noise\nschedule that minimizes the variance of the resulting VLB estimator, leading to\nfaster optimization. Combining these advances with architectural improvements,\nwe obtain state-of-the-art likelihoods on image density estimation benchmarks,\noutperforming autoregressive models that have dominated these benchmarks for\nmany years, with often significantly faster optimization. In addition, we show\nhow to turn the model into a bits-back compression scheme, and demonstrate\nlossless compression rates close to the theoretical optimum.",
          "link": "http://arxiv.org/abs/2107.00630",
          "publishedOn": "2021-07-02T01:58:02.457Z",
          "wordCount": 636,
          "title": "Variational Diffusion Models. (arXiv:2107.00630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telgarsky_M/0/1/0/all/0/1\">Matus Telgarsky</a>",
          "description": "We present and analyze a momentum-based gradient method for training linear\nclassifiers with an exponentially-tailed loss (e.g., the exponential or\nlogistic loss), which maximizes the classification margin on separable data at\na rate of $\\widetilde{\\mathcal{O}}(1/t^2)$. This contrasts with a rate of\n$\\mathcal{O}(1/\\log(t))$ for standard gradient descent, and $\\mathcal{O}(1/t)$\nfor normalized gradient descent. This momentum-based method is derived via the\nconvex dual of the maximum-margin problem, and specifically by applying\nNesterov acceleration to this dual, which manages to result in a simple and\nintuitive method in the primal. This dual view can also be used to derive a\nstochastic variant, which performs adaptive non-uniform sampling via the dual\nvariables.",
          "link": "http://arxiv.org/abs/2107.00595",
          "publishedOn": "2021-07-02T01:58:02.451Z",
          "wordCount": 546,
          "title": "Fast Margin Maximization via Dual Acceleration. (arXiv:2107.00595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_P/0/1/0/all/0/1\">Pegah Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheshkal_S/0/1/0/all/0/1\">Sajad Amouei Sheshkal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L.Hammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Processing medical data to find abnormalities is a time-consuming and costly\ntask, requiring tremendous efforts from medical experts. Therefore, Ai has\nbecome a popular tool for the automatic processing of medical data, acting as a\nsupportive tool for doctors. AI tools highly depend on data for training the\nmodels. However, there are several constraints to access to large amounts of\nmedical data to train machine learning algorithms in the medical domain, e.g.,\ndue to privacy concerns and the costly, time-consuming medical data annotation\nprocess. To address this, in this paper we present a novel synthetic data\ngeneration pipeline called SinGAN-Seg to produce synthetic medical data with\nthe corresponding annotated ground truth masks. We show that these synthetic\ndata generation pipelines can be used as an alternative to bypass privacy\nconcerns and as an alternative way to produce artificial segmentation datasets\nwith corresponding ground truth masks to avoid the tedious medical data\nannotation process. As a proof of concept, we used an open polyp segmentation\ndataset. By training UNet++ using both the real polyp segmentation dataset and\nthe corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we\nshow that the synthetic data can achieve a very close performance to the real\ndata when the real segmentation datasets are large enough. In addition, we show\nthat synthetic data generated from the SinGAN-Seg pipeline improving the\nperformance of segmentation algorithms when the training dataset is very small.\nSince our SinGAN-Seg pipeline is applicable for any medical dataset, this\npipeline can be used with any other segmentation datasets.",
          "link": "http://arxiv.org/abs/2107.00471",
          "publishedOn": "2021-07-02T01:58:02.440Z",
          "wordCount": 718,
          "title": "SinGAN-Seg: Synthetic Training Data Generation for Medical Image Segmentation. (arXiv:2107.00471v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00464",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_C/0/1/0/all/0/1\">Chris Junchi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1\">Nicolas Loizou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1\">Gauthier Gidel</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We study the stochastic bilinear minimax optimization problem, presenting an\nanalysis of the Stochastic ExtraGradient (SEG) method with constant step size,\nand presenting variations of the method that yield favorable convergence. We\nfirst note that the last iterate of the basic SEG method only contracts to a\nfixed neighborhood of the Nash equilibrium, independent of the step size. This\ncontrasts sharply with the standard setting of minimization where standard\nstochastic algorithms converge to a neighborhood that vanishes in proportion to\nthe square-root (constant) step size. Under the same setting, however, we prove\nthat when augmented with iteration averaging, SEG provably converges to the\nNash equilibrium, and such a rate is provably accelerated by incorporating a\nscheduled restarting procedure. In the interpolation setting, we achieve an\noptimal convergence rate up to tight constants. We present numerical\nexperiments that validate our theoretical findings and demonstrate the\neffectiveness of the SEG method when equipped with iteration averaging and\nrestarting.",
          "link": "http://arxiv.org/abs/2107.00464",
          "publishedOn": "2021-07-02T01:58:02.428Z",
          "wordCount": 623,
          "title": "On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging. (arXiv:2107.00464v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bei Peng</a>",
          "description": "Cooperative problems under continuous control have always been the focus of\nmulti-agent reinforcement learning. Existing algorithms suffer from the problem\nof uneven learning degree with the increase of the number of agents. In this\npaper, a new structure for a multi-agent actor critic is proposed, and the\nself-attention mechanism is applied in the critic network and the value\ndecomposition method used to solve the uneven problem. The proposed algorithm\nmakes full use of the samples in the replay memory buffer to learn the behavior\nof a class of agents. First, a new update method is proposed for policy\nnetworks that promotes learning efficiency. Second, the utilization of samples\nis improved, at the same time reflecting the ability of perspective-taking\namong groups. Finally, the \"deceptive signal\" in training is eliminated and the\nlearning degree among agents is more uniform than in the existing methods.\nMultiple experiments were conducted in two typical scenarios of a multi-agent\nparticle environment. Experimental results show that the proposed algorithm can\nperform better than the state-of-the-art ones, and that it exhibits higher\nlearning efficiency with an increasing number of agents.",
          "link": "http://arxiv.org/abs/2107.00284",
          "publishedOn": "2021-07-02T01:58:02.408Z",
          "wordCount": 622,
          "title": "SA-MATD3:Self-attention-based multi-agent continuous control method in cooperative environments. (arXiv:2107.00284v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-07-02T01:58:02.401Z",
          "wordCount": 633,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parada_Mayorga_A/0/1/0/all/0/1\">Alejandro Parada-Mayorga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "We study algebraic neural networks (AlgNNs) with commutative algebras which\nunify diverse architectures such as Euclidean convolutional neural networks,\ngraph neural networks, and group neural networks under the umbrella of\nalgebraic signal processing. An AlgNN is a stacked layered information\nprocessing structure where each layer is conformed by an algebra, a vector\nspace and a homomorphism between the algebra and the space of endomorphisms of\nthe vector space. Signals are modeled as elements of the vector space and are\nprocessed by convolutional filters that are defined as the images of the\nelements of the algebra under the action of the homomorphism. We analyze\nstability of algebraic filters and AlgNNs to deformations of the homomorphism\nand derive conditions on filters that lead to Lipschitz stable operators. We\nconclude that stable algebraic filters have frequency responses -- defined as\neigenvalue domain representations -- whose derivative is inversely proportional\nto the frequency -- defined as eigenvalue magnitudes. It follows that for a\ngiven level of discriminability, AlgNNs are more stable than algebraic filters,\nthereby explaining their better empirical performance. This same phenomenon has\nbeen proven for Euclidean convolutional neural networks and graph neural\nnetworks. Our analysis shows that this is a deep algebraic property shared by a\nnumber of architectures.",
          "link": "http://arxiv.org/abs/2009.01433",
          "publishedOn": "2021-07-02T01:58:02.392Z",
          "wordCount": 688,
          "title": "Algebraic Neural Networks: Stability to Deformations. (arXiv:2009.01433v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viale_A/0/1/0/all/0/1\">Alberto Viale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Autonomous Driving (AD) related features provide new forms of mobility that\nare also beneficial for other kind of intelligent and autonomous systems like\nrobots, smart transportation, and smart industries. For these applications, the\ndecisions need to be made fast and in real-time. Moreover, in the quest for\nelectric mobility, this task must follow low power policy, without affecting\nmuch the autonomy of the mean of transport or the robot. These two challenges\ncan be tackled using the emerging Spiking Neural Networks (SNNs). When deployed\non a specialized neuromorphic hardware, SNNs can achieve high performance with\nlow latency and low power consumption. In this paper, we use an SNN connected\nto an event-based camera for facing one of the key problems for AD, i.e., the\nclassification between cars and other objects. To consume less power than\ntraditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The\nexperiments are made following an offline supervised learning rule, followed by\nmapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our\nbest experiment achieves an accuracy on offline implementation of 86%, that\ndrops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware\nimplementation has maximum 0.72 ms of latency for every sample, and consumes\nonly 310 mW. To the best of our knowledge, this work is the first\nimplementation of an event-based car classifier on a Neuromorphic Chip.",
          "link": "http://arxiv.org/abs/2107.00401",
          "publishedOn": "2021-07-02T01:58:02.385Z",
          "wordCount": 694,
          "title": "CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor. (arXiv:2107.00401v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mayee Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohoni_N/0/1/0/all/0/1\">Nimit Sohoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poms_F/0/1/0/all/0/1\">Fait Poms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1\">Kayvon Fatahalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "Machine learning models are often deployed in different settings than they\nwere trained and validated on, posing a challenge to practitioners who wish to\npredict how well the deployed model will perform on a target distribution. If\nan unlabeled sample from the target distribution is available, along with a\nlabeled sample from a possibly different source distribution, standard\napproaches such as importance weighting can be applied to estimate performance\non the target. However, importance weighting struggles when the source and\ntarget distributions have non-overlapping support or are high-dimensional.\nTaking inspiration from fields such as epidemiology and polling, we develop\nMandoline, a new evaluation framework that mitigates these issues. Our key\ninsight is that practitioners may have prior knowledge about the ways in which\nthe distribution shifts, which we can use to better guide the importance\nweighting procedure. Specifically, users write simple \"slicing functions\" -\nnoisy, potentially correlated binary functions intended to capture possible\naxes of distribution shift - to compute reweighted performance estimates. We\nfurther describe a density ratio estimation framework for the slices and show\nhow its estimation error scales with slice quality and dataset size. Empirical\nvalidation on NLP and vision tasks shows that \\name can estimate performance on\nthe target distribution up to $3\\times$ more accurately compared to standard\nbaselines.",
          "link": "http://arxiv.org/abs/2107.00643",
          "publishedOn": "2021-07-02T01:58:02.366Z",
          "wordCount": 654,
          "title": "Mandoline: Model Evaluation under Distribution Shift. (arXiv:2107.00643v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tseran_H/0/1/0/all/0/1\">Hanna Tseran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>",
          "description": "Learning with neural networks relies on the complexity of the representable\nfunctions, but more importantly, the particular assignment of typical\nparameters to functions of different complexity. Taking the number of\nactivation regions as a complexity measure, recent works have shown that the\npractical complexity of deep ReLU networks is often far from the theoretical\nmaximum. In this work we show that this phenomenon also occurs in networks with\nmaxout (multi-argument) activation functions and when considering the decision\nboundaries in classification tasks. We also show that the parameter space has a\nmultitude of full-dimensional regions with widely different complexity, and\nobtain nontrivial lower bounds on the expected complexity. Finally, we\ninvestigate different parameter initialization procedures and show that they\ncan increase the speed of convergence in training.",
          "link": "http://arxiv.org/abs/2107.00379",
          "publishedOn": "2021-07-02T01:58:02.358Z",
          "wordCount": 559,
          "title": "On the Expected Complexity of Maxout Networks. (arXiv:2107.00379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillot_P/0/1/0/all/0/1\">Pierre Gillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parviainen_P/0/1/0/all/0/1\">Pekka Parviainen</a>",
          "description": "Bayesian networks represent relations between variables using a directed\nacyclic graph (DAG). Learning the DAG is an NP-hard problem and exact learning\nalgorithms are feasible only for small sets of variables. We propose two\nscalable heuristics for learning DAGs in the linear structural equation case.\nOur methods learn the DAG by alternating between unconstrained gradient\ndescent-based step to optimize an objective function and solving a maximum\nacyclic subgraph problem to enforce acyclicity. Thanks to this decoupling, our\nmethods scale up beyond thousands of variables.",
          "link": "http://arxiv.org/abs/2107.00571",
          "publishedOn": "2021-07-02T01:58:02.351Z",
          "wordCount": 518,
          "title": "Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics. (arXiv:2107.00571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1\">Etai Littwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saremi_O/0/1/0/all/0/1\">Omid Saremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilak_V/0/1/0/all/0/1\">Vimal Thilak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Greg Yang</a>",
          "description": "We analyze the learning dynamics of infinitely wide neural networks with a\nfinite sized bottle-neck. Unlike the neural tangent kernel limit, a bottleneck\nin an otherwise infinite width network al-lows data dependent feature learning\nin its bottle-neck representation. We empirically show that a single bottleneck\nin infinite networks dramatically accelerates training when compared to purely\nin-finite networks, with an improved overall performance. We discuss the\nacceleration phenomena by drawing similarities to infinitely wide deep linear\nmodels, where the acceleration effect of a bottleneck can be understood\ntheoretically.",
          "link": "http://arxiv.org/abs/2107.00364",
          "publishedOn": "2021-07-02T01:58:02.344Z",
          "wordCount": 529,
          "title": "Implicit Acceleration and Feature Learning inInfinitely Wide Neural Networks with Bottlenecks. (arXiv:2107.00364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Eduardo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_A/0/1/0/all/0/1\">Andres Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>",
          "description": "Recent studies have put into question the commonly assumed shift invariance\nproperty of convolutional networks, showing that small shifts in the input can\naffect the output predictions substantially. In this paper, we ask whether lack\nof shift invariance is a problem in sound event classification, and whether\nthere are benefits in addressing it. Specifically, we evaluate two pooling\nmethods to improve shift invariance in CNNs, based on low-pass filtering and\nadaptive sampling of incoming feature maps. These methods are implemented via\nsmall architectural modifications inserted into the pooling layers of CNNs. We\nevaluate the effect of these architectural changes on the FSD50K dataset using\nmodels of different capacity and in presence of strong regularization. We show\nthat these modifications consistently improve sound event classification in all\ncases considered, without adding any (or adding very few) trainable parameters,\nwhich makes them an appealing alternative to conventional pooling layers. The\noutcome is a new state-of-the-art mAP of 0.541 on the FSD50K classification\nbenchmark.",
          "link": "http://arxiv.org/abs/2107.00623",
          "publishedOn": "2021-07-02T01:58:02.337Z",
          "wordCount": 606,
          "title": "Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks. (arXiv:2107.00623v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Training the large deep neural networks that dominate NLP requires large\ndatasets. Many of these are collected automatically or via crowdsourcing, and\nmay exhibit systematic biases or annotation artifacts. By the latter, we mean\ncorrelations between inputs and outputs that are spurious, insofar as they do\nnot represent a generally held causal relationship between features and\nclasses; models that exploit such correlations may appear to perform a given\ntask well, but fail on out of sample data. In this paper we propose methods to\nfacilitate identification of training data artifacts, using new hybrid\napproaches that combine saliency maps (which highlight important input\nfeatures) with instance attribution methods (which retrieve training samples\ninfluential to a given prediction). We show that this proposed training-feature\nattribution approach can be used to uncover artifacts in training data, and use\nit to identify previously unreported artifacts in a few standard NLP datasets.\nWe execute a small user study to evaluate whether these methods are useful to\nNLP researchers in practice, with promising results. We make code for all\nmethods and experiments in this paper available.",
          "link": "http://arxiv.org/abs/2107.00323",
          "publishedOn": "2021-07-02T01:58:02.302Z",
          "wordCount": 617,
          "title": "Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "We study image inverse problems with a normalizing flow prior. Our\nformulation views the solution as the maximum a posteriori estimate of the\nimage conditioned on the measurements. This formulation allows us to use noise\nmodels with arbitrary dependencies as well as non-linear forward operators. We\nempirically validate the efficacy of our method on various inverse problems,\nincluding compressed sensing with quantized measurements and denoising with\nhighly structured noise patterns. We also present initial theoretical recovery\nguarantees for solving inverse problems with a flow prior.",
          "link": "http://arxiv.org/abs/2003.08089",
          "publishedOn": "2021-07-02T01:58:02.282Z",
          "wordCount": 563,
          "title": "Solving Inverse Problems with a Flow-based Noise Model. (arXiv:2003.08089v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
          "link": "http://arxiv.org/abs/2107.00645",
          "publishedOn": "2021-07-02T01:58:02.276Z",
          "wordCount": 616,
          "title": "Global Filter Networks for Image Classification. (arXiv:2107.00645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.",
          "link": "http://arxiv.org/abs/2103.15722",
          "publishedOn": "2021-07-02T01:58:02.269Z",
          "wordCount": 705,
          "title": "Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asai_M/0/1/0/all/0/1\">Masataro Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajino_H/0/1/0/all/0/1\">Hiroshi Kajino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukunaga_A/0/1/0/all/0/1\">Alex Fukunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muise_C/0/1/0/all/0/1\">Christian Muise</a>",
          "description": "Current domain-independent, classical planners require symbolic models of the\nproblem domain and instance as input, resulting in a knowledge acquisition\nbottleneck. Meanwhile, although deep learning has achieved significant success\nin many fields, the knowledge is encoded in a subsymbolic representation which\nis incompatible with symbolic systems such as planners. We propose Latplan, an\nunsupervised architecture combining deep learning and classical planning. Given\nonly an unlabeled set of image pairs showing a subset of transitions allowed in\nthe environment (training inputs), Latplan learns a complete propositional PDDL\naction model of the environment. Later, when a pair of images representing the\ninitial and the goal states (planning inputs) is given, Latplan finds a plan to\nthe goal state in a symbolic latent space and returns a visualized plan\nexecution. We evaluate Latplan using image-based versions of 6 planning\ndomains: 8-puzzle, 15-Puzzle, Blocksworld, Sokoban and Two variations of\nLightsOut.",
          "link": "http://arxiv.org/abs/2107.00110",
          "publishedOn": "2021-07-02T01:58:02.250Z",
          "wordCount": 586,
          "title": "Classical Planning in Deep Latent Space. (arXiv:2107.00110v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00400",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quach_M/0/1/0/all/0/1\">Maurice Quach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenzise_G/0/1/0/all/0/1\">Giuseppe Valenzise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duhamel_P/0/1/0/all/0/1\">Pierre Duhamel</a>",
          "description": "This paper proposes a lossless point cloud (PC) geometry compression method\nthat uses neural networks to estimate the probability distribution of voxel\noccupancy. First, to take into account the PC sparsity, our method adaptively\npartitions a point cloud into multiple voxel block sizes. This partitioning is\nsignalled via an octree. Second, we employ a deep auto-regressive generative\nmodel to estimate the occupancy probability of each voxel given the previously\nencoded ones. We then employ the estimated probabilities to code efficiently a\nblock using a context-based arithmetic coder. Our context has variable size and\ncan expand beyond the current block to learn more accurate probabilities. We\nalso consider using data augmentation techniques to increase the generalization\ncapability of the learned probability models, in particular in the presence of\nnoise and lower-density point clouds. Experimental evaluation, performed on a\nvariety of point clouds from four different datasets and with diverse\ncharacteristics, demonstrates that our method reduces significantly (by up to\n30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.",
          "link": "http://arxiv.org/abs/2107.00400",
          "publishedOn": "2021-07-02T01:58:02.242Z",
          "wordCount": 652,
          "title": "Lossless Coding of Point Cloud Geometry using a Deep Generative Model. (arXiv:2107.00400v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.",
          "link": "http://arxiv.org/abs/2107.00451",
          "publishedOn": "2021-07-02T01:58:02.235Z",
          "wordCount": 597,
          "title": "VideoLightFormer: Lightweight Action Recognition using Transformers. (arXiv:2107.00451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Solving multi-goal reinforcement learning (RL) problems with sparse rewards\nis generally challenging. Existing approaches have utilized goal relabeling on\ncollected experiences to alleviate issues raised from sparse rewards. However,\nthese methods are still limited in efficiency and cannot make full use of\nexperiences. In this paper, we propose Model-based Hindsight Experience Replay\n(MHER), which exploits experiences more efficiently by leveraging environmental\ndynamics to generate virtual achieved goals. Replacing original goals with\nvirtual goals generated from interaction with a trained dynamics model leads to\na novel relabeling method, \\emph{model-based relabeling} (MBR). Based on MBR,\nMHER performs both reinforcement learning and supervised learning for efficient\npolicy improvement. Theoretically, we also prove the supervised part in MHER,\ni.e., goal-conditioned supervised learning with MBR data, optimizes a lower\nbound on the multi-goal RL objective. Experimental results in several\npoint-based tasks and simulated robotics environments show that MHER achieves\nsignificantly higher sample efficiency than previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.00306",
          "publishedOn": "2021-07-02T01:58:02.229Z",
          "wordCount": 592,
          "title": "MHER: Model-based Hindsight Experience Replay. (arXiv:2107.00306v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00462",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wurster_S/0/1/0/all/0/1\">Skylar W. Wurster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hanqi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterka_T/0/1/0/all/0/1\">Thomas Peterka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_M/0/1/0/all/0/1\">Mukund Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiayi Xu</a>",
          "description": "We present an approach for hierarchical super resolution (SR) using neural\nnetworks on an octree data representation. We train a hierarchy of neural\nnetworks, each capable of 2x upscaling in each spatial dimension between two\nlevels of detail, and use these networks in tandem to facilitate large scale\nfactor super resolution, scaling with the number of trained networks. We\nutilize these networks in a hierarchical super resolution algorithm that\nupscales multiresolution data to a uniform high resolution without introducing\nseam artifacts on octree node boundaries. We evaluate application of this\nalgorithm in a data reduction framework by dynamically downscaling input data\nto an octree-based data structure to represent the multiresolution data before\ncompressing for additional storage reduction. We demonstrate that our approach\navoids seam artifacts common to multiresolution data formats, and show how\nneural network super resolution assisted data reduction can preserve global\nfeatures better than compressors alone at the same compression ratios.",
          "link": "http://arxiv.org/abs/2107.00462",
          "publishedOn": "2021-07-02T01:58:02.213Z",
          "wordCount": 609,
          "title": "Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization. (arXiv:2107.00462v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "Although attention mechanisms have been applied to a variety of deep learning\nmodels and have been shown to improve the prediction performance, it has been\nreported to be vulnerable to perturbations to the mechanism. To overcome the\nvulnerability to perturbations in the mechanism, we are inspired by adversarial\ntraining (AT), which is a powerful regularization technique for enhancing the\nrobustness of the models. In this paper, we propose a general training\ntechnique for natural language processing tasks, including AT for attention\n(Attention AT) and more interpretable AT for attention (Attention iAT). The\nproposed techniques improved the prediction performance and the model\ninterpretability by exploiting the mechanisms with AT. In particular, Attention\niAT boosts those advantages by introducing adversarial perturbation, which\nenhances the difference in the attention of the sentences. Evaluation\nexperiments with ten open datasets revealed that AT for attention mechanisms,\nespecially Attention iAT, demonstrated (1) the best performance in nine out of\nten tasks and (2) more interpretable attention (i.e., the resulting attention\ncorrelated more strongly with gradient-based word importance) for all tasks.\nAdditionally, the proposed techniques are (3) much less dependent on\nperturbation size in AT. Our code is available at\nhttps://github.com/shunk031/attention-meets-perturbation",
          "link": "http://arxiv.org/abs/2009.12064",
          "publishedOn": "2021-07-02T01:58:02.206Z",
          "wordCount": 683,
          "title": "Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training. (arXiv:2009.12064v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:58:02.197Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Ji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV), one of the most important technology\nfor biometric identification, has been widely adopted in security-critic\napplications, including transaction authentication and access control. However,\nprevious works have shown ASV is seriously vulnerable to recently emerged\nadversarial attacks, yet effective countermeasures against them are limited. In\nthis paper, we adopt neural vocoders to spot adversarial samples for ASV. We\nuse neural vocoder to re-synthesize audio and find that the difference between\nthe ASV scores for the original and re-synthesized audio is a good indicator to\ndistinguish genuine and adversarial samples. As the very beginning work in this\ndirection of detecting adversarial samples for ASV, there is no reliable\nbaseline for comparison. So we first implement Griffin-Lim for detection and\nset it as our baseline. The proposed method accomplishes effective detection\nperformance and outperforms all the baselines in all the settings. We also show\nthe neural vocoder adopted in the detection framework is dataset independent.\nOur codes will be made open-source for future works to do comparison.",
          "link": "http://arxiv.org/abs/2107.00309",
          "publishedOn": "2021-07-02T01:58:02.190Z",
          "wordCount": 623,
          "title": "Spotting adversarial samples for speaker verification by neural vocoders. (arXiv:2107.00309v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wu-Jun Li</a>",
          "description": "Distributed learning has become a hot research topic, due to its wide\napplication in cluster-based large-scale learning, federated learning, edge\ncomputing and so on. Most distributed learning methods assume no error and\nattack on the workers. However, many unexpected cases, such as communication\nerror and even malicious attack, may happen in real applications. Hence,\nByzantine learning (BL), which refers to distributed learning with attack or\nerror, has recently attracted much attention. Most existing BL methods are\nsynchronous, which will result in slow convergence when there exist\nheterogeneous workers. Furthermore, in some applications like federated\nlearning and edge computing, synchronization cannot even be performed most of\nthe time due to the online workers (clients or edge servers). Hence,\nasynchronous BL (ABL) is more general and practical than synchronous BL (SBL).\nTo the best of our knowledge, there exist only two ABL methods. One of them\ncannot resist malicious attack. The other needs to store some training\ninstances on the server, which has the privacy leak problem. In this paper, we\npropose a novel method, called buffered asynchronous stochastic gradient\ndescent (BASGD), for BL. BASGD is an asynchronous method. Furthermore, BASGD\nhas no need to store any training instances on the server, and hence can\npreserve privacy in ABL. BASGD is theoretically proved to have the ability of\nresisting against error and malicious attack. Moreover, BASGD has a similar\ntheoretical convergence rate to that of vanilla asynchronous SGD (ASGD), with\nan extra constant variance. Empirical results show that BASGD can significantly\noutperform vanilla ASGD and other ABL baselines, when there exists error or\nattack on workers.",
          "link": "http://arxiv.org/abs/2003.00937",
          "publishedOn": "2021-07-02T01:58:02.170Z",
          "wordCount": 728,
          "title": "BASGD: Buffered Asynchronous SGD for Byzantine Learning. (arXiv:2003.00937v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00385",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Maslej_Kresnakova_V/0/1/0/all/0/1\">Viera Maslej-Kre&#x161;&#x148;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bouchefry_K/0/1/0/all/0/1\">Khadija El Bouchefry</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Butka_P/0/1/0/all/0/1\">Peter Butka</a>",
          "description": "Machine learning techniques have been increasingly used in astronomical\napplications and have proven to successfully classify objects in image data\nwith high accuracy. The current work uses archival data from the Faint Images\nof the Radio Sky at Twenty Centimeters (FIRST) to classify radio galaxies into\nfour classes: Fanaroff-Riley Class I (FRI), Fanaroff-Riley Class II (FRII),\nBent-Tailed (BENT), and Compact (COMPT). The model presented in this work is\nbased on Convolutional Neural Networks (CNNs). The proposed architecture\ncomprises three parallel blocks of convolutional layers combined and processed\nfor final classification by two feed-forward layers. Our model classified\nselected classes of radio galaxy sources on an independent testing subset with\nan average of 96\\% for precision, recall, and F1 score. The best selected\naugmentation techniques were rotations, horizontal or vertical flips, and\nincrease of brightness. Shifts, zoom and decrease of brightness worsened the\nperformance of the model. The current results show that model developed in this\nwork is able to identify different morphological classes of radio galaxies with\na high efficiency and performance",
          "link": "http://arxiv.org/abs/2107.00385",
          "publishedOn": "2021-07-02T01:58:02.152Z",
          "wordCount": 663,
          "title": "Morphological classification of compact and extended radio galaxies using convolutional neural networks and data augmentation techniques. (arXiv:2107.00385v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Buliao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunhui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>",
          "description": "Incomplete instances with various missing attributes in many real-world\napplications have brought challenges to the classification tasks. Missing\nvalues imputation methods are often employed to replace the missing values with\nsubstitute values. However, this process often separates the imputation and\nclassification, which may lead to inferior performance since label information\nare often ignored during imputation. Moreover, traditional methods may rely on\nimproper assumptions to initialize the missing values, whereas the\nunreliability of such initialization might lead to inferior performance. To\naddress these problems, a novel semi-supervised conditional normalizing flow\n(SSCFlow) is proposed in this paper. SSCFlow explicitly utilizes the label\ninformation to facilitate the imputation and classification simultaneously by\nestimating the conditional distribution of incomplete instances with a novel\nsemi-supervised normalizing flow. Moreover, SSCFlow treats the initialized\nmissing values as corrupted initial imputation and iteratively reconstructs\ntheir latent representations with an overcomplete denoising autoencoder to\napproximate their true conditional distribution. Experiments on real-world\ndatasets demonstrate the robustness and effectiveness of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2106.01708",
          "publishedOn": "2021-07-02T01:58:02.123Z",
          "wordCount": 609,
          "title": "Semi-supervised Learning with Missing Values Imputation. (arXiv:2106.01708v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00371",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Sheng Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_Z/0/1/0/all/0/1\">Zongming Ma</a>",
          "description": "Generalized correlation analysis (GCA) is concerned with uncovering linear\nrelationships across multiple datasets. It generalizes canonical correlation\nanalysis that is designed for two datasets. We study sparse GCA when there are\npotentially multiple generalized correlation tuples in data and the loading\nmatrix has a small number of nonzero rows. It includes sparse CCA and sparse\nPCA of correlation matrices as special cases. We first formulate sparse GCA as\ngeneralized eigenvalue problems at both population and sample levels via a\ncareful choice of normalization constraints. Based on a Lagrangian form of the\nsample optimization problem, we propose a thresholded gradient descent\nalgorithm for estimating GCA loading vectors and matrices in high dimensions.\nWe derive tight estimation error bounds for estimators generated by the\nalgorithm with proper initialization. We also demonstrate the prowess of the\nalgorithm on a number of synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.00371",
          "publishedOn": "2021-07-02T01:58:02.115Z",
          "wordCount": 565,
          "title": "Sparse GCA and Thresholded Gradient Descent. (arXiv:2107.00371v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1905.02515",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1\">Emilia Oikarinen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1\">Andreas Henelius</a>",
          "description": "Efficient explorative data analysis systems must take into account both what\na user knows and wants to know. This paper proposes a principled framework for\ninteractive visual exploration of relations in data, through views most\ninformative given the user's current knowledge and objectives. The user can\ninput pre-existing knowledge of relations in the data and also formulate\nspecific exploration interests, which are then taken into account in the\nexploration. The idea is to steer the exploration process towards the interests\nof the user, instead of showing uninteresting or already known relations. The\nuser's knowledge is modelled by a distribution over data sets parametrised by\nsubsets of rows and columns of data, called tile constraints. We provide a\ncomputationally efficient implementation of this concept based on constrained\nrandomisation. Furthermore, we describe a novel dimensionality reduction method\nfor finding the views most informative to the user, which at the limit of no\nbackground knowledge and with generic objectives reduces to PCA. We show that\nthe method is suitable for interactive use and is robust to noise, outperforms\nstandard projection pursuit visualisation methods, and gives understandable and\nuseful results in analysis of real-world data. We provide an open-source\nimplementation of the framework.",
          "link": "http://arxiv.org/abs/1905.02515",
          "publishedOn": "2021-07-02T01:58:02.108Z",
          "wordCount": 676,
          "title": "Guided Visual Exploration of Relations in Data Sets. (arXiv:1905.02515v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00469",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Amir_I/0/1/0/all/0/1\">Idan Amir</a>, <a href=\"http://arxiv.org/find/math/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We study the generalization performance of $\\text{full-batch}$ optimization\nalgorithms for stochastic convex optimization: these are first-order methods\nthat only access the exact gradient of the empirical risk (rather than\ngradients with respect to individual data points), that include a wide range of\nalgorithms such as gradient descent, mirror descent, and their regularized\nand/or accelerated variants. We provide a new separation result showing that,\nwhile algorithms such as stochastic gradient descent can generalize and\noptimize the population risk to within $\\epsilon$ after $O(1/\\epsilon^2)$\niterations, full-batch methods either need at least $\\Omega(1/\\epsilon^4)$\niterations or exhibit a dimension-dependent sample complexity.",
          "link": "http://arxiv.org/abs/2107.00469",
          "publishedOn": "2021-07-02T01:58:02.088Z",
          "wordCount": 534,
          "title": "Never Go Full Batch (in Stochastic Convex Optimization). (arXiv:2107.00469v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02728",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1\">Meimei Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1\">David B. Dunson</a>",
          "description": "There has been huge interest in studying human brain connectomes inferred\nfrom different imaging modalities and exploring their relationship with human\ntraits, such as cognition. Brain connectomes are usually represented as\nnetworks, with nodes corresponding to different regions of interest (ROIs) and\nedges to connection strengths between ROIs. Due to the high-dimensionality and\nnon-Euclidean nature of networks, it is challenging to depict their population\ndistribution and relate them to human traits. Current approaches focus on\nsummarizing the network using either pre-specified topological features or\nprincipal components analysis (PCA). In this paper, building on recent advances\nin deep learning, we develop a nonlinear latent factor model to characterize\nthe population distribution of brain graphs and infer the relationships between\nbrain structural connectomes and human traits. We refer to our method as Graph\nAuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging\ndatasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human\nConnectome Project (HCP) for adults, to understand the structural brain\nconnectome and its relationship with cognition. Numerical results demonstrate\nhuge advantages of GATE over competitors in terms of prediction accuracy,\nstatistical inference and computing efficiency. We found that structural\nconnectomes have a stronger association with a wide range of human cognitive\ntraits than was apparent using previous approaches.",
          "link": "http://arxiv.org/abs/1911.02728",
          "publishedOn": "2021-07-02T01:58:02.081Z",
          "wordCount": 679,
          "title": "Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets. (arXiv:1911.02728v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1\">Meir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyanskiy_Y/0/1/0/all/0/1\">Yury Polyanskiy</a>",
          "description": "We consider the question of sequential prediction under the log-loss in terms\nof cumulative regret. Namely, given a hypothesis class of distributions,\nlearner sequentially predicts the (distribution of the) next letter in sequence\nand its performance is compared to the baseline of the best constant predictor\nfrom the hypothesis class. The well-specified case corresponds to an additional\nassumption that the data-generating distribution belongs to the hypothesis\nclass as well. Here we present results in the more general misspecified case.\nDue to special properties of the log-loss, the same problem arises in the\ncontext of competitive-optimality in density estimation, and model selection.\nFor the $d$-dimensional Gaussian location hypothesis class, we show that\ncumulative regrets in the well-specified and misspecified cases asymptotically\ncoincide. In other words, we provide an $o(1)$ characterization of the\ndistribution-free (or PAC) regret in this case -- the first such result as far\nas we know. We recall that the worst-case (or individual-sequence) regret in\nthis case is larger by an additive constant ${d\\over 2} + o(1)$. Surprisingly,\nneither the traditional Bayesian estimators, nor the Shtarkov's normalized\nmaximum likelihood achieve the PAC regret and our estimator requires special\n\"robustification\" against heavy-tailed data. In addition, we show two general\nresults for misspecified regret: the existence and uniqueness of the optimal\nestimator, and the bound sandwiching the misspecified regret between\nwell-specified regrets with (asymptotically) close hypotheses classes.",
          "link": "http://arxiv.org/abs/2102.00050",
          "publishedOn": "2021-07-02T01:58:02.043Z",
          "wordCount": 688,
          "title": "Sequential prediction under log-loss and misspecification. (arXiv:2102.00050v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damirchi_H/0/1/0/all/0/1\">Hamed Damirchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorrambakht_R/0/1/0/all/0/1\">Rooholla Khorrambakht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghirad_H/0/1/0/all/0/1\">Hamid D. Taghirad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshiri_B/0/1/0/all/0/1\">Behzad Moshiri</a>",
          "description": "The incremental poses computed through odometry can be integrated over time\nto calculate the pose of a device with respect to an initial location. The\nresulting global pose may be used to formulate a second, consistency based,\nloss term in a deep odometry setting. In such cases where multiple losses are\nimposed on a network, the uncertainty over each output can be derived to weigh\nthe different loss terms in a maximum likelihood setting. However, when\nimposing a constraint on the integrated transformation, due to how only\nodometry is estimated at each iteration of the algorithm, there is no\ninformation about the uncertainty associated with the global pose to weigh the\nglobal loss term. In this paper, we associate uncertainties with the output\nposes of a deep odometry network and propagate the uncertainties through each\niteration. Our goal is to use the estimated covariance matrix at each\nincremental step to weigh the loss at the corresponding step while weighting\nthe global loss term using the compounded uncertainty. This formulation\nprovides an adaptive method to weigh the incremental and integrated loss terms\nagainst each other, noting the increase in uncertainty as new estimates arrive.\nWe provide quantitative and qualitative analysis of pose estimates and show\nthat our method surpasses the accuracy of the state-of-the-art Visual Odometry\napproaches. Then, uncertainty estimates are evaluated and comparisons against\nfixed baselines are provided. Finally, the uncertainty values are used in a\nrealistic example to show the effectiveness of uncertainty quantification for\nlocalization.",
          "link": "http://arxiv.org/abs/2107.00366",
          "publishedOn": "2021-07-02T01:58:02.036Z",
          "wordCount": 695,
          "title": "A Consistency-Based Loss for Deep Odometry Through Uncertainty Propagation. (arXiv:2107.00366v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00363",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dewolf_N/0/1/0/all/0/1\">Nicolas Dewolf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baets_B/0/1/0/all/0/1\">Bernard De Baets</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Waegeman_W/0/1/0/all/0/1\">Willem Waegeman</a>",
          "description": "Over the last few decades, various methods have been proposed for estimating\nprediction intervals in regression settings, including Bayesian methods,\nensemble methods, direct interval estimation methods and conformal prediction\nmethods. An important issue is the calibration of these methods: the generated\nprediction intervals should have a predefined coverage level, without being\noverly conservative. In this work, we review the above four classes of methods\nfrom a conceptual and experimental point of view. Results on benchmark data\nsets from various domains highlight large fluctuations in performance from one\ndata set to another. These observations can be attributed to the violation of\ncertain assumptions that are inherent to some classes of methods. We illustrate\nhow conformal prediction can be used as a general calibration procedure for\nmethods that deliver poor results without a calibration step.",
          "link": "http://arxiv.org/abs/2107.00363",
          "publishedOn": "2021-07-02T01:58:02.029Z",
          "wordCount": 566,
          "title": "Well-calibrated prediction intervals for regression problems. (arXiv:2107.00363v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frankel_A/0/1/0/all/0/1\">Ari Frankel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safta_C/0/1/0/all/0/1\">Cosmin Safta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alleman_C/0/1/0/all/0/1\">Coleman Alleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Reese Jones</a>",
          "description": "Predicting the evolution of a representative sample of a material with\nmicrostructure is a fundamental problem in homogenization. In this work we\npropose a graph convolutional neural network that utilizes the discretized\nrepresentation of the initial microstructure directly, without segmentation or\nclustering. Compared to feature-based and pixel-based convolutional neural\nnetwork models, the proposed method has a number of advantages: (a) it is deep\nin that it does not require featurization but can benefit from it, (b) it has a\nsimple implementation with standard convolutional filters and layers, (c) it\nworks natively on unstructured and structured grid data without interpolation\n(unlike pixel-based convolutional neural networks), and (d) it preserves\nrotational invariance like other graph-based convolutional neural networks. We\ndemonstrate the performance of the proposed network and compare it to\ntraditional pixel-based convolution neural network models and feature-based\ngraph convolutional neural networks on three large datasets.",
          "link": "http://arxiv.org/abs/2107.00090",
          "publishedOn": "2021-07-02T01:58:02.019Z",
          "wordCount": 586,
          "title": "Mesh-based graph convolutional neural network models of processes with complex initial states. (arXiv:2107.00090v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Muntabir Hasan Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayanetti_H/0/1/0/all/0/1\">Himarsha R. Jayanetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>",
          "description": "Electronic Theses and Dissertations (ETDs) contain domain knowledge that can\nbe used for many digital library tasks, such as analyzing citation networks and\npredicting research trends. Automatic metadata extraction is important to build\nscalable digital library search engines. Most existing methods are designed for\nborn-digital documents, so they often fail to extract metadata from scanned\ndocuments such as for ETDs. Traditional sequence tagging methods mainly rely on\ntext-based features. In this paper, we propose a conditional random field (CRF)\nmodel that combines text-based and visual features. To verify the robustness of\nour model, we extended an existing corpus and created a new ground truth corpus\nconsisting of 500 ETD cover pages with human validated metadata. Our\nexperiments show that CRF with visual features outperformed both a heuristic\nand a CRF model with only text-based features. The proposed model achieved\n81.3%-96% F1 measure on seven metadata fields. The data and source code are\npublicly available on Google Drive (https://tinyurl.com/y8kxzwrp) and a GitHub\nrepository (https://github.com/lamps-lab/ETDMiner/tree/master/etd_crf),\nrespectively.",
          "link": "http://arxiv.org/abs/2107.00516",
          "publishedOn": "2021-07-02T01:58:01.876Z",
          "wordCount": 630,
          "title": "Automatic Metadata Extraction Incorporating Visual Features from Scanned Electronic Theses and Dissertations. (arXiv:2107.00516v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1\">Allan Gr&#xf8;nlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogsgaard_M/0/1/0/all/0/1\">Mikael H&#xf8;gsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamma_L/0/1/0/all/0/1\">Lior Kamma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1\">Kasper Green Larsen</a>",
          "description": "Explaining the surprising generalization performance of deep neural networks\nis an active and important line of research in theoretical machine learning.\nInfluential work by Arora et al. (ICML'18) showed that, noise stability\nproperties of deep nets occurring in practice can be used to provably compress\nmodel representations. They then argued that the small representations of\ncompressed networks imply good generalization performance albeit only of the\ncompressed nets. Extending their compression framework to yield generalization\nbounds for the original uncompressed networks remains elusive.\n\nOur main contribution is the establishment of a compression-based framework\nfor proving generalization bounds. The framework is simple and powerful enough\nto extend the generalization bounds by Arora et al. to also hold for the\noriginal network. To demonstrate the flexibility of the framework, we also show\nthat it allows us to give simple proofs of the strongest known generalization\nbounds for other popular machine learning models, namely Support Vector\nMachines and Boosting.",
          "link": "http://arxiv.org/abs/2106.07989",
          "publishedOn": "2021-07-02T01:58:01.867Z",
          "wordCount": 619,
          "title": "Compression Implies Generalization. (arXiv:2106.07989v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1\">Kei Uchizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_H/0/1/0/all/0/1\">Haruki Abe</a>",
          "description": "We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n\nWe first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.",
          "link": "http://arxiv.org/abs/2107.00223",
          "publishedOn": "2021-07-02T01:58:01.839Z",
          "wordCount": 752,
          "title": "Circuit Complexity of Visual Search. (arXiv:2107.00223v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02642",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Christoffersen_B/0/1/0/all/0/1\">Benjamin Christoffersen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Clements_M/0/1/0/all/0/1\">Mark Clements</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Humphreys_K/0/1/0/all/0/1\">Keith Humphreys</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Missing values with mixed data types is a common problem in a large number of\nmachine learning applications such as processing of surveys and in different\nmedical applications. Recently, Gaussian copula models have been suggested as a\nmeans of performing imputation of missing values using a probabilistic\nframework. While the present Gaussian copula models have shown to yield state\nof the art performance, they have two limitations: they are based on an\napproximation that is fast but may be imprecise and they do not support\nunordered multinomial variables. We address the first limitation using direct\nand arbitrarily precise approximations both for model estimation and imputation\nby using randomized quasi-Monte Carlo procedures. The method we provide has\nlower errors for the estimated model parameters and the imputed values,\ncompared to previously proposed methods. We also extend the previous Gaussian\ncopula models to include unordered multinomial variables in addition to the\npresent support of ordinal, binary, and continuous variables.",
          "link": "http://arxiv.org/abs/2102.02642",
          "publishedOn": "2021-07-02T01:58:01.832Z",
          "wordCount": 632,
          "title": "Asymptotically Exact and Fast Gaussian Copula Models for Imputation of Mixed Data Types. (arXiv:2102.02642v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hsiang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mario Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1\">Flavio P. Calmon</a>",
          "description": "Disparate treatment occurs when a machine learning model yields different\ndecisions for individuals based on a sensitive attribute (e.g., age, sex). In\ndomains where prediction accuracy is paramount, it could potentially be\nacceptable to fit a model which exhibits disparate treatment. To evaluate the\neffect of disparate treatment, we compare the performance of split classifiers\n(i.e., classifiers trained and deployed separately on each group) with\ngroup-blind classifiers (i.e., classifiers which do not use a sensitive\nattribute). We introduce the benefit-of-splitting for quantifying the\nperformance improvement by splitting classifiers. Computing the\nbenefit-of-splitting directly from its definition could be intractable since it\ninvolves solving optimization problems over an infinite-dimensional functional\nspace. Under different performance measures, we (i) prove an equivalent\nexpression for the benefit-of-splitting which can be efficiently computed by\nsolving small-scale convex programs; (ii) provide sharp upper and lower bounds\nfor the benefit-of-splitting which reveal precise conditions where a\ngroup-blind classifier will always suffer from a non-trivial performance gap\nfrom the split classifiers. In the finite sample regime, splitting is not\nnecessarily beneficial and we provide data-dependent bounds to understand this\neffect. Finally, we validate our theoretical results through numerical\nexperiments on both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2002.04788",
          "publishedOn": "2021-07-02T01:58:01.822Z",
          "wordCount": 693,
          "title": "To Split or Not to Split: The Impact of Disparate Treatment in Classification. (arXiv:2002.04788v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1\">Erik Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacVittie_K/0/1/0/all/0/1\">Korey MacVittie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilly_J/0/1/0/all/0/1\">John Lilly</a>",
          "description": "Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.",
          "link": "http://arxiv.org/abs/2107.00436",
          "publishedOn": "2021-07-02T01:58:01.804Z",
          "wordCount": 570,
          "title": "Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "Recently, Vision Transformer and its variants have shown great promise on\nvarious computer vision tasks. The ability of capturing short- and long-range\nvisual dependencies through self-attention is arguably the main source for the\nsuccess. But it also brings challenges due to quadratic computational overhead,\nespecially for the high-resolution vision tasks (e.g., object detection). In\nthis paper, we present focal self-attention, a new mechanism that incorporates\nboth fine-grained local and coarse-grained global interactions. Using this new\nmechanism, each token attends the closest surrounding tokens at fine\ngranularity but the tokens far away at coarse granularity, and thus can capture\nboth short- and long-range visual dependencies efficiently and effectively.\nWith focal self-attention, we propose a new variant of Vision Transformer\nmodels, called Focal Transformer, which achieves superior performance over the\nstate-of-the-art vision Transformers on a range of public image classification\nand object detection benchmarks. In particular, our Focal Transformer models\nwith a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8\nTop-1 accuracy, respectively, on ImageNet classification at 224x224 resolution.\nUsing Focal Transformers as the backbones, we obtain consistent and substantial\nimprovements over the current state-of-the-art Swin Transformers for 6\ndifferent object detection methods trained with standard 1x and 3x schedules.\nOur largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs\non COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation,\ncreating new SoTA on three of the most challenging computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.00641",
          "publishedOn": "2021-07-02T01:58:01.787Z",
          "wordCount": 689,
          "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers. (arXiv:2107.00641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00296",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuhao Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain diseases, most of them give few\nexplanations on how to make predictions. Inspired by Koch's Postulates, the\nfoundation in evidence-based medicine (EBM) to identify the pathogen, we\npropose to exploit the interpretability of deep learning application in medical\ndiagnosis. By determining and isolating the neuron activation patterns on which\ndiabetic retinopathy (DR) detector relies to make decisions, we demonstrate the\ndirect relation between the isolated neuron activation and lesions for a\npathological explanation. To be specific, we first define novel pathological\ndescriptors using activated neurons of the DR detector to encode both spatial\nand appearance information of lesions. Then, to visualize the symptom encoded\nin the descriptor, we propose Patho-GAN, a new network to synthesize medically\nplausible retinal images. By manipulating these descriptors, we could even\narbitrarily control the position, quantity, and categories of generated\nlesions. We also show that our synthesized images carry the symptoms directly\nrelated to diabetic retinopathy diagnosis. Our generated images are both\nqualitatively and quantitatively superior to the ones by previous methods.\nBesides, compared to existing methods that take hours to generate an image, our\nsecond level speed endows the potential to be an effective solution for data\naugmentation.",
          "link": "http://arxiv.org/abs/2107.00296",
          "publishedOn": "2021-07-02T01:58:01.770Z",
          "wordCount": 667,
          "title": "Explainable Diabetic Retinopathy Detection and Retinal Image Generation. (arXiv:2107.00296v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cook_C/0/1/0/all/0/1\">Curtiss B. Cook</a>",
          "description": "Contextual multi-armed bandit has shown to be an effective tool in\nrecommender systems. In this paper, we study a novel problem of multi-facet\nbandits involving a group of bandits, each characterizing the users' needs from\none unique aspect. In each round, for the given user, we need to select one arm\nfrom each bandit, such that the combination of all arms maximizes the final\nreward. This problem can find immediate applications in E-commerce, healthcare,\netc. To address this problem, we propose a novel algorithm, named MuFasa, which\nutilizes an assembled neural network to jointly learn the underlying reward\nfunctions of multiple bandits. It estimates an Upper Confidence Bound (UCB)\nlinked with the expected reward to balance between exploitation and\nexploration. Under mild assumptions, we provide the regret analysis of MuFasa.\nIt can achieve the near-optimal $\\widetilde{ \\mathcal{O}}((K+1)\\sqrt{T})$\nregret bound where $K$ is the number of bandits and $T$ is the number of played\nrounds. Furthermore, we conduct extensive experiments to show that MuFasa\noutperforms strong baselines on real-world data sets.",
          "link": "http://arxiv.org/abs/2106.03039",
          "publishedOn": "2021-07-02T01:58:01.752Z",
          "wordCount": 626,
          "title": "Multi-facet Contextual Bandits: A Neural Network Perspective. (arXiv:2106.03039v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_L/0/1/0/all/0/1\">Lu Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Few-shot learning (FSL) aims to train a strong classifier using limited\nlabeled examples. Many existing works take the meta-learning approach, sampling\nfew-shot tasks in turn and optimizing the few-shot learner's performance on\nclassifying the query examples. In this paper, we point out two potential\nweaknesses of this approach. First, the sampled query examples may not provide\nsufficient supervision for the few-shot learner. Second, the effectiveness of\nmeta-learning diminishes sharply with increasing shots (i.e., the number of\ntraining examples per class). To resolve these issues, we propose a novel\nobjective to directly train the few-shot learner to perform like a strong\nclassifier. Concretely, we associate each sampled few-shot task with a strong\nclassifier, which is learned with ample labeled examples. The strong classifier\nhas a better generalization ability and we use it to supervise the few-shot\nlearner. We present an efficient way to construct the strong classifier, making\nour proposed objective an easily plug-and-play term to existing meta-learning\nbased FSL methods. We validate our approach in combinations with many\nrepresentative meta-learning methods. On several benchmark datasets including\nminiImageNet and tiredImageNet, our approach leads to a notable improvement\nacross a variety of tasks. More importantly, with our approach, meta-learning\nbased FSL methods can consistently outperform non-meta-learning based ones,\neven in a many-shot setting, greatly strengthening their applicability.",
          "link": "http://arxiv.org/abs/2107.00197",
          "publishedOn": "2021-07-02T01:58:01.724Z",
          "wordCount": 654,
          "title": "Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:58:01.632Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin-Qiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yao-Xiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "One of the key issues for imitation learning lies in making policy learned\nfrom limited samples to generalize well in the whole state-action space. This\nproblem is much more severe in high-dimensional state environments, such as\ngame playing with raw pixel inputs. Under this situation, even state-of-the-art\nadversary-based imitation learning algorithms fail. Through empirical studies,\nwe find that the main cause lies in the failure of training a powerful\ndiscriminator to generate meaningful rewards in high-dimensional environments.\nAlthough it seems that dimensionality reduction can help, a straightforward\napplication of off-the-shelf methods cannot achieve good performance. In this\nwork, we show in theory that the balance between dimensionality reduction and\ndiscriminative training is essential for effective learning. To achieve this\ntarget, we propose HashReward, which utilizes the idea of supervised hashing to\nrealize such an ideal balance. Experimental results show that HashReward could\noutperform state-of-the-art methods for a large gap under the challenging\nhigh-dimensional environments.",
          "link": "http://arxiv.org/abs/1909.03773",
          "publishedOn": "2021-07-02T01:58:01.625Z",
          "wordCount": 628,
          "title": "Imitation Learning from Pixel-Level Demonstrations by HashReward. (arXiv:1909.03773v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>",
          "description": "Graph neural networks (GNN) have recently emerged as a vehicle for applying\ndeep network architectures to graph and relational data. However, given the\nincreasing size of industrial datasets, in many practical situations the\nmessage passing computations required for sharing information across GNN layers\nare no longer scalable. Although various sampling methods have been introduced\nto approximate full-graph training within a tractable budget, there remain\nunresolved complications such as high variances and limited theoretical\nguarantees. To address these issues, we build upon existing work and treat GNN\nneighbor sampling as a multi-armed bandit problem but with a newly-designed\nreward function that introduces some degree of bias designed to reduce variance\nand avoid unstable, possibly-unbounded pay outs. And unlike prior bandit-GNN\nuse cases, the resulting policy leads to near-optimal regret while accounting\nfor the GNN training dynamics introduced by SGD. From a practical standpoint,\nthis translates into lower variance estimates and competitive or superior test\naccuracy across several benchmarks.",
          "link": "http://arxiv.org/abs/2103.01089",
          "publishedOn": "2021-07-02T01:58:01.617Z",
          "wordCount": 623,
          "title": "A Biased Graph Neural Network Sampler with Near-Optimal Regret. (arXiv:2103.01089v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Grace Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Linghan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>",
          "description": "The ability to transfer a policy from one environment to another is a\npromising avenue for efficient robot learning in realistic settings where task\nsupervision is not available. This can allow us to take advantage of\nenvironments well suited for training, such as simulators or laboratories, to\nlearn a policy for a real robot in a home or office. To succeed, such policy\ntransfer must overcome both the visual domain gap (e.g. different illumination\nor background) and the dynamics domain gap (e.g. different robot calibration or\nmodelling error) between source and target environments. However, prior policy\ntransfer approaches either cannot handle a large domain gap or can only address\none type of domain gap at a time. In this paper, we propose a novel policy\ntransfer method with iterative \"environment grounding\", IDAPT, that alternates\nbetween (1) directly minimizing both visual and dynamics domain gaps by\ngrounding the source environment in the target environment domains, and (2)\ntraining a policy on the grounded source environment. This iterative training\nprogressively aligns the domains between the two environments and adapts the\npolicy to the target environment. Once trained, the policy can be directly\nexecuted on the target environment. The empirical results on locomotion and\nrobotic manipulation tasks demonstrate that our approach can effectively\ntransfer a policy across visual and dynamics domain gaps with minimal\nsupervision and interaction with the target environment. Videos and code are\navailable at https://clvrai.com/idapt .",
          "link": "http://arxiv.org/abs/2107.00339",
          "publishedOn": "2021-07-02T01:58:01.611Z",
          "wordCount": 688,
          "title": "Policy Transfer across Visual and Dynamics Domain Gaps via Iterative Grounding. (arXiv:2107.00339v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02409",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>",
          "description": "We study the training of finite-width two-layer smoothed ReLU networks for\nbinary classification using the logistic loss. We show that gradient descent\ndrives the training loss to zero if the initial loss is small enough. When the\ndata satisfies certain cluster and separation conditions and the network is\nwide enough, we show that one step of gradient descent reduces the loss\nsufficiently that the first result applies.",
          "link": "http://arxiv.org/abs/2012.02409",
          "publishedOn": "2021-07-02T01:58:01.593Z",
          "wordCount": 549,
          "title": "When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1\">Sebastian Flennerhag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprechmann_P/0/1/0/all/0/1\">Pablo Sprechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visin_F/0/1/0/all/0/1\">Francesco Visin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1\">Alexandre Galashov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapturowski_S/0/1/0/all/0/1\">Steven Kapturowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1\">Diana L. Borsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andre Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "An effective approach to exploration in reinforcement learning is to rely on\nan agent's uncertainty over the optimal policy, which can yield near-optimal\nexploration strategies in tabular settings. However, in non-tabular settings\nthat involve function approximators, obtaining accurate uncertainty estimates\nis almost as challenging a problem. In this paper, we highlight that value\nestimates are easily biased and temporally inconsistent. In light of this, we\npropose a novel method for estimating uncertainty over the value function that\nrelies on inducing a distribution over temporal difference errors. This\nexploration signal controls for state-action transitions so as to isolate\nuncertainty in value that is due to uncertainty over the agent's parameters.\nBecause our measure of uncertainty conditions on state-action transitions, we\ncannot act on this measure directly. Instead, we incorporate it as an intrinsic\nreward and treat exploration as a separate learning problem, induced by the\nagent's temporal difference uncertainties. We introduce a distinct exploration\npolicy that learns to collect data with high estimated uncertainty, which gives\nrise to a curriculum that smoothly changes throughout learning and vanishes in\nthe limit of perfect value estimates. We evaluate our method on hard\nexploration tasks, including Deep Sea and Atari 2600 environments and find that\nour proposed form of exploration facilitates both diverse and deep exploration.",
          "link": "http://arxiv.org/abs/2010.02255",
          "publishedOn": "2021-07-02T01:58:01.586Z",
          "wordCount": 703,
          "title": "Temporal Difference Uncertainties as a Signal for Exploration. (arXiv:2010.02255v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshem_A/0/1/0/all/0/1\">Amir Leshem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>",
          "description": "We study a decentralized channel allocation problem in an ad-hoc Internet of\nThings network underlaying on the spectrum licensed to a primary cellular\nnetwork. In the considered network, the impoverished channel sensing/probing\ncapability and computational resource on the IoT devices make them difficult to\nacquire the detailed Channel State Information (CSI) for the shared multiple\nchannels. In practice, the unknown patterns of the primary users' transmission\nactivities and the time-varying CSI (e.g., due to small-scale fading or device\nmobility) also cause stochastic changes in the channel quality. Decentralized\nIoT links are thus expected to learn channel conditions online based on partial\nobservations, while acquiring no information about the channels that they are\nnot operating on. They also have to reach an efficient, collision-free solution\nof channel allocation with limited coordination. Our study maps this problem\ninto a contextual multi-player, multi-armed bandit game, and proposes a purely\ndecentralized, three-stage policy learning algorithm through trial-and-error.\nTheoretical analyses shows that the proposed scheme guarantees the IoT links to\njointly converge to the social optimal channel allocation with a sub-linear\n(i.e., polylogarithmic) regret with respect to the operational time.\nSimulations demonstrate that it strikes a good balance between efficiency and\nnetwork scalability when compared with the other state-of-the-art decentralized\nbandit algorithms.",
          "link": "http://arxiv.org/abs/2003.13314",
          "publishedOn": "2021-07-02T01:58:01.579Z",
          "wordCount": 714,
          "title": "Decentralized Learning for Channel Allocation in IoT Networks over Unlicensed Bandwidth as a Contextual Multi-player Multi-armed Bandit Game. (arXiv:2003.13314v3 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04998",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>",
          "description": "We establish conditions under which gradient descent applied to fixed-width\ndeep networks drives the logistic loss to zero, and prove bounds on the rate of\nconvergence. Our analysis applies for smoothed approximations to the ReLU, such\nas Swish and the Huberized ReLU, proposed in previous applied work. We provide\ntwo sufficient conditions for convergence. The first is simply a bound on the\nloss at initialization. The second is a data separation condition used in prior\nanalyses.",
          "link": "http://arxiv.org/abs/2102.04998",
          "publishedOn": "2021-07-02T01:58:01.572Z",
          "wordCount": 553,
          "title": "When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?. (arXiv:2102.04998v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1\">Ricardo Luna Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonetti_M/0/1/0/all/0/1\">Matteo Leonetti</a>",
          "description": "In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of\ntasks to prepare for and learn faster in new, unseen, but related tasks. The\ntraining tasks are usually hand-crafted to be representative of the expected\ndistribution of test tasks and hence all used in training. We show that given a\nset of training tasks, learning can be both faster and more effective (leading\nto better performance in the test tasks), if the training tasks are\nappropriately selected. We propose a task selection algorithm,\nInformation-Theoretic Task Selection (ITTS), based on information theory, which\noptimizes the set of tasks used for training in meta-RL, irrespectively of how\nthey are generated. The algorithm establishes which training tasks are both\nsufficiently relevant for the test tasks, and different enough from one\nanother. We reproduce different meta-RL experiments from the literature and\nshow that ITTS improves the final performance in all of them.",
          "link": "http://arxiv.org/abs/2011.01054",
          "publishedOn": "2021-07-02T01:58:01.565Z",
          "wordCount": 607,
          "title": "Information-theoretic Task Selection for Meta-Reinforcement Learning. (arXiv:2011.01054v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>",
          "description": "While theoretically appealing, the application of the Wasserstein distance to\nlarge-scale machine learning problems has been hampered by its prohibitive\ncomputational cost. The sliced Wasserstein distance and its variants improve\nthe computational efficiency through the random projection, yet they suffer\nfrom low accuracy if the number of projections is not sufficiently large,\nbecause the majority of projections result in trivially small values. In this\nwork, we propose a new family of distance metrics, called augmented sliced\nWasserstein distances (ASWDs), constructed by first mapping samples to\nhigher-dimensional hypersurfaces parameterized by neural networks. It is\nderived from a key observation that (random) linear projections of samples\nresiding on these hypersurfaces would translate to much more flexible nonlinear\nprojections in the original sample space, so they can capture complex\nstructures of the data distribution. We show that the hypersurfaces can be\noptimized by gradient ascent efficiently. We provide the condition under which\nthe ASWD is a valid metric and show that this can be obtained by an injective\nneural network architecture. Numerical results demonstrate that the ASWD\nsignificantly outperforms other Wasserstein variants for both synthetic and\nreal-world problems.",
          "link": "http://arxiv.org/abs/2006.08812",
          "publishedOn": "2021-07-02T01:58:01.548Z",
          "wordCount": 660,
          "title": "Augmented Sliced Wasserstein Distances. (arXiv:2006.08812v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunfei Yang</a>",
          "description": "This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results establish the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structures or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrated around a low-dimensional set, we show that the learning rates of\nGANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into the generator and discriminator approximation error and\nthe statistical error, which may be of independent interest.",
          "link": "http://arxiv.org/abs/2105.13010",
          "publishedOn": "2021-07-02T01:58:01.541Z",
          "wordCount": 628,
          "title": "An error analysis of generative adversarial networks for learning distributions. (arXiv:2105.13010v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1\">Aahlad Puli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily H. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1\">Eric K. Oermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Deep predictive models often make use of spurious correlations between the\nlabel and the covariates that differ between training and test distributions.\nIn many classification tasks, spurious correlations are induced by a changing\nrelationship between the label and some nuisance variables correlated with the\ncovariates. For example, in classifying animals in natural images, the\nbackground, which is the nuisance, can predict the type of animal, but this\nnuisance label relationship does not always hold. This nuisance-label\nrelationship does not always hold. We formalize a family of distributions that\nonly differ in the nuisance-label relationship and and introduce a distribution\nwhere this relationship is broken called the nuisance-randomized distribution.\nWe introduce a set of predictive models built from the nuisance-randomized\ndistribution with representations, that when conditioned on, do not correlate\nthe label and the nuisance. For models in this set, we lower bound the\nperformance for any member of the family with the mutual information between\nthe representation and the label under the nuisance-randomized distribution. To\nbuild predictive models that maximize the performance lower bound, we develop\nNuisance-Randomized Distillation (NURD). We evaluate NURD on a synthetic\nexample, colored-MNIST, and classifying chest X-rays. When using non-lung\npatches as the nuisance in classifying chest X-rays, NURD produces models that\npredict pneumonia under strong spurious correlations.",
          "link": "http://arxiv.org/abs/2107.00520",
          "publishedOn": "2021-07-02T01:58:01.534Z",
          "wordCount": 652,
          "title": "Predictive Modeling in the Presence of Nuisance-Induced Spurious Correlations. (arXiv:2107.00520v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manohar_Alers_N/0/1/0/all/0/1\">Nelson Manohar-Alers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiguo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "We present DeClaW, a system for detecting, classifying, and warning of\nadversarial inputs presented to a classification neural network. In contrast to\ncurrent state-of-the-art methods that, given an input, detect whether an input\nis clean or adversarial, we aim to also identify the types of adversarial\nattack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract\nstatistical profiles, which we term as anomaly feature vectors, from a set of\nlatent features. Preliminary findings suggest that AFVs can help distinguish\namong several types of adversarial attacks (e.g., PGD versus Carlini-Wagner)\nwith close to 93% accuracy on the CIFAR-10 dataset. The results open the door\nto using AFV-based methods for exploring not only adversarial attack detection\nbut also classification of the attack type and then design of attack-specific\nmitigation strategies.",
          "link": "http://arxiv.org/abs/2107.00561",
          "publishedOn": "2021-07-02T01:58:01.527Z",
          "wordCount": 593,
          "title": "Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples. (arXiv:2107.00561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00391",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lopez_Ramos_L/0/1/0/all/0/1\">Luis Miguel Lopez-Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_K/0/1/0/all/0/1\">Kevin Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beferull_Lozano_B/0/1/0/all/0/1\">Baltasar Beferull-Lozano</a>",
          "description": "A method for nonlinear topology identification is proposed, based on the\nassumption that a collection of time series are generated in two steps: i) a\nvector autoregressive process in a latent space, and ii) a nonlinear,\ncomponent-wise, monotonically increasing observation mapping. The latter\nmappings are assumed invertible, and are modelled as shallow neural networks,\nso that their inverse can be numerically evaluated, and their parameters can be\nlearned using a technique inspired in deep learning. Due to the function\ninversion, the back-propagation step is not straightforward, and this paper\nexplains the steps needed to calculate the gradients applying implicit\ndifferentiation. Whereas the model explainability is the same as that for\nlinear VAR processes, preliminary numerical tests show that the prediction\nerror becomes smaller.",
          "link": "http://arxiv.org/abs/2107.00391",
          "publishedOn": "2021-07-02T01:58:01.521Z",
          "wordCount": 587,
          "title": "Explainable nonlinear modelling of multiple time series with invertible neural networks. (arXiv:2107.00391v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1\">Nicol&#xf2; Cesa-Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1\">Vianney Perchet</a>",
          "description": "We introduce a novel theoretical framework for Return On Investment (ROI)\nmaximization in repeated decision-making. Our setting is motivated by the use\ncase of companies that regularly receive proposals for technological\ninnovations and want to quickly decide whether they are worth implementing. We\ndesign an algorithm for learning ROI-maximizing decision-making policies over a\nsequence of innovation proposals. Our algorithm provably converges to an\noptimal policy in class $\\Pi$ at a rate of order\n$\\min\\big\\{1/(N\\Delta^2),N^{-1/3}\\}$, where $N$ is the number of innovations\nand $\\Delta$ is the suboptimality gap in $\\Pi$. A significant hurdle of our\nformulation, which sets it aside from other online learning problems such as\nbandits, is that running a policy does not provide an unbiased estimate of its\nperformance.",
          "link": "http://arxiv.org/abs/1905.11797",
          "publishedOn": "2021-07-02T01:58:01.503Z",
          "wordCount": 619,
          "title": "A New Theoretical Framework for Fast and Accurate Online Decision-Making. (arXiv:1905.11797v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00102",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Malenica_I/0/1/0/all/0/1\">Ivana Malenica</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bibaut_A/0/1/0/all/0/1\">Aurelien Bibaut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Laan_M/0/1/0/all/0/1\">Mark J. van der Laan</a>",
          "description": "The current work is motivated by the need for robust statistical methods for\nprecision medicine; as such, we address the need for statistical methods that\nprovide actionable inference for a single unit at any point in time. We aim to\nlearn an optimal, unknown choice of the controlled components of the design in\norder to optimize the expected outcome; with that, we adapt the randomization\nmechanism for future time-point experiments based on the data collected on the\nindividual over time. Our results demonstrate that one can learn the optimal\nrule based on a single sample, and thereby adjust the design at any point t\nwith valid inference for the mean target parameter. This work provides several\ncontributions to the field of statistical precision medicine. First, we define\na general class of averages of conditional causal parameters defined by the\ncurrent context for the single unit time-series data. We define a nonparametric\nmodel for the probability distribution of the time-series under few\nassumptions, and aim to fully utilize the sequential randomization in the\nestimation procedure via the double robust structure of the efficient influence\ncurve of the proposed target parameter. We present multiple\nexploration-exploitation strategies for assigning treatment, and methods for\nestimating the optimal rule. Lastly, we present the study of the data-adaptive\ninference on the mean under the optimal treatment rule, where the target\nparameter adapts over time in response to the observed context of the\nindividual. Our target parameter is pathwise differentiable with an efficient\ninfluence function that is doubly robust - which makes it easier to estimate\nthan previously proposed variations. We characterize the limit distribution of\nour estimator under a Donsker condition expressed in terms of a notion of\nbracketing entropy adapted to martingale settings.",
          "link": "http://arxiv.org/abs/2102.00102",
          "publishedOn": "2021-07-02T01:58:01.495Z",
          "wordCount": 754,
          "title": "Adaptive Sequential Design for a Single Time-Series. (arXiv:2102.00102v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00534",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Shi_Z/0/1/0/all/0/1\">Zijian Shi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Cartlidge_J/0/1/0/all/0/1\">John Cartlidge</a>",
          "description": "The limit order book (LOB) depicts the fine-grained demand and supply\nrelationship for financial assets and is widely used in market microstructure\nstudies. Nevertheless, the availability and high cost of LOB data restrict its\nwider application. The LOB recreation model (LOBRM) was recently proposed to\nbridge this gap by synthesizing the LOB from trades and quotes (TAQ) data.\nHowever, in the original LOBRM study, there were two limitations: (1)\nexperiments were conducted on a relatively small dataset containing only one\nday of LOB data; and (2) the training and testing were performed in a\nnon-chronological fashion, which essentially re-frames the task as\ninterpolation and potentially introduces lookahead bias. In this study, we\nextend the research on LOBRM and further validate its use in real-world\napplication scenarios. We first advance the workflow of LOBRM by (1) adding a\ntime-weighted z-score standardization for the LOB and (2) substituting the\nordinary differential equation kernel with an exponential decay kernel to lower\ncomputation complexity. Experiments are conducted on the extended LOBSTER\ndataset in a chronological fashion, as it would be used in a real-world\napplication. We find that (1) LOBRM with decay kernel is superior to\ntraditional non-linear models, and module ensembling is effective; (2)\nprediction accuracy is negatively related to the volatility of order volumes\nresting in the LOB; (3) the proposed sparse encoding method for TAQ exhibits\ngood generalization ability and can facilitate manifold tasks; and (4) the\ninfluence of stochastic drift on prediction accuracy can be alleviated by\nincreasing historical samples.",
          "link": "http://arxiv.org/abs/2107.00534",
          "publishedOn": "2021-07-02T01:58:01.489Z",
          "wordCount": 718,
          "title": "The Limit Order Book Recreation Model (LOBRM): An Extended Analysis. (arXiv:2107.00534v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Notin_P/0/1/0/all/0/1\">Pascal Notin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Optimization in the latent space of variational autoencoders is a promising\napproach to generate high-dimensional discrete objects that maximize an\nexpensive black-box property (e.g., drug-likeness in molecular generation,\nfunction approximation with arithmetic expressions). However, existing methods\nlack robustness as they may decide to explore areas of the latent space for\nwhich no data was available during training and where the decoder can be\nunreliable, leading to the generation of unrealistic or invalid objects. We\npropose to leverage the epistemic uncertainty of the decoder to guide the\noptimization process. This is not trivial though, as a naive estimation of\nuncertainty in the high-dimensional and structured settings we consider would\nresult in high estimator variance. To solve this problem, we introduce an\nimportance sampling-based estimator that provides more robust estimates of\nepistemic uncertainty. Our uncertainty-guided optimization approach does not\nrequire modifications of the model architecture nor the training process. It\nproduces samples with a better trade-off between black-box objective and\nvalidity of the generated samples, sometimes improving both simultaneously. We\nillustrate these advantages across several experimental settings in digit\ngeneration, arithmetic expression approximation and molecule generation for\ndrug design.",
          "link": "http://arxiv.org/abs/2107.00096",
          "publishedOn": "2021-07-02T01:58:01.482Z",
          "wordCount": 624,
          "title": "Improving black-box optimization in VAE latent space using decoder uncertainty. (arXiv:2107.00096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "In Byzantine robust distributed or federated learning, a central server wants\nto train a machine learning model over data distributed across multiple\nworkers. However, a fraction of these workers may deviate from the prescribed\nalgorithm and send arbitrary messages. While this problem has received\nsignificant attention recently, most current defenses assume that the workers\nhave identical data. For realistic cases when the data across workers are\nheterogeneous (non-iid), we design new attacks which circumvent current\ndefenses, leading to significant loss of performance. We then propose a simple\nresampling scheme that adapts existing robust algorithms to heterogeneous\ndatasets at a negligible computational cost. We also theoretically and\nexperimentally validate our approach, showing that combining resampling with\nexisting robust algorithms is effective against challenging attacks. Our work\nis the first to establish guaranteed convergence for the non-iid Byzantine\nrobust problem under realistic assumptions.",
          "link": "http://arxiv.org/abs/2006.09365",
          "publishedOn": "2021-07-02T01:58:01.475Z",
          "wordCount": 632,
          "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Resampling. (arXiv:2006.09365v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>",
          "description": "Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.",
          "link": "http://arxiv.org/abs/2005.00642",
          "publishedOn": "2021-07-02T01:58:01.452Z",
          "wordCount": 640,
          "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction. (arXiv:2005.00642v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Roy Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1\">Lillian J. Ratliff</a>",
          "description": "As data-driven methods are deployed in real-world settings, the processes\nthat generate the observed data will often react to the decisions of the\nlearner. For example, a data source may have some incentive for the algorithm\nto provide a particular label (e.g. approve a bank loan), and manipulate their\nfeatures accordingly. Work in strategic classification and decision-dependent\ndistributions seeks to characterize the closed-loop behavior of deploying\nlearning algorithms by explicitly considering the effect of the classifier on\nthe underlying data distribution. More recently, works in performative\nprediction seek to classify the closed-loop behavior by considering general\nproperties of the mapping from classifier to data distribution, rather than an\nexplicit form. Building on this notion, we analyze repeated risk minimization\nas the perturbed trajectories of the gradient flows of performative risk\nminimization. We consider the case where there may be multiple local minimizers\nof performative risk, motivated by real world situations where the initial\nconditions may have significant impact on the long-term behavior of the system.\nAs a motivating example, we consider a company whose current employee\ndemographics affect the applicant pool they interview: the initial demographics\nof the company can affect the long-term hiring policies of the company. We\nprovide sufficient conditions to characterize the region of attraction for the\nvarious equilibria in this settings. Additionally, we introduce the notion of\nperformative alignment, which provides a geometric condition on the convergence\nof repeated risk minimization to performative risk minimizers.",
          "link": "http://arxiv.org/abs/2107.00055",
          "publishedOn": "2021-07-02T01:58:01.439Z",
          "wordCount": 684,
          "title": "Which Echo Chamber? Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Warrington_A/0/1/0/all/0/1\">Andrew Warrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavington_J/0/1/0/all/0/1\">J. Wilder Lavington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scibior_A/0/1/0/all/0/1\">Adam &#x15a;cibior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "Policies for partially observed Markov decision processes can be efficiently\nlearned by imitating policies for the corresponding fully observed Markov\ndecision processes. Unfortunately, existing approaches for this kind of\nimitation learning have a serious flaw: the expert does not know what the\ntrainee cannot see, and so may encourage actions that are sub-optimal, even\nunsafe, under partial information. We derive an objective to instead train the\nexpert to maximize the expected reward of the imitating agent policy, and use\nit to construct an efficient algorithm, adaptive asymmetric DAgger (A2D), that\njointly trains the expert and the agent. We show that A2D produces an expert\npolicy that the agent can safely imitate, in turn outperforming policies\nlearned by imitating a fixed expert.",
          "link": "http://arxiv.org/abs/2012.15566",
          "publishedOn": "2021-07-02T01:58:01.422Z",
          "wordCount": 593,
          "title": "Robust Asymmetric Learning in POMDPs. (arXiv:2012.15566v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "In membership/subscriber acquisition and retention, we sometimes need to\nrecommend marketing content for multiple pages in sequence. Different from\ngeneral sequential decision making process, the use cases have a simpler flow\nwhere customers per seeing recommended content on each page can only return\nfeedback as moving forward in the process or dropping from it until a\ntermination state. We refer to this type of problems as sequential decision\nmaking in linear--flow. We propose to formulate the problem as an MDP with\nBandits where Bandits are employed to model the transition probability matrix.\nAt recommendation time, we use Thompson sampling (TS) to sample the transition\nprobabilities and allocate the best series of actions with analytical solution\nthrough exact dynamic programming. The way that we formulate the problem allows\nus to leverage TS's efficiency in balancing exploration and exploitation and\nBandit's convenience in modeling actions' incompatibility. In the simulation\nstudy, we observe the proposed MDP with Bandits algorithm outperforms\nQ-learning with $\\epsilon$-greedy and decreasing $\\epsilon$, independent\nBandits, and interaction Bandits. We also find the proposed algorithm's\nperformance is the most robust to changes in the across-page interdependence\nstrength.",
          "link": "http://arxiv.org/abs/2107.00204",
          "publishedOn": "2021-07-02T01:58:01.413Z",
          "wordCount": 638,
          "title": "Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow. (arXiv:2107.00204v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1\">Hadi Beik-Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1\">S&#xf8;ren Hauberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1\">Georgios Arvanitidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozo_L/0/1/0/all/0/1\">Leonel Rozo</a>",
          "description": "For robots to work alongside humans and perform in unstructured environments,\nthey must learn new motion skills and adapt them to unseen situations on the\nfly. This demands learning models that capture relevant motion patterns, while\noffering enough flexibility to adapt the encoded skills to new requirements,\nsuch as dynamic obstacle avoidance. We introduce a Riemannian manifold\nperspective on this problem, and propose to learn a Riemannian manifold from\nhuman demonstrations on which geodesics are natural motion skills. We realize\nthis with a variational autoencoder (VAE) over the space of position and\norientations of the robot end-effector. Geodesic motion skills let a robot plan\nmovements from and to arbitrary points on the data manifold. They also provide\na straightforward method to avoid obstacles by redefining the ambient metric in\nan online fashion. Moreover, geodesics naturally exploit the manifold resulting\nfrom multiple--mode tasks to design motions that were not explicitly\ndemonstrated previously. We test our learning framework using a 7-DoF robotic\nmanipulator, where the robot satisfactorily learns and reproduces realistic\nskills featuring elaborated motion patterns, avoids previously unseen\nobstacles, and generates novel movements in multiple-mode settings.",
          "link": "http://arxiv.org/abs/2106.04315",
          "publishedOn": "2021-07-02T01:58:01.396Z",
          "wordCount": 638,
          "title": "Learning Riemannian Manifolds for Geodesic Motion Skills. (arXiv:2106.04315v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loizou_N/0/1/0/all/0/1\">Nicolas Loizou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1\">Hugo Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1\">Gauthier Gidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1\">Ioannis Mitliagkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "Two of the most prominent algorithms for solving unconstrained smooth games\nare the classical stochastic gradient descent-ascent (SGDA) and the recently\nintroduced stochastic consensus optimization (SCO) (Mescheder et al., 2017).\nSGDA is known to converge to a stationary point for specific classes of games,\nbut current convergence analyses require a bounded variance assumption. SCO is\nused successfully for solving large-scale adversarial problems, but its\nconvergence guarantees are limited to its deterministic variant. In this work,\nwe introduce the expected co-coercivity condition, explain its benefits, and\nprovide the first last-iterate convergence guarantees of SGDA and SCO under\nthis condition for solving a class of stochastic variational inequality\nproblems that are potentially non-monotone. We prove linear convergence of both\nmethods to a neighborhood of the solution when they use constant step-size, and\nwe propose insightful stepsize-switching rules to guarantee convergence to the\nexact solution. In addition, our convergence guarantees hold under the\narbitrary sampling paradigm, and as such, we give insights into the complexity\nof minibatching.",
          "link": "http://arxiv.org/abs/2107.00052",
          "publishedOn": "2021-07-02T01:58:01.389Z",
          "wordCount": 633,
          "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity. (arXiv:2107.00052v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_N/0/1/0/all/0/1\">Nina Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitri_O/0/1/0/all/0/1\">Omar de Mitri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hang Beom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Windberger_A/0/1/0/all/0/1\">Alexander Windberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>",
          "description": "Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.",
          "link": "http://arxiv.org/abs/2107.00360",
          "publishedOn": "2021-07-02T01:58:01.380Z",
          "wordCount": 606,
          "title": "Towards Measuring Bias in Image Classification. (arXiv:2107.00360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nissani_D/0/1/0/all/0/1\">Daniel N. Nissani</a> (Nissensohn)",
          "description": "Generative neural networks are able to mimic intricate probability\ndistributions such as those of handwritten text, natural images, etc. Since\ntheir inception several models were proposed. The most successful of these were\nbased on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy\n(MMD) relatively complex architectures and schemes. Surprisingly, a very simple\narchitecture (a single feed-forward neural network) in conjunction with an\nobvious optimization goal (Kullback_Leibler divergence) was apparently\noverlooked. This paper demonstrates that such a model (denoted SGN for its\nsimplicity) is able to generate samples visually and quantitatively competitive\nas compared with the fore-mentioned state of the art methods.",
          "link": "http://arxiv.org/abs/2106.09330",
          "publishedOn": "2021-07-02T01:58:01.362Z",
          "wordCount": 540,
          "title": "A Simple Generative Network. (arXiv:2106.09330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM).",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-02T01:58:01.334Z",
          "wordCount": 625,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Ashwinkumar Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1\">Tim Oates</a>",
          "description": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.",
          "link": "http://arxiv.org/abs/2107.00124",
          "publishedOn": "2021-07-02T01:58:01.326Z",
          "wordCount": 555,
          "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment. (arXiv:2107.00124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Garrido_Merchan_E/0/1/0/all/0/1\">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1\">Daniel Hern&#xe1;ndez-Lobato</a>",
          "description": "Real-world problems often involve the optimization of several objectives\nunder multiple constraints. An example is the hyper-parameter tuning problem of\nmachine learning algorithms. In particular, the minimization of the estimation\nof the generalization error of a deep neural network and at the same time the\nminimization of its prediction time. We may also consider as a constraint that\nthe deep neural network must be implemented in a chip with an area below some\nsize. Here, both the objectives and the constraint are black boxes, i.e.,\nfunctions whose analytical expressions are unknown and are expensive to\nevaluate. Bayesian optimization (BO) methodologies have given state-of-the-art\nresults for the optimization of black-boxes. Nevertheless, most BO methods are\nsequential and evaluate the objectives and the constraints at just one input\nlocation, iteratively. Sometimes, however, we may have resources to evaluate\nseveral configurations in parallel. Notwithstanding, no parallel BO method has\nbeen proposed to deal with the optimization of multiple objectives under\nseveral constraints. If the expensive evaluations can be carried out in\nparallel (as when a cluster of computers is available), sequential evaluations\nresult in a waste of resources. This article introduces PPESMOC, Parallel\nPredictive Entropy Search for Multi-objective Bayesian Optimization with\nConstraints, an information-based batch method for the simultaneous\noptimization of multiple expensive-to-evaluate black-box functions under the\npresence of several constraints. Iteratively, PPESMOC selects a batch of input\nlocations at which to evaluate the black-boxes so as to maximally reduce the\nentropy of the Pareto set of the optimization problem. We present empirical\nevidence in the form of synthetic, benchmark and real-world experiments that\nillustrate the effectiveness of PPESMOC.",
          "link": "http://arxiv.org/abs/2004.00601",
          "publishedOn": "2021-07-02T01:58:01.319Z",
          "wordCount": 725,
          "title": "Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints. (arXiv:2004.00601v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shiji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qizhe Zhang</a>",
          "description": "The neural network with $1$-Lipschitz property based on $\\ell_\\infty$-dist\nneuron has a theoretical guarantee in certified $\\ell_\\infty$ robustness.\nHowever, due to the inherent difficulties in the training of the network, the\ncertified accuracy of previous work is limited. In this paper, we propose two\napproaches to deal with these difficuties. Aiming at the characteristics of the\ntraining process based on $\\ell_\\infty$-norm neural network, we introduce the\nEMA method to improve the training process. Considering the randomness of the\ntraining algorithm, we propose an ensemble method based on trained base models\nthat have the $1$-Lipschitz property and gain significant improvement in the\nsmall parameter network. Moreover, we give the theoretical analysis of the\nensemble method based on the $1$-Lipschitz property on the certified\nrobustness, which ensures the effectiveness and stability of the algorithm. Our\ncode is available at\nhttps://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.",
          "link": "http://arxiv.org/abs/2107.00230",
          "publishedOn": "2021-07-02T01:58:01.311Z",
          "wordCount": 572,
          "title": "Boosting Certified $\\ell_\\infty$ Robustness with EMA Method and Ensemble Model. (arXiv:2107.00230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_G/0/1/0/all/0/1\">Gaurab Bhattacharya</a>",
          "description": "In recent times, the trend in very large scale integration (VLSI) industry is\nmulti-dimensional, for example, reduction of energy consumption, occupancy of\nless space, precise result, less power dissipation, faster response. To meet\nthese needs, the hardware architecture should be reliable and robust to these\nproblems. Recently, neural network and deep learning has been started to impact\nthe present research paradigm significantly which consists of parameters in the\norder of millions, nonlinear function for activation, convolutional operation\nfor feature extraction, regression for classification, generative adversarial\nnetworks. These operations involve huge calculation and memory overhead.\nPresently available DSP processors are incapable of performing these operations\nand they mostly face the problems, for example, memory overhead, performance\ndrop and compromised accuracy. Moreover, if a huge silicon area is powered to\naccelerate the operation using parallel computation, the ICs will be having\nsignificant chance of burning out due to the considerable generation of heat.\nHence, novel dark silicon constraint is developed to reduce the heat\ndissipation without sacrificing the accuracy. Similarly, different algorithms\nhave been adapted to design a DSP processor compatible for fast performance in\nneural network, activation function, convolutional neural network and\ngenerative adversarial network. In this review, we illustrate the recent\ndevelopments in hardware for accelerating the efficient implementation of deep\nlearning networks with enhanced performance. The techniques investigated in\nthis review are expected to direct future research challenges of hardware\noptimization for high-performance computations.",
          "link": "http://arxiv.org/abs/2107.00092",
          "publishedOn": "2021-07-02T01:58:01.287Z",
          "wordCount": 671,
          "title": "From DNNs to GANs: Review of efficient hardware architectures for deep learning. (arXiv:2107.00092v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, traditional learning over patch-wise features using\nconvolutional neural networks limits the model when attempting to capture\nglobal contextual information. The phenotypical and topological distribution of\nconstituent histological entities play a critical role in tissue diagnosis. As\nsuch, graph data representations and deep learning have attracted significant\nattention for encoding tissue representations, and capturing intra- and inter-\nentity level interactions. In this review, we provide a conceptual grounding of\ngraph-based deep learning and discuss its current success for tumor\nlocalization and classification, tumor invasion and staging, image retrieval,\nand survival prediction. We provide an overview of these methods in a\nsystematic manner organized by the graph representation of the input image\nincluding whole slide images and tissue microarrays. We also outline the\nlimitations of existing techniques, and suggest potential future advances in\nthis domain.",
          "link": "http://arxiv.org/abs/2107.00272",
          "publishedOn": "2021-07-02T01:58:01.185Z",
          "wordCount": 617,
          "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Program synthesis from input-output examples has been a long-standing\nchallenge, and recent works have demonstrated some success in designing deep\nneural networks for program synthesis. However, existing efforts in\ninput-output neural program synthesis have been focusing on domain-specific\nlanguages, thus the applicability of previous approaches to synthesize code in\nfull-fledged popular programming languages, such as C, remains a question. The\nmain challenges lie in two folds. On the one hand, the program search space\ngrows exponentially when the syntax and semantics of the programming language\nbecome more complex, which poses higher requirements on the synthesis\nalgorithm. On the other hand, increasing the complexity of the programming\nlanguage also imposes more difficulties on data collection, since building a\nlarge-scale training set for input-output program synthesis require random\nprogram generators to sample programs and input-output examples. In this work,\nwe take the first step to synthesize C programs from input-output examples. In\nparticular, we propose LaSynth, which learns the latent representation to\napproximate the execution of partially generated programs, even if their\nsemantics are not well-defined. We demonstrate the possibility of synthesizing\nelementary C code from input-output examples, and leveraging learned execution\nsignificantly improves the prediction performance over existing approaches.\nMeanwhile, compared to the randomly generated ground-truth programs, LaSynth\nsynthesizes more concise programs that resemble human-written code. We show\nthat training on these synthesized programs further improves the prediction\nperformance for both Karel and C program synthesis, indicating the promise of\nleveraging the learned program synthesizer to improve the dataset quality for\ninput-output program synthesis.",
          "link": "http://arxiv.org/abs/2107.00101",
          "publishedOn": "2021-07-02T01:58:01.152Z",
          "wordCount": 691,
          "title": "Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages. (arXiv:2107.00101v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>",
          "description": "We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.",
          "link": "http://arxiv.org/abs/2107.00228",
          "publishedOn": "2021-07-02T01:58:01.137Z",
          "wordCount": 545,
          "title": "Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Training deep neural networks with an $L_0$ regularization is one of the\nprominent approaches for network pruning or sparsification. The method prunes\nthe network during training by encouraging weights to become exactly zero.\nHowever, recent work of Gale et al. reveals that although this method yields\nhigh compression rates on smaller datasets, it performs inconsistently on\nlarge-scale learning tasks, such as ResNet50 on ImageNet. We analyze this\nphenomenon through the lens of variational inference and find that it is likely\ndue to the independent modeling of binary gates, the mean-field approximation,\nwhich is known in Bayesian statistics for its poor performance due to the crude\napproximation. To mitigate this deficiency, we propose a dependency modeling of\nbinary gates, which can be modeled effectively as a multi-layer perceptron\n(MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a\ndependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10,\nCIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$\noutperforms the original $L_0$-HC algorithm of Louizos et al. by a significant\nmargin, especially on ImageNet. Compared with the state-of-the-arts network\nsparsification algorithms, our dependency modeling makes the $L_0$-based\nsparsification once again very competitive on large-scale learning tasks. Our\nsource code is available at https://github.com/leo-yangli/dep-l0.",
          "link": "http://arxiv.org/abs/2107.00070",
          "publishedOn": "2021-07-02T01:58:01.131Z",
          "wordCount": 646,
          "title": "Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency Modeling. (arXiv:2107.00070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00195",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_W/0/1/0/all/0/1\">Wei-Ming Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "In quantum and quantum-inspired machine learning, the very first step is to\nembed the data in quantum space known as Hilbert space. Developing quantum\nkernel function (QKF), which defines the distances among the samples in the\nHilbert space, belongs to the fundamental topics for machine learning. In this\nwork, we propose the rescaled logarithmic fidelity (RLF) and a non-parametric\nactive learning in the quantum space, which we name as RLF-NAL. The rescaling\ntakes advantage of the non-linearity of the kernel to tune the mutual distances\nof samples in the Hilbert space, and meanwhile avoids the exponentially-small\nfidelities between quantum many-qubit states. We compare RLF-NAL with several\nwell-known non-parametric algorithms including naive Bayes classifiers,\n$k$-nearest neighbors, and spectral clustering. Our method exhibits excellent\naccuracy particularly for the unsupervised case with no labeled samples and the\nfew-shot cases with small numbers of labeled samples. With the visualizations\nby t-SNE, our results imply that the machine learning in the Hilbert space\ncomplies with the principles of maximal coding rate reduction, where the\nlow-dimensional data exhibit within-class compressibility, between-class\ndiscrimination, and overall diversity. Our proposals can be applied to other\nquantum and quantum-inspired machine learning, including the methods using the\nparametric models such as tensor networks, quantum circuits, and quantum neural\nnetworks.",
          "link": "http://arxiv.org/abs/2107.00195",
          "publishedOn": "2021-07-02T01:58:01.110Z",
          "wordCount": 654,
          "title": "Non-parametric Active Learning and Rate Reduction in Many-body Hilbert Space with Rescaled Logarithmic Fidelity. (arXiv:2107.00195v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_J/0/1/0/all/0/1\">Jonathan Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>",
          "description": "Gaussian processes remain popular as a flexible and expressive model class,\nbut the computational cost of kernel hyperparameter optimization stands as a\nmajor limiting factor to their scaling and broader adoption. Recent work has\nmade great strides combining stochastic estimation with iterative numerical\ntechniques, essentially boiling down GP inference to the cost of (many)\nmatrix-vector multiplies. Preconditioning -- a highly effective step for any\niterative method involving matrix-vector multiplication -- can be used to\naccelerate convergence and thus reduce bias in hyperparameter optimization.\nHere, we prove that preconditioning has an additional benefit that has been\npreviously unexplored. It not only reduces the bias of the $\\log$-marginal\nlikelihood estimator and its derivatives, but it also simultaneously can reduce\nvariance at essentially negligible cost. We leverage this result to derive\nsample-efficient algorithms for GP hyperparameter optimization requiring as few\nas $\\mathcal{O}(\\log(\\varepsilon^{-1}))$ instead of\n$\\mathcal{O}(\\varepsilon^{-2})$ samples to achieve error $\\varepsilon$. Our\ntheoretical results enable provably efficient and scalable optimization of\nkernel hyperparameters, which we validate empirically on a set of large-scale\nbenchmark problems. There, variance reduction via preconditioning results in an\norder of magnitude speedup in hyperparameter optimization of exact GPs.",
          "link": "http://arxiv.org/abs/2107.00243",
          "publishedOn": "2021-07-02T01:58:01.103Z",
          "wordCount": 632,
          "title": "Reducing the Variance of Gaussian Process Hyperparameter Optimization with Preconditioning. (arXiv:2107.00243v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08775",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Benitez_J/0/1/0/all/0/1\">Jose Antonio Lara Benitez</a>, <a href=\"http://arxiv.org/find/math/1/au:+Toghani_M/0/1/0/all/0/1\">Mohammad Taha Toghani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron Wolfe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We present a novel, practical, and provable approach for solving diagonally\nconstrained semi-definite programming (SDP) problems at scale using accelerated\nnon-convex programming. Our algorithm non-trivially combines acceleration\nmotions from convex optimization with coordinate power iteration and matrix\nfactorization techniques. The algorithm is extremely simple to implement, and\nadds only a single extra hyperparameter -- momentum. We prove that our method\nadmits local linear convergence in the neighborhood of the optimum and always\nconverges to a first-order critical point. Experimentally, we showcase the\nmerits of our method on three major application domains: MaxCut, MaxSAT, and\nMIMO signal detection. In all cases, our methodology provides significant\nspeedups over non-convex and convex SDP solvers -- 5X faster than\nstate-of-the-art non-convex solvers, and 9 to 10^3 X faster than convex SDP\nsolvers -- with comparable or improved solution quality.",
          "link": "http://arxiv.org/abs/2106.08775",
          "publishedOn": "2021-07-02T01:58:01.094Z",
          "wordCount": 607,
          "title": "Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs. (arXiv:2106.08775v1 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Brian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Miaolan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>",
          "description": "Tree ensembles distribute feature importance evenly amongst groups of\ncorrelated features. The average feature ranking of the correlated group is\nsuppressed, which reduces interpretability and complicates feature selection.\nIn this paper we present ControlBurn, a feature selection algorithm that uses a\nweighted LASSO-based feature selection method to prune unnecessary features\nfrom tree ensembles, just as low-intensity fire reduces overgrown vegetation.\nLike the linear LASSO, ControlBurn assigns all the feature importance of a\ncorrelated group of features to a single feature. Moreover, the algorithm is\nefficient and only requires a single training iteration to run, unlike\niterative wrapper-based feature selection methods. We show that ControlBurn\nperforms substantially better than feature selection methods with comparable\ncomputational costs on datasets with correlated features.",
          "link": "http://arxiv.org/abs/2107.00219",
          "publishedOn": "2021-07-02T01:58:01.087Z",
          "wordCount": 552,
          "title": "ControlBurn: Feature Selection by Sparse Forests. (arXiv:2107.00219v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Knowledge Distillation (KD) is a popular technique to transfer knowledge from\na teacher model or ensemble to a student model. Its success is generally\nattributed to the privileged information on similarities/consistency between\nthe class distributions or intermediate feature representations of the teacher\nmodel and the student model. However, directly pushing the student model to\nmimic the probabilities/features of the teacher model to a large extent limits\nthe student model in learning undiscovered knowledge/features. In this paper,\nwe propose a novel inheritance and exploration knowledge distillation framework\n(IE-KD), in which a student model is split into two parts - inheritance and\nexploration. The inheritance part is learned with a similarity loss to transfer\nthe existing learned knowledge from the teacher model to the student model,\nwhile the exploration part is encouraged to learn representations different\nfrom the inherited ones with a dis-similarity loss. Our IE-KD framework is\ngeneric and can be easily combined with existing distillation or mutual\nlearning methods for training deep neural networks. Extensive experiments\ndemonstrate that these two parts can jointly push the student model to learn\nmore diversified and effective representations, and our IE-KD can be a general\ntechnique to improve the student network to achieve SOTA performance.\nFurthermore, by applying our IE-KD to the training of two networks, the\nperformance of both can be improved w.r.t. deep mutual learning. The code and\nmodels of IE-KD will be make publicly available at\nhttps://github.com/yellowtownhz/IE-KD.",
          "link": "http://arxiv.org/abs/2107.00181",
          "publishedOn": "2021-07-02T01:58:01.080Z",
          "wordCount": 693,
          "title": "Revisiting Knowledge Distillation: An Inheritance and Exploration Framework. (arXiv:2107.00181v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hu Ding</a>",
          "description": "In this big data era, we often confront large-scale data in many machine\nlearning tasks. A common approach for dealing with large-scale data is to build\na small summary, {\\em e.g.,} coreset, that can efficiently represent the\noriginal input. However, real-world datasets usually contain outliers and most\nexisting coreset construction methods are not resilient against outliers (in\nparticular, the outliers can be located arbitrarily in the space by an\nadversarial attacker). In this paper, we propose a novel robust coreset method\nfor the {\\em continuous-and-bounded learning} problem (with outliers) which\nincludes a broad range of popular optimization objectives in machine learning,\nlike logistic regression and $ k $-means clustering. Moreover, our robust\ncoreset can be efficiently maintained in fully-dynamic environment. To the best\nof our knowledge, this is the first robust and fully-dynamic coreset\nconstruction method for these optimization problems. We also conduct the\nexperiments to evaluate the effectiveness of our robust coreset in practice.",
          "link": "http://arxiv.org/abs/2107.00068",
          "publishedOn": "2021-07-02T01:58:01.053Z",
          "wordCount": 595,
          "title": "Robust Coreset for Continuous-and-Bounded Learning (with Outliers). (arXiv:2107.00068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1\">Hadi Beik-Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleintinger_B/0/1/0/all/0/1\">Benedikt Pleintinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hulin_T/0/1/0/all/0/1\">Thomas Hulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisich_P/0/1/0/all/0/1\">Philipp Reisich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Annika Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_A/0/1/0/all/0/1\">Aaron Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lii_N/0/1/0/all/0/1\">Neal Y. Lii</a>",
          "description": "Telerobotic systems must adapt to new environmental conditions and deal with\nhigh uncertainty caused by long-time delays. As one of the best alternatives to\nhuman-level intelligence, Reinforcement Learning (RL) may offer a solution to\ncope with these issues. This paper proposes to integrate RL with the Model\nMediated Teleoperation (MMT) concept. The teleoperator interacts with a\nsimulated virtual environment, which provides instant feedback. Whereas\nfeedback from the real environment is delayed, feedback from the model is\ninstantaneous, leading to high transparency. The MMT is realized in combination\nwith an intelligent system with two layers. The first layer utilizes Dynamic\nMovement Primitives (DMP) which accounts for certain changes in the avatar\nenvironment. And, the second layer addresses the problems caused by uncertainty\nin the model using RL methods. Augmented reality was also provided to fuse the\navatar device and virtual environment models for the teleoperator. Implemented\non DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods\nare able to find different solutions when changes are applied to the object\nposition after the demonstration. The results also show DMPs to be effective at\nadapting to new conditions where there is no uncertainty involved.",
          "link": "http://arxiv.org/abs/2107.00359",
          "publishedOn": "2021-07-02T01:58:01.046Z",
          "wordCount": 652,
          "title": "Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time Delays Using Reinforcement Learning. (arXiv:2107.00359v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00283",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Detection of colon polyps has become a trending topic in the intersecting\nfields of machine learning and gastrointestinal endoscopy. The focus has mainly\nbeen on per-frame classification. More recently, polyp segmentation has gained\nattention in the medical community. Segmentation has the advantage of being\nmore accurate than per-frame classification or object detection as it can show\nthe affected area in greater detail. For our contribution to the EndoCV 2021\nsegmentation challenge, we propose two separate approaches. First, a\nsegmentation model named TriUNet composed of three separate UNet models.\nSecond, we combine TriUNet with an ensemble of well-known segmentation models,\nnamely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called\nDivergentNets to produce more generalizable medical image segmentation masks.\nIn addition, we propose a modified Dice loss that calculates loss only for a\nsingle class when performing multiclass segmentation, forcing the model to\nfocus on what is most important. Overall, the proposed methods achieved the\nbest average scores for each respective round in the challenge, with TriUNet\nbeing the winning model in Round I and DivergentNets being the winning model in\nRound II of the segmentation generalization challenge at EndoCV 2021. The\nimplementation of our approach is made publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2107.00283",
          "publishedOn": "2021-07-02T01:58:01.038Z",
          "wordCount": 691,
          "title": "DivergentNets: Medical Image Segmentation by Network Ensemble. (arXiv:2107.00283v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Ching-Seh Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Keystroke dynamics can be used to analyze the way that users type by\nmeasuring various aspects of keyboard input. Previous work has demonstrated the\nfeasibility of user authentication and identification utilizing keystroke\ndynamics. In this research, we consider a wide variety of machine learning and\ndeep learning techniques based on fixed-text keystroke-derived features, we\noptimize the resulting models, and we compare our results to those obtained in\nrelated research. We find that models based on extreme gradient boosting\n(XGBoost) and multi-layer perceptrons (MLP)perform well in our experiments. Our\nbest models outperform previous comparable research.",
          "link": "http://arxiv.org/abs/2107.00507",
          "publishedOn": "2021-07-02T01:58:01.030Z",
          "wordCount": 525,
          "title": "Machine Learning and Deep Learning for Fixed-Text Keystroke Dynamics. (arXiv:2107.00507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chane_Sane_E/0/1/0/all/0/1\">Elliot Chane-Sane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>",
          "description": "Goal-conditioned reinforcement learning endows an agent with a large variety\nof skills, but it often struggles to solve tasks that require more temporally\nextended reasoning. In this work, we propose to incorporate imagined subgoals\ninto policy learning to facilitate learning of complex tasks. Imagined subgoals\nare predicted by a separate high-level policy, which is trained simultaneously\nwith the policy and its critic. This high-level policy predicts intermediate\nstates halfway to the goal using the value function as a reachability metric.\nWe don't require the policy to reach these subgoals explicitly. Instead, we use\nthem to define a prior policy, and incorporate this prior into a KL-constrained\npolicy iteration scheme to speed up and regularize learning. Imagined subgoals\nare used during policy learning, but not during test time, where we only apply\nthe learned policy. We evaluate our approach on complex robotic navigation and\nmanipulation tasks and show that it outperforms existing methods by a large\nmargin.",
          "link": "http://arxiv.org/abs/2107.00541",
          "publishedOn": "2021-07-02T01:58:01.011Z",
          "wordCount": 593,
          "title": "Goal-Conditioned Reinforcement Learning with Imagined Subgoals. (arXiv:2107.00541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1\">Alireza Mousavi Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouei_A/0/1/0/all/0/1\">Amir Mohammad Abouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>",
          "description": "Adversarial training tends to result in models that are less accurate on\nnatural (unperturbed) examples compared to standard models. This can be\nattributed to either an algorithmic shortcoming or a fundamental property of\nthe training data distribution, which admits different solutions for optimal\nstandard and adversarial classifiers. In this work, we focus on the latter case\nunder a binary Gaussian mixture classification problem. Unlike earlier work, we\naim to derive the natural accuracy gap between the optimal Bayes and\nadversarial classifiers, and study the effect of different distributional\nparameters, namely separation between class centroids, class proportions, and\nthe covariance matrix, on the derived gap. We show that under certain\nconditions, the natural error of the optimal adversarial classifier, as well as\nthe gap, are locally minimized when classes are balanced, contradicting the\nperformance of the Bayes classifier where perfect balance induces the worst\naccuracy. Moreover, we show that with an $\\ell_\\infty$ bounded perturbation and\nan adversarial budget of $\\epsilon$, this gap is $\\Theta(\\epsilon^2)$ for the\nworst-case parameters, which for suitably small $\\epsilon$ indicates the\ntheoretical possibility of achieving robust classifiers with near-perfect\naccuracy, which is rarely reflected in practical algorithms.",
          "link": "http://arxiv.org/abs/2107.00247",
          "publishedOn": "2021-07-02T01:58:01.004Z",
          "wordCount": 640,
          "title": "The Interplay between Distribution Parameters and the Accuracy-Robustness Tradeoff in Classification. (arXiv:2107.00247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have achieved impressive performance in various biomedical\napplications. Most existing methods tend to define the adjacency matrix among\nsamples manually based on meta-features, and then obtain the node embeddings\nfor downstream tasks by Graph Representation Learning (GRL). However, it is not\neasy for these approaches to generalize to unseen samples. Meanwhile, the\ncomplex correlation between modalities is also ignored. As a result, these\nfactors inevitably yield the inadequacy of providing valid information about\nthe patient's condition for a reliable diagnosis. In this paper, we propose an\nend-to-end Multimodal Graph Learning framework (MMGL) for disease prediction.\nTo effectively exploit the rich information across multi-modality associated\nwith diseases, amodal-attentional multi-modal fusion is proposed to integrate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the adjacency matrix\nmanually as existing methods, the latent graph structure can be captured\nthrough a novel way of adaptive graph learning. It could be jointly optimized\nwith the prediction model, thus revealing the intrinsic connections among\nsamples. Unlike the previous transductive methods, our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction problems is then carefully designed\nand presented, demonstrating that MMGL obtains more favorable performances. In\naddition, we also visualize and analyze the learned graph structure to provide\nmore reliable decision support for doctors in real medical applications and\ninspiration for disease research.",
          "link": "http://arxiv.org/abs/2107.00206",
          "publishedOn": "2021-07-02T01:58:00.993Z",
          "wordCount": 693,
          "title": "Multi-modal Graph Learning for Disease Prediction. (arXiv:2107.00206v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James T. Kwok</a>",
          "description": "Transformer-based models are popularly used in natural language processing\n(NLP). Its core component, self-attention, has aroused widespread interest. To\nunderstand the self-attention mechanism, a direct method is to visualize the\nattention map of a pre-trained model. Based on the patterns observed, a series\nof efficient Transformers with different sparse attention masks have been\nproposed. From a theoretical perspective, universal approximability of\nTransformer-based models is also recently proved. However, the above\nunderstanding and analysis of self-attention is based on a pre-trained model.\nTo rethink the importance analysis in self-attention, we study the significance\nof different positions in attention matrix during pre-training. A surprising\nresult is that diagonal elements in the attention map are the least important\ncompared with other attention positions. We provide a proof showing that these\ndiagonal elements can indeed be removed without deteriorating model\nperformance. Furthermore, we propose a Differentiable Attention Mask (DAM)\nalgorithm, which further guides the design of the SparseBERT. Extensive\nexperiments verify our interesting findings and illustrate the effect of the\nproposed algorithm.",
          "link": "http://arxiv.org/abs/2102.12871",
          "publishedOn": "2021-07-02T01:58:00.986Z",
          "wordCount": 648,
          "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention. (arXiv:2102.12871v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.10178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Pushpendu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_J/0/1/0/all/0/1\">Jajati Keshari Sahoo</a>",
          "description": "We employ both random forests and LSTM networks (more precisely CuDNNLSTM) as\ntraining methodologies to analyze their effectiveness in forecasting\nout-of-sample directional movements of constituent stocks of the S&P 500 from\nJanuary 1993 till December 2018 for intraday trading. We introduce a\nmulti-feature setting consisting not only of the returns with respect to the\nclosing prices, but also with respect to the opening prices and intraday\nreturns. As trading strategy, we use Krauss et al. (2017) and Fischer & Krauss\n(2018) as benchmark. On each trading day, we buy the 10 stocks with the highest\nprobability and sell short the 10 stocks with the lowest probability to\noutperform the market in terms of intraday returns -- all with equal monetary\nweight. Our empirical results show that the multi-feature setting provides a\ndaily return, prior to transaction costs, of 0.64% using LSTM networks, and\n0.54% using random forests. Hence we outperform the single-feature setting in\nFischer & Krauss (2018) and Krauss et al. (2017) consisting only of the daily\nreturns with respect to the closing prices, having corresponding daily returns\nof 0.41% and of 0.39% with respect to LSTM and random forests, respectively.",
          "link": "http://arxiv.org/abs/2004.10178",
          "publishedOn": "2021-07-02T01:58:00.979Z",
          "wordCount": 672,
          "title": "Forecasting directional movements of stock prices for intraday trading using LSTM and random forests. (arXiv:2004.10178v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1\">Juan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1\">Bowei Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamhoua_C/0/1/0/all/0/1\">Charles Kamhoua</a>",
          "description": "Deep neural network (DNN) is a popular model implemented in many systems to\nhandle complex tasks such as image classification, object recognition, natural\nlanguage processing etc. Consequently DNN structural vulnerabilities become\npart of the security vulnerabilities in those systems. In this paper we study\nthe root cause of DNN adversarial examples. We examine the DNN response surface\nto understand its classification boundary. Our study reveals the structural\nproblem of DNN classification boundary that leads to the adversarial examples.\nExisting attack algorithms can generate from a handful to a few hundred\nadversarial examples given one clean image. We show there are infinitely many\nadversarial images given one clean sample, all within a small neighborhood of\nthe clean sample. We then define DNN uncertainty regions and show\ntransferability of adversarial examples is not universal. We also argue that\ngeneralization error, the large sample theoretical guarantee established for\nDNN, cannot adequately capture the phenomenon of adversarial examples. We need\nnew theory to measure DNN robustness.",
          "link": "http://arxiv.org/abs/2107.00003",
          "publishedOn": "2021-07-02T01:58:00.955Z",
          "wordCount": 597,
          "title": "Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions. (arXiv:2107.00003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_Hernandez_A/0/1/0/all/0/1\">Alejandro Morales-Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1\">Agnieszka Jastrzebska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoof_K/0/1/0/all/0/1\">Koen Vanhoof</a>",
          "description": "Forecasting windmill time series is often the basis of other processes such\nas anomaly detection, health monitoring, or maintenance scheduling. The amount\nof data generated on windmill farms makes online learning the most viable\nstrategy to follow. Such settings require retraining the model each time a new\nbatch of data is available. However, update the model with the new information\nis often very expensive to perform using traditional Recurrent Neural Networks\n(RNNs). In this paper, we use Long Short-term Cognitive Networks (LSTCNs) to\nforecast windmill time series in online settings. These recently introduced\nneural systems consist of chained Short-term Cognitive Network blocks, each\nprocessing a temporal data chunk. The learning algorithm of these blocks is\nbased on a very fast, deterministic learning rule that makes LSTCNs suitable\nfor online learning tasks. The numerical simulations using a case study with\nfour windmills showed that our approach reported the lowest forecasting errors\nwith respect to a simple RNN, a Long Short-term Memory, a Gated Recurrent Unit,\nand a Hidden Markov Model. What is perhaps more important is that the LSTCN\napproach is significantly faster than these state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00425",
          "publishedOn": "2021-07-02T01:58:00.942Z",
          "wordCount": 627,
          "title": "Online learning of windmill time series using Long Short-term Cognitive Networks. (arXiv:2107.00425v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.09693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zeyi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaomin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "Federated learning has been a hot research topic in enabling the\ncollaborative training of machine learning models among different organizations\nunder the privacy restrictions. As researchers try to support more machine\nlearning models with different privacy-preserving approaches, there is a\nrequirement in developing systems and infrastructures to ease the development\nof various federated learning algorithms. Similar to deep learning systems such\nas PyTorch and TensorFlow that boost the development of deep learning,\nfederated learning systems (FLSs) are equivalently important, and face\nchallenges from various aspects such as effectiveness, efficiency, and privacy.\nIn this survey, we conduct a comprehensive review on federated learning\nsystems. To achieve smooth flow and guide future research, we introduce the\ndefinition of federated learning systems and analyze the system components.\nMoreover, we provide a thorough categorization for federated learning systems\naccording to six different aspects, including data distribution, machine\nlearning model, privacy mechanism, communication architecture, scale of\nfederation and motivation of federation. The categorization can help the design\nof federated learning systems as shown in our case studies. By systematically\nsummarizing the existing federated learning systems, we present the design\nfactors, case studies, and future research opportunities.",
          "link": "http://arxiv.org/abs/1907.09693",
          "publishedOn": "2021-07-02T01:58:00.934Z",
          "wordCount": 720,
          "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection. (arXiv:1907.09693v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.11331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_M/0/1/0/all/0/1\">Masahiro Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuho Watanabe</a>",
          "description": "DP-means clustering was obtained as an extension of $K$-means clustering.\nWhile it is implemented with a simple and efficient algorithm, it can estimate\nthe number of clusters simultaneously. However, DP-means is specifically\ndesigned for the average distortion measure. Therefore, it is vulnerable to\noutliers in data, and can cause large maximum distortion in clusters. In this\nwork, we extend the objective function of the DP-means to $f$-separable\ndistortion measures and propose a unified learning algorithm to overcome the\nabove problems by selecting the function $f$. Further, the influence function\nof the estimated cluster center is analyzed to evaluate the robustness against\noutliers. We demonstrate the performance of the generalized method by numerical\nexperiments using real datasets.",
          "link": "http://arxiv.org/abs/1901.11331",
          "publishedOn": "2021-07-02T01:58:00.925Z",
          "wordCount": 586,
          "title": "Generalized Dirichlet-process-means for $f$-separable distortion measures. (arXiv:1901.11331v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent advance in deep offline reinforcement learning (RL) has made it\npossible to train strong robotic agents from offline datasets. However,\ndepending on the quality of the trained agents and the application being\nconsidered, it is often desirable to fine-tune such agents via further online\ninteractions. In this paper, we observe that state-action distribution shift\nmay lead to severe bootstrap error during fine-tuning, which destroys the good\ninitial policy obtained via offline RL. To address this issue, we first propose\na balanced replay scheme that prioritizes samples encountered online while also\nencouraging the use of near-on-policy samples from the offline dataset.\nFurthermore, we leverage multiple Q-functions trained pessimistically offline,\nthereby preventing overoptimism concerning unfamiliar actions at novel states\nduring the initial training phase. We show that the proposed method improves\nsample-efficiency and final performance of the fine-tuned robotic agents on\nvarious locomotion and manipulation tasks. Our code is available at:\nhttps://github.com/shlee94/Off2OnRL.",
          "link": "http://arxiv.org/abs/2107.00591",
          "publishedOn": "2021-07-02T01:58:00.917Z",
          "wordCount": 589,
          "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble. (arXiv:2107.00591v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/1910.11390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Tiered graph autoencoders provide the architecture and mechanisms for\nlearning tiered latent representations and latent spaces for molecular graphs\nthat explicitly represent and utilize groups (e.g., functional groups). This\nenables the utilization and exploration of tiered molecular latent spaces,\neither individually - the node (atom) tier, the group tier, or the graph\n(molecule) tier - or jointly, as well as navigation across the tiers. In this\npaper, we discuss the use of tiered graph autoencoders together with graph\nprediction for molecular graphs. We show features of molecular graphs used, and\ngroups in molecular graphs identified for some sample molecules. We briefly\nreview graph prediction and the QM9 dataset for background information, and\ndiscuss the use of tiered graph embeddings for graph prediction, particularly\nweighted group pooling. We find that functional groups and ring groups\neffectively capture and represent the chemical essence of molecular graphs\n(structures). Further, tiered graph autoencoders and graph prediction together\nprovide effective, efficient and interpretable deep learning for molecular\ngraphs, with the former providing unsupervised, transferable learning and the\nlatter providing supervised, task-optimized learning.",
          "link": "http://arxiv.org/abs/1910.11390",
          "publishedOn": "2021-07-02T01:58:00.898Z",
          "wordCount": 652,
          "title": "Deep Learning for Molecular Graphs with Tiered Graph Autoencoders and Graph Prediction. (arXiv:1910.11390v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "Momentum is a widely used technique for gradient-based optimizers in deep\nlearning. In this paper, we propose a decaying momentum (\\textsc{Demon}) rule.\nWe conduct the first large-scale empirical analysis of momentum decay methods\nfor modern neural network optimization, in addition to the most popular\nlearning rate decay schedules. Across 28 relevant combinations of models,\nepochs, datasets, and optimizers, \\textsc{Demon} achieves the highest number of\nTop-1 and Top-3 finishes at 39\\% and 85\\% respectively, almost doubling the\nsecond-placed learning rate cosine schedule at 17\\% and 60\\%, respectively.\n\\textsc{Demon} also outperforms other widely used schedulers including, but not\nlimited to, the learning rate step schedule, linear schedule, OneCycle\nschedule, and exponential schedule. Compared with the widely used learning rate\nstep schedule, \\textsc{Demon} is observed to be less sensitive to parameter\ntuning, which is critical to training neural networks in practice. Results are\ndemonstrated across a variety of settings and architectures, including image\nclassification, generative models, and language models. \\textsc{Demon} is easy\nto implement, requires no additional tuning, and incurs almost no extra\ncomputational overhead compared to the vanilla counterparts. Code is readily\navailable.",
          "link": "http://arxiv.org/abs/1910.04952",
          "publishedOn": "2021-07-02T01:58:00.891Z",
          "wordCount": 677,
          "title": "Demon: Improved Neural Network Training with Momentum Decay. (arXiv:1910.04952v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While agents trained by Reinforcement Learning (RL) can solve increasingly\nchallenging tasks directly from visual observations, generalizing learned\nskills to novel environments remains very challenging. Extensive use of data\naugmentation is a promising technique for improving generalization in RL, but\nit is often found to decrease sample efficiency and can even lead to\ndivergence. In this paper, we investigate causes of instability when using data\naugmentation in common off-policy RL algorithms. We identify two problems, both\nrooted in high-variance Q-targets. Based on our findings, we propose a simple\nyet effective technique for stabilizing this class of algorithms under\naugmentation. We perform extensive empirical evaluation of image-based RL using\nboth ConvNets and Vision Transformers (ViT) on a family of benchmarks based on\nDeepMind Control Suite, as well as in robotic manipulation tasks. Our method\ngreatly improves stability and sample efficiency of ConvNets under\naugmentation, and achieves generalization results competitive with\nstate-of-the-art methods for image-based RL. We further show that our method\nscales to RL with ViT-based architectures, and that data augmentation may be\nespecially important in this setting.",
          "link": "http://arxiv.org/abs/2107.00644",
          "publishedOn": "2021-07-02T01:58:00.872Z",
          "wordCount": 630,
          "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation. (arXiv:2107.00644v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.03201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yognjin Lee</a>",
          "description": "Deep neural networks have relieved a great deal of burden on human experts in\nrelation to feature engineering. However, comparable efforts are instead\nrequired to determine effective architectures. In addition, as the sizes of\nnetworks have grown overly large, a considerable amount of resources is also\ninvested in reducing the sizes. The sparsification of an over-complete model\naddresses these problems as it removes redundant components and connections. In\nthis study, we propose a fully differentiable sparsification method for deep\nneural networks which allows parameters to be zero during training via\nstochastic gradient descent. Thus, the proposed method can learn the sparsified\nstructure and weights of a network in an end-to-end manner. The method is\ndirectly applicable to various modern deep neural networks and imposes minimum\nmodification to existing models. To the best of our knowledge, this is the\nfirst fully [sub-]differentiable sparsification method that zeroes out\nparameters. It provides a foundation for future structure learning and model\ncompression methods.",
          "link": "http://arxiv.org/abs/1910.03201",
          "publishedOn": "2021-07-02T01:58:00.856Z",
          "wordCount": 635,
          "title": "Differentiable Sparsification for Deep Neural Networks. (arXiv:1910.03201v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00594",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1\">Slim Essid</a>",
          "description": "Through solving pretext tasks, self-supervised learning leverages unlabeled\ndata to extract useful latent representations replacing traditional input\nfeatures in the downstream task. In various application domains, including\ncomputer vision, natural language processing and audio/speech signal\nprocessing, a wide range of features where engineered through decades of\nresearch efforts. As it turns out, learning to predict such features has proven\nto be a particularly relevant pretext task leading to building useful\nself-supervised representations that prove to be effective for downstream\ntasks. However, methods and common practices for combining such pretext tasks,\nwhere each task targets a different group of features for better performance on\nthe downstream task have not been explored and understood properly. In fact,\nthe process relies almost exclusively on a computationally heavy experimental\nprocedure, which becomes intractable with the increase of the number of pretext\ntasks. This paper introduces a method to select a group of pretext tasks among\na set of candidates. The method we propose estimates properly calibrated\nweights for the partial losses corresponding to the considered pretext tasks\nduring the self-supervised training process. The experiments conducted on\nspeaker recognition and automatic speech recognition validate our approach, as\nthe groups selected and weighted with our method perform better than classic\nbaselines, thus facilitating the selection and combination of relevant\npseudo-labels for self-supervised representation learning.",
          "link": "http://arxiv.org/abs/2107.00594",
          "publishedOn": "2021-07-02T01:58:00.849Z",
          "wordCount": 668,
          "title": "Pretext Tasks selection for multitask self-supervised speech representation learning. (arXiv:2107.00594v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pira_G/0/1/0/all/0/1\">Giacomo Pira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Spiking Neural Networks (SNNs), despite being energy-efficient when\nimplemented on neuromorphic hardware and coupled with event-based Dynamic\nVision Sensors (DVS), are vulnerable to security threats, such as adversarial\nattacks, i.e., small perturbations added to the input for inducing a\nmisclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet\nefficient adversarial attack methodologies targeted to perturb the event\nsequences that compose the input of the SNNs. First, we show that noise filters\nfor DVS can be used as defense mechanisms against adversarial attacks.\nAfterwards, we implement several attacks and test them in the presence of two\ntypes of noise filters for DVS cameras. The experimental results show that the\nfilters can only partially defend the SNNs against our proposed DVS-Attacks.\nUsing the best settings for the noise filters, our proposed Mask Filter-Aware\nDash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset\nand by more than 65% on the MNIST dataset, compared to the original clean\nframes. The source code of all the proposed DVS-Attacks and noise filters is\nreleased at https://github.com/albertomarchisio/DVS-Attacks.",
          "link": "http://arxiv.org/abs/2107.00415",
          "publishedOn": "2021-07-02T01:58:00.829Z",
          "wordCount": 633,
          "title": "DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks. (arXiv:2107.00415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Marcel Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>",
          "description": "We have implemented training of neural networks in secure multi-party\ncomputation (MPC) using quantization commonly used in the said setting. To the\nbest of our knowledge, we are the first to present an MNIST classifier purely\ntrained in MPC that comes within 0.2 percent of the accuracy of the same\nconvolutional neural network trained via plaintext computation. More\nconcretely, we have trained a network with two convolution and two dense layers\nto 99.2% accuracy in 25 epochs. This took 3.5 hours in our MPC implementation\n(under one hour for 99% accuracy).",
          "link": "http://arxiv.org/abs/2107.00501",
          "publishedOn": "2021-07-02T01:58:00.823Z",
          "wordCount": 515,
          "title": "Secure Quantized Training for Deep Learning. (arXiv:2107.00501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.05081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1\">Sai Ravela</a>",
          "description": "The use of artificial neural networks as models of chaotic dynamics has been\nrapidly expanding. Still, a theoretical understanding of how neural networks\nlearn chaos is lacking. Here, we employ a geometric perspective to show that\nneural networks can efficiently model chaotic dynamics by becoming structurally\nchaotic themselves. We first confirm neural network's efficiency in emulating\nchaos by showing that a parsimonious neural network trained only on few data\npoints can reconstruct strange attractors, extrapolate outside training data\nboundaries, and accurately predict local divergence rates. We then posit that\nthe trained network's map comprises sequential geometric stretching, rotation,\nand compression operations. These geometric operations indicate topological\nmixing and chaos, explaining why neural networks are naturally suitable to\nemulate chaotic dynamics.",
          "link": "http://arxiv.org/abs/1912.05081",
          "publishedOn": "2021-07-02T01:58:00.814Z",
          "wordCount": 620,
          "title": "Neural Networks as Geometric Chaotic Maps. (arXiv:1912.05081v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_N/0/1/0/all/0/1\">Nikhil Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthiah_S/0/1/0/all/0/1\">Sathappah Muthiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_P/0/1/0/all/0/1\">Patrick Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Manish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burne_K/0/1/0/all/0/1\">Katy Burne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunachalam_P/0/1/0/all/0/1\">Prakash Arunachalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCormick_H/0/1/0/all/0/1\">Hays &#x27;Skip&#x27; McCormick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>",
          "description": "We describe lessons learned from developing and deploying machine learning\nmodels at scale across the enterprise in a range of financial analytics\napplications. These lessons are presented in the form of antipatterns. Just as\ndesign patterns codify best software engineering practices, antipatterns\nprovide a vocabulary to describe defective practices and methodologies. Here we\ncatalog and document numerous antipatterns in financial ML operations (MLOps).\nSome antipatterns are due to technical errors, while others are due to not\nhaving sufficient knowledge of the surrounding context in which ML results are\nused. By providing a common vocabulary to discuss these situations, our intent\nis that antipatterns will support better documentation of issues, rapid\ncommunication between stakeholders, and faster resolution of problems. In\naddition to cataloging antipatterns, we describe solutions, best practices, and\nfuture directions toward MLOps maturity.",
          "link": "http://arxiv.org/abs/2107.00079",
          "publishedOn": "2021-07-02T01:58:00.734Z",
          "wordCount": 574,
          "title": "Using AntiPatterns to avoid MLOps Mistakes. (arXiv:2107.00079v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wonju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Seok-Yong Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jooeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minje Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechil_K/0/1/0/all/0/1\">Kirill Chechil</a>",
          "description": "While many real-world data streams imply that they change frequently in a\nnonstationary way, most of deep learning methods optimize neural networks on\ntraining data, and this leads to severe performance degradation when dataset\nshift happens. However, it is less possible to annotate or inspect newly\nstreamed data by humans, and thus it is desired to measure model drift at\ninference time in an unsupervised manner. In this paper, we propose a novel\nmethod of model drift estimation by exploiting statistics of batch\nnormalization layer on unlabeled test data. To remedy possible sampling error\nof streamed input data, we adopt low-rank approximation to each\nrepresentational layer. We show the effectiveness of our method not only on\ndataset shift detection but also on model selection when there are multiple\ncandidate models among model zoo or training trajectories in an unsupervised\nway. We further demonstrate the consistency of our method by comparing model\ndrift scores between different network architectures.",
          "link": "http://arxiv.org/abs/2107.00191",
          "publishedOn": "2021-07-02T01:58:00.727Z",
          "wordCount": 616,
          "title": "Unsupervised Model Drift Estimation with Batch Normalization Statistics for Dataset Shift Detection and Model Selection. (arXiv:2107.00191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hladik_M/0/1/0/all/0/1\">Michael Hladik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "The use of external background knowledge can be beneficial for the task of\nmatching schemas or ontologies automatically. In this paper, we exploit six\ngeneral-purpose knowledge graphs as sources of background knowledge for the\nmatching task. The background sources are evaluated by applying three different\nexploitation strategies. We find that explicit strategies still outperform\nlatent ones and that the choice of the strategy has a greater impact on the\nfinal alignment than the actual background dataset on which the strategy is\napplied. While we could not identify a universally superior resource, BabelNet\nachieved consistently good results. Our best matcher configuration with\nBabelNet performs very competitively when compared to other matching systems\neven though no dataset-specific optimizations were made.",
          "link": "http://arxiv.org/abs/2107.00001",
          "publishedOn": "2021-07-02T01:58:00.709Z",
          "wordCount": 563,
          "title": "Background Knowledge in Schema Matching: Strategy vs. Data. (arXiv:2107.00001v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00088",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1\">Sean Hooten</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1\">Thomas Van Vaerenbergh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1\">Raymond G. Beausoleil</a>",
          "description": "We present a proof-of-concept technique for the inverse design of\nelectromagnetic devices motivated by the policy gradient method in\nreinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE\nCriteria for Enhanced Design). This technique uses a probabilistic generative\nneural network interfaced with an electromagnetic solver to assist in the\ndesign of photonic devices, such as grating couplers. We show that PHORCED\nobtains better performing grating coupler designs than local gradient-based\ninverse design via the adjoint method, while potentially providing faster\nconvergence over competing state-of-the-art generative methods. Furthermore, we\nimplement transfer learning with PHORCED, demonstrating that a neural network\ntrained to optimize 8$^\\circ$ grating couplers can then be re-trained on\ngrating couplers with alternate scattering angles while requiring >$10\\times$\nfewer simulations than control cases.",
          "link": "http://arxiv.org/abs/2107.00088",
          "publishedOn": "2021-07-02T01:58:00.702Z",
          "wordCount": 574,
          "title": "Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Separating a song into vocal and accompaniment components is an active\nresearch topic, and recent years witnessed an increased performance from\nsupervised training using deep learning techniques. We propose to apply the\nvisual information corresponding to the singers' vocal activities to further\nimprove the quality of the separated vocal signals. The video frontend model\ntakes the input of mouth movement and fuses it into the feature embeddings of\nan audio-based separation framework. To facilitate the network to learn\naudiovisual correlation of singing activities, we add extra vocal signals\nirrelevant to the mouth movement to the audio mixture during training. We\ncreate two audiovisual singing performance datasets for training and\nevaluation, respectively, one curated from audition recordings on the Internet,\nand the other recorded in house. The proposed method outperforms audio-based\nmethods in terms of separation quality on most test recordings. This advantage\nis especially pronounced when there are backing vocals in the accompaniment,\nwhich poses a great challenge for audio-only methods.",
          "link": "http://arxiv.org/abs/2107.00231",
          "publishedOn": "2021-07-02T01:58:00.695Z",
          "wordCount": 589,
          "title": "Audiovisual Singing Voice Separation. (arXiv:2107.00231v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1\">Abolfazl Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We explore methodologies to improve the robustness of generative adversarial\nimitation learning (GAIL) algorithms to observation noise. Towards this\nobjective, we study the effect of local Lipschitzness of the discriminator and\nthe generator on the robustness of policies learned by GAIL. In many robotics\napplications, the learned policies by GAIL typically suffer from a degraded\nperformance at test time since the observations from the environment might be\ncorrupted by noise. Hence, robustifying the learned policies against the\nobservation noise is of critical importance. To this end, we propose a\nregularization method to induce local Lipschitzness in the generator and the\ndiscriminator of adversarial imitation learning methods. We show that the\nmodified objective leads to learning significantly more robust policies.\nMoreover, we demonstrate -- both theoretically and experimentally -- that\ntraining a locally Lipschitz discriminator leads to a locally Lipschitz\ngenerator, thereby improving the robustness of the resultant policy. We perform\nextensive experiments on simulated robot locomotion environments from the\nMuJoCo suite that demonstrate the proposed method learns policies that\nsignificantly outperform the state-of-the-art generative adversarial imitation\nlearning algorithm when applied to test scenarios with noise-corrupted\nobservations.",
          "link": "http://arxiv.org/abs/2107.00116",
          "publishedOn": "2021-07-02T01:58:00.688Z",
          "wordCount": 616,
          "title": "Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00179",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_T/0/1/0/all/0/1\">T. Tony Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wei_H/0/1/0/all/0/1\">Hongji Wei</a>",
          "description": "Distributed minimax estimation and distributed adaptive estimation under\ncommunication constraints for Gaussian sequence model and white noise model are\nstudied. The minimax rate of convergence for distributed estimation over a\ngiven Besov class, which serves as a benchmark for the cost of adaptation, is\nestablished. We then quantify the exact communication cost for adaptation and\nconstruct an optimally adaptive procedure for distributed estimation over a\nrange of Besov classes. The results demonstrate significant differences between\nnonparametric function estimation in the distributed setting and the\nconventional centralized setting. For global estimation, adaptation in general\ncannot be achieved for free in the distributed setting. The new technical tools\nto obtain the exact characterization for the cost of adaptation can be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.00179",
          "publishedOn": "2021-07-02T01:58:00.673Z",
          "wordCount": 574,
          "title": "Distributed Nonparametric Function Estimation: Optimal Rate of Convergence and Cost of Adaptation. (arXiv:2107.00179v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.",
          "link": "http://arxiv.org/abs/2107.00166",
          "publishedOn": "2021-07-02T01:58:00.648Z",
          "wordCount": 620,
          "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Prateek Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1\">Kumar Divya Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johri_P/0/1/0/all/0/1\">Prashant Johri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Dikhsa Arya</a>",
          "description": "Processed data are insightful, and crude data are obtuse. A serious threat to\ndata reliability is missing values. Such data leads to inaccurate analysis and\nwrong predictions. We propose an efficient technique to impute the missing\nvalue in the dataset based on correlation called FCMI (Feature Correlation\nbased Missing Data Imputation). We have considered the correlation of the\nattributes of the dataset, and that is our central idea. Our proposed algorithm\npicks the highly correlated attributes of the dataset and uses these attributes\nto build a regression model whose parameters are optimized such that the\ncorrelation of the dataset is maintained. Experiments conducted on both\nclassification and regression datasets show that the proposed imputation\ntechnique outperforms existing imputation algorithms.",
          "link": "http://arxiv.org/abs/2107.00100",
          "publishedOn": "2021-07-02T01:58:00.637Z",
          "wordCount": 550,
          "title": "FCMI: Feature Correlation based Missing Data Imputation. (arXiv:2107.00100v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.",
          "link": "http://arxiv.org/abs/2107.00080",
          "publishedOn": "2021-07-02T01:58:00.630Z",
          "wordCount": 565,
          "title": "Regressing Location on Text for Probabilistic Geocoding. (arXiv:2107.00080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1\">Tehrim Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sumin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.",
          "link": "http://arxiv.org/abs/2107.00233",
          "publishedOn": "2021-07-02T01:58:00.613Z",
          "wordCount": 604,
          "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning. (arXiv:2107.00233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wanning Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>",
          "description": "Knowledge distillation has caught a lot of attention in Federated Learning\n(FL) recently. It has the advantage for FL to train on heterogeneous clients\nwhich have different data size and data structure. However, data samples across\nall devices are usually not independent and identically distributed\n(non-i.i.d), posing additional challenges to the convergence and speed of\nfederated learning. As FL randomly asks the clients to join the training\nprocess and each client only learns from local non-i.i.d data, which makes\nlearning processing even slower. In order to solve this problem, an intuitive\nidea is using the global model to guide local training. In this paper, we\npropose a novel global knowledge distillation method, named FedGKD, which\nlearns the knowledge from past global models to tackle down the local bias\ntraining problem. By learning from global knowledge and consistent with current\nlocal models, FedGKD learns a global knowledge model in FL. To demonstrate the\neffectiveness of the proposed method, we conduct extensive experiments on\nvarious CV datasets (CIFAR-10/100) and settings (non-i.i.d data). The\nevaluation results show that FedGKD outperforms previous state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.00051",
          "publishedOn": "2021-07-02T01:58:00.598Z",
          "wordCount": 613,
          "title": "Global Knowledge Distillation in Federated Learning. (arXiv:2107.00051v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "In real-world applications, data often come in a growing manner, where the\ndata volume and the number of classes may increase dynamically. This will bring\na critical challenge for learning: given the increasing data volume or the\nnumber of classes, one has to instantaneously adjust the neural model capacity\nto obtain promising performance. Existing methods either ignore the growing\nnature of data or seek to independently search an optimal architecture for a\ngiven dataset, and thus are incapable of promptly adjusting the architectures\nfor the changed data. To address this, we present a neural architecture\nadaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust\nprevious architectures on the growing data. Specifically, we introduce an\narchitecture adjuster to generate a suitable architecture for each data\nsnapshot, based on the previous architecture and the different extent between\ncurrent and previous data distributions. Furthermore, we propose an adaptation\ncondition to determine the necessity of adjustment, thereby avoiding\nunnecessary and time-consuming adjustments. Extensive experiments on two growth\nscenarios (increasing data volume and number of classes) demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.00254",
          "publishedOn": "2021-07-02T01:58:00.589Z",
          "wordCount": 626,
          "title": "AdaXpert: Adapting Neural Architecture for Growing Data. (arXiv:2107.00254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galayko_D/0/1/0/all/0/1\">Dimitri Galayko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trocan_M/0/1/0/all/0/1\">Maria Trocan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawan_M/0/1/0/all/0/1\">Mohamad Sawan</a>",
          "description": "Autoencoders are composed of coding and decoding units, hence they hold the\ninherent potential of high-performance data compression and signal compressed\nsensing. The main disadvantages of current autoencoders comprise the following\nseveral aspects: the research objective is not data reconstruction but feature\nrepresentation; the performance evaluation of data recovery is neglected; it is\nhard to achieve lossless data reconstruction by pure autoencoders, even by pure\ndeep learning. This paper aims for image reconstruction of autoencoders,\nemploys cascade decoders-based autoencoders, perfects the performance of image\nreconstruction, approaches gradually lossless image recovery, and provides\nsolid theory and application basis for autoencoders-based image compression and\ncompressed sensing. The proposed serial decoders-based autoencoders include the\narchitectures of multi-level decoders and the related optimization algorithms.\nThe cascade decoders consist of general decoders, residual decoders,\nadversarial decoders and their combinations. It is evaluated by the\nexperimental results that the proposed autoencoders outperform the classical\nautoencoders in the performance of image reconstruction.",
          "link": "http://arxiv.org/abs/2107.00002",
          "publishedOn": "2021-07-02T01:58:00.574Z",
          "wordCount": 579,
          "title": "Cascade Decoders-Based Autoencoders for Image Reconstruction. (arXiv:2107.00002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning -- in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.",
          "link": "http://arxiv.org/abs/2103.05247",
          "publishedOn": "2021-07-01T01:59:34.732Z",
          "wordCount": 600,
          "title": "Pretrained Transformers as Universal Computation Engines. (arXiv:2103.05247v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06315",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wen_L/0/1/0/all/0/1\">Linjie Wen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Jinglai Li</a>",
          "description": "We propose a linear-mapping based variational Ensemble Kalman filter for\nsequential Bayesian filtering problems with generic observation models.\nSpecifically, the proposed method is formulated as to construct a linear\nmapping from the prior ensemble to the posterior one, and the linear mapping is\ncomputed via a variational Bayesian formulation, i.e., by minimizing the\nKullback-Leibler divergence between the transformed distribution by the linear\nmapping and the actual posterior. A gradient descent scheme is proposed to\nsolve the resulting optimization problem. With numerical examples we\ndemonstrate that the method has competitive performance against existing\nmethods.",
          "link": "http://arxiv.org/abs/2103.06315",
          "publishedOn": "2021-07-01T01:59:34.726Z",
          "wordCount": 550,
          "title": "Linear-Mapping based Variational Ensemble Kalman Filter. (arXiv:2103.06315v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Saurabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>",
          "description": "A rich body of prior work has highlighted the existence of communication\nbottlenecks in synchronous data-parallel training. To alleviate these\nbottlenecks, a long line of recent work proposes gradient and model compression\nmethods. In this work, we evaluate the efficacy of gradient compression methods\nand compare their scalability with optimized implementations of synchronous\ndata-parallel SGD across more than 200 different setups. Surprisingly, we\nobserve that only in 6 cases out of more than 200, gradient compression methods\nprovide speedup over optimized synchronous data-parallel training in the\ntypical data-center setting. We conduct an extensive investigation to identify\nthe root causes of this phenomenon, and offer a performance model that can be\nused to identify the benefits of gradient compression for a variety of system\nsetups. Based on our analysis, we propose a list of desirable properties that\ngradient compression methods should satisfy, in order for them to provide a\nmeaningful end-to-end speedup.",
          "link": "http://arxiv.org/abs/2103.00543",
          "publishedOn": "2021-07-01T01:59:34.720Z",
          "wordCount": 634,
          "title": "On the Utility of Gradient Compression in Distributed Training Systems. (arXiv:2103.00543v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henry_R/0/1/0/all/0/1\">Robin Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "Active network management (ANM) of electricity distribution networks include\nmany complex stochastic sequential optimization problems. These problems need\nto be solved for integrating renewable energies and distributed storage into\nfuture electrical grids. In this work, we introduce Gym-ANM, a framework for\ndesigning reinforcement learning (RL) environments that model ANM tasks in\nelectricity distribution networks. These environments provide new playgrounds\nfor RL research in the management of electricity networks that do not require\nan extensive knowledge of the underlying dynamics of such systems. Along with\nthis work, we are releasing an implementation of an introductory\ntoy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We\nalso show that state-of-the-art RL algorithms can already achieve good\nperformance on ANM6-Easy when compared against a model predictive control (MPC)\napproach. Finally, we provide guidelines to create new Gym-ANM environments\ndiffering in terms of (a) the distribution network topology and parameters, (b)\nthe observation space, (c) the modelling of the stochastic processes present in\nthe system, and (d) a set of hyperparameters influencing the reward signal.\nGym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.",
          "link": "http://arxiv.org/abs/2103.07932",
          "publishedOn": "2021-07-01T01:59:34.714Z",
          "wordCount": 669,
          "title": "Gym-ANM: Reinforcement Learning Environments for Active Network Management Tasks in Electricity Distribution Systems. (arXiv:2103.07932v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:34.708Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "Policy optimization methods are popular reinforcement learning algorithms,\nbecause their incremental and on-policy nature makes them more stable than the\nvalue-based counterparts. However, the same properties also make them slow to\nconverge and sample inefficient, as the on-policy requirement precludes data\nreuse and the incremental updates couple large iteration complexity into the\nsample complexity. These characteristics have been observed in experiments as\nwell as in theory in the recent work of~\\citet{agarwal2020pc}, which provides a\npolicy optimization method PCPG that can robustly find near optimal polices for\napproximately linear Markov decision processes but suffers from an extremely\npoor sample complexity compared with value-based techniques.\n\nIn this paper, we propose a new algorithm, COPOE, that overcomes the sample\ncomplexity issue of PCPG while retaining its robustness to model\nmisspecification. Compared with PCPG, COPOE makes several important algorithmic\nenhancements, such as enabling data reuse, and uses more refined analysis\ntechniques, which we expect to be more broadly applicable to designing new\nreinforcement learning algorithms. The result is an improvement in sample\ncomplexity from $\\widetilde{O}(1/\\epsilon^{11})$ for PCPG to\n$\\widetilde{O}(1/\\epsilon^3)$ for PCPG, nearly bridging the gap with\nvalue-based techniques.",
          "link": "http://arxiv.org/abs/2103.12923",
          "publishedOn": "2021-07-01T01:59:34.692Z",
          "wordCount": 650,
          "title": "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation. (arXiv:2103.12923v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamlath_B/0/1/0/all/0/1\">Buddhima Gamlath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinrui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_A/0/1/0/all/0/1\">Adam Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1\">Ola Svensson</a>",
          "description": "We study the problem of explainable clustering in the setting first\nformalized by Moshkovitz, Dasgupta, Rashtchian, and Frost (ICML 2020). A\n$k$-clustering is said to be explainable if it is given by a decision tree\nwhere each internal node splits data points with a threshold cut in a single\ndimension (feature), and each of the $k$ leaves corresponds to a cluster. We\ngive an algorithm that outputs an explainable clustering that loses at most a\nfactor of $O(\\log^2 k)$ compared to an optimal (not necessarily explainable)\nclustering for the $k$-medians objective, and a factor of $O(k \\log^2 k)$ for\nthe $k$-means objective. This improves over the previous best upper bounds of\n$O(k)$ and $O(k^2)$, respectively, and nearly matches the previous $\\Omega(\\log\nk)$ lower bound for $k$-medians and our new $\\Omega(k)$ lower bound for\n$k$-means. The algorithm is remarkably simple. In particular, given an initial\nnot necessarily explainable clustering in $\\mathbb{R}^d$, it is oblivious to\nthe data points and runs in time $O(dk \\log^2 k)$, independent of the number of\ndata points $n$. Our upper and lower bounds also generalize to objectives given\nby higher $\\ell_p$-norms.",
          "link": "http://arxiv.org/abs/2106.16147",
          "publishedOn": "2021-07-01T01:59:34.686Z",
          "wordCount": 621,
          "title": "Nearly-Tight and Oblivious Algorithms for Explainable Clustering. (arXiv:2106.16147v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "The partially observable Markov decision process (POMDP) is a principled\ngeneral framework for robot decision making under uncertainty, but POMDP\nplanning suffers from high computational complexity, when long-term planning is\nrequired. While temporally-extended macro-actions help to cut down the\neffective planning horizon and significantly improve computational efficiency,\nhow do we acquire good macro-actions? This paper proposes Macro-Action\nGenerator-Critic (MAGIC), which performs offline learning of macro-actions\noptimized for online POMDP planning. Specifically, MAGIC learns a macro-action\ngenerator end-to-end, using an online planner's performance as the feedback.\nDuring online planning, the generator generates on the fly situation-aware\nmacro-actions conditioned on the robot's belief and the environment context. We\nevaluated MAGIC on several long-horizon planning tasks both in simulation and\non a real robot. The experimental results show that the learned macro-actions\noffer significant benefits in online planning performance, compared with\nprimitive actions and handcrafted macro-actions.",
          "link": "http://arxiv.org/abs/2011.03813",
          "publishedOn": "2021-07-01T01:59:34.680Z",
          "wordCount": 622,
          "title": "MAGIC: Learning Macro-Actions for Online POMDP Planning. (arXiv:2011.03813v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_I/0/1/0/all/0/1\">Idan Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We give a new separation result between the generalization performance of\nstochastic gradient descent (SGD) and of full-batch gradient descent (GD) in\nthe fundamental stochastic convex optimization model. While for SGD it is\nwell-known that $O(1/\\epsilon^2)$ iterations suffice for obtaining a solution\nwith $\\epsilon$ excess expected risk, we show that with the same number of\nsteps GD may overfit and emit a solution with $\\Omega(1)$ generalization error.\nMoreover, we show that in fact $\\Omega(1/\\epsilon^4)$ iterations are necessary\nfor GD to match the generalization performance of SGD, which is also tight due\nto recent work by Bassily et al. (2020). We further discuss how regularizing\nthe empirical risk minimized by GD essentially does not change the above\nresult, and revisit the concepts of stability, implicit bias and the role of\nthe learning algorithm in generalization.",
          "link": "http://arxiv.org/abs/2102.01117",
          "publishedOn": "2021-07-01T01:59:34.658Z",
          "wordCount": 604,
          "title": "SGD Generalizes Better Than GD (And Regularization Doesn't Help). (arXiv:2102.01117v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhaka_A/0/1/0/all/0/1\">Akash Kumar Dhaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalina_A/0/1/0/all/0/1\">Alejandro Catalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welandawe_M/0/1/0/all/0/1\">Manushi Welandawe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_M/0/1/0/all/0/1\">Michael Riis Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huggins_J/0/1/0/all/0/1\">Jonathan Huggins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>",
          "description": "Current black-box variational inference (BBVI) methods require the user to\nmake numerous design choices -- such as the selection of variational objective\nand approximating family -- yet there is little principled guidance on how to\ndo so. We develop a conceptual framework and set of experimental tools to\nunderstand the effects of these choices, which we leverage to propose best\npractices for maximizing posterior approximation accuracy. Our approach is\nbased on studying the pre-asymptotic tail behavior of the density ratios\nbetween the joint distribution and the variational approximation, then\nexploiting insights and tools from the importance sampling literature. Our\nframework and supporting experiments help to distinguish between the behavior\nof BBVI methods for approximating low-dimensional versus\nmoderate-to-high-dimensional posteriors. In the latter case, we show that\nmass-covering variational objectives are difficult to optimize and do not\nimprove accuracy, but flexible variational families can improve accuracy and\nthe effectiveness of importance sampling -- at the cost of additional\noptimization challenges. Therefore, for moderate-to-high-dimensional posteriors\nwe recommend using the (mode-seeking) exclusive KL divergence since it is the\neasiest to optimize, and improving the variational family or using model\nparameter transformations to make the posterior and optimal variational\napproximation more similar. On the other hand, in low-dimensional settings, we\nshow that heavy-tailed variational families and mass-covering divergences are\neffective and can increase the chances that the approximation can be improved\nby importance sampling.",
          "link": "http://arxiv.org/abs/2103.01085",
          "publishedOn": "2021-07-01T01:59:34.652Z",
          "wordCount": 704,
          "title": "Challenges and Opportunities in High-dimensional Variational Inference. (arXiv:2103.01085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hanlin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>",
          "description": "Scalable training of large models (like BERT and GPT-3) requires careful\noptimization rooted in model design, architecture, and system capabilities.\nFrom a system standpoint, communication has become a major bottleneck,\nespecially on commodity systems with standard TCP interconnects that offer\nlimited network bandwidth. Communication compression is an important technique\nto reduce training time on such systems. One of the most effective methods is\nerror-compensated compression, which offers robust convergence speed even under\n1-bit compression. However, state-of-the-art error compensation techniques only\nwork with basic optimizers like SGD and momentum SGD, which are linearly\ndependent on the gradients. They do not work with non-linear gradient-based\noptimizers like Adam, which offer state-of-the-art convergence efficiency and\naccuracy for models like BERT. In this paper, we propose 1-bit Adam that\nreduces the communication volume by up to $5\\times$, offers much better\nscalability, and provides the same convergence speed as uncompressed Adam. Our\nkey finding is that Adam's variance (non-linear term) becomes stable (after a\nwarmup phase) and can be used as a fixed precondition for the rest of the\ntraining (compression phase). Experiments on up to 256 GPUs show that 1-bit\nAdam enables up to $3.3\\times$ higher throughput for BERT-Large pre-training\nand up to $2.9\\times$ higher throughput for SQuAD fine-tuning. In addition, we\nprovide theoretical analysis for our proposed work.",
          "link": "http://arxiv.org/abs/2102.02888",
          "publishedOn": "2021-07-01T01:59:34.646Z",
          "wordCount": 706,
          "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. (arXiv:2102.02888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1\">Isel Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1\">Agnieszka Jastrzebska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>",
          "description": "In this paper, we present a recurrent neural system named Long Short-term\nCognitive Networks (LSTCNs) as a generalisation of the Short-term Cognitive\nNetwork (STCN) model. Such a generalisation is motivated by the difficulty of\nforecasting very long time series in an efficient, greener fashion. The LSTCN\nmodel can be defined as a collection of STCN blocks, each processing a specific\ntime patch of the (multivariate) time series being modelled. In this neural\nensemble, each block passes information to the subsequent one in the form of a\nweight matrix referred to as the prior knowledge matrix. As a second\ncontribution, we propose a deterministic learning algorithm to compute the\nlearnable weights while preserving the prior knowledge resulting from previous\nlearning processes. As a third contribution, we introduce a feature influence\nscore as a proxy to explain the forecasting process in multivariate time\nseries. The simulations using three case studies show that our neural system\nreports small forecasting errors while being up to thousands of times faster\nthan state-of-the-art recurrent models.",
          "link": "http://arxiv.org/abs/2106.16233",
          "publishedOn": "2021-07-01T01:59:34.640Z",
          "wordCount": 594,
          "title": "Long Short-term Cognitive Networks. (arXiv:2106.16233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) starts to be widely used to enhance the performance of\nmulti-user multiple-input multiple-output (MU-MIMO) receivers. However, it is\nstill unclear if such methods are truly competitive with respect to\nconventional methods in realistic scenarios and under practical constraints. In\naddition to enabling accurate signal reconstruction on realistic channel\nmodels, MU-MIMO receive algorithms must allow for easy adaptation to a varying\nnumber of users without the need for retraining. In contrast to existing work,\nwe propose an ML-enhanced MU-MIMO receiver that builds on top of a conventional\nlinear minimum mean squared error (LMMSE) architecture. It preserves the\ninterpretability and scalability of the LMMSE receiver, while improving its\naccuracy in two ways. First, convolutional neural networks (CNNs) are used to\ncompute an approximation of the second-order statistics of the channel\nestimation error which are required for accurate equalization. Second, a\nCNN-based demapper jointly processes a large number of orthogonal\nfrequency-division multiplexing (OFDM) symbols and subcarriers, which allows it\nto compute better log likelihood ratios (LLRs) by compensating for channel\naging. The resulting architecture can be used in the up- and downlink and is\ntrained in an end-to-end manner, removing the need for hard-to-get perfect\nchannel state information (CSI) during the training phase. Simulation results\ndemonstrate consistent performance improvements over the baseline which are\nespecially pronounced in high mobility scenarios.",
          "link": "http://arxiv.org/abs/2012.08177",
          "publishedOn": "2021-07-01T01:59:34.634Z",
          "wordCount": 699,
          "title": "Machine Learning for MU-MIMO Receive Processing in OFDM Systems. (arXiv:2012.08177v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mylonas_C/0/1/0/all/0/1\">Charilaos Mylonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_I/0/1/0/all/0/1\">Imad Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzi_E/0/1/0/all/0/1\">Eleni Chatzi</a>",
          "description": "Graph Networks (GNs) enable the fusion of prior knowledge and relational\nreasoning with flexible function approximations. In this work, a general\nGN-based model is proposed which takes full advantage of the relational\nmodeling capabilities of GNs and extends these to probabilistic modeling with\nVariational Bayes (VB). To that end, we combine complementary pre-existing\napproaches on VB for graph data and propose an approach that relies on\ngraph-structured latent and conditioning variables. It is demonstrated that\nNeural Processes can also be viewed through the lens of the proposed model. We\nshow applications on the problem of structured probability density modeling for\nsimulated and real wind farm monitoring data, as well as on the meta-learning\nof simulated Gaussian Process data. We release the source code, along with the\nsimulated datasets.",
          "link": "http://arxiv.org/abs/2106.16049",
          "publishedOn": "2021-07-01T01:59:34.628Z",
          "wordCount": 590,
          "title": "Relational VAE: A Continuous Latent Variable Model for Graph Structured Data. (arXiv:2106.16049v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohnishi_M/0/1/0/all/0/1\">Motoya Ohnishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1\">Isao Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1\">Kendall Lowrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1\">Masahiro Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1\">Yoshinobu Kawahara</a>",
          "description": "Most modern reinforcement learning algorithms optimize a cumulative\nsingle-step cost along a trajectory. The optimized motions are often\n'unnatural', representing, for example, behaviors with sudden accelerations\nthat waste energy and lack predictability. In this work, we present a novel\nparadigm of controlling nonlinear systems via the minimization of the Koopman\nspectrum cost: a cost over the Koopman operator of the controlled dynamics.\nThis induces a broader class of dynamical behaviors that evolve over stable\nmanifolds such as nonlinear oscillators, closed loops, and smooth movements. We\ndemonstrate that some dynamics realizations that are not possible with a\ncumulative cost are feasible in this paradigm. Moreover, we present a provably\nefficient online learning algorithm for our problem that enjoys a sub-linear\nregret bound under some structural assumptions.",
          "link": "http://arxiv.org/abs/2106.15775",
          "publishedOn": "2021-07-01T01:59:34.622Z",
          "wordCount": 571,
          "title": "Koopman Spectrum Nonlinear Regulator and Provably Efficient Online Learning. (arXiv:2106.15775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>",
          "description": "Computing an AUC as a performance measure to compare the quality of different\nmachine learning models is one of the final steps of many research projects.\nMany of these methods are trained on privacy-sensitive data and there are\nseveral different approaches like $\\epsilon$-differential privacy, federated\nmachine learning and methods based on cryptographic approaches if the datasets\ncannot be shared or evaluated jointly at one place. In this setting, it can\nalso be a problem to compute the global performance measure like an AUC, since\nthe labels might also contain privacy-sensitive information. There have been\napproaches based on $\\epsilon$-differential privacy to deal with this problem,\nbut to the best of our knowledge, no exact privacy preserving solution has been\nintroduced. In this paper, we propose an MPC-based framework, called \\fw{},\nwith private merging of sorted lists and novel methods for comparing two\nsecret-shared values, selecting between two secret-shared values, converting\nthe modulus, and performing division to compute the exact AUC as one could\nobtain on the pooled original test samples. With \\fw{} computation of the exact\narea under precision-recall curve and receiver operating characteristic curve\nis even possible when ties between prediction confidence values exist. To show\nthe applicability of \\fw{}, we use it to evaluate a model trained to predict\nacute myeloid leukemia therapy response and we also assess its scalability via\nexperiments on synthetic data. The experiments show that we efficiently compute\nexactly the same AUC with both evaluation metrics in a privacy preserving\nmanner as one can obtain on the pooled test samples in the plaintext domain.\nOur solution provides security against semi-honest corruption of at most one of\nthe servers performing the secure computation.",
          "link": "http://arxiv.org/abs/2102.08788",
          "publishedOn": "2021-07-01T01:59:34.605Z",
          "wordCount": 753,
          "title": "ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves with Secure 3-Party Computation. (arXiv:2102.08788v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07850",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1\">Adrien Corenflos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1\">James Thornton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Particle Filtering (PF) methods are an established class of procedures for\nperforming inference in non-linear state-space models. Resampling is a key\ningredient of PF, necessary to obtain low variance likelihood and states\nestimates. However, traditional resampling methods result in PF-based loss\nfunctions being non-differentiable with respect to model and PF parameters. In\na variational inference context, resampling also yields high variance gradient\nestimates of the PF-based evidence lower bound. By leveraging optimal transport\nideas, we introduce a principled differentiable particle filter and provide\nconvergence results. We demonstrate this novel method on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2102.07850",
          "publishedOn": "2021-07-01T01:59:34.588Z",
          "wordCount": 571,
          "title": "Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_R/0/1/0/all/0/1\">Ricardo Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_H/0/1/0/all/0/1\">Hugo L. Fernandes</a>",
          "description": "The Orienteering Problem with Time Windows (OPTW) is a combinatorial\noptimization problem where the goal is to maximize the total score collected\nfrom different visited locations. The application of neural network models to\ncombinatorial optimization has recently shown promising results in dealing with\nsimilar problems, like the Travelling Salesman Problem. A neural network allows\nlearning solutions using reinforcement learning or supervised learning,\ndepending on the available data. After the learning stage, it can be\ngeneralized and quickly fine-tuned to further improve performance and\npersonalization. The advantages are evident since, for real-world applications,\nsolution quality, personalization, and execution times are all important\nfactors that should be taken into account.\n\nThis study explores the use of Pointer Network models trained using\nreinforcement learning to solve the OPTW problem. We propose a modified\narchitecture that leverages Pointer Networks to better address problems related\nwith dynamic time-dependent constraints. Among its various applications, the\nOPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the\nPointer Network with the TTDP problem in mind, by sampling variables that can\nchange across tourists visiting a particular instance-region: starting\nposition, starting time, available time, and the scores given to each point of\ninterest. Once a model-region is trained, it can infer a solution for a\nparticular tourist using beam search. We based the assessment of our approach\non several existing benchmark OPTW instances. We show that it generalizes\nacross different tourists that visit each region and that it generally\noutperforms the most commonly used heuristic, while computing the solution in\nrealistic times.",
          "link": "http://arxiv.org/abs/2011.03647",
          "publishedOn": "2021-07-01T01:59:34.575Z",
          "wordCount": 718,
          "title": "A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. (arXiv:2011.03647v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchkin_N/0/1/0/all/0/1\">Nikita Puchkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1\">Nikita Zhivotovskiy</a>",
          "description": "We show that in pool-based active classification without assumptions on the\nunderlying distribution, if the learner is given the power to abstain from some\npredictions by paying the price marginally smaller than the average loss $1/2$\nof a random guess, exponential savings in the number of label requests are\npossible whenever they are possible in the corresponding realizable problem. We\nextend this result to provide a necessary and sufficient condition for\nexponential savings in pool-based active classification under the model\nmisspecification.",
          "link": "http://arxiv.org/abs/2102.00451",
          "publishedOn": "2021-07-01T01:59:34.558Z",
          "wordCount": 556,
          "title": "Exponential Savings in Agnostic Active Learning through Abstention. (arXiv:2102.00451v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16239",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1\">Tomasz Piotrowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1\">Renato L. G. Cavalcante</a>",
          "description": "We derive conditions for the existence of fixed points of neural networks, an\nimportant research objective to understand their behavior in modern\napplications involving autoencoders and loop unrolling techniques, among\nothers. In particular, we focus on networks with nonnegative inputs and\nnonnegative network parameters, as often considered in the literature. We show\nthat such networks can be recognized as monotonic and (weakly) scalable\nfunctions within the framework of nonlinear Perron-Frobenius theory. This fact\nenables us to derive conditions for the existence of a nonempty fixed point set\nof the neural networks, and these conditions are weaker than those obtained\nrecently using arguments in convex analysis, which are typically based on the\nassumption of nonexpansivity of the activation functions. Furthermore, we prove\nthat the shape of the fixed point set of monotonic and weakly scalable neural\nnetworks is often an interval, which degenerates to a point for the case of\nscalable networks. The chief results of this paper are verified in numerical\nsimulations, where we consider an autoencoder-type network that first\ncompresses angular power spectra in massive MIMO systems, and, second,\nreconstruct the input spectra from the compressed signal.",
          "link": "http://arxiv.org/abs/2106.16239",
          "publishedOn": "2021-07-01T01:59:34.553Z",
          "wordCount": 624,
          "title": "Fixed points of monotonic and (weakly) scalable neural networks. (arXiv:2106.16239v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leye Wang</a>",
          "description": "In the big data and AI era, context is widely exploited as extra information\nwhich makes it easier to learn a more complex pattern in machine learning\nsystems. However, most of the existing related studies seldom take context into\naccount. The difficulty lies in the unknown generalization ability of both\ncontext and its modeling techniques across different scenarios. To fill the\nabove gaps, we conduct a large-scale analytical and empirical study on the\nspatiotemporal crowd prediction (STCFP) problem that is a widely-studied and\nhot research topic. We mainly make three efforts:(i) we develop new taxonomy\nabout both context features and context modeling techniques based on extensive\ninvestigations in prevailing STCFP research; (ii) we conduct extensive\nexperiments on seven datasets with hundreds of millions of records to\nquantitatively evaluate the generalization ability of both distinct context\nfeatures and context modeling techniques; (iii) we summarize some guidelines\nfor researchers to conveniently utilize context in diverse applications.",
          "link": "http://arxiv.org/abs/2106.16046",
          "publishedOn": "2021-07-01T01:59:34.547Z",
          "wordCount": 588,
          "title": "Exploring Context Modeling Techniques on the Spatiotemporal Crowd Flow Prediction. (arXiv:2106.16046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15980",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jerfel_G/0/1/0/all/0/1\">Ghassen Jerfel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Serena Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fannjiang_C/0/1/0/all/0/1\">Clara Fannjiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heller_K/0/1/0/all/0/1\">Katherine A. Heller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1\">Yian Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Variational Inference (VI) is a popular alternative to asymptotically exact\nsampling in Bayesian inference. Its main workhorse is optimization over a\nreverse Kullback-Leibler divergence (RKL), which typically underestimates the\ntail of the posterior leading to miscalibration and potential degeneracy.\nImportance sampling (IS), on the other hand, is often used to fine-tune and\nde-bias the estimates of approximate Bayesian inference procedures. The quality\nof IS crucially depends on the choice of the proposal distribution. Ideally,\nthe proposal distribution has heavier tails than the target, which is rarely\nachievable by minimizing the RKL. We thus propose a novel combination of\noptimization and sampling techniques for approximate Bayesian inference by\nconstructing an IS proposal distribution through the minimization of a forward\nKL (FKL) divergence. This approach guarantees asymptotic consistency and a fast\nconvergence towards both the optimal IS estimator and the optimal variational\napproximation. We empirically demonstrate on real data that our method is\ncompetitive with variational boosting and MCMC.",
          "link": "http://arxiv.org/abs/2106.15980",
          "publishedOn": "2021-07-01T01:59:34.541Z",
          "wordCount": 616,
          "title": "Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence. (arXiv:2106.15980v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:34.491Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:34.477Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05944",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1\">Johannes Kirschner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1\">Tor Lattimore</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vernade_C/0/1/0/all/0/1\">Claire Vernade</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We introduce a simple and efficient algorithm for stochastic linear bandits\nwith finitely many actions that is asymptotically optimal and (nearly)\nworst-case optimal in finite time. The approach is based on the frequentist\ninformation-directed sampling (IDS) framework, with a surrogate for the\ninformation gain that is informed by the optimization problem that defines the\nasymptotic lower bound. Our analysis sheds light on how IDS balances the\ntrade-off between regret and information and uncovers a surprising connection\nbetween the recently proposed primal-dual methods and the IDS algorithm. We\ndemonstrate empirically that IDS is competitive with UCB in finite-time, and\ncan be significantly better in the asymptotic regime.",
          "link": "http://arxiv.org/abs/2011.05944",
          "publishedOn": "2021-07-01T01:59:34.464Z",
          "wordCount": 574,
          "title": "Asymptotically Optimal Information-Directed Sampling. (arXiv:2011.05944v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16087",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokalj_Filipovic_S/0/1/0/all/0/1\">Silvija Kokalj-Filipovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toliver_P/0/1/0/all/0/1\">Paul Toliver</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnson_W/0/1/0/all/0/1\">William Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miller_R/0/1/0/all/0/1\">Rob Miller</a>",
          "description": "Current radio frequency (RF) sensors at the Edge lack the computational\nresources to support practical, in-situ training for intelligent spectrum\nmonitoring, and sensor data classification in general. We propose a solution\nvia Deep Delay Loop Reservoir Computing (DLR), a processing architecture that\nsupports general machine learning algorithms on compact mobile devices by\nleveraging delay-loop reservoir computing in combination with innovative\nelectrooptical hardware. With both digital and photonic realizations of our\ndesign of the loops, DLR delivers reductions in form factor, hardware\ncomplexity and latency, compared to the State-of-the-Art (SoA). The main impact\nof the reservoir is to project the input data into a higher dimensional space\nof reservoir state vectors in order to linearly separate the input classes.\nOnce the classes are well separated, traditionally complex, power-hungry\nclassification models are no longer needed for the learning process. Yet, even\nwith simple classifiers based on Ridge regression (RR), the complexity grows at\nleast quadratically with the input size. Hence, the hardware reduction required\nfor training on compact devices is in contradiction with the large dimension of\nstate vectors. DLR employs a RR-based classifier to exceed the SoA accuracy,\nwhile further reducing power consumption by leveraging the architecture of\nparallel (split) loops. We present DLR architectures composed of multiple\nsmaller loops whose state vectors are linearly combined to create a lower\ndimensional input into Ridge regression. We demonstrate the advantages of using\nDLR for two distinct applications: RF Specific Emitter Identification (SEI) for\nIoT authentication, and wireless protocol recognition for IoT situational\nawareness.",
          "link": "http://arxiv.org/abs/2106.16087",
          "publishedOn": "2021-07-01T01:59:34.417Z",
          "wordCount": 710,
          "title": "Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors. (arXiv:2106.16087v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:34.392Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Ping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>",
          "description": "This paper studies the decentralized optimization and learning problem where\nmultiple interconnected agents aim to learn an optimal decision function\ndefined over a reproducing kernel Hilbert space by jointly minimizing a global\nobjective function, with access to their own locally observed dataset. As a\nnon-parametric approach, kernel learning faces a major challenge in distributed\nimplementation: the decision variables of local objective functions are\ndata-dependent and thus cannot be optimized under the decentralized consensus\nframework without any raw data exchange among agents. To circumvent this major\nchallenge, we leverage the random feature (RF) approximation approach to enable\nconsensus on the function modeled in the RF space by data-independent\nparameters across different agents. We then design an iterative algorithm,\ntermed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we\nfurther develop a communication-censored kernel learning (COKE) algorithm that\nreduces the communication load of DKLA by preventing an agent from transmitting\nat every iteration unless its local updates are deemed informative. Theoretical\nresults in terms of linear convergence guarantee and generalization performance\nanalysis of DKLA and COKE are provided. Comprehensive tests on both synthetic\nand real datasets are conducted to verify the communication efficiency and\nlearning effectiveness of COKE.",
          "link": "http://arxiv.org/abs/2001.10133",
          "publishedOn": "2021-07-01T01:59:34.375Z",
          "wordCount": 655,
          "title": "COKE: Communication-Censored Decentralized Kernel Learning. (arXiv:2001.10133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16079",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pihlajasalo_J/0/1/0/all/0/1\">Jaakko Pihlajasalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korpi_D/0/1/0/all/0/1\">Dani Korpi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honkala_M/0/1/0/all/0/1\">Mikko Honkala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huttunen_J/0/1/0/all/0/1\">Janne M.J. Huttunen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riihonen_T/0/1/0/all/0/1\">Taneli Riihonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talvitie_J/0/1/0/all/0/1\">Jukka Talvitie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brihuega_A/0/1/0/all/0/1\">Alberto Brihuega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uusitalo_M/0/1/0/all/0/1\">Mikko A. Uusitalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valkama_M/0/1/0/all/0/1\">Mikko Valkama</a>",
          "description": "In this paper, we propose a machine learning (ML) based physical layer\nreceiver solution for demodulating OFDM signals that are subject to a high\nlevel of nonlinear distortion. Specifically, a novel deep learning based\nconvolutional neural network receiver is devised, containing layers in both\ntime- and frequency domains, allowing to demodulate and decode the transmitted\nbits reliably despite the high error vector magnitude (EVM) in the transmit\nsignal. Extensive set of numerical results is provided, in the context of 5G NR\nuplink incorporating also measured terminal power amplifier characteristics.\nThe obtained results show that the proposed receiver system is able to clearly\noutperform classical linear receivers as well as existing ML receiver\napproaches, especially when the EVM is high in comparison with modulation\norder. The proposed ML receiver can thus facilitate pushing the terminal power\namplifier (PA) systems deeper into saturation, and thereon improve the terminal\npower-efficiency, radiated power and network coverage.",
          "link": "http://arxiv.org/abs/2106.16079",
          "publishedOn": "2021-07-01T01:59:34.369Z",
          "wordCount": 617,
          "title": "HybridDeepRx: Deep Learning Receiver for High-EVM Signals. (arXiv:2106.16079v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sidak Pal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1\">Gregor Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.",
          "link": "http://arxiv.org/abs/2106.16225",
          "publishedOn": "2021-07-01T01:59:34.364Z",
          "wordCount": 627,
          "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps. (arXiv:2106.16225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1\">Giulio Franzese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_D/0/1/0/all/0/1\">Dimitrios Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1\">Maurizio Filippone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1\">Pietro Michiardi</a>",
          "description": "In this work, we revisit the theoretical properties of Hamiltonian stochastic\ndifferential equations (SDEs) for Bayesian posterior sampling, and we study the\ntwo types of errors that arise from numerical SDE simulation: the\ndiscretization error and the error due to noisy gradient estimates in the\ncontext of data subsampling. We consider overlooked results describing the\nergodic convergence rates of numerical integration schemes, and we produce a\nnovel analysis for the effect of mini-batches through the lens of differential\noperator splitting. In our analysis, the stochastic component of the proposed\nHamiltonian SDE is decoupled from the gradient noise, for which we make no\nnormality assumptions. This allows us to derive interesting connections among\ndifferent sampling schemes, including the original Hamiltonian Monte Carlo\n(HMC) algorithm, and explain their performance. We show that for a careful\nselection of numerical integrators, both errors vanish at a rate\n$\\mathcal{O}(\\eta^2)$, where $\\eta$ is the integrator step size. Our\ntheoretical results are supported by an empirical study on a variety of\nregression and classification tasks for Bayesian neural networks.",
          "link": "http://arxiv.org/abs/2106.16200",
          "publishedOn": "2021-07-01T01:59:34.358Z",
          "wordCount": 603,
          "title": "A Unified View of Stochastic Hamiltonian Sampling. (arXiv:2106.16200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Despite a series of recent successes in reinforcement learning (RL), many RL\nalgorithms remain sensitive to hyperparameters. As such, there has recently\nbeen interest in the field of AutoRL, which seeks to automate design decisions\nto create more general algorithms. Recent work suggests that population based\napproaches may be effective AutoRL algorithms, by learning hyperparameter\nschedules on the fly. In particular, the PB2 algorithm is able to achieve\nstrong performance in RL tasks by formulating online hyperparameter\noptimization as time varying GP-bandit problem, while also providing\ntheoretical guarantees. However, PB2 is only designed to work for continuous\nhyperparameters, which severely limits its utility in practice. In this paper\nwe introduce a new (provably) efficient hierarchical approach for optimizing\nboth continuous and categorical variables, using a new time-varying bandit\nalgorithm specifically designed for the population based training regime. We\nevaluate our approach on the challenging Procgen benchmark, where we show that\nexplicitly modelling dependence between data augmentation and other\nhyperparameters improves generalization.",
          "link": "http://arxiv.org/abs/2106.15883",
          "publishedOn": "2021-07-01T01:59:34.352Z",
          "wordCount": 599,
          "title": "Tuning Mixed Input Hyperparameters on the Fly for Efficient Population Based AutoRL. (arXiv:2106.15883v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:34.333Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhongyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1\">Dong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>",
          "description": "We introduce a unified framework, formulated as general latent space models,\nto study complex higher-order network interactions among multiple entities. Our\nframework covers several popular models in recent network analysis literature,\nincluding mixture multi-layer latent space model and hypergraph latent space\nmodel. We formulate the relationship between the latent positions and the\nobserved data via a generalized multilinear kernel as the link function. While\nour model enjoys decent generality, its maximum likelihood parameter estimation\nis also convenient via a generalized tensor decomposition procedure.We propose\na novel algorithm using projected gradient descent on Grassmannians. We also\ndevelop original theoretical guarantees for our algorithm. First, we show its\nlinear convergence under mild conditions. Second, we establish finite-sample\nstatistical error rates of latent position estimation, determined by the signal\nstrength, degrees of freedom and the smoothness of link function, for both\ngeneral and specific latent space models. We demonstrate the effectiveness of\nour method on synthetic data. We also showcase the merit of our method on two\nreal-world datasets that are conventionally described by different specific\nmodels in producing meaningful and interpretable parameter estimations and\naccurate link prediction. We demonstrate the effectiveness of our method on\nsynthetic data. We also showcase the merit of our method on two real-world\ndatasets that are conventionally described by different specific models in\nproducing meaningful and interpretable parameter estimations and accurate link\nprediction.",
          "link": "http://arxiv.org/abs/2106.16042",
          "publishedOn": "2021-07-01T01:59:34.324Z",
          "wordCount": 667,
          "title": "Latent Space Model for Higher-order Networks and Generalized Tensor Decomposition. (arXiv:2106.16042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ceran_E/0/1/0/all/0/1\">Elif Tugce Ceran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andras Gyorgy</a>",
          "description": "The time average expected age of information (AoI) is studied for status\nupdates sent over an error-prone channel from an energy-harvesting transmitter\nwith a finite-capacity battery. Energy cost of sensing new status updates is\ntaken into account as well as the transmission energy cost better capturing\npractical systems. The optimal scheduling policy is first studied under the\nhybrid automatic repeat request (HARQ) protocol when the channel and energy\nharvesting statistics are known, and the existence of a threshold-based optimal\npolicy is shown. For the case of unknown environments, average-cost\nreinforcement-learning algorithms are proposed that learn the system parameters\nand the status update policy in real-time. The effectiveness of the proposed\nmethods is demonstrated through numerical results.",
          "link": "http://arxiv.org/abs/2106.16037",
          "publishedOn": "2021-07-01T01:59:34.307Z",
          "wordCount": 577,
          "title": "Learning to Minimize Age of Information over an Unreliable Channel with Energy Harvesting. (arXiv:2106.16037v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mou_Z/0/1/0/all/0/1\">Zhiyu Mou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feifei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qihui Wu</a>",
          "description": "In this paper, we study the self-healing problem of unmanned aerial vehicle\n(UAV) swarm network (USNET) that is required to quickly rebuild the\ncommunication connectivity under unpredictable external disruptions (UEDs).\nFirstly, to cope with the one-off UEDs, we propose a graph convolutional neural\nnetwork (GCN) and find the recovery topology of the USNET in an on-line manner.\nSecondly, to cope with general UEDs, we develop a GCN based trajectory planning\nalgorithm that can make UAVs rebuild the communication connectivity during the\nself-healing process. We also design a meta learning scheme to facilitate the\non-line executions of the GCN. Numerical results show that the proposed\nalgorithms can rebuild the communication connectivity of the USNET more quickly\nthan the existing algorithms under both one-off UEDs and general UEDs. The\nsimulation results also show that the meta learning scheme can not only enhance\nthe performance of the GCN but also reduce the time complexity of the on-line\nexecutions.",
          "link": "http://arxiv.org/abs/2106.16048",
          "publishedOn": "2021-07-01T01:59:34.290Z",
          "wordCount": 595,
          "title": "Resilient UAV Swarm Communications with Graph Convolutional Neural Network. (arXiv:2106.16048v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>",
          "description": "We develop simple methods for constructing likelihoods and parameter priors\nfor learning about the parameters and structure of a Bayesian network. In\nparticular, we introduce several assumptions that permit the construction of\nlikelihoods and parameter priors for a large number of Bayesian-network\nstructures from a small set of assessments. The most notable assumption is that\nof likelihood equivalence, which says that data can not help to discriminate\nnetwork structures that encode the same assertions of conditional independence.\nWe describe the constructions that follow from these assumptions, and also\npresent a method for directly computing the marginal likelihood of a random\nsample with no missing observations. Also, we show how these assumptions lead\nto a general framework for characterizing parameter priors of multivariate\ndistributions.",
          "link": "http://arxiv.org/abs/2105.06241",
          "publishedOn": "2021-07-01T01:59:34.181Z",
          "wordCount": 592,
          "title": "Likelihoods and Parameter Priors for Bayesian Networks. (arXiv:2105.06241v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.166Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15921",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Thin_A/0/1/0/all/0/1\">Achille Thin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kotelevskii_N/0/1/0/all/0/1\">Nikita Kotelevskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1\">Maxim Panov</a>",
          "description": "Variational auto-encoders (VAE) are popular deep latent variable models which\nare trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter\nELBO and hence better variational approximations, it has been proposed to use\nimportance sampling to get a lower variance estimate of the evidence. However,\nimportance sampling is known to perform poorly in high dimensions. While it has\nbeen suggested many times in the literature to use more sophisticated\nalgorithms such as Annealed Importance Sampling (AIS) and its Sequential\nImportance Sampling (SIS) extensions, the potential benefits brought by these\nadvanced techniques have never been realized for VAE: the AIS estimate cannot\nbe easily differentiated, while SIS requires the specification of carefully\nchosen backward Markov kernels. In this paper, we address both issues and\ndemonstrate the performance of the resulting Monte Carlo VAEs on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2106.15921",
          "publishedOn": "2021-07-01T01:59:34.160Z",
          "wordCount": 567,
          "title": "Monte Carlo Variational Auto-Encoders. (arXiv:2106.15921v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pitchforth_D/0/1/0/all/0/1\">Daniel J Pitchforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1\">Timothy J Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tygesen_U/0/1/0/all/0/1\">Ulf T Tygesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_E/0/1/0/all/0/1\">Elizabeth J Cross</a>",
          "description": "The quantification of wave loading on offshore structures and components is a\ncrucial element in the assessment of their useful remaining life. In many\napplications the well-known Morison's equation is employed to estimate the\nforcing from waves with assumed particle velocities and accelerations. This\npaper develops a grey-box modelling approach to improve the predictions of the\nforce on structural members. A grey-box model intends to exploit the enhanced\npredictive capabilities of data-based modelling whilst retaining physical\ninsight into the behaviour of the system; in the context of the work carried\nout here, this can be considered as physics-informed machine learning. There\nare a number of possible approaches to establish a grey-box model. This paper\ndemonstrates two means of combining physics (white box) and data-based (black\nbox) components; one where the model is a simple summation of the two\ncomponents, the second where the white-box prediction is fed into the black box\nas an additional input. Here Morison's equation is used as the physics-based\ncomponent in combination with a data-based Gaussian process NARX - a dynamic\nvariant of the more well-known Gaussian process regression. Two key challenges\nwith employing the GP-NARX formulation that are addressed here are the\nselection of appropriate lag terms and the proper treatment of uncertainty\npropagation within the dynamic GP. The best performing grey-box model, the\nresidual modelling GP-NARX, was able to achieve a 29.13\\% and 5.48\\% relative\nreduction in NMSE over Morison's Equation and a black-box GP-NARX respectively,\nalongside significant benefits in extrapolative capabilities of the model, in\ncircumstances of low dataset coverage.",
          "link": "http://arxiv.org/abs/2105.13813",
          "publishedOn": "2021-07-01T01:59:34.129Z",
          "wordCount": 732,
          "title": "Grey-box models for wave loading prediction. (arXiv:2105.13813v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05739",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>",
          "description": "Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.",
          "link": "http://arxiv.org/abs/2106.05739",
          "publishedOn": "2021-07-01T01:59:34.100Z",
          "wordCount": 601,
          "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeonggeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Simyung Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1\">Dooyong Sung</a>",
          "description": "Keyword spotting is an important research field because it plays a key role\nin device wake-up and user interaction on smart devices. However, it is\nchallenging to minimize errors while operating efficiently in devices with\nlimited resources such as mobile phones. We present a broadcasted residual\nlearning method to achieve high accuracy with small model size and\ncomputational load. Our method configures most of the residual functions as 1D\ntemporal convolution while still allows 2D convolution together using a\nbroadcasted-residual connection that expands temporal output to\nfrequency-temporal dimension. This residual mapping enables the network to\neffectively represent useful audio features with much less computation than\nconventional convolutional neural networks. We also propose a novel network\narchitecture, Broadcasting-residual network (BC-ResNet), based on broadcasted\nresidual learning and describe how to scale up the model according to the\ntarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%\ntop-1 accuracy on Google speech command datasets v1 and v2, respectively, and\nconsistently outperform previous approaches, using fewer computations and\nparameters.",
          "link": "http://arxiv.org/abs/2106.04140",
          "publishedOn": "2021-07-01T01:59:34.094Z",
          "wordCount": 624,
          "title": "Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:34.088Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16088",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_S/0/1/0/all/0/1\">Supriya Bajpai</a>",
          "description": "In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.",
          "link": "http://arxiv.org/abs/2106.16088",
          "publishedOn": "2021-07-01T01:59:34.076Z",
          "wordCount": 540,
          "title": "Application of deep reinforcement learning for Indian stock trading automation. (arXiv:2106.16088v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07036",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1\">Austin Clyde</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brettin_T/0/1/0/all/0/1\">Thomas Brettin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Partin_A/0/1/0/all/0/1\">Alexander Partin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yoo_H/0/1/0/all/0/1\">Hyunseung Yoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Babuji_Y/0/1/0/all/0/1\">Yadu Babuji</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Blaiszik_B/0/1/0/all/0/1\">Ben Blaiszik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Merzky_A/0/1/0/all/0/1\">Andre Merzky</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Turilli_M/0/1/0/all/0/1\">Matteo Turilli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1\">Shantenu Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>",
          "description": "We propose a benchmark to study surrogate model accuracy for protein-ligand\ndocking. We share a dataset consisting of 200 million 3D complex structures and\n2D structure scores across a consistent set of 13 million \"in-stock\" molecules\nover 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work\nshows surrogate docking models have six orders of magnitude more throughput\nthan standard docking protocols on the same supercomputer node types. We\ndemonstrate the power of high-speed surrogate models by running each target\nagainst 1 billion molecules in under a day (50k predictions per GPU seconds).\nWe showcase a workflow for docking utilizing surrogate ML models as a\npre-filter. Our workflow is ten times faster at screening a library of\ncompounds than the standard technique, with an error rate less than 0.01\\% of\ndetecting the underlying best scoring 0.1\\% of compounds. Our analysis of the\nspeedup explains that to screen more molecules under a docking paradigm,\nanother order of magnitude speedup must come from model accuracy rather than\ncomputing speed (which, if increased, will not anymore alter our throughput to\nscreen molecules). We believe this is strong evidence for the community to\nbegin focusing on improving the accuracy of surrogate models to improve the\nability to screen massive compound libraries 100x or even 1000x faster than\ncurrent techniques.",
          "link": "http://arxiv.org/abs/2106.07036",
          "publishedOn": "2021-07-01T01:59:34.051Z",
          "wordCount": 734,
          "title": "Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening. (arXiv:2106.07036v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wanying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panariello_M/0/1/0/all/0/1\">Michele Panariello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>",
          "description": "This paper reports the first successful application of a differentiable\narchitecture search (DARTS) approach to the deepfake and spoofing detection\nproblems. An example of neural architecture search, DARTS operates upon a\ncontinuous, differentiable search space which enables both the architecture and\nparameters to be optimised via gradient descent. Solutions based on\npartially-connected DARTS use random channel masking in the search space to\nreduce GPU time and automatically learn and optimise complex neural\narchitectures composed of convolutional operations and residual blocks. Despite\nbeing learned quickly with little human effort, the resulting networks are\ncompetitive with the best performing systems reported in the literature. Some\nare also far less complex, containing 85% fewer parameters than a Res2Net\ncompetitor.",
          "link": "http://arxiv.org/abs/2104.03123",
          "publishedOn": "2021-07-01T01:59:34.038Z",
          "wordCount": 594,
          "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection. (arXiv:2104.03123v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1\">Ercument Ilhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1\">Jeremy Gow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1\">Diego Perez-Liebana</a>",
          "description": "Deep Reinforcement Learning (RL) techniques can benefit greatly from\nleveraging prior experience, which can be either self-generated or acquired\nfrom other entities. Action advising is a framework that provides a flexible\nway to transfer such knowledge in the form of actions between teacher-student\npeers. However, due to the realistic concerns, the number of these interactions\nis limited with a budget; therefore, it is crucial to perform these in the most\nappropriate moments. There have been several promising studies recently that\naddress this problem setting especially from the student's perspective. Despite\ntheir success, they have some shortcomings when it comes to the practical\napplicability and integrity as an overall solution to the learning from advice\nchallenge. In this paper, we extend the idea of advice reusing via teacher\nimitation to construct a unified approach that addresses both advice collection\nand advice utilisation problems. We also propose a method to automatically tune\nthe relevant hyperparameters of these components on-the-fly to make it able to\nadapt to any task with minimal human intervention. The experiments we performed\nin 5 different Atari games verify that our algorithm either surpasses or\nperforms on-par with its top competitors while being far simpler to be\nemployed. Furthermore, its individual components are also found to be providing\nsignificant advantages alone.",
          "link": "http://arxiv.org/abs/2104.08440",
          "publishedOn": "2021-07-01T01:59:34.032Z",
          "wordCount": 678,
          "title": "Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fompeyrine_D/0/1/0/all/0/1\">D. Fompeyrine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorm_E/0/1/0/all/0/1\">E. S. Vorm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricka_N/0/1/0/all/0/1\">N. Ricka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_F/0/1/0/all/0/1\">F. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrin_G/0/1/0/all/0/1\">G. Pellegrin</a>",
          "description": "Machine Learning (ML) has recently been demonstrated to rival expert-level\nhuman accuracy in prediction and detection tasks in a variety of domains,\nincluding medicine. Despite these impressive findings, however, a key barrier\nto the full realization of ML's potential in medical prognoses is technology\nacceptance. Recent efforts to produce explainable AI (XAI) have made progress\nin improving the interpretability of some ML models, but these efforts suffer\nfrom limitations intrinsic to their design: they work best at identifying why a\nsystem fails, but do poorly at explaining when and why a model's prediction is\ncorrect. We posit that the acceptability of ML predictions in expert domains is\nlimited by two key factors: the machine's horizon of prediction that extends\nbeyond human capability, and the inability for machine predictions to\nincorporate human intuition into their models. We propose the use of a novel ML\narchitecture, Neural Ordinary Differential Equations (NODEs) to enhance human\nunderstanding and encourage acceptability. Our approach prioritizes human\ncognitive intuition at the center of the algorithm design, and offers a\ndistribution of predictions rather than single outputs. We explain how this\napproach may significantly improve human-machine collaboration in prediction\ntasks in expert domains such as medical prognoses. We propose a model and\ndemonstrate, by expanding a concrete example from the literature, how our model\nadvances the vision of future hybrid Human-AI systems.",
          "link": "http://arxiv.org/abs/2102.04121",
          "publishedOn": "2021-07-01T01:59:34.016Z",
          "wordCount": 702,
          "title": "Enhancing Human-Machine Teaming for Medical Prognosis Through Neural Ordinary Differential Equations (NODEs). (arXiv:2102.04121v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:34.010Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:34.004Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>",
          "description": "We introduce a general framework for approximating regular conditional\ndistributions (RCDs). Our approximations of these RCDs are implemented by a new\nclass of geometric deep learning models with inputs in $\\mathbb{R}^d$ and\noutputs in the Wasserstein-$1$ space $\\mathcal{P}_1(\\mathbb{R}^D)$. We find\nthat the models built using our framework can approximate any continuous\nfunctions from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$ uniformly on\ncompacts, and quantitative rates are obtained. We identify two methods for\navoiding the \"curse of dimensionality\"; i.e.: the number of parameters\ndetermining the approximating neural network depends only polynomially on the\ninvolved dimension and the approximation error. The first solution describes\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated on any compact subset of $\\mathbb{R}^d$. Conversely,\nthe second approach describes sets in $\\mathbb{R}^d$, on which any function in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nOur framework is used to obtain an affirmative answer to the open conjecture of\nBishop (1994); namely: mixture density networks are universal regular\nconditional distributions. The predictive performance of the proposed models is\nevaluated against comparable learning models on various probabilistic\npredictions tasks in the context of ELMs, model uncertainty, and\nheteroscedastic regression. All the results are obtained for more general input\nand output spaces and thus apply to geometric deep learning contexts.",
          "link": "http://arxiv.org/abs/2105.07743",
          "publishedOn": "2021-07-01T01:59:33.997Z",
          "wordCount": 698,
          "title": "Universal Regular Conditional Distributions. (arXiv:2105.07743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12199",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jaiswal_P/0/1/0/all/0/1\">Prateek Jaiswal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Honnappa_H/0/1/0/all/0/1\">Harsha Honnappa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rao_V/0/1/0/all/0/1\">Vinayak A. Rao</a>",
          "description": "This paper considers data-driven chance-constrained stochastic optimization\nproblems in a Bayesian framework. Bayesian posteriors afford a principled\nmechanism to incorporate data and prior knowledge into stochastic optimization\nproblems. However, the computation of Bayesian posteriors is typically an\nintractable problem, and has spawned a large literature on approximate Bayesian\ncomputation. Here, in the context of chance-constrained optimization, we focus\non the question of statistical consistency (in an appropriate sense) of the\noptimal value, computed using an approximate posterior distribution. To this\nend, we rigorously prove a frequentist consistency result demonstrating the\nconvergence of the optimal value to the optimal value of a fixed, parameterized\nconstrained optimization problem. We augment this by also establishing a\nprobabilistic rate of convergence of the optimal value. We also prove the\nconvex feasibility of the approximate Bayesian stochastic optimization problem.\nFinally, we demonstrate the utility of our approach on an optimal staffing\nproblem for an M/M/c queueing model.",
          "link": "http://arxiv.org/abs/2106.12199",
          "publishedOn": "2021-07-01T01:59:33.992Z",
          "wordCount": 614,
          "title": "Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency. (arXiv:2106.12199v2 [math.ST] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.986Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alchihabi_A/0/1/0/all/0/1\">Abdullah Alchihabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>",
          "description": "Graph Neural Networks (GNNs) require a relatively large number of labeled\nnodes and a reliable/uncorrupted graph connectivity structure in order to\nobtain good performance on the semi-supervised node classification task. The\nperformance of GNNs can degrade significantly as the number of labeled nodes\ndecreases or the graph connectivity structure is corrupted by adversarial\nattacks or due to noises in data measurement /collection. Therefore, it is\nimportant to develop GNN models that are able to achieve good performance when\nthere is limited supervision knowledge -- a few labeled nodes and noisy graph\nstructures. In this paper, we propose a novel Dual GNN learning framework to\naddress this challenge task. The proposed framework has two GNN based node\nprediction modules. The primary module uses the input graph structure to induce\nregular node embeddings and predictions with a regular GNN baseline, while the\nauxiliary module constructs a new graph structure through fine-grained spectral\nclusterings and learns new node embeddings and predictions. By integrating the\ntwo modules in a dual GNN learning framework, we perform joint learning in an\nend-to-end fashion. This general framework can be applied on many GNN baseline\nmodels. The experimental results validate that the proposed dual GNN framework\ncan greatly outperform the GNN baseline methods when the labeled nodes are\nscarce and the graph connectivity structure is noisy.",
          "link": "http://arxiv.org/abs/2106.15755",
          "publishedOn": "2021-07-01T01:59:33.968Z",
          "wordCount": 650,
          "title": "Dual GNNs: Graph Neural Network Learning with Limited Supervision. (arXiv:2106.15755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Pravin Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Raghavendra Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_A/0/1/0/all/0/1\">Avinash Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>",
          "description": "Federated Learning allows training of data stored in distributed devices\nwithout the need for centralizing training data, thereby maintaining data\nprivacy. Addressing the ability to handle data heterogeneity (non-identical and\nindependent distribution or non-IID) is a key enabler for the wider deployment\nof Federated Learning. In this paper, we propose a novel Divide-and-Conquer\ntraining methodology that enables the use of the popular FedAvg aggregation\nalgorithm by overcoming the acknowledged FedAvg limitations in non-IID\nenvironments. We propose a novel use of Cosine-distance based Weight Divergence\nmetric to determine the exact point where a Deep Learning network can be\ndivided into class agnostic initial layers and class-specific deep layers for\nperforming a Divide and Conquer training. We show that the methodology achieves\ntrained model accuracy at par (and in certain cases exceeding) with numbers\nachieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc.\nAlso, we show that this methodology leads to compute and bandwidth\noptimizations under certain documented conditions.",
          "link": "http://arxiv.org/abs/2106.14503",
          "publishedOn": "2021-07-01T01:59:33.961Z",
          "wordCount": 618,
          "title": "Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data. (arXiv:2106.14503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:33.955Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemmaghami_H/0/1/0/all/0/1\">Houman Ghaemmaghami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "This paper proposes a novel framework for lung sound event detection,\nsegmenting continuous lung sound recordings into discrete events and performing\nrecognition on each event. Exploiting the lightweight nature of Temporal\nConvolution Networks (TCNs) and their superior results compared to their\nrecurrent counterparts, we propose a lightweight, yet robust, and completely\ninterpretable framework for lung sound event detection. We propose the use of a\nmulti-branch TCN architecture and exploit a novel fusion strategy to combine\nthe resultant features from these branches. This not only allows the network to\nretain the most salient information across different temporal granularities and\ndisregards irrelevant information, but also allows our network to process\nrecordings of arbitrary length. Results: The proposed method is evaluated on\nmultiple public and in-house benchmarks of irregular and noisy recordings of\nthe respiratory auscultation process for the identification of numerous\nauscultation events including inhalation, exhalation, crackles, wheeze,\nstridor, and rhonchi. We exceed the state-of-the-art results in all\nevaluations. Furthermore, we empirically analyse the effect of the proposed\nmulti-branch TCN architecture and the feature fusion strategy and provide\nquantitative and qualitative evaluations to illustrate their efficiency.\nMoreover, we provide an end-to-end model interpretation pipeline that\ninterprets the operations of all the components of the proposed framework. Our\nanalysis of different feature fusion strategies shows that the proposed feature\nconcatenation method leads to better suppression of non-informative features,\nwhich drastically reduces the classifier overhead resulting in a robust\nlightweight network.The lightweight nature of our model allows it to be\ndeployed in end-user devices such as smartphones, and it has the ability to\ngenerate predictions in real-time.",
          "link": "http://arxiv.org/abs/2106.15835",
          "publishedOn": "2021-07-01T01:59:33.939Z",
          "wordCount": 719,
          "title": "Robust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings. (arXiv:2106.15835v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:33.933Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nThis study uses a recent deep learning technique, the normalizing flows, to\nproduce accurate probabilistic forecasts that are crucial for decision-makers\nto face the new challenges in power systems applications. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that our methodology is competitive with other\nstate-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-07-01T01:59:33.927Z",
          "wordCount": 585,
          "title": "Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>",
          "description": "We give a fast algorithm to optimally compose privacy guarantees of\ndifferentially private (DP) algorithms to arbitrary accuracy. Our method is\nbased on the notion of privacy loss random variables to quantify the privacy\nloss of DP algorithms. The running time and memory needed for our algorithm to\napproximate the privacy curve of a DP algorithm composed with itself $k$ times\nis $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela\net al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We\ndemonstrate the utility of our algorithm by accurately computing the privacy\nloss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm\nspeeds up the privacy computations by a few orders of magnitude compared to\nprior work, while maintaining similar accuracy.",
          "link": "http://arxiv.org/abs/2106.02848",
          "publishedOn": "2021-07-01T01:59:33.903Z",
          "wordCount": 578,
          "title": "Numerical Composition of Differential Privacy. (arXiv:2106.02848v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1\">Marin Bilo&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Modeling sets is an important problem in machine learning since this type of\ndata can be found in many domains. A promising approach defines a family of\npermutation invariant densities with continuous normalizing flows. This allows\nus to maximize the likelihood directly and sample new realizations with ease.\nIn this work, we demonstrate how calculating the trace, a crucial step in this\nmethod, raises issues that occur both during training and inference, limiting\nits practicality. We propose an alternative way of defining permutation\nequivariant transformations that give closed form trace. This leads not only to\nimprovements while training, but also to better final performance. We\ndemonstrate the benefits of our approach on point processes and general set\nmodeling.",
          "link": "http://arxiv.org/abs/2010.03242",
          "publishedOn": "2021-07-01T01:59:33.897Z",
          "wordCount": 581,
          "title": "Scalable Normalizing Flows for Permutation Invariant Densities. (arXiv:2010.03242v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.11165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1\">David Bindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "Despite advances in scalable models, the inference tools used for Gaussian\nprocesses (GPs) have yet to fully capitalize on developments in computing\nhardware. We present an efficient and general approach to GP inference based on\nBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified\nbatched version of the conjugate gradients algorithm to derive all terms for\ntraining and inference in a single call. BBMM reduces the asymptotic complexity\nof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to\nscalable approximations and complex GP models simply requires a routine for\nefficient matrix-matrix multiplication with the kernel and its derivative. In\naddition, BBMM uses a specialized preconditioner to substantially speed up\nconvergence. In experiments we show that BBMM effectively uses GPU hardware to\ndramatically accelerate both exact GP inference and scalable approximations.\nAdditionally, we provide GPyTorch, a software platform for scalable GP\ninference via BBMM, built on PyTorch.",
          "link": "http://arxiv.org/abs/1809.11165",
          "publishedOn": "2021-07-01T01:59:33.888Z",
          "wordCount": 671,
          "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. (arXiv:1809.11165v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:33.882Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) can be used in various ways to improve multi-user\nmultiple-input multiple-output (MU-MIMO) receive processing. Typical approaches\neither augment a single processing step, such as symbol detection, or replace\nmultiple steps jointly by a single neural network (NN). These techniques\ndemonstrate promising results but often assume perfect channel state\ninformation (CSI) or fail to satisfy the interpretability and scalability\nconstraints imposed by practical systems. In this paper, we propose a new\nstrategy which preserves the benefits of a conventional receiver, but enhances\nspecific parts with ML components. The key idea is to exploit the orthogonal\nfrequency-division multiplexing (OFDM) signal structure to improve both the\ndemapping and the computation of the channel estimation error statistics.\nEvaluation results show that the proposed ML-enhanced receiver beats practical\nbaselines on all considered scenarios, with significant gains at high speeds.",
          "link": "http://arxiv.org/abs/2106.16074",
          "publishedOn": "2021-07-01T01:59:33.856Z",
          "wordCount": 579,
          "title": "Machine Learning-enhanced Receive Processing for MU-MIMO OFDM Systems. (arXiv:2106.16074v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:33.843Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a> (UT1), <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (UT3)",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-01T01:59:33.817Z",
          "wordCount": 726,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiafei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ya_J/0/1/0/all/0/1\">Jiangpeng Ya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Multi-goal reinforcement learning is widely applied in planning and robot\nmanipulation. Two main challenges in multi-goal reinforcement learning are\nsparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims\nto tackle the two challenges via goal relabeling. However, HER-related works\nstill need millions of samples and a huge computation. In this paper, we\npropose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step\nrelabeled returns based on $n$-step relabeling to improve sample efficiency.\nDespite the advantages of $n$-step relabeling, we theoretically and\nexperimentally prove the off-policy $n$-step bias introduced by $n$-step\nrelabeling may lead to poor performance in many environments. To address the\nabove issue, two bias-reduced MHER algorithms, MHER($\\lambda$) and Model-based\nMHER (MMHER) are presented. MHER($\\lambda$) exploits the $\\lambda$ return while\nMMHER benefits from model-based value expansions. Experimental results on\nnumerous multi-goal robotic tasks show that our solutions can successfully\nalleviate off-policy $n$-step bias and achieve significantly higher sample\nefficiency than HER and Curriculum-guided HER with little additional\ncomputation beyond HER.",
          "link": "http://arxiv.org/abs/2102.12962",
          "publishedOn": "2021-07-01T01:59:33.811Z",
          "wordCount": 643,
          "title": "Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning. (arXiv:2102.12962v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:33.793Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "Episodic reinforcement learning and contextual bandits are two widely studied\nsequential decision-making problems. Episodic reinforcement learning\ngeneralizes contextual bandits and is often perceived to be more difficult due\nto long planning horizon and unknown state-dependent transitions. The current\npaper shows that the long planning horizon and the unknown state-dependent\ntransitions (at most) pose little additional difficulty on sample complexity.\n\nWe consider the episodic reinforcement learning with $S$ states, $A$ actions,\nplanning horizon $H$, total reward bounded by $1$, and the agent plays for $K$\nepisodes. We propose a new algorithm, \\textbf{M}onotonic \\textbf{V}alue\n\\textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus.\nCompared to existing bonus constructions, the new bonus is tighter since it is\nbased on a well-designed monotonic value function. In particular, the\n\\emph{constants} in the bonus should be subtly setting to ensure optimism and\nmonotonicity.\n\nWe show MVP enjoys an $O\\left(\\left(\\sqrt{SAK} + S^2A\\right) \\poly\\log\n\\left(SAHK\\right)\\right)$ regret, approaching the\n$\\Omega\\left(\\sqrt{SAK}\\right)$ lower bound of \\emph{contextual bandits} up to\nlogarithmic terms. Notably, this result 1) \\emph{exponentially} improves the\nstate-of-the-art polynomial-time algorithms by Dann et al. [2019] and Zanette\net al. [2019] in terms of the dependency on $H$, and 2) \\emph{exponentially}\nimproves the running time in [Wang et al. 2020] and significantly improves the\ndependency on $S$, $A$ and $K$ in sample complexity.",
          "link": "http://arxiv.org/abs/2009.13503",
          "publishedOn": "2021-07-01T01:59:33.785Z",
          "wordCount": 690,
          "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon. (arXiv:2009.13503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liexin Cheng</a>",
          "description": "Loan risk for small businesses has long been a complex problem worthy of\nexploring. Predicting the loan risk can benefit entrepreneurship by developing\nmore jobs for the society. CatBoost (Categorical Boosting) is a powerful\nmachine learning algorithm suitable for dataset with many categorical variables\nlike the dataset for forecasting loan risk. In this paper, we identify the\nimportant risk factors that contribute to loan status classification problem.\nThen we compare the performance between boosting-type algorithms(especially\nCatBoost) with other traditional yet popular ones. The dataset we adopt in the\nresearch comes from the U.S. Small Business Administration (SBA) and holds a\nvery large sample size (899,164 observations and 27 features). In order to make\nthe best use of the important features in the dataset, we propose a technique\nnamed \"synthetic generation\" to develop more combined features based on\narithmetic operation, which ends up improving the accuracy and AUC of the\noriginal CatBoost model. We obtain a high accuracy of 95.84% and well-performed\nAUC of 98.80% compared with the existent literature of related research.",
          "link": "http://arxiv.org/abs/2106.07954",
          "publishedOn": "2021-07-01T01:59:33.778Z",
          "wordCount": 646,
          "title": "CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Dimension reduction (DR) aims to learn low-dimensional representations of\nhigh-dimensional data with the preservation of essential information. In the\ncontext of manifold learning, we define that the representation after\ninformation-lossless DR preserves the topological and geometric properties of\ndata manifolds formally, and propose a novel two-stage DR method, called\ninvertible manifold learning (inv-ML) to bridge the gap between theoretical\ninformation-lossless and practical DR. The first stage includes a homeomorphic\nsparse coordinate transformation to learn low-dimensional representations\nwithout destroying topology and a local isometry constraint to preserve local\ngeometry. In the second stage, a linear compression is implemented for the\ntrade-off between the target dimension and the incurred information loss in\nexcessive DR scenarios. Experiments are conducted on seven datasets with a\nneural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc\nachieves invertible DR in comparison with typical existing methods as well as\nreveals the characteristics of the learned manifolds. Through latent space\ninterpolation on real-world datasets, we find that the reliability of tangent\nspace approximated by the local neighborhood is the key to the success of\nmanifold-based DR algorithms.",
          "link": "http://arxiv.org/abs/2010.04012",
          "publishedOn": "2021-07-01T01:59:33.772Z",
          "wordCount": 652,
          "title": "Invertible Manifold Learning for Dimension Reduction. (arXiv:2010.04012v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiashuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Machine learning algorithms with empirical risk minimization are vulnerable\nunder distributional shifts due to the greedy adoption of all the correlations\nfound in training data. There is an emerging literature on tackling this\nproblem by minimizing the worst-case risk over an uncertainty set. However,\nexisting methods mostly construct ambiguity sets by treating all variables\nequally regardless of the stability of their correlations with the target,\nresulting in the overwhelmingly-large uncertainty set and low confidence of the\nlearner. In this paper, we propose a novel Stable Adversarial Learning (SAL)\nalgorithm that leverages heterogeneous data sources to construct a more\npractical uncertainty set and conduct differentiated robustness optimization,\nwhere covariates are differentiated according to the stability of their\ncorrelations with the target. We theoretically show that our method is\ntractable for stochastic gradient-based optimization and provide the\nperformance guarantees for our method. Empirical studies on both simulation and\nreal datasets validate the effectiveness of our method in terms of uniformly\ngood performance across unknown distributional shifts.",
          "link": "http://arxiv.org/abs/2106.15791",
          "publishedOn": "2021-07-01T01:59:33.756Z",
          "wordCount": 604,
          "title": "Distributionally Robust Learning with Stable Adversarial Training. (arXiv:2106.15791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1\">Nader Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>",
          "description": "In the online continual learning paradigm, agents must learn from a changing\ndistribution while respecting memory and compute constraints. Previous work in\nthis setting often tries to reduce catastrophic forgetting by limiting changes\nin the space of model parameters. In this work we instead focus on the change\nin representations of observed data that arises when previously unobserved\nclasses appear in the incoming data stream, and new classes must be\ndistinguished from previous ones. Starting from a popular approach, experience\nreplay, we consider metric learning based loss functions which, when adjusted\nto appropriately select negative samples, can effectively nudge the learned\nrepresentations to be more robust to new future classes. We show that this\nselection of negatives is in fact critical for reducing representation drift of\npreviously observed data. Motivated by this we further introduce a simple\nadjustment to the standard cross entropy loss used in prior experience replay\nthat achieves similar effect. Our approach directly improves the performance of\nexperience replay for this setting, obtaining state-of-the-art results on\nseveral existing benchmarks in online continual learning, while remaining\nefficient in both memory and compute. We release an implementation of our\nexperiments at https://github.com/naderAsadi/AML",
          "link": "http://arxiv.org/abs/2104.05025",
          "publishedOn": "2021-07-01T01:59:33.750Z",
          "wordCount": 657,
          "title": "Reducing Representation Drift in Online Continual Learning. (arXiv:2104.05025v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15933",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ged_F/0/1/0/all/0/1\">Fran&#xe7;ois Ged</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gabriel_F/0/1/0/all/0/1\">Franck Gabriel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin &#x15e;im&#x15f;ek</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>",
          "description": "For deep linear networks (DLN), various hyperparameters alter the dynamics of\ntraining dramatically. We investigate how the rank of the linear map found by\ngradient descent is affected by (1) the initialization norm and (2) the\naddition of $L_{2}$ regularization on the parameters. For (1), we study two\nregimes: (1a) the linear/lazy regime, for large norm initialization; (1b) a\n\\textquotedbl saddle-to-saddle\\textquotedbl{} regime for small initialization\nnorm. In the (1a) setting, the dynamics of a DLN of any depth is similar to\nthat of a standard linear model, without any low-rank bias. In the (1b)\nsetting, we conjecture that throughout training, gradient descent approaches a\nsequence of saddles, each corresponding to linear maps of increasing rank,\nuntil reaching a minimal rank global minimum. We support this conjecture with a\npartial proof and some numerical experiments. For (2), we show that adding a\n$L_{2}$ regularization on the parameters corresponds to the addition to the\ncost of a $L_{p}$-Schatten (quasi)norm on the linear map with $p=\\frac{2}{L}$\n(for a depth-$L$ network), leading to a stronger low-rank bias as $L$ grows.\nThe effect of $L_{2}$ regularization on the loss surface depends on the depth:\nfor shallow networks, all critical points are either strict saddles or global\nminima, whereas for deep networks, some local minima appear. We numerically\nobserve that these local minima can generalize better than global ones in some\nsettings.",
          "link": "http://arxiv.org/abs/2106.15933",
          "publishedOn": "2021-07-01T01:59:33.744Z",
          "wordCount": 672,
          "title": "Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization. (arXiv:2106.15933v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1301.6697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We show that the only parameter prior for complete Gaussian DAG models that\nsatisfies global parameter independence, complete model equivalence, and some\nweak regularity assumptions, is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\nW be an n x n, n >= 3, positive-definite symmetric matrix of random variables\nand f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if\nW_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every\nblock partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar\ncharacterizations of the normal and normal-Wishart distributions are provided\nas well. We also show how to construct a prior for every DAG model over X from\nthe prior of a single regression model.",
          "link": "http://arxiv.org/abs/1301.6697",
          "publishedOn": "2021-07-01T01:59:33.737Z",
          "wordCount": 643,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:1301.6697v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:33.731Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:33.725Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:33.700Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:33.694Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15649",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makarov_P/0/1/0/all/0/1\">Peter Makarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slangens_S/0/1/0/all/0/1\">Simon Slangens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "We propose a novel Multi-Scale Spectrogram (MSS) modelling approach to\nsynthesise speech with an improved coarse and fine-grained prosody. We present\na generic multi-scale spectrogram prediction mechanism where the system first\npredicts coarser scale mel-spectrograms that capture the suprasegmental\ninformation in speech, and later uses these coarser scale mel-spectrograms to\npredict finer scale mel-spectrograms capturing fine-grained prosody.\n\nWe present details for two specific versions of MSS called Word-level MSS and\nSentence-level MSS where the scales in our system are motivated by the\nlinguistic units. The Word-level MSS models word, phoneme, and frame-level\nspectrograms while Sentence-level MSS models sentence-level spectrogram in\naddition.\n\nSubjective evaluations show that Word-level MSS performs statistically\nsignificantly better compared to the baseline on two voices.",
          "link": "http://arxiv.org/abs/2106.15649",
          "publishedOn": "2021-07-01T01:59:33.688Z",
          "wordCount": 580,
          "title": "Multi-Scale Spectrogram Modelling for Neural Text-to-Speech. (arXiv:2106.15649v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_E/0/1/0/all/0/1\">Ermin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_R/0/1/0/all/0/1\">Randall Berry</a>",
          "description": "Federated learning enables machine learning algorithms to be trained over a\nnetwork of multiple decentralized edge devices without requiring the exchange\nof local datasets. Successfully deploying federated learning requires ensuring\nthat agents (e.g., mobile devices) faithfully execute the intended algorithm,\nwhich has been largely overlooked in the literature. In this study, we first\nuse risk bounds to analyze how the key feature of federated learning,\nunbalanced and non-i.i.d. data, affects agents' incentives to voluntarily\nparticipate and obediently follow traditional federated learning algorithms.\n\nTo be more specific, our analysis reveals that agents with less typical data\ndistributions and relatively more samples are more likely to opt out of or\ntamper with federated learning algorithms. To this end, we formulate the first\nfaithful implementation problem of federated learning and design two faithful\nfederated learning mechanisms which satisfy economic properties, scalability,\nand privacy. Further, the time complexity of computing all agents' payments in\nthe number of agents is $\\mathcal{O}(1)$. First, we design a Faithful Federated\nLearning (FFL) mechanism which approximates the Vickrey-Clarke-Groves (VCG)\npayments via an incremental computation. We show that it achieves (probably\napproximate) optimality, faithful implementation, voluntary participation, and\nsome other economic properties (such as budget balance). Second, by\npartitioning agents into several subsets, we present a scalable VCG mechanism\napproximation. We further design a scalable and Differentially Private FFL\n(DP-FFL) mechanism, the first differentially private faithful mechanism, that\nmaintains the economic properties. Our mechanism enables one to make three-way\nperformance tradeoffs among privacy, the iterations needed, and payment\naccuracy loss.",
          "link": "http://arxiv.org/abs/2106.15905",
          "publishedOn": "2021-07-01T01:59:33.670Z",
          "wordCount": 701,
          "title": "Faithful Edge Federated Learning: Scalability and Privacy. (arXiv:2106.15905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We study whether and how can we model a joint distribution $p(x,z)$ using two\nconditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated\nby the observation that deep generative models, in addition to a likelihood\nmodel $p(x|z)$, often also use an inference model $q(z|x)$ for data\nrepresentation, but they rely on a usually uninformative prior distribution\n$p(z)$ to define a joint distribution, which may render problems like posterior\ncollapse and manifold mismatch. To explore the possibility to model a joint\ndistribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and\ndeterminacy, corresponding to the existence and uniqueness of a joint\ndistribution whose conditional distributions coincide with them. We develop a\ngeneral theory for novel and operable equivalence criteria for compatibility,\nand sufficient conditions for determinacy. Based on the theory, we propose the\nCyGen framework for cyclic-conditional generative modeling, including methods\nto enforce compatibility and use the determined distribution to fit and\ngenerate data. With the prior constraint removed, CyGen better fits data and\ncaptures more representative features, supported by experiments showing better\ngeneration and downstream classification performance.",
          "link": "http://arxiv.org/abs/2106.15962",
          "publishedOn": "2021-07-01T01:59:33.664Z",
          "wordCount": 620,
          "title": "On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:33.658Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:33.652Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03248",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We develop simple methods for constructing parameter priors for model choice\namong Directed Acyclic Graphical (DAG) models. In particular, we introduce\nseveral assumptions that permit the construction of parameter priors for a\nlarge number of DAG models from a small set of assessments. We then present a\nmethod for directly computing the marginal likelihood of every DAG model given\na random sample with no missing observations. We apply this methodology to\nGaussian DAG models which consist of a recursive set of linear regression\nmodels. We show that the only parameter prior for complete Gaussian DAG models\nthat satisfies our assumptions is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\n$W$ be an $n \\times n$, $n \\ge 3$, positive-definite symmetric matrix of random\nvariables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if\nand only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of\n$\\{W_{12},W_{22}\\}$ for every block partitioning $W_{11},W_{12}, W'_{12},\nW_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart\ndistributions are provided as well.",
          "link": "http://arxiv.org/abs/2105.03248",
          "publishedOn": "2021-07-01T01:59:33.646Z",
          "wordCount": 675,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:2105.03248v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peel_T/0/1/0/all/0/1\">Thomas Peel</a>",
          "description": "Missing data is a recurrent and challenging problem, especially when using\nmachine learning algorithms for real-world applications. For this reason,\nmissing data imputation has become an active research area, in which recent\ndeep learning approaches have achieved state-of-the-art results. We propose\nDAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a\ndenoising autoencoder architecture with an attention mechanism. While most\nimputation algorithms use incomplete inputs as they would use complete data -\nup to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based\nattention mechanism to focus on the observed values of its inputs. We evaluate\nDAEMA both in terms of reconstruction capabilities and downstream prediction\nand show that it achieves superior performance to state-of-the-art algorithms\non several publicly available real-world datasets under various missingness\nsettings.",
          "link": "http://arxiv.org/abs/2106.16057",
          "publishedOn": "2021-07-01T01:59:33.629Z",
          "wordCount": 578,
          "title": "DAEMA: Denoising Autoencoder with Mask Attention. (arXiv:2106.16057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nagahama_M/0/1/0/all/0/1\">Masatoshi Nagahama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamada_K/0/1/0/all/0/1\">Koki Yamada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_Y/0/1/0/all/0/1\">Yuichi Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Graph signal processing is a ubiquitous task in many applications such as\nsensor, social, transportation and brain networks, point cloud processing, and\ngraph neural networks. Graph signals are often corrupted through sensing\nprocesses, and need to be restored for the above applications. In this paper,\nwe propose two graph signal restoration methods based on deep algorithm\nunrolling (DAU). First, we present a graph signal denoiser by unrolling\niterations of the alternating direction method of multiplier (ADMM). We then\npropose a general restoration method for linear degradation by unrolling\niterations of Plug-and-Play ADMM (PnP-ADMM). In the second method, the unrolled\nADMM-based denoiser is incorporated as a submodule. Therefore, our restoration\nmethod has a nested DAU structure. Thanks to DAU, parameters in the proposed\ndenoising/restoration methods are trainable in an end-to-end manner. Since the\nproposed restoration methods are based on iterations of a (convex) optimization\nalgorithm, the method is interpretable and keeps the number of parameters small\nbecause we only need to tune graph-independent regularization parameters. We\nsolve two main problems in existing graph signal restoration methods: 1)\nlimited performance of convex optimization algorithms due to fixed parameters\nwhich are often determined manually. 2) large number of parameters of graph\nneural networks that result in difficulty of training. Several experiments for\ngraph signal denoising and interpolation are performed on synthetic and\nreal-world data. The proposed methods show performance improvements to several\nexisting methods in terms of root mean squared error in both tasks.",
          "link": "http://arxiv.org/abs/2106.15910",
          "publishedOn": "2021-07-01T01:59:33.623Z",
          "wordCount": 683,
          "title": "Graph Signal Restoration Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:33.617Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.10502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aschenbach_M/0/1/0/all/0/1\">Martin Aschenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1\">Timo K&#xf6;tzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1\">Karen Seidel</a>",
          "description": "Learning from positive and negative information, so-called \\emph{informants},\nbeing one of the models for human and machine learning introduced by\nE.~M.~Gold, is investigated. Particularly, naturally arising questions about\nthis learning setting, originating in results on learning from solely positive\ninformation, are answered. By a carefully arranged argument learners can be\nassumed to only change their hypothesis in case it is inconsistent with the\ndata (such a learning behavior is called \\emph{conservative}). The deduced main\ntheorem states the relations between the most important delayable learning\nsuccess criteria, being the ones not ruined by a delayed in time hypothesis\noutput. Additionally, our investigations concerning the non-delayable\nrequirement of consistent learning underpin the claim for \\emph{delayability}\nbeing the right structural property to gain a deeper understanding concerning\nthe nature of learning success criteria. Moreover, we obtain an anomalous\n\\emph{hierarchy} when allowing for an increasing finite number of\n\\emph{anomalies} of the hypothesized language by the learner compared with the\nlanguage to be learned. In contrast to the vacillatory hierarchy for learning\nfrom solely positive information, we observe a \\emph{duality} depending on\nwhether infinitely many \\emph{vacillations} between different (almost) correct\nhypotheses are still considered a successful learning behavior.",
          "link": "http://arxiv.org/abs/1801.10502",
          "publishedOn": "2021-07-01T01:59:33.610Z",
          "wordCount": 686,
          "title": "Learning from Informants: Relations between Learning Success Criteria. (arXiv:1801.10502v5 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in an open-world setting. However, existing OOD detection\nsolutions can be brittle in the open world, facing various types of adversarial\nOOD inputs. While methods leveraging auxiliary OOD data have emerged, our\nanalysis on illuminative examples reveals a key insight that the majority of\nauxiliary OOD examples may not meaningfully improve or even hurt the decision\nboundary of the OOD detector, which is also observed in empirical results on\nreal data. In this paper, we provide a theoretically motivated method,\nAdversarial Training with informative Outlier Mining (ATOM), which improves the\nrobustness of OOD detection. We show that, by mining informative auxiliary OOD\ndata, one can significantly improve OOD detection performance, and somewhat\nsurprisingly, generalize to unseen adversarial attacks. ATOM achieves\nstate-of-the-art performance under a broad family of classic and adversarial\nOOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,\nATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,\nsurpassing the previous best baseline by a large margin.",
          "link": "http://arxiv.org/abs/2006.15207",
          "publishedOn": "2021-07-01T01:59:33.587Z",
          "wordCount": 669,
          "title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiaoben_Y/0/1/0/all/0/1\">You Qiaoben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Recent works demonstrate that deep reinforcement learning (DRL) models are\nvulnerable to adversarial attacks which can decrease the victim's total reward\nby manipulating the observations. Compared with adversarial attacks in\nsupervised learning, it is much more challenging to deceive a DRL model since\nthe adversary has to infer the environmental dynamics. To address this issue,\nwe reformulate the problem of adversarial attacks in function space and\nseparate the previous gradient based attacks into several subspace. Following\nthe analysis of the function space, we design a generic two-stage framework in\nthe subspace where the adversary lures the agent to a target trajectory or a\ndeceptive policy. In the first stage, we train a deceptive policy by hacking\nthe environment, and discover a set of trajectories routing to the lowest\nreward. The adversary then misleads the victim to imitate the deceptive policy\nby perturbing the observations. Our method provides a tighter theoretical upper\nbound for the attacked agent's performance than the existing approaches.\nExtensive experiments demonstrate the superiority of our method and we achieve\nthe state-of-the-art performance on both Atari and MuJoCo environments.",
          "link": "http://arxiv.org/abs/2106.15860",
          "publishedOn": "2021-07-01T01:59:33.581Z",
          "wordCount": 617,
          "title": "Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning. (arXiv:2106.15860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.575Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsirtsis_S/0/1/0/all/0/1\">Stratis Tsirtsis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lorch_L/0/1/0/all/0/1\">Lars Lorch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Testing is recommended for all close contacts of confirmed COVID-19 patients.\nHowever, existing group testing methods are oblivious to the circumstances of\ncontagion provided by contact tracing. Here, we build upon a well-known\nsemi-adaptive pool testing method, Dorfman's method with imperfect tests, and\nderive a simple group testing method based on dynamic programming that is\nspecifically designed to use the information provided by contact tracing.\nExperiments using a variety of reproduction numbers and dispersion levels,\nincluding those estimated in the context of the COVID-19 pandemic, show that\nthe pools found using our method result in a significantly lower number of\ntests than those found using standard Dorfman's method, especially when the\nnumber of contacts of an infected individual is small. Moreover, our results\nshow that our method can be more beneficial when the secondary infections are\nhighly overdispersed.",
          "link": "http://arxiv.org/abs/2106.15988",
          "publishedOn": "2021-07-01T01:59:33.568Z",
          "wordCount": 614,
          "title": "Group Testing under Superspreading Dynamics. (arXiv:2106.15988v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10657",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zaidenberg_D/0/1/0/all/0/1\">Daniela A. Zaidenberg</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "This concept paper aims to provide a brief outline of quantum computers,\nexplore existing methods of quantum image classification techniques, so\nfocusing on remote sensing applications, and discuss the bottlenecks of\nperforming these algorithms on currently available open source platforms.\nInitial results demonstrate feasibility. Next steps include expanding the size\nof the quantum hidden layer and increasing the variety of output image options.",
          "link": "http://arxiv.org/abs/2101.10657",
          "publishedOn": "2021-07-01T01:59:33.543Z",
          "wordCount": 542,
          "title": "Advantages and Bottlenecks of Quantum Machine Learning for Remote Sensing. (arXiv:2101.10657v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-07-01T01:59:33.530Z",
          "wordCount": 672,
          "title": "Off-Belief Learning. (arXiv:2103.04000v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:33.524Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:33.501Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>",
          "description": "Studying neural network loss landscapes provides insights into the nature of\nthe underlying optimization problems. Unfortunately, loss landscapes are\nnotoriously difficult to visualize in a human-comprehensible fashion. One\ncommon way to address this problem is to plot linear slices of the landscape,\nfor example from the initial state of the network to the final state after\noptimization. On the basis of this analysis, prior work has drawn broader\nconclusions about the difficulty of the optimization problem. In this paper, we\nput inferences of this kind to the test, systematically evaluating how linear\ninterpolation and final performance vary when altering the data, choice of\ninitialization, and other optimizer and architecture design choices. Further,\nwe use linear interpolation to study the role played by individual layers and\nsubstructures of the network. We find that certain layers are more sensitive to\nthe choice of initialization and optimizer hyperparameter settings, and we\nexploit these observations to design custom optimization schemes. However, our\nresults cast doubt on the broader intuition that the presence or absence of\nbarriers when interpolating necessarily relates to the success of optimization.",
          "link": "http://arxiv.org/abs/2106.16004",
          "publishedOn": "2021-07-01T01:59:33.495Z",
          "wordCount": 622,
          "title": "What can linear interpolation of neural network loss landscapes tell us?. (arXiv:2106.16004v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:33.489Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arindam Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Amitabha Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>",
          "description": "We present Fast Random projection-based One-Class Classification (FROCC), an\nextremely efficient method for one-class classification. Our method is based on\na simple idea of transforming the training data by projecting it onto a set of\nrandom unit vectors that are chosen uniformly and independently from the unit\nsphere, and bounding the regions based on separation of the data. FROCC can be\nnaturally extended with kernels. We theoretically prove that FROCC generalizes\nwell in the sense that it is stable and has low bias. FROCC achieves up to 3.1\npercent points better ROC, with 1.2--67.8x speedup in training and test times\nover a range of state-of-the-art benchmarks including the SVM and the deep\nlearning based models for the OCC task.",
          "link": "http://arxiv.org/abs/2011.14317",
          "publishedOn": "2021-07-01T01:59:33.483Z",
          "wordCount": 586,
          "title": "FROCC: Fast Random projection-based One-Class Classification. (arXiv:2011.14317v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>",
          "description": "Online metric learning has been widely exploited for large-scale data\nclassification due to the low computational cost. However, amongst online\npractical scenarios where the features are evolving (e.g., some features are\nvanished and some new features are augmented), most metric learning models\ncannot be successfully applied to these scenarios, although they can tackle the\nevolving instances efficiently. To address the challenge, we develop a new\nonline Evolving Metric Learning (EML) model for incremental and decremental\nfeatures, which can handle the instance and feature evolutions simultaneously\nby incorporating with a smoothed Wasserstein metric distance. Specifically, our\nmodel contains two essential stages: a Transforming stage (T-stage) and a\nInheriting stage (I-stage). For the T-stage, we propose to extract important\ninformation from vanished features while neglecting non-informative knowledge,\nand forward it into survived features by transforming them into a low-rank\ndiscriminative metric space. It further explores the intrinsic low-rank\nstructure of heterogeneous samples to reduce the computation and memory burden\nespecially for highly-dimensional large-scale data. For the I-stage, we inherit\nthe metric performance of survived features from the T-stage and then expand to\ninclude the new augmented features. Moreover, a smoothed Wasserstein distance\nis utilized to characterize the similarity relationships among the\nheterogeneous and complex samples, since the evolving features are not strictly\naligned in the different stages. In addition to tackling the challenges in\none-shot case, we also extend our model into multishot scenario. After deriving\nan efficient optimization strategy for both T-stage and I-stage, extensive\nexperiments on several datasets verify the superior performance of our EML\nmodel.",
          "link": "http://arxiv.org/abs/2006.15334",
          "publishedOn": "2021-07-01T01:59:33.465Z",
          "wordCount": 738,
          "title": "Evolving Metric Learning for Incremental and Decremental Features. (arXiv:2006.15334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04259",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Even_M/0/1/0/all/0/1\">Mathieu Even</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.",
          "link": "http://arxiv.org/abs/2102.04259",
          "publishedOn": "2021-07-01T01:59:33.418Z",
          "wordCount": 619,
          "title": "Concentration of Non-Isotropic Random Tensors with Applications to Learning and Empirical Risk Minimization. (arXiv:2102.04259v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:33.411Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amich_A/0/1/0/all/0/1\">Abderrahmen Amich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshete_B/0/1/0/all/0/1\">Birhanu Eshete</a>",
          "description": "Machine Learning (ML) models are susceptible to evasion attacks. Evasion\naccuracy is typically assessed using aggregate evasion rate, and it is an open\nquestion whether aggregate evasion rate enables feature-level diagnosis on the\neffect of adversarial perturbations on evasive predictions. In this paper, we\nintroduce a novel framework that harnesses explainable ML methods to guide\nhigh-fidelity assessment of ML evasion attacks. Our framework enables\nexplanation-guided correlation analysis between pre-evasion perturbations and\npost-evasion explanations. Towards systematic assessment of ML evasion attacks,\nwe propose and evaluate a novel suite of model-agnostic metrics for\nsample-level and dataset-level correlation analysis. Using malware and image\nclassifiers, we conduct comprehensive evaluations across diverse model\narchitectures and complementary feature representations. Our explanation-guided\ncorrelation analysis reveals correlation gaps between adversarial samples and\nthe corresponding perturbations performed on them. Using a case study on\nexplanation-guided evasion, we show the broader usage of our methodology for\nassessing robustness of ML models.",
          "link": "http://arxiv.org/abs/2106.15820",
          "publishedOn": "2021-07-01T01:59:33.406Z",
          "wordCount": 602,
          "title": "Explanation-Guided Diagnosis of Machine Learning Evasion Attacks. (arXiv:2106.15820v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sijia Linda Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_O/0/1/0/all/0/1\">Omkar Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1\">Austin R. Benson</a>",
          "description": "Graphs are a common model for complex relational data such as social networks\nand protein interactions, and such data can evolve over time (e.g., new\nfriendships) and be noisy (e.g., unmeasured interactions). Link prediction aims\nto predict future edges or infer missing edges in the graph, and has diverse\napplications in recommender systems, experimental design, and complex systems.\nEven though link prediction algorithms strongly depend on the set of edges in\nthe graph, existing approaches typically do not modify the graph topology to\nimprove performance. Here, we demonstrate how simply adding a set of edges,\nwhich we call a \\emph{proposal set}, to the graph as a pre-processing step can\nimprove the performance of several link prediction algorithms. The underlying\nidea is that if the edges in the proposal set generally align with the\nstructure of the graph, link prediction algorithms are further guided towards\npredicting the right edges; in other words, adding a proposal set of edges is a\nsignal-boosting pre-processing step. We show how to use existing link\nprediction algorithms to generate effective proposal sets and evaluate this\napproach on various synthetic and empirical datasets. We find that proposal\nsets meaningfully improve the accuracy of link prediction algorithms based on\nboth neighborhood heuristics and graph neural networks. Code is available at\n\\url{https://github.com/CUAI/Edge-Proposal-Sets}.",
          "link": "http://arxiv.org/abs/2106.15810",
          "publishedOn": "2021-07-01T01:59:33.389Z",
          "wordCount": 657,
          "title": "Edge Proposal Sets for Link Prediction. (arXiv:2106.15810v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1\">Yisroel Mirsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotak_J/0/1/0/all/0/1\">Jaidip Kotak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1\">Ram Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelei_D/0/1/0/all/0/1\">Deng Gelei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wenke Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>",
          "description": "AI has provided us with the ability to automate tasks, extract information\nfrom vast amounts of data, and synthesize media that is nearly\nindistinguishable from the real thing. However, positive tools can also be used\nfor negative purposes. In particular, cyber adversaries can use AI (such as\nmachine learning) to enhance their attacks and expand their campaigns.\n\nAlthough offensive AI has been discussed in the past, there is a need to\nanalyze and understand the threat in the context of organizations. For example,\nhow does an AI-capable adversary impact the cyber kill chain? Does AI benefit\nthe attacker more than the defender? What are the most significant AI threats\nfacing organizations today and what will be their impact on the future?\n\nIn this survey, we explore the threat of offensive AI on organizations.\nFirst, we present the background and discuss how AI changes the adversary's\nmethods, strategies, goals, and overall attack model. Then, through a\nliterature review, we identify 33 offensive AI capabilities which adversaries\ncan use to enhance their attacks. Finally, through a user study spanning\nindustry and academia, we rank the AI threats and provide insights on the\nadversaries.",
          "link": "http://arxiv.org/abs/2106.15764",
          "publishedOn": "2021-07-01T01:59:33.382Z",
          "wordCount": 645,
          "title": "The Threat of Offensive AI to Organizations. (arXiv:2106.15764v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15842",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_W/0/1/0/all/0/1\">Wen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiqiang Li</a>",
          "description": "Remaining useful life prediction (RUL) is one of the key technologies of\ncondition-based maintenance, which is important to maintain the reliability and\nsafety of industrial equipments. While deep learning has achieved great success\nin RUL prediction, existing methods have difficulties in processing long\nsequences and extracting information from the sensor and time step aspects. In\nthis paper, we propose Dual Aspect Self-attention based on Transformer (DAST),\na novel deep RUL prediction method. DAST consists of two encoders, which work\nin parallel to simultaneously extract features of different sensors and time\nsteps. Solely based on self-attention, the DAST encoders are more effective in\nprocessing long data sequences, and are capable of adaptively learning to focus\non more important parts of input. Moreover, the parallel feature extraction\ndesign avoids mutual influence of information from two aspects. Experimental\nresults on two real turbofan engine datasets show that our method significantly\noutperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15842",
          "publishedOn": "2021-07-01T01:59:33.370Z",
          "wordCount": 596,
          "title": "Dual Aspect Self-Attention based on Transformer for Remaining Useful Life Prediction. (arXiv:2106.15842v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1\">Carlo Ciliberto</a>",
          "description": "Finding a good way to model probability densities is key to probabilistic\ninference. An ideal model should be able to concisely approximate any\nprobability, while being also compatible with two main operations:\nmultiplications of two models (product rule) and marginalization with respect\nto a subset of the random variables (sum rule). In this work, we show that a\nrecently proposed class of positive semi-definite (PSD) models for non-negative\nfunctions is particularly suited to this end. In particular, we characterize\nboth approximation and generalization capabilities of PSD models, showing that\nthey enjoy strong theoretical guarantees. Moreover, we show that we can perform\nefficiently both sum and product rule in closed form via matrix operations,\nenjoying the same versatility of mixture models. Our results open the way to\napplications of PSD models to density estimation, decision theory and\ninference. Preliminary empirical evaluation supports our findings.",
          "link": "http://arxiv.org/abs/2106.16116",
          "publishedOn": "2021-07-01T01:59:33.364Z",
          "wordCount": 581,
          "title": "PSD Representations for Effective Probability Models. (arXiv:2106.16116v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03415",
          "author": "<a href=\"http://arxiv.org/find/nlin/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Forgoston_E/0/1/0/all/0/1\">Eric Forgoston</a>",
          "description": "Extracting predictive models from nonlinear systems is a central task in\nscientific machine learning. One key problem is the reconciliation between\nmodern data-driven approaches and first principles. Despite rapid advances in\nmachine learning techniques, embedding domain knowledge into data-driven models\nremains a challenge. In this work, we present a universal learning framework\nfor extracting predictive models from nonlinear systems based on observations.\nOur framework can readily incorporate first principle knowledge because it\nnaturally models nonlinear systems as continuous-time systems. This both\nimproves the extracted models' extrapolation power and reduces the amount of\ndata needed for training. In addition, our framework has the advantages of\nrobustness to observational noise and applicability to irregularly sampled\ndata. We demonstrate the effectiveness of our scheme by learning predictive\nmodels for a wide variety of systems including a stiff Van der Pol oscillator,\nthe Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz\nsystem, different types of domain knowledge are incorporated to demonstrate the\nstrength of knowledge embedding in data-driven system identification.",
          "link": "http://arxiv.org/abs/2010.03415",
          "publishedOn": "2021-07-01T01:59:33.358Z",
          "wordCount": 642,
          "title": "Knowledge-Based Learning of Nonlinear Dynamics and Chaos. (arXiv:2010.03415v3 [nlin.CD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seungwoong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hawoong Jeong</a>",
          "description": "Invariants and conservation laws convey critical information about the\nunderlying dynamics of a system, yet it is generally infeasible to find them\nfrom large-scale data without any prior knowledge or human insight. We propose\nConservNet to achieve this goal, a neural network that spontaneously discovers\na conserved quantity from grouped data where the members of each group share\ninvariants, similar to a general experimental setting where trajectories from\ndifferent trials are observed. As a neural network trained with a novel and\nintuitive loss function called noise-variance loss, ConservNet learns the\nhidden invariants in each group of multi-dimensional observables in a\ndata-driven, end-to-end manner. Our model successfully discovers underlying\ninvariants from the simulated systems having invariants as well as a real-world\ndouble pendulum trajectory. Since the model is robust to various noises and\ndata conditions compared to baseline, our approach is directly applicable to\nexperimental data for discovering hidden conservation laws and further, general\nrelationships between variables.",
          "link": "http://arxiv.org/abs/2102.04008",
          "publishedOn": "2021-07-01T01:59:33.342Z",
          "wordCount": 635,
          "title": "Discovering conservation laws from trajectories via machine learning. (arXiv:2102.04008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehenkel_A/0/1/0/all/0/1\">Antoine Wehenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>",
          "description": "Among likelihood-based approaches for deep generative modelling, variational\nautoencoders (VAEs) offer scalable amortized posterior inference and fast\nsampling. However, VAEs are also more and more outperformed by competing models\nsuch as normalizing flows (NFs), deep-energy models, or the new denoising\ndiffusion probabilistic models (DDPMs). In this preliminary work, we improve\nVAEs by demonstrating how DDPMs can be used for modelling the prior\ndistribution of the latent variables. The diffusion prior model improves upon\nGaussian priors of classical VAEs and is competitive with NF-based priors.\nFinally, we hypothesize that hierarchical VAEs could similarly benefit from the\nenhanced capacity of diffusion priors.",
          "link": "http://arxiv.org/abs/2106.15671",
          "publishedOn": "2021-07-01T01:59:33.284Z",
          "wordCount": 520,
          "title": "Diffusion Priors In Variational Autoencoders. (arXiv:2106.15671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saboo_K/0/1/0/all/0/1\">Krishnakant V. Saboo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1\">Anirudh Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yurui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrell_G/0/1/0/all/0/1\">Gregory A. Worrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David T. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Ravishankar K. Iyer</a>",
          "description": "We model Alzheimer's disease (AD) progression by combining differential\nequations (DEs) and reinforcement learning (RL) with domain knowledge. DEs\nprovide relationships between some, but not all, factors relevant to AD. We\nassume that the missing relationships must satisfy general criteria about the\nworking of the brain, for e.g., maximizing cognition while minimizing the cost\nof supporting cognition. This allows us to extract the missing relationships by\nusing RL to optimize an objective (reward) function that captures the above\ncriteria. We use our model consisting of DEs (as a simulator) and the trained\nRL agent to predict individualized 10-year AD progression using baseline (year\n0) features on synthetic and real data. The model was comparable or better at\npredicting 10-year cognition trajectories than state-of-the-art learning-based\nmodels. Our interpretable model demonstrated, and provided insights into,\n\"recovery/compensatory\" processes that mitigate the effect of AD, even though\nthose processes were not explicitly encoded in the model. Our framework\ncombines DEs with RL for modelling AD progression and has broad applicability\nfor understanding other neurological disorders.",
          "link": "http://arxiv.org/abs/2106.16187",
          "publishedOn": "2021-07-01T01:59:33.202Z",
          "wordCount": 621,
          "title": "Reinforcement Learning based Disease Progression Model for Alzheimer's Disease. (arXiv:2106.16187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:33.177Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Orthogonal frequency-division multiplexing (OFDM) is widely used in modern\nwireless networks thanks to its efficient handling of multipath environment.\nHowever, it suffers from a poor peak-to-average power ratio (PAPR) which\nrequires a large power backoff, degrading the power amplifier (PA) efficiency.\nIn this work, we propose to use a neural network (NN) at the transmitter to\nlearn a high-dimensional modulation scheme allowing to control the PAPR and\nadjacent channel leakage ratio (ACLR). On the receiver side, a NN-based\nreceiver is implemented to carry out demapping of the transmitted bits. The two\nNNs operate on top of OFDM, and are jointly optimized in and end-to-end manner\nusing a training algorithm that enforces constraints on the PAPR and ACLR.\nSimulation results show that the learned waveforms enable higher information\nrates than a tone reservation baseline, while satisfying predefined PAPR and\nACLR targets.",
          "link": "http://arxiv.org/abs/2106.16039",
          "publishedOn": "2021-07-01T01:59:33.148Z",
          "wordCount": 586,
          "title": "End-to-End Learning of OFDM Waveforms with PAPR and ACLR Constraints. (arXiv:2106.16039v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16194",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hojatian_H/0/1/0/all/0/1\">Hamed Hojatian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadal_J/0/1/0/all/0/1\">Jeremy Nadal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frigon_J/0/1/0/all/0/1\">Jean-Francois Frigon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leduc_Primeau_F/0/1/0/all/0/1\">Francois Leduc-Primeau</a>",
          "description": "Cell-free massive MIMO (CF-mMIMO) systems represent a promising approach to\nincrease the spectral efficiency of wireless communication systems. However,\nnear-optimal solutions require a large amount of signaling exchange between\naccess points (APs) and the network controller (NC). In addition, the use of\nhybrid beamforming in each AP reduces the number of power hungry RF chains, but\nimposes a large computational complexity to find near-optimal precoders. In\nthis letter, we propose two unsupervised deep neural networks (DNN)\narchitectures, fully and partially distributed, that can perform coordinated\nhybrid beamforming with zero or limited communication overhead between APs and\nNC, while achieving near-optimal sum-rate with a reduced computational\ncomplexity compared to conventional near-optimal solutions.",
          "link": "http://arxiv.org/abs/2106.16194",
          "publishedOn": "2021-07-01T01:59:33.112Z",
          "wordCount": 568,
          "title": "Limited-Fronthaul Cell-Free Hybrid Beamforming with Distributed Deep Neural Network. (arXiv:2106.16194v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15698",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Consoli_S/0/1/0/all/0/1\">Sergio Consoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Pezzoli_L/0/1/0/all/0/1\">Luca Tiozzo Pezzoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tosetti_E/0/1/0/all/0/1\">Elisa Tosetti</a>",
          "description": "We show how emotions extracted from macroeconomic news can be used to explain\nand forecast future behaviour of sovereign bond yield spreads in Italy and\nSpain. We use a big, open-source, database known as Global Database of Events,\nLanguage and Tone to construct emotion indicators of bond market affective\nstates. We find that negative emotions extracted from news improve the\nforecasting power of government yield spread models during distressed periods\neven after controlling for the number of negative words present in the text. In\naddition, stronger negative emotions, such as panic, reveal useful information\nfor predicting changes in spread at the short-term horizon, while milder\nemotions, such as distress, are useful at longer time horizons. Emotions\ngenerated by the Italian political turmoil propagate to the Spanish news\naffecting this neighbourhood market.",
          "link": "http://arxiv.org/abs/2106.15698",
          "publishedOn": "2021-07-01T01:59:33.100Z",
          "wordCount": 606,
          "title": "Emotions in Macroeconomic News and their Impact on the European Bond Market. (arXiv:2106.15698v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dachao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>",
          "description": "Sparse neural networks have received increasing interests due to their small\nsize compared to dense networks. Nevertheless, most existing works on neural\nnetwork theory have focused on dense neural networks, and our understanding of\nsparse networks is very limited. In this paper, we study the loss landscape of\none-hidden-layer sparse networks. We first consider sparse networks with linear\nactivations. We show that sparse linear networks can have spurious strict\nminima, which is in sharp contrast to dense linear networks which do not even\nhave spurious minima. Second, we show that spurious valleys can exist for wide\nsparse non-linear networks. This is different from wide dense networks which do\nnot have spurious valleys under mild assumptions.",
          "link": "http://arxiv.org/abs/2009.07439",
          "publishedOn": "2021-07-01T01:59:33.085Z",
          "wordCount": 586,
          "title": "On the Landscape of One-hidden-layer Sparse Networks and Beyond. (arXiv:2009.07439v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>",
          "description": "Anomaly detection is a widely explored domain in machine learning. Many\nmodels are proposed in the literature, and compared through different metrics\nmeasured on various datasets. The most popular metrics used to compare\nperformances are F1-score, AUC and AVPR. In this paper, we show that F1-score\nand AVPR are highly sensitive to the contamination rate. One consequence is\nthat it is possible to artificially increase their values by modifying the\ntrain-test split procedure. This leads to misleading comparisons between\nalgorithms in the literature, especially when the evaluation protocol is not\nwell detailed. Moreover, we show that the F1-score and the AVPR cannot be used\nto compare performances on different datasets as they do not reflect the\nintrinsic difficulty of modeling such data. Based on these observations, we\nclaim that F1-score and AVPR should not be used as metrics for anomaly\ndetection. We recommend a generic evaluation procedure for unsupervised anomaly\ndetection, including the use of other metrics such as the AUC, which are more\nrobust to arbitrary choices in the evaluation protocol.",
          "link": "http://arxiv.org/abs/2106.16020",
          "publishedOn": "2021-07-01T01:59:33.079Z",
          "wordCount": 634,
          "title": "Anomaly Detection: How to Artificially Increase your F1-Score with a Biased Evaluation Protocol. (arXiv:2106.16020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiao-Shan Gao</a>",
          "description": "In this paper, we present a robust classification-autoencoder (CAE) which has\nstrong ability to recognize outliers and defend adversaries. The basic idea is\nto change the autoencoder from an unsupervised learning method into a\nclassifier. The CAE is a modified autoencoder, where the encoder is used to\ncompress samples with different labels into disjoint compression spaces and the\ndecoder is used to recover a sample with a given label from the corresponding\ncompression space. The encoder is used as a classifier and the decoder is used\nto decide whether the classification given by the encoder is correct by\ncomparing the input sample with the output. Since adversary samples are seeming\ninevitable for the current DNN framework, we introduce the list classification\nbased on CAE to defend adversaries, which outputs several labels and the\ncorresponding samples recovered by the CAE. The CAE is evaluated using the\nMNIST dataset in great detail. It is shown that the CAE network can recognize\nalmost all outliers and the list classification contains the correct label for\nalmost all adversaries.",
          "link": "http://arxiv.org/abs/2106.15927",
          "publishedOn": "2021-07-01T01:59:33.060Z",
          "wordCount": 604,
          "title": "A Robust Classification-autoencoder to Defend Outliers and Adversaries. (arXiv:2106.15927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:33.024Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anjin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangquan Zhang</a>",
          "description": "Traditional supervised learning aims to train a classifier in the closed-set\nworld, where training and test samples share the same label space. In this\npaper, we target a more challenging and realistic setting: open-set learning\n(OSL), where there exist test samples from the classes that are unseen during\ntraining. Although researchers have designed many methods from the algorithmic\nperspectives, there are few methods that provide generalization guarantees on\ntheir ability to achieve consistent performance on different training samples\ndrawn from the same distribution. Motivated by the transfer learning and\nprobably approximate correct (PAC) theory, we make a bold attempt to study OSL\nby proving its generalization error-given training samples with size n, the\nestimation error will get close to order O_p(1/\\sqrt{n}). This is the first\nstudy to provide a generalization bound for OSL, which we do by theoretically\ninvestigating the risk of the target classifier on unknown classes. According\nto our theory, a novel algorithm, called auxiliary open-set risk (AOSR) is\nproposed to address the OSL problem. Experiments verify the efficacy of AOSR.\nThe code is available at github.com/Anjin-Liu/Openset_Learning_AOSR.",
          "link": "http://arxiv.org/abs/2106.15792",
          "publishedOn": "2021-07-01T01:59:32.980Z",
          "wordCount": 619,
          "title": "Learning Bounds for Open-Set Learning. (arXiv:2106.15792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1\">Asim Waqas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farooq_H/0/1/0/all/0/1\">Hamza Farooq</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a> (1), ((1) Rowan University, (2) University of Minnesota)",
          "description": "Motivated by graph theory, artificial neural networks (ANNs) are\ntraditionally structured as layers of neurons (nodes), which learn useful\ninformation by the passage of data through interconnections (edges). In the\nmachine learning realm, graph structures (i.e., neurons and connections) of\nANNs have recently been explored using various graph-theoretic measures linked\nto their predictive performance. On the other hand, in network science\n(NetSci), certain graph measures including entropy and curvature are known to\nprovide insight into the robustness and fragility of real-world networks. In\nthis work, we use these graph measures to explore the robustness of various\nANNs to adversarial attacks. To this end, we (1) explore the design space of\ninter-layer and intra-layers connectivity regimes of ANNs in the graph domain\nand record their predictive performance after training under different types of\nadversarial attacks, (2) use graph representations for both inter-layer and\nintra-layers connectivity regimes to calculate various graph-theoretic\nmeasures, including curvature and entropy, and (3) analyze the relationship\nbetween these graph measures and the adversarial performance of ANNs. We show\nthat curvature and entropy, while operating in the graph domain, can quantify\nthe robustness of ANNs without having to train these ANNs. Our results suggest\nthat the real-world networks, including brain networks, financial networks, and\nsocial networks may provide important clues to the neural architecture search\nfor robust ANNs. We propose a search strategy that efficiently finds robust\nANNs amongst a set of well-performing ANNs without having a need to train all\nof these ANNs.",
          "link": "http://arxiv.org/abs/2106.15850",
          "publishedOn": "2021-07-01T01:59:32.967Z",
          "wordCount": 696,
          "title": "Exploring Robustness of Neural Networks through Graph Measures. (arXiv:2106.15850v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:32.949Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16101",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In the paper, we propose a class of faster adaptive gradient descent ascent\nmethods for solving the nonconvex-strongly-concave minimax problems by using\nunified adaptive matrices used in the SUPER-ADAM \\citep{huang2021super}.\nSpecifically, we propose a fast adaptive gradient decent ascent (AdaGDA) method\nbased on the basic momentum technique, which reaches a low sample complexity of\n$O(\\kappa^4\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point without\nlarge batches, which improves the existing result of adaptive minimax\noptimization method by a factor of $O(\\sqrt{\\kappa})$. Moreover, we present an\naccelerated version of AdaGDA (VR-AdaGDA) method based on the momentum-based\nvariance reduced technique, which achieves the best known sample complexity of\n$O(\\kappa^3\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point without\nlarge batches. Further assume the bounded Lipschitz parameter of objective\nfunction, we prove that our VR-AdaGDA method reaches a lower sample complexity\nof $O(\\kappa^{2.5}\\epsilon^{-3})$ with the mini-batch size $O(\\kappa)$. In\nparticular, we provide an effective convergence analysis framework for our\nadaptive methods based on unified adaptive matrices, which include almost\nexisting adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.16101",
          "publishedOn": "2021-07-01T01:59:32.935Z",
          "wordCount": 610,
          "title": "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization. (arXiv:2106.16101v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "Stochastic Approximation (SA) is a popular approach for solving fixed-point\nequations where the information is corrupted by noise. In this paper, we\nconsider an SA involving a contraction mapping with respect to an arbitrary\nnorm, and show its finite-sample error bounds while using different stepsizes.\nThe idea is to construct a smooth Lyapunov function using the generalized\nMoreau envelope, and show that the iterates of SA have negative drift with\nrespect to that Lyapunov function. Our result is applicable in Reinforcement\nLearning (RL). In particular, we use it to establish the first-known\nconvergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,\nwe also use it to study TD-learning in the on-policy setting, and recover the\nexisting state-of-the-art results for $Q$-learning. Importantly, our\nconstruction results in only a logarithmic dependence of the convergence bound\non the size of the state-space.",
          "link": "http://arxiv.org/abs/2002.00874",
          "publishedOn": "2021-07-01T01:59:32.917Z",
          "wordCount": 649,
          "title": "Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruize Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">James Cheng</a>",
          "description": "Instances-reweighted adversarial training (IRAT) can significantly boost the\nrobustness of trained models, where data being less/more vulnerable to the\ngiven attack are assigned smaller/larger weights during training. However, when\ntested on attacks different from the given attack simulated in training, the\nrobustness may drop significantly (e.g., even worse than no reweighting). In\nthis paper, we study this problem and propose our solution--locally reweighted\nadversarial training (LRAT). The rationale behind IRAT is that we do not need\nto pay much attention to an instance that is already safe under the attack. We\nargue that the safeness should be attack-dependent, so that for the same\ninstance, its weight can change given different attacks based on the same\nmodel. Thus, if the attack simulated in training is mis-specified, the weights\nof IRAT are misleading. To this end, LRAT pairs each instance with its\nadversarial variants and performs local reweighting inside each pair, while\nperforming no global reweighting--the rationale is to fit the instance itself\nif it is immune to the attack, but not to skip the pair, in order to passively\ndefend different attacks in future. Experiments show that LRAT works better\nthan both IRAT (i.e., global reweighting) and the standard AT (i.e., no\nreweighting) when trained with an attack and tested on different attacks.",
          "link": "http://arxiv.org/abs/2106.15776",
          "publishedOn": "2021-07-01T01:59:32.895Z",
          "wordCount": 640,
          "title": "Local Reweighting for Adversarial Training. (arXiv:2106.15776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Despite the conventional wisdom that using batch normalization with weight\ndecay may improve neural network training, some recent works show their joint\nusage may cause instabilities at the late stages of training. Other works, in\ncontrast, show convergence to the equilibrium, i.e., the stabilization of\ntraining metrics. In this paper, we study this contradiction and show that\ninstead of converging to a stable equilibrium, the training dynamics converge\nto consistent periodic behavior. That is, the training process regularly\nexhibits instabilities which, however, do not lead to complete training\nfailure, but cause a new period of training. We rigorously investigate the\nmechanism underlying this discovered periodic behavior both from an empirical\nand theoretical point of view and show that this periodic behavior is indeed\ncaused by the interaction between batch normalization and weight decay.",
          "link": "http://arxiv.org/abs/2106.15739",
          "publishedOn": "2021-07-01T01:59:32.889Z",
          "wordCount": 589,
          "title": "On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay. (arXiv:2106.15739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yingbin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yinian Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "The memorization effect of deep neural network (DNN) plays a pivotal role in\nmany state-of-the-art label-noise learning methods. To exploit this property,\nthe early stopping trick, which stops the optimization at the early stage of\ntraining, is usually adopted. Current methods generally decide the early\nstopping point by considering a DNN as a whole. However, a DNN can be\nconsidered as a composition of a series of layers, and we find that the latter\nlayers in a DNN are much more sensitive to label noise, while their former\ncounterparts are quite robust. Therefore, selecting a stopping point for the\nwhole network may make different DNN layers antagonistically affected each\nother, thus degrading the final performance. In this paper, we propose to\nseparate a DNN into different parts and progressively train them to address\nthis problem. Instead of the early stopping, which trains a whole DNN all at\nonce, we initially train former DNN layers by optimizing the DNN with a\nrelatively large number of epochs. During training, we progressively train the\nlatter DNN layers by using a smaller number of epochs with the preceding layers\nfixed to counteract the impact of noisy labels. We term the proposed method as\nprogressive early stopping (PES). Despite its simplicity, compared with the\nearly stopping, PES can help to obtain more promising and stable results.\nFurthermore, by combining PES with existing approaches on noisy label training,\nwe achieve state-of-the-art performance on image classification benchmarks.",
          "link": "http://arxiv.org/abs/2106.15853",
          "publishedOn": "2021-07-01T01:59:32.871Z",
          "wordCount": 686,
          "title": "Understanding and Improving Early Stopping for Learning with Noisy Labels. (arXiv:2106.15853v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:32.864Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15767",
          "publishedOn": "2021-07-01T01:59:32.805Z",
          "wordCount": 551,
          "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes. (arXiv:2106.15767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>",
          "description": "In light of the COVID-19 pandemic, it is an open challenge and critical\npractical problem to find a optimal way to dynamically prescribe the best\npolicies that balance both the governmental resources and epidemic control in\ndifferent countries and regions. To solve this multi-dimensional tradeoff of\nexploitation and exploration, we formulate this technical challenge as a\ncontextual combinatorial bandit problem that jointly optimizes a multi-criteria\nreward function. Given the historical daily cases in a region and the past\nintervention plans in place, the agent should generate useful intervention\nplans that policy makers can implement in real time to minimizing both the\nnumber of daily COVID-19 cases and the stringency of the recommended\ninterventions. We prove this concept with simulations of multiple realistic\npolicy making scenarios.",
          "link": "http://arxiv.org/abs/2106.15808",
          "publishedOn": "2021-07-01T01:59:32.799Z",
          "wordCount": 620,
          "title": "Optimal Epidemic Control as a Contextual Combinatorial Bandit with Budget. (arXiv:2106.15808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaehyeong Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have recently achieved remarkable success in\nrepresenting graph-structured data, with rapid progress in both the node\nembedding and graph pooling methods. Yet, they mostly focus on capturing\ninformation from the nodes considering their connectivity, and not much work\nhas been done in representing the edges, which are essential components of a\ngraph. However, for tasks such as graph reconstruction and generation, as well\nas graph classification tasks for which the edges are important for\ndiscrimination, accurately representing edges of a given graph is crucial to\nthe success of the graph representation learning. To this end, we propose a\nnovel edge representation learning framework based on Dual Hypergraph\nTransformation (DHT), which transforms the edges of a graph into the nodes of a\nhypergraph. This dual hypergraph construction allows us to apply message\npassing techniques for node representations to edges. After obtaining edge\nrepresentations from the hypergraphs, we then cluster or drop edges to obtain\nholistic graph-level edge representations. We validate our edge representation\nlearning method with hypergraphs on diverse graph datasets for graph\nrepresentation and generation performance, on which our method largely\noutperforms existing graph representation learning methods. Moreover, our edge\nrepresentation learning and pooling method also largely outperforms\nstate-of-the-art graph pooling methods on graph classification, not only\nbecause of its accurate edge representation learning, but also due to its\nlossless compression of the nodes and removal of irrelevant edges for effective\nmessage passing.",
          "link": "http://arxiv.org/abs/2106.15845",
          "publishedOn": "2021-07-01T01:59:32.775Z",
          "wordCount": 665,
          "title": "Edge Representation Learning with Hypergraphs. (arXiv:2106.15845v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1\">Maria Gorlatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>",
          "description": "We consider distributed machine learning (ML) through unmanned aerial\nvehicles (UAVs) for geo-distributed device clusters. We propose five new\ntechnologies/techniques: (i) stratified UAV swarms with leader, worker, and\ncoordinator UAVs, (ii) hierarchical nested personalized federated learning\n(HN-PFL): a holistic distributed ML framework for personalized model training\nacross the worker-leader-core network hierarchy, (iii) cooperative UAV resource\npooling for distributed ML using the UAVs' local computational capabilities,\n(iv) aerial data caching and relaying for efficient data relaying to conduct\nML, and (v) concept/model drift, capturing online data variations at the\ndevices. We split the UAV-enabled model training problem as two parts. (a)\nNetwork-aware HN-PFL, where we optimize a tradeoff between energy consumption\nand ML model performance by configuring data offloading among devices-UAVs and\nUAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to\ncommunication/computation network heterogeneity. We tackle this optimization\nproblem via the method of posynomial condensation and propose a distributed\nalgorithm with a performance guarantee. (b) Macro-trajectory and learning\nduration design, which we formulate as a sequential decision making problem,\ntackled via deep reinforcement learning. Our simulations demonstrate the\nsuperiority of our methodology with regards to the distributed ML performance,\nthe optimization of network resources, and the swarm trajectory efficiency.",
          "link": "http://arxiv.org/abs/2106.15734",
          "publishedOn": "2021-07-01T01:59:32.744Z",
          "wordCount": 656,
          "title": "UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>",
          "description": "Graph neural networks (GNNs) have achieved great success in many graph-based\ntasks. Much work is dedicated to empowering GNNs with the adaptive locality\nability, which enables measuring the importance of neighboring nodes to the\ntarget node by a node-specific mechanism. However, the current node-specific\nmechanisms are deficient in distinguishing the importance of nodes in the\ntopology structure. We believe that the structural importance of neighboring\nnodes is closely related to their importance in aggregation. In this paper, we\nintroduce discrete graph curvature (the Ricci curvature) to quantify the\nstrength of structural connection of pairwise nodes. And we propose Curvature\nGraph Neural Network (CGNN), which effectively improves the adaptive locality\nability of GNNs by leveraging the structural property of graph curvature. To\nimprove the adaptability of curvature to various datasets, we explicitly\ntransform curvature into the weights of neighboring nodes by the necessary\nNegative Curvature Processing Module and Curvature Normalization Module. Then,\nwe conduct numerous experiments on various synthetic datasets and real-world\ndatasets. The experimental results on synthetic datasets show that CGNN\neffectively exploits the topology structure information, and the performance is\nimproved significantly. CGNN outperforms the baselines on 5 dense node\nclassification benchmark datasets. This study deepens the understanding of how\nto utilize advanced topology information and assign the importance of\nneighboring nodes from the perspective of graph curvature and encourages us to\nbridge the gap between graph theory and neural networks.",
          "link": "http://arxiv.org/abs/2106.15762",
          "publishedOn": "2021-07-01T01:59:32.711Z",
          "wordCount": 668,
          "title": "Curvature Graph Neural Network. (arXiv:2106.15762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15666",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miller_J/0/1/0/all/0/1\">Jacob Miller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roeder_G/0/1/0/all/0/1\">Geoffrey Roeder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>",
          "description": "We investigate a correspondence between two formalisms for discrete\nprobabilistic modeling: probabilistic graphical models (PGMs) and tensor\nnetworks (TNs), a powerful modeling framework for simulating complex quantum\nsystems. The graphical calculus of PGMs and TNs exhibits many similarities,\nwith discrete undirected graphical models (UGMs) being a special case of TNs.\nHowever, more general probabilistic TN models such as Born machines (BMs)\nemploy complex-valued hidden states to produce novel forms of correlation among\nthe probabilities. While representing a new modeling resource for capturing\nstructure in discrete probability distributions, this behavior also renders the\ndirect application of standard PGM tools impossible. We aim to bridge this gap\nby introducing a hybrid PGM-TN formalism that integrates quantum-like\ncorrelations into PGM models in a principled manner, using the\nphysically-motivated concept of decoherence. We first prove that applying\ndecoherence to the entirety of a BM model converts it into a discrete UGM, and\nconversely, that any subgraph of a discrete UGM can be represented as a\ndecohered BM. This method allows a broad family of probabilistic TN models to\nbe encoded as partially decohered BMs, a fact we leverage to combine the\nrepresentational strengths of both model families. We experimentally verify the\nperformance of such hybrid models in a sequential modeling task, and identify\npromising uses of our method within the context of existing applications of\ngraphical models.",
          "link": "http://arxiv.org/abs/2106.15666",
          "publishedOn": "2021-07-01T01:59:32.688Z",
          "wordCount": 668,
          "title": "Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework. (arXiv:2106.15666v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.553Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>",
          "description": "We study the selective learning problem introduced by Qiao and Valiant\n(2019), in which the learner observes $n$ labeled data points one at a time. At\na time of its choosing, the learner selects a window length $w$ and a model\n$\\hat\\ell$ from the model class $\\mathcal{L}$, and then labels the next $w$\ndata points using $\\hat\\ell$. The excess risk incurred by the learner is\ndefined as the difference between the average loss of $\\hat\\ell$ over those $w$\ndata points and the smallest possible average loss among all models in\n$\\mathcal{L}$ over those $w$ data points.\n\nWe give an improved algorithm, termed the hybrid exponential weights\nalgorithm, that achieves an expected excess risk of $O((\\log\\log|\\mathcal{L}| +\n\\log\\log n)/\\log n)$. This result gives a doubly exponential improvement in the\ndependence on $|\\mathcal{L}|$ over the best known bound of\n$O(\\sqrt{|\\mathcal{L}|/\\log n})$. We complement the positive result with an\nalmost matching lower bound, which suggests the worst-case optimality of the\nalgorithm.\n\nWe also study a more restrictive family of learning algorithms that are\nbounded-recall in the sense that when a prediction window of length $w$ is\nchosen, the learner's decision only depends on the most recent $w$ data points.\nWe analyze an exponential weights variant of the ERM algorithm in Qiao and\nValiant (2019). This new algorithm achieves an expected excess risk of\n$O(\\sqrt{\\log |\\mathcal{L}|/\\log n})$, which is shown to be nearly optimal\namong all bounded-recall learners. Our analysis builds on a generalized version\nof the selective mean prediction problem in Drucker (2013); Qiao and Valiant\n(2019), which may be of independent interest.",
          "link": "http://arxiv.org/abs/2106.15662",
          "publishedOn": "2021-07-01T01:59:32.547Z",
          "wordCount": 699,
          "title": "Exponential Weights Algorithms for Selective Learning. (arXiv:2106.15662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "When a deep learning model is deployed in the wild, it can encounter test\ndata drawn from distributions different from the training data distribution and\nsuffer drop in performance. For safe deployment, it is essential to estimate\nthe accuracy of the pre-trained model on the test data. However, the labels for\nthe test inputs are usually not immediately available in practice, and\nobtaining them can be expensive. This observation leads to two challenging\ntasks: (1) unsupervised accuracy estimation, which aims to estimate the\naccuracy of a pre-trained classifier on a set of unlabeled test inputs; (2)\nerror detection, which aims to identify mis-classified test inputs. In this\npaper, we propose a principled and practically effective framework that\nsimultaneously addresses the two tasks. The proposed framework iteratively\nlearns an ensemble of models to identify mis-classified data points and\nperforms self-training to improve the ensemble with the identified points.\nTheoretical analysis demonstrates that our framework enjoys provable guarantees\nfor both accuracy estimation and error detection under mild conditions readily\nsatisfied by practical deep learning models. Along with the framework, we\nproposed and experimented with two instantiations and achieved state-of-the-art\nresults on 59 tasks. For example, on iWildCam, one instantiation reduces the\nestimation error for unsupervised accuracy estimation by at least 70% and\nimproves the F1 score for error detection by at least 4.7% compared to existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.15728",
          "publishedOn": "2021-07-01T01:59:32.505Z",
          "wordCount": 665,
          "title": "Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Annie Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1\">Thomas B&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kononova_A/0/1/0/all/0/1\">Anna V. Kononova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "This paper surveys the field of multiagent deep reinforcement learning. The\ncombination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from\nsingle-agent to multiagent environments. Dealing with multiple agents is\ninherently more complex as (a) the future rewards depend on the joint actions\nof multiple players and (b) the computational complexity of functions\nincreases. We present the most common multiagent problem representations and\ntheir main challenges, and identify five research areas that address one or\nmore of these challenges: centralised training and decentralised execution,\nopponent modelling, communication, efficient coordination, and reward shaping.\nWe find that many computational studies rely on unrealistic assumptions or are\nnot generalisable to other settings; they struggle to overcome the curse of\ndimensionality or nonstationarity. Approaches from psychology and sociology\ncapture promising relevant behaviours such as communication and coordination.\nWe suggest that, for multiagent reinforcement learning to be successful, future\nresearch addresses these challenges with an interdisciplinary approach to open\nup new possibilities for more human-oriented solutions in multiagent\nreinforcement learning.",
          "link": "http://arxiv.org/abs/2106.15691",
          "publishedOn": "2021-07-01T01:59:32.499Z",
          "wordCount": 636,
          "title": "Multiagent Deep Reinforcement Learning: Challenges and Directions Towards Human-Like Approaches. (arXiv:2106.15691v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.482Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}