{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.02317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a>",
          "description": "Dialogue policy learning, a subtask that determines the content of system\nresponse generation and then the degree of task completion, is essential for\ntask-oriented dialogue systems. However, the unbalanced distribution of system\nactions in dialogue datasets often causes difficulty in learning to generate\ndesired actions and responses. In this paper, we propose a\nretrieve-and-memorize framework to enhance the learning of system actions.\nSpecially, we first design a neural context-aware retrieval module to retrieve\nmultiple candidate system actions from the training set given a dialogue\ncontext. Then, we propose a memory-augmented multi-decoder network to generate\nthe system actions conditioned on the candidate actions, which allows the\nnetwork to adaptively select key information in the candidate actions and\nignore noises. We conduct experiments on the large-scale multi-domain\ntask-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental\nresults show that our method achieves competitive performance among several\nstate-of-the-art models in the context-to-response generation task.",
          "link": "http://arxiv.org/abs/2106.02317",
          "publishedOn": "2021-06-29T01:55:14.378Z",
          "wordCount": 607,
          "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:14.265Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>",
          "description": "Research on overlapped and discontinuous named entity recognition (NER) has\nreceived increasing attention. The majority of previous work focuses on either\noverlapped or discontinuous entities. In this paper, we propose a novel\nspan-based model that can recognize both overlapped and discontinuous entities\njointly. The model includes two major steps. First, entity fragments are\nrecognized by traversing over all possible text spans, thus, overlapped\nentities can be recognized. Second, we perform relation classification to judge\nwhether a given pair of entity fragments to be overlapping or succession. In\nthis way, we can recognize not only discontinuous entities, and meanwhile\ndoubly check the overlapped entities. As a whole, our model can be regarded as\na relation extraction paradigm essentially. Experimental results on multiple\nbenchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly\ncompetitive for overlapped and discontinuous NER.",
          "link": "http://arxiv.org/abs/2106.14373",
          "publishedOn": "2021-06-29T01:55:14.249Z",
          "wordCount": 586,
          "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition. (arXiv:2106.14373v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Sanhui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongfeng Li</a>",
          "description": "We present a knowledge-grounded dialog system developed for the ninth Dialog\nSystem Technology Challenge (DSTC9) Track 1 - Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access. We leverage\ntransfer learning with existing language models to accomplish the tasks in this\nchallenge track. Specifically, we divided the task into four sub-tasks and\nfine-tuned several Transformer models on each of the sub-tasks. We made\nadditional changes that yielded gains in both performance and efficiency,\nincluding the combination of the model with traditional entity-matching\ntechniques, and the addition of a pointer network to the output layer of the\nlanguage model.",
          "link": "http://arxiv.org/abs/2106.14444",
          "publishedOn": "2021-06-29T01:55:14.236Z",
          "wordCount": 539,
          "title": "A Knowledge-Grounded Dialog System Based on Pre-Trained Language Models. (arXiv:2106.14444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:14.161Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:14.155Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuejiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.",
          "link": "http://arxiv.org/abs/2008.11015",
          "publishedOn": "2021-06-29T01:55:14.104Z",
          "wordCount": 641,
          "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been several efforts to understand what information such\nrepresentations contain. A common strategy to use such representations is to\nfine-tune them for an end task. However, how fine-tuning for a task changes the\nunderlying space is less studied. In this work, we study the English BERT\nfamily and use two probing techniques to analyze how fine-tuning changes the\nspace. Our experiments reveal that fine-tuning improves performance because it\npushes points associated with a label away from other labels. By comparing the\nrepresentations before and after fine-tuning, we also discover that fine-tuning\ndoes not change the representations arbitrarily; instead, it adjusts the\nrepresentations to downstream tasks while preserving the original structure.\nFinally, using carefully constructed experiments, we show that fine-tuning can\nencode training sets in a representation, suggesting an overfitting problem of\na new kind.",
          "link": "http://arxiv.org/abs/2106.14282",
          "publishedOn": "2021-06-29T01:55:14.079Z",
          "wordCount": 574,
          "title": "A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:14.072Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:13.966Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.954Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:13.893Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chengli Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>",
          "description": "Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.",
          "link": "http://arxiv.org/abs/2104.01558",
          "publishedOn": "2021-06-29T01:55:13.859Z",
          "wordCount": 651,
          "title": "Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>",
          "description": "This paper describes the submission to the IWSLT 2021 offline speech\ntranslation task by the UPC Machine Translation group. The task consists of\nbuilding a system capable of translating English audio recordings extracted\nfrom TED talks into German text. Submitted systems can be either cascade or\nend-to-end and use a custom or given segmentation. Our submission is an\nend-to-end speech translation system, which combines pre-trained models\n(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,\nand uses an efficient fine-tuning technique, which trains only 20% of its total\nparameters. We show that adding an Adapter to the system and pre-training it,\ncan increase the convergence speed and the final result, with which we achieve\na BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble\nthat obtains 28.22 BLEU score on the same set. Our submission also uses a\ncustom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for\nidentifying periods of untranscribable text and can bring improvements of 2.5\nto 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the\ngiven segmentation.",
          "link": "http://arxiv.org/abs/2105.04512",
          "publishedOn": "2021-06-29T01:55:13.784Z",
          "wordCount": 676,
          "title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021. (arXiv:2105.04512v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Avik Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>",
          "description": "Intent classification is a major task in spoken language understanding (SLU).\nSince most models are built with pre-collected in-domain (IND) training\nutterances, their ability to detect unsupported out-of-domain (OOD) utterances\nhas a critical effect in practical use. Recent works have shown that using\nextra data and labels can improve the OOD detection performance, yet it could\nbe costly to collect such data. This paper proposes to train a model with only\nIND data while supporting both IND intent classification and OOD detection. Our\nmethod designs a novel domain-regularized module (DRM) to reduce the\noverconfident phenomenon of a vanilla classifier, achieving a better\ngeneralization in both cases. Besides, DRM can be used as a drop-in replacement\nfor the last layer in any neural network-based intent classifier, providing a\nlow-cost strategy for a significant improvement. The evaluation on four\ndatasets shows that our method built on BERT and RoBERTa models achieves\nstate-of-the-art performance against existing approaches and the strong\nbaselines we created for the comparisons.",
          "link": "http://arxiv.org/abs/2106.14464",
          "publishedOn": "2021-06-29T01:55:13.709Z",
          "wordCount": 604,
          "title": "Enhancing the Generalization for Intent Classification and Out-of-Domain Detection in SLU. (arXiv:2106.14464v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Luyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>",
          "description": "Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.",
          "link": "http://arxiv.org/abs/2106.14371",
          "publishedOn": "2021-06-29T01:55:13.536Z",
          "wordCount": 672,
          "title": "Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.456Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)\nby only using self-attention relation distillation for task-agnostic\ncompression of pretrained Transformers. In particular, we define multi-head\nself-attention relations as scaled dot-product between the pairs of query, key,\nand value vectors within each self-attention module. Then we employ the above\nrelational knowledge to train the student model. Besides its simplicity and\nunified principle, more favorably, there is no restriction in terms of the\nnumber of student's attention heads, while most previous work has to guarantee\nthe same head number between teacher and student. Moreover, the fine-grained\nself-attention relations tend to fully exploit the interaction knowledge\nlearned by Transformer. In addition, we thoroughly examine the layer selection\nstrategy for teacher models, rather than just relying on the last layer as in\nMiniLM. We conduct extensive experiments on compressing both monolingual and\nmultilingual pretrained models. Experimental results demonstrate that our\nmodels distilled from base-size and large-size teachers (BERT, RoBERTa and\nXLM-R) outperform the state-of-the-art.",
          "link": "http://arxiv.org/abs/2012.15828",
          "publishedOn": "2021-06-29T01:55:13.441Z",
          "wordCount": 638,
          "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers. (arXiv:2012.15828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1\">Shriya Atmakuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "Learning vector representations for words is one of the most fundamental\ntopics in NLP, capable of capturing syntactic and semantic relationships useful\nin a variety of downstream NLP tasks. Vector representations can be limiting,\nhowever, in that typical scoring such as dot product similarity intertwines\nposition and magnitude of the vector in space. Exciting innovations in the\nspace of representation learning have proposed alternative fundamental\nrepresentations, such as distributions, hyperbolic vectors, or regions. Our\nmodel, Word2Box, takes a region-based approach to the problem of word\nrepresentation, representing words as $n$-dimensional rectangles. These\nrepresentations encode position and breadth independently and provide\nadditional geometric operations such as intersection and containment which\nallow them to model co-occurrence patterns vectors struggle with. We\ndemonstrate improved performance on various word similarity tasks, particularly\non less common words, and perform a qualitative analysis exploring the\nadditional unique expressivity provided by Word2Box.",
          "link": "http://arxiv.org/abs/2106.14361",
          "publishedOn": "2021-06-29T01:55:13.434Z",
          "wordCount": 589,
          "title": "Word2Box: Learning Word Representation Using Box Embeddings. (arXiv:2106.14361v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:13.427Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Joseph Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Weile Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgakoudis_G/0/1/0/all/0/1\">Giorgis Georgakoudis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerfert_J/0/1/0/all/0/1\">Johannes Doerfert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_O/0/1/0/all/0/1\">Oscar Hernandez</a>",
          "description": "This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.",
          "link": "http://arxiv.org/abs/2106.14332",
          "publishedOn": "2021-06-29T01:55:13.415Z",
          "wordCount": 590,
          "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation. (arXiv:2106.14332v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Currently, there are more than a dozen Russian-language corpora for sentiment\nanalysis, differing in the source of the texts, domain, size, number and ratio\nof sentiment classes, and annotation method. This work examines publicly\navailable Russian-language corpora, presents their qualitative and quantitative\ncharacteristics, which make it possible to get an idea of the current landscape\nof the corpora for sentiment analysis. The ranking of corpora by annotation\nquality is proposed, which can be useful when choosing corpora for training and\ntesting. The influence of the training dataset on the performance of sentiment\nanalysis is investigated based on the use of the deep neural network model\nBERT. The experiments with review corpora allow us to conclude that on average\nthe quality of models increases with an increase in the number of training\ncorpora. For the first time, quality scores were obtained for the corpus of\nreviews of ROMIP seminars based on the BERT model. Also, the study proposes the\ntask of the building a universal model for sentiment analysis.",
          "link": "http://arxiv.org/abs/2106.14434",
          "publishedOn": "2021-06-29T01:55:13.399Z",
          "wordCount": 601,
          "title": "Current Landscape of the Russian Sentiment Corpora. (arXiv:2106.14434v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:13.393Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ayush Singh</a>",
          "description": "Natural interface to database (NLIDB) has been researched a lot during the\npast decades. In the core of NLIDB, is a semantic parser used to convert\nnatural language into SQL. Solutions from traditional NLP methodology focuses\non grammar rule pattern learning and pairing via intermediate logic forms.\nAlthough those methods give an acceptable performance on certain specific\ndatabase and parsing tasks, they are hard to generalize and scale. On the other\nhand, recent progress in neural deep learning seems to provide a promising\ndirection towards building a general NLIDB system. Unlike the traditional\napproach, those neural methodologies treat the parsing problem as a\nsequence-to-sequence learning problem. In this paper, we experimented on\nseveral sequence-to-sequence learning models and evaluate their performance on\ngeneral database parsing task.",
          "link": "http://arxiv.org/abs/2106.13858",
          "publishedOn": "2021-06-29T01:55:13.386Z",
          "wordCount": 560,
          "title": "Semantic Parsing Natural Language into Relational Algebra. (arXiv:2106.13858v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yong-Bin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>",
          "description": "Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat \\url{https://github.com/csebuetnlp/xl-sum}.",
          "link": "http://arxiv.org/abs/2106.13822",
          "publishedOn": "2021-06-29T01:55:13.378Z",
          "wordCount": 662,
          "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages. (arXiv:2106.13822v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1\">Zeinab Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1\">Mehrnoush ShamsFard</a>",
          "description": "Recognizing causal elements and causal relations in text is one of the\nchallenging issues in natural language processing; specifically, in low\nresource languages such as Persian. In this research we prepare a causality\nhuman annotated corpus for the Persian language which consists of 4446\nsentences and 5128 causal relations and three labels of cause, effect and\ncausal mark -- if possibl -- are specified for each relation. We have used this\ncorpus to train a system for detecting causal elements boundaries. Also, we\npresent a causality detection benchmark for three machine learning methods and\ntwo deep learning systems based on this corpus. Performance evaluations\nindicate that our best total result is obtained through CRF classifier which\nhas F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep\nlearning method with Accuracy equal to %91.4.",
          "link": "http://arxiv.org/abs/2106.14165",
          "publishedOn": "2021-06-29T01:55:13.370Z",
          "wordCount": 575,
          "title": "Persian Causality Corpus (PerCause) and the Causality Detection Benchmark. (arXiv:2106.14165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fishcheva_I/0/1/0/all/0/1\">Irina Fishcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goloviznina_V/0/1/0/all/0/1\">Valeriya Goloviznina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Argumentation mining is a field of computational linguistics that is devoted\nto extracting from texts and classifying arguments and relations between them,\nas well as constructing an argumentative structure. A significant obstacle to\nresearch in this area for the Russian language is the lack of annotated\nRussian-language text corpora. This article explores the possibility of\nimproving the quality of argumentation mining using the extension of the\nRussian-language version of the Argumentative Microtext Corpus (ArgMicro) based\non the machine translation of the Persuasive Essays Corpus (PersEssays). To\nmake it possible to use these two corpora combined, we propose a Joint Argument\nAnnotation Scheme based on the schemes used in ArgMicro and PersEssays. We\nsolve the problem of classifying argumentative discourse units (ADUs) into two\nclasses - \"pro\" (\"for\") and \"opp\" (\"against\") using traditional machine\nlearning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT\nmodel). An ensemble of XGBoost and BERT models was proposed, which showed the\nhighest performance of ADUs classification for both corpora.",
          "link": "http://arxiv.org/abs/2106.14438",
          "publishedOn": "2021-06-29T01:55:13.355Z",
          "wordCount": 615,
          "title": "Traditional Machine Learning and Deep Learning Models for Argumentation Mining in Russian Texts. (arXiv:2106.14438v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "In recent years, reference-based and supervised summarization evaluation\nmetrics have been widely explored. However, collecting human-annotated\nreferences and ratings are costly and time-consuming. To avoid these\nlimitations, we propose a training-free and reference-free summarization\nevaluation metric. Our metric consists of a centrality-weighted relevance score\nand a self-referenced redundancy score. The relevance score is computed between\nthe pseudo reference built from the source document and the given summary,\nwhere the pseudo reference content is weighted by the sentence centrality to\nprovide importance guidance. Besides an $F_1$-based relevance score, we also\ndesign an $F_\\beta$-based variant that pays more attention to the recall score.\nAs for the redundancy score of the summary, we compute a self-masked similarity\nscore with the summary itself to evaluate the redundant information in the\nsummary. Finally, we combine the relevance and redundancy scores to produce the\nfinal evaluation score of the given summary. Extensive experiments show that\nour methods can significantly outperform existing methods on both\nmulti-document and single-document summarization evaluation.",
          "link": "http://arxiv.org/abs/2106.13945",
          "publishedOn": "2021-06-29T01:55:13.349Z",
          "wordCount": 607,
          "title": "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy. (arXiv:2106.13945v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeedizade_M/0/1/0/all/0/1\">Mohammad Javad Saeedizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabian_N/0/1/0/all/0/1\">Najmeh Torabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_Bidgoli_B/0/1/0/all/0/1\">Behrouz Minaei-Bidgoli</a>",
          "description": "Link prediction is the task of predicting missing relations between entities\nof the knowledge graph by inferring from the facts contained in it. Recent work\nin link prediction has attempted to provide a model for increasing link\nprediction accuracy by using more layers in neural network architecture or\nmethods that add to the computational complexity of models. This paper we\nproposed a method for refining the knowledge graph, which makes the knowledge\ngraph more informative, and link prediction operations can be performed more\naccurately using relatively fast translational models. Translational link\nprediction models, such as TransE, TransH, TransD, etc., have much less\ncomplexity than deep learning approaches. This method uses the hierarchy of\nrelationships and also the hierarchy of entities in the knowledge graph to add\nthe entity information as a new entity to the graph and connect it to the nodes\nwhich contain this information in their hierarchy. Our experiments show that\nour method can significantly increase the performance of translational link\nprediction methods in H@10, MR, MRR.",
          "link": "http://arxiv.org/abs/2106.14233",
          "publishedOn": "2021-06-29T01:55:13.343Z",
          "wordCount": 614,
          "title": "KGRefiner: Knowledge Graph Refinement for Improving Accuracy of Translational Link Prediction Methods. (arXiv:2106.14233v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:13.334Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:13.326Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_F/0/1/0/all/0/1\">Fusataka Kuniyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozawa_J/0/1/0/all/0/1\">Jun Ozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>",
          "description": "In the field of inorganic materials science, there is a growing demand to\nextract knowledge such as physical properties and synthesis processes of\nmaterials by machine-reading a large number of papers. This is because\nmaterials researchers refer to many papers in order to come up with promising\nterms of experiments for material synthesis. However, there are only a few\nsystems that can extract material names and their properties. This study\nproposes a large-scale natural language processing (NLP) pipeline for\nextracting material names and properties from materials science literature to\nenable the search and retrieval of results in materials science. Therefore, we\npropose a label definition for extracting material names and properties and\naccordingly build a corpus containing 836 annotated paragraphs extracted from\n301 papers for training a named entity recognition (NER) model. Experimental\nresults demonstrate the utility of this NER model; it achieves successful\nextraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our\napproach, we present a thorough evaluation on a real-world automatically\nannotated corpus by applying our trained NER model to 12,895 materials science\npapers. We analyze the trend in materials science by visualizing the outputs of\nthe NLP pipeline. For example, the country-by-year analysis indicates that in\nrecent years, the number of papers on \"MoS2,\" a material used in perovskite\nsolar cells, has been increasing rapidly in China but decreasing in the United\nStates. Further, according to the conditions-by-year analysis, the processing\ntemperature of the catalyst material \"PEDOT:PSS\" is shifting below 200 degree,\nand the number of reports with a processing time exceeding 5 h is increasing\nslightly.",
          "link": "http://arxiv.org/abs/2106.14157",
          "publishedOn": "2021-06-29T01:55:13.309Z",
          "wordCount": 700,
          "title": "Analyzing Research Trends in Inorganic Materials Literature Using NLP. (arXiv:2106.14157v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_S/0/1/0/all/0/1\">Sara Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darzi_A/0/1/0/all/0/1\">Ali Darzi</a>",
          "description": "Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.",
          "link": "http://arxiv.org/abs/2106.13833",
          "publishedOn": "2021-06-29T01:55:13.303Z",
          "wordCount": 580,
          "title": "Persian Rhetorical Structure Theory. (arXiv:2106.13833v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1\">Bernardo Oviedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>",
          "description": "Analyzing political ideology and polarization is of critical importance in\nadvancing our understanding of the political context in society. Recent\nresearch has made great strides towards understanding the ideological bias\n(i.e., stance) of news media along a left-right spectrum. In this work, we take\na novel approach and study the ideology of the policy under discussion teasing\napart the nuanced co-existence of stance and ideology. Aligned with the\ntheoretical accounts in political science, we treat ideology as a\nmulti-dimensional construct, and introduce the first diachronic dataset of news\narticles whose political ideology under discussion is annotated by trained\npolitical scientists and linguists at the paragraph-level. We showcase that\nthis framework enables quantitative analysis of polarization, a temporal,\nmultifaceted measure of ideological distance. We further present baseline\nmodels for ideology prediction.",
          "link": "http://arxiv.org/abs/2106.14387",
          "publishedOn": "2021-06-29T01:55:13.297Z",
          "wordCount": 574,
          "title": "Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lianbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Huimin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiliang Zhang</a>",
          "description": "Extracting relational triples from texts is a fundamental task in knowledge\ngraph construction. The popular way of existing methods is to jointly extract\nentities and relations using a single model, which often suffers from the\noverlapping triple problem. That is, there are multiple relational triples that\nshare the same entities within one sentence. In this work, we propose an\neffective cascade dual-decoder approach to extract overlapping relational\ntriples, which includes a text-specific relation decoder and a\nrelation-corresponded entity decoder. Our approach is straightforward: the\ntext-specific relation decoder detects relations from a sentence according to\nits text semantics and treats them as extra features to guide the entity\nextraction; for each extracted relation, which is with trainable embedding, the\nrelation-corresponded entity decoder detects the corresponding head and tail\nentities using a span-based tagging scheme. In this way, the overlapping triple\nproblem is tackled naturally. Experiments on two public datasets demonstrate\nthat our proposed approach outperforms state-of-the-art methods and achieves\nbetter F1 scores under the strict evaluation metric. Our implementation is\navailable at https://github.com/prastunlp/DualDec.",
          "link": "http://arxiv.org/abs/2106.14163",
          "publishedOn": "2021-06-29T01:55:13.290Z",
          "wordCount": 612,
          "title": "Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction. (arXiv:2106.14163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:13.282Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Min Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiasheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>",
          "description": "This paper describes our approach to DSTC 9 Track 2: Cross-lingual\nMulti-domain Dialog State Tracking, the task goal is to build a Cross-lingual\ndialog state tracker with a training set in rich resource language and a\ntesting set in low resource language. We formulate a method for joint learning\nof slot operation classification task and state tracking task respectively.\nFurthermore, we design a novel mask mechanism for fusing contextual information\nabout dialogue, the results show the proposed model achieves excellent\nperformance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in\nMultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.",
          "link": "http://arxiv.org/abs/2106.14433",
          "publishedOn": "2021-06-29T01:55:13.250Z",
          "wordCount": 544,
          "title": "Efficient Dialogue State Tracking by Masked Hierarchical Transformer. (arXiv:2106.14433v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:13.244Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:13.210Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:13.194Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_R/0/1/0/all/0/1\">Romina Etezadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "Question answering systems may find the answers to users' questions from\neither unstructured texts or structured data such as knowledge graphs.\nAnswering questions using supervised learning approaches including deep\nlearning models need large training datasets. In recent years, some datasets\nhave been presented for the task of Question answering over knowledge graphs,\nwhich is the focus of this paper. Although many datasets in English were\nproposed, there have been a few question-answering datasets in Persian. This\npaper introduces \\textit{PeCoQ}, a dataset for Persian question answering. This\ndataset contains 10,000 complex questions and answers extracted from the\nPersian knowledge graph, FarsBase. For each question, the SPARQL query and two\nparaphrases that were written by linguists are provided as well. There are\ndifferent types of complexities in the dataset, such as multi-relation,\nmulti-entity, ordinal, and temporal constraints. In this paper, we discuss the\ndataset's characteristics and describe our methodology for building it.",
          "link": "http://arxiv.org/abs/2106.14167",
          "publishedOn": "2021-06-29T01:55:13.181Z",
          "wordCount": 593,
          "title": "PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge Graph. (arXiv:2106.14167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lachmy_R/0/1/0/all/0/1\">Royi Lachmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>",
          "description": "Forming and interpreting abstraction is a core process in human\ncommunication. In particular, when giving and performing complex instructions\nstated in natural language (NL), people may naturally evoke abstract constructs\nsuch as objects, loops, conditions and functions to convey their intentions in\nan efficient and precise way. Yet, interpreting and grounding abstraction\nstated in NL has not been systematically studied in NLP/AI. To elicit\nnaturally-occurring abstractions in NL we develop the Hexagons referential\ngame, where players describe increasingly complex images on a two-dimensional\nHexagons board, and other players need to follow these instructions to recreate\nthe images. Using this game we collected the Hexagons dataset, which consists\nof 164 images and over 3000 naturally-occurring instructions, rich with diverse\nabstractions. Results of our baseline models on an instruction-to-execution\ntask derived from the Hexagons dataset confirm that higher-level abstractions\nin NL are indeed more challenging for current systems to process. Thus, this\ndataset exposes a new and challenging dimension for grounded semantic parsing,\nand we propose it for the community as a future benchmark to explore more\nsophisticated and high-level communication within NLP applications.",
          "link": "http://arxiv.org/abs/2106.14321",
          "publishedOn": "2021-06-29T01:55:13.172Z",
          "wordCount": 622,
          "title": "Draw Me a Flower: Grounding Formal Abstract Structures Stated in Informal Natural Language. (arXiv:2106.14321v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lujun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>",
          "description": "Automatic chat summarization can help people quickly grasp important\ninformation from numerous chat messages. Unlike conventional documents, chat\nlogs usually have fragmented and evolving topics. In addition, these logs\ncontain a quantity of elliptical and interrogative sentences, which make the\nchat summarization highly context dependent. In this work, we propose a novel\nunsupervised framework called RankAE to perform chat summarization without\nemploying manually labeled data. RankAE consists of a topic-oriented ranking\nstrategy that selects topic utterances according to centrality and diversity\nsimultaneously, as well as a denoising auto-encoder that is carefully designed\nto generate succinct but context-informative summaries based on the selected\nutterances. To evaluate the proposed method, we collect a large-scale dataset\nof chat logs from a customer service environment and build an annotated set\nonly for model evaluation. Experimental results show that RankAE significantly\noutperforms other unsupervised methods and is able to generate high-quality\nsummaries in terms of relevance and topic coverage.",
          "link": "http://arxiv.org/abs/2012.07300",
          "publishedOn": "2021-06-28T01:57:54.755Z",
          "wordCount": 640,
          "title": "Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders. (arXiv:2012.07300v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "In this paper, we analyze the interplay between the use of offensive language\nand mental health. We acquired publicly available datasets created for\noffensive language identification and depression detection and we train\ncomputational models to compare the use of offensive language in social media\nposts written by groups of individuals with and without self-reported\ndepression diagnosis. We also look at samples written by groups of individuals\nwhose posts show signs of depression according to recent related studies. Our\nanalysis indicates that offensive language is more frequently used in the\nsamples written by individuals with self-reported depression as well as\nindividuals showing signs of depression. The results discussed here open new\navenues in research in politeness/offensiveness and mental health.",
          "link": "http://arxiv.org/abs/2105.14888",
          "publishedOn": "2021-06-28T01:57:54.735Z",
          "wordCount": 585,
          "title": "An Exploratory Analysis of the Relation Between Offensive Language and Mental Health. (arXiv:2105.14888v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09474",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Previous works on neural text-to-speech (TTS) have been addressed on limited\nspeed in training and inference time, robustness for difficult synthesis\nconditions, expressiveness, and controllability. Although several approaches\nresolve some limitations, there has been no attempt to solve all weaknesses at\nonce. In this paper, we propose STYLER, an expressive and controllable TTS\nframework with high-speed and robust synthesis. Our novel audio-text aligning\nmethod called Mel Calibrator and excluding autoregressive decoding enable rapid\ntraining and inference and robust synthesis on unseen data. Also, disentangled\nstyle factor modeling under supervision enlarges the controllability in\nsynthesizing process leading to expressive TTS. On top of it, a novel noise\nmodeling pipeline using domain adversarial training and Residual Decoding\nempowers noise-robust style transfer, decomposing the noise without any\nadditional label. Various experiments demonstrate that STYLER is more effective\nin speed and robustness than expressive TTS with autoregressive decoding and\nmore expressive and controllable than reading style non-autoregressive TTS.\nSynthesis samples and experiment results are provided via our demo page, and\ncode is available publicly.",
          "link": "http://arxiv.org/abs/2103.09474",
          "publishedOn": "2021-06-28T01:57:54.702Z",
          "wordCount": 684,
          "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech. (arXiv:2103.09474v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1\">Kishlay Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>",
          "description": "Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13711",
          "publishedOn": "2021-06-28T01:57:54.687Z",
          "wordCount": 668,
          "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1\">Brandon Kates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1\">Anant Kharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.",
          "link": "http://arxiv.org/abs/2106.13743",
          "publishedOn": "2021-06-28T01:57:54.647Z",
          "wordCount": 677,
          "title": "Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1\">Andargachew Mekonnen Gezmu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bati_T/0/1/0/all/0/1\">Tesfaye Bayu Bati</a>",
          "description": "This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be useful for machine\ntranslation of an under-resourced language, Amharic. The corpus is larger than\npreviously compiled corpora; it is released for research purposes. We trained\nneural machine translation and phrase-based statistical machine translation\nmodels using the corpus. In the automatic evaluation, neural machine\ntranslation models outperform phrase-based statistical machine translation\nmodels.",
          "link": "http://arxiv.org/abs/2104.03543",
          "publishedOn": "2021-06-28T01:57:54.598Z",
          "wordCount": 535,
          "title": "Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>",
          "description": "Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.",
          "link": "http://arxiv.org/abs/2005.05144",
          "publishedOn": "2021-06-28T01:57:54.581Z",
          "wordCount": 620,
          "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
          "link": "http://arxiv.org/abs/2007.15779",
          "publishedOn": "2021-06-28T01:57:54.572Z",
          "wordCount": 708,
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1\">Marcos Garcia</a>",
          "description": "This paper presents a multilingual study of word meaning representations in\ncontext. We assess the ability of both static and contextualized models to\nadequately represent different lexical-semantic relations, such as homonymy and\nsynonymy. To do so, we created a new multilingual dataset that allows us to\nperform a controlled evaluation of several factors such as the impact of the\nsurrounding context or the overlap between words, conveying the same or\ndifferent senses. A systematic assessment on four scenarios shows that the best\nmonolingual models based on Transformers can adequately disambiguate homonyms\nin context. However, as they rely heavily on context, these models fail at\nrepresenting words with different senses when occurring in similar sentences.\nExperiments are performed in Galician, Portuguese, English, and Spanish, and\nboth the dataset (with more than 3,000 evaluation items) and new models are\nfreely released with this study.",
          "link": "http://arxiv.org/abs/2106.13553",
          "publishedOn": "2021-06-28T01:57:54.559Z",
          "wordCount": 590,
          "title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy. (arXiv:2106.13553v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Much recent effort has been invested in non-autoregressive neural machine\ntranslation, which appears to be an efficient alternative to state-of-the-art\nautoregressive machine translation on modern GPUs. In contrast to the latter,\nwhere generation is sequential, the former allows generation to be parallelized\nacross target token positions. Some of the latest non-autoregressive models\nhave achieved impressive translation quality-speed tradeoffs compared to\nautoregressive baselines. In this work, we reexamine this tradeoff and argue\nthat autoregressive baselines can be substantially sped up without loss in\naccuracy. Specifically, we study autoregressive models with encoders and\ndecoders of varied depths. Our extensive experiments show that given a\nsufficiently deep encoder, a single-layer autoregressive decoder can\nsubstantially outperform strong non-autoregressive models with comparable\ninference speed. We show that the speed disadvantage for autoregressive\nbaselines compared to non-autoregressive methods has been overestimated in\nthree aspects: suboptimal layer allocation, insufficient speed measurement, and\nlack of knowledge distillation. Our results establish a new protocol for future\nresearch toward fast, accurate machine translation. Our code is available at\nhttps://github.com/jungokasai/deep-shallow.",
          "link": "http://arxiv.org/abs/2006.10369",
          "publishedOn": "2021-06-28T01:57:54.517Z",
          "wordCount": 666,
          "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. (arXiv:2006.10369v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aiswarya Vinod Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamyal_H/0/1/0/all/0/1\">Hira Dhamyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>",
          "description": "Open-set speaker recognition can be regarded as a metric learning problem,\nwhich is to maximize inter-class variance and minimize intra-class variance.\nSupervised metric learning can be categorized into entity-based learning and\nproxy-based learning. Most of the existing metric learning objectives like\nContrastive, Triplet, Prototypical, GE2E, etc all belong to the former\ndivision, the performance of which is either highly dependent on sample mining\nstrategy or restricted by insufficient label information in the mini-batch.\nProxy-based losses mitigate both shortcomings, however, fine-grained\nconnections among entities are either not or indirectly leveraged. This paper\nproposes a Masked Proxy (MP) loss which directly incorporates both proxy-based\nrelationships and pair-based relationships. We further propose Multinomial\nMasked Proxy (MMP) loss to leverage the hardness of speaker pairs. These\nmethods have been applied to evaluate on VoxCeleb test set and reach\nstate-of-the-art Equal Error Rate(EER).",
          "link": "http://arxiv.org/abs/2011.04491",
          "publishedOn": "2021-06-28T01:57:54.488Z",
          "wordCount": 612,
          "title": "Masked Proxy Loss For Text-Independent Speaker Verification. (arXiv:2011.04491v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "ELECTRA pretrains a discriminator to detect replaced tokens, where the\nreplacements are sampled from a generator trained with masked language\nmodeling. Despite the compelling performance, ELECTRA suffers from the\nfollowing two issues. First, there is no direct feedback loop from\ndiscriminator to generator, which renders replacement sampling inefficient.\nSecond, the generator's prediction tends to be over-confident along with\ntraining, making replacements biased to correct tokens. In this paper, we\npropose two methods to improve replacement sampling for ELECTRA pre-training.\nSpecifically, we augment sampling with a hardness prediction mechanism, so that\nthe generator can encourage the discriminator to learn what it has not\nacquired. We also prove that efficient sampling reduces the training variance\nof the discriminator. Moreover, we propose to use a focal loss for the\ngenerator in order to relieve oversampling of correct tokens as replacements.\nExperimental results show that our method improves ELECTRA pre-training on\nvarious downstream tasks.",
          "link": "http://arxiv.org/abs/2106.13715",
          "publishedOn": "2021-06-28T01:57:54.421Z",
          "wordCount": 589,
          "title": "Learning to Sample Replacements for ELECTRA Pre-Training. (arXiv:2106.13715v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1\">Hrishikesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1\">Bradly Alicea</a>",
          "description": "Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nthis http URL with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.",
          "link": "http://arxiv.org/abs/2106.13767",
          "publishedOn": "2021-06-28T01:57:54.388Z",
          "wordCount": 654,
          "title": "Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-06-28T01:57:54.318Z",
          "wordCount": 689,
          "title": "Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Recent years have witnessed the rapid advance in neural machine translation\n(NMT), the core of which lies in the encoder-decoder architecture. Inspired by\nthe recent progress of large-scale pre-trained language models on machine\ntranslation in a limited scenario, we firstly demonstrate that a single\nlanguage model (LM4MT) can achieve comparable performance with strong\nencoder-decoder NMT models on standard machine translation benchmarks, using\nthe same training data and similar amount of model parameters. LM4MT can also\neasily utilize source-side texts as additional supervision. Though modeling the\nsource- and target-language texts with the same mechanism, LM4MT can provide\nunified representations for both source and target sentences, which can better\ntransfer knowledge across languages. Extensive experiments on pivot-based and\nzero-shot translation tasks show that LM4MT can outperform the encoder-decoder\nNMT model by a large margin.",
          "link": "http://arxiv.org/abs/2106.13627",
          "publishedOn": "2021-06-28T01:57:54.292Z",
          "wordCount": 576,
          "title": "Language Models are Good Translators. (arXiv:2106.13627v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lujun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>",
          "description": "In a customer service system, dialogue summarization can boost service\nefficiency by automatically creating summaries for long spoken dialogues in\nwhich customers and agents try to address issues about specific topics. In this\nwork, we focus on topic-oriented dialogue summarization, which generates highly\nabstractive summaries that preserve the main ideas from dialogues. In spoken\ndialogues, abundant dialogue noise and common semantics could obscure the\nunderlying informative content, making the general topic modeling approaches\ndifficult to apply. In addition, for customer service, role-specific\ninformation matters and is an indispensable part of a summary. To effectively\nperform topic modeling on dialogues and capture multi-role information, in this\nwork we propose a novel topic-augmented two-stage dialogue summarizer (TDS)\njointly with a saliency-aware neural topic model (SATM) for topic-oriented\nsummarization of customer service dialogues. Comprehensive studies on a\nreal-world Chinese customer service dataset demonstrated the superiority of our\nmethod against several strong baselines.",
          "link": "http://arxiv.org/abs/2012.07311",
          "publishedOn": "2021-06-28T01:57:54.260Z",
          "wordCount": 637,
          "title": "Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling. (arXiv:2012.07311v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration.",
          "link": "http://arxiv.org/abs/2106.13736",
          "publishedOn": "2021-06-28T01:57:54.252Z",
          "wordCount": 594,
          "title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1\">Katrin Erk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbelot_A/0/1/0/all/0/1\">Aurelie Herbelot</a>",
          "description": "In this paper, we derive a notion of 'word meaning in context' which accounts\nfor the wide range of lexical shifts and ambiguities observed in utterance\ninterpretation. We characterize the lexical comprehension process as a\ncombination of cognitive semantics and Discourse Representation Theory,\nformalized as a 'situation description system': a probabilistic model which\ntakes utterance understanding to be the mental process of describing one or\nmore situations that would account for an observed utterance. Our model uses\ninsights from different types of generative models to capture the interplay of\nlocal and global contexts and their joint influence upon the lexical\nrepresentation of sentence constituents. We implement the system using a\ndirected graphical model, and apply it to examples containing various\ncontextualisation phenomena.",
          "link": "http://arxiv.org/abs/2009.07936",
          "publishedOn": "2021-06-28T01:57:54.245Z",
          "wordCount": 582,
          "title": "How to marry a star: probabilistic constraints for meaning in context. (arXiv:2009.07936v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10907",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1\">Elena Voita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "In Neural Machine Translation (and, more generally, conditional language\nmodeling), the generation of a target token is influenced by two types of\ncontext: the source and the prefix of the target sequence. While many attempts\nto understand the internal workings of NMT models have been made, none of them\nexplicitly evaluates relative source and target contributions to a generation\ndecision. We argue that this relative contribution can be evaluated by adopting\na variant of Layerwise Relevance Propagation (LRP). Its underlying\n'conservation principle' makes relevance propagation unique: differently from\nother methods, it evaluates not an abstract quantity reflecting token\nimportance, but the proportion of each token's influence. We extend LRP to the\nTransformer and conduct an analysis of NMT models which explicitly evaluates\nthe source and target relative contributions to the generation process. We\nanalyze changes in these contributions when conditioning on different types of\nprefixes, when varying the training objective or the amount of training data,\nand during the training process. We find that models trained with more data\ntend to rely on source information more and to have more sharp token\ncontributions; the training process is non-monotonic with several stages of\ndifferent nature.",
          "link": "http://arxiv.org/abs/2010.10907",
          "publishedOn": "2021-06-28T01:57:54.237Z",
          "wordCount": 680,
          "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation. (arXiv:2010.10907v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1808.00054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>",
          "description": "Humans read by making a sequence of fixations and saccades. They often skip\nwords, without apparent detriment to understanding. We offer a novel\nexplanation for skipping: readers optimize a tradeoff between performing a\nlanguage-related task and fixating as few words as possible. We propose a\nneural architecture that combines an attention module (deciding whether to skip\nwords) and a task module (memorizing the input). We show that our model\npredicts human skipping behavior, while also modeling reading times well, even\nthough it skips 40% of the input. A key prediction of our model is that\ndifferent reading tasks should result in different skipping behaviors. We\nconfirm this prediction in an eye-tracking experiment in which participants\nanswers questions about a text. We are able to capture these experimental\nresults using the our model, replacing the memorization module with a task\nmodule that performs neural question answering.",
          "link": "http://arxiv.org/abs/1808.00054",
          "publishedOn": "2021-06-28T01:57:54.194Z",
          "wordCount": 611,
          "title": "Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1\">Florina Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Subhajit Das</a>",
          "description": "Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.",
          "link": "http://arxiv.org/abs/2106.13411",
          "publishedOn": "2021-06-28T01:57:54.127Z",
          "wordCount": 646,
          "title": "Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gezmu_A/0/1/0/all/0/1\">Andargachew Mekonnen Gezmu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lema_T/0/1/0/all/0/1\">Tirufat Tesifaye Lema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyoum_B/0/1/0/all/0/1\">Binyam Ephrem Seyoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>",
          "description": "This paper presents a manually annotated spelling error corpus for Amharic,\nlingua franca in Ethiopia. The corpus is designed to be used for the evaluation\nof spelling error detection and correction. The misspellings are tagged as\nnon-word and real-word errors. In addition, the contextual information\navailable in the corpus makes it useful in dealing with both types of spelling\nerrors.",
          "link": "http://arxiv.org/abs/2106.13521",
          "publishedOn": "2021-06-28T01:57:54.098Z",
          "wordCount": 503,
          "title": "Manually Annotated Spelling Error Corpus for Amharic. (arXiv:2106.13521v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1\">Hieu-Thi Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>",
          "description": "Generally speaking, the main objective when training a neural speech\nsynthesis system is to synthesize natural and expressive speech from the output\nlayer of the neural network without much attention given to the hidden layers.\nHowever, by learning useful latent representation, the system can be used for\nmany more practical scenarios. In this paper, we investigate the use of\nquantized vectors to model the latent linguistic embedding and compare it with\nthe continuous counterpart. By enforcing different policies over the latent\nspaces in the training, we are able to obtain a latent linguistic embedding\nthat takes on different properties while having a similar performance in terms\nof quality and speaker similarity. Our experiments show that the voice cloning\nsystem built with vector quantization has only a small degradation in terms of\nperceptive evaluations, but has a discrete latent space that is useful for\nreducing the representation bit-rate, which is desirable for data transferring,\nor limiting the information leaking, which is important for speaker\nanonymization and other tasks of that nature.",
          "link": "http://arxiv.org/abs/2106.13479",
          "publishedOn": "2021-06-28T01:57:54.043Z",
          "wordCount": 624,
          "title": "Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance. (arXiv:2106.13479v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>",
          "description": "Ambiguity is a characteristic of natural language, which makes expression\nideas flexible. However, in a domain that requires accurate statements, it\nbecomes a barrier. Specifically, a single word can have many meanings and\nmultiple words can have the same meaning. When translating a text into a\nforeign language, the translator needs to determine the exact meaning of each\nelement in the original sentence to produce the correct translation sentence.\nFrom that observation, in this paper, we propose ParaLaw Nets, a pretrained\nmodel family using sentence-level cross-lingual information to reduce ambiguity\nand increase the performance in legal text processing. This approach achieved\nthe best result in the Question Answering task of COLIEE-2021.",
          "link": "http://arxiv.org/abs/2106.13403",
          "publishedOn": "2021-06-28T01:57:54.010Z",
          "wordCount": 574,
          "title": "ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text Processing. (arXiv:2106.13403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1\">Ivana Bala&#x17e;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.",
          "link": "http://arxiv.org/abs/2106.13353",
          "publishedOn": "2021-06-28T01:57:54.004Z",
          "wordCount": 592,
          "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.",
          "link": "http://arxiv.org/abs/2106.13474",
          "publishedOn": "2021-06-28T01:57:53.783Z",
          "wordCount": 604,
          "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains. (arXiv:2106.13474v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>",
          "description": "COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.",
          "link": "http://arxiv.org/abs/2106.13405",
          "publishedOn": "2021-06-28T01:57:53.734Z",
          "wordCount": 532,
          "title": "JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>",
          "description": "This article introduces byteSteady -- a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data -- DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.",
          "link": "http://arxiv.org/abs/2106.13302",
          "publishedOn": "2021-06-28T01:57:53.715Z",
          "wordCount": 571,
          "title": "byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.",
          "link": "http://arxiv.org/abs/2106.13375",
          "publishedOn": "2021-06-28T01:57:53.704Z",
          "wordCount": 693,
          "title": "Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1\">Hope McGovern</a>",
          "description": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.",
          "link": "http://arxiv.org/abs/2106.13382",
          "publishedOn": "2021-06-28T01:57:53.661Z",
          "wordCount": 569,
          "title": "A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premnadh_S/0/1/0/all/0/1\">Shyamnath Premnadh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1\">Maria Maleshkova</a>",
          "description": "In recent years, there have been significant developments in Question\nAnswering over Knowledge Graphs (KGQA). Despite all the notable advancements,\ncurrent KGQA systems only focus on answer generation techniques and not on\nanswer verbalization. However, in real-world scenarios (e.g., voice assistants\nsuch as Alexa, Siri, etc.), users prefer verbalized answers instead of a\ngenerated response. This paper addresses the task of answer verbalization for\n(complex) question answering over knowledge graphs. In this context, we propose\na multi-task-based answer verbalization framework: VOGUE (Verbalization thrOuGh\nmUlti-task lEarning). The VOGUE framework attempts to generate a verbalized\nanswer using a hybrid approach through a multi-task learning paradigm. Our\nframework can generate results based on using questions and queries as inputs\nconcurrently. VOGUE comprises four modules that are trained simultaneously\nthrough multi-task learning. We evaluate our framework on existing datasets for\nanswer verbalization, and it outperforms all current baselines on both BLEU and\nMETEOR scores.",
          "link": "http://arxiv.org/abs/2106.13316",
          "publishedOn": "2021-06-28T01:57:53.600Z",
          "wordCount": 593,
          "title": "VOGUE: Answer Verbalization through Multi-Task Learning. (arXiv:2106.13316v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1\">Harsh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleshkova_M/0/1/0/all/0/1\">Maria Maleshkova</a>",
          "description": "This paper addresses the task of (complex) conversational question answering\nover a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic\nparSing with trAnsformer and Graph atteNtion nEtworks). It is the first\napproach, which employs a transformer architecture extended with Graph\nAttention Networks for multi-task neural semantic parsing. LASAGNE uses a\ntransformer model for generating the base logical forms, while the Graph\nAttention model is used to exploit correlations between (entity) types and\npredicates to produce node representations. LASAGNE also includes a novel\nentity recognition module which detects, links, and ranks all relevant entities\nin the question context. We evaluate LASAGNE on a standard dataset for complex\nsequential question answering, on which it outperforms existing baseline\naverages on all question types. Specifically, we show that LASAGNE improves the\nF1-score on eight out of ten question types; in some cases, the increase in\nF1-score is more than 20% compared to the state of the art.",
          "link": "http://arxiv.org/abs/2104.01569",
          "publishedOn": "2021-06-25T02:00:46.117Z",
          "wordCount": 642,
          "title": "Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks. (arXiv:2104.01569v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1\">Mark Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez Rodr&#xed;guez</a>",
          "description": "We present the system submission from the FASTPARSE team for the EUD Shared\nTask at IWPT 2021. We engaged in the task last year by focusing on efficiency.\nThis year we have focused on experimenting with new ideas on a limited time\nbudget. Our system is based on splitting the EUD graph into several trees,\nbased on linguistic criteria. We predict these trees using a sequence-labelling\nparser and combine them into an EUD graph. The results were relatively poor,\nalthough not a total disaster and could probably be improved with some\npolishing of the system's rough edges.",
          "link": "http://arxiv.org/abs/2106.13155",
          "publishedOn": "2021-06-25T02:00:46.058Z",
          "wordCount": 549,
          "title": "Splitting EUD graphs into trees: A quick and clatty approach. (arXiv:2106.13155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-25T02:00:44.993Z",
          "wordCount": 645,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "QA models based on pretrained language mod-els have achieved remarkable\nperformance on various benchmark datasets.However, QA models do not generalize\nwell to unseen data that falls outside the training distribution, due to\ndistributional shifts.Data augmentation (DA) techniques which drop/replace\nwords have shown to be effective in regularizing the model from overfitting to\nthe training data.Yet, they may adversely affect the QA tasks since they incur\nsemantic changes that may lead to wrong answers for the QA task. To tackle this\nproblem, we propose a simple yet effective DA method based on a stochastic\nnoise generator, which learns to perturb the word embedding of the input\nquestions and context without changing their semantics. We validate the\nperformance of the QA models trained with our word embedding perturbation on a\nsingle source dataset, on five different target domains.The results show that\nour method significantly outperforms the baselineDA methods. Notably, the model\ntrained with ours outperforms the model trained with more than 240K\nartificially generated QA pairs.",
          "link": "http://arxiv.org/abs/2105.02692",
          "publishedOn": "2021-06-25T02:00:44.973Z",
          "wordCount": 637,
          "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA. (arXiv:2105.02692v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1\">Nicolai Pogrebnyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1\">Shohreh Shaghaghian</a>",
          "description": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.",
          "link": "http://arxiv.org/abs/2106.04641",
          "publishedOn": "2021-06-25T02:00:44.963Z",
          "wordCount": 543,
          "title": "Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacupaj_E/0/1/0/all/0/1\">Endri Kacupaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_H/0/1/0/all/0/1\">Harsh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "Neural semantic parsing approaches have been widely used for Question\nAnswering (QA) systems over knowledge graphs. Such methods provide the\nflexibility to handle QA datasets with complex queries and a large number of\nentities. In this work, we propose a novel framework named CARTON, which\nperforms multi-task semantic parsing for handling the problem of conversational\nquestion answering over a large-scale knowledge graph. Our framework consists\nof a stack of pointer networks as an extension of a context transformer model\nfor parsing the input question and the dialog history. The framework generates\na sequence of actions that can be executed on the knowledge graph. We evaluate\nCARTON on a standard dataset for complex sequential question answering on which\nCARTON outperforms all baselines. Specifically, we observe performance\nimprovements in F1-score on eight out of ten question types compared to the\nprevious state of the art. For logical reasoning questions, an improvement of\n11 absolute points is reached.",
          "link": "http://arxiv.org/abs/2103.07766",
          "publishedOn": "2021-06-25T02:00:44.946Z",
          "wordCount": 637,
          "title": "Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs. (arXiv:2103.07766v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "Inferring social relations from dialogues is vital for building emotionally\nintelligent robots to interpret human language better and act accordingly. We\nmodel the social network as an And-or Graph, named SocAoG, for the consistency\nof relations among a group and leveraging attributes as inference cues.\nMoreover, we formulate a sequential structure prediction task, and propose an\n$\\alpha$-$\\beta$-$\\gamma$ strategy to incrementally parse SocAoG for the\ndynamic inference upon any incoming utterance: (i) an $\\alpha$ process\npredicting attributes and relations conditioned on the semantics of dialogues,\n(ii) a $\\beta$ process updating the social relations based on related\nattributes, and (iii) a $\\gamma$ process updating individual's attributes based\non interpersonal social relations. Empirical results on DialogRE and MovieGraph\nshow that our model infers social relations more accurately than the\nstate-of-the-art methods. Moreover, the ablation study shows the three\nprocesses complement each other, and the case study demonstrates the dynamic\nrelational inference.",
          "link": "http://arxiv.org/abs/2106.01006",
          "publishedOn": "2021-06-25T02:00:44.940Z",
          "wordCount": 622,
          "title": "SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
          "link": "http://arxiv.org/abs/2106.13219",
          "publishedOn": "2021-06-25T02:00:44.927Z",
          "wordCount": 596,
          "title": "Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:44.921Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>",
          "description": "With the recent success of pre-trained models in NLP, a significant focus was\nput on interpreting their representations. One of the most prominent approaches\nis structural probing (Hewitt and Manning, 2019), where a linear projection of\nword embeddings is performed in order to approximate the topology of dependency\nstructures. In this work, we introduce a new type of structural probing, where\nthe linear projection is decomposed into 1. isomorphic space rotation; 2.\nlinear scaling that identifies and scales the most relevant dimensions. In\naddition to syntactic dependency, we evaluate our method on novel tasks\n(lexical hypernymy and position in a sentence). We jointly train the probes for\nmultiple tasks and experimentally show that lexical and syntactic information\nis separated in the representations. Moreover, the orthogonal constraint makes\nthe Structural Probes less vulnerable to memorization.",
          "link": "http://arxiv.org/abs/2012.15228",
          "publishedOn": "2021-06-25T02:00:44.903Z",
          "wordCount": 583,
          "title": "Introducing Orthogonal Constraint in Structural Probes. (arXiv:2012.15228v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tammewar_A/0/1/0/all/0/1\">Aniruddha Tammewar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>",
          "description": "Personal Narratives (PN) - recollections of facts, events, and thoughts from\none's own experience - are often used in everyday conversations. So far, PNs\nhave mainly been explored for tasks such as valence prediction or emotion\nclassification (e.g. happy, sad). However, these tasks might overlook more\nfine-grained information that could prove to be relevant for understanding PNs.\nIn this work, we propose a novel task for Narrative Understanding: Emotion\nCarrier Recognition (ECR). Emotion carriers, the text fragments that carry the\nemotions of the narrator (e.g. loss of a grandpa, high school reunion), provide\na fine-grained description of the emotion state. We explore the task of ECR in\na corpus of PNs manually annotated with emotion carriers and investigate\ndifferent machine learning models for the task. We propose evaluation\nstrategies for ECR including metrics that can be appropriate for different\ntasks.",
          "link": "http://arxiv.org/abs/2008.07481",
          "publishedOn": "2021-06-25T02:00:44.896Z",
          "wordCount": 604,
          "title": "Emotion Carrier Recognition from Personal Narratives. (arXiv:2008.07481v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Terrance Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1\">Anna Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1\">Michal Muszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1\">Ryo Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1\">David Brent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.",
          "link": "http://arxiv.org/abs/2106.13213",
          "publishedOn": "2021-06-25T02:00:44.881Z",
          "wordCount": 656,
          "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>",
          "description": "In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.",
          "link": "http://arxiv.org/abs/2106.13033",
          "publishedOn": "2021-06-25T02:00:44.853Z",
          "wordCount": 548,
          "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>",
          "description": "Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.",
          "link": "http://arxiv.org/abs/2010.07987",
          "publishedOn": "2021-06-25T02:00:44.846Z",
          "wordCount": 632,
          "title": "Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1\">Tasnim Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Transfer learning has yielded state-of-the-art (SoTA) results in many\nsupervised NLP tasks. However, annotated data for every target task in every\ntarget language is rare, especially for low-resource languages. We propose\nUXLA, a novel unsupervised data augmentation framework for zero-resource\ntransfer learning scenarios. In particular, UXLA aims to solve cross-lingual\nadaptation problems from a source language task distribution to an unknown\ntarget language task distribution, assuming no training label in the target\nlanguage. At its core, UXLA performs simultaneous self-training with data\naugmentation and unsupervised sample selection. To show its effectiveness, we\nconduct extensive experiments on three diverse zero-resource cross-lingual\ntransfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the\nbaselines by a good margin. With an in-depth framework dissection, we\ndemonstrate the cumulative contributions of different components to its\nsuccess.",
          "link": "http://arxiv.org/abs/2004.13240",
          "publishedOn": "2021-06-25T02:00:44.829Z",
          "wordCount": 611,
          "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "We introduce the largest transcribed Arabic speech corpus, QASR, collected\nfrom the broadcast domain. This multi-dialect speech dataset contains 2,000\nhours of speech sampled at 16kHz crawled from Aljazeera news channel. The\ndataset is released with lightly supervised transcriptions, aligned with the\naudio segments. Unlike previous datasets, QASR contains linguistically\nmotivated segmentation, punctuation, speaker information among others. QASR is\nsuitable for training and evaluating speech recognition systems, acoustics-\nand/or linguistics- based Arabic dialect identification, punctuation\nrestoration, speaker identification, speaker linking, and potentially other NLP\nmodules for spoken data. In addition to QASR transcription, we release a\ndataset of 130M words to aid in designing and training a better language model.\nWe show that end-to-end automatic speech recognition trained on QASR reports a\ncompetitive word error rate compared to the previous MGB-2 corpus. We report\nbaseline results for downstream natural language processing tasks such as named\nentity recognition using speech transcript. We also report the first baseline\nfor Arabic punctuation restoration. We make the corpus available for the\nresearch community.",
          "link": "http://arxiv.org/abs/2106.13000",
          "publishedOn": "2021-06-25T02:00:44.817Z",
          "wordCount": 645,
          "title": "QASR: QCRI Aljazeera Speech Resource -- A Large Scale Annotated Arabic Speech Corpus. (arXiv:2106.13000v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13985",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>",
          "description": "Humans have been shown to give contrastive explanations, which explain why an\nobserved event happened rather than some other counterfactual event (the\ncontrast case). Despite the influential role that contrastivity plays in how\nhumans explain, this property is largely missing from current methods for\nexplaining NLP models. We present Minimal Contrastive Editing (MiCE), a method\nfor producing contrastive explanations of model predictions in the form of\nedits to inputs that change model outputs to the contrast case. Our experiments\nacross three tasks--binary sentiment classification, topic classification, and\nmultiple-choice question answering--show that MiCE is able to produce edits\nthat are not only contrastive, but also minimal and fluent, consistent with\nhuman contrastive edits. We demonstrate how MiCE edits can be used for two use\ncases in NLP system development--debugging incorrect model outputs and\nuncovering dataset artifacts--and thereby illustrate that producing contrastive\nexplanations is a promising research direction for model interpretability.",
          "link": "http://arxiv.org/abs/2012.13985",
          "publishedOn": "2021-06-25T02:00:44.796Z",
          "wordCount": 612,
          "title": "Explaining NLP Models via Minimal Contrastive Editing (MiCE). (arXiv:2012.13985v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1\">Sahar Ghannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caubriere_A/0/1/0/all/0/1\">Antoine Caubri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1\">Salima Mdhaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1\">Ga&#xeb;lle Laperri&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1\">Bassam Jabaian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>",
          "description": "Spoken language understanding (SLU) topic has seen a lot of progress these\nlast three years, with the emergence of end-to-end neural approaches. Spoken\nlanguage understanding refers to natural language processing tasks related to\nsemantic extraction from speech signal, like named entity recognition from\nspeech or slot filling task in a context of human-machine dialogue.\nClassically, SLU tasks were processed through a cascade approach that consists\nin applying, firstly, an automatic speech recognition process, followed by a\nnatural language processing module applied to the automatic transcriptions.\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model. More recent works on\nself-supervised training with unlabeled data open new perspectives in term of\nperformance for automatic speech recognition and natural language processing.\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of additional data. We\nalso present our last results that significantly outperform the current\nstate-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for\nthe last state-of-the-art system presented this year.",
          "link": "http://arxiv.org/abs/2106.13045",
          "publishedOn": "2021-06-25T02:00:44.789Z",
          "wordCount": 658,
          "title": "Where are we in semantic concept extraction for Spoken Language Understanding?. (arXiv:2106.13045v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1\">Yannis Katsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1\">Mustafa Canim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feifei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>",
          "description": "Recent advances in transformers have enabled Table Question Answering (Table\nQA) systems to achieve high accuracy and SOTA results on open domain datasets\nlike WikiTableQuestions and WikiSQL. Such transformers are frequently\npre-trained on open-domain content such as Wikipedia, where they effectively\nencode questions and corresponding tables from Wikipedia as seen in Table QA\ndataset. However, web tables in Wikipedia are notably flat in their layout,\nwith the first row as the sole column header. The layout lends to a relational\nview of tables where each row is a tuple. Whereas, tables in domain-specific\nbusiness or scientific documents often have a much more complex layout,\nincluding hierarchical row and column headers, in addition to having\nspecialized vocabulary terms from that domain.\n\nTo address this problem, we introduce the domain-specific Table QA dataset\nAIT-QA (Airline Industry Table QA). The dataset consists of 515 questions\nauthored by human annotators on 116 tables extracted from public U.S. SEC\nfilings (publicly available at: https://www.sec.gov/edgar.shtml) of major\nairline companies for the fiscal years 2017-2019. We also provide annotations\npertaining to the nature of questions, marking those that require hierarchical\nheaders, domain-specific terminology, and paraphrased forms. Our zero-shot\nbaseline evaluation of three transformer-based SOTA Table QA methods - TaPAS\n(end-to-end), TaBERT (semantic parsing-based), and RCI (row-column\nencoding-based) - clearly exposes the limitation of these methods in this\npractical setting, with the best accuracy at just 51.8\\% (RCI). We also present\npragmatic table preprocessing steps used to pivot and project these complex\ntables into a layout suitable for the SOTA Table QA models.",
          "link": "http://arxiv.org/abs/2106.12944",
          "publishedOn": "2021-06-25T02:00:44.771Z",
          "wordCount": 714,
          "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry. (arXiv:2106.12944v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuntian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Lawrence An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnicow_K/0/1/0/all/0/1\">Kenneth Resnicow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1\">Ver&#xf3;nica P&#xe9;rez-Rosas</a>",
          "description": "A growing number of people engage in online health forums, making it\nimportant to understand the quality of the advice they receive. In this paper,\nwe explore the role of expertise in responses provided to help-seeking posts\nregarding mental health. We study the differences between (1) interactions with\npeers; and (2) interactions with self-identified mental health professionals.\nFirst, we show that a classifier can distinguish between these two groups,\nindicating that their language use does in fact differ. To understand this\ndifference, we perform several analyses addressing engagement aspects,\nincluding whether their comments engage the support-seeker further as well as\nlinguistic aspects, such as dominant language and linguistic style matching.\nOur work contributes toward the developing efforts of understanding how health\nexperts engage with health information- and support-seekers in social networks.\nMore broadly, it is a step toward a deeper understanding of the styles of\ninteractions that cultivate supportive engagement in online communities.",
          "link": "http://arxiv.org/abs/2106.12976",
          "publishedOn": "2021-06-25T02:00:44.764Z",
          "wordCount": 601,
          "title": "Exploring Self-Identified Counseling Expertise in Online Support Forums. (arXiv:2106.12976v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1\">Alessandro Solbiati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1\">Georgios Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Shivani Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Shubham Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1\">Jacques Cali</a>",
          "description": "Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.",
          "link": "http://arxiv.org/abs/2106.12978",
          "publishedOn": "2021-06-25T02:00:44.700Z",
          "wordCount": 519,
          "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandrahas/0/1/0/all/0/1\">Chandrahas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Pratim Talukdar</a>",
          "description": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation\nphrase, tail noun phrase) triples such as (tesla, return to, new york)\nextracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap\nfor a domain, they are very sparse and far from being directly usable in an end\ntask. Therefore, the task of predicting new facts, i.e., link prediction,\nbecomes an important step while using these graphs in downstream tasks such as\ntext comprehension, question answering, and web search query recommendation.\nLearning embeddings for OpenKGs is one approach for link prediction that has\nreceived some attention lately. However, on careful examination, we found that\ncurrent OpenKG link prediction algorithms often predict noun phrases (NPs) with\nincompatible types for given noun and relation phrases. We address this problem\nin this work and propose OKGIT that improves OpenKG link prediction using novel\ntype compatibility score and type regularization. With extensive experiments on\nmultiple datasets, we show that the proposed method achieves state-of-the-art\nperformance while producing type compatible NPs in the link prediction task.",
          "link": "http://arxiv.org/abs/2106.12806",
          "publishedOn": "2021-06-25T02:00:44.690Z",
          "wordCount": 613,
          "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types. (arXiv:2106.12806v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1\">Christiaan Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "Acoustic word embedding models map variable duration speech segments to fixed\ndimensional vectors, enabling efficient speech search and discovery. Previous\nwork explored how embeddings can be obtained in zero-resource settings where no\nlabelled data is available in the target language. The current best approach\nuses transfer learning: a single supervised multilingual model is trained using\nlabelled data from multiple well-resourced languages and then applied to a\ntarget zero-resource language (without fine-tuning). However, it is still\nunclear how the specific choice of training languages affect downstream\nperformance. Concretely, here we ask whether it is beneficial to use training\nlanguages related to the target. Using data from eleven languages spoken in\nSouthern Africa, we experiment with adding data from different language\nfamilies while controlling for the amount of data per language. In word\ndiscrimination and query-by-example search evaluations, we show that training\non languages from the same family gives large improvements. Through\nfiner-grained analysis, we show that training on even just a single related\nlanguage gives the largest gain. We also find that adding data from unrelated\nlanguages generally doesn't hurt performance.",
          "link": "http://arxiv.org/abs/2106.12834",
          "publishedOn": "2021-06-25T02:00:44.681Z",
          "wordCount": 641,
          "title": "Multilingual transfer of acoustic word embeddings improves when training on languages related to the target zero-resource language. (arXiv:2106.12834v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murauer_B/0/1/0/all/0/1\">Benjamin Murauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschuggnall_M/0/1/0/all/0/1\">Michael Tschuggnall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_G/0/1/0/all/0/1\">G&#xfc;nther Specht</a>",
          "description": "In the last decade, machine translation has become a popular means to deal\nwith multilingual digital content. By providing higher quality translations,\nobfuscating the source language of a text becomes more attractive. In this\npaper, we analyze the ability to detect the source language from the translated\noutput of two widely used commercial machine translation systems by utilizing\nmachine-learning algorithms with basic textual features like n-grams.\nEvaluations show that the source language can be reconstructed with high\naccuracy for documents that contain a sufficient amount of translated text. In\naddition, we analyze how the document size influences the performance of the\nprediction, as well as how limiting the set of possible source languages\nimproves the classification accuracy.",
          "link": "http://arxiv.org/abs/2106.12830",
          "publishedOn": "2021-06-25T02:00:44.648Z",
          "wordCount": 571,
          "title": "On the Influence of Machine Translation on Language Origin Obfuscation. (arXiv:2106.12830v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-06-25T02:00:44.639Z",
          "wordCount": 526,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1\">Dalton Schutte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1\">Anu Bompelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1\">Marcelo Fiszman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1\">Jeffrey R. Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1\">Terrence Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.",
          "link": "http://arxiv.org/abs/2106.12741",
          "publishedOn": "2021-06-25T02:00:44.619Z",
          "wordCount": 714,
          "title": "Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shang-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>",
          "description": "Given the clinical notes written in electronic health records (EHRs), it is\nchallenging to predict the diagnostic codes which is formulated as a\nmulti-label classification task. The large set of labels, the hierarchical\ndependency, and the imbalanced data make this prediction task extremely hard.\nMost existing work built a binary prediction for each label independently,\nignoring the dependencies between labels. To address this problem, we propose a\ntwo-stage framework to improve automatic ICD coding by capturing the label\ncorrelation. Specifically, we train a label set distribution estimator to\nrescore the probability of each label set candidate generated by a base\npredictor. This paper is the first attempt at learning the label set\ndistribution as a reranking module for medical code prediction. In the\nexperiments, our proposed framework is able to improve upon best-performing\npredictors on the benchmark MIMIC datasets. The source code of this project is\navailable at https://github.com/MiuLab/ICD-Correlation.",
          "link": "http://arxiv.org/abs/2106.12800",
          "publishedOn": "2021-06-25T02:00:44.598Z",
          "wordCount": 592,
          "title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding. (arXiv:2106.12800v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "This paper describes FBK's system submission to the IWSLT 2021 Offline Speech\nTranslation task. We participated with a direct model, which is a\nTransformer-based architecture trained to translate English speech audio data\ninto German texts. The training pipeline is characterized by knowledge\ndistillation and a two-step fine-tuning procedure. Both knowledge distillation\nand the first fine-tuning step are carried out on manually segmented real and\nsynthetic data, the latter being generated with an MT system trained on the\navailable corpora. Differently, the second fine-tuning step is carried out on a\nrandom segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce\nthe performance drops occurring when a speech translation model trained on\nmanually segmented data (i.e. an ideal, sentence-like segmentation) is\nevaluated on automatically segmented audio (i.e. actual, more realistic testing\nconditions). For the same purpose, a custom hybrid segmentation procedure that\naccounts for both audio content (pauses) and for the length of the produced\nsegments is applied to the test data before passing them to the system. At\ninference time, we compared this procedure with a baseline segmentation method\nbased on Voice Activity Detection (VAD). Our results indicate the effectiveness\nof the proposed hybrid approach, shown by a reduction of the gap with manual\nsegmentation from 8.3 to 1.4 BLEU points.",
          "link": "http://arxiv.org/abs/2106.12607",
          "publishedOn": "2021-06-25T02:00:44.574Z",
          "wordCount": 660,
          "title": "Dealing with training and test segmentation mismatch: FBK@IWSLT2021. (arXiv:2106.12607v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1\">Sara Evensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>",
          "description": "Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.",
          "link": "http://arxiv.org/abs/2106.12767",
          "publishedOn": "2021-06-25T02:00:44.536Z",
          "wordCount": 660,
          "title": "TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1\">Jia Wei Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Mei Shin Oh</a>",
          "description": "Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.",
          "link": "http://arxiv.org/abs/2106.12744",
          "publishedOn": "2021-06-25T02:00:44.512Z",
          "wordCount": 677,
          "title": "An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>",
          "description": "We analyze the process of creating word embedding feature representations\ndesigned for a learning task when annotated data is scarce, for example, in\ndepressive language detection from Tweets. We start with a rich word embedding\npre-trained from a large general dataset, which is then augmented with\nembeddings learned from a much smaller and more specific domain dataset through\na simple non-linear mapping mechanism. We also experimented with several other\nmore sophisticated methods of such mapping including, several auto-encoder\nbased and custom loss-function based methods that learn embedding\nrepresentations through gradually learning to be close to the words of similar\nsemantics and distant to dissimilar semantics. Our strengthened representations\nbetter capture the semantics of the depression domain, as it combines the\nsemantics learned from the specific domain coupled with word coverage from the\ngeneral language. We also present a comparative performance analyses of our\nword embedding representations with a simple bag-of-words model, well known\nsentiment and psycholinguistic lexicons, and a general pre-trained word\nembedding. When used as feature representations for several different machine\nlearning methods, including deep learning models in a depressive Tweets\nidentification task, we show that our augmented word embedding representations\nachieve a significantly better F1 score than the others, specially when applied\nto a high quality dataset. Also, we present several data ablation tests which\nconfirm the efficacy of our augmentation techniques.",
          "link": "http://arxiv.org/abs/2106.12797",
          "publishedOn": "2021-06-25T02:00:44.500Z",
          "wordCount": 707,
          "title": "A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>",
          "description": "Traditionally, character-level transduction problems have been solved with\nfinite-state models designed to encode structural and linguistic knowledge of\nthe underlying process, whereas recent approaches rely on the power and\nflexibility of sequence-to-sequence models with attention. Focusing on the less\nexplored unsupervised learning scenario, we compare the two model classes side\nby side and find that they tend to make different types of errors even when\nachieving comparable performance. We analyze the distributions of different\nerror classes using two unsupervised tasks as testbeds: converting informally\nromanized text into the native script of its language (for Russian, Arabic, and\nKannada) and translating between a pair of closely related languages (Serbian\nand Bosnian). Finally, we investigate how combining finite-state and\nsequence-to-sequence models at decoding time affects the output quantitatively\nand qualitatively.",
          "link": "http://arxiv.org/abs/2106.12698",
          "publishedOn": "2021-06-25T02:00:44.478Z",
          "wordCount": 574,
          "title": "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction. (arXiv:2106.12698v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>",
          "description": "With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.",
          "link": "http://arxiv.org/abs/2106.12700",
          "publishedOn": "2021-06-25T02:00:44.455Z",
          "wordCount": 626,
          "title": "Bidding via Clustering Ads Intentions: an Efficient Search Engine Marketing System for E-commerce. (arXiv:2106.12700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-06-25T02:00:44.409Z",
          "wordCount": 628,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1\">J. Harry Caufield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1\">Kevin Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Calvin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_P/0/1/0/all/0/1\">Peipei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The clinical named entity recognition (CNER) task seeks to locate and\nclassify clinical terminologies into predefined categories, such as diagnostic\nprocedure, disease disorder, severity, medication, medication dosage, and sign\nsymptom. CNER facilitates the study of side-effect on medications including\nidentification of novel phenomena and human-focused information extraction.\nExisting approaches in extracting the entities of interests focus on using\nstatic word embeddings to represent each word. However, one word can have\ndifferent interpretations that depend on the context of the sentences.\nEvidently, static word embeddings are insufficient to integrate the diverse\ninterpretation of a word. To overcome this challenge, the technique of\ncontextualized word embedding has been introduced to better capture the\nsemantic meaning of each word based on its context. Two of these language\nmodels, ELMo and Flair, have been widely used in the field of Natural Language\nProcessing to generate the contextualized word embeddings on domain-generic\ndocuments. However, these embeddings are usually too general to capture the\nproximity among vocabularies of specific domains. To facilitate various\ndownstream applications using clinical case reports (CCRs), we pre-train two\ndeep contextualized language models, Clinical Embeddings from Language Model\n(C-ELMo) and Clinical Contextual String Embeddings (C-Flair) using the\nclinical-related corpus from the PubMed Central. Explicit experiments show that\nour models gain dramatic improvements compared to both static word embeddings\nand domain-generic language models.",
          "link": "http://arxiv.org/abs/2106.12608",
          "publishedOn": "2021-06-25T02:00:44.401Z",
          "wordCount": 672,
          "title": "Clinical Named Entity Recognition using Contextualized Token Representations. (arXiv:2106.12608v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1\">Kabil Essefar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1\">Nabil El Mamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1\">Ahmed Khoumsi</a>",
          "description": "The prominence of figurative language devices, such as sarcasm and irony,\nposes serious challenges for Arabic Sentiment Analysis (SA). While previous\nresearch works tackle SA and sarcasm detection separately, this paper\nintroduces an end-to-end deep Multi-Task Learning (MTL) model, allowing\nknowledge interaction between the two tasks. Our MTL model's architecture\nconsists of a Bidirectional Encoder Representation from Transformers (BERT)\nmodel, a multi-task attention interaction module, and two task classifiers. The\noverall obtained results show that our proposed model outperforms its\nsingle-task counterparts on both SA and sarcasm detection sub-tasks.",
          "link": "http://arxiv.org/abs/2106.12488",
          "publishedOn": "2021-06-24T01:51:43.048Z",
          "wordCount": 536,
          "title": "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in Arabic Language. (arXiv:2106.12488v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_O/0/1/0/all/0/1\">Oliver Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1\">Daniel Buschek</a>",
          "description": "We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.",
          "link": "http://arxiv.org/abs/2106.12314",
          "publishedOn": "2021-06-24T01:51:42.964Z",
          "wordCount": 611,
          "title": "CharacterChat: Supporting the Creation of Fictional Characters through Conversation and Progressive Manifestation with a Chatbot. (arXiv:2106.12314v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>",
          "description": "Broad-coverage meaning representations in NLP mostly focus on explicitly\nexpressed content. More importantly, the scarcity of datasets annotating\ndiverse implicit roles limits empirical studies into their linguistic nuances.\nFor example, in the web review \"Great service!\", the provider and consumer are\nimplicit arguments of different types. We examine an annotated corpus of\nfine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully\nre-annotating it, resolving several inconsistencies. Subsequently, we present\nthe first transition-based neural parser that can handle implicit arguments\ndynamically, and experiment with two different transition systems on the\nimproved dataset. We find that certain types of implicit arguments are more\ndifficult to parse than others and that the simpler system is more accurate in\nrecovering implicit arguments, despite having a lower overall parsing score,\nattesting current reasoning limitations of NLP models. This work will\nfacilitate a better understanding of implicit and underspecified language, by\nincorporating it holistically into meaning representations.",
          "link": "http://arxiv.org/abs/2106.02561",
          "publishedOn": "2021-06-24T01:51:42.958Z",
          "wordCount": 599,
          "title": "Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiapeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1\">Etienne Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters.",
          "link": "http://arxiv.org/abs/2106.12144",
          "publishedOn": "2021-06-24T01:51:42.903Z",
          "wordCount": 600,
          "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trentin_E/0/1/0/all/0/1\">Edmondo Trentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1\">Roberto Gretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1\">Marco Matassoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1\">Daniele Falavigna</a>",
          "description": "The paper copes with the task of automatic assessment of second language\nproficiency from the language learners' spoken responses to test prompts. The\ntask has significant relevance to the field of computer assisted language\nlearning. The approach presented in the paper relies on two separate modules:\n(1) an automatic speech recognition system that yields text transcripts of the\nspoken interactions involved, and (2) a multiple classifier system based on\ndeep learners that ranks the transcripts into proficiency classes. Different\ndeep neural network architectures (both feed-forward and recurrent) are\nspecialized over diverse representations of the texts in terms of: a reference\ngrammar, the outcome of probabilistic language models, several word embeddings,\nand two bag-of-word models. Combination of the individual classifiers is\nrealized either via a probabilistic pseudo-joint model, or via a neural mixture\nof experts. Using the data of the third Spoken CALL Shared Task challenge, the\nhighest values to date were obtained in terms of three popular evaluation\nmetrics.",
          "link": "http://arxiv.org/abs/2106.12475",
          "publishedOn": "2021-06-24T01:51:42.862Z",
          "wordCount": 602,
          "title": "Mixtures of Deep Neural Experts for Automated Speech Scoring. (arXiv:2106.12475v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peiqing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xunyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>",
          "description": "Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26\nparticipants, who labeled their rationales to answer the question. Besides, we\nconducted a rationale evaluation session in which participants were asked to\nanswering the question solely based on labeled rationales, confirming that the\nlabeled rationales were of high quality and can sufficiently support question\nanswering.",
          "link": "http://arxiv.org/abs/2106.12373",
          "publishedOn": "2021-06-24T01:51:42.857Z",
          "wordCount": 570,
          "title": "PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1\">Ning Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
          "link": "http://arxiv.org/abs/2003.08271",
          "publishedOn": "2021-06-24T01:51:42.735Z",
          "wordCount": 603,
          "title": "Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Premkumar Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.",
          "link": "http://arxiv.org/abs/2105.04054",
          "publishedOn": "2021-06-24T01:51:42.706Z",
          "wordCount": 631,
          "title": "Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "Arguably, the visual perception of conversational agents to the physical\nworld is a key way for them to exhibit the human-like intelligence.\nImage-grounded conversation is thus proposed to address this challenge.\nExisting works focus on exploring the multimodal dialog models that ground the\nconversation on a given image. In this paper, we take a step further to study\nimage-grounded conversation under a fully open-ended setting where no paired\ndialog and image are assumed available. Specifically, we present Maria, a\nneural conversation agent powered by the visual world experiences which are\nretrieved from a large-scale image index. Maria consists of three flexible\ncomponents, i.e., text-to-image retriever, visual concept detector and\nvisual-knowledge-grounded response generator. The retriever aims to retrieve a\ncorrelated image to the dialog from an image index, while the visual concept\ndetector extracts rich visual knowledge from the image. Then, the response\ngenerator is grounded on the extracted visual knowledge and dialog context to\ngenerate the target response. Extensive experiments demonstrate Maria\noutperforms previous state-of-the-art methods on automatic metrics and human\nevaluation, and can generate informative responses that have some visual\ncommonsense of the physical world.",
          "link": "http://arxiv.org/abs/2105.13073",
          "publishedOn": "2021-06-24T01:51:42.643Z",
          "wordCount": 666,
          "title": "Maria: A Visual Experience Powered Conversational Agent. (arXiv:2105.13073v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pretrained vision models such as VGG16 and ResNet\n\nIndex Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain\nAdaptation.",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-06-24T01:51:42.624Z",
          "wordCount": 675,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Olaleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it is possible\nto train a model that can detect whether a particular text keyword occurs in\nspeech utterances or not. Here we investigate whether visually grounded speech\nmodels can also do keyword localisation: predicting where, within an utterance,\na given textual keyword occurs without any explicit text-based or alignment\nsupervision. We specifically consider whether incorporating attention into a\nconvolutional model is beneficial for localisation. Although absolute\nlocalisation performance with visually supervised models is still modest\n(compared to using unordered bag-of-word text labels for supervision), we show\nthat attention provides a large gain in performance over previous visually\ngrounded models. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating the\nword 'backstroke' for the query keyword 'swimming'.",
          "link": "http://arxiv.org/abs/2106.08859",
          "publishedOn": "2021-06-24T01:51:42.618Z",
          "wordCount": 621,
          "title": "Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1\">Abdellah El Mekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1\">Abdelkader El Mahdaouy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1\">Kabil Essefar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1\">Nabil El Mamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1\">Ismail Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1\">Ahmed Khoumsi</a>",
          "description": "Dialect and standard language identification are crucial tasks for many\nArabic natural language processing applications. In this paper, we present our\ndeep learning-based system, submitted to the second NADI shared task for\ncountry-level and province-level identification of Modern Standard Arabic (MSA)\nand Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task\nLearning (MTL) model to tackle both country-level and province-level MSA/DA\nidentification. The latter MTL model consists of a shared Bidirectional Encoder\nRepresentation Transformers (BERT) encoder, two task-specific attention layers,\nand two classifiers. Our key idea is to leverage both the task-discriminative\nand the inter-task shared features for country and province MSA/DA\nidentification. The obtained results show that our MTL model outperforms\nsingle-task models on most subtasks.",
          "link": "http://arxiv.org/abs/2106.12495",
          "publishedOn": "2021-06-24T01:51:42.599Z",
          "wordCount": 573,
          "title": "BERT-based Multi-Task Model for Country and Province Level Modern Standard Arabic and Dialectal Arabic Identification. (arXiv:2106.12495v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.11561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1\">Itamar Ciochetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1\">Emerson Bertolo</a>",
          "description": "The objective of this paper is to develop predictive models to classify\nBrazilian legal proceedings in three possible classes of status: (i) archived\nproceedings, (ii) active proceedings, and (iii) suspended proceedings. This\nproblem's resolution is intended to assist public and private institutions in\nmanaging large portfolios of legal proceedings, providing gains in scale and\nefficiency. In this paper, legal proceedings are made up of sequences of short\ntexts called \"motions.\" We combined several natural language processing (NLP)\nand machine learning techniques to solve the problem. Although working with\nPortuguese NLP, which can be challenging due to lack of resources, our\napproaches performed remarkably well in the classification task, achieving\nmaximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93\n(weighted). Furthermore, we could extract and interpret the patterns learned by\none of our models besides quantifying how those patterns relate to the\nclassification task. The interpretability step is important among machine\nlearning legal applications and gives us an exciting insight into how black-box\nmodels make decisions.",
          "link": "http://arxiv.org/abs/2003.11561",
          "publishedOn": "2021-06-24T01:51:42.584Z",
          "wordCount": 677,
          "title": "Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaying Zhu</a>",
          "description": "Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.",
          "link": "http://arxiv.org/abs/2105.14300",
          "publishedOn": "2021-06-24T01:51:42.572Z",
          "wordCount": 629,
          "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thaker_K/0/1/0/all/0/1\">Khushboo Thaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Daqing He</a>",
          "description": "Faceted summarization provides briefings of a document from different\nperspectives. Readers can quickly comprehend the main points of a long document\nwith the help of a structured outline. However, little research has been\nconducted on this subject, partially due to the lack of large-scale faceted\nsummarization datasets. In this study, we present FacetSum, a faceted\nsummarization benchmark built on Emerald journal articles, covering a diverse\nrange of domains. Different from traditional document-summary pairs, FacetSum\nprovides multiple summaries, each targeted at specific sections of a long\ndocument, including the purpose, method, findings, and value. Analyses and\nempirical results on our dataset reveal the importance of bringing structure\ninto summaries. We believe FacetSum will spur further advances in summarization\nresearch and foster the development of NLP systems that can leverage the\nstructured information in both long texts and summaries.",
          "link": "http://arxiv.org/abs/2106.00130",
          "publishedOn": "2021-06-24T01:51:42.566Z",
          "wordCount": 603,
          "title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. (arXiv:2106.00130v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Aires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1\">Du&#x161;an Vari&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>",
          "description": "Lexically constrained machine translation allows the user to manipulate the\noutput sentence by enforcing the presence or absence of certain words and\nphrases. Although current approaches can enforce terms to appear in the\ntranslation, they often struggle to make the constraint word form agree with\nthe rest of the generated output. Our manual analysis shows that 46% of the\nerrors in the output of a baseline constrained model for English to Czech\ntranslation are related to agreement. We investigate mechanisms to allow neural\nmachine translation to infer the correct word inflection given lemmatized\nconstraints. In particular, we focus on methods based on training the model\nwith constraints provided as part of the input sequence. Our experiments on the\nEnglish-Czech language pair show that this approach improves the translation of\nconstrained terms in both automatic and manual evaluation by reducing errors in\nagreement. Our approach thus eliminates inflection errors, without introducing\nnew errors or decreasing the overall quality of the translation.",
          "link": "http://arxiv.org/abs/2106.12398",
          "publishedOn": "2021-06-24T01:51:42.503Z",
          "wordCount": 594,
          "title": "End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages. (arXiv:2106.12398v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.",
          "link": "http://arxiv.org/abs/2012.09807",
          "publishedOn": "2021-06-24T01:51:42.485Z",
          "wordCount": 589,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>",
          "description": "We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.",
          "link": "http://arxiv.org/abs/2010.03802",
          "publishedOn": "2021-06-24T01:51:42.479Z",
          "wordCount": 662,
          "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1\">Chundra Aroor Cathcart</a>",
          "description": "This paper addresses a series of complex and unresolved issues in the\nhistorical phonology of West Iranian languages. The West Iranian languages\n(Persian, Kurdish, Balochi, and other languages) display a high degree of\nnon-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to\nlanguage contact; we argue, however, that an oversimplified view of the\nprocesses at work has prevailed in the literature on West Iranian dialectology,\nwith specialists assuming that deviations from an expected outcome in a given\nnon-Persian language are due to lexical borrowing from some chronological stage\nof Persian. It is demonstrated that this qualitative approach yields at times\nproblematic conclusions stemming from the lack of explicit probabilistic\ninferences regarding the distribution of the data: Persian may not be the sole\ndonor language; additionally, borrowing at the lexical level is not always the\nmechanism that introduces irregularity. In many cases, the possibility that\nWest Iranian languages show different reflexes in different conditioning\nenvironments remains under-explored. We employ a novel Bayesian approach\ndesigned to overcome these problems and tease apart the different determinants\nof irregularity in patterns of West Iranian sound change. Our methodology\nallows us to provisionally resolve a number of outstanding questions in the\nliterature on West Iranian dialectology concerning the dialectal affiliation of\ncertain sound changes. We outline future directions for work of this sort.",
          "link": "http://arxiv.org/abs/2001.05297",
          "publishedOn": "2021-06-24T01:51:42.453Z",
          "wordCount": 683,
          "title": "Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aithal_M/0/1/0/all/0/1\">Madhusudhan Aithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>",
          "description": "Prior work has revealed that positive words occur more frequently than\nnegative words in human expressions, which is typically attributed to\npositivity bias, a tendency for people to report positive views of reality. But\nwhat about the language used in negative reviews? Consistent with prior work,\nwe show that English negative reviews tend to contain more positive words than\nnegative words, using a variety of datasets. We reconcile this observation with\nprior findings on the pragmatics of negation, and show that negations are\ncommonly associated with positive words in negative reviews. Furthermore, in\nnegative reviews, the majority of sentences with positive words express\nnegative opinions based on sentiment classifiers, indicating some form of\nnegation.",
          "link": "http://arxiv.org/abs/2106.12056",
          "publishedOn": "2021-06-24T01:51:42.447Z",
          "wordCount": 553,
          "title": "On Positivity Bias in Negative Reviews. (arXiv:2106.12056v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>",
          "description": "Commonsense reasoning is one of the key problems in natural language\nprocessing, but the relative scarcity of labeled data holds back the progress\nfor languages other than English. Pretrained cross-lingual models are a source\nof powerful language-agnostic representations, yet their inherent reasoning\ncapabilities are still actively studied. In this work, we design a simple\napproach to commonsense reasoning which trains a linear classifier with weights\nof multi-head attention as features. To evaluate this approach, we create a\nmultilingual Winograd Schema corpus by processing several datasets from prior\nwork within a standardized pipeline and measure cross-lingual generalization\nability in terms of out-of-sample performance. The method performs\ncompetitively with recent supervised and unsupervised approaches for\ncommonsense reasoning, even when applied to other languages in a zero-shot\nmanner. Also, we demonstrate that most of the performance is given by the same\nsmall subset of attention heads for all studied languages, which provides\nevidence of universal reasoning capabilities in multilingual encoders.",
          "link": "http://arxiv.org/abs/2106.12066",
          "publishedOn": "2021-06-24T01:51:42.424Z",
          "wordCount": 622,
          "title": "It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07920",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Naowarat_B/0/1/0/all/0/1\">Burin Naowarat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kongthaworn_T/0/1/0/all/0/1\">Thananchai Kongthaworn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karunratanakul_K/0/1/0/all/0/1\">Korrawe Karunratanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sheng Hui Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chuangsuwanich_E/0/1/0/all/0/1\">Ekapol Chuangsuwanich</a>",
          "description": "Code-Switching (CS) remains a challenge for Automatic Speech Recognition\n(ASR), especially character-based models. With the combined choice of\ncharacters from multiple languages, the outcome from character-based models\nsuffers from phoneme duplication, resulting in language-inconsistent spellings.\nWe propose Contextualized Connectionist Temporal Classification (CCTC) loss to\nencourage spelling consistencies of a character-based non-autoregressive ASR\nwhich allows for faster inference. The CCTC loss conditions the main prediction\non the predicted contexts to ensure language consistency in the spellings. In\ncontrast to existing CTC-based approaches, CCTC loss does not require\nframe-level alignments, since the context ground truth is obtained from the\nmodel's estimated path. Compared to the same model trained with regular CTC\nloss, our method consistently improved the ASR performance on both CS and\nmonolingual corpora.",
          "link": "http://arxiv.org/abs/2005.07920",
          "publishedOn": "2021-06-24T01:51:42.416Z",
          "wordCount": 605,
          "title": "Reducing Spelling Inconsistencies in Code-Switching ASR using Contextualized CTC Loss. (arXiv:2005.07920v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yuanxing Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>",
          "description": "Event extraction is a fundamental task for natural language processing.\nFinding the roles of event arguments like event participants is essential for\nevent extraction. However, doing so for real-life event descriptions is\nchallenging because an argument's role often varies in different contexts.\nWhile the relationship and interactions between multiple arguments are useful\nfor settling the argument roles, such information is largely ignored by\nexisting approaches. This paper presents a better approach for event extraction\nby explicitly utilizing the relationships of event arguments. We achieve this\nthrough a carefully designed task-oriented dialogue system. To model the\nargument relation, we employ reinforcement learning and incremental learning to\nextract multiple arguments via a multi-turned, iterative process. Our approach\nleverages knowledge of the already extracted arguments of the same sentence to\ndetermine the role of arguments that would be difficult to decide individually.\nIt then uses the newly obtained information to improve the decisions of\npreviously extracted arguments. This two-way feedback process allows us to\nexploit the argument relations to effectively settle argument roles, leading to\nbetter sentence understanding and event extraction. Experimental results show\nthat our approach consistently outperforms seven state-of-the-art event\nextraction methods for the classification of events and argument role and\nargument identification.",
          "link": "http://arxiv.org/abs/2106.12384",
          "publishedOn": "2021-06-24T01:51:42.409Z",
          "wordCount": 645,
          "title": "Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations. (arXiv:2106.12384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dinglan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.",
          "link": "http://arxiv.org/abs/2106.12566",
          "publishedOn": "2021-06-24T01:51:42.400Z",
          "wordCount": 670,
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junxia Lin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ledolter_J/0/1/0/all/0/1\">Johannes Ledolter</a> (2) ((1) Georgetown University Medical Center, Georgetown University, (2) Tippie College of Business, University of Iowa)",
          "description": "The focus of our paper is the identification and correction of non-word\nerrors in OCR text. Such errors may be the result of incorrect insertion,\ndeletion, or substitution of a character, or the transposition of two adjacent\ncharacters within a single word. Or, it can be the result of word boundary\nproblems that lead to run-on errors and incorrect-split errors. The traditional\nN-gram correction methods can handle single-word errors effectively. However,\nthey show limitations when dealing with split and merge errors. In this paper,\nwe develop an unsupervised method that can handle both errors. The method we\ndevelop leads to a sizable improvement in the correction rates. This tutorial\npaper addresses very difficult word correction problems - namely incorrect\nrun-on and split errors - and illustrates what needs to be considered when\naddressing such problems. We outline a possible approach and assess its success\non a limited study.",
          "link": "http://arxiv.org/abs/2106.12030",
          "publishedOn": "2021-06-24T01:51:42.391Z",
          "wordCount": 603,
          "title": "A Simple and Practical Approach to Improve Misspellings in OCR Text. (arXiv:2106.12030v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1\">Umut &#xd6;zdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslan_B/0/1/0/all/0/1\">B&#xfc;&#x15f;ra Arslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1\">D. Emre Ta&#x15f;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1\">G&#xf6;k&#xe7;e Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1\">&#x15e;&#xfc;kr&#xfc; Ozan</a>",
          "description": "In this study, a natural language processing-based (NLP-based) method is\nproposed for the sector-wise automatic classification of ad texts created on\nonline advertising platforms. Our data set consists of approximately 21,000\nlabeled advertising texts from 12 different sectors. In the study, the\nBidirectional Encoder Representations from Transformers (BERT) model, which is\na transformer-based language model that is recently used in fields such as text\nclassification in the natural language processing literature, was used. The\nclassification efficiencies obtained using a pre-trained BERT model for the\nTurkish language are shown in detail.",
          "link": "http://arxiv.org/abs/2106.10899",
          "publishedOn": "2021-06-24T01:51:42.366Z",
          "wordCount": 554,
          "title": "Ad Text Classification with Transformer-Based Natural Language Processing Methods. (arXiv:2106.10899v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1\">Ting-hao</a> (Kenneth) <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>",
          "description": "Atomic clauses are fundamental text units for understanding complex\nsentences. Identifying the atomic sentences within complex sentences is\nimportant for applications such as summarization, argument mining, discourse\nanalysis, discourse parsing, and question answering. Previous work mainly\nrelies on rule-based methods dependent on parsing. We propose a new task to\ndecompose each complex sentence into simple sentences derived from the tensed\nclauses in the source, and a novel problem formulation as a graph edit task.\nOur neural model learns to Accept, Break, Copy or Drop elements of a graph that\ncombines word adjacency and grammatical dependencies. The full processing\npipeline includes modules for graph construction, graph editing, and sentence\ngeneration from the output graph. We introduce DeSSE, a new dataset designed to\ntrain and evaluate complex sentence decomposition, and MinWiki, a subset of\nMinWikiSplit. ABCD achieves comparable performance as two parsing baselines on\nMinWiki. On DeSSE, which has a more even balance of complex sentence types, our\nmodel achieves higher accuracy on the number of atomic sentences than an\nencoder-decoder baseline. Results include a detailed error analysis.",
          "link": "http://arxiv.org/abs/2106.12027",
          "publishedOn": "2021-06-24T01:51:42.350Z",
          "wordCount": 649,
          "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>",
          "description": "The growth rate in the amount of biomedical documents is staggering.\nUnlocking information trapped in these documents can enable researchers and\npractitioners to operate confidently in the information world. Biomedical NER,\nthe task of recognising biomedical names, is usually employed as the first step\nof the NLP pipeline. Standard NER models, based on sequence tagging technique,\nare good at recognising short entity mentions in the generic domain. However,\nthere are several open challenges of applying these models to recognise\nbiomedical names: 1) Biomedical names may contain complex inner structure\n(discontinuity and overlapping) which cannot be recognised using standard\nsequence tagging technique; 2) The training of NER models usually requires\nlarge amount of labelled data, which are difficult to obtain in the biomedical\ndomain; and, 3) Commonly used language representation models are pre-trained on\ngeneric data; a domain shift therefore exists between these models and target\nbiomedical data. To deal with these challenges, we explore several research\ndirections and make the following contributions: 1) we propose a\ntransition-based NER model which can recognise discontinuous mentions; 2) We\ndevelop a cost-effective approach that nominates the suitable pre-training\ndata; and, 3) We design several data augmentation methods for NER. Our\ncontributions have obvious practical implications, especially when new\nbiomedical applications are needed. Our proposed data augmentation methods can\nhelp the NER model achieve decent performance, requiring only a small amount of\nlabelled data. Our investigation regarding selecting pre-training data can\nimprove the model by incorporating language representation models, which are\npre-trained using in-domain data. Finally, our proposed transition-based NER\nmodel can further improve the performance by recognising discontinuous\nmentions.",
          "link": "http://arxiv.org/abs/2106.12230",
          "publishedOn": "2021-06-24T01:51:42.344Z",
          "wordCount": 693,
          "title": "Recognising Biomedical Names: Challenges and Solutions. (arXiv:2106.12230v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haider_T/0/1/0/all/0/1\">Thomas Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Evgeny Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menninghaus_W/0/1/0/all/0/1\">Winfried Menninghaus</a>",
          "description": "Most approaches to emotion analysis of social media, literature, news, and\nother domains focus exclusively on basic emotion categories as defined by Ekman\nor Plutchik. However, art (such as literature) enables engagement in a broader\nrange of more complex and subtle emotions. These have been shown to also\ninclude mixed emotional responses. We consider emotions in poetry as they are\nelicited in the reader, rather than what is expressed in the text or intended\nby the author. Thus, we conceptualize a set of aesthetic emotions that are\npredictive of aesthetic appreciation in the reader, and allow the annotation of\nmultiple labels per line to capture mixed emotions within their context. We\nevaluate this novel setting in an annotation experiment both with carefully\ntrained experts and via crowdsourcing. Our annotation with experts leads to an\nacceptable agreement of kappa = .70, resulting in a consistent dataset for\nfuture large scale analysis. Finally, we conduct first emotion classification\nexperiments based on BERT, showing that identifying aesthetic emotions is\nchallenging in our data, with up to .52 F1-micro on the German subset. Data and\nresources are available at https://github.com/tnhaider/poetry-emotion",
          "link": "http://arxiv.org/abs/2003.07723",
          "publishedOn": "2021-06-24T01:51:42.337Z",
          "wordCount": 692,
          "title": "PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry. (arXiv:2003.07723v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>",
          "description": "A growing effort in NLP aims to build datasets of human explanations.\nHowever, the term explanation encompasses a broad range of notions, each with\ndifferent properties and ramifications. Our goal is to provide an overview of\ndiverse types of explanations and human limitations, and discuss implications\nfor collecting and using explanations in NLP. Inspired by prior work in\npsychology and cognitive sciences, we group existing human explanations in NLP\ninto three categories: proximal mechanism, evidence, and procedure. These three\ntypes differ in nature and have implications for the resultant explanations.\nFor instance, procedure is not considered explanations in psychology and\nconnects with a rich body of work on learning from instructions. The diversity\nof explanations is further evidenced by proxy questions that are needed for\nannotators to interpret and answer open-ended why questions. Finally,\nexplanations may require different, often deeper, understandings than\npredictions, which casts doubt on whether humans can provide useful\nexplanations in some tasks.",
          "link": "http://arxiv.org/abs/2106.11988",
          "publishedOn": "2021-06-24T01:51:42.329Z",
          "wordCount": 595,
          "title": "On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1\">Mana Ihori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1\">Naoki Makishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1\">Akihiko Takashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1\">Shota Orihashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1\">Ryo Masumura</a>",
          "description": "In this paper, we propose a novel spoken-text-style conversion method that\ncan simultaneously execute multiple style conversion modules such as\npunctuation restoration and disfluency deletion without preparing matched\ndatasets. In practice, transcriptions generated by automatic speech recognition\nsystems are not highly readable because they often include many disfluencies\nand do not include punctuation marks. To improve their readability, multiple\nspoken-text-style conversion modules that individually model a single\nconversion task are cascaded because matched datasets that simultaneously\nhandle multiple conversion tasks are often unavailable. However, the cascading\nis unstable against the order of tasks because of the chain of conversion\nerrors. Besides, the computation cost of the cascading must be higher than the\nsingle conversion. To execute multiple conversion tasks simultaneously without\npreparing matched datasets, our key idea is to distinguish individual\nconversion tasks using the on-off switch. In our proposed zero-shot joint\nmodeling, we switch the individual tasks using multiple switching tokens,\nenabling us to utilize a zero-shot learning approach to executing simultaneous\nconversions. Our experiments on joint modeling of disfluency deletion and\npunctuation restoration demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.12131",
          "publishedOn": "2021-06-24T01:51:42.304Z",
          "wordCount": 628,
          "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks using Switching Tokens. (arXiv:2106.12131v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14152",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiujia Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.",
          "link": "http://arxiv.org/abs/2103.14152",
          "publishedOn": "2021-06-24T01:51:42.294Z",
          "wordCount": 629,
          "title": "Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:42.283Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.",
          "link": "http://arxiv.org/abs/2106.06600",
          "publishedOn": "2021-06-23T01:48:38.902Z",
          "wordCount": 693,
          "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1\">Stuart Shieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>",
          "description": "Targeted syntactic evaluations have demonstrated the ability of language\nmodels to perform subject-verb agreement given difficult contexts. To elucidate\nthe mechanisms by which the models accomplish this behavior, this study applies\ncausal mediation analysis to pre-trained neural language models. We investigate\nthe magnitude of models' preferences for grammatical inflections, as well as\nwhether neurons process subject-verb agreement similarly across sentences with\ndifferent syntactic structures. We uncover similarities and differences across\narchitectures and model sizes -- notably, that larger models do not necessarily\nlearn stronger preferences. We also observe two distinct mechanisms for\nproducing subject-verb agreement depending on the syntactic structure of the\ninput sentence. Finally, we find that language models rely on similar sets of\nneurons when given sentences with similar syntactic structure.",
          "link": "http://arxiv.org/abs/2106.06087",
          "publishedOn": "2021-06-23T01:48:38.764Z",
          "wordCount": 600,
          "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "Now, the pre-training technique is ubiquitous in natural language processing\nfield. ProphetNet is a pre-training based natural language generation method\nwhich shows powerful performance on English text summarization and question\ngeneration tasks. In this paper, we extend ProphetNet into other domains and\nlanguages, and present the ProphetNet family pre-training models, named\nProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We\npre-train a cross-lingual generation model ProphetNet-Multi, a Chinese\ngeneration model ProphetNet-Zh, two open-domain dialog generation models\nProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG\n(Programming Language Generation) model ProphetNet-Code to show the generation\nperformance besides NLG (Natural Language Generation) tasks. In our\nexperiments, ProphetNet-X models achieve new state-of-the-art performance on 10\nbenchmarks. All the models of ProphetNet-X share the same model structure,\nwhich allows users to easily switch between different models. We make the code\nand models publicly available, and we will keep updating more pre-training\nmodels and finetuning scripts.",
          "link": "http://arxiv.org/abs/2104.08006",
          "publishedOn": "2021-06-23T01:48:38.731Z",
          "wordCount": 648,
          "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation. (arXiv:2104.08006v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "This paper describes the system submitted to the IWSLT 2021 Multilingual\nSpeech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified\ntransformer architecture for our MultiST model, so that the data from different\nmodalities (i.e., speech and text) and different tasks (i.e., Speech\nRecognition, Machine Translation, and Speech Translation) can be exploited to\nenhance the model's ability. Specifically, speech and text inputs are firstly\nfed to different feature extractors to extract acoustic and textual features,\nrespectively. Then, these features are processed by a shared encoder--decoder\narchitecture. We apply several training techniques to improve the performance,\nincluding multi-task learning, task-level curriculum learning, data\naugmentation, etc. Our final system achieves significantly better results than\nbilingual baselines on supervised language pairs and yields reasonable results\non zero-shot language pairs.",
          "link": "http://arxiv.org/abs/2106.00197",
          "publishedOn": "2021-06-23T01:48:38.717Z",
          "wordCount": 599,
          "title": "Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021. (arXiv:2106.00197v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever ``understand'' raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of ``assertions'':\ntextual contexts that provide indirect clues about the underlying semantics. We\nstudy whether assertions enable a system to emulate representations preserving\nsemantic relations like equivalence. We find that assertions enable semantic\nemulation of languages that satisfy a strong notion of semantic transparency.\nHowever, for classes of languages where the same expression can take different\nvalues in different contexts, we show that emulation can become uncomputable.\nFinally, we discuss differences between our formal model and natural language,\nexploring how our results generalize to a modal setting and other semantic\nrelations. Together, our results suggest that assertions in code or language do\nnot provide sufficient signal to fully emulate semantic representations. We\nformalize ways in which ungrounded language models appear to be fundamentally\nlimited in their ability to ``understand''.",
          "link": "http://arxiv.org/abs/2104.10809",
          "publishedOn": "2021-06-23T01:48:38.643Z",
          "wordCount": 674,
          "title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?. (arXiv:2104.10809v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>",
          "description": "Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.",
          "link": "http://arxiv.org/abs/2106.05544",
          "publishedOn": "2021-06-23T01:48:38.636Z",
          "wordCount": 615,
          "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:38.597Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.",
          "link": "http://arxiv.org/abs/2104.08653",
          "publishedOn": "2021-06-23T01:48:38.589Z",
          "wordCount": 649,
          "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-23T01:48:38.581Z",
          "wordCount": 544,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhufeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.",
          "link": "http://arxiv.org/abs/2106.01033",
          "publishedOn": "2021-06-23T01:48:38.573Z",
          "wordCount": 698,
          "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1\">Ginevra Carbone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>",
          "description": "Plug-and-play language models (PPLMs) enable topic-conditioned natural\nlanguage generation by pairing large pre-trained generators with attribute\nmodels used to steer the predicted token distribution towards the selected\ntopic. Despite their computational efficiency, PPLMs require large amounts of\nlabeled texts to effectively balance generation fluency and proper\nconditioning, making them unsuitable for low-resource settings. We present\nETC-NLG, an approach leveraging topic modeling annotations to enable\nfully-unsupervised End-to-end Topic-Conditioned Natural Language Generation\nover emergent topics in unlabeled document collections. We first test the\neffectiveness of our approach in a low-resource setting for Italian, evaluating\nthe conditioning for both topic models and gold annotations. We then perform a\ncomparative evaluation of ETC-NLG for Italian and English using a parallel\ncorpus. Finally, we propose an automatic approach to estimate the effectiveness\nof conditioning on the generated utterances.",
          "link": "http://arxiv.org/abs/2008.10875",
          "publishedOn": "2021-06-23T01:48:38.422Z",
          "wordCount": 599,
          "title": "ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation. (arXiv:2008.10875v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md Mahfuz ibn Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>",
          "description": "As neural machine translation (NMT) systems become an important part of\nprofessional translator pipelines, a growing body of work focuses on combining\nNMT with terminologies. In many scenarios and particularly in cases of domain\nadaptation, one expects the MT output to adhere to the constraints provided by\na terminology. In this work, we propose metrics to measure the consistency of\nMT output with regards to a domain terminology. We perform studies on the\nCOVID-19 domain over 5 languages, also performing terminology-targeted human\nevaluation. We open-source the code for computing all proposed metrics:\nhttps://github.com/mahfuzibnalam/terminology_evaluation",
          "link": "http://arxiv.org/abs/2106.11891",
          "publishedOn": "2021-06-23T01:48:38.396Z",
          "wordCount": 584,
          "title": "On the Evaluation of Machine Translation for Terminology Consistency. (arXiv:2106.11891v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Most of the existing works for dialogue generation are data-driven models\ntrained directly on corpora crawled from websites. They mainly focus on\nimproving the model architecture to produce better responses but pay little\nattention to considering the quality of the training data contrastively. In\nthis paper, we propose a multi-level contrastive learning paradigm to model the\nfine-grained quality of the responses with respect to the query. A Rank-aware\nCalibration (RC) network is designed to construct the multi-level contrastive\noptimization objectives. Since these objectives are calculated based on the\nsentence level, which may erroneously encourage/suppress the generation of\nuninformative/informative words. To tackle this incidental issue, on one hand,\nwe design an exquisite token-level strategy for estimating the instance loss\nmore accurately. On the other hand, we build a Knowledge Inference (KI)\ncomponent to capture the keyword knowledge from the reference during training\nand exploit such information to encourage the generation of informative words.\nWe evaluate the proposed model on a carefully annotated dialogue dataset and\nthe results suggest that our model can generate more relevant and diverse\nresponses compared to the baseline models.",
          "link": "http://arxiv.org/abs/2009.09147",
          "publishedOn": "2021-06-23T01:48:38.347Z",
          "wordCount": 644,
          "title": "Enhancing Dialogue Generation via Multi-Level Contrastive Learning. (arXiv:2009.09147v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Current task-oriented dialog (TOD) systems mostly manage structured knowledge\n(e.g. databases and tables) to guide the goal-oriented conversations. However,\nthey fall short of handling dialogs which also involve unstructured knowledge\n(e.g. reviews and documents). In this paper, we formulate a task of modeling\nTOD grounded on a fusion of structured and unstructured knowledge. To address\nthis task, we propose a TOD system with semi-structured knowledge management,\nSeKnow, which extends the belief state to manage knowledge with both structured\nand unstructured contents. Furthermore, we introduce two implementations of\nSeKnow based on a non-pretrained sequence-to-sequence model and a pretrained\nlanguage model, respectively. Both implementations use the end-to-end manner to\njointly optimize dialog modeling grounded on structured and unstructured\nknowledge. We conduct experiments on the modified version of MultiWOZ 2.1\ndataset, where dialogs are processed to involve semi-structured knowledge.\nExperimental results show that SeKnow has strong performances in both\nend-to-end dialog and intermediate knowledge management, compared to existing\nTOD systems and their extensions with pipeline knowledge management schemes.",
          "link": "http://arxiv.org/abs/2106.11796",
          "publishedOn": "2021-06-23T01:48:38.337Z",
          "wordCount": 610,
          "title": "End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yi-Ling Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>",
          "description": "Tackling online hatred using informed textual responses - called counter\nnarratives - has been brought under the spotlight recently. Accordingly, a\nresearch line has emerged to automatically generate counter narratives in order\nto facilitate the direct intervention in the hate discussion and to prevent\nhate content from further spreading. Still, current neural approaches tend to\nproduce generic/repetitive responses and lack grounded and up-to-date evidence\nsuch as facts, statistics, or examples. Moreover, these models can create\nplausible but not necessarily true arguments. In this paper we present the\nfirst complete knowledge-bound counter narrative generation pipeline, grounded\nin an external knowledge repository that can provide more informative content\nto fight online hatred. Together with our approach, we present a series of\nexperiments that show its feasibility to produce suitable and informative\ncounter narratives in in-domain and cross-domain settings.",
          "link": "http://arxiv.org/abs/2106.11783",
          "publishedOn": "2021-06-23T01:48:38.327Z",
          "wordCount": 590,
          "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech. (arXiv:2106.11783v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1\">Priyam Tejaswin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1\">Dhruv Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.",
          "link": "http://arxiv.org/abs/2106.11388",
          "publishedOn": "2021-06-23T01:48:38.283Z",
          "wordCount": 598,
          "title": "How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Staniek_M/0/1/0/all/0/1\">Michael Staniek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>",
          "description": "In semantic parsing of geographical queries against real-world databases such\nas OpenStreetMap (OSM), unique correct answers do not necessarily exist.\nInstead, the truth might be lying in the eye of the user, who needs to enter an\ninteractive setup where ambiguities can be resolved and parsing mistakes can be\ncorrected. Our work presents an approach to interactive semantic parsing where\nan explicit error detection is performed, and a clarification question is\ngenerated that pinpoints the suspected source of ambiguity or error and\ncommunicates it to the human user. Our experimental results show that a\ncombination of entropy-based uncertainty detection and beam search, together\nwith multi-source training on clarification question, initial parse, and user\nanswer, results in improvements of 1.2% F1 score on a parser that already\nperforms at 90.26% on the NLMaps dataset for OSM semantic parsing.",
          "link": "http://arxiv.org/abs/2106.11739",
          "publishedOn": "2021-06-23T01:48:38.270Z",
          "wordCount": 566,
          "title": "Error-Aware Interactive Semantic Parsing of OpenStreetMap. (arXiv:2106.11739v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Inspired by evidence that pretrained language models (LMs) encode commonsense\nknowledge, recent work has applied LMs to automatically populate commonsense\nknowledge graphs (CKGs). However, there is a lack of understanding on their\ngeneralization to multiple CKGs, unseen relations, and novel entities. This\npaper analyzes the ability of LMs to perform generalizable commonsense\ninference, in terms of knowledge capacity, transferability, and induction. Our\nexperiments with these three aspects show that: (1) LMs can adapt to different\nschemas defined by multiple CKGs but fail to reuse the knowledge to generalize\nto new relations. (2) Adapted LMs generalize well to unseen subjects, but less\nso on novel objects. Future work should investigate how to improve the\ntransferability and induction of commonsense mining from LMs.",
          "link": "http://arxiv.org/abs/2106.11533",
          "publishedOn": "2021-06-23T01:48:38.262Z",
          "wordCount": 564,
          "title": "Do Language Models Perform Generalizable Commonsense Inference?. (arXiv:2106.11533v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Distant supervision for relation extraction provides uniform bag labels for\neach sentence inside the bag, while accurate sentence labels are important for\ndownstream applications that need the exact relation type. Directly using bag\nlabels for sentence-level training will introduce much noise, thus severely\ndegrading performance. In this work, we propose the use of negative training\n(NT), in which a model is trained using complementary labels regarding that\n``the instance does not belong to these complementary labels\". Since the\nprobability of selecting a true label as a complementary label is low, NT\nprovides less noisy information. Furthermore, the model trained with NT is able\nto separate the noisy data from the training data. Based on NT, we propose a\nsentence-level framework, SENT, for distant relation extraction. SENT not only\nfilters the noisy data to construct a cleaner dataset, but also performs a\nre-labeling process to transform the noisy data into useful training data, thus\nfurther benefiting the model's performance. Experimental results show the\nsignificant improvement of the proposed method over previous methods on\nsentence-level evaluation and de-noise effect.",
          "link": "http://arxiv.org/abs/2106.11566",
          "publishedOn": "2021-06-23T01:48:38.253Z",
          "wordCount": 618,
          "title": "SENT: Sentence-level Distant Relation Extraction via Negative Training. (arXiv:2106.11566v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor the transformer-encoder layers.",
          "link": "http://arxiv.org/abs/2106.11483",
          "publishedOn": "2021-06-23T01:48:38.245Z",
          "wordCount": 501,
          "title": "A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "The majority of existing methods for empathetic response generation rely on\nthe emotion of the context to generate empathetic responses. However, empathy\nis much more than generating responses with an appropriate emotion. It also\noften entails subtle expressions of understanding and personal resonance with\nthe situation of the other interlocutor. Unfortunately, such qualities are\ndifficult to quantify and the datasets lack the relevant annotations. To\naddress this issue, in this paper we propose an approach that relies on\nexemplars to cue the generative model on fine stylistic properties that signal\nempathy to the interlocutor. To this end, we employ dense passage retrieval to\nextract relevant exemplary responses from the training set. Three elements of\nhuman communication -- emotional presence, interpretation, and exploration, and\nsentiment are additionally introduced using synthetic labels to guide the\ngeneration towards empathy. The human evaluation is also extended by these\nelements of human communication. We empirically show that these approaches\nyield significant improvements in empathetic response quality in terms of both\nautomated and human-evaluated metrics. The implementation is available at\nhttps://github.com/declare-lab/exemplary-empathy.",
          "link": "http://arxiv.org/abs/2106.11791",
          "publishedOn": "2021-06-23T01:48:38.223Z",
          "wordCount": 622,
          "title": "Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>",
          "description": "Routing methods in capsule networks often learn a hierarchical relationship\nfor capsules in successive layers, but the intra-relation between capsules in\nthe same layer is less studied, while this intra-relation is a key factor for\nthe semantic understanding in text data. Therefore, in this paper, we introduce\na new capsule network with graph routing to learn both relationships, where\ncapsules in each layer are treated as the nodes of a graph. We investigate\nstrategies to yield adjacency and degree matrix with three different distances\nfrom a layer of capsules, and propose the graph routing mechanism between those\ncapsules. We validate our approach on five text classification datasets, and\nour findings suggest that the approach combining bottom-up routing and top-down\nattention performs the best. Such an approach demonstrates generalization\ncapability across datasets. Compared to the state-of-the-art routing methods,\nthe improvements in accuracy in the five datasets we used were 0.82, 0.39,\n0.07, 1.01, and 0.02, respectively.",
          "link": "http://arxiv.org/abs/2106.11531",
          "publishedOn": "2021-06-23T01:48:38.211Z",
          "wordCount": 592,
          "title": "Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1\">Matthew Richardson</a>",
          "description": "The goal of database question answering is to enable natural language\nquerying of real-life relational databases in diverse application domains.\nRecently, large-scale datasets such as Spider and WikiSQL facilitated novel\nmodeling techniques for text-to-SQL parsing, improving zero-shot generalization\nto unseen databases. In this work, we examine the challenges that still prevent\nthese techniques from practical deployment. First, we present KaggleDBQA, a new\ncross-domain evaluation dataset of real Web databases, with domain-specific\ndata types, original formatting, and unrestricted questions. Second, we\nre-examine the choice of evaluation tasks for text-to-SQL parsers as applied in\nreal-life settings. Finally, we augment our in-domain evaluation task with\ndatabase documentation, a naturally occurring source of implicit domain\nknowledge. We show that KaggleDBQA presents a challenge to state-of-the-art\nzero-shot parsers but a more realistic evaluation setting and creative use of\nassociated database documentation boosts their accuracy by over 13.2%, doubling\ntheir performance.",
          "link": "http://arxiv.org/abs/2106.11455",
          "publishedOn": "2021-06-23T01:48:38.072Z",
          "wordCount": 591,
          "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers. (arXiv:2106.11455v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\nthis http URL on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.",
          "link": "http://arxiv.org/abs/2106.11520",
          "publishedOn": "2021-06-23T01:48:38.063Z",
          "wordCount": 638,
          "title": "BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Modern pre-trained language models are mostly built upon backbones stacking\nself-attention and feed-forward layers in an interleaved order. In this paper,\nbeyond this stereotyped layer pattern, we aim to improve pre-trained models by\nexploiting layer variety from two aspects: the layer type set and the layer\norder. Specifically, besides the original self-attention and feed-forward\nlayers, we introduce convolution into the layer type set, which is\nexperimentally found beneficial to pre-trained models. Furthermore, beyond the\noriginal interleaved order, we explore more layer orders to discover more\npowerful architectures. However, the introduced layer variety leads to a large\narchitecture space of more than billions of candidates, while training a single\ncandidate model from scratch already requires huge computation cost, making it\nnot affordable to search such a space by directly training large amounts of\ncandidate models. To solve this problem, we first pre-train a supernet from\nwhich the weights of all candidate models can be inherited, and then adopt an\nevolutionary algorithm guided by pre-training accuracy to find the optimal\narchitecture. Extensive experiments show that LV-BERT model obtained by our\nmethod outperforms BERT and its variants on various downstream tasks. For\nexample, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than\nthe strong baseline ELECTRA-small.",
          "link": "http://arxiv.org/abs/2106.11740",
          "publishedOn": "2021-06-23T01:48:38.047Z",
          "wordCount": 661,
          "title": "LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:38.037Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:38.013Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "Cross-lingual adaptation with multilingual pre-trained language models\n(mPTLMs) mainly consists of two lines of works: zero-shot approach and\ntranslation-based approach, which have been studied extensively on the\nsequence-level tasks. We further verify the efficacy of these cross-lingual\nadaptation approaches by evaluating their performances on more fine-grained\nsequence tagging tasks. After re-examining their strengths and drawbacks, we\npropose a novel framework to consolidate the zero-shot approach and the\ntranslation-based approach for better adaptation performance. Instead of simply\naugmenting the source data with the machine-translated data, we tailor-make a\nwarm-up mechanism to quickly update the mPTLMs with the gradients estimated on\na few translated data. Then, the adaptation approach is applied to the refined\nparameters and the cross-lingual transfer is performed in a warm-start way. The\nexperimental results on nine target languages demonstrate that our method is\nbeneficial to the cross-lingual adaptation of various sequence tagging tasks.",
          "link": "http://arxiv.org/abs/2010.12405",
          "publishedOn": "2021-06-23T01:48:38.000Z",
          "wordCount": 617,
          "title": "Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond. (arXiv:2010.12405v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:37.985Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1\">Shamane Siriwardhana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1\">Rivindu Weerasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Elliott Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1\">Suranga Nanayakkara</a>",
          "description": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.",
          "link": "http://arxiv.org/abs/2106.11517",
          "publishedOn": "2021-06-23T01:48:37.975Z",
          "wordCount": 509,
          "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1\">Iurii Mokrii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>",
          "description": "Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.",
          "link": "http://arxiv.org/abs/2103.03335",
          "publishedOn": "2021-06-23T01:48:37.967Z",
          "wordCount": 653,
          "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1\">Zeerak Waseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.",
          "link": "http://arxiv.org/abs/2106.11410",
          "publishedOn": "2021-06-23T01:48:37.958Z",
          "wordCount": 590,
          "title": "A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.",
          "link": "http://arxiv.org/abs/2106.11403",
          "publishedOn": "2021-06-23T01:48:37.932Z",
          "wordCount": 691,
          "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1\">Melissa Chase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1\">Esha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1\">Marcello Hasegawa</a>",
          "description": "In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n\nIn this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.",
          "link": "http://arxiv.org/abs/2106.11384",
          "publishedOn": "2021-06-23T01:48:37.921Z",
          "wordCount": 640,
          "title": "Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Neural machine translation (NMT) is sensitive to domain shift. In this paper,\nwe address this problem in an active learning setting where we can spend a\ngiven budget on translating in-domain data, and gradually fine-tune a\npre-trained out-of-domain NMT model on the newly translated data. Existing\nactive learning methods for NMT usually select sentences based on uncertainty\nscores, but these methods require costly translation of full sentences even\nwhen only one or two key phrases within the sentence are informative. To\naddress this limitation, we re-examine previous work from the phrase-based\nmachine translation (PBMT) era that selected not full sentences, but rather\nindividual phrases. However, while incorporating these phrases into PBMT\nsystems was relatively simple, it is less trivial for NMT systems, which need\nto be trained on full sequences to capture larger structural properties of\nsentences unique to the new domain. To overcome these hurdles, we propose to\nselect both full sentences and individual phrases from unlabelled data in the\nnew domain for routing to human translators. In a German-English translation\ntask, our active learning approach achieves consistent improvements over\nuncertainty-based sentence selection methods, improving up to 1.2 BLEU score\nover strong active learning baselines.",
          "link": "http://arxiv.org/abs/2106.11375",
          "publishedOn": "2021-06-23T01:48:37.904Z",
          "wordCount": 626,
          "title": "Phrase-level Active Learning for Neural Machine Translation. (arXiv:2106.11375v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>",
          "description": "One of the main challenges in conversational question answering (CQA) is to\nresolve the conversational dependency, such as anaphora and ellipsis. However,\nexisting approaches do not explicitly train QA models on how to resolve the\ndependency, and thus these models are limited in understanding human dialogues.\nIn this paper, we propose a novel framework, ExCorD (Explicit guidance on how\nto resolve Conversational Dependency) to enhance the abilities of QA models in\ncomprehending conversational context. ExCorD first generates self-contained\nquestions that can be understood without the conversation history, then trains\na QA model with the pairs of original and self-contained questions using a\nconsistency-based regularizer. In our experiments, we demonstrate that ExCorD\nsignificantly improves the QA models' performance by up to 1.2 F1 on QuAC, and\n5.2 F1 on CANARD, while addressing the limitations of the existing approaches.",
          "link": "http://arxiv.org/abs/2106.11575",
          "publishedOn": "2021-06-23T01:48:37.891Z",
          "wordCount": 588,
          "title": "Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering. (arXiv:2106.11575v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-06-29T01:55:13.421Z",
          "wordCount": 667,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianxin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1\">Yiqun Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yanan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Sequential recommendation aims to leverage users' historical behaviors to\npredict their next interaction. Existing works have not yet addressed two main\nchallenges in sequential recommendation. First, user behaviors in their rich\nhistorical sequences are often implicit and noisy preference signals, they\ncannot sufficiently reflect users' actual preferences. In addition, users'\ndynamic preferences often change rapidly over time, and hence it is difficult\nto capture user patterns in their historical sequences. In this work, we\npropose a graph neural network model called SURGE (short for SeqUential\nRecommendation with Graph neural nEtworks) to address these two issues.\nSpecifically, SURGE integrates different types of preferences in long-term user\nbehaviors into clusters in the graph by re-constructing loose item sequences\ninto tight item-item interest graphs based on metric learning. This helps\nexplicitly distinguish users' core interests, by forming dense clusters in the\ninterest graph. Then, we perform cluster-aware and query-aware graph\nconvolutional propagation and graph pooling on the constructed graph. It\ndynamically fuses and extracts users' current activated core interests from\nnoisy user behavior sequences. We conduct extensive experiments on both public\nand proprietary industrial datasets. Experimental results demonstrate\nsignificant performance gains of our proposed method compared to\nstate-of-the-art methods. Further studies on sequence length confirm that our\nmethod can model long behavioral sequences effectively and efficiently.",
          "link": "http://arxiv.org/abs/2106.14226",
          "publishedOn": "2021-06-29T01:55:13.218Z",
          "wordCount": 652,
          "title": "Sequential Recommendation with Graph Neural Networks. (arXiv:2106.14226v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:13.161Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:13.150Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:13.143Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.121Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.112Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:13.104Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yile Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyong Peng</a>",
          "description": "One key property in recommender systems is the long-tail distribution in\nuser-item interactions where most items only have few user feedback. Improving\nthe recommendation of tail items can promote novelty and bring positive effects\nto both users and providers, and thus is a desirable property of recommender\nsystems. Current novel recommendation studies over-emphasize the importance of\ntail items without differentiating the degree of users' intent on popularity\nand often incur a sharp decline of accuracy. Moreover, none of existing methods\nhas ever taken the extreme case of tail items, i.e., cold-start items without\nany interaction, into consideration.\n\nIn this work, we first disclose the mechanism that drives a user's\ninteraction towards popular or niche items by disentangling her intent into\nconformity influence (popularity) and personal interests (preference). We then\npresent a unified end-to-end framework to simultaneously optimize accuracy and\nnovelty targets based on the disentangled intent of popularity and that of\npreference. We further develop a new paradigm for novel recommendation of\ncold-start items which exploits the self-supervised learning technique to model\nthe correlation between collaborative features and content features. We conduct\nextensive experimental results on three real-world datasets. The results\ndemonstrate that our proposed model yields significant improvements over the\nstate-of-the-art baselines in terms of accuracy, novelty, coverage, and\ntrade-off.",
          "link": "http://arxiv.org/abs/2106.14388",
          "publishedOn": "2021-06-29T01:55:13.094Z",
          "wordCount": 644,
          "title": "Intent Disentanglement and Feature Self-supervision for Novel Recommendation. (arXiv:2106.14388v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1\">Ludovico Boratto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1\">Gianni Fenu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1\">Mirko Marras</a>",
          "description": "Considering the impact of recommendations on item providers is one of the\nduties of multi-sided recommender systems. Item providers are key stakeholders\nin online platforms, and their earnings and plans are influenced by the\nexposure their items receive in recommended lists. Prior work showed that\ncertain minority groups of providers, characterized by a common sensitive\nattribute (e.g., gender or race), are being disproportionately affected by\nindirect and unintentional discrimination. Our study in this paper handles a\nsituation where ($i$) the same provider is associated with multiple items of a\nlist suggested to a user, ($ii$) an item is created by more than one provider\njointly, and ($iii$) predicted user-item relevance scores are biasedly\nestimated for items of provider groups. Under this scenario, we assess\ndisparities in relevance, visibility, and exposure, by simulating diverse\nrepresentations of the minority group in the catalog and the interactions.\nBased on emerged unfair outcomes, we devise a treatment that combines\nobservation upsampling and loss regularization, while learning user-item\nrelevance scores. Experiments on real-world data demonstrate that our treatment\nleads to lower disparate relevance. The resulting recommended lists show fairer\nvisibility and exposure, higher minority item coverage, and negligible loss in\nrecommendation utility.",
          "link": "http://arxiv.org/abs/2006.04279",
          "publishedOn": "2021-06-29T01:55:13.076Z",
          "wordCount": 683,
          "title": "Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems. (arXiv:2006.04279v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1\">Mykola Makhortykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1\">Aleksandra Urman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1\">Roberto Ulloa</a>",
          "description": "Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.",
          "link": "http://arxiv.org/abs/2106.14072",
          "publishedOn": "2021-06-29T01:55:13.044Z",
          "wordCount": 627,
          "title": "Detecting race and gender bias in visual representation of AI on web search engines. (arXiv:2106.14072v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.015Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weixue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yukun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daiting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>",
          "description": "Retrieval is a crucial stage in web search that identifies a small set of\nquery-relevant candidates from a billion-scale corpus. Discovering more\nsemantically-related candidates in the retrieval stage is very promising to\nexpose more high-quality results to the end users. However, it still remains\nnon-trivial challenges of building and deploying effective retrieval models for\nsemantic matching in real search engine. In this paper, we describe the\nretrieval system that we developed and deployed in Baidu Search. The system\nexploits the recent state-of-the-art Chinese pretrained language model, namely\nEnhanced Representation through kNowledge IntEgration (ERNIE), which\nfacilitates the system with expressive semantic matching. In particular, we\ndeveloped an ERNIE-based retrieval model, which is equipped with 1) expressive\nTransformer-based semantic encoders, and 2) a comprehensive multi-stage\ntraining paradigm. More importantly, we present a practical system workflow for\ndeploying the model in web-scale retrieval. Eventually, the system is fully\ndeployed into production, where rigorous offline and online experiments were\nconducted. The results show that the system can perform high-quality candidate\nretrieval, especially for those tail queries with uncommon demands. Overall,\nthe new retrieval system facilitated by pretrained language model (i.e., ERNIE)\ncan largely improve the usability and applicability of our search engine.",
          "link": "http://arxiv.org/abs/2106.03373",
          "publishedOn": "2021-06-28T01:57:53.900Z",
          "wordCount": 664,
          "title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search. (arXiv:2106.03373v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1\">jinhong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwuk Lee</a>",
          "description": "Session-based recommendation aims at predicting the next item given a\nsequence of previous items consumed in the session, e.g., on e-commerce or\nmultimedia streaming services. Specifically, session data exhibits some unique\ncharacteristics, i.e., session consistency and sequential dependency over items\nwithin the session, repeated item consumption, and session timeliness. In this\npaper, we propose simple-yet-effective linear models for considering the\nholistic aspects of the sessions. The comprehensive nature of our models helps\nimprove the quality of session-based recommendation. More importantly, it\nprovides a generalized framework for reflecting different perspectives of\nsession data. Furthermore, since our models can be solved by closed-form\nsolutions, they are highly scalable. Experimental results demonstrate that the\nproposed linear models show competitive or state-of-the-art performance in\nvarious metrics on several real-world datasets.",
          "link": "http://arxiv.org/abs/2103.16104",
          "publishedOn": "2021-06-28T01:57:53.803Z",
          "wordCount": 599,
          "title": "Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1\">Hrishikesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alicea_B/0/1/0/all/0/1\">Bradly Alicea</a>",
          "description": "Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nthis http URL with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.",
          "link": "http://arxiv.org/abs/2106.13767",
          "publishedOn": "2021-06-28T01:57:53.746Z",
          "wordCount": 654,
          "title": "Sentiment Progression based Searching and Indexing of Literary Textual Artefacts. (arXiv:2106.13767v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lesota_O/0/1/0/all/0/1\">Oleg Lesota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Daniel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasserbauer_K/0/1/0/all/0/1\">Klaus Antonius Grasserbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>",
          "description": "Existing neural ranking models follow the text matching paradigm, where\ndocument-to-query relevance is estimated through predicting the matching score.\nDrawing from the rich literature of classical generative retrieval models, we\nintroduce and formalize the paradigm of deep generative retrieval models\ndefined via the cumulative probabilities of generating query terms. This\nparadigm offers a grounded probabilistic view on relevance estimation while\nstill enabling the use of modern neural architectures. In contrast to the\nmatching paradigm, the probabilistic nature of generative rankers readily\noffers a fine-grained measure of uncertainty. We adopt several current neural\ngenerative models in our framework and introduce a novel generative ranker\n(T-PGN), which combines the encoding capacity of Transformers with the Pointer\nGenerator Network model. We conduct an extensive set of evaluation experiments\non passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking collections. Our results show the\nsignificantly higher performance of the T-PGN model when compared with other\ngenerative models. Lastly, we demonstrate that exploiting the uncertainty\ninformation of deep generative rankers opens new perspectives to\nquery/collection understanding, and significantly improves the cut-off\nprediction task.",
          "link": "http://arxiv.org/abs/2106.13618",
          "publishedOn": "2021-06-28T01:57:53.674Z",
          "wordCount": 629,
          "title": "A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models. (arXiv:2106.13618v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_K/0/1/0/all/0/1\">Kishlay Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>",
          "description": "Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13711",
          "publishedOn": "2021-06-28T01:57:53.608Z",
          "wordCount": 668,
          "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks. (arXiv:2106.13711v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lixin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dehong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Suqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daiting Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhifan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weiyue Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhicong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>",
          "description": "As the heart of a search engine, the ranking system plays a crucial role in\nsatisfying users' information demands. More recently, neural rankers fine-tuned\nfrom pre-trained language models (PLMs) establish state-of-the-art ranking\neffectiveness. However, it is nontrivial to directly apply these PLM-based\nrankers to the large-scale web search system due to the following challenging\nissues:(1) the prohibitively expensive computations of massive neural PLMs,\nespecially for long texts in the web-document, prohibit their deployments in an\nonline ranking system that demands extremely low latency;(2) the discrepancy\nbetween existing ranking-agnostic pre-training objectives and the ad-hoc\nretrieval scenarios that demand comprehensive relevance modeling is another\nmain barrier for improving the online ranking system;(3) a real-world search\nengine typically involves a committee of ranking components, and thus the\ncompatibility of the individually fine-tuned ranking model is critical for a\ncooperative ranking system. In this work, we contribute a series of\nsuccessfully applied techniques in tackling these exposed issues when deploying\nthe state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the\nonline search engine system. We first articulate a novel practice to\ncost-efficiently summarize the web document and contextualize the resultant\nsummary content with the query using a cheap yet powerful Pyramid-ERNIE\narchitecture. Then we endow an innovative paradigm to finely exploit the\nlarge-scale noisy and biased post-click behavioral data for relevance-oriented\npre-training. We also propose a human-anchored fine-tuning strategy tailored\nfor the online ranking system, aiming to stabilize the ranking signals across\nvarious online components. Extensive offline and online experimental results\nshow that the proposed techniques significantly boost the search engine's\nperformance.",
          "link": "http://arxiv.org/abs/2105.11108",
          "publishedOn": "2021-06-28T01:57:53.587Z",
          "wordCount": 751,
          "title": "Pre-trained Language Model based Ranking in Baidu Search. (arXiv:2105.11108v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Ben Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>",
          "description": "Fairness in recommendation has attracted increasing attention due to bias and\ndiscrimination possibly caused by traditional recommenders. In Interactive\nRecommender Systems (IRS), user preferences and the system's fairness status\nare constantly changing over time. Existing fairness-aware recommenders mainly\nconsider fairness in static settings. Directly applying existing methods to IRS\nwill result in poor recommendation. To resolve this problem, we propose a\nreinforcement learning based framework, FairRec, to dynamically maintain a\nlong-term balance between accuracy and fairness in IRS. User preferences and\nthe system's fairness status are jointly compressed into the state\nrepresentation to generate recommendations. FairRec aims at maximizing our\ndesigned cumulative reward that combines accuracy and fairness. Extensive\nexperiments validate that FairRec can improve fairness, while preserving good\nrecommendation quality.",
          "link": "http://arxiv.org/abs/2106.13386",
          "publishedOn": "2021-06-28T01:57:53.503Z",
          "wordCount": 564,
          "title": "Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning. (arXiv:2106.13386v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Russell_Rose_T/0/1/0/all/0/1\">Tony Russell-Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gooch_P/0/1/0/all/0/1\">Philip Gooch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>",
          "description": "Knowledge workers (such as healthcare information professionals, patent\nagents and recruitment professionals) undertake work tasks where search forms a\ncore part of their duties. In these instances, the search task is often complex\nand time-consuming and requires specialist expert knowledge to formulate\naccurate search strategies. Interactive features such as query expansion can\nplay a key role in supporting these tasks. However, generating query\nsuggestions within a professional search context requires that consideration be\ngiven to the specialist, structured nature of the search strategies they\nemploy. In this paper, we investigate a variety of query expansion methods\napplied to a collection of Boolean search strategies used in a variety of\nreal-world professional search tasks. The results demonstrate the utility of\ncontext-free distributional language models and the value of using linguistic\ncues such as ngram order to optimise the balance between precision and recall.",
          "link": "http://arxiv.org/abs/2106.13528",
          "publishedOn": "2021-06-28T01:57:53.495Z",
          "wordCount": 576,
          "title": "Interactive query expansion for professional search applications. (arXiv:2106.13528v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>",
          "description": "The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.",
          "link": "http://arxiv.org/abs/2106.13732",
          "publishedOn": "2021-06-28T01:57:53.232Z",
          "wordCount": 695,
          "title": "Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shijie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhouyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Spreadsheet table detection is the task of detecting all tables on a given\nsheet and locating their respective ranges. Automatic table detection is a key\nenabling technique and an initial step in spreadsheet data intelligence.\nHowever, the detection task is challenged by the diversity of table structures\nand table layouts on the spreadsheet. Considering the analogy between a cell\nmatrix as spreadsheet and a pixel matrix as image, and encouraged by the\nsuccessful application of Convolutional Neural Networks (CNN) in computer\nvision, we have developed TableSense, a novel end-to-end framework for\nspreadsheet table detection. First, we devise an effective cell featurization\nscheme to better leverage the rich information in each cell; second, we develop\nan enhanced convolutional neural network model for table detection to meet the\ndomain-specific requirement on precise table boundary detection; third, we\npropose an effective uncertainty metric to guide an active learning based smart\nsampling algorithm, which enables the efficient build-up of a training dataset\nwith 22,176 tables on 10,220 sheets with broad coverage of diverse table\nstructures and layouts. Our evaluation shows that TableSense is highly\neffective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a\nsignificant improvement over both the current detection algorithm that are used\nin commodity spreadsheet tools and state-of-the-art convolutional neural\nnetworks in computer vision.",
          "link": "http://arxiv.org/abs/2106.13500",
          "publishedOn": "2021-06-28T01:57:53.189Z",
          "wordCount": 648,
          "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural Networks. (arXiv:2106.13500v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.",
          "link": "http://arxiv.org/abs/2106.13375",
          "publishedOn": "2021-06-28T01:57:53.178Z",
          "wordCount": 693,
          "title": "Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:53.164Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1\">Badal Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1\">Debangan Thakuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1\">Nilutpal Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Navarun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1\">Bhaskarananda Boro</a>",
          "description": "Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users' penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users' watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.",
          "link": "http://arxiv.org/abs/2106.12970",
          "publishedOn": "2021-06-25T02:00:44.393Z",
          "wordCount": 644,
          "title": "RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:44.372Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salatino_A/0/1/0/all/0/1\">Angelo Salatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannocci_A/0/1/0/all/0/1\">Andrea Mannocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osborne_F/0/1/0/all/0/1\">Francesco Osborne</a>",
          "description": "Analysing research trends and predicting their impact on academia and\nindustry is crucial to gain a deeper understanding of the advances in a\nresearch field and to inform critical decisions about research funding and\ntechnology adoption. In the last years, we saw the emergence of several\npublicly-available and large-scale Scientific Knowledge Graphs fostering the\ndevelopment of many data-driven approaches for performing quantitative analyses\nof research trends. This chapter presents an innovative framework for\ndetecting, analysing, and forecasting research topics based on a large-scale\nknowledge graph characterising research articles according to the research\ntopics from the Computer Science Ontology. We discuss the advantages of a\nsolution based on a formal representation of topics and describe how it was\napplied to produce bibliometric studies and innovative tools for analysing and\npredicting research dynamics.",
          "link": "http://arxiv.org/abs/2106.12875",
          "publishedOn": "2021-06-25T02:00:44.353Z",
          "wordCount": 569,
          "title": "Detection, Analysis, and Prediction of Research Topics with Scientific Knowledge Graphs. (arXiv:2106.12875v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1\">Simon Poon</a>",
          "description": "Process mining is a relatively new subject which builds a bridge between\nprocess modelling and data mining. An exclusive choice in a process model\nusually splits the process into different branches. However, in some processes,\nit is possible to switch from one branch to another. The inductive miner\nguarantees to return sound process models, but fails to return a precise model\nwhen there are switch behaviours between different exclusive choice branches\ndue to the limitation of process trees. In this paper, we present a novel\nextension to the process tree model to support switch behaviours between\ndifferent branches of the exclusive choice operator and propose a novel\nextension to the inductive miner to discover sound process models with switch\nbehaviours. The proposed discovery technique utilizes the theory of a previous\nstudy to detect possible switch behaviours. We apply both artificial and\npublicly-available datasets to evaluate our approach. Our results show that our\napproach can improve the precision of discovered models by 36% while\nmaintaining high fitness values compared to the original inductive miner.",
          "link": "http://arxiv.org/abs/2106.12765",
          "publishedOn": "2021-06-25T02:00:44.307Z",
          "wordCount": 629,
          "title": "A Novel Approach to Discover Switch Behaviours in Process Mining. (arXiv:2106.12765v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1\">Luigi Asprino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colonna_C/0/1/0/all/0/1\">Christian Colonna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mongiovi_M/0/1/0/all/0/1\">Misael Mongiov&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porena_M/0/1/0/all/0/1\">Margherita Porena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1\">Valentina Presutti</a>",
          "description": "We present a novel approach to knowledge graph visualization based on\nontology design patterns. This approach relies on OPLa (Ontology Pattern\nLanguage) annotations and on a catalogue of visual frames, which are associated\nwith foundational ontology design patterns. We demonstrate that this approach\nsignificantly reduces the cognitive load required to users for visualizing and\ninterpreting a knowledge graph and guides the user in exploring it through\nmeaningful thematic paths provided by ontology patterns.",
          "link": "http://arxiv.org/abs/2106.12857",
          "publishedOn": "2021-06-25T02:00:44.277Z",
          "wordCount": 510,
          "title": "Pattern-based Visualization of Knowledge Graphs. (arXiv:2106.12857v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1\">Jia Wei Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Mei Shin Oh</a>",
          "description": "Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.",
          "link": "http://arxiv.org/abs/2106.12744",
          "publishedOn": "2021-06-25T02:00:44.245Z",
          "wordCount": 677,
          "title": "An Automated Knowledge Mining and Document Classification System with Multi-model Transfer Learning. (arXiv:2106.12744v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12741",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schutte_D/0/1/0/all/0/1\">Dalton Schutte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1\">Jake Vasilakes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1\">Anu Bompelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiszman_M/0/1/0/all/0/1\">Marcelo Fiszman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_J/0/1/0/all/0/1\">Jeffrey R. Bishop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_T/0/1/0/all/0/1\">Terrence Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.",
          "link": "http://arxiv.org/abs/2106.12741",
          "publishedOn": "2021-06-25T02:00:44.227Z",
          "wordCount": 714,
          "title": "Discovering novel drug-supplement interactions using a dietary supplements knowledge graph generated from the biomedical literature. (arXiv:2106.12741v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daniel Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Choon-Hui Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1\">Kedarnath Kolluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1\">Nikhil Shandilya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1\">Vyacheslav Ievgrafov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Japinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "We consider the problem of semantic matching in product search: given a\ncustomer query, retrieve all semantically related products from a huge catalog\nof size 100 million, or more. Because of large catalog spaces and real-time\nlatency constraints, semantic matching algorithms not only desire high recall\nbut also need to have low latency. Conventional lexical matching approaches\n(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but\nfail to capture behavioral signals between queries and products. In contrast,\nembedding-based models learn semantic representations from customer behavior\ndata, but the performance is often limited by shallow neural encoders due to\nlatency constraints. Semantic product search can be viewed as an eXtreme\nMulti-label Classification (XMC) problem, where customer queries are input\ninstances and products are output labels. In this paper, we aim to improve\nsemantic product search by using tree-based XMC models where inference time\ncomplexity is logarithmic in the number of products. We consider hierarchical\nlinear models with n-gram features for fast real-time inference.\nQuantitatively, our method maintains a low latency of 1.25 milliseconds per\nquery and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a\ncompeting embedding-based DSSM model. Our model is robust to weight pruning\nwith varying thresholds, which can flexibly meet different system requirements\nfor online deployments. Qualitatively, our method can retrieve products that\nare complementary to existing product search system and add diversity to the\nmatch set.",
          "link": "http://arxiv.org/abs/2106.12657",
          "publishedOn": "2021-06-25T02:00:44.209Z",
          "wordCount": 695,
          "title": "Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>",
          "description": "Recommender systems -- and especially matrix factorization-based\ncollaborative filtering algorithms -- play a crucial role in mediating our\naccess to online information. We show that such algorithms induce a particular\nkind of stereotyping: if preferences for a \\textit{set} of items are\nanti-correlated in the general user population, then those items may not be\nrecommended together to a user, regardless of that user's preferences and\nratings history. First, we introduce a notion of \\textit{joint accessibility},\nwhich measures the extent to which a set of items can jointly be accessed by\nusers. We then study joint accessibility under the standard factorization-based\ncollaborative filtering framework, and provide theoretical necessary and\nsufficient conditions when joint accessibility is violated. Moreover, we show\nthat these conditions can easily be violated when the users are represented by\na single feature vector. To improve joint accessibility, we further propose an\nalternative modelling fix, which is designed to capture the diverse multiple\ninterests of each user using a multi-vector representation. We conduct\nextensive experiments on real and simulated datasets, demonstrating the\nstereotyping problem with standard single-vector matrix factorization models.",
          "link": "http://arxiv.org/abs/2106.12622",
          "publishedOn": "2021-06-25T02:00:44.164Z",
          "wordCount": 615,
          "title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1\">Marah Abdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1\">Benjamin D. Pedigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweti Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1\">Vince Lyzinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1\">Piali~Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.",
          "link": "http://arxiv.org/abs/2106.12621",
          "publishedOn": "2021-06-25T02:00:44.150Z",
          "wordCount": 616,
          "title": "Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:42.471Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenghao Xu</a>",
          "description": "Multi-armed bandits (MAB) provide a principled online learning approach to\nattain the balance between exploration and exploitation. Due to the superior\nperformance and low feedback learning without the learning to act in multiple\nsituations, Multi-armed Bandits drawing widespread attention in applications\nranging such as recommender systems. Likewise, within the recommender system,\ncollaborative filtering (CF) is arguably the earliest and most influential\nmethod in the recommender system. Crucially, new users and an ever-changing\npool of recommended items are the challenges that recommender systems need to\naddress. For collaborative filtering, the classical method is training the\nmodel offline, then perform the online testing, but this approach can no longer\nhandle the dynamic changes in user preferences which is the so-called cold\nstart. So how to effectively recommend items to users in the absence of\neffective information? To address the aforementioned problems, a multi-armed\nbandit based collaborative filtering recommender system has been proposed,\nnamed BanditMF. BanditMF is designed to address two challenges in the\nmulti-armed bandits algorithm and collaborative filtering: (1) how to solve the\ncold start problem for collaborative filtering under the condition of scarcity\nof valid information, (2) how to solve the sub-optimal problem of bandit\nalgorithms in strong social relations domains caused by independently\nestimating unknown parameters associated with each user and ignoring\ncorrelations between users.",
          "link": "http://arxiv.org/abs/2106.10898",
          "publishedOn": "2021-06-24T01:51:42.312Z",
          "wordCount": 664,
          "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Muyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "One of the key challenges in Sequential Recommendation (SR) is how to extract\nand represent user preferences. Traditional SR methods rely on the next item as\nthe supervision signal to guide preference extraction and representation. We\npropose a novel learning strategy, named preference editing. The idea is to\nforce the SR model to discriminate the common and unique preferences in\ndifferent sequences of interactions between users and the recommender system.\nBy doing so, the SR model is able to learn how to identify common and unique\nuser preferences, and thereby do better user preference extraction and\nrepresentation. We propose a transformer based SR model, named MrTransformer\n(Multi-preference Transformer), that concatenates some special tokens in front\nof the sequence to represent multiple user preferences and makes sure they\ncapture different aspects through a preference coverage mechanism. Then, we\ndevise a preference editing-based self-supervised learning mechanism for\ntraining MrTransformer which contains two main operations: preference\nseparation and preference recombination. The former separates the common and\nunique user preferences for a given pair of sequences. The latter swaps the\ncommon preferences to obtain recombined user preferences for each sequence.\nBased on the preference separation and preference recombination operations, we\ndefine two types of SSL loss that require that the recombined preferences are\nsimilar to the original ones, and the common preferences are close to each\nother.\n\nWe carry out extensive experiments on two benchmark datasets. MrTransformer\nwith preference editing significantly outperforms state-of-the-art SR methods\nin terms of Recall, MRR and NDCG. We find that long sequences whose user\npreferences are harder to extract and represent benefit most from preference\nediting.",
          "link": "http://arxiv.org/abs/2106.12120",
          "publishedOn": "2021-06-24T01:51:42.106Z",
          "wordCount": 706,
          "title": "Improving Transformer-based Sequential Recommenders through Preference Editing. (arXiv:2106.12120v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delianidi_M/0/1/0/all/0/1\">Marina Delianidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salampasis_M/0/1/0/all/0/1\">Michail Salampasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diamantaras_K/0/1/0/all/0/1\">Konstantinos Diamantaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siomos_T/0/1/0/all/0/1\">Theodosios Siomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsalis_A/0/1/0/all/0/1\">Alkiviadis Katsalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaveli_I/0/1/0/all/0/1\">Iphigenia Karaveli</a>",
          "description": "We present a graph-based approach for the data management tasks and the\nefficient operation of a system for session-based next-item recommendations.\nThe proposed method can collect data continuously and incrementally from an\necommerce web site, thus seemingly prepare the necessary data infrastructure\nfor the recommendation algorithm to operate without any excessive training\nphase. Our work aims at developing a recommender method that represents a\nbalance between data processing and management efficiency requirements and the\neffectiveness of the recommendations produced. We use the Neo4j graph database\nto implement a prototype of such a system. Furthermore, we use an industry\ndataset corresponding to a typical e-commerce session-based scenario, and we\nreport on experiments using our graph-based approach and other state-of-the-art\nmachine learning and deep learning methods.",
          "link": "http://arxiv.org/abs/2106.12085",
          "publishedOn": "2021-06-24T01:51:42.093Z",
          "wordCount": 580,
          "title": "A Graph-based Method for Session-based Recommendations. (arXiv:2106.12085v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sunouchi_M/0/1/0/all/0/1\">Motohiro Sunouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Masaharu Yoshioka</a>",
          "description": "This paper proposes new acoustic feature signatures based on the multiscale\nfractal dimension (MFD), which are robust against the diversity of\nenvironmental sounds, for the content-based similarity search. The diversity of\nsound sources and acoustic compositions is a typical feature of environmental\nsounds. Several acoustic features have been proposed for environmental sounds.\nAmong them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),\nwhich describes frequency-domain features. However, in addition to these\nfeatures in the frequency domain, environmental sounds have other important\nfeatures in the time domain with various time scales. In our previous paper, we\nproposed enhanced multiscale fractal dimension signature (EMFD) for\nenvironmental sounds. This paper extends EMFD by using the kernel density\nestimation method, which results in better performance of the similarity search\ntasks. Furthermore, it newly proposes another acoustic feature signature based\non MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).\nThe MFD-VL signature describes several features of the time-varying envelope\nfor long periods of time. The MFD-VL signature has stability and robustness\nagainst background noise and small fluctuations in the parameters of sound\nsources, which are produced in field recordings. We discuss the effectiveness\nof these signatures in the similarity sound search by comparing with acoustic\nfeatures proposed in the DCASE 2018 challenges. Due to the unique\ndescriptiveness of our proposed signatures, we confirmed the signatures are\neffective when they are used with other acoustic features.",
          "link": "http://arxiv.org/abs/2102.02964",
          "publishedOn": "2021-06-24T01:51:42.072Z",
          "wordCount": 707,
          "title": "Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal Dimension for Similarity Search of Environmental Sounds. (arXiv:2102.02964v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_C/0/1/0/all/0/1\">Charmaine Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1\">Simon Poon</a>",
          "description": "The insights revealed from process mining heavily rely on the quality of\nevent logs. Activities extracted from healthcare information systems with the\nfree-text nature may lead to inconsistent labels. Such inconsistency would then\nlead to redundancy of activity labels, which refer to labels that have\ndifferent syntax but share the same behaviours. The identifications of these\nlabels from data-driven process discovery are difficult and rely heavily on\nresource-intensive human review. Existing work achieves low accuracy either\nredundant activity labels are in low occurrence frequency or the existence of\nnumerical data values as attributes in event logs. However, these phenomena are\ncommonly observed in healthcare information systems. In this paper, we propose\nan approach to detect redundant activity labels using control-flow relations\nand numerical data values from event logs. Natural Language Processing is also\nintegrated into our method to assess semantic similarity between labels, which\nprovides users with additional insights. We have evaluated our approach through\nsynthetic logs generated from the real-life Sepsis log and a case study using\nthe MIMIC-III data set. The results demonstrate that our approach can\nsuccessfully detect redundant activity labels. This approach can add value to\nthe preprocessing step to generate more representative event logs for process\nmining tasks in the healthcare domain.",
          "link": "http://arxiv.org/abs/2103.16061",
          "publishedOn": "2021-06-24T01:51:42.012Z",
          "wordCount": 677,
          "title": "A Novel Approach to Detect Redundant Activity Labels For More Representative Event Logs. (arXiv:2103.16061v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bingqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>",
          "description": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.",
          "link": "http://arxiv.org/abs/2012.09807",
          "publishedOn": "2021-06-24T01:51:41.975Z",
          "wordCount": 589,
          "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1\">Andreea Iana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "In today's academic publishing model, especially in Computer Science,\nconferences commonly constitute the main platforms for releasing the latest\npeer-reviewed advancements in their respective fields. However, choosing a\nsuitable academic venue for publishing one's research can represent a\nchallenging task considering the plethora of available conferences,\nparticularly for those at the start of their academic careers, or for those\nseeking to publish outside of their usual domain. In this paper, we propose\nGraphConfRec, a conference recommender system which combines SciGraph and graph\nneural networks, to infer suggestions based not only on title and abstract, but\nalso on co-authorship and citation relationships. GraphConfRec achieves a\nrecall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention\nnetwork-based recommendation model. A user study with 25 subjects supports the\npositive results.",
          "link": "http://arxiv.org/abs/2106.12340",
          "publishedOn": "2021-06-24T01:51:41.960Z",
          "wordCount": 577,
          "title": "GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>",
          "description": "Automatic processing of bibliographic data becomes very important in digital\nlibraries, data science and machine learning due to its importance in keeping\npace with the significant increase of published papers every year from one side\nand to the inherent challenges from the other side. This processing has several\naspects including but not limited to I) Automatic extraction of references from\nPDF documents, II) Building an accurate citation graph, III) Author name\ndisambiguation, etc. Bibliographic data is heterogeneous by nature and occurs\nin both structured (e.g. citation graph) and unstructured (e.g. publications)\nformats. Therefore, it requires data science and machine learning techniques to\nbe processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on\nBibliographic Data Analysis and Processing.",
          "link": "http://arxiv.org/abs/2106.12320",
          "publishedOn": "2021-06-24T01:51:41.930Z",
          "wordCount": 572,
          "title": "BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1\">Jurek Leonhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Machine learning models for the ad-hoc retrieval of documents and passages\nhave recently shown impressive improvements due to better language\nunderstanding using large pre-trained language models. However, these\nover-parameterized models are inherently non-interpretable and do not provide\nany information on the parts of the documents that were used to arrive at a\ncertain prediction.\n\nIn this paper we introduce the select and rank paradigm for document ranking,\nwhere interpretability is explicitly ensured when scoring longer documents.\nSpecifically, we first select sentences in a document based on the input query\nand then predict the query-document score based only on the selected sentences,\nacting as an explanation. We treat sentence selection as a latent variable\ntrained jointly with the ranker from the final output. We conduct extensive\nexperiments to demonstrate that our inherently interpretable select-and-rank\napproach is competitive in comparison to other state-of-the-art methods and\nsometimes even outperforms them. This is due to our novel end-to-end training\napproach based on weighted reservoir sampling that manages to train the\nselector despite the stochastic sentence selection. We also show that our\nsentence selection approach can be used to provide explanations for models that\noperate on only parts of the document, such as BERT.",
          "link": "http://arxiv.org/abs/2106.12460",
          "publishedOn": "2021-06-24T01:51:41.910Z",
          "wordCount": 626,
          "title": "Learnt Sparsity for Effective and Interpretable Document Ranking. (arXiv:2106.12460v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-06-23T01:48:37.718Z",
          "wordCount": 577,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.",
          "link": "http://arxiv.org/abs/2101.08293",
          "publishedOn": "2021-06-23T01:48:37.688Z",
          "wordCount": 706,
          "title": "What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yinyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1\">Wendy Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "The Turing Award is recognized as the most influential and prestigious award\nin the field of computer science(CS). With the rise of the science of science\n(SciSci), a large amount of bibliographic data has been analyzed in an attempt\nto understand the hidden mechanism of scientific evolution. These include the\nanalysis of the Nobel Prize, including physics, chemistry, medicine, etc. In\nthis article, we extract and analyze the data of 72 Turing Award laureates from\nthe complete bibliographic data, fill the gap in the lack of Turing Award\nanalysis, and discover the development characteristics of computer science as\nan independent discipline. First, we show most Turing Award laureates have\nlong-term and high-quality educational backgrounds, and more than 61% of them\nhave a degree in mathematics, which indicates that mathematics has played a\nsignificant role in the development of computer science. Secondly, the data\nshows that not all scholars have high productivity and high h-index; that is,\nthe number of publications and h-index is not the leading indicator for\nevaluating the Turing Award. Third, the average age of awardees has increased\nfrom 40 to around 70 in recent years. This may be because new breakthroughs\ntake longer, and some new technologies need time to prove their influence.\nBesides, we have also found that in the past ten years, international\ncollaboration has experienced explosive growth, showing a new paradigm in the\nform of collaboration. It is also worth noting that in recent years, the\nemergence of female winners has also been eye-catching. Finally, by analyzing\nthe personal publication records, we find that many people are more likely to\npublish high-impact articles during their high-yield periods.",
          "link": "http://arxiv.org/abs/2106.11534",
          "publishedOn": "2021-06-23T01:48:37.677Z",
          "wordCount": 724,
          "title": "Turing Award elites revisited: patterns of productivity, collaboration, authorship and impact. (arXiv:2106.11534v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1\">Iurii Mokrii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1\">Leonid Boytsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>",
          "description": "Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.",
          "link": "http://arxiv.org/abs/2103.03335",
          "publishedOn": "2021-06-23T01:48:37.666Z",
          "wordCount": 653,
          "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:37.598Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:37.576Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.",
          "link": "http://arxiv.org/abs/2106.11403",
          "publishedOn": "2021-06-23T01:48:37.549Z",
          "wordCount": 691,
          "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>",
          "description": "Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.",
          "link": "http://arxiv.org/abs/2106.07346",
          "publishedOn": "2021-06-23T01:48:37.521Z",
          "wordCount": 611,
          "title": "A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.",
          "link": "http://arxiv.org/abs/2104.08653",
          "publishedOn": "2021-06-23T01:48:37.499Z",
          "wordCount": 649,
          "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhufeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>",
          "description": "Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.",
          "link": "http://arxiv.org/abs/2106.01033",
          "publishedOn": "2021-06-23T01:48:37.480Z",
          "wordCount": 698,
          "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11846",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Wright_A/0/1/0/all/0/1\">Austin P Wright</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Saad_Falcon_J/0/1/0/all/0/1\">Jon Saad-Falcon</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tomprou_M/0/1/0/all/0/1\">Maria Tomprou</a>",
          "description": "As job markets worldwide have become more competitive and applicant selection\ncriteria have become more opaque, and different (and sometimes contradictory)\ninformation and advice is available for job seekers wishing to progress in\ntheir careers, it has never been more difficult to determine which factors in a\nr\\'esum\\'e most effectively help career progression. In this work we present a\nnovel, large scale dataset of over half a million r\\'esum\\'es with preliminary\nanalysis to begin to answer empirically which factors help or hurt people\nwishing to transition to more senior roles as they progress in their career. We\nfind that previous experience forms the most important factor, outweighing\nother aspects of human capital, and find which language factors in a r\\'esum\\'e\nhave significant effects. This lays the groundwork for future inquiry in career\ntrajectories using large scale data analysis and natural language processing\ntechniques.",
          "link": "http://arxiv.org/abs/2106.11846",
          "publishedOn": "2021-06-23T01:48:37.435Z",
          "wordCount": 618,
          "title": "Quantifying the Impact of Human Capital, Job History, and Language Factors on Job Seniority with a Large-scale Analysis of Resumes. (arXiv:2106.11846v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greiner_Petter_A/0/1/0/all/0/1\">Andre Greiner-Petter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubotz_M/0/1/0/all/0/1\">Moritz Schubotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Fabian Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breitinger_C/0/1/0/all/0/1\">Corinna Breitinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohl_H/0/1/0/all/0/1\">Howard S. Cohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Mathematical notation, i.e., the writing system used to communicate concepts\nin mathematics, encodes valuable information for a variety of information\nsearch and retrieval systems. Yet, mathematical notations remain mostly\nunutilized by today's systems. In this paper, we present the first in-depth\nstudy on the distributions of mathematical notation in two large scientific\ncorpora: the open access arXiv (2.5B mathematical objects) and the mathematical\nreviewing service for pure and applied mathematics zbMATH (61M mathematical\nobjects). Our study lays a foundation for future research projects on\nmathematical information retrieval for large scientific corpora. Further, we\ndemonstrate the relevance of our results to a variety of use-cases. For\nexample, to assist semantic extraction systems, to improve scientific search\nengines, and to facilitate specialized math recommendation systems. The\ncontributions of our presented research are as follows: (1) we present the\nfirst distributional analysis of mathematical formulae on arXiv and zbMATH; (2)\nwe retrieve relevant mathematical objects for given textual search queries\n(e.g., linking $P_{n}^{(\\alpha, \\beta)}\\!\\left(x\\right)$ with `Jacobi\npolynomial'); (3) we extend zbMATH's search engine by providing relevant\nmathematical formulae; and (4) we exemplify the applicability of the results by\npresenting auto-completion for math inputs as the first contribution to math\nrecommendation systems. To expedite future research projects, we have made\navailable our source code and data.",
          "link": "http://arxiv.org/abs/2002.02712",
          "publishedOn": "2021-06-23T01:48:37.249Z",
          "wordCount": 715,
          "title": "Discovering Mathematical Objects of Interest -- A Study of Mathematical Notations. (arXiv:2002.02712v3 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1\">Shamane Siriwardhana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1\">Rivindu Weerasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Elliott Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1\">Suranga Nanayakkara</a>",
          "description": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.",
          "link": "http://arxiv.org/abs/2106.11517",
          "publishedOn": "2021-06-23T01:48:37.203Z",
          "wordCount": 509,
          "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:13.227Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.055Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:13.025Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "In this paper, we address the text-to-audio grounding issue, namely,\ngrounding the segments of the sound event described by a natural language query\nin the untrimmed audio. This is a newly proposed but challenging audio-language\ntask, since it requires to not only precisely localize all the on- and off-sets\nof the desired segments in the audio, but to perform comprehensive acoustic and\nlinguistic understandings and reason the multimodal interactions between the\naudio and query. To tackle those problems, the existing method treats the query\nholistically as a single unit by a global query representation, which fails to\nhighlight the keywords that contain rich semantics. Besides, this method has\nnot fully exploited interactions between the query and audio. Moreover, since\nthe audio and queries are arbitrary and variable in length, many meaningless\nparts of them are not filtered out in this method, which hinders the grounding\nof the desired segments.\n\nTo this end, we propose a novel Query Graph with Cross-gating Attention\n(QGCA) model, which models the comprehensive relations between the words in\nquery through a novel query graph. Besides, to capture the fine-grained\ninteractions between audio and query, a cross-modal attention module that\nassigns higher weights to the keywords is introduced to generate the\nsnippet-specific query representations. Finally, we also design a cross-gating\nmodule to emphasize the crucial parts as well as weaken the irrelevant ones in\nthe audio and query. We extensively evaluate the proposed QGCA model on the\npublic Audiogrounding dataset with significant improvements over several\nstate-of-the-art methods. Moreover, further ablation study shows the consistent\neffectiveness of different modules in the proposed QGCA model.",
          "link": "http://arxiv.org/abs/2106.14136",
          "publishedOn": "2021-06-29T01:55:12.995Z",
          "wordCount": 707,
          "title": "Query-graph with Cross-gating Attention Model for Text-to-Audio Grounding. (arXiv:2106.14136v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zekuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>",
          "description": "Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.14076",
          "publishedOn": "2021-06-29T01:55:12.531Z",
          "wordCount": 633,
          "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zytko_D/0/1/0/all/0/1\">Douglas Zytko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Jacob Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundquist_N/0/1/0/all/0/1\">Nathaniel Lundquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Medina Taylor</a>",
          "description": "Immersive stories for health are 360-degree videos that intend to alter\nviewer perceptions about behaviors detrimental to health. They have potential\nto inform public health at scale, however, immersive story design is still in\nearly stages and largely devoid of best practices. This paper presents a focus\ngroup study with 147 viewers of an immersive story about binge drinking\nexperienced through VR headsets and mobile phones. The objective of the study\nis to identify aspects of immersive story design that influence attitudes\ntowards the health issue exhibited, and to understand how health information is\nconsumed in immersive stories. Findings emphasize the need for an immersive\nstory to provide reasoning behind character engagement in the focal health\nbehavior, to show the main character clearly engaging in the behavior, and to\nenable viewers to experience escalating symptoms of the behavior before the\npenultimate health consequence. Findings also show how the design of supporting\ncharacters can inadvertently distract viewers and lead them to justify the\ndetrimental behavior being exhibited. The paper concludes with design\nconsiderations for enabling immersive stories to better inform public\nperception of health issues.",
          "link": "http://arxiv.org/abs/2106.13921",
          "publishedOn": "2021-06-29T01:55:12.509Z",
          "wordCount": 652,
          "title": "Immersive Stories for Health Information: Design Considerations from Binge Drinking in VR. (arXiv:2106.13921v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14014",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tandon_P/0/1/0/all/0/1\">Pulkit Tandon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandak_S/0/1/0/all/0/1\">Shubham Chandak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pataranutaporn_P/0/1/0/all/0/1\">Pat Pataranutaporn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yimeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mapuranga_A/0/1/0/all/0/1\">Anesu M. Mapuranga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_P/0/1/0/all/0/1\">Pattie Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissman_T/0/1/0/all/0/1\">Tsachy Weissman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>",
          "description": "Video represents the majority of internet traffic today leading to a\ncontinuous technological arms race between generating higher quality content,\ntransmitting larger file sizes and supporting network infrastructure. Adding to\nthis is the recent COVID-19 pandemic fueled surge in the use of video\nconferencing tools. Since videos take up substantial bandwidth (~100 Kbps to\nfew Mbps), improved video compression can have a substantial impact on network\nperformance for live and pre-recorded content, providing broader access to\nmultimedia content worldwide. In this work, we present a novel video\ncompression pipeline, called Txt2Vid, which substantially reduces data\ntransmission rates by compressing webcam videos (\"talking-head videos\") to a\ntext transcript. The text is transmitted and decoded into a realistic\nreconstruction of the original video using recent advances in deep learning\nbased voice cloning and lip syncing models. Our generative pipeline achieves\ntwo to three orders of magnitude reduction in the bitrate as compared to the\nstandard audio-video codecs (encoders-decoders), while maintaining equivalent\nQuality-of-Experience based on a subjective evaluation by users (n=242) in an\nonline study. The code for this work is available at\nhttps://github.com/tpulkit/txt2vid.git.",
          "link": "http://arxiv.org/abs/2106.14014",
          "publishedOn": "2021-06-29T01:55:12.484Z",
          "wordCount": 686,
          "title": "Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text. (arXiv:2106.14014v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuewei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand shape and\npositions. Feature extraction of multi-modal CS is a key step in CS\nrecognition. Recent supervised deep learning based methods suffer from noisy CS\ndata annotations especially for hand shape modality. In this work, we first\npropose a self-supervised contrastive learning method to learn the feature\nrepresentation of image without using labels. Secondly, a small amount of\nmanually annotated CS data are used to fine-tune the first module. Thirdly, we\npresent a module, which combines Bi-LSTM and self-attention networks to further\nlearn sequential features with temporal and contextual information. Besides, to\nenlarge the volume and the diversity of the current limited CS datasets, we\nbuild a new British English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved in CS\nphoneme recognition correctness compared with the state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.14016",
          "publishedOn": "2021-06-29T01:55:12.438Z",
          "wordCount": 641,
          "title": "An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech. (arXiv:2106.14016v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Ziyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuewei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain a complete\nphonetic repertoire. Current deep learning based methods on automatic CS\nrecognition suffer from a common problem, which is the data scarcity. Until\nnow, there are only two public single speaker datasets for French (238\nsentences) and British English (97 sentences). In this work, we propose a\ncross-modal knowledge distillation method with teacher-student structure, which\ntransfers audio speech information to CS to overcome the limited data problem.\nFirstly, we pretrain a teacher model for CS recognition with a large amount of\nopen source audio speech data, and simultaneously pretrain the feature\nextractors for lips and hands using CS data. Then, we distill the knowledge\nfrom teacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for the first\ntime. The proposed method is evaluated on French and British English CS\ndatasets, showing superior CS recognition performance to the state-of-the-art\n(SOTA) by a large margin.",
          "link": "http://arxiv.org/abs/2106.13686",
          "publishedOn": "2021-06-28T01:57:53.477Z",
          "wordCount": 645,
          "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition. (arXiv:2106.13686v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:53.203Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:53.133Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyowon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scriney_M/0/1/0/all/0/1\">Michael Scriney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>",
          "description": "Much of the delivery of University education is now by synchronous or\nasynchronous video. For students, one of the challenges is managing the sheer\nvolume of such video material as video presentations of taught material are\ndifficult to abbreviate and summarise because they do not have highlights which\nstand out. Apart from video bookmarks there are no tools available to determine\nwhich parts of video content should be replayed at revision time or just before\nexaminations. We have developed and deployed a digital library for managing\nvideo learning material which has many dozens of hours of short-form video\ncontent from a range of taught courses for hundreds of students at\nundergraduate level. Through a web browser we allow students to access and play\nthese videos and we log their anonymised playback usage. From these logs we\nscore to each segment of each video based on the amount of playback it receives\nfrom across all students, whether the segment has been re-wound and re-played\nin the same student session, whether the on-screen window is the window in\nfocus on the student's desktop/laptop, and speed of playback. We also\nincorporate negative scoring if a video segment is skipped or fast-forward, and\noverarching all this we include a decay function based on recency of playback,\nso the most recent days of playback contribute more to the video segment\nscores. For each video in the library we present a usage-based graph which\nallows students to see which parts of each video attract the most playback from\ntheir peers, which helps them select material at revision time. Usage of the\nsystem is fully anonymised and GDPR-compliant.",
          "link": "http://arxiv.org/abs/2106.13504",
          "publishedOn": "2021-06-28T01:57:53.067Z",
          "wordCount": 710,
          "title": "Usage-based Summaries of Learning Videos. (arXiv:2106.13504v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1\">Jaros&#x142;aw Samelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Doma&#x144;ski</a>",
          "description": "The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.13574",
          "publishedOn": "2021-06-28T01:57:53.036Z",
          "wordCount": 627,
          "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cetinkaya_E/0/1/0/all/0/1\">Ekrem Cetinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanbari_M/0/1/0/all/0/1\">Mohammad Ghanbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmerer_C/0/1/0/all/0/1\">Christian Timmerer</a>",
          "description": "High-Efficiency Video Coding (HEVC) surpasses its predecessors in encoding\nefficiency by introducing new coding tools at the cost of an increased encoding\ntime-complexity. The Coding Tree Unit (CTU) is the main building block used in\nHEVC. In the HEVC standard, frames are divided into CTUs with the predetermined\nsize of up to 64x64 pixels. Each CTU is then divided recursively into a number\nof equally sized square areas, known as Coding Units (CUs). Although this\ndiversity of frame partitioning increases encoding efficiency, it also causes\nan increase in the time complexity due to the increased number of ways to find\nthe optimal partitioning. To address this complexity, numerous algorithms have\nbeen proposed to eliminate unnecessary searches during partitioning CTUs by\nexploiting the correlation in the video. In this paper, existing CTU depth\ndecision algorithms for HEVC are surveyed. These algorithms are categorized\ninto two groups, namely statistics and machine learning approaches. Statistics\napproaches are further subdivided into neighboring and inherent approaches.\nNeighboring approaches exploit the similarity between adjacent CTUs to limit\nthe depth range of the current CTU, while inherent approaches use only the\navailable information within the current CTU. Machine learning approaches try\nto extract and exploit similarities implicitly. Traditional methods like\nsupport vector machines or random forests use manually selected features, while\nrecently proposed deep learning methods extract features during training.\nFinally, this paper discusses extending these methods to more recent video\ncoding formats such as Versatile Video Coding (VVC) and AOMedia Video 1(AV1).",
          "link": "http://arxiv.org/abs/2104.08328",
          "publishedOn": "2021-06-25T02:00:43.994Z",
          "wordCount": 712,
          "title": "CTU Depth Decision Algorithms for HEVC: A Survey. (arXiv:2104.08328v2 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1\">Balamurali B T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1\">Hwan Ing Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Saumitra Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1\">Oon Hoe Teoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Sung Shin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Khai Pin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jer Ming Chen</a>",
          "description": "Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.",
          "link": "http://arxiv.org/abs/2106.12174",
          "publishedOn": "2021-06-24T01:51:41.860Z",
          "wordCount": 722,
          "title": "Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references\nfor the VIS community.",
          "link": "http://arxiv.org/abs/2106.11963",
          "publishedOn": "2021-06-23T01:48:37.697Z",
          "wordCount": 599,
          "title": "Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:17.839Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14758",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Daoye Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jingwei Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhaoxiang Ye</a>",
          "description": "Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.",
          "link": "http://arxiv.org/abs/2105.14758",
          "publishedOn": "2021-06-29T01:55:17.641Z",
          "wordCount": 614,
          "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.568Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:17.561Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:17.539Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to prove certain mathematical\nresults concerning the probability distribution of image pixel intensities\ncharacterized as generic $2D$ integer arrays. The size of the smallest bounded\nregion within an image is estimated for segmenting an image, from which, the\nequilibrium distribution of intensities can be recovered. From the estimated\nbounded regions, properties of the sub-optimal and equilibrium distributions of\nintensities are derived, which leads to an image compression methodology\nwhereby only slightly more than half of all pixels are required for a\nworst-case reconstruction of the original image. An example in unsupervised\nobject detection illustrates the mathematical results.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-06-29T01:55:17.533Z",
          "wordCount": 635,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v8 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Jiajun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention",
          "link": "http://arxiv.org/abs/2106.09003",
          "publishedOn": "2021-06-29T01:55:17.527Z",
          "wordCount": 594,
          "title": "Invertible Attention. (arXiv:2106.09003v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.",
          "link": "http://arxiv.org/abs/2105.08913",
          "publishedOn": "2021-06-29T01:55:17.521Z",
          "wordCount": 604,
          "title": "Multiple Meta-model Quantifying for Medical Visual Question Answering. (arXiv:2105.08913v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dongchen Lu</a>",
          "description": "Rotated object detection is a challenging issue of computer vision field.\nLoss of spatial information and confusion of parametric order have been the\nbottleneck for rotated detection accuracy. In this paper, we propose an\norientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of\nkeypoints to characterize the target and predict the keypoint heatmap on ROI to\nform a rotated target. By proposing the orientation-sensitive heatmap, OSKDet\ncould learn the shape and direction of rotated target implicitly and has\nstronger modeling capabilities for target representation, which improves the\nlocalization accuracy and acquires high quality detection results. To extract\nhighly effective features at border areas, we design a rotation-aware\ndeformable convolution module. Furthermore, we explore a new keypoint reorder\nalgorithm and feature fusion module based on the angle distribution to\neliminate the confusion of keypoint order. Experimental results on several\npublic benchmarks show the state-of-the-art performance of OSKDet.\nSpecifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and\n97.18% on UCAS-AOD, respectively.",
          "link": "http://arxiv.org/abs/2104.08697",
          "publishedOn": "2021-06-29T01:55:17.488Z",
          "wordCount": 621,
          "title": "OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated Object Detection. (arXiv:2104.08697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:17.464Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>",
          "description": "We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.",
          "link": "http://arxiv.org/abs/2105.10441",
          "publishedOn": "2021-06-29T01:55:17.458Z",
          "wordCount": 686,
          "title": "Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhexiong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "The transformer networks are particularly good at modeling long-range\ndependencies within a long sequence. In this paper, we conduct research on\napplying the transformer networks for salient object detection (SOD). We adopt\nthe dense transformer backbone for fully supervised RGB image based SOD, RGB-D\nimage pair based SOD, and weakly supervised SOD within a unified framework\nbased on the observation that the transformer backbone can provide accurate\nstructure modeling, which makes it powerful in learning from weak labels with\nless structure information. Further, we find that the vision transformer\narchitectures do not offer direct spatial supervision, instead encoding\nposition as a feature. Therefore, we investigate the contributions of two\nstrategies to provide stronger spatial supervision through the transformer\nlayers within our unified framework, namely deep supervision and\ndifficulty-aware learning. We find that deep supervision can get gradients back\ninto the higher level features, thus leads to uniform activation within the\nsame semantic object. Difficulty-aware learning on the other hand is capable of\nidentifying the hard pixels for effective hard negative mining. We also\nvisualize features of conventional backbone and transformer backbone before and\nafter fine-tuning them for SOD, and find that transformer backbone encodes more\naccurate object structure information and more distinct semantic information\nwithin the lower and higher level features respectively. We also apply our\nmodel to camouflaged object detection (COD) and achieve similar observations as\nthe above three SOD tasks. Extensive experimental results on various SOD and\nCOD tasks illustrate that transformer networks can transform SOD and COD,\nleading to new benchmarks for each related task. The source code and\nexperimental results are available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.",
          "link": "http://arxiv.org/abs/2104.10127",
          "publishedOn": "2021-06-29T01:55:17.443Z",
          "wordCount": 757,
          "title": "Transformer Transforms Salient Object Detection and Camouflaged Object Detection. (arXiv:2104.10127v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1\">Himanshu Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Sentiment analysis can provide a suitable lead for the tools used in software\nengineering along with the API recommendation systems and relevant libraries to\nbe used. In this context, the existing tools like SentiCR, SentiStrength-SE,\netc. exhibited low f1-scores that completely defeats the purpose of deployment\nof such strategies, thereby there is enough scope for performance improvement.\nRecent advancements show that transformer based pre-trained models (e.g., BERT,\nRoBERTa, ALBERT, etc.) have displayed better results in the text classification\ntask. Following this context, the present research explores different\nBERT-based models to analyze the sentences in GitHub comments, Jira comments,\nand Stack Overflow posts. The paper presents three different strategies to\nanalyse BERT based model for sentiment analysis, where in the first strategy\nthe BERT based pre-trained models are fine-tuned; in the second strategy an\nensemble model is developed from BERT variants, and in the third strategy a\ncompressed model (Distil BERT) is used. The experimental results show that the\nBERT based ensemble approach and the compressed BERT model attain improvements\nby 6-12% over prevailing tools for the F1 measure on all three datasets.",
          "link": "http://arxiv.org/abs/2106.02581",
          "publishedOn": "2021-06-29T01:55:17.436Z",
          "wordCount": 636,
          "title": "BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1\">Peri Akiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1\">Kristin Dana</a>",
          "description": "The costly process of obtaining semantic segmentation labels has driven\nresearch towards weakly supervised semantic segmentation (WSSS) methods, using\nonly image-level, point, or box labels. The lack of dense scene representation\nrequires methods to increase complexity to obtain additional semantic\ninformation about the scene, often done through multiple stages of training and\nrefinement. Current state-of-the-art (SOTA) models leverage image-level labels\nto produce class activation maps (CAMs) which go through multiple stages of\nrefinement before they are thresholded to make pseudo-masks for supervision.\nThe multi-stage approach is computationally expensive, and dependency on\nimage-level labels for CAMs generation lacks generalizability to more complex\nscenes. In contrary, our method offers a single-stage approach generalizable to\narbitrary dataset, that is trainable from scratch, without any dependency on\npre-trained backbones, classification, or separate refinement tasks. We utilize\npoint annotations to generate reliable, on-the-fly pseudo-masks through refined\nand filtered features. While our method requires point annotations that are\nonly slightly more expensive than image-level annotations, we are to\ndemonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as\nsignificantly outperform other SOTA WSSS methods on recent real-world datasets\n(CRAID, CityPersons, IAD).",
          "link": "http://arxiv.org/abs/2106.10309",
          "publishedOn": "2021-06-29T01:55:17.398Z",
          "wordCount": 637,
          "title": "Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Channel Pruning has been long studied to compress CNNs for efficient image\nclassification. Prior works implement channel pruning in an unexplainable\nmanner, which tends to reduce the final classification errors while failing to\nconsider the internal influence of each channel. In this paper, we conduct\nchannel pruning in a white box. Through deep visualization of feature maps\nactivated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we choose to preserve channels contributing to most categories.\nSpecifically, to model the contribution of each channel to differentiating\ncategories, we develop a class-wise mask for each channel, implemented in a\ndynamic training manner w.r.t. the input image's category. On the basis of the\nlearned class-wise mask, we perform a global voting mechanism to remove\nchannels with less category discrimination. Lastly, a fine-tuning process is\nconducted to recover the performance of the pruned model. To our best\nknowledge, it is the first time that CNN interpretability theory is considered\nto guide channel pruning. Extensive experiments on representative image\nclassification tasks demonstrate the superiority of our White-Box over many\nstate-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with even\n0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a\n45.6% FLOPs reduction with only a small loss of 0.83% in the top-1 accuracy for\nResNet-50.",
          "link": "http://arxiv.org/abs/2104.11883",
          "publishedOn": "2021-06-29T01:55:17.096Z",
          "wordCount": 707,
          "title": "Channel Pruning in a White Box for Efficient Image Classification. (arXiv:2104.11883v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1\">Xiaofei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>",
          "description": "This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n\nIn this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.",
          "link": "http://arxiv.org/abs/2106.07409",
          "publishedOn": "2021-06-29T01:55:17.090Z",
          "wordCount": 569,
          "title": "3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>",
          "description": "The rapid progress in 3D scene understanding has come with growing demand for\ndata; however, collecting and annotating 3D scenes (e.g. point clouds) are\nnotoriously hard. For example, the number of scenes (e.g. indoor rooms) that\ncan be accessed and scanned might be limited; even given sufficient data,\nacquiring 3D labels (e.g. instance masks) requires intensive human labor. In\nthis paper, we explore data-efficient learning for 3D point cloud. As a first\nstep towards this direction, we propose Contrastive Scene Contexts, a 3D\npre-training method that makes use of both point-level correspondences and\nspatial contexts in a scene. Our method achieves state-of-the-art results on a\nsuite of benchmarks where training data or labels are scarce. Our study reveals\nthat exhaustive labelling of 3D point clouds might be unnecessary; and\nremarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%\n(instance segmentation) and 96% (semantic segmentation) of the baseline\nperformance that uses full annotations.",
          "link": "http://arxiv.org/abs/2012.09165",
          "publishedOn": "2021-06-29T01:55:17.067Z",
          "wordCount": 635,
          "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Abhiraj Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>",
          "description": "FloodNet is a high-resolution image dataset acquired by a small UAV platform,\nDJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a\nunique challenge of advancing the damage assessment process for post-disaster\nscenarios using unlabeled and limited labeled dataset. We propose a solution to\naddress their classification and semantic segmentation challenge. We approach\nthis problem by generating pseudo labels for both classification and\nsegmentation during training and slowly incrementing the amount by which the\npseudo label loss affects the final loss. Using this semi-supervised method of\ntraining helped us improve our baseline supervised loss by a huge margin for\nclassification, allowing the model to generalize and perform better on the\nvalidation and test splits of the dataset. In this paper, we compare and\ncontrast the various methods and models for image classification and semantic\nsegmentation on the FloodNet dataset.",
          "link": "http://arxiv.org/abs/2105.08655",
          "publishedOn": "2021-06-29T01:55:17.060Z",
          "wordCount": 611,
          "title": "Semi-Supervised Classification and Segmentation on High Resolution Aerial Images. (arXiv:2105.08655v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1\">Levy Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1\">Alceu Bissoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>",
          "description": "Self-supervised pre-training appears as an advantageous alternative to\nsupervised pre-trained for transfer learning. By synthesizing annotations on\npretext tasks, self-supervision allows to pre-train models on large amounts of\npseudo-labels before fine-tuning them on the target task. In this work, we\nassess self-supervision for the diagnosis of skin lesions, comparing three\nself-supervised pipelines to a challenging supervised baseline, on five test\ndatasets comprising in- and out-of-distribution samples. Our results show that\nself-supervision is competitive both in improving accuracies and in reducing\nthe variability of outcomes. Self-supervision proves particularly useful for\nlow training data scenarios ($<1\\,500$ and $<150$ samples), where its ability\nto stabilize the outcomes is essential to provide sound results.",
          "link": "http://arxiv.org/abs/2106.09229",
          "publishedOn": "2021-06-29T01:55:17.041Z",
          "wordCount": 568,
          "title": "An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03072",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheon_M/0/1/0/all/0/1\">Manri Cheon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1\">Sungjun Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1\">Byungyeon Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Junwoo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Haiyang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bin_Y/0/1/0/all/0/1\">Yi Bin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Yuqing Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_H/0/1/0/all/0/1\">Hengliang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jingyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1\">Longtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Y/0/1/0/all/0/1\">Yiting Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thong_W/0/1/0/all/0/1\">William Thong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_J/0/1/0/all/0/1\">Jose Costa Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Hengxing Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_P/0/1/0/all/0/1\">Pengfei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayyoubzadeh_S/0/1/0/all/0/1\">Seyed Mehdi Ayyoubzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammou_D/0/1/0/all/0/1\">Dounia Hammou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1\">Sewoong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_G/0/1/0/all/0/1\">Gwangjin Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsubota_K/0/1/0/all/0/1\">Koki Tsubota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akutsu_H/0/1/0/all/0/1\">Hiroaki Akutsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>",
          "description": "This paper reports on the NTIRE 2021 challenge on perceptual image quality\nassessment (IQA), held in conjunction with the New Trends in Image Restoration\nand Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image\nprocessing technology, perceptual image processing algorithms based on\nGenerative Adversarial Networks (GAN) have produced images with more realistic\ntextures. These output images have completely different characteristics from\ntraditional distortions, thus pose a new challenge for IQA methods to evaluate\ntheir visual quality. In comparison with previous IQA challenges, the training\nand testing datasets in this challenge include the outputs of perceptual image\nprocessing algorithms and the corresponding subjective scores. Thus they can be\nused to develop and evaluate IQA methods on GAN-based distortions. The\nchallenge has 270 registered participants in total. In the final testing stage,\n13 participating teams submitted their models and fact sheets. Almost all of\nthem have achieved much better results than existing IQA methods, while the\nwinning method can demonstrate state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.03072",
          "publishedOn": "2021-06-29T01:55:17.004Z",
          "wordCount": 730,
          "title": "NTIRE 2021 Challenge on Perceptual Image Quality Assessment. (arXiv:2105.03072v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Tan Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ly_N/0/1/0/all/0/1\">Nam Tuan Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "In this paper, we propose an RNN-Transducer model for recognizing Japanese\nand Chinese offline handwritten text line images. As far as we know, it is the\nfirst approach that adopts the RNN-Transducer model for offline handwritten\ntext recognition. The proposed model consists of three main components: a\nvisual feature encoder that extracts visual features from an input image by CNN\nand then encodes the visual features by BLSTM; a linguistic context encoder\nthat extracts and encodes linguistic features from the input image by embedded\nlayers and LSTM; and a joint decoder that combines and then decodes the visual\nfeatures and the linguistic features into the final label sequence by fully\nconnected and softmax layers. The proposed model takes advantage of both visual\nand linguistic information from the input image. In the experiments, we\nevaluated the performance of the proposed model on the two datasets: Kuzushiji\nand SCUT-EPT. Experimental results show that the proposed model achieves\nstate-of-the-art performance on all datasets.",
          "link": "http://arxiv.org/abs/2106.14459",
          "publishedOn": "2021-06-29T01:55:16.982Z",
          "wordCount": 607,
          "title": "Recurrent neural network transducer for Japanese and Chinese offline handwritten text recognition. (arXiv:2106.14459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-06-29T01:55:16.953Z",
          "wordCount": 681,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.",
          "link": "http://arxiv.org/abs/2106.10026",
          "publishedOn": "2021-06-29T01:55:16.946Z",
          "wordCount": 586,
          "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:16.924Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.900Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Ying Dai</a>",
          "description": "To establish an appropriate model for photo aesthetic assessment, in this\npaper, a D-measure which reflects the disentanglement degree of the final layer\nFC nodes of CNN is introduced. By combining F-measure with D-measure to obtain\na FD measure, an algorithm of determining the optimal model from the multiple\nphoto score prediction models generated by CNN-based repetitively self-revised\nlearning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)\nand the assessment interest region(AIR) of the models are defined and\ncalculated. The experimental results show that the FD measure is effective for\nestablishing the appropriate model from the multiple score prediction models\nwith different CNN structures. Moreover, the FD-determined optimal models with\nthe comparatively high FD always have the FFP an AIR which are close to the\nhuman's aesthetic perception when enjoying photos.",
          "link": "http://arxiv.org/abs/2106.03316",
          "publishedOn": "2021-06-29T01:55:16.876Z",
          "wordCount": 591,
          "title": "Exploring to establish an appropriate model for image aesthetic assessment via CNN-based RSRL: An empirical study. (arXiv:2106.03316v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.861Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>",
          "description": "Transformer, which can benefit from global (long-range) information modeling\nusing self-attention mechanisms, has been successful in natural language\nprocessing and 2D image classification recently. However, both local and global\nfeatures are crucial for dense prediction tasks, especially for 3D medical\nimage segmentation. In this paper, we for the first time exploit Transformer in\n3D CNN for MRI Brain Tumor Segmentation and propose a novel network named\nTransBTS based on the encoder-decoder structure. To capture the local 3D\ncontext information, the encoder first utilizes 3D CNN to extract the\nvolumetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer and\nperforms progressive upsampling to predict the detailed segmentation map.\nExtensive experimental results on both BraTS 2019 and 2020 datasets show that\nTransBTS achieves comparable or higher results than previous state-of-the-art\n3D methods for brain tumor segmentation on 3D MRI scans. The source code is\navailable at https://github.com/Wenxuan-1119/TransBTS",
          "link": "http://arxiv.org/abs/2103.04430",
          "publishedOn": "2021-06-29T01:55:16.749Z",
          "wordCount": 640,
          "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer. (arXiv:2103.04430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:16.698Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:16.644Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "There are many real-life use cases such as barcode scanning or billboard\nreading where people need to detect objects and read the object contents.\nCommonly existing methods are first trying to localize object regions, then\ndetermine layout and lastly classify content units. However, for simple fixed\nstructured objects like license plates, this approach becomes overkill and\nlengthy to run. This work aims to solve this detect-and-read problem in a\nlightweight way by integrating multi-digit recognition into a one-stage object\ndetection model. Our unified method not only eliminates the duplication in\nfeature extraction (one for localizing, one again for classifying) but also\nprovides useful contextual information around object regions for\nclassification. Additionally, our choice of backbones and modifications in\narchitecture, loss function, data augmentation and training make the method\nrobust, efficient and speedy. Secondly, we made a public benchmark dataset of\ndiverse real-life 1D barcodes for a reliable evaluation, which we collected,\nannotated and checked carefully. Eventually, experimental results prove the\nmethod's efficiency on the barcode problem by outperforming industrial tools in\nboth detecting and decoding rates with a real-time fps at a VGA-similar\nresolution. It also did a great job expectedly on the license-plate recognition\ntask (on the AOLP dataset) by outperforming the current state-of-the-art method\nsignificantly in terms of recognition rate and inference time.",
          "link": "http://arxiv.org/abs/2102.07354",
          "publishedOn": "2021-06-29T01:55:16.303Z",
          "wordCount": 698,
          "title": "QuickBrowser: A Unified Model to Detect and Read Simple Object in Real-time. (arXiv:2102.07354v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:16.269Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:16.216Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Songhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiwei Liang</a>",
          "description": "Facial Expression Recognition (FER) is a classification task that points to\nface variants. Hence, there are certain affinity features between facial\nexpressions, receiving little attention in the FER literature. Convolution\npadding, despite helping capture the edge information, causes erosion of the\nfeature map simultaneously. After multi-layer filling convolution, the output\nfeature map named albino feature definitely weakens the representation of the\nexpression. To tackle these challenges, we propose a novel architecture named\nAmending Representation Module (ARM). ARM is a substitute for the pooling\nlayer. Theoretically, it can be embedded in the back end of any network to deal\nwith the Padding Erosion. ARM efficiently enhances facial expression\nrepresentation from two different directions: 1) reducing the weight of eroded\nfeatures to offset the side effect of padding, and 2) sharing affinity features\nover mini-batch to strengthen the representation learning. Experiments on\npublic benchmarks prove that our ARM boosts the performance of FER remarkably.\nThe validation accuracies are respectively 92.05% on RAF-DB, 65.2% on\nAffect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our\nimplementation and trained models are available at\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.",
          "link": "http://arxiv.org/abs/2103.10189",
          "publishedOn": "2021-06-29T01:55:16.181Z",
          "wordCount": 647,
          "title": "Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning-Hsu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>",
          "description": "Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&V-planes (referred to as\nPanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.",
          "link": "http://arxiv.org/abs/2106.14166",
          "publishedOn": "2021-06-29T01:55:16.149Z",
          "wordCount": 584,
          "title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:16.106Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:16.057Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:16.045Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:16.038Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1\">Martin Ferianc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Divyansh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongxiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel Rodrigues</a>",
          "description": "Fully convolutional U-shaped neural networks have largely been the dominant\napproach for pixel-wise image segmentation. In this work, we tackle two defects\nthat hinder their deployment in real-world applications: 1) Predictions lack\nuncertainty quantification that may be crucial to many decision-making systems;\n2) Large memory storage and computational consumption demanding extensive\nhardware resources. To address these issues and improve their practicality we\ndemonstrate a few-parameter compact Bayesian convolutional architecture, that\nachieves a marginal improvement in accuracy in comparison to related work using\nsignificantly fewer parameters and compute operations. The architecture\ncombines parameter-efficient operations such as separable convolutions,\nbilinear interpolation, multi-scale feature propagation and Bayesian inference\nfor per-pixel uncertainty quantification through Monte Carlo Dropout. The best\nperforming configurations required fewer than 2.5 million parameters on diverse\nchallenging datasets with few observations.",
          "link": "http://arxiv.org/abs/2104.06957",
          "publishedOn": "2021-06-29T01:55:16.016Z",
          "wordCount": 610,
          "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation. (arXiv:2104.06957v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14403",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.",
          "link": "http://arxiv.org/abs/2106.14403",
          "publishedOn": "2021-06-29T01:55:16.009Z",
          "wordCount": 660,
          "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrmann_D/0/1/0/all/0/1\">Daniel F&#xe4;hrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1\">Jan Niklas Kolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "Soft-biometrics play an important role in face biometrics and related fields\nsince these might lead to biased performances, threatens the user's privacy, or\nare valuable for commercial aspects. Current face databases are specifically\nconstructed for the development of face recognition applications. Consequently,\nthese databases contain large amount of face images but lack in the number of\nattribute annotations and the overall annotation correctness. In this work, we\npropose MAADFace, a new face annotations database that is characterized by the\nlarge number of its high-quality attribute annotations. MAADFace is build on\nthe VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.\nUsing a novel annotation transfer-pipeline that allows an accurate\nlabel-transfer from multiple source-datasets to a target-dataset, MAAD-Face\nconsists of 123.9M attribute annotations of 47 different binary attributes.\nConsequently, it provides 15 and 137 times more attribute labels than CelebA\nand LFW. Our investigation on the annotation quality by three human evaluators\ndemonstrated the superiority of the MAAD-Face annotations over existing\ndatabases. Additionally, we make use of the large amount of high-quality\nannotations from MAAD-Face to study the viability of soft-biometrics for\nrecognition, providing insights about which attributes support genuine and\nimposter decisions. The MAAD-Face annotations dataset is publicly available.",
          "link": "http://arxiv.org/abs/2012.01030",
          "publishedOn": "2021-06-29T01:55:15.962Z",
          "wordCount": 680,
          "title": "MAAD-Face: A Massively Annotated Attribute Dataset for Face Images. (arXiv:2012.01030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Semantic segmentation has made tremendous progress in recent years. However,\nsatisfying performance highly depends on a large number of pixel-level\nannotations. Therefore, in this paper, we focus on the semi-supervised\nsegmentation problem where only a small set of labeled data is provided with a\nmuch larger collection of totally unlabeled images. Nevertheless, due to the\nlimited annotations, models may overly rely on the contexts available in the\ntraining data, which causes poor generalization to the scenes unseen before. A\npreferred high-level representation should capture the contextual information\nwhile not losing self-awareness. Therefore, we propose to maintain the\ncontext-aware consistency between features of the same identity but with\ndifferent contexts, making the representations robust to the varying\nenvironments. Moreover, we present the Directional Contrastive Loss (DC Loss)\nto accomplish the consistency in a pixel-to-pixel manner, only requiring the\nfeature with lower quality to be aligned towards its counterpart. In addition,\nto avoid the false-negative samples and filter the uncertain positive samples,\nwe put forward two sampling strategies. Extensive experiments show that our\nsimple yet effective method surpasses current state-of-the-art methods by a\nlarge margin and also generalizes well with extra image-level annotations.",
          "link": "http://arxiv.org/abs/2106.14133",
          "publishedOn": "2021-06-29T01:55:15.955Z",
          "wordCount": 643,
          "title": "Semi-supervised Semantic Segmentation with Directional Context-aware Consistency. (arXiv:2106.14133v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12931",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1\">Tashin Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1\">Noor Hossain Nuri Sabab</a>",
          "description": "Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.",
          "link": "http://arxiv.org/abs/2009.12931",
          "publishedOn": "2021-06-29T01:55:15.798Z",
          "wordCount": 699,
          "title": "Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14248",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Accelerating multi-modal magnetic resonance (MR) imaging is a new and\neffective solution for fast MR imaging, providing superior performance in\nrestoring the target modality from its undersampled counterpart with guidance\nfrom an auxiliary modality. However, existing works simply introduce the\nauxiliary modality as prior information, lacking in-depth investigations on the\npotential mechanisms for fusing two modalities. Further, they usually rely on\nthe convolutional neural networks (CNNs), which focus on local information and\nprevent them from fully capturing the long-distance dependencies of global\nknowledge. To this end, we propose a multi-modal transformer (MTrans), which is\ncapable of transferring multi-scale features from the target modality to the\nauxiliary modality, for accelerated MR imaging. By restructuring the\ntransformer architecture, our MTrans gains a powerful ability to capture deep\nmulti-modal information. More specifically, the target modality and the\nauxiliary modality are first split into two branches and then fused using a\nmulti-modal transformer module. This module is based on an improved multi-head\nattention mechanism, named the cross attention module, which absorbs features\nfrom the auxiliary modality that contribute to the target modality. Our\nframework provides two appealing benefits: (i) MTrans is the first attempt at\nusing improved transformers for multi-modal MR imaging, affording more global\ninformation compared with CNN-based methods. (ii) A new cross attention module\nis proposed to exploit the useful information in each branch at different\nscales. It affords both distinct structural information and subtle pixel-level\ninformation, which supplement the target modality effectively.",
          "link": "http://arxiv.org/abs/2106.14248",
          "publishedOn": "2021-06-29T01:55:15.773Z",
          "wordCount": 687,
          "title": "MTrans: Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyungsik Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngrock Oh</a>",
          "description": "Increasing demands for understanding the internal behavior of convolutional\nneural networks (CNNs) have led to remarkable improvements in explanation\nmethods. Particularly, several class activation mapping (CAM) based methods,\nwhich generate visual explanation maps by a linear combination of activation\nmaps from CNNs, have been proposed. However, the majority of the methods lack a\nclear theoretical basis on how they assign the coefficients of the linear\ncombination. In this paper, we revisit the intrinsic linearity of CAM with\nrespect to the activation maps; we construct an explanation model of CNN as a\nlinear function of binary variables that denote the existence of the\ncorresponding activation maps. With this approach, the explanation model can be\ndetermined by additive feature attribution methods in an analytic manner. We\nthen demonstrate the adequacy of SHAP values, which is a unique solution for\nthe explanation model with a set of desirable properties, as the coefficients\nof CAM. Since the exact SHAP values are unattainable, we introduce an efficient\napproximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can\nestimate the SHAP values of the activation maps with high speed and accuracy.\nFurthermore, it greatly outperforms other previous CAM-based methods in both\nqualitative and quantitative aspects.",
          "link": "http://arxiv.org/abs/2102.05228",
          "publishedOn": "2021-06-29T01:55:15.744Z",
          "wordCount": 653,
          "title": "Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>",
          "description": "Image-based 3D object modeling refers to the process of converting raw\noptical images to 3D digital representations of the objects. Very often, such\nmodels are desired to be dimensionally true, semantically labeled with\nphotorealistic appearance (reality-based modeling). Laser scanning was deemed\nas the standard (and direct) way to obtaining highly accurate 3D measurements\nof objects, while one would have to abide the high acquisition cost and its\nunavailability on some of the platforms. Nowadays the image-based methods\nbackboned by the recently developed advanced dense image matching algorithms\nand geo-referencing paradigms, are becoming the dominant approaches, due to its\nhigh flexibility, availability and low cost. The largely automated geometric\nprocessing of images in a 3D object reconstruction workflow, from\nordered/unordered raw imagery to textured meshes, is becoming a critical part\nof the reality-based 3D modeling. This article summarizes the overall geometric\nprocessing workflow, with focuses on introducing the state-of-the-art methods\nof three major components of geometric processing: 1) geo-referencing; 2) Image\ndense matching 3) texture mapping. Finally, we will draw conclusions and share\nour outlooks of the topics discussed in this article.",
          "link": "http://arxiv.org/abs/2106.14307",
          "publishedOn": "2021-06-29T01:55:15.738Z",
          "wordCount": 610,
          "title": "Geometric Processing for Image-based 3D Object Modeling. (arXiv:2106.14307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1\">Kira Maag</a>",
          "description": "Instance segmentation of images is an important tool for automated scene\nunderstanding. Neural networks are usually trained to optimize their overall\nperformance in terms of accuracy. Meanwhile, in applications such as automated\ndriving, an overlooked pedestrian seems more harmful than a falsely detected\none. In this work, we present a false negative detection method for image\nsequences based on inconsistencies in time series of tracked instances given\nthe availability of image sequences in online applications. As the number of\ninstances can be greatly increased by this algorithm, we apply a false positive\npruning using uncertainty estimates aggregated over instances. To this end,\ninstance-wise metrics are constructed which characterize uncertainty and\ngeometry of a given instance or are predicated on depth estimation. The\nproposed method serves as a post-processing step applicable to any neural\nnetwork that can also be trained on single frames only. In our tests, we obtain\nan improved trade-off between false negative and false positive instances by\nour fused detection approach in comparison to the use of an ordinary score\nvalue provided by the instance segmentation network during inference.",
          "link": "http://arxiv.org/abs/2106.14474",
          "publishedOn": "2021-06-29T01:55:15.732Z",
          "wordCount": 615,
          "title": "False Negative Reduction in Video Instance Segmentation using Uncertainty Estimates. (arXiv:2106.14474v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:15.697Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolcha_Y/0/1/0/all/0/1\">Yalew Tolcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_T/0/1/0/all/0/1\">Tae Joon Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Barcodes are ubiquitous and have been used in most of critical daily\nactivities for decades. However, most of traditional decoders require\nwell-founded barcode under a relatively standard condition. While wilder\nconditioned barcodes such as underexposed, occluded, blurry, wrinkled and\nrotated are commonly captured in reality, those traditional decoders show\nweakness of recognizing. Several works attempted to solve those challenging\nbarcodes, but many limitations still exist. This work aims to solve the\ndecoding problem using deep convolutional neural network with the possibility\nof running on portable devices. Firstly, we proposed a special modification of\ninference based on the feature of having checksum and test-time augmentation,\nnamed as Smart Inference (SI) in prediction phase of a trained model. SI\nconsiderably boosts accuracy and reduces the false prediction for trained\nmodels. Secondly, we have created a large practical evaluation dataset of real\ncaptured 1D barcode under various challenging conditions to test our methods\nvigorously, which is publicly available for other researchers. The experiments'\nresults demonstrated the SI effectiveness with the highest accuracy of 95.85%\nwhich outperformed many existing decoders on the evaluation set. Finally, we\nsuccessfully minimized the best model by knowledge distillation to a shallow\nmodel which is shown to have high accuracy (90.85%) with good inference speed\nof 34.2 ms per image on a real edge device.",
          "link": "http://arxiv.org/abs/2004.06297",
          "publishedOn": "2021-06-29T01:55:15.479Z",
          "wordCount": 706,
          "title": "Smart Inference for Multidigit Convolutional Neural Network based Barcode Decoding. (arXiv:2004.06297v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>",
          "description": "Advances in technology have led to the development of methods that can create\ndesired visual multimedia. In particular, image generation using deep learning\nhas been extensively studied across diverse fields. In comparison, video\ngeneration, especially on conditional inputs, remains a challenging and less\nexplored area. To narrow this gap, we aim to train our model to produce a video\ncorresponding to a given text description. We propose a novel training\nframework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),\nwhich evolves frame-by-frame and finally produces a full-length video. In the\nfirst phase, we focus on creating a high-quality single video frame while\nlearning the relationship between the text and an image. As the steps proceed,\nour model is trained gradually on more number of consecutive frames.This\nstep-by-step learning process helps stabilize the training and enables the\ncreation of high-resolution video based on conditional text descriptions.\nQualitative and quantitative experimental results on various datasets\ndemonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2009.02018",
          "publishedOn": "2021-06-29T01:55:15.417Z",
          "wordCount": 628,
          "title": "TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator. (arXiv:2009.02018v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huatian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiannan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jigen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>",
          "description": "Discriminating small moving objects within complex visual environments is a\nsignificant challenge for autonomous micro robots that are generally limited in\ncomputational power. By exploiting their highly evolved visual systems, flying\ninsects can effectively detect mates and track prey during rapid pursuits, even\nthough the small targets equate to only a few pixels in their visual field. The\nhigh degree of sensitivity to small target movement is supported by a class of\nspecialized neurons called small target motion detectors (STMDs). Existing\nSTMD-based computational models normally comprise four sequentially arranged\nneural layers interconnected via feedforward loops to extract information on\nsmall target motion from raw visual inputs. However, feedback, another\nimportant regulatory circuit for motion perception, has not been investigated\nin the STMD pathway and its functional roles for small target motion detection\nare not clear. In this paper, we propose an STMD-based neural network with\nfeedback connection (Feedback STMD), where the network output is temporally\ndelayed, then fed back to the lower layers to mediate neural responses. We\ncompare the properties of the model with and without the time-delay feedback\nloop, and find it shows preference for high-velocity objects. Extensive\nexperiments suggest that the Feedback STMD achieves superior detection\nperformance for fast-moving small targets, while significantly suppressing\nbackground false positive movements which display lower velocities. The\nproposed feedback model provides an effective solution in robotic visual\nsystems for detecting fast-moving small targets that are always salient and\npotentially threatening.",
          "link": "http://arxiv.org/abs/2001.05846",
          "publishedOn": "2021-06-29T01:55:15.002Z",
          "wordCount": 753,
          "title": "A Time-Delay Feedback Neural Network for Discriminating Small, Fast-Moving Targets in Complex Dynamic Environments. (arXiv:2001.05846v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Monocular depth prediction plays a crucial role in understanding 3D scene\ngeometry. Although recent methods have achieved impressive progress in terms of\nevaluation metrics such as the pixel-wise relative error, most methods neglect\nthe geometric constraints in the 3D space. In this work, we show the importance\nof the high-order 3D geometric constraints for depth prediction. By designing a\nloss term that enforces a simple geometric constraint, namely, virtual normal\ndirections determined by randomly sampled three points in the reconstructed 3D\nspace, we significantly improve the accuracy and robustness of monocular depth\nestimation. Significantly, the virtual normal loss can not only improve the\nperformance of learning metric depth, but also disentangle the scale\ninformation and enrich the model with better shape information. Therefore, when\nnot having access to absolute metric depth training data, we can use virtual\nnormal to learn a robust affine-invariant depth generated on diverse scenes. In\nexperiments, We show state-of-the-art results of learning metric depth on NYU\nDepth-V2 and KITTI. From the high-quality predicted depth, we are now able to\nrecover good 3D structures of the scene such as the point cloud and surface\nnormal directly, eliminating the necessity of relying on additional models as\nwas previously done. To demonstrate the excellent generalizability of learning\naffine-invariant depth on diverse data with the virtual normal loss, we\nconstruct a large-scale and diverse dataset for training affine-invariant\ndepth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five\ndatasets with the zero-shot test setting. Code is available at:\nhttps://git.io/Depth",
          "link": "http://arxiv.org/abs/2103.04216",
          "publishedOn": "2021-06-29T01:55:14.996Z",
          "wordCount": 774,
          "title": "Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction. (arXiv:2103.04216v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nobis_F/0/1/0/all/0/1\">Felix Nobis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiei_E/0/1/0/all/0/1\">Ehsan Shafiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karle_P/0/1/0/all/0/1\">Phillip Karle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betz_J/0/1/0/all/0/1\">Johannes Betz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienkamp_M/0/1/0/all/0/1\">Markus Lienkamp</a>",
          "description": "Automotive traffic scenes are complex due to the variety of possible\nscenarios, objects, and weather conditions that need to be handled. In contrast\nto more constrained environments, such as automated underground trains,\nautomotive perception systems cannot be tailored to a narrow field of specific\ntasks but must handle an ever-changing environment with unforeseen events. As\ncurrently no single sensor is able to reliably perceive all relevant activity\nin the surroundings, sensor data fusion is applied to perceive as much\ninformation as possible. Data fusion of different sensors and sensor modalities\non a low abstraction level enables the compensation of sensor weaknesses and\nmisdetections among the sensors before the information-rich sensor data are\ncompressed and thereby information is lost after a sensor-individual object\ndetection. This paper develops a low-level sensor fusion network for 3D object\ndetection, which fuses lidar, camera, and radar data. The fusion network is\ntrained and evaluated on the nuScenes data set. On the test set, fusion of\nradar data increases the resulting AP (Average Precision) detection score by\nabout 5.1% in comparison to the baseline lidar network. The radar sensor fusion\nproves especially beneficial in inclement conditions such as rain and night\nscenes. Fusing additional camera data contributes positively only in\nconjunction with the radar fusion, which shows that interdependencies of the\nsensors are important for the detection result. Additionally, the paper\nproposes a novel loss to handle the discontinuity of a simple yaw\nrepresentation for object detection. Our updated loss increases the detection\nand orientation estimation performance for all sensor input configurations. The\ncode for this research has been made available on GitHub.",
          "link": "http://arxiv.org/abs/2106.14087",
          "publishedOn": "2021-06-29T01:55:14.991Z",
          "wordCount": 705,
          "title": "Radar Voxel Fusion for 3D Object Detection. (arXiv:2106.14087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:14.984Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "The recent vision transformer(i.e.for image classification) learns non-local\nattentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic\ncorrespondence of different labels, and the consistency of the feature\nrepresentations and semantic embeddings, which are critical for biomedical\nsegmentation. In this paper, we tackle the above issues by proposing a unified\ntransformer network, termed Multi-Compound Transformer (MCTrans), which\nincorporates rich feature learning and semantic structure mining into a unified\nframework. Specifically, MCTrans embeds the multi-scale convolutional features\nas a sequence of tokens and performs intra- and inter-scale self-attention,\nrather than single-scale attention in previous works. In addition, a learnable\nproxy embedding is also introduced to model semantic relationship and feature\nenhancement by using self-attention and cross-attention, respectively. MCTrans\ncan be easily plugged into a UNet-like network and attains a significant\nimprovement over the state-of-the-art methods in biomedical image segmentation\nin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,\n3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,\nKavirs, ISIC2018 dataset, respectively. Code is available at\nhttps://github.com/JiYuanFeng/MCTrans.",
          "link": "http://arxiv.org/abs/2106.14385",
          "publishedOn": "2021-06-29T01:55:14.978Z",
          "wordCount": 623,
          "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation. (arXiv:2106.14385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>",
          "description": "The past few years have witnessed great progress in the domain of face\nrecognition thanks to advances in deep learning. However, cross pose face\nrecognition remains a significant challenge. It is difficult for many deep\nlearning algorithms to narrow the performance gap caused by pose variations;\nthe main reasons for this relate to the intra-class discrepancy between face\nimages in different poses and the pose imbalances of training datasets.\nLearning pose-robust features by traversing to the feature space of frontal\nfaces provides an effective and cheap way to alleviate this problem. In this\npaper, we present a method for progressively transforming profile face\nrepresentations to the canonical pose with an attentive pair-wise loss.\nFirstly, to reduce the difficulty of directly transforming the profile face\nfeatures into a frontal pose, we propose to learn the feature residual between\nthe source pose and its nearby pose in a block-byblock fashion, and thus\ntraversing to the feature space of a smaller pose by adding the learned\nresidual. Secondly, we propose an attentive pair-wise loss to guide the feature\ntransformation progressing in the most effective direction. Finally, our\nproposed progressive module and attentive pair-wise loss are light-weight and\neasy to implement, adding only about 7:5% extra parameters. Evaluations on the\nCFP and CPLFW datasets demonstrate the superiority of our proposed method. Code\nis available at https://github.com/hjy1312/AGPM.",
          "link": "http://arxiv.org/abs/2106.14124",
          "publishedOn": "2021-06-29T01:55:14.970Z",
          "wordCount": 662,
          "title": "Attention-guided Progressive Mapping for Profile Face Recognition. (arXiv:2106.14124v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dan Yang</a>",
          "description": "Data augmentation is a powerful technique for improving the performance of\nthe few-shot classification task. It generates more samples as supplements, and\nthen this task can be transformed into a common supervised learning issue for\nsolution. However, most mainstream data augmentation based approaches only\nconsider the single modality information, which leads to the low diversity and\nquality of generated features. In this paper, we present a novel multi-modal\ndata augmentation approach named Dizygotic Conditional Variational AutoEncoder\n(DCVAE) for addressing the aforementioned issue. DCVAE conducts feature\nsynthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the\nsame seed but different modality conditions in a dizygotic symbiosis manner.\nSubsequently, the generated features of two CVAEs are adaptively combined to\nyield the final feature, which can be converted back into its paired conditions\nwhile ensuring these conditions are consistent with the original conditions not\nonly in representation but also in function. DCVAE essentially provides a new\nidea of data augmentation in various multi-modal scenarios by exploiting the\ncomplement of different modality prior information. Extensive experimental\nresults demonstrate our work achieves state-of-the-art performances on\nminiImageNet, CIFAR-FS and CUB datasets, and is able to work well in the\npartial modality absence case.",
          "link": "http://arxiv.org/abs/2106.14467",
          "publishedOn": "2021-06-29T01:55:14.955Z",
          "wordCount": 647,
          "title": "Dizygotic Conditional Variational AutoEncoder for Multi-Modal and Partial Modality Absent Few-Shot Learning. (arXiv:2106.14467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:14.946Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shaozuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>",
          "description": "This paper presents the Rail-5k dataset for benchmarking the performance of\nvisual algorithms in a real-world application scenario, namely the rail surface\ndefects detection task. We collected over 5k high-quality images from railways\nacross China, and annotated 1100 images with the help from railway experts to\nidentify the most common 13 types of rail defects. The dataset can be used for\ntwo settings both with unique challenges, the first is the fully-supervised\nsetting using the 1k+ labeled images for training, fine-grained nature and\nlong-tailed distribution of defect classes makes it hard for visual algorithms\nto tackle. The second is the semi-supervised learning setting facilitated by\nthe 4k unlabeled images, these 4k images are uncurated containing possible\nimage corruptions and domain shift with the labeled images, which can not be\neasily tackle by previous semi-supervised learning methods. We believe our\ndataset could be a valuable benchmark for evaluating robustness and reliability\nof visual algorithms.",
          "link": "http://arxiv.org/abs/2106.14366",
          "publishedOn": "2021-06-29T01:55:14.938Z",
          "wordCount": 593,
          "title": "Rail-5k: a Real-World Dataset for Rail Surface Defects Detection. (arXiv:2106.14366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:14.925Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jain_R/0/1/0/all/0/1\">Rohit Kumar Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaj_S/0/1/0/all/0/1\">Sibaji Gaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "Knee Osteoarthritis (OA) is a destructive joint disease identified by joint\nstiffness, pain, and functional disability concerning millions of lives across\nthe globe. It is generally assessed by evaluating physical symptoms, medical\nhistory, and other joint screening tests like radiographs, Magnetic Resonance\nImaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the\nconventional methods are very subjective, which forms a barrier in detecting\nthe disease progression at an early stage. This paper presents a deep\nlearning-based framework, namely OsteoHRNet, that automatically assesses the\nKnee OA severity in terms of Kellgren and Lawrence (KL) grade classification\nfrom X-rays. As a primary novelty, the proposed approach is built upon one of\nthe most recent deep models, called the High-Resolution Network (HRNet), to\ncapture the multi-scale features of knee X-rays. In addition, we have also\nincorporated an attention mechanism to filter out the counterproductive\nfeatures and boost the performance further. Our proposed model has achieved the\nbest multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of\nthe OAI dataset, which is a remarkable gain over the existing best-published\nworks. We have also employed the Gradient-based Class Activation Maps\n(Grad-CAMs) visualization to justify the proposed network learning.",
          "link": "http://arxiv.org/abs/2106.14292",
          "publishedOn": "2021-06-29T01:55:14.920Z",
          "wordCount": 680,
          "title": "Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network. (arXiv:2106.14292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1\">Tomasz Szandala</a>",
          "description": "In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation",
          "link": "http://arxiv.org/abs/2104.04945",
          "publishedOn": "2021-06-29T01:55:14.914Z",
          "wordCount": 653,
          "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:14.908Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1\">Praveen Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rwik Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Varun Jain</a>",
          "description": "In self driving car applications, there is a requirement to predict the\nlocation of the lane given an input RGB front facing image. In this paper, we\npropose an architecture that allows us to increase the speed and robustness of\nroad detection without a large hit in accuracy by introducing an underlying\nshared feature space that is propagated over time, which serves as a flowing\ndynamic memory. By utilizing the gist of previous frames, we train the network\nto predict the current road with a greater accuracy and lesser deviation from\nprevious frames.",
          "link": "http://arxiv.org/abs/2106.14184",
          "publishedOn": "2021-06-29T01:55:14.902Z",
          "wordCount": 518,
          "title": "Memory Guided Road Detection. (arXiv:2106.14184v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.06013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Text spotting in natural scene images is of great importance for many image\nunderstanding tasks. It includes two sub-tasks: text detection and recognition.\nIn this work, we propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nsuch as image cropping and feature re-calculation, word separation, and\ncharacter grouping.\n\nIn contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end and is able to handle text of arbitrary shapes. The\nconvolutional features are calculated only once and shared by both detection\nand recognition modules. Through multi-task training, the learned features\nbecome more discriminate and improve the overall performance. By employing the\n$2$D attention model in word recognition, the irregularity of text can be\nrobustly addressed. It provides the spatial location for each character, which\nnot only helps local feature extraction in word recognition, but also indicates\nan orientation angle to refine text localization. Our proposed method has\nachieved state-of-the-art performance on several standard text spotting\nbenchmarks, including both regular and irregular ones.",
          "link": "http://arxiv.org/abs/1906.06013",
          "publishedOn": "2021-06-29T01:55:14.897Z",
          "wordCount": 703,
          "title": "Towards End-to-End Text Spotting in Natural Scenes. (arXiv:1906.06013v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:14.887Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1\">Mete Ahishali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>",
          "description": "In this study, we propose a novel approach to predict the distances of the\ndetected objects in an observed scene. The proposed approach modifies the\nrecently proposed Convolutional Support Estimator Networks (CSENs). CSENs are\ndesigned to compute a direct mapping for the Support Estimation (SE) task in a\nrepresentation-based classification problem. We further propose and demonstrate\nthat representation-based methods (sparse or collaborative representation) can\nbe used in well-designed regression problems. To the best of our knowledge,\nthis is the first representation-based method proposed for performing a\nregression task by utilizing the modified CSENs; and hence, we name this novel\napproach as Representation-based Regression (RbR). The initial version of CSENs\nhas a proxy mapping stage (i.e., a coarse estimation for the support set) that\nis required for the input. In this study, we improve the CSEN model by\nproposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly\noptimize the so-called proxy mapping stage along with convolutional layers. The\nexperimental evaluations using the KITTI 3D Object Detection distance\nestimation dataset show that the proposed method can achieve a significantly\nimproved distance estimation performance over all competing methods. Finally,\nthe software implementations of the methods are publicly shared at\nhttps://github.com/meteahishali/CSENDistance.",
          "link": "http://arxiv.org/abs/2106.14208",
          "publishedOn": "2021-06-29T01:55:14.881Z",
          "wordCount": 642,
          "title": "Representation Based Regression for Object Distance Estimation. (arXiv:2106.14208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:14.875Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14256",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Boing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>",
          "description": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "link": "http://arxiv.org/abs/2106.14256",
          "publishedOn": "2021-06-29T01:55:14.832Z",
          "wordCount": 764,
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:14.822Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14349",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yongyang Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Wide field small aperture telescopes (WFSATs) are mainly used to obtain\nscientific information of point--like and streak--like celestial objects.\nHowever, qualities of images obtained by WFSATs are seriously affected by the\nbackground noise and variable point spread functions. Developing high speed and\nhigh efficiency data processing method is of great importance for further\nscientific research. In recent years, deep neural networks have been proposed\nfor detection and classification of celestial objects and have shown better\nperformance than classical methods. In this paper, we further extend abilities\nof the deep neural network based astronomical target detection framework to\nmake it suitable for photometry and astrometry. We add new branches into the\ndeep neural network to obtain types, magnitudes and positions of different\ncelestial objects at the same time. Tested with simulated data, we find that\nour neural network has better performance in photometry than classical methods.\nBecause photometry and astrometry are regression algorithms, which would obtain\nhigh accuracy measurements instead of rough classification results, the\naccuracy of photometry and astrometry results would be affected by different\nobservation conditions. To solve this problem, we further propose to use\nreference stars to train our deep neural network with transfer learning\nstrategy when observation conditions change. The photometry framework proposed\nin this paper could be used as an end--to--end quick data processing framework\nfor WFSATs, which can further increase response speed and scientific outputs of\nWFSATs.",
          "link": "http://arxiv.org/abs/2106.14349",
          "publishedOn": "2021-06-29T01:55:14.809Z",
          "wordCount": 715,
          "title": "The Deep Neural Network based Photometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1\">Chun-Fu</a> (Richard) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>",
          "description": "We propose a new perspective on video understanding by casting the video\nrecognition problem as an image recognition task. We show that an image\nclassifier alone can suffice for video understanding without temporal modeling.\nOur approach is simple and universal. It composes input frames into a super\nimage to train an image classifier to fulfill the task of action recognition,\nin exactly the same way as classifying an image. We prove the viability of such\nan idea by demonstrating strong and promising performance on four public\ndatasets including Kinetics400, Something-to-something (V2), MiT and Jester,\nusing a recently developed vision transformer. We also experiment with the\nprevalent ResNet image classifiers in computer vision to further validate our\nidea. The results on Kinetics400 are comparable to some of the best-performed\nCNN approaches based on spatio-temporal modeling. our code and models will be\nmade available at https://github.com/IBM/sifar-pytorch.",
          "link": "http://arxiv.org/abs/2106.14104",
          "publishedOn": "2021-06-29T01:55:14.793Z",
          "wordCount": 578,
          "title": "An Image Classifier Can Suffice Video Understanding. (arXiv:2106.14104v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:14.777Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zizheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>",
          "description": "Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in\nhuman environments is an important yet challenging task for future\nhome-assistant robots. The space of 3D articulated objects is exceptionally\nrich in their myriad semantic categories, diverse shape geometry, and\ncomplicated part functionality. Previous works mostly abstract kinematic\nstructure with estimated joint parameters and part poses as the visual\nrepresentations for manipulating 3D articulated objects. In this paper, we\npropose object-centric actionable visual priors as a novel\nperception-interaction handshaking point that the perception system outputs\nmore actionable guidance than kinematic structure estimation, by predicting\ndense geometry-aware, interaction-aware, and task-aware visual action\naffordance and trajectory proposals. We design an interaction-for-perception\nframework VAT-Mart to learn such actionable visual representations by\nsimultaneously training a curiosity-driven reinforcement learning policy\nexploring diverse interaction trajectories and a perception module summarizing\nand generalizing the explored knowledge for pointwise predictions among diverse\nshapes. Experiments prove the effectiveness of the proposed approach using the\nlarge-scale PartNet-Mobility dataset in SAPIEN environment and show promising\ngeneralization capabilities to novel test shapes, unseen object categories, and\nreal-world data. Project page: https://hyperplane-lab.github.io/vat-mart",
          "link": "http://arxiv.org/abs/2106.14440",
          "publishedOn": "2021-06-29T01:55:14.751Z",
          "wordCount": 638,
          "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murhij_Y/0/1/0/all/0/1\">Youshaa Murhij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1\">Dmitry Yudin</a>",
          "description": "In this paper, we present a real-time 3D detection approach considering\ntime-spatial feature map aggregation from different time steps of deep neural\nmodel inference (named feature map flow, FMF). Proposed approach improves the\nquality of 3D detection center-based baseline and provides real-time\nperformance on the nuScenes and Waymo benchmark. Code is available at\nhttps://github.com/YoushaaMurhij/FMFNet",
          "link": "http://arxiv.org/abs/2106.14101",
          "publishedOn": "2021-06-29T01:55:14.743Z",
          "wordCount": 502,
          "title": "Real-time 3D Object Detection using Feature Map Flow. (arXiv:2106.14101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:14.736Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "In autonomous driving, goal-based multi-trajectory prediction methods are\nproved to be effective recently, where they first score goal candidates, then\nselect a final set of goals, and finally complete trajectories based on the\nselected goals. However, these methods usually involve goal predictions based\non sparse predefined anchors. In this work, we propose an anchor-free model,\nnamed DenseTNT, which performs dense goal probability estimation for trajectory\nprediction. Our model achieves state-of-the-art performance, and ranks 1st on\nthe Waymo Open Dataset Motion Prediction Challenge.",
          "link": "http://arxiv.org/abs/2106.14160",
          "publishedOn": "2021-06-29T01:55:14.730Z",
          "wordCount": 523,
          "title": "DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Di Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaofeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Naifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>",
          "description": "In this paper, we propose a novel task, Manipulation Question Answering\n(MQA), where the robot performs manipulation actions to change the environment\nin order to answer a given question. To solve this problem, a framework\nconsisting of a QA module and a manipulation module is proposed. For the QA\nmodule, we adopt the method for the Visual Question Answering (VQA) task. For\nthe manipulation module, a Deep Q Network (DQN) model is designed to generate\nmanipulation actions for the robot to interact with the environment. We\nconsider the situation where the robot continuously manipulating objects inside\na bin until the answer to the question is found. Besides, a novel dataset that\ncontains a variety of object models, scenarios and corresponding\nquestion-answer pairs is established in a simulation environment. Extensive\nexperiments have been conducted to validate the effectiveness of the proposed\nframework.",
          "link": "http://arxiv.org/abs/2003.04641",
          "publishedOn": "2021-06-29T01:55:14.712Z",
          "wordCount": 631,
          "title": "MQA: Answering the Question via Robotic Manipulation. (arXiv:2003.04641v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>",
          "description": "Video understanding is a challenging problem with great impact on the\nabilities of autonomous agents working in the real-world. Yet, solutions so far\nhave been computationally intensive, with the fastest algorithms running for\nmore than half a second per video snippet on powerful GPUs. We propose a novel\nidea on video architecture learning - Tiny Video Networks - which automatically\ndesigns highly efficient models for video understanding. The tiny video models\nrun with competitive performance for as low as 37 milliseconds per video on a\nCPU and 10 milliseconds on a standard GPU.",
          "link": "http://arxiv.org/abs/1910.06961",
          "publishedOn": "2021-06-29T01:55:14.704Z",
          "wordCount": 548,
          "title": "Tiny Video Networks. (arXiv:1910.06961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14192",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pang_J/0/1/0/all/0/1\">Jianye Pang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yungeng Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the\nsystemic study of shape, abundance, and distribution of macromolecular\nstructures in single cells in near-atomic resolution. However, the systematic\nand efficient $\\textit{de novo}$ recognition and recovery of macromolecular\nstructures captured by Cryo-ET are very challenging due to the structural\ncomplexity and imaging limits. Even macromolecules with identical structures\nhave various appearances due to different orientations and imaging limits, such\nas noise and the missing wedge effect. Explicitly disentangling the semantic\nfeatures of macromolecules is crucial for performing several downstream\nanalyses on the macromolecules. This paper has addressed the problem by\nproposing a 3D Spatial Variational Autoencoder that explicitly disentangle the\nstructure, orientation, and shift of macromolecules. Extensive experiments on\nboth synthesized and real cryo-ET datasets and cross-domain evaluations\ndemonstrate the efficacy of our method.",
          "link": "http://arxiv.org/abs/2106.14192",
          "publishedOn": "2021-06-29T01:55:14.697Z",
          "wordCount": 572,
          "title": "Disentangling semantic features of macromolecules in Cryo-Electron Tomography. (arXiv:2106.14192v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yulun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1\">Fernando Herrera Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_Granda_C/0/1/0/all/0/1\">Carlos Nieto-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "This paper presents Kimera-Multi, the first multi-robot system that (i) is\nrobust and capable of identifying and rejecting incorrect inter and intra-robot\nloop closures resulting from perceptual aliasing, (ii) is fully distributed and\nonly relies on local (peer-to-peer) communication to achieve distributed\nlocalization and mapping, and (iii) builds a globally consistent\nmetric-semantic 3D mesh model of the environment in real-time, where faces of\nthe mesh are annotated with semantic labels. Kimera-Multi is implemented by a\nteam of robots equipped with visual-inertial sensors. Each robot builds a local\ntrajectory estimate and a local mesh using Kimera. When communication is\navailable, robots initiate a distributed place recognition and robust pose\ngraph optimization protocol based on a novel distributed graduated\nnon-convexity algorithm. The proposed protocol allows the robots to improve\ntheir local trajectory estimates by leveraging inter-robot loop closures while\nbeing robust to outliers. Finally, each robot uses its improved trajectory\nestimate to correct the local mesh using mesh deformation techniques.\n\nWe demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking\ndatasets, and challenging outdoor datasets collected using ground robots. Both\nreal and simulated experiments involve long trajectories (e.g., up to 800\nmeters per robot). The experiments show that Kimera-Multi (i) outperforms the\nstate of the art in terms of robustness and accuracy, (ii) achieves estimation\nerrors comparable to a centralized SLAM system while being fully distributed,\n(iii) is parsimonious in terms of communication bandwidth, (iv) produces\naccurate metric-semantic 3D meshes, and (v) is modular and can be also used for\nstandard 3D reconstruction (i.e., without semantic labels) or for trajectory\nestimation (i.e., without reconstructing a 3D mesh).",
          "link": "http://arxiv.org/abs/2106.14386",
          "publishedOn": "2021-06-29T01:55:14.690Z",
          "wordCount": 715,
          "title": "Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "In this paper, we propose a novel image process scheme called class-based\nexpansion learning for image classification, which aims at improving the\nsupervision-stimulation frequency for the samples of the confusing classes.\nClass-based expansion learning takes a bottom-up growing strategy in a\nclass-based expansion optimization fashion, which pays more attention to the\nquality of learning the fine-grained classification boundaries for the\npreferentially selected classes. Besides, we develop a class confusion\ncriterion to select the confusing class preferentially for training. In this\nway, the classification boundaries of the confusing classes are frequently\nstimulated, resulting in a fine-grained form. Experimental results demonstrate\nthe effectiveness of the proposed scheme on several benchmarks.",
          "link": "http://arxiv.org/abs/2106.14412",
          "publishedOn": "2021-06-29T01:55:14.683Z",
          "wordCount": 549,
          "title": "Progressive Class-based Expansion Learning For Image Classification. (arXiv:2106.14412v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_Y/0/1/0/all/0/1\">Yong Tang and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>",
          "description": "Image matting is an ill-posed problem that aims to estimate the opacity of\nforeground pixels in an image. However, most existing deep learning-based\nmethods still suffer from the coarse-grained details. In general, these\nalgorithms are incapable of felicitously distinguishing the degree of\nexploration between deterministic domains (certain FG and BG pixels) and\nundetermined domains (uncertain in-between pixels), or inevitably lose\ninformation in the continuous sampling process, leading to a sub-optimal\nresult. In this paper, we propose a novel network named Prior-Induced\nInformation Alignment Matting Network (PIIAMatting), which can efficiently\nmodel the distinction of pixel-wise response maps and the correlation of\nlayer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation\nmechanism (DGM) and an Information Alignment strategy (IA). Specifically, the\nDGM can dynamically acquire a pixel-wise domain response map learned from the\nprior distribution. The response map can present the relationship between the\nopacity variation and the convergence process during training. On the other\nhand, the IA comprises an Information Match Module (IMM) and an Information\nAggregation Module (IAM), jointly scheduled to match and aggregate the adjacent\nlayer-wise features adaptively. Besides, we also develop a Multi-Scale\nRefinement (MSR) module to integrate multi-scale receptive field information at\nthe refinement stage to recover the fluctuating appearance details. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed\nPIIAMatting performs favourably against state-of-the-art image matting methods\non the Alphamatting.com, Composition-1K and Distinctions-646 dataset.",
          "link": "http://arxiv.org/abs/2106.14439",
          "publishedOn": "2021-06-29T01:55:14.667Z",
          "wordCount": 671,
          "title": "Prior-Induced Information Alignment for Image Matting. (arXiv:2106.14439v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1\">Igor Udovichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "Several methods for inversion of face recognition models were recently\npresented, attempting to reconstruct a face from deep templates. Although some\nof these approaches work in a black-box setup using only face embeddings,\nusually, on the end-user side, only similarity scores are provided. Therefore,\nthese algorithms are inapplicable in such scenarios. We propose a novel\napproach that allows reconstructing the face querying only similarity scores of\nthe black-box model. While our algorithm operates in a more general setup,\nexperiments show that it is query efficient and outperforms the existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.14290",
          "publishedOn": "2021-06-29T01:55:14.659Z",
          "wordCount": 526,
          "title": "Darker than Black-Box: Face Reconstruction from Similarity Queries. (arXiv:2106.14290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:14.653Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis…",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:14.648Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:14.640Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Townim Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalisha_M/0/1/0/all/0/1\">Mahira Jalisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1\">Ali Cheraghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>",
          "description": "When we fine-tune a well-trained deep learning model for a new set of\nclasses, the network learns new concepts but gradually forgets the knowledge of\nold training. In some real-life applications, we may be interested in learning\nnew classes without forgetting the capability of previous experience. Such\nlearning without forgetting problem is often investigated using 2D image\nrecognition tasks. In this paper, considering the growth of depth camera\ntechnology, we address the same problem for the 3D point cloud object data.\nThis problem becomes more challenging in the 3D domain than 2D because of the\nunavailability of large datasets and powerful pretrained backbone models. We\ninvestigate knowledge distillation techniques on 3D data to reduce catastrophic\nforgetting of the previous training. Moreover, we improve the distillation\nprocess by using semantic word vectors of object classes. We observe that\nexploring the interrelation of old and new knowledge during training helps to\nlearn new concepts without forgetting old ones. Experimenting on three 3D point\ncloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic\n(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish\nnew baseline results on learning without forgetting for 3D data. This research\nwill instigate many future works in this area.",
          "link": "http://arxiv.org/abs/2106.14275",
          "publishedOn": "2021-06-29T01:55:14.621Z",
          "wordCount": 647,
          "title": "Learning without Forgetting for 3D Point Cloud Objects. (arXiv:2106.14275v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:14.564Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>",
          "description": "3D recovery from multi-stereo and stereo images, as an important application\nof the image-based perspective geometry, serves many applications in computer\nvision, remote sensing and Geomatics. In this chapter, the authors utilize the\nimaging geometry and present approaches that perform 3D reconstruction from\ncross-view images that are drastically different in their viewpoints. We\nintroduce our framework that takes ground-view images and satellite images for\nfull 3D recovery, which includes necessary methods in satellite and\nground-based point cloud generation from images, 3D data co-registration,\nfusion and mesh generation. We demonstrate our proposed framework on a dataset\nconsisting of twelve satellite images and 150k video frames acquired through a\nvehicle-mounted Go-pro camera and demonstrate the reconstruction results. We\nhave also compared our results with results generated from an intuitive\nprocessing pipeline that involves typical geo-registration and meshing methods.",
          "link": "http://arxiv.org/abs/2106.14306",
          "publishedOn": "2021-06-29T01:55:14.548Z",
          "wordCount": 569,
          "title": "3D Reconstruction through Fusion of Cross-View Images. (arXiv:2106.14306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoumzadeh_A/0/1/0/all/0/1\">Abbas Masoumzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>",
          "description": "Many deep learning based methods are designed to remove non-uniform\n(spatially variant) motion blur caused by object motion and camera shake\nwithout knowing the blur kernel. Some methods directly output the latent sharp\nimage in one stage, while others utilize a multi-stage strategy (\\eg\nmulti-scale, multi-patch, or multi-temporal) to gradually restore the sharp\nimage. However, these methods have the following two main issues: 1) The\ncomputational cost of multi-stage is high; 2) The same convolution kernel is\napplied in different regions, which is not an ideal choice for non-uniform\nblur. Hence, non-uniform motion deblurring is still a challenging and open\nproblem. In this paper, we propose a new architecture which consists of\nmultiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to\ndeblur an image end-to-end with more flexibility. Multiple ASPDC modules\nimplicitly learn the pixel-specific motion with different dilation rates in the\nsame layer to handle movements of different magnitude. To improve the training,\nwe also propose a reblurring network to map the deblurred output back to the\nblurred input, which constrains the solution space. Our experimental results\nshow that the proposed method outperforms state-of-the-art methods on the\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14336",
          "publishedOn": "2021-06-29T01:55:14.542Z",
          "wordCount": 638,
          "title": "Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:14.524Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:14.518Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1\">Hitoshi Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1\">Yasutomo Kawanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1\">Hiroshi Murase</a>",
          "description": "Multiple human tracking is a fundamental problem for scene understanding.\nAlthough both accuracy and speed are required in real-world applications,\nrecent tracking methods based on deep learning have focused on accuracy and\nrequire substantial running time. This study aims to improve running speed by\nperforming human detection at a certain frame interval because it accounts for\nmost of the running time. The question is how to maintain accuracy while\nskipping human detection. In this paper, we propose a method that complements\nthe detection results with optical flow, based on the fact that someone's\nappearance does not change much between adjacent frames. To maintain the\ntracking accuracy, we introduce robust interest point selection within human\nregions and a tracking termination metric calculated by the distribution of the\ninterest points. On the MOT20 dataset in the MOTChallenge, the proposed\nSDOF-Tracker achieved the best performance in terms of the total running speed\nwhile maintaining the MOTA metric. Our code is available at\nhttps://anonymous.4open.science/r/sdof-tracker-75AE.",
          "link": "http://arxiv.org/abs/2106.14259",
          "publishedOn": "2021-06-29T01:55:14.511Z",
          "wordCount": 601,
          "title": "SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "We propose a method of Category-level 6D Object Pose and Size Estimation\n(COPSE) from a single depth image, without external pose-annotated real-world\ntraining data. While previous works exploit visual cues in RGB(D) images, our\nmethod makes inferences based on the rich geometric information of the object\nin the depth channel alone. Essentially, our framework explores such geometric\ninformation by learning the unified 3D Orientation-Consistent Representations\n(3D-OCR) module, and further enforced by the property of Geometry-constrained\nReflection Symmetry (GeoReS) module. The magnitude information of object size\nand the center point is finally estimated by Mirror-Paired Dimensional\nEstimation (MPDE) module. Extensive experiments on the category-level NOCS\nbenchmark demonstrate that our framework competes with state-of-the-art\napproaches that require labeled real-world images. We also deploy our approach\nto a physical Baxter robot to perform manipulation tasks on unseen but\ncategory-known instances, and the results further validate the efficacy of our\nproposed model. Our videos are available in the supplementary material.",
          "link": "http://arxiv.org/abs/2106.14193",
          "publishedOn": "2021-06-29T01:55:14.465Z",
          "wordCount": 607,
          "title": "DONet: Learning Category-Level 6D Object Pose and Size Estimation from Depth Observation. (arXiv:2106.14193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>",
          "description": "Face anti-spoofing (FAS) is an indispensable and widely used module in face\nrecognition systems. Although high accuracy has been achieved, a FAS system\nwill never be perfect due to the non-stationary applied environments and the\npotential emergence of new types of presentation attacks in real-world\napplications. In practice, given a handful of labeled samples from a new\ndeployment scenario (target domain) and abundant labeled face images in the\nexisting source domain, the FAS system is expected to perform well in the new\nscenario without sacrificing the performance on the original domain. To this\nend, we identify and address a more practical problem: Few-Shot Domain\nExpansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since\nwith insufficient target domain training samples, the model may suffer from\nboth overfitting to the target domain and catastrophic forgetting of the source\ndomain. To address the problem, this paper proposes a Style transfer-based\nAugmentation for Semantic Alignment (SASA) framework. We propose to augment the\ntarget data by generating auxiliary samples based on photorealistic style\ntransfer. With the assistant of the augmented data, we further propose a\ncarefully designed mechanism to align different domains from both\ninstance-level and distribution-level, and then stabilize the performance on\nthe source domain with a less-forgetting constraint. Two benchmarks are\nproposed to simulate the FSDE-FAS scenarios, and the experimental results show\nthat the proposed SASA method outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14162",
          "publishedOn": "2021-06-29T01:55:14.458Z",
          "wordCount": 664,
          "title": "Few-Shot Domain Expansion for Face Anti-Spoofing. (arXiv:2106.14162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "We propose a novel method on refining cross-person gaze prediction task with\neye/face images only by explicitly modelling the person-specific differences.\nSpecifically, we first assume that we can obtain some initial gaze prediction\nresults with existing method, which we refer to as InitNet, and then introduce\nthree modules, the Validity Module (VM), Self-Calibration (SC) and\nPerson-specific Transform (PT)) Module. By predicting the reliability of\ncurrent eye/face images, our VM is able to identify invalid samples, e.g. eye\nblinking images, and reduce their effects in our modelling process. Our SC and\nPT module then learn to compensate for the differences on valid samples only.\nThe former models the translation offsets by bridging the gap between initial\npredictions and dataset-wise distribution. And the later learns more general\nperson-specific transformation by incorporating the information from existing\ninitial predictions of the same person. We validate our ideas on three publicly\navailable datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed\nmethod outperforms the SOTA methods significantly on all of them, e.g.\nrespectively 21.7%, 36.0% and 32.9% relative performance improvements. We won\nthe GAZE 2021 Competition on the EVE dataset. Our code can be found here\nhttps://github.com/bjj9/EVE_SCPT.",
          "link": "http://arxiv.org/abs/2106.14183",
          "publishedOn": "2021-06-29T01:55:14.396Z",
          "wordCount": 636,
          "title": "The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation. (arXiv:2106.14183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Recently, transformer has achieved remarkable performance on a variety of\ncomputer vision applications. Compared with mainstream convolutional neural\nnetworks, vision transformers are often of sophisticated architectures for\nextracting powerful feature representations, which are more difficult to be\ndeveloped on mobile devices. In this paper, we present an effective\npost-training quantization algorithm for reducing the memory storage and\ncomputational costs of vision transformers. Basically, the quantization task\ncan be regarded as finding the optimal low-bit quantization intervals for\nweights and inputs, respectively. To preserve the functionality of the\nattention mechanism, we introduce a ranking loss into the conventional\nquantization objective that aims to keep the relative order of the\nself-attention results after quantization. Moreover, we thoroughly analyze the\nrelationship between quantization loss of different layers and the feature\ndiversity, and explore a mixed-precision quantization scheme by exploiting the\nnuclear norm of each attention map and output feature. The effectiveness of the\nproposed method is verified on several benchmark models and datasets, which\noutperforms the state-of-the-art post-training quantization algorithms. For\ninstance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on\nImageNet dataset with about 8-bit quantization.",
          "link": "http://arxiv.org/abs/2106.14156",
          "publishedOn": "2021-06-29T01:55:14.390Z",
          "wordCount": 618,
          "title": "Post-Training Quantization for Vision Transformer. (arXiv:2106.14156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:14.384Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Battal_A/0/1/0/all/0/1\">Abdullah F. Al-Battal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morton_T/0/1/0/all/0/1\">Timothy Morton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+1_Y/0/1/0/all/0/1\">Yifeng Bu 1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_I/0/1/0/all/0/1\">Imanuel R Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_R/0/1/0/all/0/1\">Radhika Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "Ultrasound scanning is essential in several medical diagnostic and\ntherapeutic applications. It is used to visualize and analyze anatomical\nfeatures and structures that influence treatment plans. However, it is both\nlabor intensive, and its effectiveness is operator dependent. Real-time\naccurate and robust automatic detection and tracking of anatomical structures\nwhile scanning would significantly impact diagnostic and therapeutic procedures\nto be consistent and efficient. In this paper, we propose a deep learning\nframework to automatically detect and track a specific anatomical target\nstructure in ultrasound scans. Our framework is designed to be accurate and\nrobust across subjects and imaging devices, to operate in real-time, and to not\nrequire a large training set. It maintains a localization precision and recall\nhigher than 90% when trained on training sets that are as small as 20% in size\nof the original training set. The framework backbone is a weakly trained\nsegmentation neural network based on U-Net. We tested the framework on two\ndifferent ultrasound datasets with the aim to detect and track the Vagus nerve,\nwhere it outperformed current state-of-the-art real-time object detection\nnetworks.",
          "link": "http://arxiv.org/abs/2106.13849",
          "publishedOn": "2021-06-29T01:55:14.359Z",
          "wordCount": 671,
          "title": "A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection. (arXiv:2106.13849v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:14.344Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>",
          "description": "Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13939",
          "publishedOn": "2021-06-29T01:55:14.323Z",
          "wordCount": 582,
          "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:14.277Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:14.271Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "The geodatabase (vectorized data) nowadays becomes a rather standard digital\ncity infrastructure; however, updating geodatabase efficiently and economically\nremains a fundamental and practical issue in the geospatial industry. The cost\nof building a geodatabase is extremely high and labor intensive, and very often\nthe maps we use have several months and even years of latency. One solution is\nto develop more automated methods for (vectorized) geospatial data generation,\nwhich has been proven a difficult task in the past decades. An alternative\nsolution is to first detect the differences between the new data and the\nexisting geospatial data, and then only update the area identified as changes.\nThe second approach is becoming more favored due to its high practicality and\nflexibility. A highly relevant technique is change detection. This article aims\nto provide an overview the state-of-the-art change detection methods in the\nfield of Remote Sensing and Geomatics to support the task of updating\ngeodatabases. Data used for change detection are highly disparate, we therefore\nstructure our review intuitively based on the dimension of the data, being 1)\nchange detection with 2D data; 2) change detection with 3D data. Conclusions\nwill be drawn based on the reviewed efforts in the field, and we will share our\noutlooks of the topic of updating geodatabases.",
          "link": "http://arxiv.org/abs/2106.14309",
          "publishedOn": "2021-06-29T01:55:14.242Z",
          "wordCount": 642,
          "title": "Change Detection for Geodatabase Updating. (arXiv:2106.14309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nningchuan Xiao</a>",
          "description": "The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.",
          "link": "http://arxiv.org/abs/2106.14049",
          "publishedOn": "2021-06-29T01:55:14.227Z",
          "wordCount": 639,
          "title": "Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree Based Method. (arXiv:2106.14049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:14.219Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>",
          "description": "Current vision and language tasks usually take complete visual data (e.g.,\nraw images or videos) as input, however, practical scenarios may often consist\nthe situations where part of the visual information becomes inaccessible due to\nvarious reasons e.g., restricted view with fixed camera or intentional vision\nblock for security concerns. As a step towards the more practical application\nscenarios, we introduce a novel task that aims to describe a video using the\nnatural language dialog between two agents as a supplementary information\nsource given incomplete visual data. Different from most existing\nvision-language tasks where AI systems have full access to images or video\nclips, which may reveal sensitive information such as recognizable human faces\nor voices, we intentionally limit the visual input for AI systems and seek a\nmore secure and transparent information medium, i.e., the natural language\ndialog, to supplement the missing visual information. Specifically, one of the\nintelligent agents - Q-BOT - is given two semantic segmented frames from the\nbeginning and the end of the video, as well as a finite number of opportunities\nto ask relevant natural language questions before describing the unseen video.\nA-BOT, the other agent who has access to the entire video, assists Q-BOT to\naccomplish the goal by answering the asked questions. We introduce two\ndifferent experimental settings with either a generative (i.e., agents generate\nquestions and answers freely) or a discriminative (i.e., agents select the\nquestions and answers from candidates) internal dialog generation process. With\nthe proposed unified QA-Cooperative networks, we experimentally demonstrate the\nknowledge transfer process between the two dialog agents and the effectiveness\nof using the natural language dialog as a supplement for incomplete implicit\nvisions.",
          "link": "http://arxiv.org/abs/2106.14069",
          "publishedOn": "2021-06-29T01:55:14.202Z",
          "wordCount": 722,
          "title": "Saying the Unseen: Video Descriptions via Dialog Agents. (arXiv:2106.14069v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedi_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Riedi</a>",
          "description": "This paper overviews two interdependent issues important for mining remote\nsensing data (e.g. images) obtained from atmospheric monitoring missions. The\nfirst issue relates the building new public datasets and benchmarks, which are\nhot priority of the remote sensing community. The second issue is the\ninvestigation of deep learning methodologies for atmospheric data\nclassification based on vast amount of data without annotations and with\nlocalized annotated data provided by sparse observing networks at the surface.\nThe targeted application is air quality assessment and prediction. Air quality\nis defined as the pollution level linked with several atmospheric constituents\nsuch as gases and aerosols. There are dependency relationships between the bad\nair quality, caused by air pollution, and the public health. The target\napplication is the development of a fast prediction model for local and\nregional air quality assessment and tracking. The results of mining data will\nhave significant implication for citizen and decision makers by providing a\nfast prediction and reliable air quality monitoring system able to cover the\nlocal and regional scale through intelligent extrapolation of sparse\nground-based in situ measurement networks.",
          "link": "http://arxiv.org/abs/2106.13992",
          "publishedOn": "2021-06-29T01:55:14.196Z",
          "wordCount": 606,
          "title": "Mining atmospheric data. (arXiv:2106.13992v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixiong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>",
          "description": "Since human-labeled samples are free for the target set, unsupervised person\nre-identification (Re-ID) has attracted much attention in recent years, by\nadditionally exploiting the source set. However, due to the differences on\ncamera styles, illumination and backgrounds, there exists a large gap between\nsource domain and target domain, introducing a great challenge on cross-domain\nmatching. To tackle this problem, in this paper we propose a novel method named\nDual-stream Reciprocal Disentanglement Learning (DRDL), which is quite\nefficient in learning domain-invariant features. In DRDL, two encoders are\nfirst constructed for id-related and id-unrelated feature extractions, which\nare respectively measured by their associated classifiers. Furthermore,\nfollowed by an adversarial learning strategy, both streams reciprocally and\npositively effect each other, so that the id-related features and id-unrelated\nfeatures are completely disentangled from a given image, allowing the encoder\nto be powerful enough to obtain the discriminative but domain-invariant\nfeatures. In contrast to existing approaches, our proposed method is free from\nimage generation, which not only reduces the computational complexity\nremarkably, but also removes redundant information from id-related features.\nExtensive experiments substantiate the superiority of our proposed method\ncompared with the state-of-the-arts. The source code has been released in\nhttps://github.com/lhf12278/DRDL.",
          "link": "http://arxiv.org/abs/2106.13929",
          "publishedOn": "2021-06-29T01:55:14.191Z",
          "wordCount": 647,
          "title": "Dual-Stream Reciprocal Disentanglement Learning for Domain Adaption Person Re-Identification. (arXiv:2106.13929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>",
          "description": "The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.",
          "link": "http://arxiv.org/abs/2106.13883",
          "publishedOn": "2021-06-29T01:55:14.184Z",
          "wordCount": 617,
          "title": "Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>",
          "description": "In this paper, we propose a novel encoder, called ShapeEditor, for\nhigh-resolution, realistic and high-fidelity face exchange. First of all, in\norder to ensure sufficient clarity and authenticity, our key idea is to use an\nadvanced pretrained high-quality random face image generator, i.e. StyleGAN, as\nbackbone. Secondly, we design ShapeEditor, a two-step encoder, to make the\nswapped face integrate the identity and attribute of the input faces. In the\nfirst step, we extract the identity vector of the source image and the\nattribute vector of the target image respectively; in the second step, we map\nthe concatenation of identity vector and attribute vector into the\n$\\mathcal{W+}$ potential space. In addition, for learning to map into the\nlatent space of StyleGAN, we propose a set of self-supervised loss functions\nwith which the training data do not need to be labeled manually. Extensive\nexperiments on the test dataset show that the results of our method not only\nhave a great advantage in clarity and authenticity than other state-of-the-art\nmethods, but also reflect the sufficient integration of identity and attribute.",
          "link": "http://arxiv.org/abs/2106.13984",
          "publishedOn": "2021-06-29T01:55:14.176Z",
          "wordCount": 616,
          "title": "ShapeEditer: a StyleGAN Encoder for Face Swapping. (arXiv:2106.13984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao-zhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.",
          "link": "http://arxiv.org/abs/2106.14132",
          "publishedOn": "2021-06-29T01:55:14.149Z",
          "wordCount": 677,
          "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:14.142Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kartikeya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujit_P/0/1/0/all/0/1\">P.B. Sujit</a>",
          "description": "We present OffRoadTranSeg, the first end-to-end framework for semi-supervised\nsegmentation in unstructured outdoor environment using transformers and\nautomatic data selection for labelling. The offroad segmentation is a scene\nunderstanding approach that is widely used in autonomous driving. The popular\noffroad segmentation method is to use fully connected convolution layers and\nlarge labelled data, however, due to class imbalance, there will be several\nmismatches and also some classes may not be detected. Our approach is to do the\ntask of offroad segmentation in a semi-supervised manner. The aim is to provide\na model where self supervised vision transformer is used to fine-tune offroad\ndatasets with self-supervised data collection for labelling using depth\nestimation. The proposed method is validated on RELLIS-3D and RUGD offroad\ndatasets. The experiments show that OffRoadTranSeg outperformed other state of\nthe art models, and also solves the RELLIS-3D class imbalance problem.",
          "link": "http://arxiv.org/abs/2106.13963",
          "publishedOn": "2021-06-29T01:55:14.136Z",
          "wordCount": 581,
          "title": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments. (arXiv:2106.13963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bendre_N/0/1/0/all/0/1\">Nihar Bendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Kevin Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>",
          "description": "With the ever-increasing amount of data, the central challenge in multimodal\nlearning involves limitations of labelled samples. For the task of\nclassification, techniques such as meta-learning, zero-shot learning, and\nfew-shot learning showcase the ability to learn information about novel classes\nbased on prior knowledge. Recent techniques try to learn a cross-modal mapping\nbetween the semantic space and the image space. However, they tend to ignore\nthe local and global semantic knowledge. To overcome this problem, we propose a\nMultimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent\nspace of image features and the semantic space. In our approach we concatenate\nmultimodal data to a single embedding before passing it to the VAE for learning\nthe latent space. We propose the use of a multi-modal loss during the\nreconstruction of the feature embedding through the decoder. Our approach is\ncapable to correlating modalities and exploit the local and global semantic\nknowledge for novel sample predictions. Our experimental results using a MLP\nclassifier on four benchmark datasets show that our proposed model outperforms\nthe current state-of-the-art approaches for generalized zero-shot learning.",
          "link": "http://arxiv.org/abs/2106.14082",
          "publishedOn": "2021-06-29T01:55:14.121Z",
          "wordCount": 630,
          "title": "Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder with Semantic Concepts. (arXiv:2106.14082v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mostafa Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.",
          "link": "http://arxiv.org/abs/2106.13920",
          "publishedOn": "2021-06-29T01:55:14.116Z",
          "wordCount": 652,
          "title": "CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Ang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>",
          "description": "As a core problem in computer vision, the performance of object detection has\nimproved drastically in the past few years. Despite their impressive\nperformance, object detectors suffer from a lack of interpretability.\nVisualization techniques have been developed and widely applied to introspect\nthe decisions made by other kinds of deep learning models; however, visualizing\nobject detectors has been underexplored. In this paper, we propose using\ninversion as a primary tool to understand modern object detectors and develop\nan optimization-based approach to layout inversion, allowing us to generate\nsynthetic images recognized by trained detectors as containing a desired\nconfiguration of objects. We reveal intriguing properties of detectors by\napplying our layout inversion technique to a variety of modern object\ndetectors, and further investigate them via validation experiments: they rely\non qualitatively different features for classification and regression; they\nlearn canonical motifs of commonly co-occurring objects; they use diff erent\nvisual cues to recognize objects of varying sizes. We hope our insights can\nhelp practitioners improve object detectors.",
          "link": "http://arxiv.org/abs/2106.13933",
          "publishedOn": "2021-06-29T01:55:14.109Z",
          "wordCount": 593,
          "title": "Inverting and Understanding Object Detectors. (arXiv:2106.13933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:14.097Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:14.063Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:14.057Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nimi_S/0/1/0/all/0/1\">Sumaiya Tabassum Nimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_M/0/1/0/all/0/1\">Md Adnan Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Yusuf Sarwar Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yugyung Lee</a>",
          "description": "Collaborative inference enables resource-constrained edge devices to make\ninferences by uploading inputs (e.g., images) to a server (i.e., cloud) where\nthe heavy deep learning models run. While this setup works cost-effectively for\nsuccessful inferences, it severely underperforms when the model faces input\nsamples on which the model was not trained (known as Out-of-Distribution (OOD)\nsamples). If the edge devices could, at least, detect that an input sample is\nan OOD, that could potentially save communication and computation resources by\nnot uploading those inputs to the server for inference workload. In this paper,\nwe propose a novel lightweight OOD detection approach that mines important\nfeatures from the shallow layers of a pretrained CNN model and detects an input\nsample as ID (In-Distribution) or OOD based on a distance function defined on\nthe reduced feature space. Our technique (a) works on pretrained models without\nany retraining of those models, and (b) does not expose itself to any OOD\ndataset (all detection parameters are obtained from the ID training dataset).\nTo this end, we develop EARLIN (EARLy OOD detection for Collaborative\nINference) that takes a pretrained model and partitions the model at the OOD\ndetection layer and deploys the considerably small OOD part on an edge device\nand the rest on the cloud. By experimenting using real datasets and a prototype\nimplementation, we show that our technique achieves better results than other\napproaches in terms of overall accuracy and cost when tested against popular\nOOD datasets on top of popular deep learning models pretrained on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.13842",
          "publishedOn": "2021-06-29T01:55:14.050Z",
          "wordCount": 702,
          "title": "EARLIN: Early Out-of-Distribution Detection for Resource-efficient Collaborative Inference. (arXiv:2106.13842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1\">Andrew S. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junchi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1\">Abdeslam Boularias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_A/0/1/0/all/0/1\">Aaron M. Dollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Highly constrained manipulation tasks continue to be challenging for\nautonomous robots as they require high levels of precision, typically less than\n1mm, which is often incompatible with what can be achieved by traditional\nperception systems. This paper demonstrates that the combination of\nstate-of-the-art object tracking with passively adaptive mechanical hardware\ncan be leveraged to complete precision manipulation tasks with tight,\nindustrially-relevant tolerances (0.25mm). The proposed control method closes\nthe loop through vision by tracking the relative 6D pose of objects in the\nrelevant workspace. It adjusts the control reference of both the compliant\nmanipulator and the hand to complete object insertion tasks via within-hand\nmanipulation. Contrary to previous efforts for insertion, our method does not\nrequire expensive force sensors, precision manipulators, or time-consuming,\nonline learning, which is data hungry. Instead, this effort leverages\nmechanical compliance and utilizes an object agnostic manipulation model of the\nhand learned offline, off-the-shelf motion planning, and an RGBD-based object\ntracker trained solely with synthetic data. These features allow the proposed\nsystem to easily generalize and transfer to new tasks and environments. This\npaper describes in detail the system components and showcases its efficacy with\nextensive experiments involving tight tolerance peg-in-hole insertion tasks of\nvarious geometries as well as open-world constrained placement tasks.",
          "link": "http://arxiv.org/abs/2106.14070",
          "publishedOn": "2021-06-29T01:55:14.043Z",
          "wordCount": 658,
          "title": "Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks. (arXiv:2106.14070v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:14.024Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasileiou_V/0/1/0/all/0/1\">Vasiliki I. Vasileiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardaris_N/0/1/0/all/0/1\">Nikolaos Kardaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "Nowadays, the interaction between humans and robots is constantly expanding,\nrequiring more and more human motion recognition applications to operate in\nreal time. However, most works on temporal action detection and recognition\nperform these tasks in offline manner, i.e. temporally segmented videos are\nclassified as a whole. In this paper, based on the recently proposed framework\nof Temporal Recurrent Networks, we explore how temporal context and human\nmovement dynamics can be effectively employed for online action detection. Our\napproach uses various state-of-the-art architectures and appropriately combines\nthe extracted features in order to improve action detection. We evaluate our\nmethod on a challenging but widely used dataset for temporal action\nlocalization, THUMOS'14. Our experiments show significant improvement over the\nbaseline method, achieving state-of-the art results on THUMOS'14.",
          "link": "http://arxiv.org/abs/2106.13967",
          "publishedOn": "2021-06-29T01:55:14.017Z",
          "wordCount": 579,
          "title": "Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos. (arXiv:2106.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:14.012Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changho Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>",
          "description": "In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.",
          "link": "http://arxiv.org/abs/2106.13953",
          "publishedOn": "2021-06-29T01:55:14.003Z",
          "wordCount": 623,
          "title": "In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14033",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "The recurrent mechanism has recently been introduced into U-Net in various\nmedical image segmentation tasks. Existing studies have focused on promoting\nnetwork recursion via reusing building blocks. Although network parameters\ncould be greatly saved, computational costs still increase inevitably in\naccordance with the pre-set iteration time. In this work, we study a\nmulti-scale upgrade of a bi-directional skip connected network and then\nautomatically discover an efficient architecture by a novel two-phase Neural\nArchitecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method\nreduces the network computational cost by sifting out ineffective multi-scale\nfeatures at different levels and iterations. We evaluate BiX-NAS on two\nsegmentation tasks using three different medical image datasets, and the\nexperimental results show that our BiX-NAS searched architecture achieves the\nstate-of-the-art performance with significantly lower computational cost.",
          "link": "http://arxiv.org/abs/2106.14033",
          "publishedOn": "2021-06-29T01:55:13.988Z",
          "wordCount": 584,
          "title": "BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation. (arXiv:2106.14033v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Traditionally, CNN models possess hierarchical structures and utilize the\nfeature mapping of the last layer to obtain the prediction output. However, it\ncan be difficulty to settle the optimal network depth and make the middle\nlayers learn distinguished features. This paper proposes the Interflow\nalgorithm specially for traditional CNN models. Interflow divides CNNs into\nseveral stages according to the depth and makes predictions by the feature\nmappings in each stage. Subsequently, we input these prediction branches into a\nwell-designed attention module, which learns the weights of these prediction\nbranches, aggregates them and obtains the final output. Interflow weights and\nfuses the features learned in both shallower and deeper layers, making the\nfeature information at each stage processed reasonably and effectively,\nenabling the middle layers to learn more distinguished features, and enhancing\nthe model representation ability. In addition, Interflow can alleviate gradient\nvanishing problem, lower the difficulty of network depth selection, and lighten\npossible over-fitting problem by introducing attention mechanism. Besides, it\ncan avoid network degradation as a byproduct. Compared with the original model,\nthe CNN model with Interflow achieves higher test accuracy on multiple\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14073",
          "publishedOn": "2021-06-29T01:55:13.979Z",
          "wordCount": 621,
          "title": "Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:13.973Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:13.960Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thao_N/0/1/0/all/0/1\">Nguyen Hieu Thao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soloviev_O/0/1/0/all/0/1\">Oleg Soloviev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noom_J/0/1/0/all/0/1\">Jacques Noom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhaegen_M/0/1/0/all/0/1\">Michel Verhaegen</a>",
          "description": "We propose and study the single-frame anisoplanatic deconvolution problem\nassociated with image classification using machine learning algorithms, named\nthe nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR\nproblem is done and the so-called defocus removal (DR) algorithm for solving it\nis proposed. Global convergence of the DR algorithm is established without\nimposing any unverifiable assumption. Numerical results on simulation data show\nsignificant features of DR including solvability, noise robustness,\nconvergence, model insensitivity and computational efficiency. Physical\nrelevance of the NDR problem and practicability of the DR algorithm are tested\non experimental data. Back to the application that originally motivated the\ninvestigation of the NDR problem, we show that the DR algorithm can improve the\naccuracy of classifying distorted images using convolutional neural networks.\nThe key difference of this paper compared to most existing works on\nsingle-frame anisoplanatic deconvolution is that the new method does not\nrequire the data image to be decomposable into isoplanatic subregions.\nTherefore, solution approaches partitioning the image into isoplanatic zones\nare not applicable to the NDR problem and those handling the entire image such\nas the DR algorithm need to be developed and analyzed.",
          "link": "http://arxiv.org/abs/2106.13864",
          "publishedOn": "2021-06-29T01:55:13.936Z",
          "wordCount": 637,
          "title": "Nonuniform Defocus Removal for Image Classification. (arXiv:2106.13864v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:13.930Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbad_Z/0/1/0/all/0/1\">Zakariae Abbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maliani_A/0/1/0/all/0/1\">Ahmed Drissi El Maliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaoui_S/0/1/0/all/0/1\">Said Ouatik El Alaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassouni_M/0/1/0/all/0/1\">Mohammed El Hassouni</a>",
          "description": "In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.",
          "link": "http://arxiv.org/abs/2106.14060",
          "publishedOn": "2021-06-29T01:55:13.923Z",
          "wordCount": 638,
          "title": "A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval. (arXiv:2106.14060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:13.916Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Riko Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1\">Koji Mineshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekki_D/0/1/0/all/0/1\">Daisuke Bekki</a>",
          "description": "This paper introduces a new video-and-language dataset with human actions for\nmultimodal logical inference, which focuses on intentional and aspectual\nexpressions that describe dynamic human actions. The dataset consists of 200\nvideos, 5,554 action labels, and 1,942 action triplets of the form <subject,\npredicate, object> that can be translated into logical semantic\nrepresentations. The dataset is expected to be useful for evaluating multimodal\ninference systems between videos and semantically complicated sentences\nincluding negation and quantification.",
          "link": "http://arxiv.org/abs/2106.14137",
          "publishedOn": "2021-06-29T01:55:13.898Z",
          "wordCount": 522,
          "title": "Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference. (arXiv:2106.14137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_A/0/1/0/all/0/1\">Arturo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trullo_R/0/1/0/all/0/1\">Roger Trullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wielhorski_Y/0/1/0/all/0/1\">Yanneck Wielhorski</a>",
          "description": "In this work we propose a novel and fully automated method for extracting the\nyarn geometrical features in woven composites so that a direct parametrization\nof the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not\nonly to perform yarn segmentation from tomographic images but rather to provide\na complete descriptive modeling of the fabric. As such, this direct approach\nimproves on previous methods that use voxel-wise masks as intermediate\nrepresentations followed by re-meshing operations (yarn envelope estimation).\nThe proposed approach employs two deep neural network architectures (U-Net and\nMask RCNN). First, we train the U-Net to generate synthetic CT images from the\ncorresponding FE simulations. This allows to generate large quantities of\nannotated data without requiring costly manual annotations. This data is then\nused to train the Mask R-CNN, which is focused on predicting contour points\naround each of the yarns in the image. Experimental results show that our\nmethod is accurate and robust for performing yarn instance segmentation on CT\nimages, this is further validated by quantitative and qualitative analyses.",
          "link": "http://arxiv.org/abs/2106.13982",
          "publishedOn": "2021-06-29T01:55:13.881Z",
          "wordCount": 630,
          "title": "Descriptive Modeling of Textiles using FE Simulations and Deep Learning. (arXiv:2106.13982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortinhal_T/0/1/0/all/0/1\">Tiago Cortinhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurnaz_F/0/1/0/all/0/1\">Fatih Kurnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Aksoy</a>",
          "description": "In this work, we present a simple yet effective framework to address the\ndomain translation problem between different sensor modalities with unique data\nformats. By relying only on the semantics of the scene, our modular generative\nframework can, for the first time, synthesize a panoramic color image from a\ngiven full 3D LiDAR point cloud. The framework starts with semantic\nsegmentation of the point cloud, which is initially projected onto a spherical\nsurface. The same semantic segmentation is applied to the corresponding camera\nimage. Next, our new conditional generative model adversarially learns to\ntranslate the predicted LiDAR segment maps to the camera image counterparts.\nFinally, generated image segments are processed to render the panoramic scene\nimages. We provide a thorough quantitative evaluation on the SemanticKitti\ndataset and show that our proposed framework outperforms other strong baseline\nmodels.\n\nOur source code is available at\nhttps://github.com/halmstad-University/TITAN-NET",
          "link": "http://arxiv.org/abs/2106.13974",
          "publishedOn": "2021-06-29T01:55:13.874Z",
          "wordCount": 585,
          "title": "Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images. (arXiv:2106.13974v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1\">Nikita Durasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Deep neural networks have amply demonstrated their prowess but estimating the\nreliability of their predictions remains challenging. Deep Ensembles are widely\nconsidered as being one of the best methods for generating uncertainty\nestimates but are very expensive to train and evaluate. MC-Dropout is another\npopular alternative, which is less expensive, but also less reliable. Our\ncentral intuition is that there is a continuous spectrum of ensemble-like\nmodels of which MC-Dropout and Deep Ensembles are extreme examples. The first\nuses an effectively infinite number of highly correlated models while the\nsecond relies on a finite number of independent models.\n\nTo combine the benefits of both, we introduce Masksembles. Instead of\nrandomly dropping parts of the network as in MC-dropout, Masksemble relies on a\nfixed number of binary masks, which are parameterized in a way that allows to\nchange correlations between individual models. Namely, by controlling the\noverlap between the masks and their density one can choose the optimal\nconfiguration for the task at hand. This leads to a simple and easy to\nimplement method with performance on par with Ensembles at a fraction of the\ncost. We experimentally validate Masksembles on two widely used datasets,\nCIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.08334",
          "publishedOn": "2021-06-28T01:57:56.209Z",
          "wordCount": 672,
          "title": "Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.11752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Ting-Wu Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1\">Diana Marculescu</a>",
          "description": "Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.",
          "link": "http://arxiv.org/abs/2007.11752",
          "publishedOn": "2021-06-28T01:57:56.052Z",
          "wordCount": 730,
          "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.00143",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Welk_M/0/1/0/all/0/1\">Martin Welk</a>",
          "description": "Having been studied since long by statisticians, multivariate median concepts\nfound their way into the image processing literature in the course of the last\ndecades, being used to construct robust and efficient denoising filters for\nmultivariate images such as colour images but also matrix-valued images. Based\non the similarities between image and geometric data as results of the sampling\nof continuous physical quantities, it can be expected that the understanding of\nmultivariate median filters for images provides a starting point for the\ndevelopment of shape processing techniques. This paper presents an overview of\nmultivariate median concepts relevant for image and shape processing. It\nfocusses on their mathematical principles and discusses important properties\nespecially in the context of image processing.",
          "link": "http://arxiv.org/abs/1911.00143",
          "publishedOn": "2021-06-28T01:57:55.954Z",
          "wordCount": 589,
          "title": "Multivariate Medians for Image and Shape Analysis. (arXiv:1911.00143v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balbastre_Y/0/1/0/all/0/1\">Yael Balbastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzarito_M/0/1/0/all/0/1\">Michela Azzarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_C/0/1/0/all/0/1\">Christian Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callaghan_M/0/1/0/all/0/1\">Martina F. Callaghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashburner_J/0/1/0/all/0/1\">John Ashburner</a>",
          "description": "Quantitative MR imaging is increasingly favoured for its richer information\ncontent and standardised measures. However, computing quantitative parameter\nmaps, such as those encoding longitudinal relaxation rate (R1), apparent\ntransverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),\ninvolves inverting a highly non-linear function. Many methods for deriving\nparameter maps assume perfect measurements and do not consider how noise is\npropagated through the estimation procedure, resulting in needlessly noisy\nmaps. Instead, we propose a probabilistic generative (forward) model of the\nentire dataset, which is formulated and inverted to jointly recover (log)\nparameter maps with a well-defined probabilistic interpretation (e.g., maximum\nlikelihood or maximum a posteriori). The second order optimisation we propose\nfor model fitting achieves rapid and stable convergence thanks to a novel\napproximate Hessian. We demonstrate the utility of our flexible framework in\nthe context of recovering more accurate maps from data acquired using the\npopular multi-parameter mapping protocol. We also show how to incorporate a\njoint total variation prior to further decrease the noise in the maps, noting\nthat the probabilistic formulation allows the uncertainty on the recovered\nparameter maps to be estimated. Our implementation uses a PyTorch backend and\nbenefits from GPU acceleration. It is available at\nhttps://github.com/balbasty/nitorch.",
          "link": "http://arxiv.org/abs/2102.01604",
          "publishedOn": "2021-06-28T01:57:55.936Z",
          "wordCount": 680,
          "title": "Model-based multi-parameter mapping. (arXiv:2102.01604v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n\nThis paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.",
          "link": "http://arxiv.org/abs/2106.09862",
          "publishedOn": "2021-06-28T01:57:55.764Z",
          "wordCount": 686,
          "title": "Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Transformer in computer vision has recently shown encouraging progress. In\nthis work, we improve the original Pyramid Vision Transformer (PVTv1) by adding\nthree improvement designs, which include (1) locally continuous features with\nconvolutions, (2) position encodings with zero paddings, and (3) linear\ncomplexity attention layers with average pooling. With these simple\nmodifications, our PVTv2 significantly improves PVTv1 on classification,\ndetection, and segmentation. Moreover, PVTv2 achieves much better performance\nthan recent works, including Swin Transformer, under ImageNet-1K pre-training.\nWe hope this work will make state-of-the-art vision Transformer research more\naccessible. Code is available at https://github.com/whai362/PVT .",
          "link": "http://arxiv.org/abs/2106.13797",
          "publishedOn": "2021-06-28T01:57:55.715Z",
          "wordCount": 543,
          "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1\">Thomas Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadlbauer_P/0/1/0/all/0/1\">Pascal Stadlbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1\">Mathias Parger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_A/0/1/0/all/0/1\">Andreas Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Joerg H. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_C/0/1/0/all/0/1\">Chakravarty R. Alla Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplanyan_A/0/1/0/all/0/1\">Anton Kaplanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>",
          "description": "The recent research explosion around implicit neural representations, such as\nNeRF, shows that there is immense potential for implicitly storing high-quality\nscene and lighting information in compact neural networks. However, one major\nlimitation preventing the use of NeRF in real-time rendering applications is\nthe prohibitive computational cost of excessive network evaluations along each\nview ray, requiring dozens of petaFLOPS. In this work, we bring compact neural\nrepresentations closer to practical rendering of synthetic content in real-time\napplications, such as games and virtual reality. We show that the number of\nsamples required for each view ray can be significantly reduced when samples\nare placed around surfaces in the scene without compromising image quality. To\nthis end, we propose a depth oracle network that predicts ray sample locations\nfor each view ray with a single network evaluation. We show that using a\nclassification network around logarithmically discretized and spherically\nwarped depth values is essential to encode surface locations rather than\ndirectly estimating depth. The combination of these techniques leads to DONeRF,\nour compact dual network design with a depth oracle network as its first step\nand a locally sampled shading network for ray accumulation. With DONeRF, we\nreduce the inference costs by up to 48x compared to NeRF when conditioning on\navailable ground truth depth information. Compared to concurrent acceleration\nmethods for raymarching-based neural representations, DONeRF does not require\nadditional memory for explicit caching or acceleration structures, and can\nrender interactively (20 frames per second) on a single GPU.",
          "link": "http://arxiv.org/abs/2103.03231",
          "publishedOn": "2021-06-28T01:57:55.677Z",
          "wordCount": 766,
          "title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. (arXiv:2103.03231v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.09734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Na Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuan-Hai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huajun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu-Ting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1\">Naihua Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Nai-Yang Deng</a>",
          "description": "Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.",
          "link": "http://arxiv.org/abs/1910.09734",
          "publishedOn": "2021-06-28T01:57:55.627Z",
          "wordCount": 616,
          "title": "Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08886",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jinyuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Reconstructing magnetic resonance (MR) images from undersampled data is a\nchallenging problem due to various artifacts introduced by the under-sampling\noperation. Recent deep learning-based methods for MR image reconstruction\nusually leverage a generic auto-encoder architecture which captures low-level\nfeatures at the initial layers and high-level features at the deeper layers.\nSuch networks focus much on global features which may not be optimal to\nreconstruct the fully-sampled image. In this paper, we propose an\nOver-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which\nconsists of an overcomplete and an undercomplete Convolutional Recurrent Neural\nNetwork(CRNN). The overcomplete branch gives special attention in learning\nlocal structures by restraining the receptive field of the network. Combining\nit with the undercomplete branch leads to a network which focuses more on\nlow-level features without losing out on the global structures. Extensive\nexperiments on two datasets demonstrate that the proposed method achieves\nsignificant improvements over the compressed sensing and popular deep\nlearning-based methods with less number of trainable parameters.",
          "link": "http://arxiv.org/abs/2106.08886",
          "publishedOn": "2021-06-28T01:57:55.574Z",
          "wordCount": 632,
          "title": "Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "Obtaining a useful estimate of an object from highly incomplete imaging\nmeasurements remains a holy grail of imaging science. Deep learning methods\nhave shown promise in learning object priors or constraints to improve the\nconditioning of an ill-posed imaging inverse problem. In this study, a\nframework for estimating an object of interest that is semantically related to\na known prior image, is proposed. An optimization problem is formulated in the\ndisentangled latent space of a style-based generative model, and semantically\nmeaningful constraints are imposed using the disentangled latent representation\nof the prior image. Stable recovery from incomplete measurements with the help\nof a prior image is theoretically analyzed. Numerical experiments demonstrating\nthe superior performance of our approach as compared to related methods are\npresented.",
          "link": "http://arxiv.org/abs/2102.12525",
          "publishedOn": "2021-06-28T01:57:55.566Z",
          "wordCount": 603,
          "title": "Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1\">Anthony Yezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>",
          "description": "In current clinical practice, noisy and artifact-ridden weekly cone-beam\ncomputed tomography (CBCT) images are only used for patient setup during\nradiotherapy. Treatment planning is done once at the beginning of the treatment\nusing high-quality planning CT (pCT) images and manual contours for\norgans-at-risk (OARs) structures. If the quality of the weekly CBCT images can\nbe improved while simultaneously segmenting OAR structures, this can provide\ncritical information for adapting radiotherapy mid-treatment as well as for\nderiving biomarkers for treatment response. Using a novel physics-based data\naugmentation strategy, we synthesize a large dataset of perfectly/inherently\nregistered planning CT and synthetic-CBCT pairs for locally advanced lung\ncancer patient cohort, which are then used in a multitask 3D deep learning\nframework to simultaneously segment and translate real weekly CBCT images to\nhigh-quality planning CT-like images. We compared the synthetic CT and OAR\nsegmentations generated by the model to real planning CT and manual OAR\nsegmentations and showed promising results. The real week 1 (baseline) CBCT\nimages which had an average MAE of 162.77 HU compared to pCT images are\ntranslated to synthetic CT images that exhibit a drastically improved average\nMAE of 29.31 HU and average structural similarity of 92% with the pCT images.\nThe average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,\nheart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow\nclinicians to adjust treatment plans using only the routine low-quality CBCT\nimages, potentially improving patient outcomes. Our code, data, and pre-trained\nmodels will be made available via our physics-based data augmentation library,\nPhysics-ArX, at https://github.com/nadeemlab/Physics-ArX.",
          "link": "http://arxiv.org/abs/2103.05690",
          "publishedOn": "2021-06-28T01:57:55.546Z",
          "wordCount": 750,
          "title": "Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1\">Esther Bron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessen_W/0/1/0/all/0/1\">Wiro Niessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1\">Eppo Wolvius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1\">Gennady Roshchupkin</a>",
          "description": "Confounding bias is a crucial problem when applying machine learning to\npractice, especially in clinical practice. We consider the problem of learning\nrepresentations independent to multiple biases. In literature, this is mostly\nsolved by purging the bias information from learned representations. We however\nexpect this strategy to harm the diversity of information in the\nrepresentation, and thus limiting its prospective usage (e.g., interpretation).\nTherefore, we propose to mitigate the bias while keeping almost all information\nin the latent representations, which enables us to observe and interpret them\nas well. To achieve this, we project latent features onto a learned vector\ndirection, and enforce the independence between biases and projected features\nrather than all learned features. To interpret the mapping between projected\nfeatures and input data, we propose projection-wise disentangling: a sampling\nand reconstruction along the learned vector direction. The proposed method was\nevaluated on the analysis of 3D facial shape and patient characteristics\n(N=5011). Experiments showed that this conceptually simple method achieved\nstate-of-the-art fair prediction performance and interpretability, showing its\ngreat potential for clinical applications.",
          "link": "http://arxiv.org/abs/2106.13734",
          "publishedOn": "2021-06-28T01:57:55.532Z",
          "wordCount": 630,
          "title": "Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis. (arXiv:2106.13734v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1\">Max Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchheim_B/0/1/0/all/0/1\">Benito Buchheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1\">Amir Semmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1\">J&#xfc;rgen D&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Matthias Trapp</a>",
          "description": "We present StyleTune, a mobile app for interactive multi-level control of\nneural style transfers that facilitates creative adjustments of style elements\nand enables high output fidelity. In contrast to current mobile neural style\ntransfer apps, StyleTune supports users to adjust both the size and orientation\nof style elements, such as brushstrokes and texture patches, on a global as\nwell as local level. To this end, we propose a novel stroke-adaptive\nfeed-forward style transfer network, that enables control over stroke size and\nintensity and allows a larger range of edits than current approaches. For\nadditional level-of-control, we propose a network agnostic method for\nstroke-orientation adjustment by utilizing the rotation-variance of CNNs. To\nachieve high output fidelity, we further add a patch-based style transfer\nmethod that enables users to obtain output resolutions of more than 20\nMegapixel. Our approach empowers users to create many novel results that are\nnot possible with current mobile neural style transfer apps.",
          "link": "http://arxiv.org/abs/2106.13787",
          "publishedOn": "2021-06-28T01:57:55.447Z",
          "wordCount": 598,
          "title": "Interactive Multi-level Stroke Control for Neural Style Transfer. (arXiv:2106.13787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tszhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_K/0/1/0/all/0/1\">Kun Bai</a>",
          "description": "Deep Convolutional Neural Networks (DCNNs) and their variants have been\nwidely used in large scale face recognition(FR) recently. Existing methods have\nachieved good performance on many FR benchmarks. However, most of them suffer\nfrom two major problems. First, these methods converge quite slowly since they\noptimize the loss functions in a high-dimensional and sparse Gaussian Sphere.\nSecond, the high dimensionality of features, despite the powerful descriptive\nability, brings difficulty to the optimization, which may lead to a sub-optimal\nlocal optimum. To address these problems, we propose a simple yet efficient\ntraining mechanism called MultiFace, where we approximate the original\nhigh-dimensional features by the ensemble of low-dimensional features. The\nproposed mechanism is also generic and can be easily applied to many advanced\nFR models. Moreover, it brings the benefits of good interpretability to FR\nmodels via the clustering effect. In detail, the ensemble of these\nlow-dimensional features can capture complementary yet discriminative\ninformation, which can increase the intra-class compactness and inter-class\nseparability. Experimental results show that the proposed mechanism can\naccelerate 2-3 times with the softmax loss and 1.2-1.5 times with Arcface or\nCosface, while achieving state-of-the-art performances in several benchmark\ndatasets. Especially, the significant improvements on large-scale\ndatasets(e.g., IJB and MageFace) demonstrate the flexibility of our new\ntraining mechanism.",
          "link": "http://arxiv.org/abs/2101.09899",
          "publishedOn": "2021-06-28T01:57:55.435Z",
          "wordCount": 693,
          "title": "MultiFace: A Generic Training Mechanism for Boosting Face Recognition Performance. (arXiv:2101.09899v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1\">Noorul Wahab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1\">Islam M Miligy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1\">Katherine Dodd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1\">Harvir Sahota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1\">Michael Toss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wenqi Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Young Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1\">Giorgos Hadjigeorghiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1\">Abhir Bhalerao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1\">Ayat Lashen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1\">Asmaa Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1\">Ayaka Katayama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1\">Henry O Ebili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1\">Matthew Parkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1\">Tom Sorell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1\">Emily Hero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1\">Hesham Eldaly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1\">Yee Wah Tsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kishore Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1\">Emad Rakha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>",
          "description": "Recent advances in whole slide imaging (WSI) technology have led to the\ndevelopment of a myriad of computer vision and artificial intelligence (AI)\nbased diagnostic, prognostic, and predictive algorithms. Computational\nPathology (CPath) offers an integrated solution to utilize information embedded\nin pathology WSIs beyond what we obtain through visual assessment. For\nautomated analysis of WSIs and validation of machine learning (ML) models,\nannotations at the slide, tissue and cellular levels are required. The\nannotation of important visual constructs in pathology images is an important\ncomponent of CPath projects. Improper annotations can result in algorithms\nwhich are hard to interpret and can potentially produce inaccurate and\ninconsistent results. Despite the crucial role of annotations in CPath\nprojects, there are no well-defined guidelines or best practices on how\nannotations should be carried out. In this paper, we address this shortcoming\nby presenting the experience and best practices acquired during the execution\nof a large-scale annotation exercise involving a multidisciplinary team of\npathologists, ML experts and researchers as part of the Pathology image data\nLake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a\nreal-world case study along with examples of different types of annotations,\ndiagnostic algorithm, annotation data dictionary and annotation constructs. The\nanalyses reported in this work highlight best practice recommendations that can\nbe used as annotation guidelines over the lifecycle of a CPath project.",
          "link": "http://arxiv.org/abs/2106.13689",
          "publishedOn": "2021-06-28T01:57:55.427Z",
          "wordCount": 729,
          "title": "Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandivarapu_J/0/1/0/all/0/1\">Jaya Krishna Mandivarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunch_E/0/1/0/all/0/1\">Eric Bunch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Qian You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Glenn Fung</a>",
          "description": "Document image classification remains a popular research area because it can\nbe commercialized in many enterprise applications across different industries.\nRecent advancements in large pre-trained computer vision and language models\nand graph neural networks has lent document image classification many tools.\nHowever using large pre-trained models usually requires substantial computing\nresources which could defeat the cost-saving advantages of automatic document\nimage classification. In the paper we propose an efficient document image\nclassification framework that uses graph convolution neural networks and\nincorporates textual, visual and layout information of the document. We have\nrigorously benchmarked our proposed algorithm against several state-of-art\nvision and language models on both publicly available dataset and a real-life\ninsurance document classification dataset. Empirical results on both publicly\navailable and real-world data show that our methods achieve near SOTA\nperformance yet require much less computing resources and time for model\ntraining and inference. This results in solutions than offer better cost\nadvantages, especially in scalable deployment for enterprise applications. The\nresults showed that our algorithm can achieve classification performance quite\nclose to SOTA. We also provide comprehensive comparisons of computing\nresources, model sizes, train and inference time between our proposed methods\nand baselines. In addition we delineate the cost per image using our method and\nother baselines.",
          "link": "http://arxiv.org/abs/2106.13802",
          "publishedOn": "2021-06-28T01:57:55.419Z",
          "wordCount": 649,
          "title": "Efficient Document Image Classification Using Region-Based Graph Neural Network. (arXiv:2106.13802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>",
          "description": "We propose a theoretical approach towards the training numerical stability of\nVariational AutoEncoders (VAE). Our work is motivated by recent studies\nempowering VAEs to reach state of the art generative results on complex image\ndatasets. These very deep VAE architectures, as well as VAEs using more complex\noutput distributions, highlight a tendency to haphazardly produce high training\ngradients as well as NaN losses. The empirical fixes proposed to train them\ndespite their limitations are neither fully theoretically grounded nor\ngenerally sufficient in practice. Building on this, we localize the source of\nthe problem at the interface between the model's neural networks and their\noutput probabilistic distributions. We explain a common source of instability\nstemming from an incautious formulation of the encoded Normal distribution's\nvariance, and apply the same approach on other, less obvious sources. We show\nthat by implementing small changes to the way we parameterize the Normal\ndistributions on which they rely, VAEs can securely be trained.",
          "link": "http://arxiv.org/abs/2106.13739",
          "publishedOn": "2021-06-28T01:57:55.399Z",
          "wordCount": 583,
          "title": "Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yudong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongjin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>",
          "description": "Generating high-fidelity talking head video by fitting with the input audio\nsequence is a challenging problem that receives considerable attentions\nrecently. In this paper, we address this problem with the aid of neural scene\nrepresentation networks. Our method is completely different from existing\nmethods that rely on intermediate representations like 2D landmarks or 3D face\nmodels to bridge the gap between audio input and video output. Specifically,\nthe feature of input audio signal is directly fed into a conditional implicit\nfunction to generate a dynamic neural radiance field, from which a\nhigh-fidelity talking-head video corresponding to the audio signal is\nsynthesized using volume rendering. Another advantage of our framework is that\nnot only the head (with hair) region is synthesized as previous methods did,\nbut also the upper body is generated via two individual neural radiance fields.\nExperimental results demonstrate that our novel framework can (1) produce\nhigh-fidelity and natural results, and (2) support free adjustment of audio\nsignals, viewing directions, and background images.",
          "link": "http://arxiv.org/abs/2103.11078",
          "publishedOn": "2021-06-28T01:57:55.390Z",
          "wordCount": 648,
          "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. (arXiv:2103.11078v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maeoki_S/0/1/0/all/0/1\">Sho Maeoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "In this paper we undertake the task of text-based video moment retrieval from\na corpus of videos. To train the model, text-moment paired datasets were used\nto learn the correct correspondences. In typical training methods, ground-truth\ntext-moment pairs are used as positive pairs, whereas other pairs are regarded\nas negative pairs. However, aside from the ground-truth pairs, some text-moment\npairs should be regarded as positive. In this case, one text annotation can be\npositive for many video moments. Conversely, one video moment can be\ncorresponded to many text annotations. Thus, there are many-to-many\ncorrespondences between the text annotations and video moments. Based on these\ncorrespondences, we can form potentially relevant pairs, which are not given as\nground truth yet are not negative; effectively incorporating such relevant\npairs into training can improve the retrieval performance. The text query\nshould describe what is happening in a video moment. Hence, different video\nmoments annotated with similar texts, which contain a similar action, are\nlikely to hold the similar action, thus these pairs can be considered as\npotentially relevant pairs. In this paper, we propose a novel training method\nthat takes advantage of potentially relevant pairs, which are detected based on\nlinguistic analysis about text annotation. Experiments on two benchmark\ndatasets revealed that our method improves the retrieval performance both\nquantitatively and qualitatively.",
          "link": "http://arxiv.org/abs/2106.13566",
          "publishedOn": "2021-06-28T01:57:55.383Z",
          "wordCount": 668,
          "title": "Video Moment Retrieval with Text Query Considering Many-to-Many Correspondence Using Potentially Relevant Pair. (arXiv:2106.13566v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roma_H/0/1/0/all/0/1\">Hessi Roma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amelia_J/0/1/0/all/0/1\">James Amelia</a>",
          "description": "The performance of image recognition like human pose detection, trained with\nsimulated images would usually get worse due to the divergence between real and\nsimulated data. To make the distribution of a simulated image close to that of\nreal one, there are several works applying GAN-based image-to-image\ntransformation methods, e.g., SimGAN and CycleGAN. However, these methods would\nnot be sensitive enough to the various change in pose and shape of subjects,\nespecially when the training data are imbalanced, e.g., some particular poses\nand shapes are minor in the training data. To overcome this problem, we propose\nto introduce the label information of subjects, e.g., pose and type of objects\nin the training of CycleGAN, and lead it to obtain label-wise transforamtion\nmodels. We evaluate our proposed method called Label-CycleGAN, through\nexperiments on the digit image transformation from SVHN to MNIST and the\nsurveillance camera image transformation from simulated to real images.",
          "link": "http://arxiv.org/abs/2106.13696",
          "publishedOn": "2021-06-28T01:57:55.361Z",
          "wordCount": 579,
          "title": "Image-to-image Transformation with Auxiliary Condition. (arXiv:2106.13696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingnan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "We consider a category-level perception problem, where one is given 3D sensor\ndata picturing an object of a given category (e.g. a car), and has to\nreconstruct the pose and shape of the object despite intra-class variability\n(i.e. different car models have different shapes). We consider an active shape\nmodel, where -- for an object category -- we are given a library of potential\nCAD models describing objects in that category, and we adopt a standard\nformulation where pose and shape estimation are formulated as a non-convex\noptimization. Our first contribution is to provide the first certifiably\noptimal solver for pose and shape estimation. In particular, we show that\nrotation estimation can be decoupled from the estimation of the object\ntranslation and shape, and we demonstrate that (i) the optimal object rotation\ncan be computed via a tight (small-size) semidefinite relaxation, and (ii) the\ntranslation and shape parameters can be computed in closed-form given the\nrotation. Our second contribution is to add an outlier rejection layer to our\nsolver, hence making it robust to a large number of misdetections. Towards this\ngoal, we wrap our optimal solver in a robust estimation scheme based on\ngraduated non-convexity. To further enhance robustness to outliers, we also\ndevelop the first graph-theoretic formulation to prune outliers in\ncategory-level perception, which removes outliers via convex hull and maximum\nclique computations; the resulting approach is robust to 70%-90% outliers. Our\nthird contribution is an extensive experimental evaluation. Besides providing\nan ablation study on a simulated dataset and on the PASCAL3D+ dataset, we\ncombine our solver with a deep-learned keypoint detector, and show that the\nresulting approach improves over the state of the art in vehicle pose\nestimation in the ApolloScape datasets.",
          "link": "http://arxiv.org/abs/2104.08383",
          "publishedOn": "2021-06-28T01:57:55.354Z",
          "wordCount": 762,
          "title": "Optimal Pose and Shape Estimation for Category-level 3D Object Perception. (arXiv:2104.08383v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1\">Anna Esteve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1\">David Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Recently, bladder cancer has been significantly increased in terms of\nincidence and mortality. Currently, two subtypes are known based on tumour\ngrowth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).\nIn this work, we focus on the MIBC subtype because it is of the worst prognosis\nand can spread to adjacent organs. We present a self-learning framework to\ngrade bladder cancer from histological images stained via immunohistochemical\ntechniques. Specifically, we propose a novel Deep Convolutional Embedded\nAttention Clustering (DCEAC) which allows classifying histological patches into\ndifferent severity levels of the disease, according to the patterns established\nin the literature. The proposed DCEAC model follows a two-step fully\nunsupervised learning methodology to discern between non-tumour, mild and\ninfiltrative patterns from high-resolution samples of 512x512 pixels. Our\nsystem outperforms previous clustering-based methods by including a\nconvolutional attention module, which allows refining the features of the\nlatent space before the classification stage. The proposed network exceeds\nstate-of-the-art approaches by 2-3% across different metrics, achieving a final\naverage accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported\nclass activation maps evidence that our model is able to learn by itself the\nsame patterns that clinicians consider relevant, without incurring prior\nannotation steps. This fact supposes a breakthrough in muscle-invasive bladder\ncancer grading which bridges the gap with respect to train the model on\nlabelled data.",
          "link": "http://arxiv.org/abs/2106.13559",
          "publishedOn": "2021-06-28T01:57:55.346Z",
          "wordCount": 686,
          "title": "A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arslanturk_S/0/1/0/all/0/1\">Suzan Arslanturk</a>",
          "description": "Point cloud upsampling using deep learning has been paid various efforts in\nthe past few years. Recent supervised deep learning methods are restricted to\nthe size of training data and is limited in terms of covering all shapes of\npoint clouds. Besides, the acquisition of such amount of data is unrealistic,\nand the network generally performs less powerful than expected on unseen\nrecords. In this paper, we present an unsupervised approach to upsample point\nclouds internally referred as \"Zero Shot\" Point Cloud Upsampling (ZSPU) at\nholistic level. Our approach is solely based on the internal information\nprovided by a particular point cloud without patching in both self-training and\ntesting phases. This single-stream design significantly reduces the training\ntime of the upsampling task, by learning the relation between low-resolution\n(LR) point clouds and their high (original) resolution (HR) counterparts. This\nassociation will provide super-resolution (SR) outputs when original point\nclouds are loaded as input. We demonstrate competitive performance on benchmark\npoint cloud datasets when compared to other upsampling methods. Furthermore,\nZSPU achieves superior qualitative results on shapes with complex local details\nor high curvatures.",
          "link": "http://arxiv.org/abs/2106.13765",
          "publishedOn": "2021-06-28T01:57:55.328Z",
          "wordCount": 611,
          "title": "\"Zero Shot\" Point Cloud Upsampling. (arXiv:2106.13765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Recently, transformers have shown great superiority in solving computer\nvision tasks by modeling images as a sequence of manually-split patches with\nself-attention mechanism. However, current architectures of vision transformers\n(ViTs) are simply inherited from natural language processing (NLP) tasks and\nhave not been sufficiently investigated and optimized. In this paper, we make a\nfurther step by examining the intrinsic structure of transformers for vision\ntasks and propose an architecture search method, dubbed ViTAS, to search for\nthe optimal architecture with similar hardware budgets. Concretely, we design a\nnew effective yet efficient weight sharing paradigm for ViTs, such that\narchitectures with different token embedding, sequence size, number of heads,\nwidth, and depth can be derived from a single super-transformer. Moreover, to\ncater for the variance of distinct architectures, we introduce \\textit{private}\nclass token and self-attention maps in the super-transformer. In addition, to\nadapt the searching for different budgets, we propose to search the sampling\nprobability of identity operation. Experimental results show that our ViTAS\nattains excellent results compared to existing pure transformer architectures.\nFor example, with $1.3$G FLOPs budget, our searched architecture achieves\n$74.7\\%$ top-$1$ accuracy on ImageNet and is $2.5\\%$ superior than the current\nbaseline ViT architecture. Code is available at\n\\url{https://github.com/xiusu/ViTAS}.",
          "link": "http://arxiv.org/abs/2106.13700",
          "publishedOn": "2021-06-28T01:57:55.295Z",
          "wordCount": 646,
          "title": "Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xueying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>",
          "description": "Cross-modal retrieval aims to enable flexible retrieval experience by\ncombining multimedia data such as image, video, text, and audio. One core of\nunsupervised approaches is to dig the correlations among different object\nrepresentations to complete satisfied retrieval performance without requiring\nexpensive labels. In this paper, we propose a Graph Pattern Loss based\nDiversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to\ndeeply analyze correlations among representations. First, we propose a\ndiversified attention feature projector by considering the interaction between\ndifferent representations to generate multiple representations of an instance.\nThen, we design a novel graph pattern loss to explore the correlations among\ndifferent representations, in this graph all possible distances between\ndifferent representations are considered. In addition, a modality classifier is\nadded to explicitly declare the corresponding modalities of features before\nfusion and guide the network to enhance discrimination ability. We test GPLDAN\non four public datasets. Compared with the state-of-the-art cross-modal\nretrieval methods, the experimental results demonstrate the performance and\ncompetitiveness of GPLDAN.",
          "link": "http://arxiv.org/abs/2106.13552",
          "publishedOn": "2021-06-28T01:57:55.288Z",
          "wordCount": 640,
          "title": "Graph Pattern Loss based Diversified Attention Network for Cross-Modal Retrieval. (arXiv:2106.13552v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1\">Roc&#xed;o del Amor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1\">Rafael Verd&#xfa;-Monedero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1\">Juan Morales-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Glaucoma is one of the leading causes of blindness worldwide and Optical\nCoherence Tomography (OCT) is the quintessential imaging technique for its\ndetection. Unlike most of the state-of-the-art studies focused on glaucoma\ndetection, in this paper, we propose, for the first time, a novel framework for\nglaucoma grading using raw circumpapillary B-scans. In particular, we set out a\nnew OCT-based hybrid network which combines hand-driven and deep learning\nalgorithms. An OCT-specific descriptor is proposed to extract hand-crafted\nfeatures related to the retinal nerve fibre layer (RNFL). In parallel, an\ninnovative CNN is developed using skip-connections to include tailored residual\nand attention modules to refine the automatic features of the latent space. The\nproposed architecture is used as a backbone to conduct a novel few-shot\nlearning based on static and dynamic prototypical networks. The k-shot paradigm\nis redefined giving rise to a supervised end-to-end system which provides\nsubstantial improvements discriminating between healthy, early and advanced\nglaucoma samples. The training and evaluation processes of the dynamic\nprototypical network are addressed from two fused databases acquired via\nHeidelberg Spectralis system. Validation and testing results reach a\ncategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.\nBesides, the high performance reported by the proposed model for glaucoma\ndetection deserves a special mention. The findings from the class activation\nmaps are directly in line with the clinicians' opinion since the heatmaps\npointed out the RNFL as the most relevant structure for glaucoma diagnosis.",
          "link": "http://arxiv.org/abs/2106.13551",
          "publishedOn": "2021-06-28T01:57:55.282Z",
          "wordCount": 708,
          "title": "Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Vision-Language Pre-training (VLP) aims to learn multi-modal representations\nfrom image-text pairs and serves for downstream vision-language tasks in a\nfine-tuning fashion. The dominant VLP models adopt a CNN-Transformer\narchitecture, which embeds images with a CNN, and then aligns images and text\nwith a Transformer. Visual relationship between visual contents plays an\nimportant role in image understanding and is the basic for inter-modal\nalignment learning. However, CNNs have limitations in visual relation learning\ndue to local receptive field's weakness in modeling long-range dependencies.\nThus the two objectives of learning visual relation and inter-modal alignment\nare encapsulated in the same Transformer network. Such design might restrict\nthe inter-modal alignment learning in the Transformer by ignoring the\nspecialized characteristic of each objective. To tackle this, we propose a\nfully Transformer visual embedding for VLP to better learn visual relation and\nfurther promote inter-modal alignment. Specifically, we propose a metric named\nInter-Modality Flow (IMF) to measure the interaction between vision and\nlanguage modalities (i.e., inter-modality). We also design a novel masking\noptimization mechanism named Masked Feature Regression (MFR) in Transformer to\nfurther promote the inter-modality learning. To the best of our knowledge, this\nis the first study to explore the benefit of Transformer for visual feature\nlearning in VLP. We verify our method on a wide range of vision-language tasks,\nincluding Visual Question Answering (VQA), Visual Entailment and Visual\nReasoning. Our approach not only outperforms the state-of-the-art VLP\nperformance, but also shows benefits on the IMF metric.",
          "link": "http://arxiv.org/abs/2106.13488",
          "publishedOn": "2021-06-28T01:57:55.274Z",
          "wordCount": 687,
          "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training. (arXiv:2106.13488v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "We present animatable neural radiance fields for detailed human avatar\ncreation from monocular videos. Our approach extends neural radiance fields\n(NeRF) to the dynamic scenes with human movements via introducing explicit\npose-guided deformation while learning the scene representation network. In\nparticular, we estimate the human pose for each frame and learn a constant\ncanonical space for the detailed human template, which enables natural shape\ndeformation from the observation space to the canonical space under the\nexplicit control of the pose parameters. To compensate for inaccurate pose\nestimation, we introduce the pose refinement strategy that updates the initial\npose during the learning process, which not only helps to learn more accurate\nhuman reconstruction but also accelerates the convergence. In experiments we\nshow that the proposed approach achieves 1) implicit human geometry and\nappearance reconstruction with high-quality details, 2) photo-realistic\nrendering of the human from arbitrary views, and 3) animation of the human with\narbitrary poses.",
          "link": "http://arxiv.org/abs/2106.13629",
          "publishedOn": "2021-06-28T01:57:55.255Z",
          "wordCount": 599,
          "title": "Animatable Neural Radiance Fields from Monocular RGB Video. (arXiv:2106.13629v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>",
          "description": "Recent advances in image synthesis enables one to translate images by\nlearning the mapping between a source domain and a target domain. Existing\nmethods tend to learn the distributions by training a model on a variety of\ndatasets, with results evaluated largely in a subjective manner. Relatively few\nworks in this area, however, study the potential use of semantic image\ntranslation methods for image recognition tasks. In this paper, we explore the\nuse of Single Image Texture Translation (SITT) for data augmentation. We first\npropose a lightweight model for translating texture to images based on a single\ninput of source texture, allowing for fast training and testing. Based on SITT,\nwe then explore the use of augmented data in long-tailed and few-shot image\nclassification tasks. We find the proposed method is capable of translating\ninput data into a target domain, leading to consistent improved image\nrecognition performance. Finally, we examine how SITT and related image\ntranslation methods can provide a basis for a data-efficient, augmentation\nengineering approach to model training.",
          "link": "http://arxiv.org/abs/2106.13804",
          "publishedOn": "2021-06-28T01:57:55.244Z",
          "wordCount": 612,
          "title": "Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuning Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "The detection of 3D objects from LiDAR data is a critical component in most\nautonomous driving systems. Safe, high speed driving needs larger detection\nranges, which are enabled by new LiDARs. These larger detection ranges require\nmore efficient and accurate detection models. Towards this goal, we propose\nRange Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in\norder to tackle real time 3D object detection in this extended detection\nregime. RSN predicts foreground points from range images and applies sparse\nconvolutions on the selected foreground points to detect objects. The\nlightweight 2D convolutions on dense range images results in significantly\nfewer selected foreground points, thus enabling the later sparse convolutions\nin RSN to efficiently operate. Combining features from the range image further\nenhance detection accuracy. RSN runs at more than 60 frames per second on a\n150m x 150m detection region on Waymo Open Dataset (WOD) while being more\naccurate than previously published detectors. As of 11/2020, RSN is ranked\nfirst in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based\npedestrian and vehicle detection, while being several times faster than\nalternatives.",
          "link": "http://arxiv.org/abs/2106.13365",
          "publishedOn": "2021-06-28T01:57:55.237Z",
          "wordCount": 640,
          "title": "RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection. (arXiv:2106.13365v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>",
          "description": "The detection of nuclei and cells in histology images is of great value in\nboth clinical practice and pathological studies. However, multiple reasons such\nas morphological variations of nuclei or cells make it a challenging task where\nconventional object detection methods cannot obtain satisfactory performance in\nmany cases. A detection task consists of two sub-tasks, classification and\nlocalization. Under the condition of dense object detection, classification is\na key to boost the detection performance. Considering this, we propose\nsimilarity based region proposal networks (SRPN) for nuclei and cells detection\nin histology images. In particular, a customized convolution layer termed as\nembedding layer is designed for network building. The embedding layer is added\ninto the region proposal networks, enabling the networks to learn\ndiscriminative features based on similarity learning. Features obtained by\nsimilarity learning can significantly boost the classification performance\ncompared to conventional methods. SRPN can be easily integrated into standard\nconvolutional neural networks architectures such as the Faster R-CNN and\nRetinaNet. We test the proposed approach on tasks of multi-organ nuclei\ndetection and signet ring cells detection in histological images. Experimental\nresults show that networks applying similarity learning achieved superior\nperformance on both tasks when compared to their counterparts. In particular,\nthe proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark\nfor nuclei segmentation and detection while compared to previous methods, and\non the signet ring cell detection benchmark when compared with baselines. The\nsourcecode is publicly available at:\nhttps://github.com/sigma10010/nuclei_cells_det.",
          "link": "http://arxiv.org/abs/2106.13556",
          "publishedOn": "2021-06-28T01:57:55.231Z",
          "wordCount": 701,
          "title": "SRPN: similarity-based region proposal networks for nuclei and cells detection in histology images. (arXiv:2106.13556v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>",
          "description": "Recent studies have shown that neural network (NN) based image classifiers\nare highly vulnerable to adversarial examples, which poses a threat to\nsecurity-sensitive image recognition task. Prior work has shown that JPEG\ncompression can combat the drop in classification accuracy on adversarial\nexamples to some extent. But, as the compression ratio increases, traditional\nJPEG compression is insufficient to defend those attacks but can cause an\nabrupt accuracy decline to the benign images. In this paper, with the aim of\nfully filtering the adversarial perturbations, we firstly make modifications to\ntraditional JPEG compression algorithm which becomes more favorable for NN.\nSpecifically, based on an analysis of the frequency coefficient, we design a\nNN-favored quantization table for compression. Considering compression as a\ndata augmentation strategy, we then combine our model-agnostic preprocess with\nnoisy training. We fine-tune the pre-trained model by training with images\nencoded at different compression levels, thus generating multiple classifiers.\nFinally, since lower (higher) compression ratio can remove both perturbations\nand original features slightly (aggressively), we use these trained multiple\nmodels for model ensemble. The majority vote of the ensemble of models is\nadopted as final predictions. Experiments results show our method can improve\ndefense efficiency while maintaining original accuracy.",
          "link": "http://arxiv.org/abs/2106.13394",
          "publishedOn": "2021-06-28T01:57:55.222Z",
          "wordCount": 640,
          "title": "Countering Adversarial Examples: Combining Input Transformation and Noisy Training. (arXiv:2106.13394v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhipeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>",
          "description": "Generative modeling has recently shown great promise in computer vision, but\nit has mostly focused on synthesizing visually realistic images. In this paper,\nmotivated by multi-task learning of shareable feature representations, we\nconsider a novel problem of learning a shared generative model that is useful\nacross various visual perception tasks. Correspondingly, we propose a general\nmulti-task oriented generative modeling (MGM) framework, by coupling a\ndiscriminative multi-task network with a generative network. While it is\nchallenging to synthesize both RGB images and pixel-level annotations in\nmulti-task scenarios, our framework enables us to use synthesized images paired\nwith only weak annotations (i.e., image-level scene labels) to facilitate\nmultiple visual tasks. Experimental evaluation on challenging multi-task\nbenchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework\nimproves the performance of all the tasks by large margins, consistently\noutperforming state-of-the-art multi-task approaches.",
          "link": "http://arxiv.org/abs/2106.13409",
          "publishedOn": "2021-06-28T01:57:55.202Z",
          "wordCount": 568,
          "title": "Generative Modeling for Multi-task Visual Learning. (arXiv:2106.13409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>",
          "description": "In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at \\url{https://github.com/ZENGXH/NPDRAW}.",
          "link": "http://arxiv.org/abs/2106.13435",
          "publishedOn": "2021-06-28T01:57:55.196Z",
          "wordCount": 627,
          "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Modelfor Image Generation. (arXiv:2106.13435v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1\">Luca Moschella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>",
          "description": "In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13679",
          "publishedOn": "2021-06-28T01:57:55.190Z",
          "wordCount": 657,
          "title": "Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1\">Vignesh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1\">Nils Strodthoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jackie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>",
          "description": "There is an increasing number of medical use-cases where classification\nalgorithms based on deep neural networks reach performance levels that are\ncompetitive with human medical experts. To alleviate the challenges of small\ndataset sizes, these systems often rely on pretraining. In this work, we aim to\nassess the broader implications of these approaches. For diabetic retinopathy\ngrading as exemplary use case, we compare the impact of different training\nprocedures including recently established self-supervised pretraining methods\nbased on contrastive learning. To this end, we investigate different aspects\nsuch as quantitative performance, statistics of the learned feature\nrepresentations, interpretability and robustness to image distortions. Our\nresults indicate that models initialized from ImageNet pretraining report a\nsignificant increase in performance, generalization and robustness to image\ndistortions. In particular, self-supervised models show further benefits to\nsupervised models. Self-supervised models with initialization from ImageNet\npretraining not only report higher performance, they also reduce overfitting to\nlarge lesions along with improvements in taking into account minute lesions\nindicative of the progression of the disease. Understanding the effects of\npretraining in a broader sense that goes beyond simple performance comparisons\nis of crucial importance for the broader medical imaging community beyond the\nuse-case considered in this work.",
          "link": "http://arxiv.org/abs/2106.13497",
          "publishedOn": "2021-06-28T01:57:55.181Z",
          "wordCount": 654,
          "title": "On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy. (arXiv:2106.13497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuning Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>",
          "description": "3D object detection is vital for many robotics applications. For tasks where\na 2D perspective range image exists, we propose to learn a 3D representation\ndirectly from this range image view. To this end, we designed a 2D\nconvolutional network architecture that carries the 3D spherical coordinates of\neach pixel throughout the network. Its layers can consume any arbitrary\nconvolution kernel in place of the default inner product kernel and exploit the\nunderlying local geometry around each pixel. We outline four such kernels: a\ndense kernel according to the bag-of-words paradigm, and three graph kernels\ninspired by recent graph neural network advances: the Transformer, the\nPointNet, and the Edge Convolution. We also explore cross-modality fusion with\nthe camera image, facilitated by operating in the perspective range image view.\nOur method performs competitively on the Waymo Open Dataset and improves the\nstate-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also\nefficient in that our smallest model, which still outperforms the popular\nPointPillars in quality, requires 180 times fewer FLOPS and model parameters",
          "link": "http://arxiv.org/abs/2106.13381",
          "publishedOn": "2021-06-28T01:57:55.175Z",
          "wordCount": 635,
          "title": "To the Point: Efficient 3D Object Detection in the Range Image with Graph Convolution Kernels. (arXiv:2106.13381v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1\">Damien Scieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsung Kim</a>",
          "description": "This paper considers classification problems with hierarchically organized\nclasses. We force the classifier (hyperplane) of each class to belong to a\nsphere manifold, whose center is the classifier of its super-class. Then,\nindividual sphere manifolds are connected based on their hierarchical\nrelations. Our technique replaces the last layer of a neural network by\ncombining a spherical fully-connected layer with a hierarchical layer. This\nregularization is shown to improve the performance of widely used deep neural\nnetwork architectures (ResNet and DenseNet) on publicly available datasets\n(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).",
          "link": "http://arxiv.org/abs/2106.13549",
          "publishedOn": "2021-06-28T01:57:55.155Z",
          "wordCount": 524,
          "title": "Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1\">Yuki Endo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1\">Yoshihiro Kanamori</a>",
          "description": "Semantic image synthesis is a process for generating photorealistic images\nfrom a single semantic mask. To enrich the diversity of multimodal image\nsynthesis, previous methods have controlled the global appearance of an output\nimage by learning a single latent space. However, a single latent code is often\ninsufficient for capturing various object styles because object appearance\ndepends on multiple factors. To handle individual factors that determine object\nstyles, we propose a class- and layer-wise extension to the variational\nautoencoder (VAE) framework that allows flexible control over each object class\nat the local to global levels by learning multiple latent spaces. Furthermore,\nwe demonstrate that our method generates images that are both plausible and\nmore diverse compared to state-of-the-art methods via extensive experiments\nwith real and synthetic datasets inthree different domains. We also show that\nour method enables a wide range of applications in image synthesis and editing\ntasks.",
          "link": "http://arxiv.org/abs/2106.13416",
          "publishedOn": "2021-06-28T01:57:55.148Z",
          "wordCount": 603,
          "title": "Diversifying Semantic Image Synthesis and Editing via Class- and Layer-wise VAEs. (arXiv:2106.13416v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samelak_J/0/1/0/all/0/1\">Jaros&#x142;aw Samelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Doma&#x144;ski</a>",
          "description": "The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.",
          "link": "http://arxiv.org/abs/2106.13574",
          "publishedOn": "2021-06-28T01:57:55.139Z",
          "wordCount": 627,
          "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding. (arXiv:2106.13574v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bongini_F/0/1/0/all/0/1\">Francesco Bongini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlincioni_L/0/1/0/all/0/1\">Lorenzo Berlincioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1\">Marco Bertini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>",
          "description": "In this paper we propose a novel data augmentation approach for visual\ncontent domains that have scarce training datasets, compositing synthetic 3D\nobjects within real scenes. We show the performance of the proposed system in\nthe context of object detection in thermal videos, a domain where 1) training\ndatasets are very limited compared to visible spectrum datasets and 2) creating\nfull realistic synthetic scenes is extremely cumbersome and expensive due to\nthe difficulty in modeling the thermal properties of the materials of the\nscene. We compare different augmentation strategies, including state of the art\napproaches obtained through RL techniques, the injection of simulated data and\nthe employment of a generative model, and study how to best combine our\nproposed augmentation with these other techniques.Experimental results\ndemonstrate the effectiveness of our approach, and our single-modality detector\nachieves state-of-the-art results on the FLIR ADAS dataset.",
          "link": "http://arxiv.org/abs/2106.13603",
          "publishedOn": "2021-06-28T01:57:55.114Z",
          "wordCount": 597,
          "title": "Partially fake it till you make it: mixing real and fake thermal images for improved object detection. (arXiv:2106.13603v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>",
          "description": "Model-based eye tracking has been a dominant approach for eye gaze tracking\nbecause of its ability to generalize to different subjects, without the need of\nany training data and eye gaze annotations. Model-based eye tracking, however,\nis susceptible to eye feature detection errors, in particular for eye tracking\nin the wild. To address this issue, we propose a Bayesian framework for\nmodel-based eye tracking. The proposed system consists of a cascade-Bayesian\nConvolutional Neural Network (c-BCNN) to capture the probabilistic\nrelationships between eye appearance and its landmarks, and a geometric eye\nmodel to estimate eye gaze from the eye landmarks. Given a testing eye image,\nthe Bayesian framework can generate, through Bayesian inference, the eye gaze\ndistribution without explicit landmark detection and model training, based on\nwhich it not only estimates the most likely eye gaze but also its uncertainty.\nFurthermore, with Bayesian inference instead of point-based inference, our\nmodel can not only generalize better to different sub-jects, head poses, and\nenvironments but also is robust to image noise and landmark detection errors.\nFinally, with the estimated gaze uncertainty, we can construct a cascade\narchitecture that allows us to progressively improve gaze estimation accuracy.\nCompared to state-of-the-art model-based and learning-based methods, the\nproposed Bayesian framework demonstrates significant improvement in\ngeneralization capability across several benchmark datasets and in accuracy and\nrobustness under challenging real-world conditions.",
          "link": "http://arxiv.org/abs/2106.13387",
          "publishedOn": "2021-06-28T01:57:55.106Z",
          "wordCount": 644,
          "title": "Bayesian Eye Tracking. (arXiv:2106.13387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1\">Nicholas Gyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>",
          "description": "The ability to perform causal and counterfactual reasoning are central\nproperties of human intelligence. Decision-making systems that can perform\nthese types of reasoning have the potential to be more generalizable and\ninterpretable. Simulations have helped advance the state-of-the-art in this\ndomain, by providing the ability to systematically vary parameters (e.g.,\nconfounders) and generate examples of the outcomes in the case of\ncounterfactual scenarios. However, simulating complex temporal causal events in\nmulti-agent scenarios, such as those that exist in driving and vehicle\nnavigation, is challenging. To help address this, we present a high-fidelity\nsimulation environment that is designed for developing algorithms for causal\ndiscovery and counterfactual reasoning in the safety-critical context. A core\ncomponent of our work is to introduce \\textit{agency}, such that it is simple\nto define and create complex scenarios using high-level definitions. The\nvehicles then operate with agency to complete these objectives, meaning\nlow-level behaviors need only be controlled if necessary. We perform\nexperiments with three state-of-the-art methods to create baselines and\nhighlight the affordances of this environment. Finally, we highlight challenges\nand opportunities for future work.",
          "link": "http://arxiv.org/abs/2106.13364",
          "publishedOn": "2021-06-28T01:57:55.087Z",
          "wordCount": 636,
          "title": "CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunhong Pan</a>",
          "description": "Previous methods for skeleton-based gesture recognition mostly arrange the\nskeleton sequence into a pseudo picture or spatial-temporal graph and apply\ndeep Convolutional Neural Network (CNN) or Graph Convolutional Network (GCN)\nfor feature extraction. Although achieving superior results, these methods have\ninherent limitations in dynamically capturing local features of interactive\nhand parts, and the computing efficiency still remains a serious issue. In this\nwork, the self-attention mechanism is introduced to alleviate this problem.\nConsidering the hierarchical structure of hand joints, we propose an efficient\nhierarchical self-attention network (HAN) for skeleton-based gesture\nrecognition, which is based on pure self-attention without any CNN, RNN or GCN\noperators. Specifically, the joint self-attention module is used to capture\nspatial features of fingers, the finger self-attention module is designed to\naggregate features of the whole hand. In terms of temporal features, the\ntemporal self-attention module is utilized to capture the temporal dynamics of\nthe fingers and the entire hand. Finally, these features are fused by the\nfusion self-attention module for gesture classification. Experiments show that\nour method achieves competitive results on three gesture recognition datasets\nwith much lower computational complexity.",
          "link": "http://arxiv.org/abs/2106.13391",
          "publishedOn": "2021-06-28T01:57:55.010Z",
          "wordCount": 629,
          "title": "HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based Gesture Recognition. (arXiv:2106.13391v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>",
          "description": "Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.",
          "link": "http://arxiv.org/abs/2106.13301",
          "publishedOn": "2021-06-28T01:57:55.003Z",
          "wordCount": 644,
          "title": "Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13328",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1\">Yize Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patney_A/0/1/0/all/0/1\">Anjul Patney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Webb_R/0/1/0/all/0/1\">Richard Webb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>",
          "description": "Previous blind or No Reference (NR) video quality assessment (VQA) models\nlargely rely on features drawn from natural scene statistics (NSS), but under\nthe assumption that the image statistics are stationary in the spatial domain.\nSeveral of these models are quite successful on standard pictures. However, in\nVirtual Reality (VR) applications, foveated video compression is regaining\nattention, and the concept of space-variant quality assessment is of interest,\ngiven the availability of increasingly high spatial and temporal resolution\ncontents and practical ways of measuring gaze direction. Distortions from\nfoveated video compression increase with increased eccentricity, implying that\nthe natural scene statistics are space-variant. Towards advancing the\ndevelopment of foveated compression / streaming algorithms, we have devised a\nno-reference (NR) foveated video quality assessment model, called FOVQA, which\nis based on new models of space-variant natural scene statistics (NSS) and\nnatural video statistics (NVS). Specifically, we deploy a space-variant\ngeneralized Gaussian distribution (SV-GGD) model and a space-variant\nasynchronous generalized Gaussian distribution (SV-AGGD) model of mean\nsubtracted contrast normalized (MSCN) coefficients and products of neighboring\nMSCN coefficients, respectively. We devise a foveated video quality predictor\nthat extracts radial basis features, and other features that capture\nperceptually annoying rapid quality fall-offs. We find that FOVQA achieves\nstate-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as\ncompared with other leading FIQA / VQA models. we have made our implementation\nof FOVQA available at: this http URL",
          "link": "http://arxiv.org/abs/2106.13328",
          "publishedOn": "2021-06-28T01:57:54.996Z",
          "wordCount": 673,
          "title": "FOVQA: Blind Foveated Video Quality Assessment. (arXiv:2106.13328v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13315",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_A/0/1/0/all/0/1\">Angela F. Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasmussen_B/0/1/0/all/0/1\">Brandon Rasmussen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scheller_E/0/1/0/all/0/1\">Eva L. Scheller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greenberger_R/0/1/0/all/0/1\">Rebecca Greenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehlmann_B/0/1/0/all/0/1\">Bethany L. Ehlmann</a>",
          "description": "The application of infrared hyperspectral imagery to geological problems is\nbecoming more popular as data become more accessible and cost-effective.\nClustering and classifying spectrally similar materials is often a first step\nin applications ranging from economic mineral exploration on Earth to planetary\nexploration on Mars. Semi-manual classification guided by expertly developed\nspectral parameters can be time consuming and biased, while supervised methods\nrequire abundant labeled data and can be difficult to generalize. Here we\ndevelop a fully unsupervised workflow for feature extraction and clustering\ninformed by both expert spectral geologist input and quantitative metrics. Our\npipeline uses a lightweight autoencoder followed by Gaussian mixture modeling\nto map the spectral diversity within any image. We validate the performance of\nour pipeline at submillimeter-scale with expert-labelled data from the Oman\nophiolite drill core and evaluate performance at meters-scale with partially\nclassified orbital data of Jezero Crater on Mars (the landing site for the\nPerseverance rover). We additionally examine the effects of various\npreprocessing techniques used in traditional analysis of hyperspectral imagery.\nThis pipeline provides a fast and accurate clustering map of similar geological\nmaterials and consistently identifies and separates major mineral classes in\nboth laboratory imagery and remote sensing imagery. We refer to our pipeline as\n\"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals\n(GyPSUM).\"",
          "link": "http://arxiv.org/abs/2106.13315",
          "publishedOn": "2021-06-28T01:57:54.988Z",
          "wordCount": 685,
          "title": "Generalized Unsupervised Clustering of Hyperspectral Images of Geological Targets in the Near Infrared. (arXiv:2106.13315v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1\">George Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">Jasmeet Judge</a>",
          "description": "Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, Domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%\nmore weeks with highest cosine similarity than the other NNs during test years.\nThe DgNN and Sequential NN were more robust during periods of abnormal crop\nprogress, though estimating the Silking-Grainfill transition was difficult for\nall methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.",
          "link": "http://arxiv.org/abs/2106.13323",
          "publishedOn": "2021-06-28T01:57:54.963Z",
          "wordCount": 679,
          "title": "Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:54.947Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Generalising deep models to new data from new centres (termed here domains)\nremains a challenge. This is largely attributed to shifts in data statistics\n(domain shifts) between source and unseen domains. Recently, gradient-based\nmeta-learning approaches where the training data are split into meta-train and\nmeta-test sets to simulate and handle the domain shifts during training have\nshown improved generalisation performance. However, the current fully\nsupervised meta-learning approaches are not scalable for medical image\nsegmentation, where large effort is required to create pixel-wise annotations.\nMeanwhile, in a low data regime, the simulated domain shifts may not\napproximate the true domain shifts well across source and unseen domains. To\naddress this problem, we propose a novel semi-supervised meta-learning\nframework with disentanglement. We explicitly model the representations related\nto domain shifts. Disentangling the representations and combining them to\nreconstruct the input image allows unlabeled data to be used to better\napproximate the true domain shifts for meta-learning. Hence, the model can\nachieve better generalisation performance, especially when there is a limited\namount of labeled data. Experiments show that the proposed method is robust on\ndifferent segmentation tasks and achieves state-of-the-art generalisation\nperformance on two public benchmarks.",
          "link": "http://arxiv.org/abs/2106.13292",
          "publishedOn": "2021-06-28T01:57:54.877Z",
          "wordCount": 637,
          "title": "Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1\">Mayu Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_I/0/1/0/all/0/1\">Ittetsu Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoye_T/0/1/0/all/0/1\">Takao Onoye</a>",
          "description": "How far can we go with textual representations for understanding pictures? In\nimage understanding, it is essential to use concise but detailed image\nrepresentations. Deep visual features extracted by vision models, such as\nFaster R-CNN, are prevailing used in multiple tasks, and especially in visual\nquestion answering (VQA). However, conventional deep visual features may\nstruggle to convey all the details in an image as we humans do. Meanwhile, with\nrecent language models' progress, descriptive text may be an alternative to\nthis problem. This paper delves into the effectiveness of textual\nrepresentations for image understanding in the specific context of VQA. We\npropose to take description-question pairs as input, instead of deep visual\nfeatures, and fed them into a language-only Transformer model, simplifying the\nprocess and the computational cost. We also experiment with data augmentation\ntechniques to increase the diversity in the training set and avoid learning\nstatistical bias. Extensive evaluations have shown that textual representations\nrequire only about a hundred words to compete with deep visual features on both\nVQA 2.0 and VQA-CP v2.",
          "link": "http://arxiv.org/abs/2106.13445",
          "publishedOn": "2021-06-28T01:57:54.820Z",
          "wordCount": 623,
          "title": "A Picture May Be Worth a Hundred Words for Visual Question Answering. (arXiv:2106.13445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>",
          "description": "Breakthroughs in machine learning in the last decade have led to `digital\nintelligence', i.e. machine learning models capable of learning from vast\namounts of labeled data to perform several digital tasks such as speech\nrecognition, face recognition, machine translation and so on. The goal of this\nthesis is to make progress towards designing algorithms capable of `physical\nintelligence', i.e. building intelligent autonomous navigation agents capable\nof learning to perform complex navigation tasks in the physical world involving\nvisual perception, natural language understanding, reasoning, planning, and\nsequential decision making. Despite several advances in classical navigation\nmethods in the last few decades, current navigation agents struggle at\nlong-term semantic navigation tasks. In the first part of the thesis, we\ndiscuss our work on short-term navigation using end-to-end reinforcement\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\nlanguage grounding, and reasoning. In the second part, we present a new class\nof navigation methods based on modular learning and structured explicit map\nrepresentations, which leverage the strengths of both classical and end-to-end\nlearning methods, to tackle long-term navigation tasks. We show that these\nmethods are able to effectively tackle challenges such as localization,\nmapping, long-term planning, exploration and learning semantic priors. These\nmodular learning methods are capable of long-term spatial and semantic\nunderstanding and achieve state-of-the-art results on various navigation tasks.",
          "link": "http://arxiv.org/abs/2106.13415",
          "publishedOn": "2021-06-28T01:57:54.800Z",
          "wordCount": 671,
          "title": "Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "Conventional saliency prediction models typically learn a deterministic\nmapping from images to the corresponding ground truth saliency maps. In this\npaper, we study the saliency prediction problem from the perspective of\ngenerative models by learning a conditional probability distribution over\nsaliency maps given an image, and treating the prediction as a sampling\nprocess. Specifically, we propose a generative cooperative saliency prediction\nframework based on the generative cooperative networks, where a conditional\nlatent variable model and a conditional energy-based model are jointly trained\nto predict saliency in a cooperative manner. We call our model the SalCoopNets.\nThe latent variable model serves as a fast but coarse predictor to efficiently\nproduce an initial prediction, which is then refined by the iterative Langevin\nrevision of the energy-based model that serves as a fine predictor. Such a\ncoarse-to-fine cooperative saliency prediction strategy offers the best of both\nworlds. Moreover, we generalize our framework to the scenario of weakly\nsupervised saliency prediction, where saliency annotation of training images is\npartially observed, by proposing a cooperative learning while recovering\nstrategy. Lastly, we show that the learned energy function can serve as a\nrefinement module that can refine the results of other pre-trained saliency\nprediction models. Experimental results show that our generative model can\nachieve state-of-the-art performance. Our code is publicly available at:\n\\url{https://github.com/JingZhang617/SalCoopNets}.",
          "link": "http://arxiv.org/abs/2106.13389",
          "publishedOn": "2021-06-28T01:57:54.791Z",
          "wordCount": 649,
          "title": "Energy-Based Generative Cooperative Saliency Prediction. (arXiv:2106.13389v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Long Hoang Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>",
          "description": "Video Question Answering (Video QA) is a powerful testbed to develop new AI\ncapabilities. This task necessitates learning to reason about objects,\nrelations, and events across visual and linguistic domains in space-time.\nHigh-level reasoning demands lifting from associative visual pattern\nrecognition to symbol-like manipulation over objects, their behavior and\ninteractions. Toward reaching this goal we propose an object-oriented reasoning\napproach in that video is abstracted as a dynamic stream of interacting\nobjects. At each stage of the video event flow, these objects interact with\neach other, and their interactions are reasoned about with respect to the query\nand under the overall context of a video. This mechanism is materialized into a\nfamily of general-purpose neural units and their multi-level architecture\ncalled Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.\nThis neural model maintains the objects' consistent lifelines in the form of a\nhierarchically nested spatio-temporal graph. Within this graph, the dynamic\ninteractive object-oriented representations are built up along the video\nsequence, hierarchically abstracted in a bottom-up manner, and converge toward\nthe key information for the correct answer. The method is evaluated on multiple\nmajor Video QA datasets and establishes new state-of-the-arts in these tasks.\nAnalysis into the model's behavior indicates that object-oriented reasoning is\na reliable, interpretable and efficient approach to Video QA.",
          "link": "http://arxiv.org/abs/2106.13432",
          "publishedOn": "2021-06-28T01:57:54.783Z",
          "wordCount": 654,
          "title": "Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>",
          "description": "In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.",
          "link": "http://arxiv.org/abs/2106.13266",
          "publishedOn": "2021-06-28T01:57:54.776Z",
          "wordCount": 699,
          "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval. (arXiv:2106.13266v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.13272",
          "publishedOn": "2021-06-28T01:57:54.764Z",
          "wordCount": 699,
          "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenthaler_S/0/1/0/all/0/1\">S&#xe9;bastien Morgenthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Micha&#xeb;l Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1\">George Drettakis</a>",
          "description": "We introduce a neural relighting algorithm for captured indoors scenes, that\nallows interactive free-viewpoint navigation. Our method allows illumination to\nbe changed synthetically, while coherently rendering cast shadows and complex\nglossy materials. We start with multiple images of the scene and a 3D mesh\nobtained by multi-view stereo (MVS) reconstruction. We assume that lighting is\nwell-explained as the sum of a view-independent diffuse component and a\nview-dependent glossy term concentrated around the mirror reflection direction.\nWe design a convolutional network around input feature maps that facilitate\nlearning of an implicit representation of scene materials and illumination,\nenabling both relighting and free-viewpoint navigation. We generate these input\nmaps by exploiting the best elements of both image-based and physically-based\nrendering. We sample the input views to estimate diffuse scene irradiance, and\ncompute the new illumination caused by user-specified light sources using path\ntracing. To facilitate the network's understanding of materials and synthesize\nplausible glossy reflections, we reproject the views and compute mirror images.\nWe train the network on a synthetic dataset where each scene is also\nreconstructed with MVS. We show results of our algorithm relighting real indoor\nscenes and performing free-viewpoint navigation with complex and realistic\nglossy reflections, which so far remained out of reach for view-synthesis\ntechniques.",
          "link": "http://arxiv.org/abs/2106.13299",
          "publishedOn": "2021-06-28T01:57:54.640Z",
          "wordCount": 645,
          "title": "Free-viewpoint Indoor Neural Relighting from Multi-view Stereo. (arXiv:2106.13299v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.",
          "link": "http://arxiv.org/abs/2106.13239",
          "publishedOn": "2021-06-28T01:57:54.589Z",
          "wordCount": 623,
          "title": "Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bowen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>",
          "description": "The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory cost. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4X speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nthis http URL",
          "link": "http://arxiv.org/abs/2106.12620",
          "publishedOn": "2021-06-25T02:00:47.371Z",
          "wordCount": 652,
          "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-06-25T02:00:47.304Z",
          "wordCount": 620,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_I/0/1/0/all/0/1\">Indrani Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_I/0/1/0/all/0/1\">Indranil Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omprakash_C/0/1/0/all/0/1\">Charitha Omprakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stober_S/0/1/0/all/0/1\">Sebastian Stober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikulovic_S/0/1/0/all/0/1\">Sanja Mikulovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1\">Pavol Bauer</a>",
          "description": "The assessment of laboratory animal behavior is of central interest in modern\nneuroscience research. Behavior is typically studied in terms of pose changes,\nwhich are ideally captured in three dimensions. This requires triangulation\nover a multi-camera system which view the animal from different angles.\nHowever, this is challenging in realistic laboratory setups due to occlusions\nand other technical constrains. Here we propose the usage of lift-pose models\nthat allow for robust 3D pose estimation of freely moving rodents from a single\nview camera view. To obtain high-quality training data for the pose-lifting, we\nfirst perform geometric calibration in a camera setup involving bottom as well\nas side views of the behaving animal. We then evaluate the performance of two\npreviously proposed model architectures under given inference perspectives and\nconclude that reliable 3D pose inference can be obtained using temporal\nconvolutions. With this work we would like to contribute to a more robust and\ndiverse behavior tracking of freely moving rodents for a wide range of\nexperiments and setups in the neuroscience community.",
          "link": "http://arxiv.org/abs/2106.12993",
          "publishedOn": "2021-06-25T02:00:47.267Z",
          "wordCount": 649,
          "title": "Evaluation of deep lift pose models for 3D rodent pose estimation based on geometrically triangulated data. (arXiv:2106.12993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12776",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lyngdoh_R/0/1/0/all/0/1\">Rosly Boy Lyngdoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahadevan_A/0/1/0/all/0/1\">Anand S Sahadevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_T/0/1/0/all/0/1\">Touseef Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathore_P/0/1/0/all/0/1\">Pradyuman Singh Rathore</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_M/0/1/0/all/0/1\">Manoj Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Kumar Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Arundhati Misra</a>",
          "description": "Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3\nbased quantum GIS (QGIS) plugin designed to process and analyse hyperspectral\n(Hx) images. It is developed to guarantee full usage of present and future Hx\nairborne or spaceborne sensors and provides access to advanced algorithms for\nHx data processing. The software is freely available and offers a range of\nbasic and advanced tools such as atmospheric correction (for airborne AVIRISNG\nimage), standard processing tools as well as powerful machine learning and Deep\nLearning interfaces for Hx data analysis.",
          "link": "http://arxiv.org/abs/2106.12776",
          "publishedOn": "2021-06-25T02:00:47.097Z",
          "wordCount": 566,
          "title": "AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral Image Analysis. (arXiv:2106.12776v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1\">Sagar Mandiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1\">Rashid Sheikh</a>",
          "description": "Deep Learning as a field has been successfully used to solve a plethora of\ncomplex problems, the likes of which we could not have imagined a few decades\nback. But as many benefits as it brings, there are still ways in which it can\nbe used to bring harm to our society. Deep fakes have been proven to be one\nsuch problem, and now more than ever, when any individual can create a fake\nimage or video simply using an application on the smartphone, there need to be\nsome countermeasures, with which we can detect if the image or video is a fake\nor real and dispose of the problem threatening the trustworthiness of online\ninformation. Although the Deep fakes created by neural networks, may seem to be\nas real as a real image or video, it still leaves behind spatial and temporal\ntraces or signatures after moderation, these signatures while being invisible\nto a human eye can be detected with the help of a neural network trained to\nspecialize in Deep fake detection. In this paper, we analyze several such\nstates of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\nNet) and compare them against each other, to find an optimal solution for\nvarious scenarios like real-time deep fake detection to be deployed in online\nsocial media platforms where the classification should be made as fast as\npossible or for a small news agency where the classification need not be in\nreal-time but requires utmost accuracy.",
          "link": "http://arxiv.org/abs/2106.12605",
          "publishedOn": "2021-06-25T02:00:46.822Z",
          "wordCount": 713,
          "title": "Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1\">Ritik Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1\">Rishika Kushwah</a>",
          "description": "The reliance of humans over machines has never been so high such that from\nobject classification in photographs to adding sound to silent movies\neverything can be performed with the help of deep learning and machine learning\nalgorithms. Likewise, Handwritten text recognition is one of the significant\nareas of research and development with a streaming number of possibilities that\ncould be attained. Handwriting recognition (HWR), also known as Handwritten\nText Recognition (HTR), is the ability of a computer to receive and interpret\nintelligible handwritten input from sources such as paper documents,\nphotographs, touch-screens and other devices [1]. Apparently, in this paper, we\nhave performed handwritten digit recognition with the help of MNIST datasets\nusing Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\nConvolution Neural Network (CNN) models. Our main objective is to compare the\naccuracy of the models stated above along with their execution time to get the\nbest possible model for digit recognition.",
          "link": "http://arxiv.org/abs/2106.12614",
          "publishedOn": "2021-06-25T02:00:46.719Z",
          "wordCount": 620,
          "title": "Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1\">Benjamin Keltjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1\">Tom van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido de Croon</a>",
          "description": "Self-supervised deep learning methods have leveraged stereo images for\ntraining monocular depth estimation. Although these methods show strong results\non outdoor datasets such as KITTI, they do not match performance of supervised\nmethods on indoor environments with camera rotation. Indoor, rotated scenes are\ncommon for less constrained applications and pose problems for two reasons:\nabundance of low texture regions and increased complexity of depth cues for\nimages under rotation. In an effort to extend self-supervised learning to more\ngeneralised environments we propose two additions. First, we propose a novel\nFilled Disparity Loss term that corrects for ambiguity of image reconstruction\nerror loss in textureless regions. Specifically, we interpolate disparity in\nuntextured regions, using the estimated disparity from surrounding textured\nareas, and use L1 loss to correct the original estimation. Our experiments show\nthat depth estimation is substantially improved on low-texture scenes, without\nany loss on textured scenes, when compared to Monodepth by Godard et al.\nSecondly, we show that training with an application's representative rotations,\nin both pitch and roll, is sufficient to significantly improve performance over\nthe entire range of expected rotation. We demonstrate that depth estimation is\nsuccessfully generalised as performance is not lost when evaluated on test sets\nwith no camera rotation. Together these developments enable a broader use of\nself-supervised learning of monocular depth estimation for complex\nenvironments.",
          "link": "http://arxiv.org/abs/2106.12958",
          "publishedOn": "2021-06-25T02:00:46.620Z",
          "wordCount": 664,
          "title": "Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1\">Thomas Michalke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Di Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1\">Claudius Gl&#xe4;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timm_F/0/1/0/all/0/1\">Fabian Timm</a>",
          "description": "Lane detection is an essential part of the perception sub-architecture of any\nautomated driving (AD) or advanced driver assistance system (ADAS). When\nfocusing on low-cost, large scale products for automated driving, model-driven\napproaches for the detection of lane markings have proven good performance.\nMore recently, data-driven approaches have been proposed that target the\ndrivable area / freespace mainly in inner-city applications. Focus of these\napproaches is less on lane-based driving due to the fact that the lane concept\ndoes not fully apply in unstructured, residential inner-city environments.\nSo-far the concept of drivable area is seldom used for highway and inter-urban\napplications due to the specific requirements of these scenarios that require\nclear lane associations of all traffic participants. We believe that\nlane-based, mapless driving in inter-urban and highway scenarios is still not\nfully handled with sufficient robustness and availability. Especially for\nchallenging weather situations such as heavy rain, fog, low-standing sun,\ndarkness or reflections in puddles, the mapless detection of lane markings\ndecreases significantly or completely fails. We see potential in applying\nspecifically designed data-driven freespace approaches in more lane-based\ndriving applications for highways and inter-urban use. Therefore, we propose to\nclassify specifically a drivable corridor of the ego lane on pixel level with a\ndeep learning approach. Our approach is kept computationally efficient with\nonly 0.66 million parameters allowing its application in large scale products.\nThus, we were able to easily integrate into an online AD system of a test\nvehicle. We demonstrate the performance of our approach under challenging\nconditions qualitatively and quantitatively in comparison to a state-of-the-art\nmodel-driven approach.",
          "link": "http://arxiv.org/abs/2004.07639",
          "publishedOn": "2021-06-25T02:00:46.614Z",
          "wordCount": 748,
          "title": "Where can I drive? A System Approach: Deep Ego Corridor Estimation for Robust Automated Driving. (arXiv:2004.07639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Accurate lighting estimation is challenging yet critical to many computer\nvision and computer graphics tasks such as high-dynamic-range (HDR) relighting.\nExisting approaches model lighting in either frequency domain or spatial domain\nwhich is insufficient to represent the complex lighting conditions in scenes\nand tends to produce inaccurate estimation. This paper presents NeedleLight, a\nnew lighting estimation model that represents illumination with needlets and\nallows lighting estimation in both frequency domain and spatial domain jointly.\nAn optimal thresholding function is designed to achieve sparse needlets which\ntrims redundant lighting parameters and demonstrates superior localization\nproperties for illumination representation. In addition, a novel spherical\ntransport loss is designed based on optimal transport theory which guides to\nregress lighting representation parameters with consideration of the spatial\ninformation. Furthermore, we propose a new metric that is concise yet effective\nby directly evaluating the estimated illumination maps rather than rendered\nimages. Extensive experiments show that NeedleLight achieves superior lighting\nestimation consistently across multiple evaluation metrics as compared with\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.13090",
          "publishedOn": "2021-06-25T02:00:46.597Z",
          "wordCount": 613,
          "title": "Sparse Needlets for Lighting Estimation with Spherical Transport Loss. (arXiv:2106.13090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.04960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baoquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lisai Zhang</a>",
          "description": "Few-shot learning is a challenging task, which aims to learn a classifier for\nnovel classes with few examples. Pre-training based meta-learning methods\neffectively tackle the problem by pre-training a feature extractor and then\nfine-tuning it through the nearest centroid based meta-learning. However,\nresults show that the fine-tuning step makes very marginal improvements. In\nthis paper, 1) we figure out the key reason, i.e., in the pre-trained feature\nspace, the base classes already form compact clusters while novel classes\nspread as groups with large variances, which implies that fine-tuning the\nfeature extractor is less meaningful; 2) instead of fine-tuning the feature\nextractor, we focus on estimating more representative prototypes during\nmeta-learning. Consequently, we propose a novel prototype completion based\nmeta-learning framework. This framework first introduces primitive knowledge\n(i.e., class-level part or attribute annotations) and extracts representative\nattribute features as priors. Then, we design a prototype completion network to\nlearn to complete prototypes with these priors. To avoid the prototype\ncompletion error caused by primitive knowledge noises or class differences, we\nfurther develop a Gaussian based prototype fusion strategy that combines the\nmean-based and completed prototypes by exploiting the unlabeled samples.\nExtensive experiments show that our method: (i) can obtain more accurate\nprototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of\nclassification accuracy. Our code is available online.",
          "link": "http://arxiv.org/abs/2009.04960",
          "publishedOn": "2021-06-25T02:00:46.214Z",
          "wordCount": 725,
          "title": "Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guzhov_A/0/1/0/all/0/1\">Andrey Guzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "In the past, the rapidly evolving field of sound classification greatly\nbenefited from the application of methods from other domains. Today, we observe\nthe trend to fuse domain-specific tasks and approaches together, which provides\nthe community with new outstanding models.\n\nIn this work, we present an extension of the CLIP model that handles audio in\naddition to text and images. Our proposed model incorporates the ESResNeXt\naudio-model into the CLIP framework using the AudioSet dataset. Such a\ncombination enables the proposed model to perform bimodal and unimodal\nclassification and querying, while keeping CLIP's ability to generalize to\nunseen datasets in a zero-shot inference fashion.\n\nAudioCLIP achieves new state-of-the-art results in the Environmental Sound\nClassification (ESC) task, out-performing other approaches by reaching\naccuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.\nFurther it sets new baselines in the zero-shot ESC-task on the same datasets\n68.78% and 69.40%, respectively).\n\nFinally, we also assess the cross-modal querying performance of the proposed\nmodel as well as the influence of full and partial training on the results. For\nthe sake of reproducibility, our code is published.",
          "link": "http://arxiv.org/abs/2106.13043",
          "publishedOn": "2021-06-25T02:00:46.136Z",
          "wordCount": 631,
          "title": "AudioCLIP: Extending CLIP to Image, Text and Audio. (arXiv:2106.13043v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>",
          "description": "Recently, language-guided global image editing draws increasing attention\nwith growing application potentials. However, previous GAN-based methods are\nnot only confined to domain-specific, low-resolution data but also lacking in\ninterpretability. To overcome the collective difficulties, we develop a\ntext-to-operation model to map the vague editing language request into a series\nof editing operations, e.g., change contrast, brightness, and saturation. Each\noperation is interpretable and differentiable. Furthermore, the only\nsupervision in the task is the target image, which is insufficient for a stable\ntraining of sequential decisions. Hence, we propose a novel operation planning\nalgorithm to generate possible editing sequences from the target image as\npseudo ground truth. Comparison experiments on the newly collected MA5k-Req\ndataset and GIER dataset show the advantages of our methods. Code is available\nat https://jshi31.github.io/T2ONet.",
          "link": "http://arxiv.org/abs/2106.13156",
          "publishedOn": "2021-06-25T02:00:46.129Z",
          "wordCount": 572,
          "title": "Learning by Planning: Language-Guided Global Image Editing. (arXiv:2106.13156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jizong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>",
          "description": "The scarcity of labeled data often impedes the application of deep learning\nto the segmentation of medical images. Semi-supervised learning seeks to\novercome this limitation by exploiting unlabeled examples in the learning\nprocess. In this paper, we present a novel semi-supervised segmentation method\nthat leverages mutual information (MI) on categorical distributions to achieve\nboth global representation invariance and local smoothness. In this method, we\nmaximize the MI for intermediate feature embeddings that are taken from both\nthe encoder and decoder of a segmentation network. We first propose a global MI\nloss constraining the encoder to learn an image representation that is\ninvariant to geometric transformations. Instead of resorting to\ncomputationally-expensive techniques for estimating the MI on continuous\nfeature embeddings, we use projection heads to map them to a discrete cluster\nassignment where MI can be computed efficiently. Our method also includes a\nlocal MI loss to promote spatial consistency in the feature maps of the decoder\nand provide a smoother segmentation. Since mutual information does not require\na strict ordering of clusters in two different assignments, we incorporate a\nfinal consistency regularization loss on the output which helps align the\ncluster labels throughout the network. We evaluate the method on four\nchallenging publicly-available datasets for medical image segmentation.\nExperimental results show our method to outperform recently-proposed approaches\nfor semi-supervised segmentation and provide an accuracy near to full\nsupervision while training with very few annotated images.",
          "link": "http://arxiv.org/abs/2103.04813",
          "publishedOn": "2021-06-25T02:00:46.008Z",
          "wordCount": 703,
          "title": "Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization. (arXiv:2103.04813v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-25T02:00:45.986Z",
          "wordCount": 745,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1\">Henning Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>",
          "description": "Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.",
          "link": "http://arxiv.org/abs/2104.12663",
          "publishedOn": "2021-06-25T02:00:45.981Z",
          "wordCount": 619,
          "title": "CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>",
          "description": "This paper introduces EfficientNetV2, a new family of convolutional networks\nthat have faster training speed and better parameter efficiency than previous\nmodels. To develop this family of models, we use a combination of\ntraining-aware neural architecture search and scaling, to jointly optimize\ntraining speed and parameter efficiency. The models were searched from the\nsearch space enriched with new ops such as Fused-MBConv. Our experiments show\nthat EfficientNetV2 models train much faster than state-of-the-art models while\nbeing up to 6.8x smaller.\n\nOur training can be further sped up by progressively increasing the image\nsize during training, but it often causes a drop in accuracy. To compensate for\nthis accuracy drop, we propose to adaptively adjust regularization (e.g.,\ndropout and data augmentation) as well, such that we can achieve both fast\ntraining and good accuracy.\n\nWith progressive learning, our EfficientNetV2 significantly outperforms\nprevious models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on\nthe same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on\nImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while\ntraining 5x-11x faster using the same computing resources. Code will be\navailable at https://github.com/google/automl/tree/master/efficientnetv2.",
          "link": "http://arxiv.org/abs/2104.00298",
          "publishedOn": "2021-06-25T02:00:45.853Z",
          "wordCount": 663,
          "title": "EfficientNetV2: Smaller Models and Faster Training. (arXiv:2104.00298v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.11034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shijie Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanrong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Nowadays, vision-based computing tasks play an important role in various\nreal-world applications. However, many vision computing tasks, e.g. semantic\nsegmentation, are usually computationally expensive, posing a challenge to the\ncomputing systems that are resource-constrained but require fast response\nspeed. Therefore, it is valuable to develop accurate and real-time vision\nprocessing models that only require limited computational resources. To this\nend, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)\nfor achieving real-time semantic segmentation. In SGCPNet, we propose the\nstrategy of spatial-detail guided context propagation. It uses the spatial\ndetails of shallow layers to guide the propagation of the low-resolution global\ncontexts, in which the lost spatial information can be effectively\nreconstructed. In this way, the need for maintaining high-resolution features\nalong the network is freed, therefore largely improving the model efficiency.\nOn the other hand, due to the effective reconstruction of spatial details, the\nsegmentation accuracy can be still preserved. In the experiments, we validate\nthe effectiveness and efficiency of the proposed SGCPNet model. On the\nCitysacpes dataset, for example, our SGCPNet achieves 69.5 % mIoU segmentation\naccuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX\n1080 Ti GPU card.",
          "link": "http://arxiv.org/abs/2005.11034",
          "publishedOn": "2021-06-25T02:00:45.810Z",
          "wordCount": 686,
          "title": "Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1\">Jack Dymond</a>",
          "description": "When machine learning models encounter data which is out of the distribution\non which they were trained they have a tendency to behave poorly, most\nprominently over-confidence in erroneous predictions. Such behaviours will have\ndisastrous effects on real-world machine learning systems. In this field\ngraceful degradation refers to the optimisation of model performance as it\nencounters this out-of-distribution data. This work presents a definition and\ndiscussion of graceful degradation and where it can be applied in deployed\nvisual systems. Following this a survey of relevant areas is undertaken,\nnovelly splitting the graceful degradation problem into active and passive\napproaches. In passive approaches, graceful degradation is handled and achieved\nby the model in a self-contained manner, in active approaches the model is\nupdated upon encountering epistemic uncertainties. This work communicates the\nimportance of the problem and aims to prompt the development of machine\nlearning strategies that are aware of graceful degradation.",
          "link": "http://arxiv.org/abs/2106.11119",
          "publishedOn": "2021-06-25T02:00:45.805Z",
          "wordCount": 592,
          "title": "Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_M/0/1/0/all/0/1\">Meng-Jiun Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chun-Yu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Detecting human-object interactions (HOI) is an important step toward a\ncomprehensive visual understanding of machines. While detecting non-temporal\nHOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely\neven for humans to guess temporal-related HOIs (e.g., opening/closing a door)\nfrom a single video frame, where the neighboring frames play an essential role.\nHowever, conventional HOI methods operating on only static images have been\nused to predict temporal-related interactions, which is essentially guessing\nwithout temporal contexts and may lead to sub-optimal performance. In this\npaper, we bridge this gap by detecting video-based HOIs with explicit temporal\ninformation. We first show that a naive temporal-aware variant of a common\naction detection baseline does not work on video-based HOIs due to a\nfeature-inconsistency issue. We then propose a simple yet effective\narchitecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal\ninformation such as human and object trajectories, correctly-localized visual\nfeatures, and spatial-temporal masking pose features. We construct a new video\nHOI benchmark dubbed VidHOI where our proposed approach serves as a solid\nbaseline.",
          "link": "http://arxiv.org/abs/2105.11731",
          "publishedOn": "2021-06-25T02:00:45.784Z",
          "wordCount": 666,
          "title": "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos. (arXiv:2105.11731v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kaicong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_S/0/1/0/all/0/1\">Sven Simon</a>",
          "description": "Deformable image registration is a fundamental task in medical imaging. Due\nto the large computational complexity of deformable registration of volumetric\nimages, conventional iterative methods usually face the tradeoff between the\nregistration accuracy and the computation time in practice. In order to boost\nthe registration performance in both accuracy and runtime, we propose a fast\nconvolutional neural network. Specially, to efficiently utilize the memory\nresources and enlarge the model capacity, we adopt additive forwarding instead\nof channel concatenation and deepen the network in each encoder and decoder\nstage. To facilitate the learning efficiency, we leverage skip connection\nwithin the encoder and decoder stages to enable residual learning and employ an\nauxiliary loss at the bottom layer with lowest resolution to involve deep\nsupervision. Particularly, the low-resolution auxiliary loss is weighted by an\nexponentially decayed parameter during the training phase. In conjunction with\nthe main loss in high-resolution grid, a coarse-to-fine learning strategy is\nachieved. Last but not least, we introduce an auxiliary loss based on the\nsegmentation prior to improve the registration performance in Dice score.\nComparing to the auxiliary loss using average Dice score, the proposed\nmulti-label segmentation loss does not induce additional memory cost in the\ntraining phase and can be employed on images with arbitrary amount of\ncategories. In the experiments, we show FDRN outperforms the existing\nstate-of-the-art registration methods for brain MR images by resorting to the\ncompact network structure and efficient learning. Besides, FDRN is a\ngeneralized framework for image registration which is not confined to a\nparticular type of medical images or anatomy.",
          "link": "http://arxiv.org/abs/2011.02307",
          "publishedOn": "2021-06-25T02:00:45.735Z",
          "wordCount": 744,
          "title": "FDRN: A Fast Deformable Registration Network for Medical Images. (arXiv:2011.02307v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1\">Ivaxi Sheth</a>",
          "description": "Understanding accurate information on human behaviours is one of the most\nimportant tasks in machine intelligence. Human Activity Recognition that aims\nto understand human activities from a video is a challenging task due to\nvarious problems including background, camera motion and dataset variations.\nThis paper proposes two CNN based architectures with three streams which allow\nthe model to exploit the dataset under different settings. The three pathways\nare differentiated in frame rates. The single pathway, operates at a single\nframe rate captures spatial information, the slow pathway operates at low frame\nrates captures the spatial information and the fast pathway operates at high\nframe rates that capture fine temporal information. Post CNN encoders, we add\nbidirectional LSTM and attention heads respectively to capture the context and\ntemporal features. By experimenting with various algorithms on UCF-101,\nKinetics-600 and AVA dataset, we observe that the proposed models achieve\nstate-of-art performance for human action recognition task.",
          "link": "http://arxiv.org/abs/2104.13051",
          "publishedOn": "2021-06-25T02:00:45.724Z",
          "wordCount": 607,
          "title": "Three-stream network for enriched Action Recognition. (arXiv:2104.13051v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.",
          "link": "http://arxiv.org/abs/2106.02795",
          "publishedOn": "2021-06-25T02:00:45.715Z",
          "wordCount": 598,
          "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10745",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1\">Adrian Celaya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1\">Jonas A. Actor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1\">Rajarajeswari Muthusivarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1\">Evan Gates</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Caroline Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1\">Dawid Schellingerhout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1\">Beatrice Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1\">David Fuentes</a>",
          "description": "Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.",
          "link": "http://arxiv.org/abs/2104.10745",
          "publishedOn": "2021-06-25T02:00:45.674Z",
          "wordCount": 586,
          "title": "PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>",
          "description": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
          "link": "http://arxiv.org/abs/2106.13230",
          "publishedOn": "2021-06-25T02:00:45.650Z",
          "wordCount": 606,
          "title": "Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1\">Divyansh Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>",
          "description": "DNN-based face recognition models require large centrally aggregated face\ndatasets for training. However, due to the growing data privacy concerns and\nlegal restrictions, accessing and sharing face datasets has become exceedingly\ndifficult. We propose FedFace, a federated learning (FL) framework for\ncollaborative learning of face recognition models in a privacy-aware manner.\nFedFace utilizes the face images available on multiple clients to learn an\naccurate and generalizable face recognition model where the face images stored\nat each client are neither shared with other clients nor the central host and\neach client is a mobile device containing face images pertaining to only the\nowner of the device (one identity per client). Our experiments show the\neffectiveness of FedFace in enhancing the verification performance of\npre-trained face recognition system on standard face verification benchmarks\nnamely LFW, IJB-A, and IJB-C.",
          "link": "http://arxiv.org/abs/2104.03008",
          "publishedOn": "2021-06-25T02:00:45.641Z",
          "wordCount": 598,
          "title": "FedFace: Collaborative Learning of Face Recognition Model. (arXiv:2104.03008v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>",
          "description": "Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks by significant margins. Compared to Nerfies, HyperNeRF reduces average\nerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as\nmeasured by LPIPS.",
          "link": "http://arxiv.org/abs/2106.13228",
          "publishedOn": "2021-06-25T02:00:45.625Z",
          "wordCount": 664,
          "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bui_K/0/1/0/all/0/1\">Kevin Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Fredrick Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingyong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jack Xin</a>",
          "description": "Convolutional neural networks (CNNs) have developed to become powerful models\nfor various computer vision tasks ranging from object detection to semantic\nsegmentation. However, most of state-of-the-art CNNs can not be deployed\ndirectly on edge devices such as smartphones and drones, which need low latency\nunder limited power and memory bandwidth. One popular, straightforward approach\nto compressing CNNs is network slimming, which imposes $\\ell_1$ regularization\non the channel-associated scaling factors via the batch normalization layers\nduring training. Network slimming thereby identifies insignificant channels\nthat can be pruned for inference. In this paper, we propose replacing the\n$\\ell_1$ penalty with an alternative sparse, nonconvex penalty in order to\nyield a more compressed and/or accurate CNN architecture. We investigate\n$\\ell_p (0 < p < 1)$, transformed $\\ell_1$ (T$\\ell_1$), minimax concave penalty\n(MCP), and smoothly clipped absolute deviation (SCAD) due to their recent\nsuccesses and popularity in solving sparse optimization problems, such as\ncompressed sensing and variable selection. We demonstrate the effectiveness of\nnetwork slimming with nonconvex penalties on VGGNet, Densenet, and Resnet on\nstandard image classification datasets. Based on the numerical experiments,\nT$\\ell_1$ preserves model accuracy against channel pruning, $\\ell_{1/2, 3/4}$\nyield better compressed models with similar accuracies after retraining as\n$\\ell_1$, and MCP and SCAD provide more accurate models after retraining with\nsimilar compression as $\\ell_1$. Network slimming with T$\\ell_1$ regularization\nalso outperforms the latest Bayesian modification of network slimming in\ncompressing a CNN architecture in terms of memory storage while preserving its\nmodel accuracy after channel pruning.",
          "link": "http://arxiv.org/abs/2010.01242",
          "publishedOn": "2021-06-25T02:00:45.613Z",
          "wordCount": 736,
          "title": "Improving Network Slimming with Nonconvex Regularization. (arXiv:2010.01242v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanjun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanping Hu</a>",
          "description": "Temporal grounding aims to localize temporal boundaries within untrimmed\nvideos by language queries, but it faces the challenge of two types of\ninevitable human uncertainties: query uncertainty and label uncertainty. The\ntwo uncertainties stem from human subjectivity, leading to limited\ngeneralization ability of temporal grounding. In this work, we propose a novel\nDeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We\nexplicitly disentangle each query into a relation feature and a modified\nfeature. The relation feature, which is mainly based on skeleton-like words\n(including nouns and verbs), aims to extract basic and consistent information\nin the presence of query uncertainty. Meanwhile, modified feature assigned with\nstyle-like words (including adjectives, adverbs, etc) represents the subjective\ninformation, and thus brings personalized predictions; De-bias - We propose a\nde-bias mechanism to generate diverse predictions, aim to alleviate the bias\ncaused by single-style annotations in the presence of label uncertainty.\nMoreover, we put forward new multi-label metrics to diversify the performance\nevaluation. Extensive experiments show that our approach is more effective and\nrobust than state-of-the-arts on Charades-STA and ActivityNet Captions\ndatasets.",
          "link": "http://arxiv.org/abs/2103.16848",
          "publishedOn": "2021-06-25T02:00:45.599Z",
          "wordCount": 655,
          "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding. (arXiv:2103.16848v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "The computation and memory needed for Convolutional Neural Network (CNN)\ninference can be reduced by pruning weights from the trained network. Pruning\nis guided by a pruning saliency, which heuristically approximates the change in\nthe loss function associated with the removal of specific weights. Many pruning\nsignals have been proposed, but the performance of each heuristic depends on\nthe particular trained network. This leaves the data scientist with a difficult\nchoice. When using any one saliency metric for the entire pruning process, we\nrun the risk of the metric assumptions being invalidated, leading to poor\ndecisions being made by the metric. Ideally we could combine the best aspects\nof different saliency metrics. However, despite an extensive literature review,\nwe are unable to find any prior work on composing different saliency metrics.\nThe chief difficulty lies in combining the numerical output of different\nsaliency metrics, which are not directly comparable.\n\nWe propose a method to compose several primitive pruning saliencies, to\nexploit the cases where each saliency measure does well. Our experiments show\nthat the composition of saliencies avoids many poor pruning choices identified\nby individual saliencies. In most cases our method finds better selections than\neven the best individual pruning saliency.",
          "link": "http://arxiv.org/abs/2004.03376",
          "publishedOn": "2021-06-25T02:00:45.594Z",
          "wordCount": 690,
          "title": "Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics. In this paper, we conduct the first, large-scale\nuser study on 320 lay and 11 expert users to shed light on the effectiveness of\nstate-of-the-art attribution methods in assisting humans in ImageNet\nclassification, Stanford Dogs fine-grained classification, and these two tasks\nbut when the input image contains adversarial perturbations. We found that, in\noverall, feature attribution is surprisingly not more effective than showing\nhumans nearest training-set examples. On a hard task of fine-grained dog\ncategorization, presenting attribution maps to humans does not help, but\ninstead hurts the performance of human-AI teams compared to AI alone.\nImportantly, we found automatic attribution-map evaluation measures to\ncorrelate poorly with the actual human-AI team performance. Our findings\nencourage the community to rigorously test their methods on the downstream\nhuman-in-the-loop applications and to rethink the existing evaluation metrics.",
          "link": "http://arxiv.org/abs/2105.14944",
          "publishedOn": "2021-06-25T02:00:45.588Z",
          "wordCount": 653,
          "title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aadith Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1\">Sasikanth Kotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Given the increase in the use of personal data for training Deep Neural\nNetworks (DNNs) in tasks such as medical imaging and diagnosis, differentially\nprivate training of DNNs is surging in importance and there is a huge body of\nwork focusing on providing better privacy-utility trade-off. However, little\nattention is given to the interpretability of these models, and how the\napplication of DP affects the quality of interpretations. We propose an\nextensive study into the effects of DP training on DNNs, especially on medical\nimaging applications, on the APTOS dataset.",
          "link": "http://arxiv.org/abs/2106.13203",
          "publishedOn": "2021-06-25T02:00:45.571Z",
          "wordCount": 544,
          "title": "When Differential Privacy Meets Interpretability: A Case Study. (arXiv:2106.13203v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1\">Youssef A.Mejjati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milefchik_I/0/1/0/all/0/1\">Isa Milefchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present an algorithm that learns a coarse 3D representation of objects\nfrom unposed multi-view 2D mask supervision, then uses it to generate detailed\nmask and image texture. In contrast to existing voxel-based methods for unposed\nobject reconstruction, our approach learns to represent the generated shape and\npose with a set of self-supervised canonical 3D anisotropic Gaussians via a\nperspective camera, and a set of per-image transforms. We show that this\napproach can robustly estimate a 3D space for the camera and object, while\nrecent baselines sometimes struggle to reconstruct coherent 3D spaces in this\nsetting. We show results on synthetic datasets with realistic lighting, and\ndemonstrate object insertion with interactive posing. With our work, we help\nmove towards structured representations that handle more real-world variation\nin learning-based object reconstruction.",
          "link": "http://arxiv.org/abs/2106.13215",
          "publishedOn": "2021-06-25T02:00:45.565Z",
          "wordCount": 574,
          "title": "GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed Silhouettes. (arXiv:2106.13215v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1\">Mohammad Babaeizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Taghi Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1\">Dumitru Erhan</a>",
          "description": "An agent that is capable of predicting what happens next can perform a\nvariety of tasks through planning with no additional training. Furthermore,\nsuch an agent can internally represent the complex dynamics of the real-world\nand therefore can acquire a representation useful for a variety of visual\nperception tasks. This makes predicting the future frames of a video,\nconditioned on the observed past and potentially future actions, an interesting\ntask which remains exceptionally challenging despite many recent advances.\nExisting video prediction models have shown promising results on simple narrow\nbenchmarks but they generate low quality predictions on real-life datasets with\nmore complicated dynamics or broader domain. There is a growing body of\nevidence that underfitting on the training data is one of the primary causes\nfor the low quality predictions. In this paper, we argue that the inefficient\nuse of parameters in the current video models is the main reason for\nunderfitting. Therefore, we introduce a new architecture, named FitVid, which\nis capable of severe overfitting on the common benchmarks while having similar\nparameter count as the current state-of-the-art models. We analyze the\nconsequences of overfitting, illustrating how it can produce unexpected\noutcomes such as generating high quality output by repeating the training data,\nand how it can be mitigated using existing image augmentation techniques. As a\nresult, FitVid outperforms the current state-of-the-art models across four\ndifferent video prediction benchmarks on four different metrics.",
          "link": "http://arxiv.org/abs/2106.13195",
          "publishedOn": "2021-06-25T02:00:45.559Z",
          "wordCount": 675,
          "title": "FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1\">Claudio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1\">Stefano Berretti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pala_P/0/1/0/all/0/1\">Pietro Pala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>",
          "description": "The 3D Morphable Model (3DMM) is a powerful statistical tool for representing\n3D face shapes. To build a 3DMM, a training set of face scans in full\npoint-to-point correspondence is required, and its modeling capabilities\ndirectly depend on the variability contained in the training data. Thus, to\nincrease the descriptive power of the 3DMM, establishing a dense correspondence\nacross heterogeneous scans with sufficient diversity in terms of identities,\nethnicities, or expressions becomes essential. In this manuscript, we present a\nfully automatic approach that leverages a 3DMM to transfer its dense semantic\nannotation across raw 3D faces, establishing a dense correspondence between\nthem. We propose a novel formulation to learn a set of sparse deformation\ncomponents with local support on the face that, together with an original\nnon-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces\nand transfer its semantic annotation. We extensively experimented our approach,\nshowing it can effectively generalize to highly diverse samples and accurately\nestablish a dense correspondence even in presence of complex facial\nexpressions. The accuracy of the dense registration is demonstrated by building\na heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans\nobtained by joining three large datasets together.",
          "link": "http://arxiv.org/abs/2006.03840",
          "publishedOn": "2021-06-25T02:00:45.554Z",
          "wordCount": 709,
          "title": "A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces. (arXiv:2006.03840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13150",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weiss_N/0/1/0/all/0/1\">Nick Weiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laak_J/0/1/0/all/0/1\">Jeroen van der Laak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+StefanHeldmann/0/1/0/all/0/1\">StefanHeldmann</a>",
          "description": "We compare variational image registration in consectutive and re-stained\nsections from histopathology. We present a fully-automatic algorithm for\nnon-parametric (nonlinear) image registration and apply it to a previously\nexisting dataset from the ANHIR challenge (230 slide pairs, consecutive\nsections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs,\nca. 3000 landmarks) which is made publicly available. Registration\nhyperparameters are obtained in the ANHIR dataset and applied to the new\ndataset without modification. In the new dataset, landmark errors after\nregistration range from 13.2 micrometers for consecutive sections to 1\nmicrometer for re-stained sections. We observe that non-parametric registration\nleads to lower landmark errors in both cases, even though the effect is smaller\nin re-stained sections. The nucleus-level alignment after non-parametric\nregistration of re-stained sections provides a valuable tool to generate\nautomatic ground-truth for machine learning applications in histopathology.",
          "link": "http://arxiv.org/abs/2106.13150",
          "publishedOn": "2021-06-25T02:00:45.548Z",
          "wordCount": 589,
          "title": "High-resolution Image Registration of Consecutive and Re-stained Sections in Histopathology. (arXiv:2106.13150v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Face recognition systems are extremely vulnerable to morphing attacks, in\nwhich a morphed facial reference image can be successfully verified as two or\nmore distinct identities. In this paper, we propose a morph attack detection\nalgorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for\nidentifying morphed face images. The core of our framework is that artifacts\nresulting from the morphing process that are not discernible in the image\ndomain can be more easily identified in the spatial frequency domain. A\ndiscriminative wavelet sub-band can accentuate the disparity between a real and\na morphed image. To this end, multi-level DWT is applied to all images,\nyielding 48 mid and high-frequency sub-bands each. The entropy distributions\nfor each sub-band are calculated separately for both bona fide and morph\nimages. For some of the sub-bands, there is a marked difference between the\nentropy of the sub-band in a bona fide image and the identical sub-band's\nentropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence\n(KLD) to exploit these differences and isolate the sub-bands that are the most\ndiscriminative. We measure how discriminative a sub-band is by its KLD value\nand the 22 sub-bands with the highest KLD values are chosen for network\ntraining. Then, we train a deep Siamese neural network using these 22 selected\nsub-bands for differential morph attack detection. We examine the efficacy of\ndiscriminative wavelet sub-bands for morph attack detection and show that a\ndeep neural network trained on these sub-bands can accurately identify morph\nimagery.",
          "link": "http://arxiv.org/abs/2106.13178",
          "publishedOn": "2021-06-25T02:00:45.531Z",
          "wordCount": 711,
          "title": "Differential Morph Face Detection using Discriminative Wavelet Sub-bands. (arXiv:2106.13178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khoshrou_A/0/1/0/all/0/1\">Abdolrahman Khoshrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Eric J. Pauwels</a>",
          "description": "Singular Value Decomposition (SVD) and its close relative, Principal\nComponent Analysis (PCA), are well-known linear matrix decomposition techniques\nthat are widely used in applications such as dimension reduction and\nclustering. However, an important limitation of SVD/PCA is its sensitivity to\nnoise in the input data. In this paper, we take another look at the problem of\nregularisation and show that different formulations of the minimisation problem\nlead to qualitatively different solutions.",
          "link": "http://arxiv.org/abs/2106.12955",
          "publishedOn": "2021-06-25T02:00:45.526Z",
          "wordCount": 508,
          "title": "Regularisation for PCA- and SVD-type matrix factorisations. (arXiv:2106.12955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Bo-Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>",
          "description": "In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.",
          "link": "http://arxiv.org/abs/2106.13033",
          "publishedOn": "2021-06-25T02:00:45.521Z",
          "wordCount": 548,
          "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training for VQA Challenge 2021. (arXiv:2106.13033v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chiyu &quot;Max&quot; Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jingwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "We present ShapeFlow, a flow-based model for learning a deformation space for\nentire classes of 3D shapes with large intra-class variations. ShapeFlow allows\nlearning a multi-template deformation space that is agnostic to shape topology,\nyet preserves fine geometric details. Different from a generative space where a\nlatent vector is directly decoded into a shape, a deformation space decodes a\nvector into a continuous flow that can advect a source shape towards a target.\nSuch a space naturally allows the disentanglement of geometric style (coming\nfrom the source) and structural pose (conforming to the target). We parametrize\nthe deformation between geometries as a learned continuous flow field via a\nneural network and show that such deformations can be guaranteed to have\ndesirable properties, such as be bijectivity, freedom from self-intersections,\nor volume preservation. We illustrate the effectiveness of this learned\ndeformation space for various downstream applications, including shape\ngeneration via deformation, geometric style transfer, unsupervised learning of\na consistent parameterization for entire classes of shapes, and shape\ninterpolation.",
          "link": "http://arxiv.org/abs/2006.07982",
          "publishedOn": "2021-06-25T02:00:45.506Z",
          "wordCount": 634,
          "title": "ShapeFlow: Learnable Deformations Among 3D Shapes. (arXiv:2006.07982v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dorman_K/0/1/0/all/0/1\">Karin S. Dorman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.",
          "link": "http://arxiv.org/abs/2006.03936",
          "publishedOn": "2021-06-25T02:00:45.499Z",
          "wordCount": 614,
          "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets. (arXiv:2006.03936v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13064",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yang_T/0/1/0/all/0/1\">Tianjie Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1\">Yaoru Luo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>",
          "description": "Super-resolution microscopy overcomes the diffraction limit of conventional\nlight microscopy in spatial resolution. By providing novel spatial or\nspatio-temporal information on biological processes at nanometer resolution\nwith molecular specificity, it plays an increasingly important role in life\nsciences. However, its technical limitations require trade-offs to balance its\nspatial resolution, temporal resolution, and light exposure of samples.\nRecently, deep learning has achieved breakthrough performance in many image\nprocessing and computer vision tasks. It has also shown great promise in\npushing the performance envelope of super-resolution microscopy. In this brief\nReview, we survey recent advances in using deep learning to enhance performance\nof super-resolution microscopy. We focus primarily on how deep learning\nad-vances reconstruction of super-resolution images. Related key technical\nchallenges are discussed. Despite the challenges, deep learning is set to play\nan indispensable and transformative role in the development of super-resolution\nmicroscopy. We conclude with an outlook on how deep learning could shape the\nfuture of this new generation of light microscopy technology.",
          "link": "http://arxiv.org/abs/2106.13064",
          "publishedOn": "2021-06-25T02:00:45.484Z",
          "wordCount": 608,
          "title": "Advancing biological super-resolution microscopy through deep learning: a brief review. (arXiv:2106.13064v1 [physics.bio-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1\">Mahmoud Hossam</a>",
          "description": "Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.",
          "link": "http://arxiv.org/abs/2106.12942",
          "publishedOn": "2021-06-25T02:00:45.478Z",
          "wordCount": 715,
          "title": "High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>",
          "description": "Collaborative learning, which enables collaborative and decentralized\ntraining of deep neural networks at multiple institutions in a\nprivacy-preserving manner, is rapidly emerging as a valuable technique in\nhealthcare applications. However, its distributed nature often leads to\nsignificant heterogeneity in data distributions across institutions. Existing\ncollaborative learning approaches generally do not account for the presence of\nheterogeneity in data among institutions, or only mildly skewed label\ndistributions are studied. In this paper, we present a novel generative replay\nstrategy to address the challenge of data heterogeneity in collaborative\nlearning methods. Instead of directly training a model for task performance, we\nleverage recent image synthesis techniques to develop a novel dual model\narchitecture: a primary model learns the desired task, and an auxiliary\n\"generative replay model\" either synthesizes images that closely resemble the\ninput images or helps extract latent variables. The generative replay strategy\nis flexible to use, can either be incorporated into existing collaborative\nlearning methods to improve their capability of handling data heterogeneity\nacross institutions, or be used as a novel and individual collaborative\nlearning framework (termed FedReplay) to reduce communication cost.\nExperimental results demonstrate the capability of the proposed method in\nhandling heterogeneous data across institutions. On highly heterogeneous data\npartitions, our model achieves ~4.88% improvement in the prediction accuracy on\na diabetic retinopathy classification dataset, and ~49.8% reduction of mean\nabsolution value on a Bone Age prediction dataset, respectively, compared to\nthe state-of-the art collaborative learning methods.",
          "link": "http://arxiv.org/abs/2106.13208",
          "publishedOn": "2021-06-25T02:00:45.471Z",
          "wordCount": 683,
          "title": "Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1\">Mochu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>",
          "description": "Camouflaged object detection (COD) aims to segment camouflaged objects hiding\nin the environment, which is challenging due to the similar appearance of\ncamouflaged objects and their surroundings. Research in biology suggests that\ndepth can provide useful object localization cues for camouflaged object\ndiscovery, as all the animals have 3D perception ability. However, the depth\ninformation has not been exploited for camouflaged object detection. To explore\nthe contribution of depth for camouflage detection, we present a depth-guided\ncamouflaged object detection network with pre-computed depth maps from existing\nmonocular depth estimation methods. Due to the domain gap between the depth\nestimation dataset and our camouflaged object detection dataset, the generated\ndepth may not be accurate enough to be directly used in our framework. We then\nintroduce a depth quality assessment module to evaluate the quality of depth\nbased on the model prediction from both RGB COD branch and RGB-D COD branch.\nDuring training, only high-quality depth is used to update the modal\ninteraction module for multi-modal learning. During testing, our depth quality\nassessment module can effectively determine the contribution of depth and\nselect the RGB branch or RGB-D branch for camouflage prediction. Extensive\nexperiments on various camouflaged object detection datasets prove the\neffectiveness of our solution in exploring the depth information for\ncamouflaged object detection. Our code and data is publicly available at:\n\\url{https://github.com/JingZhang617/RGBD-COD}.",
          "link": "http://arxiv.org/abs/2106.13217",
          "publishedOn": "2021-06-25T02:00:45.466Z",
          "wordCount": 676,
          "title": "Depth Confidence-aware Camouflaged Object Detection. (arXiv:2106.13217v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>",
          "description": "A massive number of traffic fatalities are due to driver errors. To reduce\nfatalities, developing intelligent driving systems assisting drivers to\nidentify potential risks is in urgent need. Risky situations are generally\ndefined based on collision prediction in existing research. However, collisions\nare only one type of risk in traffic scenarios. We believe a more generic\ndefinition is required. In this work, we propose a novel driver-centric\ndefinition of risk, i.e., risky objects influence driver behavior. Based on\nthis definition, a new task called risk object identification is introduced. We\nformulate the task as a cause-effect problem and present a novel two-stage risk\nobject identification framework, taking inspiration from models of situation\nawareness and causal inference. A driver-centric Risk Object Identification\n(ROI) dataset is curated to evaluate the proposed system. We demonstrate\nstate-of-the-art risk object identification performance compared with strong\nbaselines on the ROI dataset. In addition, we conduct extensive ablative\nstudies to justify our design choices.",
          "link": "http://arxiv.org/abs/2106.13201",
          "publishedOn": "2021-06-25T02:00:45.460Z",
          "wordCount": 591,
          "title": "Driver-centric Risk Object Identification. (arXiv:2106.13201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soler_M/0/1/0/all/0/1\">Marti Soler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1\">Stephane Ayache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guclu_U/0/1/0/all/0/1\">Umut Guclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baro_X/0/1/0/all/0/1\">Xavier Baro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a>",
          "description": "Dealing with incomplete information is a well studied problem in the context\nof machine learning and computational intelligence. However, in the context of\ncomputer vision, the problem has only been studied in specific scenarios (e.g.,\ncertain types of occlusions in specific types of images), although it is common\nto have incomplete information in visual data. This chapter describes the\ndesign of an academic competition focusing on inpainting of images and video\nsequences that was part of the competition program of WCCI2018 and had a\nsatellite event collocated with ECCV2018. The ChaLearn Looking at People\nInpainting Challenge aimed at advancing the state of the art on visual\ninpainting by promoting the development of methods for recovering missing and\noccluded information from images and video. Three tracks were proposed in which\nvisual inpainting might be helpful but still challenging: human body pose\nestimation, text overlays removal and fingerprint denoising. This chapter\ndescribes the design of the challenge, which includes the release of three\nnovel datasets, and the description of evaluation metrics, baselines and\nevaluation protocol. The results of the challenge are analyzed and discussed in\ndetail and conclusions derived from this event are outlined.",
          "link": "http://arxiv.org/abs/2106.13071",
          "publishedOn": "2021-06-25T02:00:45.445Z",
          "wordCount": 658,
          "title": "ChaLearn Looking at People: Inpainting and Denoising challenges. (arXiv:2106.13071v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-06-25T02:00:45.439Z",
          "wordCount": 593,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xueqing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuxin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newsam_S/0/1/0/all/0/1\">Shawn Newsam</a>",
          "description": "Neural network-based semantic segmentation has achieved remarkable results\nwhen large amounts of annotated data are available, that is, in the supervised\ncase. However, such data is expensive to collect and so methods have been\ndeveloped to adapt models trained on related, often synthetic data for which\nlabels are readily available. Current adaptation approaches do not consider the\ndependence of the generalization/transferability of these models on network\narchitecture. In this paper, we perform neural architecture search (NAS) to\nprovide architecture-level perspective and analysis for domain adaptation. We\nidentify the optimization gap that exists when searching architectures for\nunsupervised domain adaptation which makes this NAS problem uniquely difficult.\nWe propose bridging this gap by using maximum mean discrepancy and regional\nweighted entropy to estimate the accuracy metric. Experimental results on\nseveral widely adopted benchmarks show that our proposed AutoAdapt framework\nindeed discovers architectures that improve the performance of a number of\nexisting adaptation techniques.",
          "link": "http://arxiv.org/abs/2106.13227",
          "publishedOn": "2021-06-25T02:00:45.434Z",
          "wordCount": 603,
          "title": "AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain Adaptation. (arXiv:2106.13227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhuang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huajun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhihai Xu</a>",
          "description": "Moving target detection plays an important role in computer vision. However,\ntraditional algorithms such as frame difference and optical flow usually suffer\nfrom low accuracy or heavy computation. Recent algorithms such as deep\nlearning-based convolutional neural networks have achieved high accuracy and\nreal-time performance, but they usually need to know the classes of targets in\nadvance, which limits the practical applications. Therefore, we proposed a\nmodel free moving target detection algorithm. This algorithm extracts the\nmoving area through the difference of image features. Then, the color and\nlocation probability map of the moving area will be calculated through maximum\na posteriori probability. And the target probability map can be obtained\nthrough the dot multiply between the two maps. Finally, the optimal moving\ntarget area can be solved by stochastic gradient descent on the target\nprobability map. Results show that the proposed algorithm achieves the highest\naccuracy compared with state-of-the-art algorithms, without needing to know the\nclasses of targets. Furthermore, as the existing datasets are not suitable for\nmoving target detection, we proposed a method for producing evaluation dataset.\nBesides, we also proved the proposed algorithm can be used to assist target\ntracking.",
          "link": "http://arxiv.org/abs/2106.12966",
          "publishedOn": "2021-06-25T02:00:45.429Z",
          "wordCount": 637,
          "title": "Class agnostic moving target detection by color and location prediction of moving area. (arXiv:2106.12966v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Heejong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>",
          "description": "Current deep learning approaches for diffusion MRI modeling circumvent the\nneed for densely-sampled diffusion-weighted images (DWIs) by directly\npredicting microstructural indices from sparsely-sampled DWIs. However, they\nimplicitly make unrealistic assumptions of static $q$-space sampling during\ntraining and reconstruction. Further, such approaches can restrict downstream\nusage of variably sampled DWIs for usages including the estimation of\nmicrostructural indices or tractography. We propose a generative adversarial\ntranslation framework for high-quality DWI synthesis with arbitrary $q$-space\nsampling given commonly acquired structural images (e.g., B0, T1, T2). Our\ntranslation network linearly modulates its internal representations conditioned\non continuous $q$-space information, thus removing the need for fixed sampling\nschemes. Moreover, this approach enables downstream estimation of high-quality\nmicrostructural maps from arbitrarily subsampled DWIs, which may be\nparticularly important in cases with sparsely sampled DWIs. Across several\nrecent methodologies, the proposed approach yields improved DWI synthesis\naccuracy and fidelity with enhanced downstream utility as quantified by the\naccuracy of scalar microstructure indices estimated from the synthesized\nimages. Code is available at\nhttps://github.com/mengweiren/q-space-conditioned-dwi-synthesis.",
          "link": "http://arxiv.org/abs/2106.13188",
          "publishedOn": "2021-06-25T02:00:45.410Z",
          "wordCount": 645,
          "title": "Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_j/0/1/0/all/0/1\">jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "Temporal action localization aims to localize starting and ending time with\naction category. Limited by GPU memory, mainstream methods pre-extract features\nfor each video. Therefore, feature quality determines the upper bound of\ndetection performance. In this technical report, we explored classic\nconvolution-based backbones and the recent surge of transformer-based\nbackbones. We found that the transformer-based methods can achieve better\nclassification performance than convolution-based, but they cannot generate\naccuracy action proposals. In addition, extracting features with larger frame\nresolution to reduce the loss of spatial information can also effectively\nimprove the performance of temporal action localization. Finally, we achieve\n42.42% in terms of mAP on validation set with a single SlowFast feature by a\nsimple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's\nmulti-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS\nsupervised Temporal Action Localization Challenge.",
          "link": "http://arxiv.org/abs/2106.13014",
          "publishedOn": "2021-06-25T02:00:45.398Z",
          "wordCount": 597,
          "title": "Exploring Stronger Feature for Temporal Action Localization. (arXiv:2106.13014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rochow_A/0/1/0/all/0/1\">Andre Rochow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Michael Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>",
          "description": "We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our\nmulti-view approach addresses the problem that view synthesis methods are often\nlimited by their depth estimation stage, where incorrect depth predictions can\nlead to large projection errors. To avoid this issue, we efficiently warp\nmultiple input images into the target frame for a range of assumed depth\nplanes. The resulting tensor representation is fed into a U-Net-like CNN with\ngated convolutions, which directly produces the novel output view. We therefore\nside-step explicit depth estimation. This improves efficiency and performance\non transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle\nboth interpolation and extrapolation tasks and outperforms state-of-the-art\nextrapolation methods on the large-scale RealEstate10k dataset. In contrast to\ncomparable methods, it is capable of real-time operation due to its lightweight\narchitecture. We further demonstrate data efficiency of FaDIV-Syn by training\nfrom fewer examples as well as its generalization to higher resolutions and\narbitrary depth ranges under severe depth discretization.",
          "link": "http://arxiv.org/abs/2106.13139",
          "publishedOn": "2021-06-25T02:00:45.392Z",
          "wordCount": 588,
          "title": "FaDIV-Syn: Fast Depth-Independent View Synthesis. (arXiv:2106.13139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Leveraging the framework of Optimal Transport, we introduce a new family of\ngenerative autoencoders with a learnable prior, called Symmetric Wasserstein\nAutoencoders (SWAEs). We propose to symmetrically match the joint distributions\nof the observed data and the latent representation induced by the encoder and\nthe decoder. The resulting algorithm jointly optimizes the modelling losses in\nboth the data and the latent spaces with the loss in the data space leading to\nthe denoising effect. With the symmetric treatment of the data and the latent\nrepresentation, the algorithm implicitly preserves the local structure of the\ndata in the latent space. To further improve the quality of the latent\nrepresentation, we incorporate a reconstruction loss into the objective, which\nsignificantly benefits both the generation and reconstruction. We empirically\nshow the superior performance of SWAEs over the state-of-the-art generative\nautoencoders in terms of classification, reconstruction, and generation.",
          "link": "http://arxiv.org/abs/2106.13024",
          "publishedOn": "2021-06-25T02:00:45.386Z",
          "wordCount": 575,
          "title": "Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>",
          "description": "Visual recognition has been dominated by convolutionalneural networks (CNNs)\nfor years. Though recently the pre-vailing vision transformers (ViTs) have\nshown great poten-tial of self-attention based models in ImageNet\nclassifica-tion, their performance is still inferior to latest SOTA CNNsif no\nextra data are provided. In this work, we aim to closethe performance gap and\ndemonstrate that attention-basedmodels are indeed able to outperform CNNs. We\nfound thatthe main factor limiting the performance of ViTs for Ima-geNet\nclassification is their low efficacy in encoding fine-level features into the\ntoken representations. To resolvethis, we introduce a noveloutlook attentionand\npresent asimple and general architecture, termed Vision Outlooker(VOLO). Unlike\nself-attention that focuses on global depen-dency modeling at a coarse level,\nthe outlook attention aimsto efficiently encode finer-level features and\ncontexts intotokens, which are shown to be critical for recognition\nper-formance but largely ignored by the self-attention. Experi-ments show that\nour VOLO achieves 87.1% top-1 accuracyon ImageNet-1K classification, being the\nfirst model exceed-ing 87% accuracy on this competitive benchmark, withoutusing\nany extra training data. In addition, the pre-trainedVOLO transfers well to\ndownstream tasks, such as seman-tic segmentation. We achieve 84.3% mIoU score\non thecityscapes validation set and 54.3% on the ADE20K valida-tion set. Code\nis available at https://github.com/sail-sg/volo.",
          "link": "http://arxiv.org/abs/2106.13112",
          "publishedOn": "2021-06-25T02:00:45.360Z",
          "wordCount": 642,
          "title": "VOLO: Vision Outlooker for Visual Recognition. (arXiv:2106.13112v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1\">Sandareka Wickramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Despite the remarkable performance, Deep Neural Networks (DNNs) behave as\nblack-boxes hindering user trust in Artificial Intelligence (AI) systems.\nResearch on opening black-box DNN can be broadly categorized into post-hoc\nmethods and inherently interpretable DNNs. While many surveys have been\nconducted on post-hoc interpretation methods, little effort is devoted to\ninherently interpretable DNNs. This paper provides a review of existing methods\nto develop DNNs with intrinsic interpretability, with a focus on Convolutional\nNeural Networks (CNNs). The aim is to understand the current progress towards\nfully interpretable DNNs that can cater to different interpretation\nrequirements. Finally, we identify gaps in current work and suggest potential\nresearch directions.",
          "link": "http://arxiv.org/abs/2106.13164",
          "publishedOn": "2021-06-25T02:00:45.352Z",
          "wordCount": 563,
          "title": "Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1\">Gregor K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1\">Paul F. J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1\">David Zimmerer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1\">Ulf Neuberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1\">Wolfgang Wick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1\">J&#xfc;rgen Debus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1\">Sabine Heiland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1\">Martin Bendszus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1\">Philipp Vollmuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>",
          "description": "The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.",
          "link": "http://arxiv.org/abs/2106.12917",
          "publishedOn": "2021-06-25T02:00:45.325Z",
          "wordCount": 626,
          "title": "Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengjie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shugong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shan Cao</a>",
          "description": "Depth completion aims to generate a dense depth map from the sparse depth map\nand aligned RGB image. However, current depth completion methods use extremely\nexpensive 64-line LiDAR(about $100,000) to obtain sparse depth maps, which will\nlimit their application scenarios. Compared with the 64-line LiDAR, the\nsingle-line LiDAR is much less expensive and much more robust. Therefore, we\npropose a method to tackle the problem of single-line depth completion, in\nwhich we aim to generate a dense depth map from the single-line LiDAR info and\nthe aligned RGB image. A single-line depth completion dataset is proposed based\non the existing 64-line depth completion dataset(KITTI). A network called\nSemantic Guided Two-Branch Network(SGTBN) which contains global and local\nbranches to extract and fuse global and local info is proposed for this task. A\nSemantic guided depth upsampling module is used in our network to make full use\nof the semantic info in RGB images. Except for the usual MSE loss, we add the\nvirtual normal loss to increase the constraint of high-order 3D geometry in our\nnetwork. Our network outperforms the state-of-the-art in the single-line depth\ncompletion task. Besides, compared with the monocular depth estimation, our\nmethod also has significant advantages in precision and model size.",
          "link": "http://arxiv.org/abs/2106.12994",
          "publishedOn": "2021-06-25T02:00:45.319Z",
          "wordCount": 640,
          "title": "SGTBN: Generating Dense Depth Maps from Single-Line LiDAR. (arXiv:2106.12994v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>",
          "description": "Understanding the 3D world from 2D projected natural images is a fundamental\nchallenge in computer vision and graphics. Recently, an unsupervised learning\napproach has garnered considerable attention owing to its advantages in data\ncollection. However, to mitigate training limitations, typical methods need to\nimpose assumptions for viewpoint distribution (e.g., a dataset containing\nvarious viewpoint images) or object shape (e.g., symmetric objects). These\nassumptions often restrict applications; for instance, the application to\nnon-rigid objects or images captured from similar viewpoints (e.g., flower or\nbird images) remains a challenge. To complement these approaches, we propose\naperture rendering generative adversarial networks (AR-GANs), which equip\naperture rendering on top of GANs, and adopt focus cues to learn the depth and\ndepth-of-field (DoF) effect of unlabeled natural images. To address the\nambiguities triggered by unsupervised setting (i.e., ambiguities between smooth\ntexture and out-of-focus blurs, and between foreground and background blurs),\nwe develop DoF mixture learning, which enables the generator to learn real\nimage distribution while generating diverse DoF images. In addition, we devise\na center focus prior to guiding the learning direction. In the experiments, we\ndemonstrate the effectiveness of AR-GANs in various datasets, such as flower,\nbird, and face images, demonstrate their portability by incorporating them into\nother 3D representation learning GANs, and validate their applicability in\nshallow DoF rendering.",
          "link": "http://arxiv.org/abs/2106.13041",
          "publishedOn": "2021-06-25T02:00:45.303Z",
          "wordCount": 689,
          "title": "Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shancheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yadong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>",
          "description": "Existing scene text removal methods mainly train an elaborate network with\npaired images to realize the function of text localization and background\nreconstruction simultaneously, but there exists two problems: 1) lacking the\nexhaustive erasure of text region and 2) causing the excessive erasure to\ntext-free areas. To handle these issues, this paper provides a novel\nProgrEssively Region-based scene Text eraser (PERT), which introduces\nregion-based modification strategy to progressively erase the pixels in only\ntext region. Firstly, PERT decomposes the STR task to several erasing stages.\nAs each stage aims to take a further step toward the text-removed image rather\nthan directly regress to the final result, the decomposed operation reduces the\nlearning difficulty in each stage, and an exhaustive erasure result can be\nobtained by iterating over lightweight erasing blocks with shared parameters.\nThen, PERT introduces a region-based modification strategy to ensure the\nintegrity of text-free areas by decoupling text localization from erasure\nprocess to guide the removal. Benefiting from the simplicity architecture, PERT\nis a simple and strong baseline, and is easy to be followed and developed.\nExtensive experiments demonstrate that PERT obtains the state-of-the-art\nresults on both synthetic and real-world datasets. Code is available\nathttps://github.com/wangyuxin87/PERT.",
          "link": "http://arxiv.org/abs/2106.13029",
          "publishedOn": "2021-06-25T02:00:45.297Z",
          "wordCount": 645,
          "title": "A Simple and Strong Baseline: Progressively Region-based Scene Text Removal Networks. (arXiv:2106.13029v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guozhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lele Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaqiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>",
          "description": "Visual Information Extraction (VIE) task aims to extract key information from\nmultifarious document images (e.g., invoices and purchase receipts). Most\nprevious methods treat the VIE task simply as a sequence labeling problem or\nclassification problem, which requires models to carefully identify each kind\nof semantics by introducing multimodal features, such as font, color, layout.\nBut simply introducing multimodal features couldn't work well when faced with\nnumeric semantic categories or some ambiguous texts. To address this issue, in\nthis paper we propose a novel key-value matching model based on a graph neural\nnetwork for VIE (MatchVIE). Through key-value matching based on relevancy\nevaluation, the proposed MatchVIE can bypass the recognitions to various\nsemantics, and simply focuses on the strong relevancy between entities.\nBesides, we introduce a simple but effective operation, Num2Vec, to tackle the\ninstability of encoded values, which helps model converge more smoothly.\nComprehensive experiments demonstrate that the proposed MatchVIE can\nsignificantly outperform previous methods. Notably, to the best of our\nknowledge, MatchVIE may be the first attempt to tackle the VIE task by modeling\nthe relevancy between keys and values and it is a good complement to the\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.12940",
          "publishedOn": "2021-06-25T02:00:45.288Z",
          "wordCount": 650,
          "title": "MatchVIE: Exploiting Match Relevancy between Entities for Visual Information Extraction. (arXiv:2106.12940v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiqi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1\">Carl S Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panneer_S/0/1/0/all/0/1\">Selvakumar Panneer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>",
          "description": "Monte Carlo rendering algorithms are widely used to produce photorealistic\ncomputer graphics images. However, these algorithms need to sample a\nsubstantial amount of rays per pixel to enable proper global illumination and\nthus require an immense amount of computation. In this paper, we present a\nhybrid rendering method to speed up Monte Carlo rendering algorithms. Our\nmethod first generates two versions of a rendering: one at a low resolution\nwith a high sample rate (LRHS) and the other at a high resolution with a low\nsample rate (HRLS). We then develop a deep convolutional neural network to fuse\nthese two renderings into a high-quality image as if it were rendered at a high\nresolution with a high sample rate. Specifically, we formulate this fusion task\nas a super resolution problem that generates a high resolution rendering from a\nlow resolution input (LRHS), assisted with the HRLS rendering. The HRLS\nrendering provides critical high frequency details which are difficult to\nrecover from the LRHS for any super resolution methods. Our experiments show\nthat our hybrid rendering algorithm is significantly faster than the\nstate-of-the-art Monte Carlo denoising methods while rendering high-quality\nimages when tested on both our own BCR dataset and the Gharbi dataset.\n\\url{https://github.com/hqqxyy/msspl}",
          "link": "http://arxiv.org/abs/2106.12802",
          "publishedOn": "2021-06-25T02:00:45.282Z",
          "wordCount": 648,
          "title": "Fast Monte Carlo Rendering via Multi-Resolution Sampling. (arXiv:2106.12802v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Ruibing Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xinqian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>",
          "description": "Person re-identification (reID) plays an important role in computer vision.\nHowever, existing methods suffer from performance degradation in occluded\nscenes. In this work, we propose an occlusion-robust block, Region Feature\nCompletion (RFC), for occluded reID. Different from most previous works that\ndiscard the occluded regions, RFC block can recover the semantics of occluded\nregions in feature space. Firstly, a Spatial RFC (SRFC) module is developed.\nSRFC exploits the long-range spatial contexts from non-occluded regions to\npredict the features of occluded regions. The unit-wise prediction task leads\nto an encoder/decoder architecture, where the region-encoder models the\ncorrelation between non-occluded and occluded region, and the region-decoder\nutilizes the spatial correlation to recover occluded region features. Secondly,\nwe introduce Temporal RFC (TRFC) module which captures the long-term temporal\ncontexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end\ntrainable and can be easily plugged into existing CNNs to form RFCnet.\nExtensive experiments are conducted on occluded and commonly holistic reID\nbenchmarks. Our method significantly outperforms existing methods on the\nocclusion datasets, while remains top even superior performance on holistic\ndatasets. The source code is available at\nhttps://github.com/blue-blue272/OccludedReID-RFCnet.",
          "link": "http://arxiv.org/abs/2106.12733",
          "publishedOn": "2021-06-25T02:00:45.276Z",
          "wordCount": 655,
          "title": "Feature Completion for Occluded Person Re-Identification. (arXiv:2106.12733v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yulei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1\">Feng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yue-Min Zhu</a>",
          "description": "To investigate whether the pleurae, airways and vessels surrounding a nodule\non non-contrast computed tomography (CT) can discriminate benign and malignant\npulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available\nCT database, was exploited for study. A total of 1556 nodules from 694 patients\nwere involved in statistical analysis, where nodules with average scorings <3\nand >3 were respectively denoted as benign and malignant. Besides, 339 nodules\nfrom 113 patients with diagnosis ground-truth were independently evaluated.\nComputer algorithms were developed to segment pulmonary structures and quantify\nthe distances to pleural surface, airways and vessels, as well as the counting\nnumber and normalized volume of airways and vessels near a nodule. Odds ratio\n(OR) and Chi-square (\\chi^2) testing were performed to demonstrate the\ncorrelation between features of surrounding structures and nodule malignancy. A\nnon-parametric receiver operating characteristic (ROC) analysis was conducted\nin logistic regression to evaluate discrimination ability of each structure.\nFor benign and malignant groups, the average distances from nodules to pleural\nsurface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and\n(1.42, 1.07) mm. The correlation between nodules and the counting number of\nairways and vessels that contact or project towards nodules are respectively\n(OR=22.96, \\chi^2=105.04) and (OR=7.06, \\chi^2=290.11). The correlation between\nnodules and the volume of airways and vessels are (OR=9.19, \\chi^2=159.02) and\n(OR=2.29, \\chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and\nvessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that\nmalignant nodules are often surrounded by more pulmonary structures compared\nwith benign ones, suggesting that features of these structures could be viewed\nas lung cancer biomarkers.",
          "link": "http://arxiv.org/abs/2106.12991",
          "publishedOn": "2021-06-25T02:00:45.270Z",
          "wordCount": 751,
          "title": "Relationship between pulmonary nodule malignancy and surrounding pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI dataset. (arXiv:2106.12991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12746",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yue_J/0/1/0/all/0/1\">Jian Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yanbo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1\">Hui Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dufaux_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Dufaux</a>",
          "description": "In-loop filtering is used in video coding to process the reconstructed frame\nin order to remove blocking artifacts. With the development of convolutional\nneural networks (CNNs), CNNs have been explored for in-loop filtering\nconsidering it can be treated as an image de-noising task. However, in addition\nto being a distorted image, the reconstructed frame is also obtained by a fixed\nline of block based encoding operations in video coding. It carries coding-unit\nbased coding distortion of some similar characteristics. Therefore, in this\npaper, we address the filtering problem from two aspects, global appearance\nrestoration for disrupted texture and local coding distortion restoration\ncaused by fixed pipeline of coding. Accordingly, a three-stream global\nappearance and local coding distortion based fusion network is developed with a\nhigh-level global feature stream, a high-level local feature stream and a\nlow-level local feature stream. Ablation study is conducted to validate the\nnecessity of different features, demonstrating that the global features and\nlocal features can complement each other in filtering and achieve better\nperformance when combined. To the best of our knowledge, we are the first one\nthat clearly characterizes the video filtering process from the above global\nappearance and local coding distortion restoration aspects with experimental\nverification, providing a clear pathway to developing filter techniques.\nExperimental results demonstrate that the proposed method significantly\noutperforms the existing single-frame based methods and achieves 13.5%, 11.3%,\n11.7% BD-Rate saving on average for AI, LDP and RA configurations,\nrespectively, compared with the HEVC reference software.",
          "link": "http://arxiv.org/abs/2106.12746",
          "publishedOn": "2021-06-25T02:00:45.264Z",
          "wordCount": 710,
          "title": "A Global Appearance and Local Coding Distortion based Fusion Framework for CNN based Filtering in Video Coding. (arXiv:2106.12746v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_P/0/1/0/all/0/1\">Parul Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1\">Rudrabha Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Sindhu B Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C V Jawahar</a>",
          "description": "We aim to solve the highly challenging task of generating continuous sign\nlanguage videos solely from speech segments for the first time. Recent efforts\nin this space have focused on generating such videos from human-annotated text\ntranscripts without considering other modalities. However, replacing speech\nwith sign language proves to be a practical solution while communicating with\npeople suffering from hearing loss. Therefore, we eliminate the need of using\ntext as input and design techniques that work for more natural, continuous,\nfreely uttered speech covering an extensive vocabulary. Since the current\ndatasets are inadequate for generating sign language directly from speech, we\ncollect and release the first Indian sign language dataset comprising\nspeech-level annotations, text transcripts, and the corresponding sign-language\nvideos. Next, we propose a multi-tasking transformer network trained to\ngenerate signer's poses from speech segments. With speech-to-text as an\nauxiliary task and an additional cross-modal discriminator, our model learns to\ngenerate continuous sign pose sequences in an end-to-end manner. Extensive\nexperiments and comparisons with other baselines demonstrate the effectiveness\nof our approach. We also conduct additional ablation studies to analyze the\neffect of different modules of our network. A demo video containing several\nresults is attached to the supplementary material.",
          "link": "http://arxiv.org/abs/2106.12790",
          "publishedOn": "2021-06-25T02:00:45.258Z",
          "wordCount": 648,
          "title": "Towards Automatic Speech to Sign Language Generation. (arXiv:2106.12790v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1\">Fahim Faisal Niloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">M. Ashraful Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Amin Ahsan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">AKM Mahbubur Rahman</a>",
          "description": "High-resolution image segmentation remains challenging and error-prone due to\nthe enormous size of intermediate feature maps. Conventional methods avoid this\nproblem by using patch based approaches where each patch is segmented\nindependently. However, independent patch segmentation induces errors,\nparticularly at the patch boundary due to the lack of contextual information in\nvery high-resolution images where the patch size is much smaller compared to\nthe full image. To overcome these limitations, in this paper, we propose a\nnovel framework to segment a particular patch by incorporating contextual\ninformation from its neighboring patches. This allows the segmentation network\nto see the target patch with a wider field of view without the need of larger\nfeature maps. Comparative analysis from a number of experiments shows that our\nproposed framework is able to segment high resolution images with significantly\nimproved mean Intersection over Union and overall accuracy.",
          "link": "http://arxiv.org/abs/2106.12902",
          "publishedOn": "2021-06-25T02:00:45.238Z",
          "wordCount": 596,
          "title": "Attention Toward Neighbors: A Context Aware Framework for High Resolution Image Segmentation. (arXiv:2106.12902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1\">Thang Q. Huynh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1\">Minh Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">Van Vu</a>",
          "description": "Radiographs are used as the most important imaging tool for identifying spine\nanomalies in clinical practice. The evaluation of spinal bone lesions, however,\nis a challenging task for radiologists. This work aims at developing and\nevaluating a deep learning-based framework, named VinDr-SpineXR, for the\nclassification and localization of abnormalities from spine X-rays. First, we\nbuild a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,\neach of which is manually annotated by an experienced radiologist with bounding\nboxes around abnormal findings in 13 categories. Using this dataset, we then\ntrain a deep learning classifier to determine whether a spine scan is abnormal\nand a detector to localize 7 crucial findings amongst the total 13. The\nVinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,\nwhich is kept separate from the training set. It demonstrates an area under the\nreceiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,\n90.02%) for the image-level classification task and a mean average precision\n(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve\nas a proof of concept and set a baseline for future research in this direction.\nTo encourage advances, the dataset, codes, and trained deep learning models are\nmade publicly available.",
          "link": "http://arxiv.org/abs/2106.12930",
          "publishedOn": "2021-06-25T02:00:45.228Z",
          "wordCount": 702,
          "title": "VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Guotao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Existing video super-resolution methods often utilize a few neighboring\nframes to generate a higher-resolution image for each frame. However, the\nredundant information between distant frames has not been fully exploited in\nthese methods: corresponding patches of the same instance appear across distant\nframes at different scales. Based on this observation, we propose a video\nsuper-resolution method with long-term cross-scale aggregation that leverages\nsimilar patches (self-exemplars) across distant frames. Our model also consists\nof a multi-reference alignment module to fuse the features derived from similar\npatches: we fuse the features of distant references to perform high-quality\nsuper-resolution. We also propose a novel and practical training strategy for\nreferenced-based super-resolution. To evaluate the performance of our proposed\nmethod, we conduct extensive experiments on our collected CarCam dataset and\nthe Waymo Open dataset, and the results demonstrate our method outperforms\nstate-of-the-art methods. Our source code will be publicly available.",
          "link": "http://arxiv.org/abs/2106.12778",
          "publishedOn": "2021-06-25T02:00:45.220Z",
          "wordCount": 576,
          "title": "Video Super-Resolution with Long-Term Self-Exemplars. (arXiv:2106.12778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadip Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1\">Swapna Sourav Rout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Sudeep Choudhary</a>",
          "description": "Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.",
          "link": "http://arxiv.org/abs/2106.12871",
          "publishedOn": "2021-06-25T02:00:45.198Z",
          "wordCount": 617,
          "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12954",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Ziqing Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "End-to-end optimization capability offers neural image compression (NIC)\nsuperior lossy compression performance. However, distinct models are required\nto be trained to reach different points in the rate-distortion (R-D) space. In\nthis paper, we consider the problem of R-D characteristic analysis and modeling\nfor NIC. We make efforts to formulate the essential mathematical functions to\ndescribe the R-D behavior of NIC using deep network and statistical modeling.\nThus continuous bit-rate points could be elegantly realized by leveraging such\nmodel via a single trained network. In this regard, we propose a plugin-in\nmodule to learn the relationship between the target bit-rate and the binary\nrepresentation for the latent variable of auto-encoder. Furthermore, we model\nthe rate and distortion characteristic of NIC as a function of the coding\nparameter $\\lambda$ respectively. Our experiments show our proposed method is\neasy to adopt and obtains competitive coding performance with fixed-rate coding\napproaches, which would benefit the practical deployment of NIC. In addition,\nthe proposed model could be applied to NIC rate control with limited bit-rate\nerror using a single network.",
          "link": "http://arxiv.org/abs/2106.12954",
          "publishedOn": "2021-06-25T02:00:45.183Z",
          "wordCount": 630,
          "title": "Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12864",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Johann Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1\">BasheerBennamoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Juan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1\">Lin Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "The astounding success made by artificial intelligence (AI) in healthcare and\nother fields proves that AI can achieve human-like performance. However,\nsuccess always comes with challenges. Deep learning algorithms are\ndata-dependent and require large datasets for training. The lack of data in the\nmedical imaging field creates a bottleneck for the application of deep learning\nto medical image analysis. Medical image acquisition, annotation, and analysis\nare costly, and their usage is constrained by ethical restrictions. They also\nrequire many resources, such as human expertise and funding. That makes it\ndifficult for non-medical researchers to have access to useful and large\nmedical data. Thus, as comprehensive as possible, this paper provides a\ncollection of medical image datasets with their associated challenges for deep\nlearning research. We have collected information of around three hundred\ndatasets and challenges mainly reported between 2013 and 2020 and categorized\nthem into four categories: head & neck, chest & abdomen, pathology & blood, and\n``others''. Our paper has three purposes: 1) to provide a most up to date and\ncomplete list that can be used as a universal reference to easily find the\ndatasets for clinical image analysis, 2) to guide researchers on the\nmethodology to test and evaluate their methods' performance and robustness on\nrelevant datasets, 3) to provide a ``route'' to relevant algorithms for the\nrelevant medical topics, and challenge leaderboards.",
          "link": "http://arxiv.org/abs/2106.12864",
          "publishedOn": "2021-06-25T02:00:45.167Z",
          "wordCount": 708,
          "title": "A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xianfeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>",
          "description": "With the rapid progress of deepfake techniques in recent years, facial video\nforgery can generate highly deceptive video contents and bring severe security\nthreats. And detection of such forgery videos is much more urgent and\nchallenging. Most existing detection methods treat the problem as a vanilla\nbinary classification problem. In this paper, the problem is treated as a\nspecial fine-grained classification problem since the differences between fake\nand real faces are very subtle. It is observed that most existing face forgery\nmethods left some common artifacts in the spatial domain and time domain,\nincluding generative defects in the spatial domain and inter-frame\ninconsistencies in the time domain. And a spatial-temporal model is proposed\nwhich has two components for capturing spatial and temporal forgery traces in\nglobal perspective respectively. The two components are designed using a novel\nlong distance attention mechanism. The one component of the spatial domain is\nused to capture artifacts in a single frame, and the other component of the\ntime domain is used to capture artifacts in consecutive frames. They generate\nattention maps in the form of patches. The attention method has a broader\nvision which contributes to better assembling global information and extracting\nlocal statistic information. Finally, the attention maps are used to guide the\nnetwork to focus on pivotal parts of the face, just like other fine-grained\nclassification methods. The experimental results on different public datasets\ndemonstrate that the proposed method achieves the state-of-the-art performance,\nand the proposed long distance attention method can effectively capture pivotal\nparts for face forgery.",
          "link": "http://arxiv.org/abs/2106.12832",
          "publishedOn": "2021-06-25T02:00:45.150Z",
          "wordCount": 696,
          "title": "Detection of Deepfake Videos Using Long Distance Attention. (arXiv:2106.12832v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Ee Fey Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">ZhiYuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1\">Wei Xiang Lim</a>",
          "description": "The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.",
          "link": "http://arxiv.org/abs/2106.12736",
          "publishedOn": "2021-06-25T02:00:45.143Z",
          "wordCount": 680,
          "title": "Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qiuyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>",
          "description": "In the past few years, we have witnessed rapid development of autonomous\ndriving. However, achieving full autonomy remains a daunting task due to the\ncomplex and dynamic driving environment. As a result, self-driving cars are\nequipped with a suite of sensors to conduct robust and accurate environment\nperception. As the number and type of sensors keep increasing, combining them\nfor better perception is becoming a natural trend. So far, there has been no\nindepth review that focuses on multi-sensor fusion based perception. To bridge\nthis gap and motivate future research, this survey devotes to review recent\nfusion-based 3D detection deep learning models that leverage multiple sensor\ndata sources, especially cameras and LiDARs. In this survey, we first introduce\nthe background of popular sensors for autonomous cars, including their common\ndata representations as well as object detection networks developed for each\ntype of sensor data. Next, we discuss some popular datasets for multi-modal 3D\nobject detection, with a special focus on the sensor data included in each\ndataset. Then we present in-depth reviews of recent multi-modal 3D detection\nnetworks by considering the following three aspects of the fusion: fusion\nlocation, fusion data representation, and fusion granularity. After a detailed\nreview, we discuss open challenges and point out possible solutions. We hope\nthat our detailed review can help researchers to embark investigations in the\narea of multi-modal 3D object detection.",
          "link": "http://arxiv.org/abs/2106.12735",
          "publishedOn": "2021-06-25T02:00:45.127Z",
          "wordCount": 669,
          "title": "Multi-Modal 3D Object Detection in Autonomous Driving: a Survey. (arXiv:2106.12735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanbin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengyang Li</a>",
          "description": "The autonomous real-time optical navigation of planetary UAV is of the key\ntechnologies to ensure the success of the exploration. In such a GPS denied\nenvironment, vision-based localization is an optimal approach. In this paper,\nwe proposed a multi-modal registration based SLAM algorithm, which estimates\nthe location of a planet UAV using a nadir view camera on the UAV compared with\npre-existing digital terrain model. To overcome the scale and appearance\ndifference between on-board UAV images and pre-installed digital terrain model,\na theoretical model is proposed to prove that topographic features of UAV image\nand DEM can be correlated in frequency domain via cross power spectrum. To\nprovide the six-DOF of the UAV, we also developed an optimization approach\nwhich fuses the geo-referencing result into a SLAM system via LBA (Local Bundle\nAdjustment) to achieve robust and accurate vision-based navigation even in\nfeatureless planetary areas. To test the robustness and effectiveness of the\nproposed localization algorithm, a new cross-source drone-based localization\ndataset for planetary exploration is proposed. The proposed dataset includes\n40200 synthetic drone images taken from nine planetary scenes with related DEM\nquery images. Comparison experiments carried out demonstrate that over the\nflight distance of 33.8km, the proposed method achieved average localization\nerror of 0.45 meters, compared to 1.31 meters by ORB-SLAM, with the processing\nspeed of 12hz which will ensure a real-time performance. We will make our\ndatasets available to encourage further work on this promising topic.",
          "link": "http://arxiv.org/abs/2106.12738",
          "publishedOn": "2021-06-25T02:00:45.120Z",
          "wordCount": 679,
          "title": "Planetary UAV localization based on Multi-modal Registration with Pre-existing Digital Terrain Model. (arXiv:2106.12738v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chumerin_N/0/1/0/all/0/1\">Nikolay Chumerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Novelty Detection methods identify samples that are not representative of a\nmodel's training set thereby flagging misleading predictions and bringing a\ngreater flexibility and transparency at deployment time. However, research in\nthis area has only considered Novelty Detection in the offline setting.\nRecently, there has been a growing realization in the computer vision community\nthat applications demand a more flexible framework - Continual Learning - where\nnew batches of data representing new domains, new classes or new tasks become\navailable at different points in time. In this setting, Novelty Detection\nbecomes more important, interesting and challenging. This work identifies the\ncrucial link between the two problems and investigates the Novelty Detection\nproblem under the Continual Learning setting. We formulate the Continual\nNovelty Detection problem and present a benchmark, where we compare several\nNovelty Detection methods under different Continual Learning settings.\n\nWe show that Continual Learning affects the behaviour of novelty detection\nalgorithms, while novelty detection can pinpoint insights in the behaviour of a\ncontinual learner. We further propose baselines and discuss possible research\ndirections. We believe that the coupling of the two problems is a promising\ndirection to bring vision models into practice.",
          "link": "http://arxiv.org/abs/2106.12964",
          "publishedOn": "2021-06-25T02:00:45.114Z",
          "wordCount": 621,
          "title": "Continual Novelty Detection. (arXiv:2106.12964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schubert_S/0/1/0/all/0/1\">Stefan Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubert_P/0/1/0/all/0/1\">Peer Neubert</a>",
          "description": "Visual place recognition is a fundamental capability for the localization of\nmobile robots. It places image retrieval in the practical context of physical\nagents operating in a physical world. It is an active field of research and\nmany different approaches have been proposed and evaluated in many different\nexperiments. In the following, we argue that due to variations of this\npractical context and individual design decisions, place recognition\nexperiments are barely comparable across different papers and that there is a\nvariety of properties that can change from one experiment to another. We\nprovide an extensive list of such properties and give examples how they can be\nused to setup a place recognition experiment easier or harder. This might be\ninteresting for different involved parties: (1) people who just want to select\na place recognition approach that is suitable for the properties of their\nparticular task at hand, (2) researchers that look for open research questions\nand are interested in particularly difficult instances, (3) authors that want\nto create reproducible papers on this topic, and (4) also reviewers that have\nthe task to identify potential problems in papers under review.",
          "link": "http://arxiv.org/abs/2106.12671",
          "publishedOn": "2021-06-25T02:00:45.108Z",
          "wordCount": 624,
          "title": "What makes visual place recognition easy or hard?. (arXiv:2106.12671v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Traditional feature-based image stitching technologies rely heavily on\nfeature detection quality, often failing to stitch images with few features or\nlow resolution. The learning-based image stitching solutions are rarely studied\ndue to the lack of labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep image stitching\nframework consisting of two stages: unsupervised coarse image alignment and\nunsupervised image reconstruction. In the first stage, we design an\nablation-based loss to constrain an unsupervised homography network, which is\nmore suitable for large-baseline scenes. Moreover, a transformer layer is\nintroduced to warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments in pixel-level\ncan be eliminated to a certain extent in feature-level, we design an\nunsupervised image reconstruction network to eliminate the artifacts from\nfeatures to pixels. Specifically, the reconstruction network can be implemented\nby a low-resolution deformation branch and a high-resolution refined branch,\nlearning the deformation rules of image stitching and enhancing the resolution\nsimultaneously. To establish an evaluation benchmark and train the learning\nframework, a comprehensive real-world image dataset for unsupervised deep image\nstitching is presented and released. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions. Even compared\nwith the supervised solutions, our image stitching quality is still preferred\nby users.",
          "link": "http://arxiv.org/abs/2106.12859",
          "publishedOn": "2021-06-25T02:00:45.101Z",
          "wordCount": 670,
          "title": "Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images. (arXiv:2106.12859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12628",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Crystal Gagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1\">Jyoti Kini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">Daniel Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Trail camera imagery has increasingly gained popularity amongst biologists\nfor conservation and ecological research. Minimal human interference required\nto operate camera traps allows capturing unbiased species activities. Several\nstudies - based on human and wildlife interactions, migratory patterns of\nvarious species, risk of extinction in endangered populations - are limited by\nthe lack of rich data and the time-consuming nature of manually annotating\ntrail camera imagery. We introduce a challenging wildlife camera trap\nclassification dataset collected from two different locations in Southwestern\nFlorida, consisting of 104,495 images featuring visually similar species,\nvarying illumination conditions, skewed class distribution, and including\nsamples of endangered species, i.e. Florida panthers. Experimental evaluations\nwith ResNet-50 architecture indicate that this image classification-based\ndataset can further push the advancements in wildlife statistical modeling. We\nwill make the dataset publicly available.",
          "link": "http://arxiv.org/abs/2106.12628",
          "publishedOn": "2021-06-25T02:00:45.078Z",
          "wordCount": 589,
          "title": "Florida Wildlife Camera Trap Dataset. (arXiv:2106.12628v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Arbitrary-shaped text detection is a challenging task since curved texts in\nthe wild are of the complex geometric layouts. Existing mainstream methods\nfollow the instance segmentation pipeline to obtain the text regions. However,\narbitraryshaped texts are difficult to be depicted through one single\nsegmentation network because of the varying scales. In this paper, we propose a\ntwo-stage segmentation-based detector, termed as NASK (Need A Second looK), for\narbitrary-shaped text detection. Compared to the traditional single-stage\nsegmentation network, our NASK conducts the detection in a coarse-to-fine\nmanner with the first stage segmentation spotting the rectangle text proposals\nand the second one retrieving compact representations. Specifically, NASK is\ncomposed of a Text Instance Segmentation (TIS) network (1st stage), a\nGeometry-aware Text RoI Alignment (GeoAlign) module, and a Fiducial pOint\neXpression (FOX) module (2nd stage). Firstly, TIS extracts the augmented\nfeatures with a novel Group Spatial and Channel Attention (GSCA) module and\nconducts instance segmentation to obtain rectangle proposals. Then, GeoAlign\nconverts these rectangles into the fixed size and encodes RoI-wise feature\nrepresentation. Finally, FOX disintegrates the text instance into serval\npivotal geometrical attributes to refine the detection results. Extensive\nexperimental results on three public benchmarks including Total-Text,\nSCUTCTW1500, and ICDAR 2015 verify that our NASK outperforms recent\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12720",
          "publishedOn": "2021-06-25T02:00:45.072Z",
          "wordCount": 652,
          "title": "All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection. (arXiv:2106.12720v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12728",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nie_G/0/1/0/all/0/1\">Guanxiong Nie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yajian Zhou</a>",
          "description": "Compressed Sensing (CS) theory simultaneously realizes the signal sampling\nand compression process, and can use fewer observations to achieve accurate\nsignal recovery, providing a solution for better and faster transmission of\nmassive data. In this paper, a ternary sampling matrix-based method with\nattention mechanism is proposed with the purpose to solve the problem that the\nCS sampling matrices in most cases are random matrices, which are irrelative to\nthe sampled signal and need a large storage space. The proposed method consists\nof three components, i.e., ternary sampling, initial reconstruction and deep\nreconstruction, with the emphasis on the ternary sampling. The main idea of the\nternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate\nthe importance of parameters at the sampling layer after the sampling matrix is\nbinarized (-1, +1), followed by pruning weight of parameters, whose importance\nis below a predefined threshold, to achieve ternarization. Furthermore, a\ncompressed sensing algorithm especially for image reconstruction is\nimplemented, on the basis of the ternary sampling matrix, which is called\nATP-Net, i.e., Attention-based ternary projection network. Experimental results\nshow that the quality of image reconstruction by means of ATP-Net maintains a\nsatisfactory level with the employment of the ternary sampling matrix, i.e.,\nthe average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately\n6% improvement compared with that of DR2-Net.",
          "link": "http://arxiv.org/abs/2106.12728",
          "publishedOn": "2021-06-25T02:00:45.044Z",
          "wordCount": 668,
          "title": "ATP-Net: An Attention-based Ternary Projection Network For Compressed Sensing. (arXiv:2106.12728v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nedorubova_A/0/1/0/all/0/1\">Anna Nedorubova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadyrova_A/0/1/0/all/0/1\">Alena Kadyrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khlyupin_A/0/1/0/all/0/1\">Aleksey Khlyupin</a>",
          "description": "Quite a few people in the world have to stay under permanent surveillance for\nhealth reasons; they include diabetic people or people with some other chronic\nconditions, the elderly and the disabled.These groups may face heightened risk\nof having life-threatening falls or of being struck by a syncope. Due to\nlimited availability of resources a substantial part of people at risk can not\nreceive necessary monitoring and thus are exposed to excessive danger.\nNowadays, this problem is usually solved via applying Human Activity\nRecognition (HAR) methods. HAR is a perspective and fast-paced Data Science\nfield, which has a wide range of application areas such as healthcare, sport,\nsecurity etc. However, the currently techniques of recognition are markedly\nlacking in accuracy, hence, the present paper suggests a highly accurate method\nfor human activity classification. Wepropose a new workflow to address the HAR\nproblem and evaluate it on the UniMiB SHAR dataset, which consists of the\naccelerometer signals. The model we suggest is based on continuous wavelet\ntransform (CWT) and convolutional neural networks (CNNs). Wavelet transform\nlocalizes signal features both in time and frequency domains and after that a\nCNN extracts these features and recognizes activity. It is also worth noting\nthat CWT converts 1D accelerometer signal into 2D images and thus enables to\nobtain better results as 2D networks have a significantly higher predictive\ncapacity. In the course of the work we build a convolutional neural network and\nvary such model parameters as number of spatial axes, number of layers, number\nof neurons in each layer, image size, type of mother wavelet, the order of zero\nmoment of mother wavelet etc. Besides, we also apply models with residual\nblocks which resulted in significantly higher metric values. Finally, we\nsucceed to reach 99.26 % accuracy and it is a worthy performance for this\nproblem.",
          "link": "http://arxiv.org/abs/2106.12666",
          "publishedOn": "2021-06-25T02:00:45.035Z",
          "wordCount": 745,
          "title": "Human Activity Recognition using Continuous Wavelet Transform and Convolutional Neural Networks. (arXiv:2106.12666v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1\">Ygor C. N. Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>",
          "description": "Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN, the GoogLeNet, from 2D images captured in multiple views of\nthe environment as the robot operates, to create consolidated representations\nof visual features acquired in the regions covered by each topological node.\nThese consolidated representations allow flexible recognition of semantic\nproperties of the regions and use in a range of visual tasks. The experiments,\nperformed using a real-world indoor dataset, showed that the method is able to\nconsolidate the visual features of regions and use them to recognize objects\nand place categories as semantic properties, and to indicate the topological\nlocation of images, with very promising results. The objects are classified\nusing the classification layer of GoogLeNet, without retraining, and the place\ncategories are recognized using a shallow Multilayer Perceptron.",
          "link": "http://arxiv.org/abs/2106.12709",
          "publishedOn": "2021-06-25T02:00:45.015Z",
          "wordCount": 662,
          "title": "Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>",
          "description": "Recent deep learning-based methods have shown promising results and runtime\nadvantages in deformable image registration. However, analyzing the effects of\nhyperparameters and searching for optimal regularization parameters prove to be\ntoo prohibitive in deep learning-based methods. This is because it involves\ntraining a substantial number of separate models with distinct hyperparameter\nvalues. In this paper, we propose a conditional image registration method and a\nnew self-supervised learning paradigm for deep deformable image registration.\nBy learning the conditional features that correlated with the regularization\nhyperparameter, we demonstrate that optimal solutions with arbitrary\nhyperparameters can be captured by a single deep convolutional neural network.\nIn addition, the smoothness of the resulting deformation field can be\nmanipulated with arbitrary strength of smoothness regularization during\ninference. Extensive experiments on a large-scale brain MRI dataset show that\nour proposed method enables the precise control of the smoothness of the\ndeformation field without sacrificing the runtime advantage or registration\naccuracy.",
          "link": "http://arxiv.org/abs/2106.12673",
          "publishedOn": "2021-06-25T02:00:44.981Z",
          "wordCount": 604,
          "title": "Conditional Deformable Image Registration with Convolutional Neural Network. (arXiv:2106.12673v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
          "link": "http://arxiv.org/abs/2103.03206",
          "publishedOn": "2021-06-24T01:51:44.801Z",
          "wordCount": 679,
          "title": "Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>",
          "description": "Generative adversarial networks (GANs) synthesize realistic images from\nrandom latent vectors. Although manipulating the latent vectors controls the\nsynthesized outputs, editing real images with GANs suffers from i)\ntime-consuming optimization for projecting real images to the latent vectors,\nii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the\nintermediate latent space has spatial dimensions, and a spatially variant\nmodulation replaces AdaIN. It makes the embedding through an encoder more\naccurate than existing optimization-based methods while maintaining the\nproperties of GANs. Experimental results demonstrate that our method\nsignificantly outperforms state-of-the-art models in various image manipulation\ntasks such as local editing and image interpolation. Last but not least,\nconventional editing methods on GANs are still valid on our StyleMapGAN. Source\ncode is available at https://github.com/naver-ai/StyleMapGAN.",
          "link": "http://arxiv.org/abs/2104.14754",
          "publishedOn": "2021-06-24T01:51:44.790Z",
          "wordCount": 608,
          "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.",
          "link": "http://arxiv.org/abs/2104.06392",
          "publishedOn": "2021-06-24T01:51:44.766Z",
          "wordCount": 739,
          "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-06-24T01:51:44.760Z",
          "wordCount": 610,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-06-24T01:51:44.713Z",
          "wordCount": 649,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02869",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1\">Ahmet Topcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1\">Sachin Jambawalikar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>",
          "description": "Visual explanation methods have an important role in the prognosis of the\npatients where the annotated data is limited or unavailable. There have been\nseveral attempts to use gradient-based attribution methods to localize\npathology from medical scans without using segmentation labels. This research\ndirection has been impeded by the lack of robustness and reliability. These\nmethods are highly sensitive to the network parameters. In this study, we\nintroduce a robust visual explanation method to address this problem for\nmedical applications. We provide an innovative visual explanation algorithm for\ngeneral purpose and as an example application, we demonstrate its effectiveness\nfor quantifying lesions in the lungs caused by the Covid-19 with high accuracy\nand robustness without using dense segmentation labels. This approach overcomes\nthe drawbacks of commonly used Grad-CAM and its extended versions. The premise\nbehind our proposed strategy is that the information flow is minimized while\nensuring the classifier prediction stays similar. Our findings indicate that\nthe bottleneck condition provides a more stable severity estimation than the\nsimilar attribution methods.",
          "link": "http://arxiv.org/abs/2104.02869",
          "publishedOn": "2021-06-24T01:51:43.731Z",
          "wordCount": 697,
          "title": "Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Weituo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1\">Gregory Spell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "The outbreak of COVID-19 Disease due to the novel coronavirus has caused a\nshortage of medical resources. To aid and accelerate the diagnosis process,\nautomatic diagnosis of COVID-19 via deep learning models has recently been\nexplored by researchers across the world. While different data-driven deep\nlearning models have been developed to mitigate the diagnosis of COVID-19, the\ndata itself is still scarce due to patient privacy concerns. Federated Learning\n(FL) is a natural solution because it allows different organizations to\ncooperatively learn an effective deep learning model without sharing raw data.\nHowever, recent studies show that FL still lacks privacy protection and may\ncause data leakage. We investigate this challenging problem by proposing a\nsimple yet effective algorithm, named \\textbf{F}ederated \\textbf{L}earning\n\\textbf{o}n Medical Datasets using \\textbf{P}artial Networks (FLOP), that\nshares only a partial model between the server and clients. Extensive\nexperiments on benchmark data and real-world healthcare tasks show that our\napproach achieves comparable or better performance while reducing the privacy\nand security risks. Of particular interest, we conduct experiments on the\nCOVID-19 dataset and find that our FLOP algorithm can allow different hospitals\nto collaboratively and effectively train a partially shared model without\nsharing local patients' data.",
          "link": "http://arxiv.org/abs/2102.05218",
          "publishedOn": "2021-06-24T01:51:43.725Z",
          "wordCount": 722,
          "title": "FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNNs. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that the token-mixing MLP is a variant of the depthwise convolution\nwith a global reception field and spatial-specific configuration. But the\nglobal reception field and the spatial-specific property make token-mixing MLP\nprone to over-fitting. In this paper, we propose a novel pure MLP architecture,\nspatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only\ncontains channel-mixing MLP. We utilize a spatial-shift operation for\ncommunications between patches. It has a local reception field and is\nspatial-agnostic. It is parameter-free and efficient for computation. The\nproposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when\ntraining on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent\nperformance as ViT on ImageNet-1K dataset with considerably simpler\narchitecture and fewer FLOPs and parameters.",
          "link": "http://arxiv.org/abs/2106.07477",
          "publishedOn": "2021-06-24T01:51:43.676Z",
          "wordCount": 678,
          "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15093",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chai_S/0/1/0/all/0/1\">Seoin Chai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fetit_A/0/1/0/all/0/1\">Ahmed E. Fetit</a>",
          "description": "Despite advances in deep learning, robustness under domain shift remains a\nmajor bottleneck in medical imaging settings. Findings on natural images\nsuggest that deep neural models can show a strong textural bias when carrying\nout image classification tasks. In this thorough empirical study, we draw\ninspiration from findings on natural images and investigate ways in which\naddressing the textural bias phenomenon could bring up the robustness of deep\nsegmentation models when applied to three-dimensional (3D) medical data. To\nachieve this, publicly available MRI scans from the Developing Human Connectome\nProject are used to study ways in which simulating textural noise can help\ntrain robust models in a complex semantic segmentation task. We contribute an\nextensive empirical investigation consisting of 176 experiments and illustrate\nhow applying specific types of simulated textural noise prior to training can\nlead to texture invariant models, resulting in improved robustness when\nsegmenting scans corrupted by previously unseen noise types and levels.",
          "link": "http://arxiv.org/abs/2011.15093",
          "publishedOn": "2021-06-24T01:51:43.661Z",
          "wordCount": 625,
          "title": "Reducing Textural Bias Improves Robustness of Deep Segmentation Models. (arXiv:2011.15093v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhangcheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiali Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_E/0/1/0/all/0/1\">Eric Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qijing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Current low-precision quantization algorithms often have the hidden cost of\nconversion back and forth from floating point to quantized integer values. This\nhidden cost limits the latency improvement realized by quantizing Neural\nNetworks. To address this, we present HAWQV3, a novel mixed-precision\ninteger-only quantization framework. The contributions of HAWQV3 are the\nfollowing: (i) An integer-only inference where the entire computational graph\nis performed only with integer multiplication, addition, and bit shifting,\nwithout any floating point operations or even integer division; (ii) A novel\nhardware-aware mixed-precision quantization method where the bit-precision is\ncalculated by solving an integer linear programming problem that balances the\ntrade-off between model perturbation and other constraints, e.g., memory\nfootprint and latency; (iii) Direct hardware deployment and open source\ncontribution for 4-bit uniform/mixed-precision quantization in TVM, achieving\nan average speed up of $1.45\\times$ for uniform 4-bit, as compared to uniform\n8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed\nmethods on ResNet18/50 and InceptionV3, for various model compression levels\nwith/without mixed precision. For ResNet50, our INT8 quantization achieves an\naccuracy of $77.58\\%$, which is $2.68\\%$ higher than prior integer-only work,\nand our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\\%$\nand still achieve $76.73\\%$ accuracy. Our framework and the TVM implementation\nhave been open sourced.",
          "link": "http://arxiv.org/abs/2011.10680",
          "publishedOn": "2021-06-24T01:51:43.582Z",
          "wordCount": 701,
          "title": "HAWQV3: Dyadic Neural Network Quantization. (arXiv:2011.10680v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-06-24T01:51:43.523Z",
          "wordCount": 670,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.11277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">Irwandi Hipiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">Hamimah Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujahid_A/0/1/0/all/0/1\">Aazani Mujahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahya_N/0/1/0/all/0/1\">Nurhartini Kamalia Yahya</a>",
          "description": "Passive biometric identification enables wildlife monitoring with minimal\ndisturbance. Using a motion-activated camera placed at an elevated position and\nfacing downwards, we collected images of sea turtle carapace, each belonging to\none of sixteen Chelonia mydas juveniles. We then learned co-variant and robust\nimage descriptors from these images, enabling indexing and retrieval. In this\nwork, we presented several classification results of sea turtle carapaces using\nthe learned image descriptors. We found that a template-based descriptor, i.e.,\nHistogram of Oriented Gradients (HOG) performed exceedingly better during\nclassification than keypoint-based descriptors. For our dataset, a\nhigh-dimensional descriptor is a must due to the minimal gradient and color\ninformation inside the carapace images. Using HOG, we obtained an average\nclassification accuracy of 65%.",
          "link": "http://arxiv.org/abs/1909.11277",
          "publishedOn": "2021-06-24T01:51:43.517Z",
          "wordCount": 624,
          "title": "Towards Automated Biometric Identification of Sea Turtles (Chelonia mydas). (arXiv:1909.11277v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaying Zhu</a>",
          "description": "Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.",
          "link": "http://arxiv.org/abs/2105.14300",
          "publishedOn": "2021-06-24T01:51:43.486Z",
          "wordCount": 629,
          "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nuriel_O/0/1/0/all/0/1\">Oren Nuriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Recent work has shown that convolutional neural network classifiers overly\nrely on texture at the expense of shape cues. We make a similar but different\ndistinction between shape and local image cues, on the one hand, and global\nimage statistics, on the other. Our method, called Permuted Adaptive Instance\nNormalization (pAdaIN), reduces the representation of global statistics in the\nhidden layers of image classifiers. pAdaIN samples a random permutation $\\pi$\nthat rearranges the samples in a given batch. Adaptive Instance Normalization\n(AdaIN) is then applied between the activations of each (non-permuted) sample\n$i$ and the corresponding activations of the sample $\\pi(i)$, thus swapping\nstatistics between the samples of the batch. Since the global image statistics\nare distorted, this swapping procedure causes the network to rely on cues, such\nas shape or texture. By choosing the random permutation with probability $p$\nand the identity permutation otherwise, one can control the effect's strength.\n\nWith the correct choice of $p$, fixed apriori for all experiments and\nselected without considering test data, our method consistently outperforms\nbaselines in multiple settings. In image classification, our method improves on\nboth CIFAR100 and ImageNet using multiple architectures. In the setting of\nrobustness, our method improves on both ImageNet-C and Cifar-100-C for multiple\narchitectures. In the setting of domain adaptation and domain generalization,\nour method achieves state of the art results on the transfer learning task from\nGTAV to Cityscapes and on the PACS benchmark.",
          "link": "http://arxiv.org/abs/2010.05785",
          "publishedOn": "2021-06-24T01:51:43.481Z",
          "wordCount": 721,
          "title": "Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification. (arXiv:2010.05785v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alexander Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard S. Zemel</a>",
          "description": "Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.",
          "link": "http://arxiv.org/abs/2009.04806",
          "publishedOn": "2021-06-24T01:51:43.468Z",
          "wordCount": 589,
          "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1\">Laura D&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1\">Felix Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1\">Alexander Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1\">Martin Pouls</a>",
          "description": "While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.",
          "link": "http://arxiv.org/abs/2104.09123",
          "publishedOn": "2021-06-24T01:51:43.462Z",
          "wordCount": 586,
          "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1\">Konstantinos Rematas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>",
          "description": "We present a method for estimating neural scenes representations of objects\ngiven only a single image. The core of our method is the estimation of a\ngeometric scaffold for the object and its use as a guide for the reconstruction\nof the underlying radiance field. Our formulation is based on a generative\nprocess that first maps a latent code to a voxelized shape, and then renders it\nto an image, with the object appearance being controlled by a second latent\ncode. During inference, we optimize both the latent codes and the networks to\nfit a test image of a new object. The explicit disentanglement of shape and\nappearance allows our model to be fine-tuned given a single image. We can then\nrender new views in a geometrically consistent manner and they represent\nfaithfully the input object. Additionally, our method is able to generalize to\nimages outside of the training domain (more realistic renderings and even real\nphotographs). Finally, the inferred geometric scaffold is itself an accurate\nestimate of the object's 3D shape. We demonstrate in several experiments the\neffectiveness of our approach in both synthetic and real images.",
          "link": "http://arxiv.org/abs/2102.08860",
          "publishedOn": "2021-06-24T01:51:43.447Z",
          "wordCount": 659,
          "title": "ShaRF: Shape-conditioned Radiance Fields from a Single View. (arXiv:2102.08860v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "Reflecting on the last few years, the biggest breakthroughs in deep\nreinforcement learning (RL) have been in the discrete action domain. Robotic\nmanipulation, however, is inherently a continuous control environment, but\nthese continuous control reinforcement learning algorithms often depend on\nactor-critic methods that are sample-inefficient and inherently difficult to\ntrain, due to the joint optimisation of the actor and critic. To that end, we\nexplore how we can bring the stability of discrete action RL algorithms to the\nrobot manipulation domain. We extend the recently released ARM algorithm, by\nreplacing the continuous next-best pose agent with a discrete next-best pose\nagent. Discretisation of rotation is trivial given its bounded nature, while\ntranslation is inherently unbounded, making discretisation difficult. We\nformulate the translation prediction as the voxel prediction problem by\ndiscretising the 3D space; however, voxelisation of a large workspace is memory\nintensive and would not work with a high density of voxels, crucial to\nobtaining the resolution needed for robotic manipulation. We therefore propose\nto apply this voxel prediction in a coarse-to-fine manner by gradually\nincreasing the resolution. In each step, we extract the highest valued voxel as\nthe predicted location, which is then used as the centre of the\nhigher-resolution voxelisation in the next step. This coarse-to-fine prediction\nis applied over several steps, giving a near-lossless prediction of the\ntranslation. We show that our new coarse-to-fine algorithm is able to\naccomplish RLBench tasks much more efficiently than the continuous control\nequivalent, and even train some real-world tasks, tabular rasa, in less than 7\nminutes, with only 3 demonstrations. Moreover, we show that by moving to a\nvoxel representation, we are able to easily incorporate observations from\nmultiple cameras.",
          "link": "http://arxiv.org/abs/2106.12534",
          "publishedOn": "2021-06-24T01:51:43.435Z",
          "wordCount": 736,
          "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1\">Alex Hernandez-Garcia</a>",
          "description": "The renaissance of artificial neural networks was catalysed by the success of\nclassification models, tagged by the community with the broader term supervised\nlearning. The extraordinary results gave rise to a hype loaded with ambitious\npromises and overstatements. Soon the community realised that the success owed\nmuch to the availability of thousands of labelled examples and supervised\nlearning went, for many, from glory to shame: Some criticised deep learning as\na whole and others proclaimed that the way forward had to be alternatives to\nsupervised learning: predictive, unsupervised, semi-supervised and, more\nrecently, self-supervised learning. However, all these seem brand names, rather\nthan actual categories of a theoretically grounded taxonomy. Moreover, the call\nto banish supervised learning was motivated by the questionable claim that\nhumans learn with little or no supervision and are capable of robust\nout-of-distribution generalisation. Here, we review insights about learning and\nsupervision in nature, revisit the notion that learning and generalisation are\nnot possible without supervision or inductive biases and argue that we will\nmake better progress if we just call it by its name.",
          "link": "http://arxiv.org/abs/2012.02526",
          "publishedOn": "2021-06-24T01:51:43.429Z",
          "wordCount": 666,
          "title": "Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1\">Hongfei Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>",
          "description": "In this work, we propose a no-reference video quality assessment method,\naiming to achieve high-generalization capability in cross-content, -resolution\nand -frame rate quality prediction. In particular, we evaluate the quality of a\nvideo by learning effective feature representations in spatial-temporal domain.\nIn the spatial domain, to tackle the resolution and content variations, we\nimpose the Gaussian distribution constraints on the quality features. The\nunified distribution can significantly reduce the domain gap between different\nvideo samples, resulting in a more generalized quality feature representation.\nAlong the temporal dimension, inspired by the mechanism of visual perception,\nwe propose a pyramid temporal aggregation module by involving the short-term\nand long-term memory to aggregate the frame-level quality. Experiments show\nthat our method outperforms the state-of-the-art methods on cross-dataset\nsettings, and achieves comparable performance on intra-dataset configurations,\ndemonstrating the high-generalization capability of the proposed method.",
          "link": "http://arxiv.org/abs/2012.13936",
          "publishedOn": "2021-06-24T01:51:43.422Z",
          "wordCount": 609,
          "title": "Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment. (arXiv:2012.13936v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1\">Justin Dulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1\">Gregory Rolwes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1\">Duke Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1\">Nadia Shakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.",
          "link": "http://arxiv.org/abs/2106.05748",
          "publishedOn": "2021-06-24T01:51:43.406Z",
          "wordCount": 630,
          "title": "Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.03734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grieggs_S/0/1/0/all/0/1\">Samuel Grieggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bingyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauch_G/0/1/0/all/0/1\">Greta Rauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>",
          "description": "The subtleties of human perception, as measured by vision scientists through\nthe use of psychophysics, are important clues to the internal workings of\nvisual recognition. For instance, measured reaction time can indicate whether a\nvisual stimulus is easy for a subject to recognize, or whether it is hard. In\nthis paper, we consider how to incorporate psychophysical measurements of\nvisual perception into the loss function of a deep neural network being trained\nfor a recognition task, under the assumption that such information can enforce\nconsistency with human behavior. As a case study to assess the viability of\nthis approach, we look at the problem of handwritten document transcription.\nWhile good progress has been made towards automatically transcribing modern\nhandwriting, significant challenges remain in transcribing historical\ndocuments. Here we describe a general enhancement strategy, underpinned by the\nnew loss formulation, which can be applied to the training regime of any deep\nlearning-based document transcription system. Through experimentation, reliable\nperformance improvement is demonstrated for the standard IAM and RIMES datasets\nfor three different network architectures. Further, we go on to show\nfeasibility for our approach on a new dataset of digitized Latin manuscripts,\noriginally produced by scribes in the Cloister of St. Gall in the the 9th\ncentury.",
          "link": "http://arxiv.org/abs/1904.03734",
          "publishedOn": "2021-06-24T01:51:43.401Z",
          "wordCount": 709,
          "title": "Measuring Human Perception to Improve Handwritten Document Transcription. (arXiv:1904.03734v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_D/0/1/0/all/0/1\">Denny Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martyniuk_D/0/1/0/all/0/1\">Darya Martyniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willems_H/0/1/0/all/0/1\">Henri Willems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmann_F/0/1/0/all/0/1\">Fabian Bergmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1\">Adrian Paschke</a>",
          "description": "Image classification is an important task in various machine learning\napplications. In recent years, a number of classification methods based on\nquantum machine learning and different quantum image encoding techniques have\nbeen proposed. In this paper, we study the effect of three different quantum\nimage encoding approaches on the performance of a convolution-inspired hybrid\nquantum-classical image classification algorithm called quanvolutional neural\nnetwork (QNN). We furthermore examine the effect of variational - i.e.\ntrainable - quantum circuits on the classification results. Our experiments\nindicate that some image encodings are better suited for variational circuits.\nHowever, our experiments show as well that there is not one best image\nencoding, but that the choice of the encoding depends on the specific\nconstraints of the application.",
          "link": "http://arxiv.org/abs/2106.07327",
          "publishedOn": "2021-06-24T01:51:43.394Z",
          "wordCount": 576,
          "title": "Variational Quanvolutional Neural Networks with enhanced image encoding. (arXiv:2106.07327v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1\">Rhea Sanjay Sukthanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.",
          "link": "http://arxiv.org/abs/2106.03959",
          "publishedOn": "2021-06-24T01:51:43.388Z",
          "wordCount": 611,
          "title": "Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Designed to learn long-range interactions on sequential data, transformers\ncontinue to show state-of-the-art results on a wide variety of tasks. In\ncontrast to CNNs, they contain no inductive bias that prioritizes local\ninteractions. This makes them expressive, but also computationally infeasible\nfor long sequences, such as high-resolution images. We demonstrate how\ncombining the effectiveness of the inductive bias of CNNs with the expressivity\nof transformers enables them to model and thereby synthesize high-resolution\nimages. We show how to (i) use CNNs to learn a context-rich vocabulary of image\nconstituents, and in turn (ii) utilize transformers to efficiently model their\ncomposition within high-resolution images. Our approach is readily applied to\nconditional synthesis tasks, where both non-spatial information, such as object\nclasses, and spatial information, such as segmentations, can control the\ngenerated image. In particular, we present the first results on\nsemantically-guided synthesis of megapixel images with transformers and obtain\nthe state of the art among autoregressive models on class-conditional ImageNet.\nCode and pretrained models can be found at\nhttps://github.com/CompVis/taming-transformers .",
          "link": "http://arxiv.org/abs/2012.09841",
          "publishedOn": "2021-06-24T01:51:43.383Z",
          "wordCount": 645,
          "title": "Taming Transformers for High-Resolution Image Synthesis. (arXiv:2012.09841v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengfei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruoxi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "Deep neural networks(DNNs) is vulnerable to be attacked by adversarial\nexamples. Black-box attack is the most threatening attack. At present,\nblack-box attack methods mainly adopt gradient-based iterative attack methods,\nwhich usually limit the relationship between the iteration step size, the\nnumber of iterations, and the maximum perturbation. In this paper, we propose a\nnew gradient iteration framework, which redefines the relationship between the\nabove three. Under this framework, we easily improve the attack success rate of\nDI-TI-MIM. In addition, we propose a gradient iterative attack method based on\ninput dropout, which can be well combined with our framework. We further\npropose a multi dropout rate version of this method. Experimental results show\nthat our best method can achieve attack success rate of 96.2\\% for defense\nmodel on average, which is higher than the state-of-the-art gradient-based\nattacks.",
          "link": "http://arxiv.org/abs/2106.01617",
          "publishedOn": "2021-06-24T01:51:43.367Z",
          "wordCount": 606,
          "title": "Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>",
          "description": "Person re-identification (re-ID) in the scenario with large spatial and\ntemporal spans has not been fully explored. This is partially because that,\nexisting benchmark datasets were mainly collected with limited spatial and\ntemporal ranges, e.g., using videos recorded in a few days by cameras in a\nspecific region of the campus. Such limited spatial and temporal ranges make it\nhard to simulate the difficulties of person re-ID in real scenarios. In this\nwork, we contribute a novel Large-scale Spatio-Temporal LaST person re-ID\ndataset, including 10,862 identities with more than 228k images. Compared with\nexisting datasets, LaST presents more challenging and high-diversity re-ID\nsettings, and significantly larger spatial and temporal ranges. For instance,\neach person can appear in different cities or countries, and in various time\nslots from daytime to night, and in different seasons from spring to winter. To\nour best knowledge, LaST is a novel person re-ID dataset with the largest\nspatio-temporal ranges. Based on LaST, we verified its challenge by conducting\na comprehensive performance evaluation of 14 re-ID algorithms. We further\npropose an easy-to-implement baseline that works well on such challenging re-ID\nsetting. We also verified that models pre-trained on LaST can generalize well\non existing datasets with short-term and cloth-changing scenarios. We expect\nLaST to inspire future works toward more realistic and challenging re-ID tasks.\nMore information about the dataset is available at\nhttps://github.com/shuxjweb/last.git.",
          "link": "http://arxiv.org/abs/2105.15076",
          "publishedOn": "2021-06-24T01:51:43.361Z",
          "wordCount": 704,
          "title": "Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chajin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeoh Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Deep learning-based image inpainting algorithms have shown great performance\nvia powerful learned prior from the numerous external natural images. However,\nthey show unpleasant results on the test image whose distribution is far from\nthe that of training images because their models are biased toward the training\nimages. In this paper, we propose a simple image inpainting algorithm with\ntest-time adaptation named AdaFill. Given a single out-of-distributed test\nimage, our goal is to complete hole region more naturally than the pre-trained\ninpainting models. To achieve this goal, we treat remained valid regions of the\ntest image as another training cues because natural images have strong internal\nsimilarities. From this test-time adaptation, our network can exploit\nexternally learned image priors from the pre-trained features as well as the\ninternal prior of the test image explicitly. Experimental results show that\nAdaFill outperforms other models on the various out-of-distribution test\nimages. Furthermore, the model named ZeroFill, that are not pre-trained also\nsometimes outperforms the pre-trained models.",
          "link": "http://arxiv.org/abs/2102.01360",
          "publishedOn": "2021-06-24T01:51:43.356Z",
          "wordCount": 626,
          "title": "Test-Time Adaptation for Out-of-distributed Image Inpainting. (arXiv:2102.01360v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07280",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>",
          "description": "Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has\na high miss rate due to a number of factors, including the geometry of the\ncolon (haustral fold and sharp bends occlusions), endoscopist inexperience or\nfatigue, endoscope field of view, etc. We present a framework to visualize the\nmissed regions per-frame during the colonoscopy, and provides a workable\nclinical solution. Specifically, we make use of 3D reconstructed virtual\ncolonoscopy (VC) data and the insight that VC and OC share the same underlying\ngeometry but differ in color, texture and specular reflections, embedded in the\nOC domain. A lossy unpaired image-to-image translation model is introduced with\nenforced shared latent space for OC and VC. This shared latent space captures\nthe geometric information while deferring the color, texture, and specular\ninformation creation to additional Gaussian noise input. This additional noise\ninput can be utilized to generate one-to-many mappings from VC to OC and OC to\nOC. The code, data and trained models will be released via our Computational\nEndoscopy Platform at https://github.com/nadeemlab/CEP.",
          "link": "http://arxiv.org/abs/2101.07280",
          "publishedOn": "2021-06-24T01:51:43.349Z",
          "wordCount": 657,
          "title": "Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent Space Representations. (arXiv:2101.07280v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>",
          "description": "Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to address such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the proposed weak predictors, their probabilities of sampling\nbetter architectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework produces\ncoarse-to-fine iteration to refine the ranking of sampling space gradually.\nExtensive experiments demonstrate that our method costs fewer samples to find\ntop-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as\nachieves the state-of-the-art ImageNet performance on the NASNet search space.\nIn particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,\nWeakNAS outperforms all of them with notable margins, e.g., requiring at least\n7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also\nabsorb them for further performance boost. We further strike the new SOTA\nresult of 81.3% in the ImageNet MobileNet Search Space. The code is available\nat https://github.com/VITA-Group/WeakNAS.",
          "link": "http://arxiv.org/abs/2102.10490",
          "publishedOn": "2021-06-24T01:51:43.337Z",
          "wordCount": 746,
          "title": "Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-06-24T01:51:43.321Z",
          "wordCount": 665,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaolong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>",
          "description": "Alongside the prevalence of mobile videos, the general public leans towards\nconsuming vertical videos on hand-held devices. To revitalize the exposure of\nhorizontal contents, we hereby set forth the exploration of automated\nhorizontal-to-vertical (abbreviated as H2V) video conversion with our proposed\nH2V framework, accompanied by an accurately annotated H2V-142K dataset.\nConcretely, H2V framework integrates video shot boundary detection, subject\nselection and multi-object tracking to facilitate the subject-preserving\nconversion, wherein the key is subject selection. To achieve so, we propose a\nRank-SS module that detects human objects, then selects the subject-to-preserve\nvia exploiting location, appearance, and salient cues. Afterward, the framework\nautomatically crops the video around the subject to produce vertical contents\nfrom horizontal sources. To build and evaluate our H2V framework, H2V-142K\ndataset is densely annotated with subject bounding boxes for 125 videos with\n132K frames and 9,500 video covers, upon which we demonstrate superior subject\nselection performance comparing to traditional salient approaches, and exhibit\npromising horizontal-to-vertical conversion performance overall. By publicizing\nthis dataset as well as our approach, we wish to pave the way for more valuable\nendeavors on the horizontal-to-vertical video conversion task.",
          "link": "http://arxiv.org/abs/2101.04051",
          "publishedOn": "2021-06-24T01:51:43.316Z",
          "wordCount": 653,
          "title": "Horizontal-to-Vertical Video Conversion. (arXiv:2101.04051v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1\">Alvaro Gomariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1\">Tiziano Portenier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1\">Patrick M. Helbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1\">Stephan Isringhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1\">Ute Suessbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1\">C&#xe9;sar Nombela-Arrieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>",
          "description": "Fluorescence microscopy allows for a detailed inspection of cells, cellular\nnetworks, and anatomical landmarks by staining with a variety of\ncarefully-selected markers visualized as color channels. Quantitative\ncharacterization of structures in acquired images often relies on automatic\nimage analysis methods. Despite the success of deep learning methods in other\nvision applications, their potential for fluorescence image analysis remains\nunderexploited. One reason lies in the considerable workload required to train\naccurate models, which are normally specific for a given combination of\nmarkers, and therefore applicable to a very restricted number of experimental\nsettings. We herein propose Marker Sampling and Excite, a neural network\napproach with a modality sampling strategy and a novel attention module that\ntogether enable (i) flexible training with heterogeneous datasets with\ncombinations of markers and (ii) successful utility of learned models on\narbitrary subsets of markers prospectively. We show that our single neural\nnetwork solution performs comparably to an upper bound scenario where an\nensemble of many networks is na\\\"ively trained for each possible marker\ncombination separately. In addition, we demonstrate the feasibility of this\nframework in high-throughput biological analysis by revising a recent\nquantitative characterization of bone marrow vasculature in 3D confocal\nmicroscopy datasets and further confirm the validity of our approach on an\nadditional, significantly different dataset of microvessels in fetal liver\ntissues. Not only can our work substantially ameliorate the use of deep\nlearning in fluorescence microscopy analysis, but it can also be utilized in\nother fields with incomplete data acquisitions and missing modalities.",
          "link": "http://arxiv.org/abs/2008.12380",
          "publishedOn": "2021-06-24T01:51:43.310Z",
          "wordCount": 758,
          "title": "Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1\">Tiago de Souza Farias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1\">Jonas Maziero</a>",
          "description": "We introduce feature alignment, a technique for obtaining approximate\nreversibility in artificial neural networks. By means of feature extraction, we\ncan train a neural network to learn an estimated map for its reverse process\nfrom outputs to inputs. Combined with variational autoencoders, we can generate\nnew samples from the same statistics as the training data. Improvements of the\nresults are obtained by using concepts from generative adversarial networks.\nFinally, we show that the technique can be modified for training neural\nnetworks locally, saving computational memory resources. Applying these\ntechniques, we report results for three vision generative tasks: MNIST,\nCIFAR-10, and celebA.",
          "link": "http://arxiv.org/abs/2106.12562",
          "publishedOn": "2021-06-24T01:51:43.304Z",
          "wordCount": 540,
          "title": "Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1710.00189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">S. Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">H. Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">I. Hipiny</a>",
          "description": "Classification of rocks is one of the fundamental tasks in a geological\nstudy. The process requires a human expert to examine sampled thin section\nimages under a microscope. In this study, we propose a method that uses\nmicroscope automation, digital image acquisition, edge detection and colour\nanalysis (histogram). We collected 60 digital images from 20 standard thin\nsections using a digital camera mounted on a conventional microscope. Each\nimage is partitioned into a finite number of cells that form a grid structure.\nEdge and colour profile of pixels inside each cell determine its\nclassification. The individual cells then determine the thin section image\nclassification via a majority voting scheme. Our method yielded successful\nresults as high as 90% to 100% precision.",
          "link": "http://arxiv.org/abs/1710.00189",
          "publishedOn": "2021-06-24T01:51:43.297Z",
          "wordCount": 612,
          "title": "Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis. (arXiv:1710.00189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lixin Duan</a>",
          "description": "Cross-domain object detection is challenging, because object detection model\nis often vulnerable to data variance, especially to the considerable domain\nshift between two distinctive domains. In this paper, we propose a new Unbiased\nMean Teacher (UMT) model for cross-domain object detection. We reveal that\nthere often exists a considerable model bias for the simple mean teacher (MT)\nmodel in cross-domain scenarios, and eliminate the model bias with several\nsimple yet highly effective strategies. In particular, for the teacher model,\nwe propose a cross-domain distillation method for MT to maximally exploit the\nexpertise of the teacher model. Moreover, for the student model, we alleviate\nits bias by augmenting training samples with pixel-level adaptation. Finally,\nfor the teaching process, we employ an out-of-distribution estimation strategy\nto select samples that most fit the current model to further enhance the\ncross-domain distillation process. By tackling the model bias issue with these\nstrategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on\nbenchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,\nrespectively, which outperforms the existing state-of-the-art results in\nnotable margins. Our implementation is available at\nhttps://github.com/kinredon/umt.",
          "link": "http://arxiv.org/abs/2003.00707",
          "publishedOn": "2021-06-24T01:51:43.262Z",
          "wordCount": 652,
          "title": "Unbiased Mean Teacher for Cross-domain Object Detection. (arXiv:2003.00707v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10823",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Temniranrat_P/0/1/0/all/0/1\">Pitchayagan Temniranrat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiratiratanapruk_K/0/1/0/all/0/1\">Kantip Kiratiratanapruk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitvimonrat_A/0/1/0/all/0/1\">Apichon Kitvimonrat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinthupinyo_W/0/1/0/all/0/1\">Wasin Sinthupinyo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patarapuwadol_S/0/1/0/all/0/1\">Sujin Patarapuwadol</a>",
          "description": "A LINE Bot System to diagnose rice diseases from actual paddy field images\nwas developed and presented in this paper. It was easy-to-use and automatic\nsystem designed to help rice farmers improve the rice yield and quality. The\ntargeted images were taken from the actual paddy environment without special\nsample preparation. We used a deep learning neural networks technique to detect\nrice diseases from the images. We developed an object detection model training\nand refinement process to improve the performance of our previous research on\nrice leave diseases detection. The process was based on analyzing the model's\npredictive results and could be repeatedly used to improve the quality of the\ndatabase in the next training of the model. The deployment model for our LINE\nBot system was created from the selected best performance technique in our\nprevious paper, YOLOv3, trained by refined training data set. The performance\nof the deployment model was measured on 5 target classes and found that the\nAverage True Positive Point improved from 91.1% in the previous paper to 95.6%\nin this study. Therefore, we used this deployment model for Rice Disease LINE\nBot system. Our system worked automatically real-time to suggest primary\ndiagnosis results to the users in the LINE group, which included rice farmers\nand rice disease specialists. They could communicate freely via chat. In the\nreal LINE Bot deployment, the model's performance was measured by our own\ndefined measurement Average True Positive Point and was found to be an average\nof 78.86%. The system was fast and took only 2-3 s for detection process in our\nsystem server.",
          "link": "http://arxiv.org/abs/2011.10823",
          "publishedOn": "2021-06-24T01:51:43.256Z",
          "wordCount": 761,
          "title": "A System for Automatic Rice Disease Detection from Rice Paddy Images Serviced via a Chatbot. (arXiv:2011.10823v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Generative adversarial networks (GANs) have gained considerable attention\nowing to their ability to reproduce images. However, they can recreate training\nimages faithfully despite image degradation in the form of blur, noise, and\ncompression, generating similarly degraded images. To solve this problem, the\nrecently proposed noise robust GAN (NR-GAN) provides a partial solution by\ndemonstrating the ability to learn a clean image generator directly from noisy\nimages using a two-generator model comprising image and noise generators.\nHowever, its application is limited to noise, which is relatively easy to\ndecompose owing to its additive and reversible characteristics, and its\napplication to irreversible image degradation, in the form of blur,\ncompression, and combination of all, remains a challenge. To address these\nproblems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that\ncan learn a clean image generator directly from degraded images without\nknowledge of degradation parameters (e.g., blur kernel types, noise amounts, or\nquality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator\nmodel composed of image, blur-kernel, noise, and quality-factor generators.\nHowever, in contrast to NR-GAN, to address irreversible characteristics, we\nintroduce masking architectures adjusting degradation strength values in a\ndata-driven manner using bypasses before and after degradation. Furthermore, to\nsuppress uncertainty caused by the combination of blur, noise, and compression,\nwe introduce adaptive consistency losses imposing consistency between\nirreversible degradation processes according to the degradation strengths. We\ndemonstrate the effectiveness of BNCR-GAN through large-scale comparative\nstudies on CIFAR-10 and a generality analysis on FFHQ. In addition, we\ndemonstrate the applicability of BNCR-GAN in image restoration.",
          "link": "http://arxiv.org/abs/2003.07849",
          "publishedOn": "2021-06-24T01:51:43.250Z",
          "wordCount": 740,
          "title": "Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krylov_I/0/1/0/all/0/1\">Ilya Krylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosov_S/0/1/0/all/0/1\">Sergei Nosov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1\">Vladislav Sovrasov</a>",
          "description": "A large scale human-labeled dataset plays an important role in creating high\nquality deep learning models. In this paper we present text annotation for Open\nImages V5 dataset. To our knowledge it is the largest among publicly available\nmanually created text annotations. Having this annotation we trained a simple\nMask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS),\nwhich achieves competitive performance or even outperforms current\nstate-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and\nTotal-Text datasets. Code for text spotting model available online at:\nhttps://github.com/openvinotoolkit/training_extensions. The model can be\nexported to OpenVINO-format and run on Intel CPUs.",
          "link": "http://arxiv.org/abs/2106.12326",
          "publishedOn": "2021-06-24T01:51:43.244Z",
          "wordCount": 541,
          "title": "Open Images V5 Text Annotation and Yet Another Mask Text Spotter. (arXiv:2106.12326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Apratim Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Accurate prediction of pedestrian and bicyclist paths is integral to the\ndevelopment of reliable autonomous vehicles in dense urban environments. The\ninteractions between vehicle and pedestrian or bicyclist have a significant\nimpact on the trajectories of traffic participants e.g. stopping or turning to\navoid collisions. Although recent datasets and trajectory prediction approaches\nhave fostered the development of autonomous vehicles yet the amount of\nvehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,\nwe propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In\nparticular, our dataset caters more diverse and complex interactions in dense\nurban scenarios compared to the existing datasets. To address the challenges in\npredicting future trajectories with dense interactions, we develop a joint\ninference model that learns an expressive multi-modal shared latent space\nacross agents in the urban scene. This enables our Joint-$\\beta$-cVAE approach\nto better model the distribution of future trajectories. We achieve state of\nthe art results on the nuScenes and Euro-PVI datasets demonstrating the\nimportance of capturing interactions between ego-vehicle and pedestrians\n(bicyclists) for accurate predictions.",
          "link": "http://arxiv.org/abs/2106.12442",
          "publishedOn": "2021-06-24T01:51:43.229Z",
          "wordCount": 622,
          "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12522",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>",
          "description": "Haustral folds are colon wall protrusions implicated for high polyp miss rate\nduring optical colonoscopy procedures. If segmented accurately, haustral folds\ncan allow for better estimation of missed surface and can also serve as\nvaluable landmarks for registering pre-treatment virtual (CT) and optical\ncolonoscopies, to guide navigation towards the anomalies found in pre-treatment\nscans. We present a novel generative adversarial network, FoldIt, for\nfeature-consistent image translation of optical colonoscopy videos to virtual\ncolonoscopy renderings with haustral fold overlays. A new transitive loss is\nintroduced in order to leverage ground truth information between haustral fold\nannotations and virtual colonoscopy renderings. We demonstrate the\neffectiveness of our model on real challenging optical colonoscopy videos as\nwell as on textured virtual colonoscopy videos with clinician-verified haustral\nfold annotations. All code and scripts to reproduce the experiments of this\npaper will be made available via our Computational Endoscopy Platform at\nhttps://github.com/nadeemlab/CEP.",
          "link": "http://arxiv.org/abs/2106.12522",
          "publishedOn": "2021-06-24T01:51:43.220Z",
          "wordCount": 606,
          "title": "FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos. (arXiv:2106.12522v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12313",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhongliang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihao Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>",
          "description": "The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world\nsince its first report in December 2019 and thoracic computed tomography (CT)\nhas become one of the main tools for its diagnosis. In recent years, deep\nlearning-based approaches have shown impressive performance in myriad image\nrecognition tasks. However, they usually require a large number of annotated\ndata for training. Inspired by Ground Glass Opacity (GGO), a common finding in\nCOIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised\npretraining method based on pseudo lesions generation and restoration for\nCOVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical\nmodel, to generate lesion-like patterns, which were then randomly pasted to the\nlung regions of normal CT images to generate pseudo COVID-19 images. The pairs\nof normal and pseudo COVID-19 images were then used to train an encoder-decoder\narchitecture based U-Net for image restoration, which does not require any\nlabelled data. The pretrained encoder was then fine-tuned using labelled data\nfor COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of\nCT images were employed for evaluation. Comprehensive experimental results\ndemonstrated that the proposed self-supervised learning approach could extract\nbetter feature representation for COVID-19 diagnosis and the accuracy of the\nproposed method outperformed the supervised model pretrained on large scale\nimages by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset,\nrespectively.",
          "link": "http://arxiv.org/abs/2106.12313",
          "publishedOn": "2021-06-24T01:51:43.214Z",
          "wordCount": 721,
          "title": "Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19 Diagnosis. (arXiv:2106.12313v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1\">Talia Konkle</a>",
          "description": "The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties",
          "link": "http://arxiv.org/abs/2006.07991",
          "publishedOn": "2021-06-24T01:51:43.207Z",
          "wordCount": 752,
          "title": "Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1\">Israa Odeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1\">Mouhammd Alkasassbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1\">Mohammad Alauthman</a>",
          "description": "Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in\ndiabetic patients. DR is a microvascular disease that affects the eye retina,\nwhich causes vessel blockage and therefore cuts the main source of nutrition\nfor the retina tissues. Treatment for this visual disorder is most effective\nwhen it is detected in its earliest stages, as severe DR can result in\nirreversible blindness. Nonetheless, DR identification requires the expertise\nof Ophthalmologists which is often expensive and time-consuming. Therefore,\nautomatic detection systems were introduced aiming to facilitate the\nidentification process, making it available globally in a time and\ncost-efficient manner. However, due to the limited reliable datasets and\nmedical records for this particular eye disease, the obtained predictions\naccuracies were relatively unsatisfying for eye specialists to rely on them as\ndiagnostic systems. Thus, we explored an ensemble-based learning strategy,\nmerging a substantial selection of well-known classification algorithms in one\nsophisticated diagnostic model. The proposed framework achieved the highest\naccuracy rates among all other common classification algorithms in the area. 4\nsubdatasets were generated to contain the top 5 and top 10 features of the\nMessidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies\nof 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original\ndataset respectively. The results imply the impressive performance of the\nsubdataset, which significantly conduces to a less complex classification\nprocess",
          "link": "http://arxiv.org/abs/2106.12545",
          "publishedOn": "2021-06-24T01:51:43.201Z",
          "wordCount": 669,
          "title": "Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.",
          "link": "http://arxiv.org/abs/2106.12378",
          "publishedOn": "2021-06-24T01:51:43.196Z",
          "wordCount": 577,
          "title": "Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1\">Zhou Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>",
          "description": "Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.",
          "link": "http://arxiv.org/abs/2106.12449",
          "publishedOn": "2021-06-24T01:51:43.177Z",
          "wordCount": 650,
          "title": "FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-06-24T01:51:43.171Z",
          "wordCount": 583,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1\">Meysam Madadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1\">Hugo Bertiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>",
          "description": "In this paper we propose the first deep unsupervised approach in human body\nreconstruction to estimate body surface from a sparse set of landmarks, so\ncalled DeepMurf. We apply a denoising autoencoder to estimate missing\nlandmarks. Then we apply an attention model to estimate body joints from\nlandmarks. Finally, a cascading network is applied to regress parameters of a\nstatistical generative model that reconstructs body. Our set of proposed loss\nfunctions allows us to train the network in an unsupervised way. Results on\nfour public datasets show that our approach accurately reconstructs the human\nbody from real world mocap data.",
          "link": "http://arxiv.org/abs/2106.12282",
          "publishedOn": "2021-06-24T01:51:43.163Z",
          "wordCount": 548,
          "title": "Deep unsupervised 3D human body reconstruction from a sparse set of landmarks. (arXiv:2106.12282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D. Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>",
          "description": "Building instance segmentation models that are data-efficient and can handle\nrare object categories is an important challenge in computer vision. Leveraging\ndata augmentations is a promising direction towards addressing this challenge.\nHere, we perform a systematic study of the Copy-Paste augmentation ([13, 12])\nfor instance segmentation where we randomly paste objects onto an image. Prior\nstudies on Copy-Paste relied on modeling the surrounding visual context for\npasting the objects. However, we find that the simple mechanism of pasting\nobjects randomly is good enough and can provide solid gains on top of strong\nbaselines. Furthermore, we show Copy-Paste is additive with semi-supervised\nmethods that leverage extra data through pseudo labeling (e.g. self-training).\nOn COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an\nimprovement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.\nWe further demonstrate that Copy-Paste can lead to significant improvements on\nthe LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge\nwinning entry by +3.6 mask AP on rare categories.",
          "link": "http://arxiv.org/abs/2012.07177",
          "publishedOn": "2021-06-24T01:51:43.157Z",
          "wordCount": 657,
          "title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. (arXiv:2012.07177v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1710.00187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1\">I. Hipiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1\">H. Ujir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minoi_J/0/1/0/all/0/1\">J.L. Minoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_S/0/1/0/all/0/1\">S.F. Samson Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairuddin_M/0/1/0/all/0/1\">M.A. Khairuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunar_M/0/1/0/all/0/1\">M.S. Sunar</a>",
          "description": "Unsupervised segmentation of action segments in egocentric videos is a\ndesirable feature in tasks such as activity recognition and content-based video\nretrieval. Reducing the search space into a finite set of action segments\nfacilitates a faster and less noisy matching. However, there exist a\nsubstantial gap in machine understanding of natural temporal cuts during a\ncontinuous human activity. This work reports on a novel gaze-based approach for\nsegmenting action segments in videos captured using an egocentric camera. Gaze\nis used to locate the region-of-interest inside a frame. By tracking two simple\nmotion-based parameters inside successive regions-of-interest, we discover a\nfinite set of temporal cuts. We present several results using combinations (of\nthe two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains\negocentric videos depicting several daily-living activities. The quality of the\ntemporal cuts is further improved by implementing two entropy measures.",
          "link": "http://arxiv.org/abs/1710.00187",
          "publishedOn": "2021-06-24T01:51:43.141Z",
          "wordCount": 636,
          "title": "Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze. (arXiv:1710.00187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1\">Amy Widdicombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1\">Simon J. Julier</a>",
          "description": "Binarized Neural Networks (BNNs) have the potential to revolutionize the way\nthat deep learning is carried out in edge computing platforms. However, the\neffectiveness of interpretability methods on these networks has not been\nassessed.\n\nIn this paper, we compare the performance of several widely used saliency\nmap-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when\napplied to Binarized or Full Precision Neural Networks (FPNNs). We found that\nthe basic Gradient method produces very similar-looking maps for both types of\nnetwork. However, SmoothGrad produces significantly noisier maps for BNNs.\nGradCAM also produces saliency maps which differ between network types, with\nsome of the BNNs having seemingly nonsensical explanations. We comment on\npossible reasons for these differences in explanations and present it as an\nexample of why interpretability techniques should be tested on a wider range of\nnetwork types.",
          "link": "http://arxiv.org/abs/2106.12569",
          "publishedOn": "2021-06-24T01:51:43.126Z",
          "wordCount": 592,
          "title": "Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1\">Stefan K. Piechnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1\">Stefan Neubauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1\">Steffen E. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>",
          "description": "The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.",
          "link": "http://arxiv.org/abs/2106.12387",
          "publishedOn": "2021-06-24T01:51:43.120Z",
          "wordCount": 762,
          "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dilber_T/0/1/0/all/0/1\">Talha Dilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Serdar Guzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostanci_E/0/1/0/all/0/1\">Erkan Bostanci</a>",
          "description": "In today's world, the amount of data produced in every field has increased at\nan unexpected level. In the face of increasing data, the importance of data\nprocessing has increased remarkably. Our resource topic is on the processing of\nvideo data, which has an important place in increasing data, and the production\nof summary videos. Within the scope of this resource, a new method for anomaly\ndetection with object-based unsupervised learning has been developed while\ncreating a video summary. By using this method, the video data is processed as\npixels and the result is produced as a video segment. The process flow can be\nbriefly summarized as follows. Objects on the video are detected according to\ntheir type, and then they are tracked. Then, the tracking history data of the\nobjects are processed, and the classifier is trained with the object type.\nThanks to this classifier, anomaly behavior of objects is detected. Video\nsegments are determined by processing video moments containing anomaly\nbehaviors. The video summary is created by extracting the detected video\nsegments from the original video and combining them. The model we developed has\nbeen tested and verified separately for single camera and dual camera systems.",
          "link": "http://arxiv.org/abs/2106.12362",
          "publishedOn": "2021-06-24T01:51:43.114Z",
          "wordCount": 641,
          "title": "A new Video Synopsis Based Approach Using Stereo Camera. (arXiv:2106.12362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Kalun Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1\">Franz-Josef Pfreundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>",
          "description": "Over the last decade, the development of deep image classification networks\nhas mostly been driven by the search for the best performance in terms of\nclassification accuracy on standardized benchmarks like ImageNet. More\nrecently, this focus has been expanded by the notion of model robustness, i.e.\nthe generalization abilities of models towards previously unseen changes in the\ndata distribution. While new benchmarks, like ImageNet-C, have been introduced\nto measure robustness properties, we argue that fixed testsets are only able to\ncapture a small portion of possible data variations and are thus limited and\nprone to generate new overfitted solutions. To overcome these drawbacks, we\nsuggest to estimate the robustness of a model directly from the structure of\nits learned feature-space. We introduce robustness indicators which are\nobtained via unsupervised clustering of latent representations inside a trained\nclassifier and show very high correlations to the model performance on\ncorrupted test data.",
          "link": "http://arxiv.org/abs/2106.12303",
          "publishedOn": "2021-06-24T01:51:43.109Z",
          "wordCount": 595,
          "title": "Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengdi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Ximeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1\">Mufeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangxi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuanqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qiushi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>",
          "description": "Diabetic retinopathy (DR) remains the most prevalent cause of vision\nimpairment and irreversible blindness in the working-age adults. Due to the\nrenaissance of deep learning (DL), DL-based DR diagnosis has become a promising\ntool for the early screening and severity grading of DR. However, training deep\nneural networks (DNNs) requires an enormous amount of carefully labeled data.\nNoisy label data may be introduced when labeling plenty of data, degrading the\nperformance of models. In this work, we propose a novel label management\nmechanism (LMM) for the DNN to overcome overfitting on the noisy data. LMM\nutilizes maximum posteriori probability (MAP) in the Bayesian statistic and\ntime-weighted technique to selectively correct the labels of unclean data,\nwhich gradually purify the training data and improve classification\nperformance. Comprehensive experiments on both synthetic noise data (Messidor\n\\& our collected DR dataset) and real-world noise data (ANIMAL-10N)\ndemonstrated that LMM could boost performance of models and is superior to\nthree state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12284",
          "publishedOn": "2021-06-24T01:51:43.104Z",
          "wordCount": 621,
          "title": "A Label Management Mechanism for Retinal Fundus Image Classification of Diabetic Retinopathy. (arXiv:2106.12284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "In this paper, we present Vision Permutator, a conceptually simple and data\nefficient MLP-like architecture for visual recognition. By realizing the\nimportance of the positional information carried by 2D feature representations,\nunlike recent MLP-like models that encode the spatial information along the\nflattened spatial dimensions, Vision Permutator separately encodes the feature\nrepresentations along the height and width dimensions with linear projections.\nThis allows Vision Permutator to capture long-range dependencies along one\nspatial direction and meanwhile preserve precise positional information along\nthe other direction. The resulting position-sensitive outputs are then\naggregated in a mutually complementing manner to form expressive\nrepresentations of the objects of interest. We show that our Vision Permutators\nare formidable competitors to convolutional neural networks (CNNs) and vision\ntransformers. Without the dependence on spatial convolutions or attention\nmechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without\nextra large-scale training data (e.g., ImageNet-22k) using only 25M learnable\nparameters, which is much better than most CNNs and vision transformers under\nthe same model size constraint. When scaling up to 88M, it attains 83.2% top-1\naccuracy. We hope this work could encourage research on rethinking the way of\nencoding spatial information and facilitate the development of MLP-like models.\nCode is available at https://github.com/Andrew-Qibin/VisionPermutator.",
          "link": "http://arxiv.org/abs/2106.12368",
          "publishedOn": "2021-06-24T01:51:43.087Z",
          "wordCount": 649,
          "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition. (arXiv:2106.12368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1\">Erika Puglisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1\">Jamila Mifdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1\">Fiora Pirri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "The abundance of clouds, located both spatially and temporally, often makes\nremote sensing applications with optical images difficult or even impossible.\nIn this manuscript, a novel method for clouds-corrupted optical image\nrestoration has been presented and developed, based on a joint data fusion\nparadigm, where three deep neural networks have been combined in order to fuse\nspatio-temporal features extracted from Sentinel-1 and Sentinel-2 time-series\nof data. It is worth highlighting that both the code and the dataset have been\nimplemented from scratch and made available to interested research for further\nanalysis and investigation.",
          "link": "http://arxiv.org/abs/2106.12226",
          "publishedOn": "2021-06-24T01:51:43.082Z",
          "wordCount": 554,
          "title": "Sentinel-1 and Sentinel-2 Spatio-Temporal Data Fusion for Clouds Removal. (arXiv:2106.12226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "This report describes the technical details of our submission to the\nEPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action\nRecognition. The EPIC-Kitchens dataset is more difficult than other video\ndomain adaptation datasets due to multi-tasks with more modalities. Firstly, to\nparticipate in the challenge, we employ a transformer to capture the spatial\ninformation from each modality. Secondly, we employ a temporal attention module\nto model temporal-wise inter-dependency. Thirdly, we employ the adversarial\ndomain adaptation network to learn the general features between labeled source\nand unlabeled target domain. Finally, we incorporate multiple modalities to\nimprove the performance by a three-stream network with late fusion. Our network\nachieves the comparable performance with the state-of-the-art baseline T$A^3$N\nand outperforms the baseline on top-1 accuracy for verb class and top-5\naccuracies for all three tasks which are verb, noun and action. Under the team\nname xy9, our submission achieved 5th place in terms of top-1 accuracy for verb\nclass and all top-5 accuracies.",
          "link": "http://arxiv.org/abs/2106.12023",
          "publishedOn": "2021-06-24T01:51:43.076Z",
          "wordCount": 611,
          "title": "Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-06-24T01:51:43.062Z",
          "wordCount": 688,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1\">Reese Timothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Self-training based unsupervised domain adaptation (UDA) has shown great\npotential to address the problem of domain shift, when applying a trained deep\nlearning model in a source domain to unlabeled target domains. However, while\nthe self-training UDA has demonstrated its effectiveness on discriminative\ntasks, such as classification and segmentation, via the reliable pseudo-label\nselection based on the softmax discrete histogram, the self-training UDA for\ngenerative tasks, such as image synthesis, is not fully investigated. In this\nwork, we propose a novel generative self-training (GST) UDA framework with\ncontinuous value prediction and regression objective for cross-domain image\nsynthesis. Specifically, we propose to filter the pseudo-label with an\nuncertainty mask, and quantify the predictive confidence of generated images\nwith practical variational Bayes learning. The fast test-time adaptation is\nachieved by a round-based alternative optimization scheme. We validated our\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\nproblem, where datasets in the source and target domains were acquired from\ndifferent scanners or centers. Extensive validations were carried out to verify\nour framework against popular adversarial training UDA methods. Results show\nthat our GST, with tagged MRI of test subjects in new target domains, improved\nthe synthesis quality by a large margin, compared with the adversarial training\nUDA methods.",
          "link": "http://arxiv.org/abs/2106.12499",
          "publishedOn": "2021-06-24T01:51:43.057Z",
          "wordCount": 671,
          "title": "Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Back_J/0/1/0/all/0/1\">Jihye Back</a>",
          "description": "Recent studies have shown remarkable success in the unsupervised image to\nimage (I2I) translation. However, due to the imbalance in the data, learning\njoint distribution for various domains is still very challenging. Although\nexisting models can generate realistic target images, it's difficult to\nmaintain the structure of the source image. In addition, training a generative\nmodel on large data in multiple domains requires a lot of time and computer\nresources. To address these limitations, we propose a novel image-to-image\ntranslation method that generates images of the target domain by finetuning a\nstylegan2 pretrained model. The stylegan2 model is suitable for unsupervised\nI2I translation on unbalanced datasets; it is highly stable, produces realistic\nimages, and even learns properly from limited data when applied with simple\nfine-tuning techniques. Thus, in this paper, we propose new methods to preserve\nthe structure of the source images and generate realistic images in the target\ndomain. The code and results are available at\nhttps://github.com/happy-jihye/Cartoon-StyleGan2",
          "link": "http://arxiv.org/abs/2106.12445",
          "publishedOn": "2021-06-24T01:51:43.043Z",
          "wordCount": 598,
          "title": "Fine-Tuning StyleGAN2 For Cartoon Face Generation. (arXiv:2106.12445v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bezugam_S/0/1/0/all/0/1\">Sai Sukruth Bezugam</a>",
          "description": "The diagnosis of blood-based diseases often involves identifying and\ncharacterizing patient blood samples. Automated methods to detect and classify\nblood cell subtypes have important medical applications. Automated medical\nimage processing and analysis offers a powerful tool for medical diagnosis. In\nthis work we tackle the problem of white blood cell classification based on the\nmorphological characteristics of their outer contour, color. The work we would\nexplore a set of preprocessing and segmentation (Color-based segmentation,\nMorphological processing, contouring) algorithms along with a set of features\nextraction methods (Corner detection algorithms and Histogram of\nGradients(HOG)), dimensionality reduction algorithms (Principal Component\nAnalysis(PCA)) that are able to recognize and classify through various\nUnsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,\nDecision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,\nNaive Bayes) algorithms different categories of white blood cells to\nEosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards\nto explore various Deep Convolutional Neural network architecture (Sqeezent,\nMobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation\nand with preprocessing. We would like to explore many algorithms to identify\nthe robust algorithm with least time complexity and low resource requirement.\nThe outcome of this work can be a cue to selection of algorithms as per\nrequirement for automated blood cell classification.",
          "link": "http://arxiv.org/abs/2106.12548",
          "publishedOn": "2021-06-24T01:51:43.037Z",
          "wordCount": 667,
          "title": "Multi-Class Classification of Blood Cells - End to End Computer Vision based diagnosis case study. (arXiv:2106.12548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1\">Grant Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Paul P Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1\">Neal Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bryan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1\">Alan C. Kwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1\">Matthew J. Shun-Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1\">Kevin M. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1\">Joseph Ebinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1\">Florian Rader</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David H. Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1\">Ingela Schnittger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1\">Euan A. Ashley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">James Y. Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Jignesh Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1\">Ronald Witteles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1\">Susan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1\">David Ouyang</a>",
          "description": "Left ventricular hypertrophy (LVH) results from chronic remodeling caused by\na broad range of systemic and cardiovascular disease including hypertension,\naortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early\ndetection and characterization of LVH can significantly impact patient care but\nis limited by under-recognition of hypertrophy, measurement error and\nvariability, and difficulty differentiating etiologies of LVH. To overcome this\nchallenge, we present EchoNet-LVH - a deep learning workflow that automatically\nquantifies ventricular hypertrophy with precision equal to human experts and\npredicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model\naccurately measures intraventricular wall thickness (mean absolute error [MAE]\n1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI\n2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and\nclassifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic\ncardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets\nfrom independent domestic and international healthcare systems, EchoNet-LVH\naccurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)\nand detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy\n(AUC 0.89) on the domestic external validation site. Leveraging measurements\nacross multiple heart beats, our model can more accurately identify subtle\nchanges in LV geometry and its causal etiologies. Compared to human experts,\nEchoNet-LVH is fully automated, allowing for reproducible, precise\nmeasurements, and lays the foundation for precision diagnosis of cardiac\nhypertrophy. As a resource to promote further innovation, we also make publicly\navailable a large dataset of 23,212 annotated echocardiogram videos.",
          "link": "http://arxiv.org/abs/2106.12511",
          "publishedOn": "2021-06-24T01:51:43.031Z",
          "wordCount": 731,
          "title": "High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>",
          "description": "Semantic segmentation from very fine resolution (VFR) urban scene images\nplays a significant role in several application scenarios including autonomous\ndriving, land cover classification, and urban planning, etc. However, the\ntremendous details contained in the VFR image severely limit the potential of\nthe existing deep learning approaches. More seriously, the considerable\nvariations in scale and appearance of objects further deteriorate the\nrepresentational capacity of those se-mantic segmentation methods, leading to\nthe confusion of adjacent objects. Addressing such is-sues represents a\npromising research field in the remote sensing community, which paves the way\nfor scene-level landscape pattern analysis and decision making. In this\nmanuscript, we pro-pose a bilateral awareness network (BANet) which contains a\ndependency path and a texture path to fully capture the long-range\nrelationships and fine-grained details in VFR images. Specif-ically, the\ndependency path is conducted based on the ResT, a novel Transformer backbone\nwith memory-efficient multi-head self-attention, while the texture path is\nbuilt on the stacked convo-lution operation. Besides, using the linear\nattention mechanism, a feature aggregation module (FAM) is designed to\neffectively fuse the dependency features and texture features. Extensive\nexperiments conducted on the three large-scale urban scene image segmentation\ndatasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid\ndataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%\nmIoU is achieved on the UAVid dataset.",
          "link": "http://arxiv.org/abs/2106.12413",
          "publishedOn": "2021-06-24T01:51:43.026Z",
          "wordCount": 677,
          "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images. (arXiv:2106.12413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12489",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1\">Xiaozhen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1\">Wenfeng Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_J/0/1/0/all/0/1\">Jifeng Ning</a>",
          "description": "Low-rankness is important in the hyperspectral image (HSI) denoising tasks.\nThe tensor nuclear norm (TNN), defined based on the tensor singular value\ndecomposition, is a state-of-the-art method to describe the low-rankness of\nHSI. However, TNN ignores some of the physical meanings of HSI in tackling the\ndenoising tasks, leading to suboptimal denoising performance. In this paper, we\npropose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and\nthe non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the\nphysical meaning of frequency components and reconsider their weights to\nimprove the low-rank representation ability of TNN. Meanwhile, we also consider\nthe correlation among two spatial dimensions and the spectral dimension of HSI\nand combine the above improvements to TNN to propose MFWTNN. Secondly, we use\nnon-convex functions to approximate the rank function of the frequency tensor\nand propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively\nchoose bigger weights for slices mainly containing noise information and\nsmaller weights for slices containing profile information. Finally, we develop\nthe efficient alternating direction method of multiplier (ADMM) based algorithm\nto solve the proposed models, and the effectiveness of our models are\nsubstantiated in simulated and real HSI datasets.",
          "link": "http://arxiv.org/abs/2106.12489",
          "publishedOn": "2021-06-24T01:51:43.020Z",
          "wordCount": 644,
          "title": "Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dongyan Guo</a>",
          "description": "The efficiency and accuracy of mapping are crucial in a large scene and\nlong-term AR applications. Multi-agent cooperative SLAM is the precondition of\nmulti-user AR interaction. The cooperation of multiple smart phones has the\npotential to improve efficiency and robustness of task completion and can\ncomplete tasks that a single agent cannot do. However, it depends on robust\ncommunication, efficient location detection, robust mapping, and efficient\ninformation sharing among agents. We propose a multi-intelligence collaborative\nmonocular visual-inertial SLAM deployed on multiple ios mobile devices with a\ncentralized architecture. Each agent can independently explore the environment,\nrun a visual-inertial odometry module online, and then send all the measurement\ninformation to a central server with higher computing resources. The server\nmanages all the information received, detects overlapping areas, merges and\noptimizes the map, and shares information with the agents when needed. We have\nverified the performance of the system in public datasets and real\nenvironments. The accuracy of mapping and fusion of the proposed system is\ncomparable to VINS-Mono which requires higher computing resources.",
          "link": "http://arxiv.org/abs/2106.12186",
          "publishedOn": "2021-06-24T01:51:43.001Z",
          "wordCount": 616,
          "title": "Collaborative Visual Inertial SLAM for Multiple Smart Phones. (arXiv:2106.12186v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled and unseen target domain, which is\nusually trained on data from both domains. Access to the source domain data at\nthe adaptation stage, however, is often limited, due to data storage or privacy\nissues. To alleviate this, in this work, we target source free UDA for\nsegmentation, and propose to adapt an ``off-the-shelf\" segmentation model\npre-trained in the source domain to the target domain, with an adaptive\nbatch-wise normalization statistics adaptation framework. Specifically, the\ndomain-specific low-order batch statistics, i.e., mean and variance, are\ngradually adapted with an exponential momentum decay scheme, while the\nconsistency of domain shareable high-order batch statistics, i.e., scaling and\nshifting parameters, is explicitly enforced by our optimization objective. The\ntransferability of each channel is adaptively measured first from which to\nbalance the contribution of each channel. Moreover, the proposed source free\nUDA framework is orthogonal to unsupervised learning methods, e.g.,\nself-entropy minimization, which can thus be simply added on top of our\nframework. Extensive experiments on the BraTS 2018 database show that our\nsource free UDA framework outperformed existing source-relaxed UDA methods for\nthe cross-subtype UDA segmentation task and yielded comparable results for the\ncross-modality UDA segmentation task, compared with a supervised UDA methods\nwith the source data.",
          "link": "http://arxiv.org/abs/2106.12497",
          "publishedOn": "2021-06-24T01:51:42.987Z",
          "wordCount": 673,
          "title": "Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ploumpis_S/0/1/0/all/0/1\">Stylianos Ploumpis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschoglou_S/0/1/0/all/0/1\">Stylianos Moschoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triantafyllou_V/0/1/0/all/0/1\">Vasileios Triantafyllou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "3D face reconstruction from a single image is a task that has garnered\nincreased interest in the Computer Vision community, especially due to its\nbroad use in a number of applications such as realistic 3D avatar creation,\npose invariant face recognition and face hallucination. Since the introduction\nof the 3D Morphable Model in the late 90's, we witnessed an explosion of\nresearch aiming at particularly tackling this task. Nevertheless, despite the\nincreasing level of detail in the 3D face reconstructions from single images\nmainly attributed to deep learning advances, finer and highly deformable\ncomponents of the face such as the tongue are still absent from all 3D face\nmodels in the literature, although being very important for the realness of the\n3D avatar representations. In this work we present the first, to the best of\nour knowledge, end-to-end trainable pipeline that accurately reconstructs the\n3D face together with the tongue. Moreover, we make this pipeline robust in\n\"in-the-wild\" images by introducing a novel GAN method tailored for 3D tongue\nsurface generation. Finally, we make publicly available to the community the\nfirst diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals\nvarying in gender, age, and ethnicity backgrounds. As we demonstrate in an\nextensive series of quantitative as well as qualitative experiments, our model\nproves to be robust and realistically captures the 3D tongue structure, even in\nadverse \"in-the-wild\" conditions.",
          "link": "http://arxiv.org/abs/2106.12302",
          "publishedOn": "2021-06-24T01:51:42.953Z",
          "wordCount": 678,
          "title": "3D human tongue reconstruction from single \"in-the-wild\" images. (arXiv:2106.12302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lanzini_E/0/1/0/all/0/1\">Edoardo Lanzini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "The natural world is long-tailed: rare classes are observed orders of\nmagnitudes less frequently than common ones, leading to highly-imbalanced data\nwhere rare classes can have only handfuls of examples. Learning from few\nexamples is a known challenge for deep learning based classification\nalgorithms, and is the focus of the field of low-shot learning. One potential\napproach to increase the training data for these rare classes is to augment the\nlimited real data with synthetic samples. This has been shown to help, but the\ndomain shift between real and synthetic hinders the approaches' efficacy when\ntested on real data.\n\nWe explore the use of image-to-image translation methods to close the domain\ngap between synthetic and real imagery for animal species classification in\ndata collected from camera traps: motion-activated static cameras used to\nmonitor wildlife. We use low-level feature alignment between source and target\ndomains to make synthetic data for a rare species generated using a graphics\nengine more \"realistic\". Compared against a system augmented with unaligned\nsynthetic data, our experiments show a considerable decrease in classification\nerror rates on a rare species.",
          "link": "http://arxiv.org/abs/2106.12212",
          "publishedOn": "2021-06-24T01:51:42.937Z",
          "wordCount": 613,
          "title": "Image-to-Image Translation of Synthetic Samples for Rare Classes. (arXiv:2106.12212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12450",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lie_J/0/1/0/all/0/1\">Ji Lie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Leida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiumei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Visual Emotion Analysis (VEA) has attracted increasing attention recently\nwith the prevalence of sharing images on social networks. Since human emotions\nare ambiguous and subjective, it is more reasonable to address VEA in a label\ndistribution learning (LDL) paradigm rather than a single-label classification\ntask. Different from other LDL tasks, there exist intrinsic relationships\nbetween emotions and unique characteristics within them, as demonstrated in\npsychological theories. Inspired by this, we propose a well-grounded\ncircular-structured representation to utilize the prior knowledge for visual\nemotion distribution learning. To be specific, we first construct an Emotion\nCircle to unify any emotional state within it. On the proposed Emotion Circle,\neach emotion distribution is represented with an emotion vector, which is\ndefined with three attributes (i.e., emotion polarity, emotion type, emotion\nintensity) as well as two properties (i.e., similarity, additivity). Besides,\nwe design a novel Progressive Circular (PC) loss to penalize the\ndissimilarities between predicted emotion vector and labeled one in a\ncoarse-to-fine manner, which further boosts the learning process in an\nemotion-specific way. Extensive experiments and comparisons are conducted on\npublic visual emotion distribution datasets, and the results demonstrate that\nthe proposed method outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.12450",
          "publishedOn": "2021-06-24T01:51:42.925Z",
          "wordCount": 636,
          "title": "A Circular-Structured Representation for Visual Emotion Distribution Learning. (arXiv:2106.12450v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.",
          "link": "http://arxiv.org/abs/2106.12407",
          "publishedOn": "2021-06-24T01:51:42.915Z",
          "wordCount": 656,
          "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wentao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1\">Chengyu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiman Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>",
          "description": "Although instance segmentation has made considerable advancement over recent\nyears, it's still a challenge to design high accuracy algorithms with real-time\nperformance. In this paper, we propose a real-time instance segmentation\nframework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask\nhead is added to predict some discriminative orientation maps, which are\nexplicitly defined as spatial offset vectors for both foreground and background\npixels. Thanks to the discrimination ability of orientation maps, masks can be\nrecovered without the need for extra foreground segmentation. All instances\nthat match with the same anchor size share a common orientation map. This\nspecial sharing strategy reduces the amortized memory utilization for mask\npredictions but without loss of mask granularity. Given the surviving box\npredictions after NMS, instance masks can be concurrently constructed from the\ncorresponding orientation maps with low complexity. Owing to the concise design\nfor mask representation and its effective integration with the anchor-based\nobject detector, our method is qualified under real-time conditions while\nmaintaining competitive accuracy. Experiments on COCO benchmark show that\nOrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a\nsingle RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.",
          "link": "http://arxiv.org/abs/2106.12204",
          "publishedOn": "2021-06-24T01:51:42.879Z",
          "wordCount": 635,
          "title": "Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.",
          "link": "http://arxiv.org/abs/2106.12300",
          "publishedOn": "2021-06-24T01:51:42.818Z",
          "wordCount": 632,
          "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zeyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1\">Bangyang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>",
          "description": "Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.\ntype 2, is an essential prognostic factor. The two subtypes of pRCC have a\nsimilar pattern, i.e., the papillary architecture, yet some subtle differences,\nincluding cellular and cell-layer level patterns. However, the cellular and\ncell-layer level patterns almost cannot be captured by existing CNN-based\nmodels in large-size histopathological images, which brings obstacles to\ndirectly applying these models to such a fine-grained classification task. This\npaper proposes a novel instance-based Vision Transformer (i-ViT) to learn\nrobust representations of histopathological images for the pRCC subtyping task\nby extracting finer features from instance patches (by cropping around\nsegmented nuclei and assigning predicted grades). The proposed i-ViT takes\ntop-K instances as input and aggregates them for capturing both the cellular\nand cell-layer level patterns by a position-embedding layer, a grade-embedding\nlayer, and a multi-head multi-layer self-attention module. To evaluate the\nperformance of the proposed framework, experienced pathologists are invited to\nselected 1162 regions of interest from 171 whole slide images of type 1 and\ntype 2 pRCC. Experimental results show that the proposed method achieves better\nperformance than existing CNN-based models with a significant margin.",
          "link": "http://arxiv.org/abs/2106.12265",
          "publishedOn": "2021-06-24T01:51:42.810Z",
          "wordCount": 653,
          "title": "Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image. (arXiv:2106.12265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_Z/0/1/0/all/0/1\">Ziko Imtiaz Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>",
          "description": "We introduce Transductive Infomation Maximization (TIM) for few-shot\nlearning. Our method maximizes the mutual information between the query\nfeatures and their label predictions for a given few-shot task, in conjunction\nwith a supervision loss based on the support set. We motivate our transductive\nloss by deriving a formal relation between the classification accuracy and\nmutual-information maximization. Furthermore, we propose a new\nalternating-direction solver, which substantially speeds up transductive\ninference over gradient-based optimization, while yielding competitive\naccuracy. We also provide a convergence analysis of our solver based on\nZangwill's theory and bound-optimization arguments. TIM inference is modular:\nit can be used on top of any base-training feature extractor. Following\nstandard transductive few-shot settings, our comprehensive experiments\ndemonstrate that TIM outperforms state-of-the-art methods significantly across\nvarious datasets and networks, while used on top of a fixed feature extractor\ntrained with simple cross-entropy on the base classes, without resorting to\ncomplex meta-learning schemes. It consistently brings between 2 % and 5 %\nimprovement in accuracy over the best performing method, not only on all the\nwell-established few-shot benchmarks but also on more challenging scenarios,\nwith random tasks, domain shift and larger numbers of classes, as in the\nrecently introduced META-DATASET. Our code is publicly available at\nhttps://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorch\nimplementation of META-DATASET, along with additional benchmarking results, at\nhttps://github.com/mboudiaf/pytorch-meta-dataset.",
          "link": "http://arxiv.org/abs/2106.12252",
          "publishedOn": "2021-06-24T01:51:42.804Z",
          "wordCount": 673,
          "title": "Mutual-Information Based Few-Shot Classification. (arXiv:2106.12252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12175",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Image denoising is of great importance for medical imaging system, since it\ncan improve image quality for disease diagnosis and downstream image analyses.\nIn a variety of applications, dynamic imaging techniques are utilized to\ncapture the time-varying features of the subject, where multiple images are\nacquired for the same subject at different time points. Although\nsignal-to-noise ratio of each time frame is usually limited by the short\nacquisition time, the correlation among different time frames can be exploited\nto improve denoising results with shared information across time frames. With\nthe success of neural networks in computer vision, supervised deep learning\nmethods show prominent performance in single-image denoising, which rely on\nlarge datasets with clean-vs-noisy image pairs. Recently, several\nself-supervised deep denoising models have been proposed, achieving promising\nresults without needing the pairwise ground truth of clean images. In the field\nof multi-image denoising, however, very few works have been done on extracting\ncorrelated information from multiple slices for denoising using self-supervised\ndeep learning methods. In this work, we propose Deformed2Self, an end-to-end\nself-supervised deep learning framework for dynamic imaging denoising. It\ncombines single-image and multi-image denoising to improve image quality and\nuse a spatial transformer network to model motion between different slices.\nFurther, it only requires a single noisy image with a few auxiliary\nobservations at different time frames for training and inference. Evaluations\non phantom and in vivo data with different noise statistics show that our\nmethod has comparable performance to other state-of-the-art unsupervised or\nself-supervised denoising methods and outperforms under high noise levels.",
          "link": "http://arxiv.org/abs/2106.12175",
          "publishedOn": "2021-06-24T01:51:42.786Z",
          "wordCount": 694,
          "title": "Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging. (arXiv:2106.12175v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Boyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>",
          "description": "Over the years, accelerating neural networks with quantization has been\nwidely studied. Unfortunately, prior efforts with diverse precisions (e.g.,\n1-bit weights and 2-bit activations) are usually restricted by limited\nprecision support on GPUs (e.g., int1 and int4). To break such restrictions, we\nintroduce the first Arbitrary Precision Neural Network framework (APNN-TC) to\nfully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,\nAPNN-TC first incorporates a novel emulation algorithm to support arbitrary\nshort bit-width computation with int1 compute primitives and XOR/AND Boolean\noperations. Second, APNN-TC integrates arbitrary precision layer designs to\nefficiently map our emulation algorithm to Tensor Cores with novel batching\nstrategies and specialized memory organization. Third, APNN-TC embodies a novel\narbitrary precision NN design to minimize memory access across layers and\nfurther improve performance. Extensive evaluations show that APNN-TC can\nachieve significant speedup over CUTLASS kernels and various NN models, such as\nResNet and VGG.",
          "link": "http://arxiv.org/abs/2106.12169",
          "publishedOn": "2021-06-24T01:51:42.780Z",
          "wordCount": 610,
          "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores. (arXiv:2106.12169v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.",
          "link": "http://arxiv.org/abs/2106.12183",
          "publishedOn": "2021-06-24T01:51:42.774Z",
          "wordCount": 707,
          "title": "A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Shaoyi Du</a>",
          "description": "Background noise and scale variation are common problems that have been long\nrecognized in crowd counting. Humans glance at a crowd image and instantly know\nthe approximate number of human and where they are through attention the crowd\nregions and the congestion degree of crowd regions with a global receptive\nfiled. Hence, in this paper, we propose a novel feedback network with\nRegion-Aware block called RANet by modeling human's Top-Down visual perception\nmechanism. Firstly, we introduce a feedback architecture to generate priority\nmaps that provide prior about candidate crowd regions in input images. The\nprior enables the RANet pay more attention to crowd regions. Then we design\nRegion-Aware block that could adaptively encode the contextual information into\ninput images through global receptive field. More specifically, we scan the\nwhole input images and its priority maps in the form of column vector to obtain\na relevance matrix estimating their similarity. The relevance matrix obtained\nwould be utilized to build global relationships between pixels. Our method\noutperforms state-of-the-art crowd counting methods on several public datasets.",
          "link": "http://arxiv.org/abs/2106.12163",
          "publishedOn": "2021-06-24T01:51:42.768Z",
          "wordCount": 618,
          "title": "Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1\">Fangyuan Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianjian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruijun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiangzhong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yusen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qingyun Dai</a>",
          "description": "In deep learning area, large-scale image datasets bring a breakthrough in the\nsuccess of object recognition and retrieval. Nowadays, as the embodiment of\ninnovation, the diversity of the industrial goods is significantly larger, in\nwhich the incomplete multiview, multimodal and multilabel are different from\nthe traditional dataset. In this paper, we introduce an industrial goods\ndataset, namely PatentNet, with numerous highly diverse, accurate and detailed\nannotations of industrial goods images, and corresponding texts. In PatentNet,\nthe images and texts are sourced from design patent. Within over 6M images and\ncorresponding texts of industrial goods labeled manually checked by\nprofessionals, PatentNet is the first ongoing industrial goods image database\nwhose varieties are wider than industrial goods datasets used previously for\nbenchmarking. PatentNet organizes millions of images into 32 classes and 219\nsubclasses based on the Locarno Classification Agreement. Through extensive\nexperiments on image classification, image retrieval and incomplete multiview\nclustering, we demonstrate that our PatentNet is much more diverse, complex,\nand challenging, enjoying higher potentials than existing industrial image\ndatasets. Furthermore, the characteristics of incomplete multiview, multimodal\nand multilabel in PatentNet are able to offer unparalleled opportunities in the\nartificial intelligence community and beyond.",
          "link": "http://arxiv.org/abs/2106.12139",
          "publishedOn": "2021-06-24T01:51:42.762Z",
          "wordCount": 649,
          "title": "PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database. (arXiv:2106.12139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shirke_A/0/1/0/all/0/1\">Aniket Shirke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golden_R/0/1/0/all/0/1\">Rebecca Golden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_M/0/1/0/all/0/1\">Mrinal Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_Miller_A/0/1/0/all/0/1\">Angela Green-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_M/0/1/0/all/0/1\">Matthew Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilger_R/0/1/0/all/0/1\">Ryan N. Dilger</a>",
          "description": "Behavioral scoring of research data is crucial for extracting domain-specific\nmetrics but is bottlenecked on the ability to analyze enormous volumes of\ninformation using human labor. Deep learning is widely viewed as a key\nadvancement to relieve this bottleneck. We identify one such domain, where deep\nlearning can be leveraged to alleviate the process of manual scoring. Novelty\npreference paradigms have been widely used to study recognition memory in pigs,\nbut analysis of these videos requires human intervention. We introduce a subset\nof such videos in the form of the 'Pig Novelty Preference Behavior' (PNPB)\ndataset that is fully annotated with pig actions and keypoints. In order to\ndemonstrate the application of state-of-the-art action recognition models on\nthis dataset, we compare LRCN, C3D, and TSM on the basis of various analytical\nmetrics and discuss common pitfalls of the models. Our methods achieve an\naccuracy of 93% and a mean Average Precision of 96% in estimating piglet\nbehavior.\n\nWe open-source our code and annotated dataset at\nhttps://github.com/AIFARMS/NOR-behavior-recognition",
          "link": "http://arxiv.org/abs/2106.12181",
          "publishedOn": "2021-06-24T01:51:42.746Z",
          "wordCount": 621,
          "title": "Vision-based Behavioral Recognition of Novelty Preference in Pigs. (arXiv:2106.12181v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12157",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Akilan_T/0/1/0/all/0/1\">Thangarajah Akilan</a>",
          "description": "The coronavirus continues to disrupt our everyday lives as it spreads at an\nexponential rate. It needs to be detected quickly in order to quarantine\npositive patients so as to avoid further spread. This work proposes a new\nconvolutional neural network (CNN) architecture called 'slow Encoding CNN. The\nproposed model's best performance wrt Sensitivity, Positive Predictive Value\n(PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19\n- Screening X-ray images for COVID-19 Infections competition's test data\nsamples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive\nclass, while PN and SN stand for the Sensitivity and PPV of the COVID-19\nnegative class.",
          "link": "http://arxiv.org/abs/2106.12157",
          "publishedOn": "2021-06-24T01:51:42.741Z",
          "wordCount": 594,
          "title": "CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis. (arXiv:2106.12157v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yusong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaogang Jia</a>",
          "description": "It is desirable to transfer the knowledge stored in a well-trained source\nmodel onto non-annotated target domain in the absence of source data. However,\nstate-of-the-art methods for source free domain adaptation (SFDA) are subject\nto strict limits: 1) access to internal specifications of source models is a\nmust; and 2) pseudo labels should be clean during self-training, making\ncritical tasks relying on semantic segmentation unreliable. Aiming at these\npitfalls, this study develops a domain adaptive solution to semantic\nsegmentation with pseudo label rectification (namely \\textit{PR-SFDA}), which\noperates in two phases: 1) \\textit{Confidence-regularized unsupervised\nlearning}: Maximum squares loss applies to regularize the target model to\nensure the confidence in prediction; and 2) \\textit{Noise-aware pseudo label\nlearning}: Negative learning enables tolerance to noisy pseudo labels in\ntraining, meanwhile positive learning achieves fast convergence. Extensive\nexperiments have been performed on domain adaptive semantic segmentation\nbenchmark, \\textit{GTA5 $\\to$ Cityscapes}. Overall, \\textit{PR-SFDA} achieves a\nperformance of 49.0 mIoU, which is very close to that of the state-of-the-art\ncounterparts. Note that the latter demand accesses to the source model's\ninternal specifications, whereas the \\textit{PR-SFDA} solution needs none as a\nsharp contrast.",
          "link": "http://arxiv.org/abs/2106.12123",
          "publishedOn": "2021-06-24T01:51:42.728Z",
          "wordCount": 645,
          "title": "Exploiting Negative Learning for Implicit Pseudo Label Rectification in Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2106.12123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1\">Tomoya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoang-Dung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1\">Bardh Hoxha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1\">Taylor T Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prokhorov_D/0/1/0/all/0/1\">Danil Prokhorov</a>",
          "description": "Deep convolutional neural networks have been widely employed as an effective\ntechnique to handle complex and practical problems. However, one of the\nfundamental problems is the lack of formal methods to analyze their behavior.\nTo address this challenge, we propose an approach to compute the exact\nreachable sets of a network given an input domain, where the reachable set is\nrepresented by the face lattice structure. Besides the computation of reachable\nsets, our approach is also capable of backtracking to the input domain given an\noutput reachable set. Therefore, a full analysis of a network's behavior can be\nrealized. In addition, an approach for fast analysis is also introduced, which\nconducts fast computation of reachable sets by considering selected sensitive\nneurons in each layer. The exact pixel-level reachability analysis method is\nevaluated on a CNN for the CIFAR10 dataset and compared to related works. The\nfast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16\narchitecture for the ImageNet dataset.",
          "link": "http://arxiv.org/abs/2106.12074",
          "publishedOn": "2021-06-24T01:51:42.722Z",
          "wordCount": 599,
          "title": "Reachability Analysis of Convolutional Neural Networks. (arXiv:2106.12074v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yariv_L/0/1/0/all/0/1\">Lior Yariv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Neural volume rendering became increasingly popular recently due to its\nsuccess in synthesizing novel views of a scene from a sparse set of input\nimages. So far, the geometry learned by neural volume rendering techniques was\nmodeled using a generic density function. Furthermore, the geometry itself was\nextracted using an arbitrary level set of the density function leading to a\nnoisy, often low fidelity reconstruction. The goal of this paper is to improve\ngeometry representation and reconstruction in neural volume rendering. We\nachieve that by modeling the volume density as a function of the geometry. This\nis in contrast to previous work modeling the geometry as a function of the\nvolume density. In more detail, we define the volume density function as\nLaplace's cumulative distribution function (CDF) applied to a signed distance\nfunction (SDF) representation. This simple density representation has three\nbenefits: (i) it provides a useful inductive bias to the geometry learned in\nthe neural volume rendering process; (ii) it facilitates a bound on the opacity\napproximation error, leading to an accurate sampling of the viewing ray.\nAccurate sampling is important to provide a precise coupling of geometry and\nradiance; and (iii) it allows efficient unsupervised disentanglement of shape\nand appearance in volume rendering. Applying this new density representation to\nchallenging scene multiview datasets produced high quality geometry\nreconstructions, outperforming relevant baselines. Furthermore, switching shape\nand appearance between scenes is possible due to the disentanglement of the\ntwo.",
          "link": "http://arxiv.org/abs/2106.12052",
          "publishedOn": "2021-06-24T01:51:42.700Z",
          "wordCount": 671,
          "title": "Volume Rendering of Neural Implicit Surfaces. (arXiv:2106.12052v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayouti_N/0/1/0/all/0/1\">Noureini Sayouti</a>",
          "description": "Image captioning has increasingly large domains of application, and fashion\nis not an exception. Having automatic item descriptions is of great interest\nfor fashion web platforms hosting sometimes hundreds of thousands of images.\nThis paper is one of the first tackling image captioning for fashion images. To\ncontribute addressing dataset diversity issues, we introduced the InFashAIv1\ndataset containing almost 16.000 African fashion item images with their titles,\nprices and general descriptions. We also used the well known DeepFashion\ndataset in addition to InFashAIv1. Captions are generated using the\n\\textit{Show and Tell} model made of CNN encoder and RNN Decoder. We showed\nthat jointly training the model on both datasets improves captions quality for\nAfrican style fashion images, suggesting a transfer learning from Western style\ndata. The InFashAIv1 dataset is released on\n\\href{https://github.com/hgilles06/infashai}{Github} to encourage works with\nmore diversity inclusion.",
          "link": "http://arxiv.org/abs/2106.12154",
          "publishedOn": "2021-06-24T01:51:42.695Z",
          "wordCount": 583,
          "title": "Neural Fashion Image Captioning : Accounting for Data Diversity. (arXiv:2106.12154v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1\">Brian D. Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talghader_V/0/1/0/all/0/1\">Vivien W. Talghader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhiyong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunkel_G/0/1/0/all/0/1\">Gary J. Kunkel</a>",
          "description": "Transmission electron microscopy (TEM) is one of the primary tools to show\nmicrostructural characterization of materials as well as film thickness.\nHowever, manual determination of film thickness from TEM images is\ntime-consuming as well as subjective, especially when the films in question are\nvery thin and the need for measurement precision is very high. Such is the case\nfor head overcoat (HOC) thickness measurements in the magnetic hard disk drive\nindustry. It is therefore necessary to develop software to automatically\nmeasure HOC thickness. In this paper, for the first time, we propose a HOC\nlayer segmentation method using NASNet-Large as an encoder and then followed by\na decoder architecture, which is one of the most commonly used architectures in\ndeep learning for image segmentation. To further improve segmentation results,\nwe are the first to propose a post-processing layer to remove irrelevant\nportions in the segmentation result. To measure the thickness of the segmented\nHOC layer, we propose a regressive convolutional neural network (RCNN) model as\nwell as orthogonal thickness calculation methods. Experimental results\ndemonstrate a higher dice score for our model which has lower mean squared\nerror and outperforms current state-of-the-art manual measurement.",
          "link": "http://arxiv.org/abs/2106.12054",
          "publishedOn": "2021-06-24T01:51:42.690Z",
          "wordCount": 635,
          "title": "Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net. (arXiv:2106.12054v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arian_R/0/1/0/all/0/1\">Reeshad Arian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>",
          "description": "This article explores subspace clustering algorithms using CUR\ndecompositions, and examines the effect of various hyperparameters in these\nalgorithms on clustering performance on two real-world benchmark datasets, the\nHopkins155 motion segmentation dataset and the Yale face dataset. Extensive\nexperiments are done for a variety of sampling methods and oversampling\nparameters for these datasets, and some guidelines for parameter choices are\ngiven for practical applications.",
          "link": "http://arxiv.org/abs/2106.12016",
          "publishedOn": "2021-06-24T01:51:42.684Z",
          "wordCount": 505,
          "title": "On Matrix Factorizations in Subspace Clustering. (arXiv:2106.12016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>",
          "description": "Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.",
          "link": "http://arxiv.org/abs/2106.12070",
          "publishedOn": "2021-06-24T01:51:42.668Z",
          "wordCount": 635,
          "title": "Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Many learning-based 3D shape semantic segmentation methods assign labels to\nshape atoms (e.g. points in a point cloud or faces in a mesh) with a\nsingle-pass approach trained in an end-to-end fashion. Such methods achieve\nimpressive performance but require large amounts of labeled training data. This\nparadigm entangles two separable subproblems: (1) decomposing a shape into\nregions and (2) assigning semantic labels to these regions. We claim that\ndisentangling these subproblems reduces the labeled data burden: (1) region\ndecomposition requires no semantic labels and could be performed in an\nunsupervised fashion, and (2) labeling shape regions instead of atoms results\nin a smaller search space and should be learnable with less labeled training\ndata. In this paper, we investigate this second claim by presenting the\nNeurally-Guided Shape Parser (NGSP), a method that learns how to assign\nsemantic labels to regions of an over-segmented 3D shape. We solve this problem\nvia MAP inference, modeling the posterior probability of a labeling assignment\nconditioned on an input shape. We employ a Monte Carlo importance sampling\napproach guided by a neural proposal network, a search-based approach made\nfeasible by assuming the input shape is decomposed into discrete regions. We\nevaluate NGSP on the task of hierarchical semantic segmentation on manufactured\n3D shapes from PartNet. We find that NGSP delivers significant performance\nimprovements over baselines that learn to label shape atoms and then aggregate\npredictions for each shape region, especially in low-data regimes. Finally, we\ndemonstrate that NGSP is robust to region granularity, as it maintains strong\nsegmentation performance even as the regions undergo significant corruption.",
          "link": "http://arxiv.org/abs/2106.12026",
          "publishedOn": "2021-06-24T01:51:42.663Z",
          "wordCount": 719,
          "title": "The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yagubbayli_F/0/1/0/all/0/1\">Farid Yagubbayli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1\">Alessio Tonioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Most modern deep learning-based multi-view 3D reconstruction techniques use\nRNNs or fusion modules to combine information from multiple images after\nencoding them. These two separate steps have loose connections and do not\nconsider all available information while encoding each view. We propose\nLegoFormer, a transformer-based model that unifies object reconstruction under\na single framework and parametrizes the reconstructed occupancy grid by its\ndecomposition factors. This reformulation allows the prediction of an object as\na set of independent structures then aggregated to obtain the final\nreconstruction. Experiments conducted on ShapeNet display the competitive\nperformance of our network with respect to the state-of-the-art methods. We\nalso demonstrate how the use of self-attention leads to increased\ninterpretability of the model output.",
          "link": "http://arxiv.org/abs/2106.12102",
          "publishedOn": "2021-06-24T01:51:42.657Z",
          "wordCount": 550,
          "title": "LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zejian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>",
          "description": "In this work, we propose a novel straightforward method for medical volume\nand sequence segmentation with limited annotations. To avert laborious\nannotating, the recent success of self-supervised learning(SSL) motivates the\npre-training on unlabeled data. Despite its success, it is still challenging to\nadapt typical SSL methods to volume/sequence segmentation, due to their lack of\nmining on local semantic discrimination and rare exploitation on volume and\nsequence structures. Based on the continuity between slices/frames and the\ncommon spatial layout of organs across volumes/sequences, we introduced a novel\nbootstrap self-supervised representation learning method by leveraging the\npredictable possibility of neighboring slices. At the core of our method is a\nsimple and straightforward dense self-supervision on the predictions of local\nrepresentations and a strategy of predicting locals based on global context,\nwhich enables stable and reliable supervision for both global and local\nrepresentation mining among volumes. Specifically, we first proposed an\nasymmetric network with an attention-guided predictor to enforce\ndistance-specific prediction and supervision on slices within and across\nvolumes/sequences. Secondly, we introduced a novel prototype-based\nforeground-background calibration module to enhance representation consistency.\nThe two parts are trained jointly on labeled and unlabeled data. When evaluated\non three benchmark datasets of medical volumes and sequences, our model\noutperforms existing methods with a large margin of 4.5\\% DSC on ACDC, 1.7\\% on\nProstate, and 2.3\\% on CAMUS. Intensive evaluations reveals the effectiveness\nand superiority of our method.",
          "link": "http://arxiv.org/abs/2106.12153",
          "publishedOn": "2021-06-24T01:51:42.649Z",
          "wordCount": 676,
          "title": "Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences. (arXiv:2106.12153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Catherine Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1\">Rachel Sparks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1\">Beate Diehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>",
          "description": "Detailed analysis of seizure semiology, the symptoms and signs which occur\nduring a seizure, is critical for management of epilepsy patients. Inter-rater\nreliability using qualitative visual analysis is often poor for semiological\nfeatures. Therefore, automatic and quantitative analysis of video-recorded\nseizures is needed for objective assessment.\n\nWe present GESTURES, a novel architecture combining convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) to learn deep\nrepresentations of arbitrarily long videos of epileptic seizures.\n\nWe use a spatiotemporal CNN (STCNN) pre-trained on large human action\nrecognition (HAR) datasets to extract features from short snippets (approx. 0.5\ns) sampled from seizure videos. We then train an RNN to learn seizure-level\nrepresentations from the sequence of features.\n\nWe curated a dataset of seizure videos from 68 patients and evaluated\nGESTURES on its ability to classify seizures into focal onset seizures (FOSs)\n(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),\nobtaining an accuracy of 98.9% using bidirectional long short-term memory\n(BLSTM) units.\n\nWe demonstrate that an STCNN trained on a HAR dataset can be used in\ncombination with an RNN to accurately represent arbitrarily long videos of\nseizures. GESTURES can provide accurate seizure classification by modeling\nsequences of semiologies.",
          "link": "http://arxiv.org/abs/2106.12014",
          "publishedOn": "2021-06-24T01:51:42.611Z",
          "wordCount": 664,
          "title": "Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shishido_T/0/1/0/all/0/1\">Tomoyuki Shishido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fati_F/0/1/0/all/0/1\">Fehmiju Fati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokushige_D/0/1/0/all/0/1\">Daisuke Tokushige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ono_Y/0/1/0/all/0/1\">Yasuhiro Ono</a>",
          "description": "Deep learning has recently been applied to optical music recognition (OMR).\nHowever, currently OMR processing from various sheet music images still lacks\nprecision to be widely applicable. Here, we present an MMdA (Measure-based\nMultimodal deep learning (DL)-driven Assembly) method allowing for end-to-end\nOMR processing from various images including inclined photo images. Using this\nmethod, measures are extracted by a deep learning model, aligned, and resized\nto be used for inference of given musical symbol components by using multiple\ndeep learning models in sequence or in parallel. Use of each standardized\nmeasure enables efficient training of the models and accurate adjustment of\nfive staff lines in each measure. Multiple musical symbol component category\nmodels with a small number of feature types can represent a diverse set of\nnotes and other musical symbols including chords. This MMdA method provides a\nsolution to end-to-end OMR processing with precision.",
          "link": "http://arxiv.org/abs/2106.12037",
          "publishedOn": "2021-06-24T01:51:42.604Z",
          "wordCount": 606,
          "title": "Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly. (arXiv:2106.12037v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>",
          "description": "This paper jointly resolves two problems in vision transformer: i) the\ncomputation of Multi-Head Self-Attention (MHSA) has high computational/space\ncomplexity; ii) recent vision transformer networks are overly tuned for image\nclassification, ignoring the difference between image classification (simple\nscenarios, more similar to NLP) and downstream scene understanding tasks\n(complicated scenarios, rich structural and contextual information). To this\nend, we note that pyramid pooling has been demonstrated to be effective in\nvarious vision tasks owing to its powerful context abstraction, and its natural\nproperty of spatial invariance is suitable to address the loss of structural\ninformation (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA\nfor alleviating its high requirement on computational resources (problem i)).\nIn this way, this pooling-based MHSA can well address the above two problems\nand is thus flexible and powerful for downstream scene understanding tasks.\nPlugged with our pooling-based MHSA, we build a downstream-task-oriented\ntransformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive\nexperiments demonstrate that, when applied P2T as the backbone network, it\nshows substantial superiority in various downstream scene understanding tasks\nsuch as semantic segmentation, object detection, instance segmentation, and\nvisual saliency detection, compared to previous CNN- and transformer-based\nnetworks. The code will be released at https://github.com/yuhuan-wu/P2T. Note\nthat this technical report will keep updating.",
          "link": "http://arxiv.org/abs/2106.12011",
          "publishedOn": "2021-06-24T01:51:42.577Z",
          "wordCount": 646,
          "title": "P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1\">Bhusan Chettri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1\">Rosa Gonz&#xe1;lez Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>",
          "description": "Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.",
          "link": "http://arxiv.org/abs/2103.14602",
          "publishedOn": "2021-06-23T01:48:40.033Z",
          "wordCount": 579,
          "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1\">Johannes Kopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_X/0/1/0/all/0/1\">Xuejian Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>",
          "description": "We present an algorithm for estimating consistent dense depth maps and camera\nposes from a monocular video. We integrate a learning-based depth prior, in the\nform of a convolutional neural network trained for single-image depth\nestimation, with geometric optimization, to estimate a smooth camera trajectory\nas well as detailed and stable depth reconstruction. Our algorithm combines two\ncomplementary techniques: (1) flexible deformation-splines for low-frequency\nlarge-scale alignment and (2) geometry-aware depth filtering for high-frequency\nalignment of fine depth details. In contrast to prior approaches, our method\ndoes not require camera poses as input and achieves robust reconstruction for\nchallenging hand-held cell phone captures containing a significant amount of\nnoise, shake, motion blur, and rolling shutter deformations. Our method\nquantitatively outperforms state-of-the-arts on the Sintel benchmark for both\ndepth and pose estimations and attains favorable qualitative results across\ndiverse wild datasets.",
          "link": "http://arxiv.org/abs/2012.05901",
          "publishedOn": "2021-06-23T01:48:40.017Z",
          "wordCount": 600,
          "title": "Robust Consistent Video Depth Estimation. (arXiv:2012.05901v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaohan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiange Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigall_E/0/1/0/all/0/1\">Eric Rigall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>",
          "description": "Textures contain a wealth of image information and are widely used in various\nfields such as computer graphics and computer vision. With the development of\nmachine learning, the texture synthesis and generation have been greatly\nimproved. As a very common element in everyday life, wallpapers contain a\nwealth of texture information, making it difficult to annotate with a simple\nsingle label. Moreover, wallpaper designers spend significant time to create\ndifferent styles of wallpaper. For this purpose, this paper proposes to\ndescribe wallpaper texture images by using multi-label semantics. Based on\nthese labels and generative adversarial networks, we present a framework for\nperception driven wallpaper texture generation and style transfer. In this\nframework, a perceptual model is trained to recognize whether the wallpapers\nproduced by the generator network are sufficiently realistic and have the\nattribute designated by given perceptual description; these multi-label\nsemantic attributes are treated as condition variables to generate wallpaper\nimages. The generated wallpaper images can be converted to those with\nwell-known artist styles using CycleGAN. Finally, using the aesthetic\nevaluation method, the generated wallpaper images are quantitatively measured.\nThe experimental results demonstrate that the proposed method can generate\nwallpaper textures conforming to human aesthetics and have artistic\ncharacteristics.",
          "link": "http://arxiv.org/abs/2106.11482",
          "publishedOn": "2021-06-23T01:48:39.981Z",
          "wordCount": 658,
          "title": "Wallpaper Texture Generation and Style Transfer Based on Multi-label Semantics. (arXiv:2106.11482v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhipeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiexi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Aming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>",
          "description": "Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal\nretrieval task, where abstract sketches are used as queries to retrieve natural\nimages under zero-shot scenario. Most existing methods regard ZS-SBIR as a\ntraditional classification problem and employ a cross-entropy or triplet-based\nloss to achieve retrieval, which neglect the problems of the domain gap between\nsketches and natural images and the large intra-class diversity in sketches.\nToward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.\nSpecifically, a cross-modal contrastive method is proposed to learn generalized\nrepresentations to smooth the domain gap by mining relations with additional\naugmented samples. Furthermore, a category-specific memory bank with sketch\nfeatures is explored to reduce intra-class diversity in the sketch domain.\nExtensive experiments demonstrate that our approach notably outperforms the\nstate-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source\ncode is publicly available at https://github.com/haowang1992/DSN.",
          "link": "http://arxiv.org/abs/2106.11841",
          "publishedOn": "2021-06-23T01:48:39.974Z",
          "wordCount": 586,
          "title": "Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2106.11841v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>",
          "description": "Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.",
          "link": "http://arxiv.org/abs/2106.11821",
          "publishedOn": "2021-06-23T01:48:39.962Z",
          "wordCount": 590,
          "title": "Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Feng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "LiDAR-based SLAM system is admittedly more accurate and stable than others,\nwhile its loop closure detection is still an open issue. With the development\nof 3D semantic segmentation for point cloud, semantic information can be\nobtained conveniently and steadily, essential for high-level intelligence and\nconductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM\nwith loop closure based on LOAM, named SA-LOAM, which leverages semantics in\nodometry as well as loop closure detection. Specifically, we propose a\nsemantic-assisted ICP, including semantically matching, downsampling and plane\nconstraint, and integrates a semantic graph-based place recognition method in\nour loop closure detection module. Benefitting from semantics, we can improve\nthe localization accuracy, detect loop closures effectively, and construct a\nglobal consistent semantic map even in large-scale scenes. Extensive\nexperiments on KITTI and Ford Campus dataset show that our system significantly\nimproves baseline performance, has generalization ability to unseen data and\nachieves competitive results compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11516",
          "publishedOn": "2021-06-23T01:48:39.955Z",
          "wordCount": 599,
          "title": "SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "This technical report presents our solution for temporal action detection\ntask in AcitivityNet Challenge 2021. The purpose of this task is to locate and\nidentify actions of interest in long untrimmed videos. The crucial challenge of\nthe task comes from that the temporal duration of action varies dramatically,\nand the target actions are typically embedded in a background of irrelevant\nactivities. Our solution builds on BMN, and mainly contains three steps: 1)\naction classification and feature encoding by Slowfast, CSN and ViViT; 2)\nproposal generation. We improve BMN by embedding the proposed Proposal Relation\nNetwork (PRN), by which we can generate proposals of high quality; 3) action\ndetection. We calculate the detection results by assigning the proposals with\ncorresponding classification results. Finally, we ensemble the results under\ndifferent settings and achieve 44.7% on the test set, which improves the\nchampion result in ActivityNet 2020 by 1.9% in terms of average mAP.",
          "link": "http://arxiv.org/abs/2106.11812",
          "publishedOn": "2021-06-23T01:48:39.948Z",
          "wordCount": 609,
          "title": "Proposal Relation Network for Temporal Action Detection. (arXiv:2106.11812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1\">Otto Seiskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1\">Pekka Rantalankila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1\">Jerry Ylilammi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>",
          "description": "We present HybVIO, a novel hybrid approach for combining filtering-based\nvisual-inertial odometry (VIO) with optimization-based SLAM. The core of our\nmethod is highly robust, independent VIO with improved IMU bias modeling,\noutlier rejection, stationarity detection, and feature track selection, which\nis adjustable to run on embedded hardware. Long-term consistency is achieved\nwith a loosely-coupled SLAM module. In academic benchmarks, our solution yields\nexcellent performance in all categories, especially in the real-time use case,\nwhere we outperform the current state-of-the-art. We also demonstrate the\nfeasibility of VIO for vehicular tracking on consumer-grade hardware using a\ncustom dataset, and show good performance in comparison to current commercial\nVISLAM alternatives.",
          "link": "http://arxiv.org/abs/2106.11857",
          "publishedOn": "2021-06-23T01:48:39.928Z",
          "wordCount": 547,
          "title": "HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1\">Marco Cannici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1\">Matteo Matteucci</a>",
          "description": "Skeleton-based Human Activity Recognition has achieved great interest in\nrecent years as skeleton data has demonstrated being robust to illumination\nchanges, body scales, dynamic camera views, and complex background. In\nparticular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated\nto be effective in learning both spatial and temporal dependencies on\nnon-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding\nof the latent information underlying the 3D skeleton is still an open problem,\nespecially when it comes to extracting effective information from joint motion\npatterns and their correlations. In this work, we propose a novel\nSpatial-Temporal Transformer network (ST-TR) which models dependencies between\njoints using the Transformer self-attention operator. In our ST-TR model, a\nSpatial Self-Attention module (SSA) is used to understand intra-frame\ninteractions between different body parts, and a Temporal Self-Attention module\n(TSA) to model inter-frame correlations. The two are combined in a two-stream\nnetwork, whose performance is evaluated on three large-scale datasets,\nNTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving\nbackbone results. Compared with methods that use the same input data, the\nproposed ST-TR achieves state-of-the-art performance on all datasets when using\njoints' coordinates as input, and results on-par with state-of-the-art when\nadding bones information.",
          "link": "http://arxiv.org/abs/2008.07404",
          "publishedOn": "2021-06-23T01:48:39.922Z",
          "wordCount": 705,
          "title": "Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks. (arXiv:2008.07404v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Abraham George Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1\">Cynthia Terrones-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1\">Anne Kiil Berthelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1\">Nora Jarrett Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1\">Lena Specht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1\">Ivan Richter Vogelius</a>",
          "description": "Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n\nSource code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.",
          "link": "http://arxiv.org/abs/2106.11942",
          "publishedOn": "2021-06-23T01:48:39.914Z",
          "wordCount": 618,
          "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1\">Quin Thames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1\">Arjun Karpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1\">Wade Norris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangting Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1\">Liviu Panait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>",
          "description": "Understanding the nutritional content of food from visual data is a\nchallenging computer vision problem, with the potential to have a positive and\nwidespread impact on public health. Studies in this area are limited to\nexisting datasets in the field that lack sufficient diversity or labels\nrequired for training models with nutritional understanding capability. We\nintroduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes\nwith corresponding video streams, depth images, component weights, and high\naccuracy nutritional content annotation. We demonstrate the potential of this\ndataset by training a computer vision algorithm capable of predicting the\ncaloric and macronutrient values of a complex, real world dish at an accuracy\nthat outperforms professional nutritionists. Further we present a baseline for\nincorporating depth sensor data to improve nutrition predictions. We will\npublicly release Nutrition5k in the hope that it will accelerate innovation in\nthe space of nutritional understanding.",
          "link": "http://arxiv.org/abs/2103.03375",
          "publishedOn": "2021-06-23T01:48:39.888Z",
          "wordCount": 630,
          "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>",
          "description": "Benefited from considerable pixel-level annotations collected from a specific\nsituation (source), the trained semantic segmentation model performs quite\nwell, but fails in a new situation (target) due to the large domain shift. To\nmitigate the domain gap, previous cross-domain semantic segmentation methods\nalways assume the co-existence of source data and target data during\ndistribution alignment. However, the access to source data in the real scenario\nmay raise privacy concerns and violate intellectual property. To tackle this\nproblem, we focus on an interesting and challenging cross-domain semantic\nsegmentation task where only the trained source model is provided to the target\ndomain, and further propose a unified framework called Domain Adaptive Semantic\nSegmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$\nconsists of three schemes, i.e., feature alignment, self-training, and\ninformation propagation. First, we mainly develop a focal entropic loss on the\nnetwork outputs to implicitly align the target features with unseen source\nfeatures via the provided source model. Second, besides positive pseudo labels\nin vanilla self-training, we first introduce negative pseudo labels to the\nfield and develop a bi-directional self-training strategy to enhance the\nrepresentation learning in the target domain. Finally, the information\npropagation scheme further reduces the intra-domain discrepancy within the\ntarget domain via pseudo semi-supervised learning. Extensive results on\nsynthesis-to-real and cross-city driving datasets validate DAS$^3$ yields\nstate-of-the-art performance, even on par with methods that need access to\nsource data.",
          "link": "http://arxiv.org/abs/2106.11653",
          "publishedOn": "2021-06-23T01:48:39.881Z",
          "wordCount": 672,
          "title": "Give Me Your Trained Model: Domain Adaptive Semantic Segmentation without Source Data. (arXiv:2106.11653v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective input transformation based adversarial\ndefense method against gray- and black-box attack, which is computationally\nefficient and does not require any adversarial training or retraining of a\nclassification model. We first show that a very simple iterative Gaussian\nsmoothing can effectively wash out adversarial noise and achieve substantially\nhigh robust accuracy. Based on the observation, we propose Self-Supervised\nIterative Contextual Smoothing (SSICS), which aims to reconstruct the original\ndiscriminative features from the Gaussian-smoothed image in context-adaptive\nmanner, while still smoothing out the adversarial noise. From the experiments\non ImageNet, we show that our SSICS achieves both high standard accuracy and\nvery competitive robust accuracy for the gray- and black-box attacks; e.g.,\ntransfer-based PGD-attack and score-based attack. A note-worthy point to stress\nis that our defense is free of computationally expensive adversarial training,\nyet, can approach its robust accuracy via input transformation.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2021-06-23T01:48:39.873Z",
          "wordCount": 599,
          "title": "Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Aadarsh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1\">Anita Mahadevan-Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyska_M/0/1/0/all/0/1\">Matthew J.Tyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millis_B/0/1/0/all/0/1\">Bryan A. Millis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>",
          "description": "Recent advances in bioimaging have provided scientists a superior high\nspatial-temporal resolution to observe dynamics of living cells as 3D\nvolumetric videos. Unfortunately, the 3D biomedical video analysis is lagging,\nimpeded by resource insensitive human curation using off-the-shelf 3D analytic\ntools. Herein, biologists often need to discard a considerable amount of rich\n3D spatial information by compromising on 2D analysis via maximum intensity\nprojection. Recently, pixel embedding-based cell instance segmentation and\ntracking provided a neat and generalizable computing paradigm for understanding\ncellular dynamics. In this work, we propose a novel spatial-temporal\nvoxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell\ninstance segmenting and tracking on 3D volumetric video sequences. Our\ncontribution is in four-fold: (1) The proposed voxel embedding generalizes the\npixel embedding with 3D context information; (2) Present a simple multi-stream\nlearning approach that allows effective spatial-temporal embedding; (3)\nAccomplished an end-to-end framework for one-stage 3D cell instance\nsegmentation and tracking without heavy parameter tuning; (4) The proposed 3D\nquantification is memory efficient via a single GPU with 12 GB memory. We\nevaluate our VoxelEmbed method on four 3D datasets (with different cell types)\nfrom the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved\nconsistent superior overall performance (OP) on two densely annotated datasets.\nThe performance is also competitive on two sparsely annotated cohorts with\n20.6% and 2% of data-set having segmentation annotations. The results\ndemonstrate that the VoxelEmbed method is a generalizable and memory-efficient\nsolution.",
          "link": "http://arxiv.org/abs/2106.11480",
          "publishedOn": "2021-06-23T01:48:39.865Z",
          "wordCount": 700,
          "title": "VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning. (arXiv:2106.11480v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1903.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1\">Jan-Henrik Lambrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",
          "link": "http://arxiv.org/abs/1903.12561",
          "publishedOn": "2021-06-23T01:48:39.858Z",
          "wordCount": 666,
          "title": "Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1\">Ronja M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1\">Sebastiano Battiato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1\">Aki H&#xe4;rm&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>",
          "description": "Intelligent systems are increasingly part of our everyday lives and have been\nintegrated seamlessly to the point where it is difficult to imagine a world\nwithout them. Physical manifestations of those systems on the other hand, in\nthe form of embodied agents or robots, have so far been used only for specific\napplications and are often limited to functional roles (e.g. in the industry,\nentertainment and military fields). Given the current growth and innovation in\nthe research communities concerned with the topics of robot navigation,\nhuman-robot-interaction and human activity recognition, it seems like this\nmight soon change. Robots are increasingly easy to obtain and use and the\nacceptance of them in general is growing. However, the design of a socially\ncompliant robot that can function as a companion needs to take various areas of\nresearch into account. This paper is concerned with the navigation aspect of a\nsocially-compliant robot and provides a survey of existing solutions for the\nrelevant areas of research as well as an outlook on possible future directions.",
          "link": "http://arxiv.org/abs/2106.11650",
          "publishedOn": "2021-06-23T01:48:39.850Z",
          "wordCount": 611,
          "title": "A Survey on Human-aware Robot Navigation. (arXiv:2106.11650v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girbau_A/0/1/0/all/0/1\">Andreu Girbau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rius_I/0/1/0/all/0/1\">Ignasi Rius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_F/0/1/0/all/0/1\">Ferran Marqu&#xe9;s</a>",
          "description": "Multiple object tracking faces several challenges that may be alleviated with\ntrajectory information. Knowing the posterior locations of an object helps\ndisambiguating and solving situations such as occlusions, re-identification,\nand identity switching. In this work, we show that trajectory estimation can\nbecome a key factor for tracking, and present TrajE, a trajectory estimator\nbased on recurrent mixture density networks, as a generic module that can be\nadded to existing object trackers. To provide several trajectory hypotheses,\nour method uses beam search. Also, relying on the same estimated trajectory, we\npropose to reconstruct a track after an occlusion occurs. We integrate TrajE\ninto two state of the art tracking algorithms, CenterTrack [63] and Tracktor\n[3]. Their respective performances in the MOTChallenge 2017 test set are\nboosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a\nnew state of the art for the CenterTrack+TrajE configuration",
          "link": "http://arxiv.org/abs/2106.10950",
          "publishedOn": "2021-06-23T01:48:39.843Z",
          "wordCount": 616,
          "title": "Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation. (arXiv:2106.10950v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>",
          "description": "Image reconstruction is likely the most predominant auxiliary task for image\nclassification. In this paper, we investigate ``estimating the Fourier\nTransform of the input image\" as a potential alternative auxiliary task, in the\nhope that it may further boost the performances on the primary task or\nintroduce novel constraints not well covered by image reconstruction. We\nexperimented with five popular classification architectures on the CIFAR-10\ndataset, and the empirical results indicated that our proposed auxiliary task\ngenerally improves the classification accuracy. More notably, the results\nshowed that in certain cases our proposed auxiliary task may enhance the\nclassifiers' resistance to adversarial attacks generated using the fast\ngradient sign method.",
          "link": "http://arxiv.org/abs/2106.11478",
          "publishedOn": "2021-06-23T01:48:39.835Z",
          "wordCount": 563,
          "title": "An Alternative Auxiliary Task for Enhancing Image Classification. (arXiv:2106.11478v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>",
          "description": "Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.",
          "link": "http://arxiv.org/abs/2106.06804",
          "publishedOn": "2021-06-23T01:48:39.808Z",
          "wordCount": 622,
          "title": "Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>",
          "description": "Recent work attempts to improve semantic segmentation performance by\nexploring well-designed architectures on a target dataset. However, it remains\nchallenging to build a unified system that simultaneously learns from various\ndatasets due to the inherent distribution shift across different datasets. In\nthis paper, we present a simple, flexible, and general method for semantic\nsegmentation, termed Cross-Dataset Collaborative Learning (CDCL). Given\nmultiple labeled datasets, we aim to improve the generalization and\ndiscrimination of feature representations on each dataset. Specifically, we\nfirst introduce a family of Dataset-Aware Blocks (DAB) as the fundamental\ncomputing units of the network, which help capture homogeneous representations\nand heterogeneous statistics across different datasets. Second, we propose a\nDataset Alternation Training (DAT) mechanism to efficiently facilitate the\noptimization procedure. We conduct extensive evaluations on four diverse\ndatasets, i.e., Cityscapes, BDD100K, CamVid, and COCO Stuff, with\nsingle-dataset and cross-dataset settings. Experimental results demonstrate our\nmethod consistently achieves notable improvements over prior single-dataset and\ncross-dataset training methods without introducing extra FLOPs. Particularly,\nwith the same architecture of PSPNet (ResNet-18), our method outperforms the\nsingle-dataset baseline by 5.65\\%, 6.57\\%, and 5.79\\% of mIoU on the validation\nsets of Cityscapes, BDD100K, CamVid, respectively. Code and models will be\nreleased.",
          "link": "http://arxiv.org/abs/2103.11351",
          "publishedOn": "2021-06-23T01:48:39.799Z",
          "wordCount": 661,
          "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation. (arXiv:2103.11351v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1\">Marko Mihajlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>",
          "description": "In this paper, we aim to create generalizable and controllable neural signed\ndistance fields (SDFs) that represent clothed humans from monocular depth\nobservations. Recent advances in deep learning, especially neural implicit\nrepresentations, have enabled human shape reconstruction and controllable\navatar generation from different sensor inputs. However, to generate realistic\ncloth deformations from novel input poses, watertight meshes or dense full-body\nscans are usually needed as inputs. Furthermore, due to the difficulty of\neffectively modeling pose-dependent cloth deformations for diverse body shapes\nand cloth types, existing approaches resort to per-subject/cloth-type\noptimization from scratch, which is computationally expensive. In contrast, we\npropose an approach that can quickly generate realistic clothed human avatars,\nrepresented as controllable neural SDFs, given only monocular depth images. We\nachieve this by using meta-learning to learn an initialization of a\nhypernetwork that predicts the parameters of neural SDFs. The hypernetwork is\nconditioned on human poses and represents a clothed neural avatar that deforms\nnon-rigidly according to the input poses. Meanwhile, it is meta-learned to\neffectively incorporate priors of diverse body shapes and cloth types and thus\ncan be much faster to fine-tune, compared to models trained from scratch. We\nqualitatively and quantitatively show that our approach outperforms\nstate-of-the-art approaches that require complete meshes as inputs while our\napproach requires only depth frames as inputs and runs orders of magnitudes\nfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is very\nrobust, being the first to generate avatars with realistic dynamic cloth\ndeformations given as few as 8 monocular depth frames.",
          "link": "http://arxiv.org/abs/2106.11944",
          "publishedOn": "2021-06-23T01:48:39.769Z",
          "wordCount": 702,
          "title": "MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Jan Moros Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1\">Jaap van de Loosdrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>",
          "description": "With the introduction of new regulations in the European Union, the future of\nBeyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the\ncreation of the theBEAST project, which aims to create an autonomous security\ndrone, with focus on those regulations and on safety. This technical paper\ndescribes the first steps of a module within this project, which revolves\naround detecting obstacles so they can be avoided in a fail-safe landing. A\ndeep learning powered object detection method is the subject of our research,\nand various experiments are held to maximize its performance, such as comparing\nvarious data augmentation techniques or YOLOv3 and YOLOv5. According to the\nresults of the experiments, we conclude that although object detection is a\npromising approach to resolve this problem, more volume of data is required for\npotential usage in a real-life application.",
          "link": "http://arxiv.org/abs/2106.11098",
          "publishedOn": "2021-06-23T01:48:39.756Z",
          "wordCount": 606,
          "title": "Obstacle Detection for BVLOS Drones. (arXiv:2106.11098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiahao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>",
          "description": "Contrastive self-supervised learning has largely narrowed the gap to\nsupervised pre-training on ImageNet. However, its success highly relies on the\nobject-centric priors of ImageNet, i.e., different augmented views of the same\nimage correspond to the same object. Such a heavily curated constraint becomes\nimmediately infeasible when pre-trained on more complex scene images with many\nobjects. To overcome this limitation, we introduce Object-level Representation\nLearning (ORL), a new self-supervised learning framework towards scene images.\nOur key insight is to leverage image-level self-supervised pre-training as the\nprior to discover object-level semantic correspondence, thus realizing\nobject-level representation learning from scene images. Extensive experiments\non COCO show that ORL significantly improves the performance of self-supervised\nlearning on scene images, even surpassing supervised ImageNet pre-training on\nseveral downstream tasks. Furthermore, ORL improves the downstream performance\nwhen more unlabeled scene images are available, demonstrating its great\npotential of harnessing unlabeled data in the wild. We hope our approach can\nmotivate future research on more general-purpose unsupervised representation\nlearning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.",
          "link": "http://arxiv.org/abs/2106.11952",
          "publishedOn": "2021-06-23T01:48:39.734Z",
          "wordCount": 608,
          "title": "Unsupervised Object-Level Representation Learning from Scene Images. (arXiv:2106.11952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "As soon as abstract mathematical computations were adapted to computation on\ndigital computers, the problem of efficient representation, manipulation, and\ncommunication of the numerical values in those computations arose. Strongly\nrelated to the problem of numerical representation is the problem of\nquantization: in what manner should a set of continuous real-valued numbers be\ndistributed over a fixed discrete set of numbers to minimize the number of bits\nrequired and also to maximize the accuracy of the attendant computations? This\nperennial problem of quantization is particularly relevant whenever memory\nand/or computational resources are severely restricted, and it has come to the\nforefront in recent years due to the remarkable performance of Neural Network\nmodels in computer vision, natural language processing, and related areas.\nMoving from floating-point representations to low-precision fixed integer\nvalues represented in four bits or less holds the potential to reduce the\nmemory footprint and latency by a factor of 16x; and, in fact, reductions of 4x\nto 8x are often realized in practice in these applications. Thus, it is not\nsurprising that quantization has emerged recently as an important and very\nactive sub-area of research in the efficient implementation of computations\nassociated with Neural Networks. In this article, we survey approaches to the\nproblem of quantizing the numerical values in deep Neural Network computations,\ncovering the advantages/disadvantages of current methods. With this survey and\nits organization, we hope to have presented a useful snapshot of the current\nresearch in quantization for Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in this area.",
          "link": "http://arxiv.org/abs/2103.13630",
          "publishedOn": "2021-06-23T01:48:39.712Z",
          "wordCount": 759,
          "title": "A Survey of Quantization Methods for Efficient Neural Network Inference. (arXiv:2103.13630v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yingying Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiuyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guohui Zheng</a>",
          "description": "Predefined evenly-distributed class centroids (PEDCC) can be widely used in\nmodels and algorithms of pattern classification, such as CNN classifiers,\nclassification autoencoders, clustering, and semi-supervised learning, etc. Its\nbasic idea is to predefine the class centers, which are evenly-distributed on\nthe unit hypersphere in feature space, to maximize the inter-class distance.\nThe previous method of generating PEDCC uses an iterative algorithm based on a\ncharge model, that is, the initial values of various centers (charge positions)\nare randomly set from the normal distribution, and the charge positions are\nupdated iteratively with the help of the repulsive force between charges of the\nsame polarity. The class centers generated by the algorithm will produce some\nerrors with the theoretically evenly-distributed points, and the generation\ntime will be longer. This paper takes advantage of regular polyhedron in\nhigh-dimensional space and the evenly distribution of points on the n\ndimensional hypersphere to generate PEDCC mathematically. Then, we discussed\nthe basic and extensive characteristics of the frames formed by PEDCC. Finally,\nexperiments show that new algorithm is not only faster than the iterative\nmethod, but also more accurate in position. The mathematical analysis and\nexperimental results of this paper can provide a theoretical tool for using\nPEDCC to solve the key problems in the field of pattern recognition, such as\ninterpretable supervised/unsupervised learning, incremental learning,\nuncertainty analysis and so on.",
          "link": "http://arxiv.org/abs/2105.00401",
          "publishedOn": "2021-06-23T01:48:39.704Z",
          "wordCount": 695,
          "title": "Generation and frame characteristics of predefined evenly-distributed class centroids for pattern classification. (arXiv:2105.00401v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features.\nThis is especially important when the number of classes per task is small.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-06-23T01:48:39.685Z",
          "wordCount": 618,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>",
          "description": "Multiple object tracking and segmentation requires detecting, tracking, and\nsegmenting objects belonging to a set of given classes. Most approaches only\nexploit the temporal dimension to address the association problem, while\nrelying on single frame predictions for the segmentation mask itself. We\npropose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich\nspatio-temporal information for online multiple object tracking and\nsegmentation. PCAN first distills a space-time memory into a set of prototypes\nand then employs cross-attention to retrieve rich information from the past\nframes. To segment each object, PCAN adopts a prototypical appearance module to\nlearn a set of contrastive foreground and background prototypes, which are then\npropagated over time. Extensive experiments demonstrate that PCAN outperforms\ncurrent video instance tracking and segmentation competition winners on both\nYoutube-VIS and BDD100K datasets, and shows efficacy to both one-stage and\ntwo-stage segmentation frameworks. Code will be available at\nthis http URL",
          "link": "http://arxiv.org/abs/2106.11958",
          "publishedOn": "2021-06-23T01:48:39.676Z",
          "wordCount": 596,
          "title": "Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation. (arXiv:2106.11958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11725",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Franziska Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorli_S/0/1/0/all/0/1\">Suzanne Sorli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1\">Oleksandr Sotnychenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_N/0/1/0/all/0/1\">Neng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1\">Miguel A. Otaduy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Dan Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "Tracking and reconstructing the 3D pose and geometry of two hands in\ninteraction is a challenging problem that has a high relevance for several\nhuman-computer interaction applications, including AR/VR, robotics, or sign\nlanguage recognition. Existing works are either limited to simpler tracking\nsettings (e.g., considering only a single hand or two spatially separated\nhands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,\nin this work we present the first real-time method for motion capture of\nskeletal pose and 3D surface geometry of hands from a single RGB camera that\nexplicitly considers close interactions. In order to address the inherent depth\nambiguities in RGB data, we propose a novel multi-task CNN that regresses\nmultiple complementary pieces of information, including segmentation, dense\nmatchings to a 3D hand model, and 2D keypoint positions, together with newly\nproposed intra-hand relative depth and inter-hand distance maps. These\npredictions are subsequently used in a generative model fitting framework in\norder to estimate pose and shape parameters of a 3D hand model for both hands.\nWe experimentally verify the individual components of our RGB two-hand tracking\nand 3D reconstruction pipeline through an extensive ablation study. Moreover,\nwe demonstrate that our approach offers previously unseen two-hand tracking\nperformance from RGB, and quantitatively and qualitatively outperforms existing\nRGB-based methods that were not explicitly designed for two-hand interactions.\nMoreover, our method even performs on-par with depth-based real-time methods.",
          "link": "http://arxiv.org/abs/2106.11725",
          "publishedOn": "2021-06-23T01:48:39.669Z",
          "wordCount": 699,
          "title": "RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. (arXiv:2106.11725v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n\nMethod: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n\nResults: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n\nConclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.",
          "link": "http://arxiv.org/abs/2105.07467",
          "publishedOn": "2021-06-23T01:48:39.662Z",
          "wordCount": 745,
          "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1\">Fujun Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhao Dong</a>",
          "description": "Reconstructing the shape and appearance of real-world objects using measured\n2D images has been a long-standing problem in computer vision. In this paper,\nwe introduce a new analysis-by-synthesis technique capable of producing\nhigh-quality reconstructions through robust coarse-to-fine optimization and\nphysics-based differentiable rendering.\n\nUnlike most previous methods that handle geometry and reflectance largely\nseparately, our method unifies the optimization of both by leveraging image\ngradients with respect to both object reflectance and geometry. To obtain\nphysically accurate gradient estimates, we develop a new GPU-based Monte Carlo\ndifferentiable renderer leveraging recent advances in differentiable rendering\ntheory to offer unbiased gradients while enjoying better performance than\nexisting tools like PyTorch3D and redner. To further improve robustness, we\nutilize several shape and material priors as well as a coarse-to-fine\noptimization strategy to reconstruct geometry. We demonstrate that our\ntechnique can produce reconstructions with higher quality than previous methods\nsuch as COLMAP and Kinect Fusion.",
          "link": "http://arxiv.org/abs/2103.15208",
          "publishedOn": "2021-06-23T01:48:39.655Z",
          "wordCount": 618,
          "title": "Unified Shape and SVBRDF Recovery using Differentiable Monte Carlo Rendering. (arXiv:2103.15208v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08533",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Simoes_M/0/1/0/all/0/1\">Miguel Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1\">Andreas Themelis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1\">Panagiotis Patrinos</a>",
          "description": "In large-scale optimization, the presence of nonsmooth and nonconvex terms in\na given problem typically makes it hard to solve. A popular approach to address\nnonsmooth terms in convex optimization is to approximate them with their\nrespective Moreau envelopes. In this work, we study the use of Lasry-Lions\ndouble envelopes to approximate nonsmooth terms that are also not convex. These\nenvelopes are an extension of the Moreau ones but exhibit an additional\nsmoothness property that makes them amenable to fast optimization algorithms.\nLasry-Lions envelopes can also be seen as an \"intermediate\" between a given\nfunction and its convex envelope, and we make use of this property to develop a\nmethod that builds a sequence of approximate subproblems that are easier to\nsolve than the original problem. We discuss convergence properties of this\nmethod when used to address composite minimization problems; additionally,\nbased on a number of experiments, we discuss settings where it may be more\nuseful than classical alternatives in two domains: signal decoding and spectral\nunmixing.",
          "link": "http://arxiv.org/abs/2103.08533",
          "publishedOn": "2021-06-23T01:48:39.633Z",
          "wordCount": 646,
          "title": "Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach. (arXiv:2103.08533v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.",
          "link": "http://arxiv.org/abs/2104.08760",
          "publishedOn": "2021-06-23T01:48:39.626Z",
          "wordCount": 739,
          "title": "Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xichan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Non-linear (large) time warping is a challenging source of nuisance in\ntime-series analysis. In this paper, we propose a novel diffeomorphic temporal\ntransformer network for both pairwise and joint time-series alignment. Our\nResNet-TW (Deep Residual Network for Time Warping) tackles the alignment\nproblem by compositing a flow of incremental diffeomorphic mappings. Governed\nby the flow equation, our Residual Network (ResNet) builds smooth, fluid and\nregular flows of velocity fields and consequently generates smooth and\ninvertible transformations (i.e. diffeomorphic warping functions). Inspired by\nthe elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,\nthe final transformation is built by the flow of time-dependent vector fields\nwhich are none other than the building blocks of our Residual Network. The\nlatter is naturally viewed as an Eulerian discretization schema of the flow\nequation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single\ninexpensive forward pass. As we show in experiments on both univariate (84\ndatasets from UCR archive) and multivariate time-series (MSR Action-3D,\nFlorence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance\nin joint alignment and classification.",
          "link": "http://arxiv.org/abs/2106.11911",
          "publishedOn": "2021-06-23T01:48:39.619Z",
          "wordCount": 625,
          "title": "Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment. (arXiv:2106.11911v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references\nfor the VIS community.",
          "link": "http://arxiv.org/abs/2106.11963",
          "publishedOn": "2021-06-23T01:48:39.612Z",
          "wordCount": 599,
          "title": "Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kelu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>",
          "description": "Adversarial training is one of the most effective approaches to improve model\nrobustness against adversarial examples. However, previous works mainly focus\non the overall robustness of the model, and the in-depth analysis on the role\nof each class involved in adversarial training is still missing. In this paper,\nwe propose to analyze the class-wise robustness in adversarial training. First,\nwe provide a detailed diagnosis of adversarial training on six benchmark\ndatasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.\nSurprisingly, we find that there are remarkable robustness discrepancies among\nclasses, leading to unbalance/unfair class-wise robustness in the robust\nmodels. Furthermore, we keep investigating the relations between classes and\nfind that the unbalanced class-wise robustness is pretty consistent among\ndifferent attack and defense methods. Moreover, we observe that the stronger\nattack methods in adversarial learning achieve performance improvement mainly\nfrom a more successful attack on the vulnerable classes (i.e., classes with\nless robustness). Inspired by these interesting findings, we design a simple\nbut effective attack method based on the traditional PGD attack, named\nTemperature-PGD attack, which proposes to enlarge the robustness disparity\namong classes with a temperature factor on the confidence distribution of each\nimage. Experiments demonstrate our method can achieve a higher attack rate than\nthe PGD attack. Furthermore, from the defense perspective, we also make some\nmodifications in the training and inference phases to improve the robustness of\nthe most vulnerable class, so as to mitigate the large difference in class-wise\nrobustness. We believe our work can contribute to a more comprehensive\nunderstanding of adversarial training as well as rethinking the class-wise\nproperties in robust models.",
          "link": "http://arxiv.org/abs/2105.14240",
          "publishedOn": "2021-06-23T01:48:39.593Z",
          "wordCount": 736,
          "title": "Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazhar_O/0/1/0/all/0/1\">Osama Mazhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babuska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>",
          "description": "Deep neural networks designed for vision tasks are often prone to failure\nwhen they encounter environmental conditions not covered by the training data.\nSingle-modal strategies are insufficient when the sensor fails to acquire\ninformation due to malfunction or its design limitations. Multi-sensor\nconfigurations are known to provide redundancy, increase reliability, and are\ncrucial in achieving robustness against asymmetric sensor failures. To address\nthe issue of changing lighting conditions and asymmetric sensor degradation in\nobject detection, we develop a multi-modal 2D object detector, and propose\ndeterministic and stochastic sensor-aware feature fusion strategies. The\nproposed fusion mechanisms are driven by the estimated sensor measurement\nreliability values/weights. Reliable object detection in harsh lighting\nconditions is essential for applications such as self-driving vehicles and\nhuman-robot interaction. We also propose a new \"r-blended\" hybrid depth\nmodality for RGB-D sensors. Through extensive experimentation, we show that the\nproposed strategies outperform the existing state-of-the-art methods on the\nFLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We\nadditionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and\ndemonstrate that the proposed object detection methodologies are highly\neffective for a variety of lighting conditions.",
          "link": "http://arxiv.org/abs/2102.12319",
          "publishedOn": "2021-06-23T01:48:39.584Z",
          "wordCount": 678,
          "title": "GEM: Glare or Gloom, I Can Still See You -- End-to-End Multimodal Object Detection. (arXiv:2102.12319v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hau Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jia-Hong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yao-Chih Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Ching-Hsien Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia-Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>",
          "description": "This paper introduces an approach for multi-human 3D pose estimation and\ntracking based on calibrated multi-view. The main challenge lies in finding the\ncross-view and temporal correspondences correctly even when several human pose\nestimations are noisy. Compare to previous solutions that construct 3D poses\nfrom multiple views, our approach takes advantage of temporal consistency to\nmatch the 2D poses estimated with previously constructed 3D skeletons in every\nview. Therefore cross-view and temporal associations are accomplished\nsimultaneously. Since the performance suffers from mistaken association and\nnoisy predictions, we design two strategies for aiming better correspondences\nand 3D reconstruction. Specifically, we propose a part-aware measurement for\n2D-3D association and a filter that can cope with 2D outliers during\nreconstruction. Our approach is efficient and effective comparing to\nstate-of-the-art methods; it achieves competitive results on two benchmarks:\n96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus\nevaluation frames to be more challenging and our proposal also reach\nwell-performed result.",
          "link": "http://arxiv.org/abs/2106.11589",
          "publishedOn": "2021-06-23T01:48:39.577Z",
          "wordCount": 619,
          "title": "Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking. (arXiv:2106.11589v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jihan Zhang</a>",
          "description": "Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2106.11769",
          "publishedOn": "2021-06-23T01:48:39.570Z",
          "wordCount": 633,
          "title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11731",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1\">Andr&#xe9;s Mart&#xed;nez Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>",
          "description": "UK Biobank (UKB) is conducting a large-scale study of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging furthermore targets\n100,000 subjects, with 70,000 follow-up sessions, enabling measurements of\norgans, muscle, and body composition. With up to 170,000 mounting MR images,\nvarious methodologies are accordingly engaged in large-scale image analysis.\nThis work presents an experimental inference engine that can automatically\npredict a comprehensive profile of subject metadata from UKB neck-to-knee body\nMRI. In cross-validation, it accurately inferred baseline characteristics such\nas age, height, weight, and sex, but also emulated measurements of body\ncomposition by DXA, organ volumes, and abstract properties like grip strength,\npulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can\nautomatically analyze thousands of subjects within hours and provide individual\nconfidence intervals. The underlying methodology is based on convolutional\nneural networks for image-based mean-variance regression on two-dimensional\nrepresentations of the MRI data. This work aims to make the proposed system\navailable for free to researchers, who can use it to obtain fast and\nfully-automated estimates of 72 different measurements immediately upon release\nof new UK Biobank image data.",
          "link": "http://arxiv.org/abs/2106.11731",
          "publishedOn": "2021-06-23T01:48:39.561Z",
          "wordCount": 650,
          "title": "MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-06-23T01:48:39.554Z",
          "wordCount": 722,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yinghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqiang Liu</a>",
          "description": "Recent researches show that deep learning model is susceptible to backdoor\nattacks. Many defenses against backdoor attacks have been proposed. However,\nexisting defense works require high computational overhead or backdoor attack\ninformation such as the trigger size, which is difficult to satisfy in\nrealistic scenarios. In this paper, a novel backdoor detection method based on\nadversarial examples is proposed. The proposed method leverages intentional\nadversarial perturbations to detect whether an image contains a trigger, which\ncan be applied in both the training stage and the inference stage (sanitize the\ntraining set in training stage and detect the backdoor instances in inference\nstage). Specifically, given an untrusted image, the adversarial perturbation is\nadded to the image intentionally. If the prediction of the model on the\nperturbed image is consistent with that on the unperturbed image, the input\nimage will be considered as a backdoor instance. Compared with most existing\ndefense works, the proposed adversarial perturbation based method requires low\ncomputational resources and maintains the visual quality of the images.\nExperimental results show that, the backdoor detection rate of the proposed\ndefense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and\nGTSRB datasets, respectively. Besides, the proposed method maintains the visual\nquality of the image as the l2 norm of the added perturbation are as low as\n2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets,\nrespectively. In addition, it is also demonstrated that the proposed method can\nachieve high defense performance against backdoor attacks under different\nattack settings (trigger transparency, trigger size and trigger pattern).\nCompared with the existing defense work (STRIP), the proposed method has better\ndetection performance on all the three datasets, and is more efficient than\nSTRIP.",
          "link": "http://arxiv.org/abs/2105.14259",
          "publishedOn": "2021-06-23T01:48:39.531Z",
          "wordCount": 746,
          "title": "Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. (arXiv:2105.14259v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1\">Henri Tomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1\">Marcus Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1\">Raimarc Dionido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1\">Mark Ty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1\">Jonric Mirando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1\">Joel Casimiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1\">Richard Guinto</a>",
          "description": "One of the most fundamental and information-laden actions humans do is to\nlook at objects. However, a survey of current works reveals that existing\ngaze-related datasets annotate only the pixel being looked at, and not the\nboundaries of a specific object of interest. This lack of object annotation\npresents an opportunity for further advancing gaze estimation research. To this\nend, we present a challenging new task called gaze object prediction, where the\ngoal is to predict a bounding box for a person's gazed-at object. To train and\nevaluate gaze networks on this task, we present the Gaze On Objects (GOO)\ndataset. GOO is composed of a large set of synthetic images (GOO Synth)\nsupplemented by a smaller subset of real images (GOO-Real) of people looking at\nobjects in a retail environment. Our work establishes extensive baselines on\nGOO by re-implementing and evaluating selected state-of-the art models on the\ntask of gaze following and domain adaptation. Code is available on github.",
          "link": "http://arxiv.org/abs/2105.10793",
          "publishedOn": "2021-06-23T01:48:39.522Z",
          "wordCount": 650,
          "title": "GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1\">Nicole Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>",
          "description": "Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.",
          "link": "http://arxiv.org/abs/2101.11453",
          "publishedOn": "2021-06-23T01:48:39.515Z",
          "wordCount": 634,
          "title": "Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1\">Taihe Yi</a>",
          "description": "Probabilistic Face Embeddings (PFE) can improve face recognition performance\nin unconstrained scenarios by integrating data uncertainty into the feature\nrepresentation. However, existing PFE methods tend to be over-confident in\nestimating uncertainty and is too slow to apply to large-scale face matching.\nThis paper proposes a regularized probabilistic face embedding method to\nimprove the robustness and speed of PFE. Specifically, the mutual likelihood\nscore (MLS) metric used in PFE is simplified to speedup the matching of face\nfeature pairs. Then, an output-constraint loss is proposed to penalize the\nvariance of the uncertainty output, which can regularize the output of the\nneural network. In addition, an identification preserving loss is proposed to\nimprove the discriminative of the MLS metric, and a multi-layer feature fusion\nmodule is proposed to improve the neural network's uncertainty estimation\nability. Comprehensive experiments show that the proposed method can achieve\ncomparable or better results in 9 benchmarks than the state-of-the-art methods,\nand can improve the performance of risk-controlled face recognition. The code\nof our work is publicly available in GitHub\n(https://github.com/KaenChan/ProbFace).",
          "link": "http://arxiv.org/abs/2102.04075",
          "publishedOn": "2021-06-23T01:48:39.508Z",
          "wordCount": 650,
          "title": "Fast and Reliable Probabilistic Face Embeddings in the Wild. (arXiv:2102.04075v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1\">Francis Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1\">Konstantinos Rematas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>",
          "description": "We propose a method to detect and reconstruct multiple 3D objects from a\nsingle RGB image. The key idea is to optimize for detection, alignment and\nshape jointly over all objects in the RGB image, while focusing on realistic\nand physically plausible reconstructions. To this end, we propose a keypoint\ndetector that localizes objects as center points and directly predicts all\nobject properties, including 9-DoF bounding boxes and 3D shapes -- all in a\nsingle forward pass. The proposed method formulates 3D shape reconstruction as\na shape selection problem, i.e. it selects among exemplar shapes from a given\ndatabase. This makes it agnostic to shape representations, which enables a\nlightweight reconstruction of realistic and visually-pleasing shapes based on\nCAD-models, while the training objective is formulated around point clouds and\nvoxel representations. A collision-loss promotes non-intersecting objects,\nfurther increasing the reconstruction realism. Given the RGB image, the\npresented approach performs lightweight reconstruction in a single-stage, it is\nreal-time capable, fully differentiable and end-to-end trainable. Our\nexperiments compare multiple approaches for 9-DoF bounding box estimation,\nevaluate the novel shape-selection mechanism and compare to recent methods in\nterms of 3D bounding box estimation and 3D shape reconstruction quality.",
          "link": "http://arxiv.org/abs/2012.11575",
          "publishedOn": "2021-06-23T01:48:39.499Z",
          "wordCount": 672,
          "title": "From Points to Multi-Object 3D Reconstruction. (arXiv:2012.11575v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Medical imaging plays a pivotal role in diagnosis and treatment in clinical\npractice. Inspired by the significant progress in automatic image captioning,\nvarious deep learning (DL)-based architectures have been proposed for\ngenerating radiology reports for medical images. However, model uncertainty\n(i.e., model reliability/confidence on report generation) is still an\nunder-explored problem. In this paper, we propose a novel method to explicitly\nquantify both the visual uncertainty and the textual uncertainty for the task\nof radiology report generation. Such multi-modal uncertainties can sufficiently\ncapture the model confidence scores at both the report-level and the\nsentence-level, and thus they are further leveraged to weight the losses for\nachieving more comprehensive model optimization. Our experimental results have\ndemonstrated that our proposed method for model uncertainty characterization\nand estimation can provide more reliable confidence scores for radiology report\ngeneration, and our proposed uncertainty-weighted losses can achieve more\ncomprehensive model optimization and result in state-of-the-art performance on\na public radiology report dataset.",
          "link": "http://arxiv.org/abs/2106.10887",
          "publishedOn": "2021-06-23T01:48:39.477Z",
          "wordCount": 611,
          "title": "Confidence-Guided Radiology Report Generation. (arXiv:2106.10887v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computing resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\nuniformly. In detail, AOT employs an identification mechanism to associate\nmultiple targets into the same high-dimensional embedding space. Thus, we can\nsimultaneously process the matching and segmentation decoding of multiple\nobjects as efficiently as processing a single object. For sufficiently modeling\nmulti-object association, a Long Short-Term Transformer is designed for\nconstructing hierarchical matching and propagation. We conduct extensive\nexperiments on both multi-object and single-object benchmarks to examine AOT\nvariant networks with different complexities. Particularly, our AOT-L\noutperforms all the state-of-the-art competitors on three popular benchmarks,\ni.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),\nwhile keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T\ncan maintain real-time multi-object speed on the above benchmarks. We ranked\n1st in the 3rd Large-scale Video Object Segmentation Challenge. The code will\nbe publicly available at https://github.com/z-x-yang/AOT.",
          "link": "http://arxiv.org/abs/2106.02638",
          "publishedOn": "2021-06-23T01:48:39.469Z",
          "wordCount": 667,
          "title": "Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>",
          "description": "Deep neural networks have reached very high accuracy on object detection but\ntheir success hinges on large amounts of labeled data. To reduce the dependency\non labels, various active-learning strategies have been proposed, typically\nbased on the confidence of the detector. However, these methods are biased\ntowards best-performing classes and can lead to acquired datasets that are not\ngood representatives of the data in the testing set. In this work, we propose a\nunified framework for active learning, that considers both the uncertainty and\nthe robustness of the detector, ensuring that the network performs accurately\nin all classes. Furthermore, our method is able to pseudo-label the very\nconfident predictions, suppressing a potential distribution drift while further\nboosting the performance of the model. Experiments show that our method\ncomprehensively outperforms a wide range of active-learning methods on PASCAL\nVOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%\nreduction in labeling cost.",
          "link": "http://arxiv.org/abs/2106.11921",
          "publishedOn": "2021-06-23T01:48:39.461Z",
          "wordCount": 601,
          "title": "Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13028",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>",
          "description": "Recently, the single image super-resolution (SISR) approaches with deep and\ncomplex convolutional neural network structures have achieved promising\nperformance. However, those methods improve the performance at the cost of\nhigher memory consumption, which is difficult to be applied for some mobile\ndevices with limited storage and computing resources. To solve this problem, we\npresent a lightweight multi-scale feature interaction network (MSFIN). For\nlightweight SISR, MSFIN expands the receptive field and adequately exploits the\ninformative features of the low-resolution observed images from various scales\nand interactive connections. In addition, we design a lightweight recurrent\nresidual channel attention block (RRCAB) so that the network can benefit from\nthe channel attention mechanism while being sufficiently lightweight. Extensive\nexperiments on some benchmarks have confirmed that our proposed MSFIN can\nachieve comparable performance against the state-of-the-arts with a more\nlightweight model.",
          "link": "http://arxiv.org/abs/2103.13028",
          "publishedOn": "2021-06-23T01:48:39.454Z",
          "wordCount": 607,
          "title": "Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network. (arXiv:2103.13028v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuanjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to\nrecognize and localize temporal starts and ends of action instances in an\nuntrimmed video with only video-level label supervision. Due to lack of\nnegative samples of background category, it is difficult for the network to\nseparate foreground and background, resulting in poor detection performance. In\nthis report, we present our 2021 HACS Challenge - Weakly-supervised Learning\nTrack solution that based on BaSNet to address above problem. Specifically, we\nfirst adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to\nget feature sequences. Then our proposed Local-Global Background Modeling\nNetwork (LGBM-Net) is trained to localize instances by using only video-level\nlabels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple\nmodels to get the final detection results and reach 22.45% mAP on the test set",
          "link": "http://arxiv.org/abs/2106.11811",
          "publishedOn": "2021-06-23T01:48:39.447Z",
          "wordCount": 594,
          "title": "Weakly-Supervised Temporal Action Localization Through Local-Global Background Modeling. (arXiv:2106.11811v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03515",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1\">Guy Bukchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Eli Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1\">Ori Shahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>",
          "description": "Few-shot learning methods offer pre-training techniques optimized for easier\nlater adaptation of the model to new classes (unseen during training) using one\nor a few examples. This adaptivity to unseen classes is especially important\nfor many practical applications where the pre-trained label space cannot remain\nfixed for effective use and the model needs to be \"specialized\" to support new\ncategories on the fly. One particularly interesting scenario, essentially\noverlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where\nthe training classes (e.g. animals) are of much `coarser granularity' than the\ntarget (test) classes (e.g. breeds). A very practical example of C2FS is when\nthe target classes are sub-classes of the training classes. Intuitively, it is\nespecially challenging as (both regular and few-shot) supervised pre-training\ntends to learn to ignore intra-class variability which is essential for\nseparating sub-classes. In this paper, we introduce a novel 'Angular\nnormalization' module that allows to effectively combine supervised and\nself-supervised contrastive pre-training to approach the proposed C2FS task,\ndemonstrating significant gains in a broad study over multiple baselines and\ndatasets. We hope that this work will help to pave the way for future research\non this new, challenging, and very practical topic of C2FS classification.",
          "link": "http://arxiv.org/abs/2012.03515",
          "publishedOn": "2021-06-23T01:48:39.424Z",
          "wordCount": 678,
          "title": "Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Arif Ul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahmudur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widberg_J/0/1/0/all/0/1\">Jared Q Widberg</a>",
          "description": "With the advancement of deep neural networks and computer vision-based Human\nActivity Recognition, employment of Point-Cloud Data technologies (LiDAR,\nmmWave) has seen a lot interests due to its privacy preserving nature. Given\nthe high promise of accurate PCD technologies, we develop, PALMAR, a\nmultiple-inhabitant activity recognition system by employing efficient signal\nprocessing and novel machine learning techniques to track individual person\ntowards developing an adaptive multi-inhabitant tracking and HAR system. More\nspecifically, we propose (i) a voxelized feature representation-based real-time\nPCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive\nOrder Hidden Markov Model based multi-person tracking and crossover ambiguity\nreduction techniques and (iii) novel adaptive deep learning-based domain\nadaptation technique to improve the accuracy of HAR in presence of data\nscarcity and diversity (device, location and population diversity). We\nexperimentally evaluate our framework and systems using (i) a real-time PCD\ncollected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants,\n(ii) one publicly available 3D LiDAR activity data (28 participants) and (iii)\nan embedded hardware prototype system which provided promising HAR performances\nin multi-inhabitants (96%) scenario with a 63% improvement of multi-person\ntracking than state-of-art framework without losing significant system\nperformances in the edge computing device.",
          "link": "http://arxiv.org/abs/2106.11902",
          "publishedOn": "2021-06-23T01:48:39.416Z",
          "wordCount": 655,
          "title": "PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in Point-Cloud Technology. (arXiv:2106.11902v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.08475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1\">Moab Arar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1\">Dov Danon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>",
          "description": "Traditional image resizing methods usually work in pixel space and use\nvarious saliency measures. The challenge is to adjust the image shape while\ntrying to preserve important content. In this paper we perform image resizing\nin feature space where the deep layers of a neural network contain rich\nimportant semantic information. We directly adjust the image feature maps,\nextracted from a pre-trained classification network, and reconstruct the\nresized image using a neural-network based optimization. This novel approach\nleverages the hierarchical encoding of the network, and in particular, the\nhigh-level discriminative power of its deeper layers, that recognizes semantic\nobjects and regions and allows maintaining their aspect ratio. Our use of\nreconstruction from deep features diminishes the artifacts introduced by\nimage-space resizing operators. We evaluate our method on benchmarks, compare\nto alternative approaches, and demonstrate its strength on challenging images.",
          "link": "http://arxiv.org/abs/1904.08475",
          "publishedOn": "2021-06-23T01:48:39.408Z",
          "wordCount": 607,
          "title": "Image Resizing by Reconstruction from Deep Features. (arXiv:1904.08475v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects in\npoint cloud data. We compare the results with different backbone architectures\nincluding the standard ones like VGG, ResNet, Inception with our backbone. Also\nwe present the optimization and ablation studies including designing an\nefficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking\nand validating our results. Our work surpasses the state of the art in this\ndomain both in terms of average precision and speed running at > 30 FPS. This\nmakes it a feasible option to be deployed in real time applications including\nself driving cars.",
          "link": "http://arxiv.org/abs/2006.01250",
          "publishedOn": "2021-06-23T01:48:39.400Z",
          "wordCount": 684,
          "title": "RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>",
          "description": "Aiming at facilitating a real-world, ever-evolving and scalable autonomous\ndriving system, we present a large-scale benchmark for standardizing the\nevaluation of different self-supervised and semi-supervised approaches by\nlearning from raw data, which is the first and largest benchmark to date.\nExisting autonomous driving systems heavily rely on `perfect' visual perception\nmodels (e.g., detection) trained using extensive annotated data to ensure the\nsafety. However, it is unrealistic to elaborately label instances of all\nscenarios and circumstances (e.g., night, extreme weather, cities) when\ndeploying a robust autonomous driving system. Motivated by recent powerful\nadvances of self-supervised and semi-supervised learning, a promising direction\nis to learn a robust detection model by collaboratively exploiting large-scale\nunlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo)\neither provides only a small amount of data or covers limited domains with full\nannotation, hindering the exploration of large-scale pre-trained models. Here,\nwe release a Large-Scale Object Detection benchmark for Autonomous driving,\nnamed as SODA10M, containing 10 million unlabeled images and 20K images labeled\nwith 6 representative object categories. To improve diversity, the images are\ncollected every ten seconds per frame within 32 different cities under\ndifferent weather conditions, periods and location scenes. We provide extensive\nexperiments and deep analyses of existing supervised state-of-the-art detection\nmodels, popular self-supervised and semi-supervised approaches, and some\ninsights about how to develop future models. The data and more up-to-date\ninformation have been released at https://soda-2d.github.io.",
          "link": "http://arxiv.org/abs/2106.11118",
          "publishedOn": "2021-06-23T01:48:39.391Z",
          "wordCount": 702,
          "title": "SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving. (arXiv:2106.11118v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "Confidence-aware learning is proven as an effective solution to prevent\nnetworks becoming overconfident. We present a confidence-aware camouflaged\nobject detection framework using dynamic supervision to produce both accurate\ncamouflage map and meaningful \"confidence\" representing model awareness about\nthe current prediction. A camouflaged object detection network is designed to\nproduce our camouflage prediction. Then, we concatenate it with the input image\nand feed it to the confidence estimation network to produce an one channel\nconfidence map.We generate dynamic supervision for the confidence estimation\nnetwork, representing the agreement of camouflage prediction with the ground\ntruth camouflage map. With the produced confidence map, we introduce\nconfidence-aware learning with the confidence map as guidance to pay more\nattention to the hard/low-confidence pixels in the loss function. We claim\nthat, once trained, our confidence estimation network can evaluate pixel-wise\naccuracy of the prediction without relying on the ground truth camouflage map.\nExtensive results on four camouflaged object detection testing datasets\nillustrate the superior performance of the proposed model in explaining the\ncamouflage prediction.",
          "link": "http://arxiv.org/abs/2106.11641",
          "publishedOn": "2021-06-23T01:48:39.371Z",
          "wordCount": 598,
          "title": "Confidence-Aware Learning for Camouflaged Object Detection. (arXiv:2106.11641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>",
          "description": "High quality facial image editing is a challenging problem in the movie\npost-production industry, requiring a high degree of control and identity\npreservation. Previous works that attempt to tackle this problem may suffer\nfrom the entanglement of facial attributes and the loss of the person's\nidentity. Furthermore, many algorithms are limited to a certain task. To tackle\nthese limitations, we propose to edit facial attributes via the latent space of\na StyleGAN generator, by training a dedicated latent transformation network and\nincorporating explicit disentanglement and identity preservation terms in the\nloss function. We further introduce a pipeline to generalize our face editing\nto videos. Our model achieves a disentangled, controllable, and\nidentity-preserving facial attribute editing, even in the challenging case of\nreal (i.e., non-synthetic) images and videos. We conduct extensive experiments\non image and video datasets and show that our model outperforms other\nstate-of-the-art methods in visual quality and quantitative evaluation.",
          "link": "http://arxiv.org/abs/2106.11895",
          "publishedOn": "2021-06-23T01:48:39.364Z",
          "wordCount": 589,
          "title": "A Latent Transformer for Disentangled and Identity-Preserving Face Editing. (arXiv:2106.11895v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "Nowadays, analysis of transparent images in the field of computer vision has\ngradually become a hot spot. In this paper, we compare the classification\nperformance of different deep learning for the problem that transparent images\nare difficult to analyze. We crop the transparent images into 8 * 8 and 224 *\n224 pixels patches in the same proportion, and then divide the two different\npixels patches into foreground and background according to groundtruch. We also\nuse 4 types of convolutional neural networks and a novel ViT network model to\ncompare the foreground and background classification experiments. We conclude\nthat ViT performs the worst in classifying 8 * 8 pixels patches, but it\noutperforms most convolutional neural networks in classifying 224 * 224.",
          "link": "http://arxiv.org/abs/2106.11582",
          "publishedOn": "2021-06-23T01:48:39.357Z",
          "wordCount": 584,
          "title": "A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising image regions,\nin each acquisition step. The problem is framed in an exploration-exploitation\nframework by combining an embedding based on Uniform Manifold Approximation to\nmodel representativeness with entropy as uncertainty measure to model\ninformativeness. We applied our proposed method to the challenging autonomous\ndriving data sets CamVid and Cityscapes and performed a quantitative comparison\nwith state-of-the-art methods. We find that our active learning method achieves\nbetter performance on CamVid compared to other methods, while on Cityscapes,\nthe performance lift was negligible.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-06-23T01:48:39.349Z",
          "wordCount": 565,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1\">Orchid Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.",
          "link": "http://arxiv.org/abs/2101.11058",
          "publishedOn": "2021-06-23T01:48:39.341Z",
          "wordCount": 634,
          "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10399",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we present a novel neural network to generate high resolution\nimages. We replace the decoder of VAE with a discriminator while using the\nencoder as it is. The encoder is fed data from a normal distribution while the\ngenerator is fed from a gaussian distribution. The combination from both is\ngiven to a discriminator which tells whether the generated image is correct or\nnot. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA\ndataset. Our network beats the previous state of the art using MMD, SSIM, log\nlikelihood, reconstruction error, ELBO and KL divergence as the evaluation\nmetrics while generating much sharper images. This work is potentially very\nexciting as we are able to combine the advantages of generative models and\ninference models in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.10399",
          "publishedOn": "2021-06-23T01:48:39.320Z",
          "wordCount": 622,
          "title": "Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1\">Benoit Guillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1\">Edoardo Remelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1\">Artem Lukoianov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Stephan Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Geometric Deep Learning has recently made striking progress with the advent\nof continuous Deep Implicit Fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is unlimited in\nresolution. Unfortunately, these methods are often unsuitable for applications\nthat require an explicit mesh-based surface representation because converting\nan implicit field to such a representation relies on the Marching Cubes\nalgorithm, which cannot be differentiated with respect to the underlying\nimplicit field. In this work, we remove this limitation and introduce a\ndifferentiable way to produce explicit surface mesh representations from Deep\nImplicit Fields. Our key insight is that by reasoning on how implicit field\nperturbations impact local surface geometry, one can ultimately differentiate\nthe 3D location of surface samples with respect to the underlying deep implicit\nfield. We exploit this to define DeepMesh -- end-to-end differentiable mesh\nrepresentation that can vary its topology. We use two different applications to\nvalidate our theoretical insight: Single view 3D Reconstruction via\nDifferentiable Rendering and Physically-Driven Shape Optimization. In both\ncases our end-to-end differentiable parameterization gives us an edge over\nstate-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2106.11795",
          "publishedOn": "2021-06-23T01:48:39.313Z",
          "wordCount": 636,
          "title": "DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shixing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Pruning is an effective method to reduce the memory footprint and FLOPs\nassociated with neural network models. However, existing structured-pruning\nmethods often result in significant accuracy degradation for moderate pruning\nlevels. To address this problem, we introduce a new Hessian Aware Pruning (HAP)\nmethod coupled with a Neural Implant approach that uses second-order\nsensitivity as a metric for structured pruning. The basic idea is to prune\ninsensitive components and to use a Neural Implant for moderately sensitive\ncomponents, instead of completely pruning them. For the latter approach, the\nmoderately sensitive components are replaced with with a low rank implant that\nis smaller and less computationally expensive than the original component. We\nuse the relative Hessian trace to measure sensitivity, as opposed to the\nmagnitude based sensitivity metric commonly used in the literature. We test HAP\nfor both computer vision tasks and natural language tasks, and we achieve new\nstate-of-the-art results. Specifically, HAP achieves less than $0.1\\%$/$0.5\\%$\ndegradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than\n70\\%/50\\% of parameters pruned. Meanwhile, HAP also achieves significantly\nbetter performance (up to 0.8\\% with 60\\% of parameters pruned) as compared to\ngradient based method for head pruning on transformer-based models. The\nframework has been open sourced and available online.",
          "link": "http://arxiv.org/abs/2101.08940",
          "publishedOn": "2021-06-23T01:48:39.305Z",
          "wordCount": 682,
          "title": "Hessian-Aware Pruning and Optimal Neural Implant. (arXiv:2101.08940v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1\">C.V.Krishnakumar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feili Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Henry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yonghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kay Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.",
          "link": "http://arxiv.org/abs/2106.11756",
          "publishedOn": "2021-06-23T01:48:39.295Z",
          "wordCount": 635,
          "title": "Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Quiros_L/0/1/0/all/0/1\">Lorenzo Quir&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_E/0/1/0/all/0/1\">Enrique Vidal</a>",
          "description": "Automatically recognizing the layout of handwritten documents is an important\nstep towards useful extraction of information from those documents. The most\ncommon application is to feed downstream applications such as automatic text\nrecognition and keyword spotting; however, the recognition of the layout also\nhelps to establish relationships between elements in the document which allows\nto enrich the information that can be extracted. Most of the modern document\nlayout analysis systems are designed to address only one part of the document\nlayout problem, namely: baseline detection or region segmentation. In contrast,\nwe evaluate the effectiveness of the Mask-RCNN architecture to address the\nproblem of baseline detection and region segmentation in an integrated manner.\nWe present experimental results on two handwritten text datasets and one\nhandwritten music dataset. The analyzed architecture yields promising results,\noutperforming state-of-the-art techniques in all three datasets.",
          "link": "http://arxiv.org/abs/2106.11797",
          "publishedOn": "2021-06-23T01:48:39.287Z",
          "wordCount": 577,
          "title": "Evaluation of a Region Proposal Architecture for Multi-task Document Layout Analysis. (arXiv:2106.11797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07609",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Monakhova_K/0/1/0/all/0/1\">Kristina Monakhova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Vi Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_G/0/1/0/all/0/1\">Grace Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Waller_L/0/1/0/all/0/1\">Laura Waller</a>",
          "description": "Compressive lensless imagers enable novel applications in an extremely\ncompact device, requiring only a phase or amplitude mask placed close to the\nsensor. They have been demonstrated for 2D and 3D microscopy, single-shot\nvideo, and single-shot hyperspectral imaging; in each of these cases, a\ncompressive-sensing-based inverse problem is solved in order to recover a 3D\ndata-cube from a 2D measurement. Typically, this is accomplished using convex\noptimization and hand-picked priors. Alternatively, deep learning-based\nreconstruction methods offer the promise of better priors, but require many\nthousands of ground truth training pairs, which can be difficult or impossible\nto acquire. In this work, we propose the use of untrained networks for\ncompressive image recovery. Our approach does not require any labeled training\ndata, but instead uses the measurement itself to update the network weights. We\ndemonstrate our untrained approach on lensless compressive 2D imaging as well\nas single-shot high-speed video recovery using the camera's rolling shutter,\nand single-shot hyperspectral imaging. We provide simulation and experimental\nverification, showing that our method results in improved image quality over\nexisting methods.",
          "link": "http://arxiv.org/abs/2103.07609",
          "publishedOn": "2021-06-23T01:48:39.264Z",
          "wordCount": 655,
          "title": "Untrained networks for compressive lensless photography. (arXiv:2103.07609v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1\">Brian D. Davison</a>",
          "description": "Domain adaptation aims to mitigate the domain gap when transferring knowledge\nfrom an existing labeled domain to a new domain. However, existing\ndisentanglement-based methods do not fully consider separation between\ndomain-invariant and domain-specific features, which means the domain-invariant\nfeatures are not discriminative. The reconstructed features are also not\nsufficiently used during training. In this paper, we propose a novel enhanced\nseparable disentanglement (ESD) model. We first employ a disentangler to\ndistill domain-invariant and domain-specific features. Then, we apply feature\nseparation enhancement processes to minimize contamination between\ndomain-invariant and domain-specific features. Finally, our model reconstructs\ncomplete feature vectors, which are used for further disentanglement during the\ntraining phase. Extensive experiments from three benchmark datasets outperform\nstate-of-the-art methods, especially on challenging cross-domain tasks.",
          "link": "http://arxiv.org/abs/2106.11915",
          "publishedOn": "2021-06-23T01:48:39.256Z",
          "wordCount": 554,
          "title": "Enhanced Separable Disentanglement for Unsupervised Domain Adaptation. (arXiv:2106.11915v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1\">Boulbaba Ben Amor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xichan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>",
          "description": "Analyzing the structure of proteins is a key part of understanding their\nfunctions and thus their role in biology at the molecular level. In addition,\ndesign new proteins in a methodical way is a major engineering challenge. In\nthis work, we introduce a joint geometric-neural networks approach for\ncomparing, deforming and generating 3D protein structures. Viewing protein\nstructures as 3D open curves, we adopt the Square Root Velocity Function (SRVF)\nrepresentation and leverage its suitable geometric properties along with Deep\nResidual Networks (ResNets) for a joint registration and comparison. Our\nResNets handle better large protein deformations while being more\ncomputationally efficient. On top of the mathematical framework, we further\ndesign a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps\noriginal, previously unseen structures, into a low-dimensional (latent)\nhyper-sphere. Motivated by the spherical structure of the pre-shape space, we\nnaturally adopt the von Mises-Fisher (vMF) distribution to model our hidden\nvariables. We test the effectiveness of our models by generating novel protein\nstructures and predicting completions of corrupted protein structures.\nExperimental results show that our method is able to generate plausible\nstructures, different from the structures in the training data.",
          "link": "http://arxiv.org/abs/2106.11920",
          "publishedOn": "2021-06-23T01:48:39.242Z",
          "wordCount": 630,
          "title": "G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. (arXiv:2106.11920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:39.232Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional image classifiers are trained by randomly sampling mini-batches\nof images. To achieve state-of-the-art performance, practitioners use\nsophisticated data augmentation schemes to expand the amount of training data\navailable for sampling. In contrast, meta-learning algorithms sample support\ndata, query data, and tasks on each training step. In this complex sampling\nscenario, data augmentation can be used not only to expand the number of images\navailable per class, but also to generate entirely new classes/tasks. We\nsystematically dissect the meta-learning pipeline and investigate the distinct\nways in which data augmentation can be integrated at both the image and class\nlevels. Our proposed meta-specific data augmentation significantly improves the\nperformance of meta-learners on few-shot classification benchmarks.",
          "link": "http://arxiv.org/abs/2010.07092",
          "publishedOn": "2021-06-23T01:48:39.223Z",
          "wordCount": 583,
          "title": "Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>",
          "description": "Semi-supervised learning has been gaining attention as it allows for\nperforming image analysis tasks such as classification with limited labeled\ndata. Some popular algorithms using Generative Adversarial Networks (GANs) for\nsemi-supervised classification share a single architecture for classification\nand discrimination. However, this may require a model to converge to a separate\ndata distribution for each task, which may reduce overall performance. While\nprogress in semi-supervised learning has been made, less addressed are\nsmall-scale, fully-supervised tasks where even unlabeled data is unavailable\nand unattainable. We therefore, propose a novel GAN model namely External\nClassifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to\nimprove classification in fully-supervised regimes. Our method leverages a GAN\nto generate artificial data used to supplement supervised classification. More\nspecifically, we attach an external classifier, hence the name EC-GAN, to the\nGAN's generator, as opposed to sharing an architecture with the discriminator.\nOur experiments demonstrate that EC-GAN's performance is comparable to the\nshared architecture method, far superior to the standard data augmentation and\nregularization-based approach, and effective on a small, realistic dataset.",
          "link": "http://arxiv.org/abs/2012.15864",
          "publishedOn": "2021-06-23T01:48:39.202Z",
          "wordCount": 647,
          "title": "EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "Chinese character recognition has attracted much research interest due to its\nwide applications. Although it has been studied for many years, some issues in\nthis field have not been completely resolved yet, e.g. the zero-shot problem.\nPrevious character-based and radical-based methods have not fundamentally\naddressed the zero-shot problem since some characters or radicals in test sets\nmay not appear in training sets under a data-hungry condition. Inspired by the\nfact that humans can generalize to know how to write characters unseen before\nif they have learned stroke orders of some characters, we propose a\nstroke-based method by decomposing each character into a sequence of strokes,\nwhich are the most basic units of Chinese characters. However, we observe that\nthere is a one-to-many relationship between stroke sequences and Chinese\ncharacters. To tackle this challenge, we employ a matching-based strategy to\ntransform the predicted stroke sequence to a specific character. We evaluate\nthe proposed method on handwritten characters, printed artistic characters, and\nscene characters. The experimental results validate that the proposed method\noutperforms existing methods on both character zero-shot and radical zero-shot\ntasks. Moreover, the proposed method can be easily generalized to other\nlanguages whose characters can be decomposed into strokes.",
          "link": "http://arxiv.org/abs/2106.11613",
          "publishedOn": "2021-06-23T01:48:39.190Z",
          "wordCount": 639,
          "title": "Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition. (arXiv:2106.11613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1901.11259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Yang_L/0/1/0/all/0/1\">Le Ou-Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>",
          "description": "Deep hashing models have been proposed as an efficient method for large-scale\nsimilarity search. However, most existing deep hashing methods only utilize\nfine-level labels for training while ignoring the natural semantic hierarchy\nstructure. This paper presents an effective method that preserves the classwise\nsimilarity of full-level semantic hierarchy for large-scale image retrieval.\nExperiments on two benchmark datasets show that our method helps improve the\nfine-level retrieval performance. Moreover, with the help of the semantic\nhierarchy, it can produce significantly better binary codes for hierarchical\nretrieval, which indicates its potential of providing more user-desired\nretrieval results.",
          "link": "http://arxiv.org/abs/1901.11259",
          "publishedOn": "2021-06-23T01:48:39.176Z",
          "wordCount": 573,
          "title": "Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval. (arXiv:1901.11259v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1\">Tomas Sipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan Sulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "In many computer vision classification tasks, class priors at test time often\ndiffer from priors on the training set. In the case of such prior shift,\nclassifiers must be adapted correspondingly to maintain close to optimal\nperformance. This paper analyzes methods for adaptation of probabilistic\nclassifiers to new priors and for estimating new priors on an unlabeled test\nset. We propose a novel method to address a known issue of prior estimation\nmethods based on confusion matrices, where inconsistent estimates of decision\nprobabilities and confusion matrices lead to negative values in the estimated\npriors. Experiments on fine-grained image classification datasets provide\ninsight into the best practice of prior shift estimation and classifier\nadaptation and show that the proposed method achieves state-of-the-art results\nin prior adaptation. Applying the best practice to two tasks with naturally\nimbalanced priors, learning from web-crawled images and plant species\nclassification, increased the recognition accuracy by 1.1% and 3.4%\nrespectively.",
          "link": "http://arxiv.org/abs/2106.11695",
          "publishedOn": "2021-06-23T01:48:39.170Z",
          "wordCount": 592,
          "title": "The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1\">Li Guanlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1\">Guo Shangwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1\">Wang Run</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1\">Xu Guowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1\">Zhang Tianwei</a>",
          "description": "This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-06-23T01:48:39.163Z",
          "wordCount": 571,
          "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huiwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1\">Koki Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1\">Han-Wei Kung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwhite_M/0/1/0/all/0/1\">Mclean Goldwhite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qingguo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zejian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>",
          "description": "We introduce a highly robust GAN-based framework for digitizing a normalized\n3D avatar of a person from a single unconstrained photo. While the input image\ncan be of a smiling person or taken in extreme lighting conditions, our method\ncan reliably produce a high-quality textured model of a person's face in\nneutral expression and skin textures under diffuse lighting condition.\nCutting-edge 3D face reconstruction methods use non-linear morphable face\nmodels combined with GAN-based decoders to capture the likeness and details of\na person but fail to produce neutral head models with unshaded albedo textures\nwhich is critical for creating relightable and animation-friendly avatars for\nintegration in virtual environments. The key challenges for existing methods to\nwork is the lack of training and ground truth data containing normalized 3D\nfaces. We propose a two-stage approach to address this problem. First, we adopt\na highly robust normalized 3D face generator by embedding a non-linear\nmorphable face model into a StyleGAN2 network. This allows us to generate\ndetailed but normalized facial assets. This inference is then followed by a\nperceptual refinement step that uses the generated assets as regularization to\ncope with the limited available training samples of normalized faces. We\nfurther introduce a Normalized Face Dataset, which consists of a combination\nphotogrammetry scans, carefully selected photographs, and generated fake people\nwith neutral expressions in diffuse lighting conditions. While our prepared\ndataset contains two orders of magnitude less subjects than cutting edge\nGAN-based 3D facial reconstruction methods, we show that it is possible to\nproduce high-quality normalized face models for very challenging unconstrained\ninput images, and demonstrate superior performance to the current\nstate-of-the-art.",
          "link": "http://arxiv.org/abs/2106.11423",
          "publishedOn": "2021-06-23T01:48:39.155Z",
          "wordCount": 723,
          "title": "Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement. (arXiv:2106.11423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>",
          "description": "In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.",
          "link": "http://arxiv.org/abs/2106.11810",
          "publishedOn": "2021-06-23T01:48:39.135Z",
          "wordCount": 619,
          "title": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11536",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liguo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Juntao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinguo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Jinxiang Chai</a>",
          "description": "We introduce an approach that accurately reconstructs 3D human poses and\ndetailed 3D full-body geometric models from single images in realtime. The key\nidea of our approach is a novel end-to-end multi-task deep learning framework\nthat uses single images to predict five outputs simultaneously: foreground\nsegmentation mask, 2D joints positions, semantic body partitions, 3D part\norientations and uv coordinates (uv map). The multi-task network architecture\nnot only generates more visual cues for reconstruction, but also makes each\nindividual prediction more accurate. The CNN regressor is further combined with\nan optimization based algorithm for accurate kinematic pose reconstruction and\nfull-body shape modeling. We show that the realtime reconstruction reaches\naccurate fitting that has not been seen before, especially for wild images. We\ndemonstrate the results of our realtime 3D pose and human body reconstruction\nsystem on various challenging in-the-wild videos. We show the system advances\nthe frontier of 3D human body and pose reconstruction from single images by\nquantitative evaluations and comparisons with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11536",
          "publishedOn": "2021-06-23T01:48:39.127Z",
          "wordCount": 614,
          "title": "Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies from Single RGB Images. (arXiv:2106.11536v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Dietary-related problems such as obesity are a growing concern in todays\nmodern world. If the current trend continues, it is most likely that the\nquality of life, in general, is significantly affected since obesity is\nassociated with other chronic diseases such as hypertension, irregular blood\nsugar levels, and increased risk of heart attacks. The primary cause of these\nproblems is poor lifestyle choices and unhealthy dietary habits, with emphasis\non a select few food groups such as sugars, fats, and carbohydrates. In this\nregard, computer-based food recognition offers automatic visual-based methods\nto assess dietary intake and help people make healthier choices. Thus, the\nfollowing paper presents a brief review of visual-based methods for food\nrecognition, including their accuracy, performance, and the use of popular food\ndatabases to evaluate existing models. The work further aims to highlight\nfuture challenges in this area. New high-quality studies for developing\nstandard benchmarks and using continual learning methods for food recognition\nare recommended.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-06-23T01:48:39.118Z",
          "wordCount": 599,
          "title": "A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11558",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1\">Mohana Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rameshan_R/0/1/0/all/0/1\">Renu M. Rameshan</a>",
          "description": "Light field technology has increasingly attracted the attention of the\nresearch community with its many possible applications. The lenslet array in\ncommercial plenoptic cameras helps capture both the spatial and angular\ninformation of light rays in a single exposure. While the resulting high\ndimensionality of light field data enables its superior capabilities, it also\nimpedes its extensive adoption. Hence, there is a compelling need for efficient\ncompression of light field images. Existing solutions are commonly composed of\nseveral separate modules, some of which may not have been designed for the\nspecific structure and quality of light field data. This increases the\ncomplexity of the codec and results in impractical decoding runtimes. We\npropose a new learning-based, disparity-aided model for compression of 4D light\nfield images capable of parallel decoding. The model is end-to-end trainable,\neliminating the need for hand-tuning separate modules and allowing joint\nlearning of rate and distortion. The disparity-aided approach ensures the\nstructural integrity of the reconstructed light fields. Comparisons with the\nstate of the art show encouraging performance in terms of PSNR and MS-SSIM\nmetrics. Also, there is a notable gain in the encoding and decoding runtimes.\nSource code is available at https://moha23.github.io/LFDAAE.",
          "link": "http://arxiv.org/abs/2106.11558",
          "publishedOn": "2021-06-23T01:48:39.105Z",
          "wordCount": 651,
          "title": "Learning-Based Practical Light Field Image Compression Using A Disparity-Aware Model. (arXiv:2106.11558v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boris_C/0/1/0/all/0/1\">Chidlovskii Boris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1\">Assem Sadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>",
          "description": "We address the problem of universal domain adaptation (UDA) in ordinal\nregression (OR), which attempts to solve classification problems in which\nlabels are not independent, but follow a natural order. We show that the UDA\ntechniques developed for classification and based on the clustering assumption,\nunder-perform in OR settings. We propose a method that complements the OR\nclassifier with an auxiliary task of order learning, which plays the double\nrole of discriminating between common and private instances, and expanding\nclass labels to the private target images via ranking. Combined with\nadversarial domain discrimination, our model is able to address the closed set,\npartial and open set configurations. We evaluate our method on three face age\nestimation datasets, and show that it outperforms the baseline methods.",
          "link": "http://arxiv.org/abs/2106.11576",
          "publishedOn": "2021-06-23T01:48:39.087Z",
          "wordCount": 558,
          "title": "Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Rachala Rohith Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panicker_M/0/1/0/all/0/1\">Mahesh Raveendranatha Panicker</a>",
          "description": "With the recent developments in neural networks, there has been a resurgence\nin algorithms for the automatic generation of simulation ready electronic\ncircuits from hand-drawn circuits. However, most of the approaches in\nliterature were confined to classify different types of electrical components\nand only a few of those methods have shown a way to rebuild the circuit\nschematic from the scanned image, which is extremely important for further\nautomation of netlist generation. This paper proposes a real-time algorithm for\nthe automatic recognition of hand-drawn electrical circuits based on object\ndetection and circuit node recognition. The proposed approach employs You Only\nLook Once version 5 (YOLOv5) for detection of circuit components and a novel\nHough transform based approach for node recognition. Using YOLOv5 object\ndetection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in\ndetecting the components. The proposed method is also able to rebuild the\ncircuit schematic with 80% accuracy.",
          "link": "http://arxiv.org/abs/2106.11559",
          "publishedOn": "2021-06-23T01:48:39.079Z",
          "wordCount": 605,
          "title": "Hand-Drawn Electrical Circuit Recognition using Object Detection and Node Recognition. (arXiv:2106.11559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiwen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hao Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linchuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiao Zheng</a>",
          "description": "Multi-label image classification (MLIC) is a fundamental and practical task,\nwhich aims to assign multiple possible labels to an image. In recent years,\nmany deep convolutional neural network (CNN) based approaches have been\nproposed which model label correlations to discover semantics of labels and\nlearn semantic representations of images. This paper advances this research\ndirection by improving both the modeling of label correlations and the learning\nof semantic representations. On the one hand, besides the local semantics of\neach label, we propose to further explore global semantics shared by multiple\nlabels. On the other hand, existing approaches mainly learn the semantic\nrepresentations at the last convolutional layer of a CNN. But it has been noted\nthat the image representations of different layers of CNN capture different\nlevels or scales of features and have different discriminative abilities. We\nthus propose to learn semantic representations at multiple convolutional\nlayers. To this end, this paper designs a Multi-layered Semantic Representation\nNetwork (MSRN) which discovers both local and global semantics of labels\nthrough modeling label correlations and utilizes the label semantics to guide\nthe semantic representations learning at multiple layers through an attention\nmechanism. Extensive experiments on four benchmark datasets including VOC 2007,\nCOCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN\nagainst state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.11596",
          "publishedOn": "2021-06-23T01:48:39.072Z",
          "wordCount": 656,
          "title": "Multi-layered Semantic Representation Network for Multi-label Image Classification. (arXiv:2106.11596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_N/0/1/0/all/0/1\">Nicholas Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">William Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "High-resolution satellite imagery has proven useful for a broad range of\ntasks, including measurement of global human population, local economic\nlivelihoods, and biodiversity, among many others. Unfortunately,\nhigh-resolution imagery is both infrequently collected and expensive to\npurchase, making it hard to efficiently and effectively scale these downstream\ntasks over both time and space. We propose a new conditional pixel synthesis\nmodel that uses abundant, low-cost, low-resolution imagery to generate accurate\nhigh-resolution imagery at locations and times in which it is unavailable. We\nshow that our model attains photo-realistic sample quality and outperforms\ncompeting baselines on a key downstream task -- object counting -- particularly\nin geographic locations where conditions on the ground are changing rapidly.",
          "link": "http://arxiv.org/abs/2106.11485",
          "publishedOn": "2021-06-23T01:48:39.060Z",
          "wordCount": 565,
          "title": "Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongho Kim</a>",
          "description": "The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.",
          "link": "http://arxiv.org/abs/2106.11528",
          "publishedOn": "2021-06-23T01:48:39.053Z",
          "wordCount": 582,
          "title": "Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:39.046Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>",
          "description": "Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.",
          "link": "http://arxiv.org/abs/2106.11541",
          "publishedOn": "2021-06-23T01:48:39.039Z",
          "wordCount": 686,
          "title": "Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Steven Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "With leveraging the weight-sharing and continuous relaxation to enable\ngradient-descent to alternately optimize the supernet weights and the\narchitecture parameters through a bi-level optimization paradigm,\n\\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream\nmethod in Neural Architecture Search (NAS) due to its simplicity and\nefficiency. However, more recent works found that the performance of the\nsearched architecture barely increases with the optimization proceeding in\nDARTS. In addition, several concurrent works show that the NAS could find more\ncompetitive architectures without labels. The above observations reveal that\nthe supervision signal in DARTS may be a poor indicator for architecture\noptimization, inspiring a foundational question: instead of using the\nsupervision signal to perform bi-level optimization, \\textit{can we find\nhigh-quality architectures \\textbf{without any training nor labels}}? We\nprovide an affirmative answer by customizing the NAS as a network pruning at\ninitialization problem. By leveraging recent techniques on the network pruning\nat initialization, we designed a FreeFlow proxy to score the importance of\ncandidate operations in NAS without any training nor labels, and proposed a\nnovel framework called \\textit{training and label free neural architecture\nsearch} (\\textbf{FreeNAS}) accordingly. We show that, without any training nor\nlabels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS\nbaselines. More importantly, our framework is extremely efficient, which\ncompletes the architecture search within only \\textbf{3.6s} and \\textbf{79s} on\na single GPU for the NAS-Bench-201 and DARTS search space, respectively. We\nhope our work inspires more attempts in solving NAS from the perspective of\npruning at initialization.",
          "link": "http://arxiv.org/abs/2106.11542",
          "publishedOn": "2021-06-23T01:48:39.016Z",
          "wordCount": 693,
          "title": "Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungmin Cha. Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-06-23T01:48:38.963Z",
          "wordCount": 631,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nazaria_K/0/1/0/all/0/1\">Kobra Nazaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_S/0/1/0/all/0/1\">Samaneh Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_B/0/1/0/all/0/1\">Bahram Sadeghi Bigham</a>",
          "description": "Skin color detection is an essential required step in various applications\nrelated to computer vision. These applications will include face detection,\nfinding pornographic images in movies and photos, finding ethnicity, age,\ndiagnosis, and so on. Therefore, proposing a proper skin detection method can\nprovide solution to several problems. In this study, first a new color space is\ncreated using FCM and PSO algorithms. Then, skin classification has been\nperformed in the new color space utilizing linear and nonlinear modes.\nAdditionally, it has been done in RGB and LAB color spaces by using ANFIS and\nneural network. Skin detection in RBG color space has been performed using\nMahalanobis distance and Euclidean distance algorithms. In comparison, this\nmethod has 18.38% higher accuracy than the most accurate method on the same\ndatabase. Additionally, this method has achieved 90.05% in equal error rate\n(1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan\ndataset, which compared to the previous method on COMPAQ database, 1-EER has\nincreased by %0.87.",
          "link": "http://arxiv.org/abs/2106.11563",
          "publishedOn": "2021-06-23T01:48:38.946Z",
          "wordCount": 621,
          "title": "Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS. (arXiv:2106.11563v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasani_B/0/1/0/all/0/1\">Bhavan Jasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Urala Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>",
          "description": "We present DocFormer -- a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).",
          "link": "http://arxiv.org/abs/2106.11539",
          "publishedOn": "2021-06-23T01:48:38.940Z",
          "wordCount": 556,
          "title": "DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khac Chinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_M/0/1/0/all/0/1\">Marc Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meunier_J/0/1/0/all/0/1\">Jean Meunier</a>",
          "description": "Gait analysis is an important aspect of clinical investigation for detecting\nneurological and musculoskeletal disorders and assessing the global health of a\npatient. In this paper we propose to focus our attention on extracting relevant\ncurvature information from the body surface provided by a depth camera. We\nassumed that the 3D mesh was made available in a previous step and demonstrated\nhow curvature maps could be useful to assess asymmetric anomalies with two\nsimple simulated abnormal gaits compared with a normal one. This research set\nthe grounds for the future development of a curvature-based gait analysis\nsystem for healthcare professionals.",
          "link": "http://arxiv.org/abs/2106.11466",
          "publishedOn": "2021-06-23T01:48:38.933Z",
          "wordCount": 541,
          "title": "Gait analysis with curvature maps: A simulation study. (arXiv:2106.11466v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jingni Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushi Zhu</a>",
          "description": "In Argoverse motion forecasting competition, the task is to predict the\nprobabilistic future trajectory distribution for the interested targets in the\ntraffic scene. We use vectorized lane map and 2 s targets' history trajectories\nas input. Then the model outputs 6 forecasted trajectories with probability for\neach target.",
          "link": "http://arxiv.org/abs/2106.11467",
          "publishedOn": "2021-06-23T01:48:38.925Z",
          "wordCount": 481,
          "title": "Multimodal trajectory forecasting based on discrete heat map. (arXiv:2106.11467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmed El-Sallab</a>",
          "description": "Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.",
          "link": "http://arxiv.org/abs/2106.11401",
          "publishedOn": "2021-06-23T01:48:38.895Z",
          "wordCount": 599,
          "title": "Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Amol S. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "While working with fingerprint images acquired from crime scenes, mobile\ncameras, or low-quality sensors, it becomes difficult for automated\nidentification systems to verify the identity due to image blur and distortion.\nWe propose a fingerprint deblurring model FDeblur-GAN, based on the conditional\nGenerative Adversarial Networks (cGANs) and multi-stage framework of the stack\nGAN. Additionally, we integrate two auxiliary sub-networks into the model for\nthe deblurring task. The first sub-network is a ridge extractor model. It is\nadded to generate ridge maps to ensure that fingerprint information and\nminutiae are preserved in the deblurring process and prevent the model from\ngenerating erroneous minutiae. The second sub-network is a verifier that helps\nthe generator to preserve the ID information during the generation process.\nUsing a database of blurred fingerprints and corresponding ridge maps, the deep\nnetwork learns to deblur from the input blurry samples. We evaluate the\nproposed method in combination with two different fingerprint matching\nalgorithms. We achieved an accuracy of 95.18% on our fingerprint database for\nthe task of matching deblurred and ground truth fingerprints.",
          "link": "http://arxiv.org/abs/2106.11354",
          "publishedOn": "2021-06-23T01:48:38.887Z",
          "wordCount": 618,
          "title": "FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network. (arXiv:2106.11354v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mattos_A/0/1/0/all/0/1\">Agatha C. H. de Mattos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McArdle_G/0/1/0/all/0/1\">Gavin McArdle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolotto_M/0/1/0/all/0/1\">Michela Bertolotto</a>",
          "description": "The UN-Habitat estimates that over one billion people live in slums around\nthe world. However, state-of-the-art techniques to detect the location of slum\nareas employ high-resolution satellite imagery, which is costly to obtain and\nprocess. As a result, researchers have started to look at utilising free and\nopen-access medium resolution satellite imagery. Yet, there is no clear\nconsensus on which data preparation and machine learning approaches are the\nmost appropriate to use with such imagery data. In this paper, we evaluate two\ntechniques (multi-spectral data and grey-level co-occurrence matrix feature\nextraction) on an open-access dataset consisting of labelled Sentinel-2 images\nwith a spatial resolution of 10 meters. Both techniques were paired with a\ncanonical correlation forests classifier. The results show that the grey-level\nco-occurrence matrix performed better than multi-spectral data for all four\ncities. It had an average accuracy for the slum class of 97% and a mean\nintersection over union of 94%, while multi-spectral data had 75% and 64% for\nthe respective metrics. These results indicate that open-access satellite\nimagery with a resolution of at least 10 meters may be suitable for keeping\ntrack of development goals such as the detection of slums in cities.",
          "link": "http://arxiv.org/abs/2106.11395",
          "publishedOn": "2021-06-23T01:48:38.880Z",
          "wordCount": 673,
          "title": "Mapping Slums with Medium Resolution Satellite Imagery: a Comparative Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix Techniques. (arXiv:2106.11395v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong Hoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sae-Young Chung</a>",
          "description": "We propose unsupervised embedding adaptation for the downstream few-shot\nclassification task. Based on findings that deep neural networks learn to\ngeneralize before memorizing, we develop Early-Stage Feature Reconstruction\n(ESFR) -- a novel adaptation scheme with feature reconstruction and\ndimensionality-driven early stopping that finds generalizable features.\nIncorporating ESFR consistently improves the performance of baseline methods on\nall standard settings, including the recently proposed transductive method.\nESFR used in conjunction with the transductive method further achieves\nstate-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB;\nespecially with 1.2%~2.0% improvements in accuracy over the previous best\nperforming method on 1-shot setting.",
          "link": "http://arxiv.org/abs/2106.11486",
          "publishedOn": "2021-06-23T01:48:38.872Z",
          "wordCount": 539,
          "title": "Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification. (arXiv:2106.11486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "Unsupervised domain adaptation is used in many machine learning applications\nwhere, during training, a model has access to unlabeled data in the target\ndomain, and a related labeled dataset. In this paper, we introduce a novel and\ngeneral domain-adversarial framework. Specifically, we derive a novel\ngeneralization bound for domain adaptation that exploits a new measure of\ndiscrepancy between distributions based on a variational characterization of\nf-divergences. It recovers the theoretical results from Ben-David et al.\n(2010a) as a special case and supports divergences used in practice. Based on\nthis bound, we derive a new algorithmic framework that introduces a key\ncorrection in the original adversarial training method of Ganin et al. (2016).\nWe show that many regularizers and ad-hoc objectives introduced over the last\nyears in this framework are then not required to achieve performance comparable\nto (if not better than) state-of-the-art domain-adversarial methods.\nExperimental analysis conducted on real-world natural language and computer\nvision datasets show that our framework outperforms existing baselines, and\nobtains the best results for f-divergences that were not considered previously\nin domain-adversarial learning.",
          "link": "http://arxiv.org/abs/2106.11344",
          "publishedOn": "2021-06-23T01:48:38.801Z",
          "wordCount": 616,
          "title": "f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:38.772Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1\">Freddie Bickford Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1\">Brett D Roads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C Love</a>",
          "description": "Top-down attention allows neural networks, both artificial and biological, to\nfocus on the information most relevant for a given task. This is known to\nenhance performance in visual perception. But it remains unclear how attention\nbrings about its perceptual boost, especially when it comes to naturalistic\nsettings like recognising an object in an everyday scene. What aspects of a\nvisual task does attention help to deal with? We aim to answer this with a\ncomputational experiment based on a general framework called task-oriented\nablation design. First we define a broad range of visual tasks and identify six\nfactors that underlie task variability. Then on each task we compare the\nperformance of two neural networks, one with top-down attention and one\nwithout. These comparisons reveal the task-dependence of attention's perceptual\nboost, giving a clearer idea of the role attention plays. Whereas many existing\ncognitive accounts link attention to stimulus-level variables, such as visual\nclutter and object scale, we find greater explanatory power in system-level\nvariables that capture the interaction between the model, the distribution of\ntraining data and the task format. This finding suggests a shift in how\nattention is studied could be fruitful. We make publicly available our code and\nresults, along with statistics relevant to ImageNet-based experiments beyond\nthis one. Our contribution serves to support the development of more human-like\nvision models and the design of more informative machine-learning experiments.",
          "link": "http://arxiv.org/abs/2106.11339",
          "publishedOn": "2021-06-23T01:48:38.755Z",
          "wordCount": 672,
          "title": "Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:38.724Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11447",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Louren&#xe7;o Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1\">Miguel Nobre Menezes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1\">Tiago Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1\">Beatriz Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1\">Fausto J. Pinto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>",
          "description": "Coronary X-ray angiography is a crucial clinical procedure for the diagnosis\nand treatment of coronary artery disease, which accounts for roughly 16% of\nglobal deaths every year. However, the images acquired in these procedures have\nlow resolution and poor contrast, making lesion detection and assessment\nchallenging. Accurate coronary artery segmentation not only helps mitigate\nthese problems, but also allows the extraction of relevant anatomical features\nfor further analysis by quantitative methods. Although automated segmentation\nof coronary arteries has been proposed before, previous approaches have used\nnon-optimal segmentation criteria, leading to less useful results. Most methods\neither segment only the major vessel, discarding important information from the\nremaining ones, or segment the whole coronary tree based mostly on contrast\ninformation, producing a noisy output that includes vessels that are not\nrelevant for diagnosis. We adopt a better-suited clinical criterion and segment\nvessels according to their clinical relevance. Additionally, we simultaneously\nperform catheter segmentation, which may be useful for diagnosis due to the\nscale factor provided by the catheter's known diameter, and is a task that has\nnot yet been performed with good results. To derive the optimal approach, we\nconducted an extensive comparative study of encoder-decoder architectures\ntrained on a combination of focal loss and a variant of generalized dice loss.\nBased on the EfficientNet and the UNet++ architectures, we propose a line of\nefficient and high-performance segmentation models using a new decoder\narchitecture, the EfficientUNet++, whose best-performing version achieved\naverage dice scores of 0.8904 and 0.7526 for the artery and catheter classes,\nrespectively, and an average generalized dice score of 0.9234.",
          "link": "http://arxiv.org/abs/2106.11447",
          "publishedOn": "2021-06-23T01:48:38.709Z",
          "wordCount": 719,
          "title": "Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "Moving Object Detection (MOD) is a crucial task for the Autonomous Driving\npipeline. MOD is usually handled via 2-stream convolutional architectures that\nincorporates both appearance and motion cues, without considering the\ninter-relations between the spatial or motion features. In this paper, we\ntackle this problem through multi-head attention mechanisms, both across the\nspatial and motion streams. We propose MODETR; a Moving Object DEtection\nTRansformer network, comprised of multi-stream transformer encoders for both\nspatial and motion modalities, and an object transformer decoder that produces\nthe moving objects bounding boxes using set predictions. The whole architecture\nis trained end-to-end using bi-partite loss. Several methods of incorporating\nmotion cues with the Transformer model are explored, including two-stream RGB\nand Optical Flow (OF) methods, and multi-stream architectures that take\nadvantage of sequence information. To incorporate the temporal information, we\npropose a new Temporal Positional Encoding (TPE) approach to extend the Spatial\nPositional Encoding(SPE) in DETR. We explore two architectural choices for\nthat, balancing between speed and time. To evaluate the our network, we perform\nthe MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of\nthe Transformer network for MOD over the state-of-the art methods. Moreover,\nthe proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",
          "link": "http://arxiv.org/abs/2106.11422",
          "publishedOn": "2021-06-23T01:48:38.699Z",
          "wordCount": 662,
          "title": "MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyolim Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>",
          "description": "Generic Event Boundary Detection (GEBD) is a newly introduced task that aims\nto detect \"general\" event boundaries that correspond to natural human\nperception. In this paper, we introduce a novel contrastive learning based\napproach to deal with the GEBD. Our intuition is that the feature similarity of\nthe video snippet would significantly vary near the event boundaries, while\nremaining relatively the same in the remaining part of the video. In our model,\nTemporal Self-similarity Matrix (TSM) is utilized as an intermediate\nrepresentation which takes on a role as an information bottleneck. With our\nmodel, we achieved significant performance boost compared to the given\nbaselines. Our code is available at\nhttps://github.com/hello-jinwoo/LOVEU-CVPR2021.",
          "link": "http://arxiv.org/abs/2106.11549",
          "publishedOn": "2021-06-23T01:48:38.671Z",
          "wordCount": 551,
          "title": "Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning Approach. (arXiv:2106.11549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1\">Daniel V. Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todt_E/0/1/0/all/0/1\">Eduardo Todt</a>",
          "description": "With the rise of automation, unmanned vehicles became a hot topic both as\ncommercial products and as a scientific research topic. It composes a\nmulti-disciplinary field of robotics that encompasses embedded systems, control\ntheory, path planning, Simultaneous Localization and Mapping (SLAM), scene\nreconstruction, and pattern recognition. In this work, we present our\nexploratory research of how sensor data fusion and state-of-the-art machine\nlearning algorithms can perform the Embodied Artificial Intelligence (E-AI)\ntask called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation\n(ObjectNav) consists of autonomous navigation using egocentric visual\nobservations to reach an object belonging to the target semantic class without\nprior knowledge of the environment. Our method reached fourth place on the\nHabitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard\nPhase.",
          "link": "http://arxiv.org/abs/2106.11379",
          "publishedOn": "2021-06-23T01:48:38.662Z",
          "wordCount": 568,
          "title": "BEyond observation: an approach for ObjectNav. (arXiv:2106.11379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Trisha Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "The advent of social media platforms has been a catalyst for the development\nof digital photography that engendered a boom in vision applications. With this\nmotivation, we introduce a large-scale dataset termed 'Photozilla', which\nincludes over 990k images belonging to 10 different photographic styles. The\ndataset is then used to train 3 classification models to automatically classify\nthe images into the relevant style which resulted in an accuracy of ~96%. With\nthe rapid evolution of digital photography, we have seen new types of\nphotography styles emerging at an exponential rate. On that account, we present\na novel Siamese-based network that uses the trained classification models as\nthe base architecture to adapt and classify unseen styles with only 25 training\nsamples. We report an accuracy of over 68% for identifying 10 other distinct\ntypes of photography styles. This dataset can be found at\nhttps://trisha025.github.io/Photozilla/",
          "link": "http://arxiv.org/abs/2106.11359",
          "publishedOn": "2021-06-23T01:48:38.651Z",
          "wordCount": 617,
          "title": "Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-06-23T01:48:38.628Z",
          "wordCount": 692,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_X/0/1/0/all/0/1\">Xingyuan Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Junran Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>",
          "description": "Transfer learning with pre-training on large-scale datasets has played an\nincreasingly significant role in computer vision and natural language\nprocessing recently. However, as there exist numerous application scenarios\nthat have distinctive demands such as certain latency constraints and\nspecialized data distributions, it is prohibitively expensive to take advantage\nof large-scale pre-training for per-task requirements. In this paper, we focus\non the area of object detection and present a transfer learning system named\nGAIA, which could automatically and efficiently give birth to customized\nsolutions according to heterogeneous downstream needs. GAIA is capable of\nproviding powerful pre-trained weights, selecting models that conform to\ndownstream demands such as latency constraints and specified data domains, and\ncollecting relevant data for practitioners who have very few datapoints for\ntheir tasks. With GAIA, we achieve promising results on COCO, Objects365, Open\nImages, Caltech, CityPersons, and UODB which is a collection of datasets\nincluding KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as\nan example, GAIA is able to efficiently produce models covering a wide range of\nlatency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and\nbells. To benefit every practitioner in the community of object detection, GAIA\nis released at https://github.com/GAIA-vision.",
          "link": "http://arxiv.org/abs/2106.11346",
          "publishedOn": "2021-06-23T01:48:38.606Z",
          "wordCount": 666,
          "title": "GAIA: A Transfer Learning System of Object Detection that Fits Your Needs. (arXiv:2106.11346v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11322",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Lebreton_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Lebreton</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Brochard_R/0/1/0/all/0/1\">Roland Brochard</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Baudry_M/0/1/0/all/0/1\">Matthieu Baudry</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jonniaux_G/0/1/0/all/0/1\">Gr&#xe9;gory Jonniaux</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Salah_A/0/1/0/all/0/1\">Adrien Hadj Salah</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kanani_K/0/1/0/all/0/1\">Keyvan Kanani</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Goff_M/0/1/0/all/0/1\">Matthieu Le Goff</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Masson_A/0/1/0/all/0/1\">Aurore Masson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ollagnier_N/0/1/0/all/0/1\">Nicolas Ollagnier</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Panicucci_P/0/1/0/all/0/1\">Paolo Panicucci</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Proag_A/0/1/0/all/0/1\">Amsha Proag</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Robin_C/0/1/0/all/0/1\">Cyril Robin</a>",
          "description": "Image Processing algorithms for vision-based navigation require reliable\nimage simulation capacities. In this paper we explain why traditional rendering\nengines may present limitations that are potentially critical for space\napplications. We introduce Airbus SurRender software v7 and provide details on\nfeatures that make it a very powerful space image simulator. We show how\nSurRender is at the heart of the development processes of our computer vision\nsolutions and we provide a series of illustrations of rendered images for\nvarious use cases ranging from Moon and Solar System exploration, to in orbit\nrendezvous and planetary robotics.",
          "link": "http://arxiv.org/abs/2106.11322",
          "publishedOn": "2021-06-23T01:48:38.550Z",
          "wordCount": 586,
          "title": "Image simulation for space applications with the SurRender software. (arXiv:2106.11322v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11330",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Simon Chun-Ho Yu</a>",
          "description": "Accurate liver and lesion segmentation from computed tomography (CT) images\nare highly demanded in clinical practice for assisting the diagnosis and\nassessment of hepatic tumor disease. However, automatic liver and lesion\nsegmentation from contrast-enhanced CT volumes is extremely challenging due to\nthe diversity in contrast, resolution, and quality of images. Previous methods\nbased on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either\nlack sufficient spatial contexts or suffer from high GPU computational cost,\nwhich limits the performance. To tackle these issues, we propose a novel\ncontext-aware PolyUNet for accurate liver and lesion segmentation. It jointly\nexplores structural diversity and consecutive t-adjacent slices to enrich\nfeature expressive power and spatial contextual information while avoiding the\noverload of GPU memory consumption. In addition, we utilize zoom out/in and\ntwo-stage refinement strategy to exclude the irrelevant contexts and focus on\nthe specific region for the fine-grained segmentation. Our method achieved very\ncompetitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS)\nChallenge among all tasks with a single model and ranked the $3^{rd}$,\n$12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion\nsegmentation, lesion detection, and tumor burden estimation, respectively.",
          "link": "http://arxiv.org/abs/2106.11330",
          "publishedOn": "2021-06-23T01:48:38.542Z",
          "wordCount": 643,
          "title": "Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images. (arXiv:2106.11330v1 [eess.IV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2102.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "Proximal Policy Optimization (PPO) is among the most widely used algorithms\nin reinforcement learning, which achieves state-of-the-art performance in many\nchallenging problems. The keys to its success are the reliable policy updates\nthrough the clipping mechanism and the multiple epochs of minibatch updates.\nThe aim of this research is to give new simple but effective alternatives to\nthe former. For this, we propose linearly and exponentially decaying clipping\nrange approaches throughout the training. With these, we would like to provide\nhigher exploration at the beginning and stronger restrictions at the end of the\nlearning phase. We investigate their performance in several classical control\nand locomotive robotic environments. During the analysis, we found that they\ninfluence the achieved rewards and are effective alternatives to the constant\nclipping method in many reinforcement learning tasks.",
          "link": "http://arxiv.org/abs/2102.10456",
          "publishedOn": "2021-06-29T01:55:20.694Z",
          "wordCount": 600,
          "title": "Decaying Clipping Range in Proximal Policy Optimization. (arXiv:2102.10456v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "An in-depth understanding of the particular environment is crucial in\nreinforcement learning (RL). To address this challenge, the decision-making\nprocess of a mobile collaborative robotic assistant modeled by the Markov\ndecision process (MDP) framework is studied in this paper. The optimal\nstate-action combinations of the MDP are calculated with the non-linear Bellman\noptimality equations. This system of equations can be solved with relative ease\nby the computational power of Wolfram Mathematica, where the obtained optimal\naction-values point to the optimal policy. Unlike other RL algorithms, this\nmethodology does not approximate the optimal behavior, it gives the exact,\nexplicit solution, which provides a strong foundation for our study. With this,\nwe offer new insights into understanding the action selection mechanisms in RL\nby presenting various small modifications on the very same schema that lead to\ndifferent optimal policies.",
          "link": "http://arxiv.org/abs/2102.10447",
          "publishedOn": "2021-06-29T01:55:20.689Z",
          "wordCount": 615,
          "title": "Importance of Environment Design in Reinforcement Learning: A Study of a Robotic Environment. (arXiv:2102.10447v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>",
          "description": "Consider a regression problem where the learner is given a large collection\nof $d$-dimensional data points, but can only query a small subset of the\nreal-valued labels. How many queries are needed to obtain a $1+\\epsilon$\nrelative error approximation of the optimum? While this problem has been\nextensively studied for least squares regression, little is known for other\nlosses. An important example is least absolute deviation regression ($\\ell_1$\nregression) which enjoys superior robustness to outliers compared to least\nsquares. We develop a new framework for analyzing importance sampling methods\nin regression problems, which enables us to show that the query complexity of\nleast absolute deviation regression is $\\Theta(d/\\epsilon^2)$ up to logarithmic\nfactors. We further extend our techniques to show the first bounds on the query\ncomplexity for any $\\ell_p$ loss with $p\\in(1,2)$. As a key novelty in our\nanalysis, we introduce the notion of robust uniform convergence, which is a new\napproximation guarantee for the empirical loss. While it is inspired by uniform\nconvergence in statistical learning, our approach additionally incorporates a\ncorrection term to avoid unnecessary variance due to outliers. This can be\nviewed as a new connection between statistical learning theory and variance\nreduction techniques in stochastic optimization, which should be of independent\ninterest.",
          "link": "http://arxiv.org/abs/2102.02322",
          "publishedOn": "2021-06-29T01:55:20.671Z",
          "wordCount": 672,
          "title": "Query Complexity of Least Absolute Deviation Regression via Robust Uniform Convergence. (arXiv:2102.02322v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:20.665Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Ensembles of deep neural networks are known to achieve state-of-the-art\nperformance in uncertainty estimation and lead to accuracy improvement. In this\nwork, we focus on a classification problem and investigate the behavior of both\nnon-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble\nas a function of the ensemble size and the member network size. We indicate the\nconditions under which CNLL follows a power law w.r.t. ensemble size or member\nnetwork size, and analyze the dynamics of the parameters of the discovered\npower laws. Our important practical finding is that one large network may\nperform worse than an ensemble of several medium-size networks with the same\ntotal number of parameters (we call this ensemble a memory split). Using the\ndetected power law-like dependencies, we can predict (1) the possible gain from\nthe ensembling of networks with given structure, (2) the optimal memory split\ngiven a memory budget, based on a relatively small number of trained networks.\n\nWe describe the memory split advantage effect in more details in\narXiv:2005.07292",
          "link": "http://arxiv.org/abs/2007.08483",
          "publishedOn": "2021-06-29T01:55:20.659Z",
          "wordCount": 648,
          "title": "On Power Laws in Deep Ensembles. (arXiv:2007.08483v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1\">Nir Shlezinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "Signal processing, communications, and control have traditionally relied on\nclassical statistical modeling techniques. Such model-based methods utilize\nmathematical formulations that represent the underlying physics, prior\ninformation and additional domain knowledge. Simple classical models are useful\nbut sensitive to inaccuracies and may lead to poor performance when real\nsystems display complex or dynamic behavior. On the other hand, purely\ndata-driven approaches that are model-agnostic are becoming increasingly\npopular as datasets become abundant and the power of modern deep learning\npipelines increases. Deep neural networks (DNNs) use generic architectures\nwhich learn to operate from data, and demonstrate excellent performance,\nespecially for supervised problems. However, DNNs typically require massive\namounts of data and immense computational resources, limiting their\napplicability for some signal processing scenarios. We are interested in hybrid\ntechniques that combine principled mathematical models with data-driven systems\nto benefit from the advantages of both approaches. Such model-based deep\nlearning methods exploit both partial domain knowledge, via mathematical\nstructures designed for specific problems, as well as learning from limited\ndata. In this article we survey the leading approaches for studying and\ndesigning model-based deep learning systems. We divide hybrid\nmodel-based/data-driven systems into categories based on their inference\nmechanism. We provide a comprehensive review of the leading approaches for\ncombining model-based algorithms with deep learning in a systematic manner,\nalong with concrete guidelines and detailed signal processing oriented examples\nfrom recent literature. Our aim is to facilitate the design and study of future\nsystems on the intersection of signal processing and machine learning that\nincorporate the advantages of both domains.",
          "link": "http://arxiv.org/abs/2012.08405",
          "publishedOn": "2021-06-29T01:55:20.653Z",
          "wordCount": 703,
          "title": "Model-Based Deep Learning. (arXiv:2012.08405v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have been widely used on modeling graph data, achieving\nimpressive results on node classification and link prediction tasks. Yet,\nobtaining an accurate representation for a graph further requires a pooling\nfunction that maps a set of node representations into a compact form. A simple\nsum or average over all node representations considers all node features\nequally without consideration of their task relevance, and any structural\ndependencies among them. Recently proposed hierarchical graph pooling methods,\non the other hand, may yield the same representation for two different graphs\nthat are distinguished by the Weisfeiler-Lehman test, as they suboptimally\npreserve information from the node features. To tackle these limitations of\nexisting graph pooling methods, we first formulate the graph pooling problem as\na multiset encoding problem with auxiliary information about the graph\nstructure, and propose a Graph Multiset Transformer (GMT) which is a multi-head\nattention based global pooling layer that captures the interaction between\nnodes according to their structural dependencies. We show that GMT satisfies\nboth injectiveness and permutation invariance, such that it is at most as\npowerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods\ncan be easily extended to the previous node clustering approaches for\nhierarchical graph pooling. Our experimental results show that GMT\nsignificantly outperforms state-of-the-art graph pooling methods on graph\nclassification benchmarks with high memory and time efficiency, and obtains\neven larger performance gain on graph reconstruction and generation tasks.",
          "link": "http://arxiv.org/abs/2102.11533",
          "publishedOn": "2021-06-29T01:55:20.646Z",
          "wordCount": 716,
          "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling. (arXiv:2102.11533v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14352",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xia_E/0/1/0/all/0/1\">Eric Xia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Various algorithms in reinforcement learning exhibit dramatic variability in\ntheir convergence rates and ultimate accuracy as a function of the problem\nstructure. Such instance-specific behavior is not captured by existing global\nminimax bounds, which are worst-case in nature. We analyze the problem of\nestimating optimal $Q$-value functions for a discounted Markov decision process\nwith discrete states and actions and identify an instance-dependent functional\nthat controls the difficulty of estimation in the $\\ell_\\infty$-norm. Using a\nlocal minimax framework, we show that this functional arises in lower bounds on\nthe accuracy on any estimation procedure. In the other direction, we establish\nthe sharpness of our lower bounds, up to factors logarithmic in the state and\naction spaces, by analyzing a variance-reduced version of $Q$-learning. Our\ntheory provides a precise way of distinguishing \"easy\" problems from \"hard\"\nones in the context of $Q$-learning, as illustrated by an ensemble with a\ncontinuum of difficulty.",
          "link": "http://arxiv.org/abs/2106.14352",
          "publishedOn": "2021-06-29T01:55:20.627Z",
          "wordCount": 587,
          "title": "Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning. (arXiv:2106.14352v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunsheng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Ziheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "In this work, we focus on large graph similarity computation problem and\npropose a novel \"embedding-coarsening-matching\" learning framework, which\noutperforms state-of-the-art methods in this task and has significant\nimprovement in time efficiency. Graph similarity computation for metrics such\nas Graph Edit Distance (GED) is typically NP-hard, and existing\nheuristics-based algorithms usually achieves a unsatisfactory trade-off between\naccuracy and efficiency. Recently the development of deep learning techniques\nprovides a promising solution for this problem by a data-driven approach which\ntrains a network to encode graphs to their own feature vectors and computes\nsimilarity based on feature vectors. These deep-learning methods can be\nclassified to two categories, embedding models and matching models. Embedding\nmodels such as GCN-Mean and GCN-Max, which directly map graphs to respective\nfeature vectors, run faster but the performance is usually poor due to the lack\nof interactions across graphs. Matching models such as GMN, whose encoding\nprocess involves interaction across the two graphs, are more accurate but\ninteraction between whole graphs brings a significant increase in time\nconsumption (at least quadratic time complexity over number of nodes). Inspired\nby large biological molecular identification where the whole molecular is first\nmapped to functional groups and then identified based on these functional\ngroups, our \"embedding-coarsening-matching\" learning framework first embeds and\ncoarsens large graphs to coarsened graphs with denser local topology and then\nmatching mechanism is deployed on the coarsened graphs for the final similarity\nscores. Detailed experiments have been conducted and the results demonstrate\nthe efficiency and effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2005.07115",
          "publishedOn": "2021-06-29T01:55:20.620Z",
          "wordCount": 761,
          "title": "Hierarchical Large-scale Graph Similarity Computation via Graph Coarsening and Matching. (arXiv:2005.07115v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:20.614Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rawal_K/0/1/0/all/0/1\">Kaivalya Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As predictive models are increasingly being deployed to make a variety of\nconsequential decisions, there is a growing emphasis on designing algorithms\nthat can provide recourse to affected individuals. Existing recourse algorithms\nfunction under the assumption that the underlying predictive model does not\nchange. However, models are regularly updated in practice for several reasons\nincluding data distribution shifts. In this work, we make the first attempt at\nunderstanding how model updates resulting from data distribution shifts impact\nthe algorithmic recourses generated by state-of-the-art algorithms. We carry\nout a rigorous theoretical and empirical analysis to address the above\nquestion. Our theoretical results establish a lower bound on the probability of\nrecourse invalidation due to model shifts, and show the existence of a tradeoff\nbetween this invalidation probability and typical notions of \"cost\" minimized\nby modern recourse generation algorithms. We experiment with multiple synthetic\nand real world datasets, capturing different kinds of distribution shifts\nincluding temporal shifts, geospatial shifts, and shifts due to data\ncorrection. These experiments demonstrate that model updation due to all the\naforementioned distribution shifts can potentially invalidate recourses\ngenerated by state-of-the-art algorithms. Our findings thus not only expose\npreviously unknown flaws in the current recourse generation paradigm, but also\npave the way for fundamentally rethinking the design and development of\nrecourse generation algorithms.",
          "link": "http://arxiv.org/abs/2012.11788",
          "publishedOn": "2021-06-29T01:55:20.599Z",
          "wordCount": 692,
          "title": "Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts. (arXiv:2012.11788v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "We design a new algorithm for the Euclidean $k$-means problem that operates\nin the local model of differential privacy. Unlike in the non-private\nliterature, differentially private algorithms for the $k$-means objective incur\nboth additive and multiplicative errors. Our algorithm significantly reduces\nthe additive error while keeping the multiplicative error the same as in\nprevious state-of-the-art results. Specifically, on a database of size $n$, our\nalgorithm guarantees $O(1)$ multiplicative error and $\\approx n^{1/2+a}$\nadditive error for an arbitrarily small constant $a>0$. All previous algorithms\nin the local model had additive error $\\approx n^{2/3+a}$. Our techniques\nextend to $k$-median clustering.\n\nWe show that the additive error we obtain is almost optimal in terms of its\ndependency on the database size $n$. Specifically, we give a simple lower bound\nshowing that every locally-private algorithm for the $k$-means objective must\nhave additive error at least $\\approx\\sqrt{n}$.",
          "link": "http://arxiv.org/abs/1907.02513",
          "publishedOn": "2021-06-29T01:55:20.593Z",
          "wordCount": 601,
          "title": "Locally Private k-Means Clustering. (arXiv:1907.02513v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:20.579Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yirong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Although achieving remarkable progress, it is very difficult to induce a\nsupervised classifier without any labeled data. Unsupervised domain adaptation\nis able to overcome this challenge by transferring knowledge from a labeled\nsource domain to an unlabeled target domain. Transferability and\ndiscriminability are two key criteria for characterizing the superiority of\nfeature representations to enable successful domain adaptation. In this paper,\na novel method called \\textit{learning TransFerable and Discriminative Features\nfor unsupervised domain adaptation} (TFDF) is proposed to optimize these two\nobjectives simultaneously. On the one hand, distribution alignment is performed\nto reduce domain discrepancy and learn more transferable representations.\nInstead of adopting \\textit{Maximum Mean Discrepancy} (MMD) which only captures\nthe first-order statistical information to measure distribution discrepancy, we\nadopt a recently proposed statistic called \\textit{Maximum Mean and Covariance\nDiscrepancy} (MMCD), which can not only capture the first-order statistical\ninformation but also capture the second-order statistical information in the\nreproducing kernel Hilbert space (RKHS). On the other hand, we propose to\nexplore both local discriminative information via manifold regularization and\nglobal discriminative information via minimizing the proposed \\textit{class\nconfusion} objective to learn more discriminative features, respectively. We\nintegrate these two objectives into the \\textit{Structural Risk Minimization}\n(RSM) framework and learn a domain-invariant classifier. Comprehensive\nexperiments are conducted on five real-world datasets and the results verify\nthe effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2003.11723",
          "publishedOn": "2021-06-29T01:55:20.573Z",
          "wordCount": 693,
          "title": "Learning transferable and discriminative features for unsupervised domain adaptation. (arXiv:2003.11723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:20.566Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1302.6808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We describe algorithms for learning Bayesian networks from a combination of\nuser knowledge and statistical data. The algorithms have two components: a\nscoring metric and a search procedure. The scoring metric takes a network\nstructure, statistical data, and a user's prior knowledge, and returns a score\nproportional to the posterior probability of the network structure given the\ndata. The search procedure generates networks for evaluation by the scoring\nmetric. Previous work has concentrated on metrics for domains containing only\ndiscrete variables, under the assumption that data represents a multinomial\nsample. In this paper, we extend this work, developing scoring metrics for\ndomains containing all continuous variables or a mixture of discrete and\ncontinuous variables, under the assumption that continuous data is sampled from\na multivariate normal distribution. Our work extends traditional statistical\napproaches for identifying vanishing regression coefficients in that we\nidentify two important assumptions, called event equivalence and parameter\nmodularity, that when combined allow the construction of prior distributions\nfor multivariate normal parameters from a single prior Bayesian network\nspecified by a user.",
          "link": "http://arxiv.org/abs/1302.6808",
          "publishedOn": "2021-06-29T01:55:20.560Z",
          "wordCount": 653,
          "title": "Learning Gaussian Networks. (arXiv:1302.6808v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14289",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/math/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study the asymmetric low-rank factorization problem: \\[\\min_{\\mathbf{U}\n\\in \\mathbb{R}^{m \\times d}, \\mathbf{V} \\in \\mathbb{R}^{n \\times d}}\n\\frac{1}{2}\\|\\mathbf{U}\\mathbf{V}^\\top -\\mathbf{\\Sigma}\\|_F^2\\] where\n$\\mathbf{\\Sigma}$ is a given matrix of size $m \\times n$ and rank $d$. This is\na canonical problem that admits two difficulties in optimization: 1)\nnon-convexity and 2) non-smoothness (due to unbalancedness of $\\mathbf{U}$ and\n$\\mathbf{V}$). This is also a prototype for more complex problems such as\nasymmetric matrix sensing and matrix completion. Despite being non-convex and\nnon-smooth, it has been observed empirically that the randomly initialized\ngradient descent algorithm can solve this problem in polynomial time. Existing\ntheories to explain this phenomenon all require artificial modifications of the\nalgorithm, such as adding noise in each iteration and adding a balancing\nregularizer to balance the $\\mathbf{U}$ and $\\mathbf{V}$.\n\nThis paper presents the first proof that shows randomly initialized gradient\ndescent converges to a global minimum of the asymmetric low-rank factorization\nproblem with a polynomial rate. For the proof, we develop 1) a new\nsymmetrization technique to capture the magnitudes of the symmetry and\nasymmetry, and 2) a quantitative perturbation analysis to approximate matrix\nderivatives. We believe both are useful for other related non-convex problems.",
          "link": "http://arxiv.org/abs/2106.14289",
          "publishedOn": "2021-06-29T01:55:20.554Z",
          "wordCount": 637,
          "title": "Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization. (arXiv:2106.14289v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1904.06366",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1\">Fan Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "We develop methodology for three-dimensional (3D) radial visualization\n(RadViz) of multidimensional datasets. Our tool is called RadViz3D and extends\nthe classical two-dimensional (2D) RadViz that visualizes multivariate data in\nthe 2D plane by mapping every observation to a point inside the unit circle. We\nshow that distributing anchor points uniformly on the 3D unit sphere provides\nthe best visualization with minimal artificial visual correlation for data with\nuncorrelated variables. However, anchor points can be placed exactly\nequi-distant from each other only for the five Platonic solids. We provide\nequi-distant anchor points for these five settings, and approximately\nequi-distant anchor points via a Fibonacci grid for the other cases. Our\nmethodology, implemented in the R package $radviz3d$, makes fully 3D RadViz\npossible and is shown to improve clarity of this nonlinear display technique on\nsimulated and real datasets.",
          "link": "http://arxiv.org/abs/1904.06366",
          "publishedOn": "2021-06-29T01:55:20.539Z",
          "wordCount": 601,
          "title": "Fully Three-dimensional Radial Visualization. (arXiv:1904.06366v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekaran_V/0/1/0/all/0/1\">V. Gunasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovi_K/0/1/0/all/0/1\">K.K. Kovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arja_S/0/1/0/all/0/1\">S. Arja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimata_R/0/1/0/all/0/1\">R. Chimata</a>",
          "description": "Renewable energy forecasting is attaining greater importance due to its\nconstant increase in contribution to the electrical power grids. Solar energy\nis one of the most significant contributors to renewable energy and is\ndependent on solar irradiation. For the effective management of electrical\npower grids, forecasting models that predict solar irradiation, with high\naccuracy, are needed. In the current study, Machine Learning techniques such as\nLinear Regression, Extreme Gradient Boosting and Genetic Algorithm Optimization\nare used to forecast solar irradiation. The data used for training and\nvalidation is recorded from across three different geographical stations in the\nUnited States that are part of the SURFRAD network. A Global Horizontal Index\n(GHI) is predicted for the models built and compared. Genetic Algorithm\nOptimization is applied to XGB to further improve the accuracy of solar\nirradiation prediction.",
          "link": "http://arxiv.org/abs/2106.13956",
          "publishedOn": "2021-06-29T01:55:20.533Z",
          "wordCount": 577,
          "title": "Solar Irradiation Forecasting using Genetic Algorithms. (arXiv:2106.13956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Robert Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1\">Nayan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rohan Jain</a>",
          "description": "Deep learning has proven to be a highly effective problem-solving tool for\nobject detection and image segmentation across various domains such as\nhealthcare and autonomous driving. At the heart of this performance lies neural\narchitecture design which relies heavily on domain knowledge and prior\nexperience on the researchers' behalf. More recently, this process of finding\nthe most optimal architectures, given an initial search space of possible\noperations, was automated by Neural Architecture Search (NAS). In this paper,\nwe evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)\nagainst data agnostic poisoning attacks on the original search space with\ncarefully designed ineffective operations. By evaluating algorithm performance\non the CIFAR-10 dataset, we empirically demonstrate how our novel search space\npoisoning (SSP) approach and multiple-instance poisoning attacks exploit design\nflaws in the ENAS controller to result in inflated prediction error rates for\nchild networks. Our results provide insights into the challenges to surmount in\nusing NAS for more adversarially robust architecture search.",
          "link": "http://arxiv.org/abs/2106.14406",
          "publishedOn": "2021-06-29T01:55:20.498Z",
          "wordCount": 630,
          "title": "Poisoning the Search Space in Neural Architecture Search. (arXiv:2106.14406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xian_L/0/1/0/all/0/1\">Lu Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1\">Henry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaz_C/0/1/0/all/0/1\">Chad M. Topaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegelmeier_L/0/1/0/all/0/1\">Lori Ziegelmeier</a>",
          "description": "One approach to understanding complex data is to study its shape through the\nlens of algebraic topology. While the early development of topological data\nanalysis focused primarily on static data, in recent years, theoretical and\napplied studies have turned to data that varies in time. A time-varying\ncollection of metric spaces as formed, for example, by a moving school of fish\nor flock of birds, can contain a vast amount of information. There is often a\nneed to simplify or summarize the dynamic behavior. We provide an introduction\nto topological summaries of time-varying metric spaces including vineyards\n[19], crocker plots [56], and multiparameter rank functions [37]. We then\nintroduce a new tool to summarize time-varying metric spaces: a crocker stack.\nCrocker stacks are convenient for visualization, amenable to machine learning,\nand satisfy a desirable continuity property which we prove. We demonstrate the\nutility of crocker stacks for a parameter identification task involving an\ninfluential model of biological aggregations [58]. Altogether, we aim to bring\nthe broader applied mathematics community up-to-date on topological summaries\nof time-varying metric spaces.",
          "link": "http://arxiv.org/abs/2010.05780",
          "publishedOn": "2021-06-29T01:55:18.547Z",
          "wordCount": 658,
          "title": "Capturing Dynamics of Time-Varying Data via Topology. (arXiv:2010.05780v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:18.531Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:18.517Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1\">Cole Hurwitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1\">Nina Kudryashova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1\">Arno Onken</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1\">Matthias H. Hennig</a>",
          "description": "Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.",
          "link": "http://arxiv.org/abs/2102.01807",
          "publishedOn": "2021-06-29T01:55:18.510Z",
          "wordCount": 527,
          "title": "Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v3 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Witt_L/0/1/0/all/0/1\">Leon Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_U/0/1/0/all/0/1\">Usama Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">KuoYeh Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1\">Felix Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>",
          "description": "The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.",
          "link": "http://arxiv.org/abs/2106.14265",
          "publishedOn": "2021-06-29T01:55:18.492Z",
          "wordCount": 687,
          "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain. (arXiv:2106.14265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1\">Nathalie Deziderio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1\">Hugo Tremonte de Carvalho</a>",
          "description": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.",
          "link": "http://arxiv.org/abs/2106.14323",
          "publishedOn": "2021-06-29T01:55:18.485Z",
          "wordCount": 580,
          "title": "Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:18.473Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaekyeom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seohong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>",
          "description": "Having the ability to acquire inherent skills from environments without any\nexternal rewards or supervision like humans is an important problem. We propose\na novel unsupervised skill discovery method named Information Bottleneck Option\nLearning (IBOL). On top of the linearization of environments that promotes more\nvarious and distant state transitions, IBOL enables the discovery of diverse\nskills. It provides the abstraction of the skills learned with the information\nbottleneck framework for the options with improved stability and encouraged\ndisentanglement. We empirically demonstrate that IBOL outperforms multiple\nstate-of-the-art unsupervised skill discovery methods on the\ninformation-theoretic evaluations and downstream tasks in MuJoCo environments,\nincluding Ant, HalfCheetah, Hopper and D'Kitty.",
          "link": "http://arxiv.org/abs/2106.14305",
          "publishedOn": "2021-06-29T01:55:18.467Z",
          "wordCount": 550,
          "title": "Unsupervised Skill Discovery with Bottleneck Option Learning. (arXiv:2106.14305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:18.454Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1\">Rustem Takhanov</a>",
          "description": "We present a new way of study of Mercer kernels, by corresponding to a\nspecial kernel $K$ a pseudo-differential operator $p({\\mathbf x}, D)$ such that\n$\\mathcal{F} p({\\mathbf x}, D)^\\dag p({\\mathbf x}, D) \\mathcal{F}^{-1}$ acts on\nsmooth functions in the same way as an integral operator associated with $K$\n(where $\\mathcal{F}$ is the Fourier transform). We show that kernels defined by\npseudo-differential operators are able to approximate uniformly any continuous\nMercer kernel on a compact set.\n\nThe symbol $p({\\mathbf x}, {\\mathbf y})$ encapsulates a lot of useful\ninformation about the structure of the Maximum Mean Discrepancy distance\ndefined by the kernel $K$. We approximate $p({\\mathbf x}, {\\mathbf y})$ with\nthe sum of the first $r$ terms of the Singular Value Decomposition of $p$,\ndenoted by $p_r({\\mathbf x}, {\\mathbf y})$. If ordered singular values of the\nintegral operator associated with $p({\\mathbf x}, {\\mathbf y})$ die down\nrapidly, the MMD distance defined by the new symbol $p_r$ differs from the\ninitial one only slightly. Moreover, the new MMD distance can be interpreted as\nan aggregated result of comparing $r$ local moments of two probability\ndistributions.\n\nThe latter results holds under the condition that right singular vectors of\nthe integral operator associated with $p$ are uniformly bounded. But even if\nthis is not satisfied we can still hold that the Hilbert-Schmidt distance\nbetween $p$ and $p_r$ vanishes. Thus, we report an interesting phenomenon: the\nMMD distance measures the difference of two probability distributions with\nrespect to a certain number of local moments, $r^\\ast$, and this number\n$r^\\ast$ depends on the speed with which singular values of $p$ die down.",
          "link": "http://arxiv.org/abs/2106.14277",
          "publishedOn": "2021-06-29T01:55:18.447Z",
          "wordCount": 690,
          "title": "How many moments does MMD compare?. (arXiv:2106.14277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04293",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tao_T/0/1/0/all/0/1\">Ting Tao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1\">Shaohua Pan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bi_S/0/1/0/all/0/1\">Shujun Bi</a>",
          "description": "This paper is concerned with the squared F(robenius)-norm regularized\nfactorization form for noisy low-rank matrix recovery problems. Under a\nsuitable assumption on the restricted condition number of the Hessian for the\nloss function, we derive an error bound to the true matrix for the non-strict\ncritical points with rank not more than that of the true matrix. Then, for the\nsquared F-norm regularized factorized least squares loss function, under the\nnoisy and full sample setting we establish its KL property of exponent $1/2$ on\nits global minimizer set, and under the noisy and partial sample setting\nachieve this property for a class of critical points. These theoretical\nfindings are also confirmed by solving the squared F-norm regularized\nfactorization problem with an accelerated alternating minimization method.",
          "link": "http://arxiv.org/abs/1911.04293",
          "publishedOn": "2021-06-29T01:55:18.442Z",
          "wordCount": 598,
          "title": "Error bound of critical points and KL property of exponent $1/2$ for squared F-norm regularized factorization. (arXiv:1911.04293v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:18.392Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>",
          "description": "We consider non-convex stochastic optimization using first-order algorithms\nfor which the gradient estimates may have heavy tails. We show that a\ncombination of gradient clipping, momentum, and normalized gradient descent\nyields convergence to critical points in high-probability with best-known rates\nfor smooth losses when the gradients only have bounded $\\mathfrak{p}$th moments\nfor some $\\mathfrak{p}\\in(1,2]$. We then consider the case of second-order\nsmooth losses, which to our knowledge have not been studied in this setting,\nand again obtain high-probability bounds for any $\\mathfrak{p}$. Moreover, our\nresults hold for arbitrary smooth norms, in contrast to the typical SGD\nanalysis which requires a Hilbert space norm. Further, we show that after a\nsuitable \"burn-in\" period, the objective value will monotonically decrease for\nevery iteration until a critical point is identified, which provides intuition\nbehind the popular practice of learning rate \"warm-up\" and also yields a\nlast-iterate guarantee.",
          "link": "http://arxiv.org/abs/2106.14343",
          "publishedOn": "2021-06-29T01:55:18.379Z",
          "wordCount": 583,
          "title": "High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails. (arXiv:2106.14343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiotras_P/0/1/0/all/0/1\">Panagiotis Tsiotras</a>",
          "description": "We explore the use of policy approximations to reduce the computational cost\nof learning Nash equilibria in zero-sum stochastic games. We propose a new\nQ-learning type algorithm that uses a sequence of entropy-regularized soft\npolicies to approximate the Nash policy during the Q-function updates. We prove\nthat under certain conditions, by updating the regularized Q-function, the\nalgorithm converges to a Nash equilibrium. We also demonstrate the proposed\nalgorithm's ability to transfer previous training experiences, enabling the\nagents to adapt quickly to new environments. We provide a dynamic\nhyper-parameter scheduling scheme to further expedite convergence. Empirical\nresults applied to a number of stochastic games verify that the proposed\nalgorithm converges to the Nash equilibrium, while exhibiting a major speed-up\nover existing algorithms.",
          "link": "http://arxiv.org/abs/2009.00162",
          "publishedOn": "2021-06-29T01:55:18.373Z",
          "wordCount": 606,
          "title": "Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation. (arXiv:2009.00162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1\">Philip Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "K-Nearest Neighbor (kNN)-based deep learning methods have been applied to\nmany applications due to their simplicity and geometric interpretability.\nHowever, the robustness of kNN-based classification models has not been\nthoroughly explored and kNN attack strategies are underdeveloped. In this\npaper, we propose an Adversarial Soft kNN (ASK) loss to both design more\neffective kNN attack strategies and to develop better defenses against them.\nOur ASK loss approach has two advantages. First, ASK loss can better\napproximate the kNN's probability of classification error than objectives\nproposed in previous works. Second, the ASK loss is interpretable: it preserves\nthe mutual information between the perturbed input and the kNN of the\nunperturbed input. We use the ASK loss to generate a novel attack method called\nthe ASK-Attack (ASK-Atk), which shows superior attack efficiency and accuracy\ndegradation relative to previous kNN attacks. Based on the ASK-Atk, we then\nderive an ASK-Defense (ASK-Def) method that optimizes the worst-case training\nloss induced by ASK-Atk.",
          "link": "http://arxiv.org/abs/2106.14300",
          "publishedOn": "2021-06-29T01:55:18.367Z",
          "wordCount": 601,
          "title": "ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense. (arXiv:2106.14300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:18.353Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:18.348Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_F/0/1/0/all/0/1\">Fatemeh Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">Joern Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "Spatial Transformer Networks (STN) can generate geometric transformations\nwhich modify input images to improve the classifier's performance. In this\nwork, we combine the idea of STN with Reinforcement Learning (RL). To this end,\nwe break the affine transformation down into a sequence of simple and discrete\ntransformations. We formulate the task as a Markovian Decision Process (MDP)\nand use RL to solve this sequential decision-making problem. STN architectures\nlearn the transformation parameters by minimizing the classification error and\nbackpropagating the gradients through a sub-differentiable sampling module. In\nour method, we are not bound to the differentiability of the sampling modules.\nMoreover, we have freedom in designing the objective rather than only\nminimizing the error; e.g., we can directly set the target as maximizing the\naccuracy. We design multiple experiments to verify the effectiveness of our\nmethod using cluttered MNIST and Fashion-MNIST datasets and show that our\nmethod outperforms STN with a proper definition of MDP components.",
          "link": "http://arxiv.org/abs/2106.14295",
          "publishedOn": "2021-06-29T01:55:18.338Z",
          "wordCount": 587,
          "title": "A Reinforcement Learning Approach for Sequential Spatial Transformer Networks. (arXiv:2106.14295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Juditsky_A/0/1/0/all/0/1\">Anatoli Juditsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kulunchakov_A/0/1/0/all/0/1\">Andrei Kulunchakov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsyntseus_H/0/1/0/all/0/1\">Hlib Tsyntseus</a>",
          "description": "In this paper, we discuss application of iterative Stochastic Optimization\nroutines to the problem of sparse signal recovery from noisy observation. Using\nStochastic Mirror Descent algorithm as a building block, we develop a\nmultistage procedure for recovery of sparse solutions to Stochastic\nOptimization problem under assumption of smoothness and quadratic minoration on\nthe expected objective. An interesting feature of the proposed algorithm is\nlinear convergence of the approximate solution during the preliminary phase of\nthe routine when the component of stochastic error in the gradient observation\nwhich is due to bad initial approximation of the optimal solution is larger\nthan the \"ideal\" asymptotic error component owing to observation noise \"at the\noptimal solution.\" We also show how one can straightforwardly enhance\nreliability of the corresponding solution by using Median-of-Means like\ntechniques.\n\nWe illustrate the performance of the proposed algorithms in application to\nclassical problems of recovery of sparse and low rank signals in linear\nregression framework. We show, under rather weak assumption on the regressor\nand noise distributions, how they lead to parameter estimates which obey (up to\nfactors which are logarithmic in problem dimension and confidence level) the\nbest known to us accuracy bounds.",
          "link": "http://arxiv.org/abs/2006.06365",
          "publishedOn": "2021-06-29T01:55:18.333Z",
          "wordCount": 643,
          "title": "Sparse recovery by reduced variance stochastic approximation. (arXiv:2006.06365v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00413",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jiancheng Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiang Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>",
          "description": "Two-stage ensemble-based forecasting methods have been studied extensively in\nthe wind power forecasting field. However, deep learning-based wind power\nforecasting studies have not investigated two aspects. In the first stage,\ndifferent learning structures considering multiple inputs and multiple outputs\nhave not been discussed. In the second stage, the model extrapolation issue has\nnot been investigated. Therefore, we develop four deep neural networks for the\nfirst stage to learn data features considering the input-and-output structure.\nWe then explore the model extrapolation issue in the second stage using\ndifferent modeling methods. Considering the overfitting issue, we propose a new\nmoving window-based algorithm using a validation set in the first stage to\nupdate the training data in both stages with two different moving window\nprocesses.Experiments were conducted at three wind farms, and the results\ndemonstrate that the model with single input multiple output structure obtains\nbetter forecasting accuracy compared to existing models. In addition, the ridge\nregression method results in a better ensemble model that can further improve\nforecasting accuracy compared to existing machine learning methods. Finally,\nthe proposed two-stage forecasting algorithm can generate more accurate and\nstable results than existing algorithms.",
          "link": "http://arxiv.org/abs/2006.00413",
          "publishedOn": "2021-06-29T01:55:18.318Z",
          "wordCount": 658,
          "title": "Two-stage framework for short-term wind power forecasting using different feature-learning models. (arXiv:2006.00413v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:18.305Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brennan_M/0/1/0/all/0/1\">Matthew Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1\">Guy Bresler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_S/0/1/0/all/0/1\">Samuel B. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1\">Tselil Schramm</a>",
          "description": "Researchers currently use a number of approaches to predict and substantiate\ninformation-computation gaps in high-dimensional statistical estimation\nproblems. A prominent approach is to characterize the limits of restricted\nmodels of computation, which on the one hand yields strong computational lower\nbounds for powerful classes of algorithms and on the other hand helps guide the\ndevelopment of efficient algorithms. In this paper, we study two of the most\npopular restricted computational models, the statistical query framework and\nlow-degree polynomials, in the context of high-dimensional hypothesis testing.\nOur main result is that under mild conditions on the testing problem, the two\nclasses of algorithms are essentially equivalent in power. As corollaries, we\nobtain new statistical query lower bounds for sparse PCA, tensor PCA and\nseveral variants of the planted clique problem.",
          "link": "http://arxiv.org/abs/2009.06107",
          "publishedOn": "2021-06-29T01:55:18.298Z",
          "wordCount": 639,
          "title": "Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent. (arXiv:2009.06107v3 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tranos_D/0/1/0/all/0/1\">Damianos Tranos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proutiere_A/0/1/0/all/0/1\">Alexandre Proutiere</a>",
          "description": "We consider Markov Decision Processes (MDPs) with deterministic transitions\nand study the problem of regret minimization, which is central to the analysis\nand design of optimal learning algorithms. We present logarithmic\nproblem-specific regret lower bounds that explicitly depend on the system\nparameter (in contrast to previous minimax approaches) and thus, truly quantify\nthe fundamental limit of performance achievable by any learning algorithm.\nDeterministic MDPs can be interpreted as graphs and analyzed in terms of their\ncycles, a fact which we leverage in order to identify a class of deterministic\nMDPs whose regret lower bound can be determined numerically. We further\nexemplify this result on a deterministic line search problem, and a\ndeterministic MDP with state-dependent rewards, whose regret lower bounds we\ncan state explicitly. These bounds share similarities with the known\nproblem-specific bound of the multi-armed bandit problem and suggest that\nnavigation on a deterministic MDP need not have an effect on the performance of\na learning algorithm.",
          "link": "http://arxiv.org/abs/2106.14338",
          "publishedOn": "2021-06-29T01:55:18.023Z",
          "wordCount": 585,
          "title": "Regret Analysis in Deterministic Reinforcement Learning. (arXiv:2106.14338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Steve Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_R/0/1/0/all/0/1\">Rangharajan Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "Training large-scale deep neural networks (DNNs) currently requires a\nsignificant amount of energy, leading to serious environmental impacts. One\npromising approach to reduce the energy costs is representing DNNs with\nlow-precision numbers. While it is common to train DNNs with forward and\nbackward propagation in low-precision, training directly over low-precision\nweights, without keeping a copy of weights in high-precision, still remains to\nbe an unsolved problem. This is due to complex interactions between learning\nalgorithms and low-precision number systems. To address this, we jointly design\na low-precision training framework involving a logarithmic number system (LNS)\nand a multiplicative weight update training method, termed LNS-Madam. LNS has a\nhigh dynamic range even in a low-bitwidth setting, leading to high energy\nefficiency and making it relevant for on-board training in energy-constrained\nedge devices. We design LNS to have the flexibility of choosing different bases\nfor weights and gradients, as they usually require different quantization gaps\nand dynamic ranges during training. By drawing the connection between LNS and\nmultiplicative update, LNS-Madam ensures low quantization error during weight\nupdate, leading to a stable convergence even if the bitwidth is limited.\nCompared to using a fixed-point or floating-point number system and training\nwith popular learning algorithms such as SGD and Adam, our joint design with\nLNS and LNS-Madam optimizer achieves better accuracy while requiring smaller\nbitwidth. Notably, with only 5-bit for gradients, the proposed training\nframework achieves accuracy comparable to full-precision state-of-the-art\nmodels such as ResNet-50 and BERT. After conducting energy estimations by\nanalyzing the math datapath units during training, the results show that our\ndesign achieves over 60x energy reduction compared to FP32 on BERT models.",
          "link": "http://arxiv.org/abs/2106.13914",
          "publishedOn": "2021-06-29T01:55:18.010Z",
          "wordCount": 716,
          "title": "Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update. (arXiv:2106.13914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1\">Christian Kroer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "Regret-based algorithms are highly efficient at finding approximate Nash\nequilibria in sequential games such as poker games. However, most regret-based\nalgorithms, including counterfactual regret minimization (CFR) and its\nvariants, rely on iterate averaging to achieve convergence. Inspired by recent\nadvances on last-iterate convergence of optimistic algorithms in zero-sum\nnormal-form games, we study this phenomenon in sequential games, and provide a\ncomprehensive study of last-iterate convergence for zero-sum extensive-form\ngames with perfect recall (EFGs), using various optimistic regret-minimization\nalgorithms over treeplexes. This includes algorithms using the vanilla entropy\nor squared Euclidean norm regularizers, as well as their dilated versions which\nadmit more efficient implementation. In contrast to CFR, we show that all of\nthese algorithms enjoy last-iterate convergence, with some of them even\nconverging exponentially fast. We also provide experiments to further support\nour theoretical results.",
          "link": "http://arxiv.org/abs/2106.14326",
          "publishedOn": "2021-06-29T01:55:17.976Z",
          "wordCount": 557,
          "title": "Last-iterate Convergence in Extensive-Form Games. (arXiv:2106.14326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:17.967Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Boris Rubinstein</a>",
          "description": "Neural networks mapping sequences to sequences (seq2seq) lead to significant\nprogress in machine translation and speech recognition. Their traditional\narchitecture includes two recurrent networks (RNs) followed by a linear\npredictor. In this manuscript we perform analysis of a corresponding algorithm\nand show that the parameters of the RNs of the well trained predictive network\nare not independent of each other. Their dependence can be used to\nsignificantly improve the network effectiveness. The traditional seq2seq\nalgorithms require short term memory of a size proportional to the predicted\nsequence length. This requirement is quite difficult to implement in a\nneuroscience context. We present a novel memoryless algorithm for seq2seq\npredictive networks and compare it to the traditional one in the context of\ntime series prediction. We show that the new algorithm is more robust and makes\npredictions with higher accuracy than the traditional one.",
          "link": "http://arxiv.org/abs/2106.14120",
          "publishedOn": "2021-06-29T01:55:17.961Z",
          "wordCount": 578,
          "title": "On a novel training algorithm for sequence-to-sequence predictive recurrent networks. (arXiv:2106.14120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:17.956Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Viraj Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Normalizing flows are among the most popular paradigms in generative\nmodeling, especially for images, primarily because we can efficiently evaluate\nthe likelihood of a data point. This is desirable both for evaluating the fit\nof a model, and for ease of training, as maximizing the likelihood can be done\nby gradient descent. However, training normalizing flows comes with\ndifficulties as well: models which produce good samples typically need to be\nextremely deep -- which comes with accompanying vanishing/exploding gradient\nproblems. A very related problem is that they are often poorly conditioned:\nsince they are parametrized as invertible maps from $\\mathbb{R}^d \\to\n\\mathbb{R}^d$, and typical training data like images intuitively is\nlower-dimensional, the learned maps often have Jacobians that are close to\nbeing singular.\n\nIn our paper, we tackle representational aspects around depth and\nconditioning of normalizing flows: both for general invertible architectures,\nand for a particular common architecture, affine couplings. We prove that\n$\\Theta(1)$ affine coupling layers suffice to exactly represent a permutation\nor $1 \\times 1$ convolution, as used in GLOW, showing that representationally\nthe choice of partition is not a bottleneck for depth. We also show that\nshallow affine coupling networks are universal approximators in Wasserstein\ndistance if ill-conditioning is allowed, and experimentally investigate related\nphenomena involving padding. Finally, we show a depth lower bound for general\nflow architectures with few neurons per layer and bounded Lipschitz constant.",
          "link": "http://arxiv.org/abs/2010.01155",
          "publishedOn": "2021-06-29T01:55:17.950Z",
          "wordCount": 698,
          "title": "Representational aspects of depth and conditioning in normalizing flows. (arXiv:2010.01155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meirman_T/0/1/0/all/0/1\">Tomer Meirman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1\">Roni Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Gilad Katz</a>",
          "description": "In data systems, activities or events are continuously collected in the field\nto trace their proper executions. Logging, which means recording sequences of\nevents, can be used for analyzing system failures and malfunctions, and\nidentifying the causes and locations of such issues. In our research we focus\non creating an Anomaly detection models for system logs. The task of anomaly\ndetection is identifying unexpected events in dataset, which differ from the\nnormal behavior. Anomaly detection models also assist in data systems analysis\ntasks.\n\nModern systems may produce such a large amount of events monitoring every\nindividual event is not feasible. In such cases, the events are often\naggregated over a fixed period of time, reporting the number of times every\nevent has occurred in that time period. This aggregation facilitates scaling,\nbut requires a different approach for anomaly detection. In this research, we\npresent a thorough analysis of the aggregated data and the relationships\nbetween aggregated events. Based on the initial phase of our research we\npresent graphs representations of our aggregated dataset, which represent the\ndifferent relationships between aggregated instances in the same context.\n\nUsing the graph representation, we propose Multiple-graphs autoencoder MGAE,\na novel convolutional graphs-autoencoder model which exploits the relationships\nof the aggregated instances in our unique dataset. MGAE outperforms standard\ngraph-autoencoder models and the different experiments. With our novel MGAE we\npresent 60% decrease in reconstruction error in comparison to standard graph\nautoencoder, which is expressed in reconstructing high-degree relationships.",
          "link": "http://arxiv.org/abs/2101.04053",
          "publishedOn": "2021-06-29T01:55:17.935Z",
          "wordCount": 738,
          "title": "Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder. (arXiv:2101.04053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14045",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ning_N/0/1/0/all/0/1\">Ning Ning</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qiu_J/0/1/0/all/0/1\">Jinwen Qiu</a>",
          "description": "The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.",
          "link": "http://arxiv.org/abs/2106.14045",
          "publishedOn": "2021-06-29T01:55:17.930Z",
          "wordCount": 574,
          "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models in R. (arXiv:2106.14045v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1\">Angela Meyer</a>",
          "description": "The trend towards larger wind turbines and remote locations of wind farms\nfuels the demand for automated condition monitoring strategies that can reduce\nthe operating cost and avoid unplanned downtime. Normal behaviour modelling has\nbeen introduced to detect anomalous deviations from normal operation based on\nthe turbine's SCADA data. A growing number of machine learning models of the\nnormal behaviour of turbine subsystems are being developed by wind farm\nmanagers to this end. However, these models need to be kept track of, be\nmaintained and require frequent updates. This research explores multi-target\nmodels as a new approach to capturing a wind turbine's normal behaviour. We\npresent an overview of multi-target regression methods, motivate their\napplication and benefits in wind turbine condition monitoring, and assess their\nperformance in a wind farm case study. We find that multi-target models are\nadvantageous in comparison to single-target modelling in that they can reduce\nthe cost and effort of practical condition monitoring without compromising on\nthe accuracy. We also outline some areas of future research.",
          "link": "http://arxiv.org/abs/2012.03074",
          "publishedOn": "2021-06-29T01:55:17.925Z",
          "wordCount": 637,
          "title": "Multi-target normal behaviour models for wind farm condition monitoring. (arXiv:2012.03074v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16955",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cieplinski_T/0/1/0/all/0/1\">Tobiasz Cieplinski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Podlewska_S/0/1/0/all/0/1\">Sabina Podlewska</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanislaw Jastrzebski</a>",
          "description": "Designing compounds with desired properties is a key element of the drug\ndiscovery process. However, measuring progress in the field has been\nchallenging due to the lack of realistic retrospective benchmarks, and the\nlarge cost of prospective validation. To close this gap, we propose a benchmark\nbased on docking, a popular computational method for assessing molecule binding\nto a protein. Concretely, the goal is to generate drug-like molecules that are\nscored highly by SMINA, a popular docking software. We observe that popular\ngraph-based generative models fail to generate molecules with a high docking\nscore when trained using a realistically sized training set. This suggests a\nlimitation of the current incarnation of models for de novo drug design.\nFinally, we propose a simplified version of the benchmark based on a simpler\nscoring function, and show that the tested models are able to partially solve\nit. We release the benchmark as an easy to use package available at\nhttps://github.com/cieplinski-tobiasz/smina-docking-benchmark. We hope that our\nbenchmark will serve as a stepping stone towards the goal of automatically\ngenerating promising drug candidates.",
          "link": "http://arxiv.org/abs/2006.16955",
          "publishedOn": "2021-06-29T01:55:17.919Z",
          "wordCount": 660,
          "title": "We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v4 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beskow_J/0/1/0/all/0/1\">Jonas Beskow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzapfel_A/0/1/0/all/0/1\">Andr&#xe9; Holzapfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexanderson_S/0/1/0/all/0/1\">Simon Alexanderson</a>",
          "description": "Dance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling a\nhigh-dimensional continuous motion signal, conditioned on an audio signal. In\nthis work we make two contributions to tackle this problem. First, we present a\nnovel probabilistic autoregressive architecture that models the distribution\nover future poses with a normalizing flow conditioned on previous poses as well\nas music context, using a multimodal transformer encoder. Second, we introduce\nthe currently largest 3D dance-motion dataset, obtained with a variety of\nmotion-capture technologies, and including both professional and casual\ndancers. Using this dataset, we compare our new model against two baselines,\nvia objective metrics and a user study, and show that both the ability to model\na probability distribution, as well as being able to attend over a large motion\nand music context are necessary to produce interesting, diverse, and realistic\ndance that matches the music.",
          "link": "http://arxiv.org/abs/2106.13871",
          "publishedOn": "2021-06-29T01:55:17.913Z",
          "wordCount": 612,
          "title": "Transflower: probabilistic autoregressive dance generation with multimodal attention. (arXiv:2106.13871v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03432",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Noack_M/0/1/0/all/0/1\">Marcus M. Noack</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sethian_J/0/1/0/all/0/1\">James A. Sethian</a>",
          "description": "Gaussian process regression is a widely-applied method for function\napproximation and uncertainty quantification. The technique has gained\npopularity recently in the machine learning community due to its robustness and\ninterpretability. The mathematical methods we discuss in this paper are an\nextension of the Gaussian-process framework. We are proposing advanced kernel\ndesigns that only allow for functions with certain desirable characteristics to\nbe elements of the reproducing kernel Hilbert space (RKHS) that underlies all\nkernel methods and serves as the sample space for Gaussian process regression.\nThese desirable characteristics reflect the underlying physics; two obvious\nexamples are symmetry and periodicity constraints. In addition, non-stationary\nkernel designs can be defined in the same framework to yield flexible\nmulti-task Gaussian processes. We will show the impact of advanced kernel\ndesigns on Gaussian processes using several synthetic and two scientific data\nsets. The results show that including domain knowledge, communicated through\nadvanced kernel designs, has a significant impact on the accuracy and relevance\nof the function approximation.",
          "link": "http://arxiv.org/abs/2102.03432",
          "publishedOn": "2021-06-29T01:55:17.898Z",
          "wordCount": 616,
          "title": "Advanced Stationary and Non-Stationary Kernel Designs for Domain-Aware Gaussian Processes. (arXiv:2102.03432v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1\">Martin Abadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1\">Gordon Plotkin</a>",
          "description": "Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and,increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n\nIn the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n\nWe establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.",
          "link": "http://arxiv.org/abs/2007.08926",
          "publishedOn": "2021-06-29T01:55:17.892Z",
          "wordCount": 701,
          "title": "Smart Choices and the Selection Monad. (arXiv:2007.08926v5 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Abhishek Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "We introduce learning and planning algorithms for average-reward MDPs,\nincluding 1) the first general proven-convergent off-policy model-free control\nalgorithm without reference states, 2) the first proven-convergent off-policy\nmodel-free prediction algorithm, and 3) the first off-policy learning algorithm\nthat converges to the actual value function rather than to the value function\nplus an offset. All of our algorithms are based on using the\ntemporal-difference error rather than the conventional error when updating the\nestimate of the average reward. Our proof techniques are a slight\ngeneralization of those by Abounadi, Bertsekas, and Borkar (2001). In\nexperiments with an Access-Control Queuing Task, we show some of the\ndifficulties that can arise when using methods that rely on reference states\nand argue that our new algorithms can be significantly easier to use.",
          "link": "http://arxiv.org/abs/2006.16318",
          "publishedOn": "2021-06-29T01:55:17.886Z",
          "wordCount": 605,
          "title": "Learning and Planning in Average-Reward Markov Decision Processes. (arXiv:2006.16318v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08925",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1\">Puyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hai Zhang</a>",
          "description": "In this paper, we are concerned with differentially private {stochastic\ngradient descent (SGD)} algorithms in the setting of stochastic convex\noptimization (SCO). Most of the existing work requires the loss to be Lipschitz\ncontinuous and strongly smooth, and the model parameter to be uniformly\nbounded. However, these assumptions are restrictive as many popular losses\nviolate these conditions including the hinge loss for SVM, the absolute loss in\nrobust regression, and even the least square loss in an unbounded domain. We\nsignificantly relax these restrictive assumptions and establish privacy and\ngeneralization (utility) guarantees for private SGD algorithms using output and\ngradient perturbations associated with non-smooth convex losses. Specifically,\nthe loss function is relaxed to have an $\\alpha$-H\\\"{o}lder continuous gradient\n(referred to as $\\alpha$-H\\\"{o}lder smoothness) which instantiates the\nLipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We\nprove that noisy SGD with $\\alpha$-H\\\"older smooth losses using gradient\nperturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and\nattain optimal excess population risk\n$\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$,\nup to logarithmic terms, with the gradient complexity $ \\mathcal{O}(\nn^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between\n$\\alpha$-H\\\"older smoothness of the loss and the computational complexity for\nprivate SGD with statistically optimal performance. In particular, our results\nindicate that $\\alpha$-H\\\"older smoothness with $\\alpha\\ge {1/2}$ is sufficient\nto guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving\noptimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$",
          "link": "http://arxiv.org/abs/2101.08925",
          "publishedOn": "2021-06-29T01:55:17.881Z",
          "wordCount": 680,
          "title": "Differentially Private SGD with Non-Smooth Losses. (arXiv:2101.08925v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "We consider a sequential setting in which a single dataset of individuals is\nused to perform adaptively-chosen analyses, while ensuring that the\ndifferential privacy loss of each participant does not exceed a pre-specified\nprivacy budget. The standard approach to this problem relies on bounding a\nworst-case estimate of the privacy loss over all individuals and all possible\nvalues of their data, for every single analysis. Yet, in many scenarios this\napproach is overly conservative, especially for \"typical\" data points which\nincur little privacy loss by participation in most of the analyses. In this\nwork, we give a method for tighter privacy loss accounting based on the value\nof a personalized privacy loss estimate for each individual in each analysis.\nTo implement the accounting method we design a filter for R\\'enyi differential\nprivacy. A filter is a tool that ensures that the privacy parameter of a\ncomposed sequence of algorithms with adaptively-chosen privacy parameters does\nnot exceed a pre-specified budget. Our filter is simpler and tighter than the\nknown filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We\napply our results to the analysis of noisy gradient descent and show that\npersonalized accounting can be practical, easy to implement, and can only make\nthe privacy-utility tradeoff tighter.",
          "link": "http://arxiv.org/abs/2008.11193",
          "publishedOn": "2021-06-29T01:55:17.873Z",
          "wordCount": 677,
          "title": "Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13823",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Shangnan_Z/0/1/0/all/0/1\">Zhou Shangnan</a>",
          "description": "Quantum machine learning is an emerging field at the intersection of machine\nlearning and quantum computing. A central quantity for the theoretical\nfoundation of quantum machine learning is the quantum cross entropy. In this\npaper, we present one operational interpretation of this quantity, that the\nquantum cross entropy is the compression rate for sub-optimal quantum source\ncoding. To do so, we give a simple, universal quantum data compression\nprotocol, which is developed based on quantum generalization of variable-length\ncoding, as well as quantum strong typicality.",
          "link": "http://arxiv.org/abs/2106.13823",
          "publishedOn": "2021-06-29T01:55:17.858Z",
          "wordCount": 532,
          "title": "Quantum Data Compression and Quantum Cross Entropy. (arXiv:2106.13823v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:17.851Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.845Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>",
          "description": "Deep equilibrium networks (DEQs) are a new class of models that eschews\ntraditional depth in favor of finding the fixed point of a single nonlinear\nlayer. These models have been shown to achieve performance competitive with the\nstate-of-the-art deep networks while using significantly less memory. Yet they\nare also slower, brittle to architectural choices, and introduce potential\ninstability to the model. In this paper, we propose a regularization scheme for\nDEQ models that explicitly regularizes the Jacobian of the fixed-point update\nequations to stabilize the learning of equilibrium models. We show that this\nregularization adds only minimal computational cost, significantly stabilizes\nthe fixed-point convergence in both forward and backward passes, and scales\nwell to high-dimensional, realistic domains (e.g., WikiText-103 language\nmodeling and ImageNet classification). Using this method, we demonstrate, for\nthe first time, an implicit-depth model that runs with approximately the same\nspeed and level of performance as popular conventional deep networks such as\nResNet-101, while still maintaining the constant memory footprint and\narchitectural simplicity of DEQs. Code is available at\nhttps://github.com/locuslab/deq .",
          "link": "http://arxiv.org/abs/2106.14342",
          "publishedOn": "2021-06-29T01:55:17.829Z",
          "wordCount": 610,
          "title": "Stabilizing Equilibrium Models by Jacobian Regularization. (arXiv:2106.14342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuezhang_L/0/1/0/all/0/1\">Liu Yuezhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "It is well known that artificial neural networks are vulnerable to\nadversarial examples, in which great efforts have been made to improve the\nrobustness. However, such examples are usually imperceptible to humans, and\nthus their effect on biological neural circuits is largely unknown. This paper\nwill investigate the adversarial robustness in a simulated cerebellum, a\nwell-studied supervised learning system in computational neuroscience.\nSpecifically, we propose to study three unique characteristics revealed in the\ncerebellum: (i) network width; (ii) long-term depression on the parallel\nfiber-Purkinje cell synapses; (iii) sparse connectivity in the granule layer,\nand hypothesize that they will be beneficial for improving robustness. To the\nbest of our knowledge, this is the first attempt to examine the adversarial\nrobustness in simulated cerebellum models.\n\nThe results are negative in the experimental phase -- no significant\nimprovements in robustness are discovered from the proposed three mechanisms.\nConsequently, the cerebellum is expected to be vulnerable to adversarial\nexamples as the deep neural networks under batch training. Neuroscientists are\nencouraged to fool the biological system in experiments with adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2012.02976",
          "publishedOn": "2021-06-29T01:55:17.807Z",
          "wordCount": 645,
          "title": "Evaluating adversarial robustness in simulated cerebellum. (arXiv:2012.02976v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashfahani_A/0/1/0/all/0/1\">Andri Ashfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1\">Edwin Lughofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "The common practice of quality monitoring in industry relies on manual\ninspection well-known to be slow, error-prone and operator-dependent. This\nissue raises strong demand for automated real-time quality monitoring developed\nfrom data-driven approaches thus alleviating from operator dependence and\nadapting to various process uncertainties. Nonetheless, current approaches do\nnot take into account the streaming nature of sensory information while relying\nheavily on hand-crafted features making them application-specific. This paper\nproposes the online quality monitoring methodology developed from recently\ndeveloped deep learning algorithms for data streams, Neural Networks with\nDynamically Evolved Capacity (NADINE), namely NADINE++. It features the\nintegration of 1-D and 2-D convolutional layers to extract natural features of\ntime-series and visual data streams captured from sensors and cameras of the\ninjection molding machines from our own project. Real-time experiments have\nbeen conducted where the online quality monitoring task is simulated on the fly\nunder the prequential test-then-train fashion - the prominent data stream\nevaluation protocol. Comparison with the state-of-the-art techniques clearly\nexhibits the advantage of NADINE++ with 4.68\\% improvement on average for the\nquality monitoring task in streaming environments. To support the reproducible\nresearch initiative, codes, results of NADINE++ along with supplementary\nmaterials and injection molding dataset are made available in\n\\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.",
          "link": "http://arxiv.org/abs/2106.13955",
          "publishedOn": "2021-06-29T01:55:17.801Z",
          "wordCount": 659,
          "title": "Autonomous Deep Quality Monitoring in Streaming Environments. (arXiv:2106.13955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalciner_M/0/1/0/all/0/1\">Mustafa Yal&#xe7;&#x131;ner</a>",
          "description": "We consider the $k$-clustering problem with $\\ell_p$-norm cost, which\nincludes $k$-median, $k$-means and $k$-center cost functions, under an\nindividual notion of fairness proposed by Jung et al. [2020]: given a set of\npoints $P$ of size $n$, a set of $k$ centers induces a fair clustering if for\nevery point $v\\in P$, $v$ can find a center among its $n/k$ closest neighbors.\nRecently, Mahabadi and Vakilian [2020] showed how to get a\n$(p^{O(p)},7)$-bicriteria approximation for the problem of fair $k$-clustering\nwith $\\ell_p$-norm cost: every point finds a center within distance at most $7$\ntimes its distance to its $(n/k)$-th closest neighbor and the $\\ell_p$-norm\ncost of the solution is at most $p^{O(p)}$ times the cost of an optimal fair\nsolution. In this work, for any $\\varepsilon>0$, we present an improved $(16^p\n+\\varepsilon,3)$-bicriteria approximation for the fair $k$-clustering with\n$\\ell_p$-norm cost. To achieve our guarantees, we extend the framework of\n[Charikar et al., 2002, Swamy, 2016] and devise a $16^p$-approximation\nalgorithm for the facility location with $\\ell_p$-norm cost under matroid\nconstraint which might be of an independent interest. Besides, our approach\nsuggests a reduction from our individually fair clustering to a clustering with\na group fairness requirement proposed by Kleindessner et al. [2019], which is\nessentially the median matroid problem [Krishnaswamy et al., 2011].",
          "link": "http://arxiv.org/abs/2106.14043",
          "publishedOn": "2021-06-29T01:55:17.792Z",
          "wordCount": 655,
          "title": "Improved Approximation Algorithms for Individually Fair Clustering. (arXiv:2106.14043v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1\">Zhiqiang Que</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Erwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marikar_U/0/1/0/all/0/1\">Umar Marikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_E/0/1/0/all/0/1\">Eric Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngadiuba_J/0/1/0/all/0/1\">Jennifer Ngadiuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_H/0/1/0/all/0/1\">Hamza Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarrestad_T/0/1/0/all/0/1\">Thea Aarrestad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1\">Vladimir Loncar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_S/0/1/0/all/0/1\">Sioni Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1\">Peter Y Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1\">Wayne Luk</a>",
          "description": "This paper presents novel reconfigurable architectures for reducing the\nlatency of recurrent neural networks (RNNs) that are used for detecting\ngravitational waves. Gravitational interferometers such as the LIGO detectors\ncapture cosmic events such as black hole mergers which happen at unknown times\nand of varying durations, producing time-series data. We have developed a new\narchitecture capable of accelerating RNN inference for analyzing time-series\ndata from LIGO detectors. This architecture is based on optimizing the\ninitiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory)\nnetwork, by identifying appropriate reuse factors for each layer. A\ncustomizable template for this architecture has been designed, which enables\nthe generation of low-latency FPGA designs with efficient resource utilization\nusing high-level synthesis tools. The proposed approach has been evaluated\nbased on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA.\nExperimental results show that with balanced II, the number of DSPs can be\nreduced up to 42% while achieving the same IIs. When compared to other\nFPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower\nlatency.",
          "link": "http://arxiv.org/abs/2106.14089",
          "publishedOn": "2021-06-29T01:55:17.786Z",
          "wordCount": 655,
          "title": "Accelerating Recurrent Neural Networks for Gravitational Wave Experiments. (arXiv:2106.14089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:17.780Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chenzhuang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tiejun Li</a>",
          "description": "In vision-based reinforcement learning (RL) tasks, it is prevalent to assign\nthe auxiliary task with a surrogate self-supervised loss so as to obtain more\nsemantic representations and improve sample efficiency. However, abundant\ninformation in self-supervised auxiliary tasks has been disregarded, since the\nrepresentation learning part and the decision-making part are separated. To\nsufficiently utilize information in the auxiliary task, we present a simple yet\neffective idea to employ self-supervised loss as an intrinsic reward, called\nIntrinsically Motivated Self-Supervised learning in Reinforcement learning\n(IM-SSR). We formally show that the self-supervised loss can be decomposed as\nexploration for novel states and robustness improvement from nuisance\nelimination. IM-SSR can be effortlessly plugged into any reinforcement learning\nwith self-supervised auxiliary objectives with nearly no additional cost.\nCombined with IM-SSR, the previous underlying algorithms achieve salient\nimprovements on both sample efficiency and generalization in various\nvision-based robotics tasks from the DeepMind Control Suite, especially when\nthe reward signal is sparse.",
          "link": "http://arxiv.org/abs/2106.13970",
          "publishedOn": "2021-06-29T01:55:17.762Z",
          "wordCount": 587,
          "title": "Intrinsically Motivated Self-supervised Learning in Reinforcement Learning. (arXiv:2106.13970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:17.755Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David W. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan J. Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This work considers predicting the relational structure of a hypergraph for a\ngiven set of vertices, as common for applications in particle physics,\nbiological systems and other complex combinatorial problems. A problem arises\nfrom the number of possible multi-way relationships, or hyperedges, scaling in\n$\\mathcal{O}(2^n)$ for a set of $n$ elements. Simply storing an indicator\ntensor for all relationships is already intractable for moderately sized $n$,\nprompting previous approaches to restrict the number of vertices a hyperedge\nconnects. Instead, we propose a recurrent hypergraph neural network that\npredicts the incidence matrix by iteratively refining an initial guess of the\nsolution. We leverage the property that most hypergraphs of interest are\nsparsely connected and reduce the memory requirement to $\\mathcal{O}(nk)$,\nwhere $k$ is the maximum number of positive edges, i.e., edges that actually\nexist. In order to counteract the linearly growing memory cost from training a\nlengthening sequence of refinement steps, we further propose an algorithm that\napplies backpropagation through time on randomly sampled subsequences. We\nempirically show that our method can match an increase in the intrinsic\ncomplexity without a performance decrease and demonstrate superior performance\ncompared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.13919",
          "publishedOn": "2021-06-29T01:55:17.749Z",
          "wordCount": 612,
          "title": "Recurrently Predicting Hypergraphs. (arXiv:2106.13919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:17.742Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:17.726Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atigh_M/0/1/0/all/0/1\">Mina Ghadimi Atigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Ressel_M/0/1/0/all/0/1\">Martin Keller-Ressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>",
          "description": "Hyperbolic space has become a popular choice of manifold for representation\nlearning of arbitrary data, from tree-like structures and text to graphs.\nBuilding on the success of deep learning with prototypes in Euclidean and\nhyperspherical spaces, a few recent works have proposed hyperbolic prototypes\nfor classification. Such approaches enable effective learning in\nlow-dimensional output spaces and can exploit hierarchical relations amongst\nclasses, but require privileged information about class labels to position the\nhyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning.\nThe main idea behind our approach is to position prototypes on the ideal\nboundary of the Poincare ball, which does not require prior label knowledge. To\nbe able to compute proximities to ideal prototypes, we introduce the penalised\nBusemann loss. We provide theory supporting the use of ideal prototypes and the\nproposed loss by proving its equivalence to logistic regression in the\none-dimensional case. Empirically, we show that our approach provides a natural\ninterpretation of classification confidence, while outperforming recent\nhyperspherical and hyperbolic prototype approaches.",
          "link": "http://arxiv.org/abs/2106.14472",
          "publishedOn": "2021-06-29T01:55:17.720Z",
          "wordCount": 593,
          "title": "Hyperbolic Busemann Learning with Ideal Prototypes. (arXiv:2106.14472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. Those HVAC\nsystems in modern smart buildings rely on real-time sensor readings, which in\npractice often suffer from various faults and could also be vulnerable to\nmalicious attacks. Such faulty sensor inputs may lead to the violation of\nindoor environment requirements (e.g., temperature, humidity, etc.) and the\nincrease of energy consumption. While many model-based approaches have been\nproposed in the literature for building HVAC control, it is costly to develop\naccurate physical models for ensuring their performance and even more\nchallenging to address the impact of sensor faults. In this work, we present a\nnovel learning-based framework for sensor fault-tolerant HVAC control, which\nincludes three deep learning based components for 1) generating temperature\nproposals with the consideration of possible sensor faults, 2) selecting one of\nthe proposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive numerical experiments, we\ndemonstrate that the proposed fault-tolerant HVAC control framework can\nsignificantly reduce building temperature violations under a variety of sensor\nfault patterns while maintaining energy efficiency.",
          "link": "http://arxiv.org/abs/2106.14144",
          "publishedOn": "2021-06-29T01:55:17.687Z",
          "wordCount": 675,
          "title": "Model-assisted Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control. (arXiv:2106.14144v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "In this letter, we propose a multi-task over-theair federated learning\n(MOAFL) framework, where multiple learning tasks share edge devices for data\ncollection and learning models under the coordination of a edge server (ES).\nSpecially, the model updates for all the tasks are transmitted and\nsuperpositioned concurrently over a non-orthogonal uplink channel via\nover-the-air computation, and the aggregation results of all the tasks are\nreconstructed at the ES through an extended version of the turbo compressed\nsensing algorithm. Both the convergence analysis and numerical results\ndemonstrate that the MOAFL framework can significantly reduce the uplink\nbandwidth consumption of multiple tasks without causing substantial learning\nperformance degradation.",
          "link": "http://arxiv.org/abs/2106.14229",
          "publishedOn": "2021-06-29T01:55:17.679Z",
          "wordCount": 543,
          "title": "Multi-task Over-the-Air Federated Learning: A Non-Orthogonal Transmission Approach. (arXiv:2106.14229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matheny_M/0/1/0/all/0/1\">Michael Matheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M. Phillips</a>",
          "description": "Consider the geometric range space $(X, \\mathcal{H}_d)$ where $X \\subset\n\\mathbb{R}^d$ and $\\mathcal{H}_d$ is the set of ranges defined by\n$d$-dimensional halfspaces. In this setting we consider that $X$ is the\ndisjoint union of a red and blue set. For each halfspace $h \\in \\mathcal{H}_d$\ndefine a function $\\Phi(h)$ that measures the \"difference\" between the fraction\nof red and fraction of blue points which fall in the range $h$. In this context\nthe maximum discrepancy problem is to find the $h^* = \\arg \\max_{h \\in (X,\n\\mathcal{H}_d)} \\Phi(h)$. We aim to instead find an $\\hat{h}$ such that\n$\\Phi(h^*) - \\Phi(\\hat{h}) \\le \\varepsilon$. This is the central problem in\nlinear classification for machine learning, in spatial scan statistics for\nspatial anomaly detection, and shows up in many other areas. We provide a\nsolution for this problem in $O(|X| + (1/\\varepsilon^d) \\log^4\n(1/\\varepsilon))$ time, which improves polynomially over the previous best\nsolutions. For $d=2$ we show that this is nearly tight through conditional\nlower bounds. For different classes of $\\Phi$ we can either provide a\n$\\Omega(|X|^{3/2 - o(1)})$ time lower bound for the exact solution with a\nreduction to APSP, or an $\\Omega(|X| + 1/\\varepsilon^{2-o(1)})$ lower bound for\nthe approximate solution with a reduction to 3SUM.\n\nA key technical result is a $\\varepsilon$-approximate halfspace range\ncounting data structure of size $O(1/\\varepsilon^d)$ with $O(\\log\n(1/\\varepsilon))$ query time, which we can build in $O(|X| + (1/\\varepsilon^d)\n\\log^4 (1/\\varepsilon))$ time.",
          "link": "http://arxiv.org/abs/2106.13851",
          "publishedOn": "2021-06-29T01:55:17.672Z",
          "wordCount": 658,
          "title": "Approximate Maximum Halfspace Discrepancy. (arXiv:2106.13851v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>",
          "description": "Contextual Bandits find important use cases in various real-life scenarios\nsuch as online advertising, recommendation systems, healthcare, etc. However,\nmost of the algorithms use flat feature vectors to represent context whereas,\nin the real world, there is a varying number of objects and relations among\nthem to model in the context. For example, in a music recommendation system,\nthe user context contains what music they listen to, which artists create this\nmusic, the artist albums, etc. Adding richer relational context representations\nalso introduces a much larger context space making exploration-exploitation\nharder. To improve the efficiency of exploration-exploitation knowledge about\nthe context can be infused to guide the exploration-exploitation strategy.\nRelational context representations allow a natural way for humans to specify\nknowledge owing to their descriptive nature. We propose an adaptation of\nKnowledge Infused Policy Gradients to the Contextual Bandit setting and a novel\nKnowledge Infused Policy Gradients Upper Confidence Bound algorithm and perform\nan experimental analysis of a simulated music recommendation dataset and\nvarious real-life datasets where expert knowledge can drastically reduce the\ntotal regret and where it cannot.",
          "link": "http://arxiv.org/abs/2106.13895",
          "publishedOn": "2021-06-29T01:55:17.649Z",
          "wordCount": 629,
          "title": "Knowledge Infused Policy Gradients with Upper Confidence Bound for Relational Bandits. (arXiv:2106.13895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szot_A/0/1/0/all/0/1\">Andrew Szot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alex Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yili Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1\">John Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestre_N/0/1/0/all/0/1\">Noah Maestre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrus_V/0/1/0/all/0/1\">Vladimir Vondrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharur_S/0/1/0/all/0/1\">Sameer Dharur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1\">Franziska Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>",
          "description": "We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual\nrobots in interactive 3D environments and complex physics-enabled scenarios. We\nmake comprehensive contributions to all levels of the embodied AI stack - data,\nsimulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an\nartist-authored, annotated, reconfigurable 3D dataset of apartments (matching\nreal spaces) with articulated objects (e.g. cabinets and drawers that can\nopen/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with\nspeeds exceeding 25,000 simulation steps per second (850x real-time) on an\n8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home\nAssistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy\nthe house, prepare groceries, set the table) that test a range of mobile\nmanipulation capabilities. These large-scale engineering contributions allow us\nto systematically compare deep reinforcement learning (RL) at scale and\nclassical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with\nan emphasis on generalization to new objects, receptacles, and layouts. We find\nthat (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a\nhierarchy with independent skills suffers from 'hand-off problems', and (3) SPA\npipelines are more brittle than RL policies.",
          "link": "http://arxiv.org/abs/2106.14405",
          "publishedOn": "2021-06-29T01:55:17.630Z",
          "wordCount": 659,
          "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat. (arXiv:2106.14405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morad_S/0/1/0/all/0/1\">Steven D. Morad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_S/0/1/0/all/0/1\">Stephan Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1\">Amanda Prorok</a>",
          "description": "Solving partially-observable Markov decision processes (POMDPs) is critical\nwhen applying deep reinforcement learning (DRL) to real-world robotics\nproblems, where agents have an incomplete view of the world. We present graph\nconvolutional memory (GCM) for solving POMDPs using deep reinforcement\nlearning. Unlike recurrent neural networks (RNNs) or transformers, GCM embeds\ndomain-specific priors into the memory recall process via a knowledge graph. By\nencapsulating priors in the graph, GCM adapts to specific tasks but remains\napplicable to any DRL task. Using graph convolutions, GCM extracts hierarchical\ngraph features, analogous to image features in a convolutional neural network\n(CNN). We show GCM outperforms long short-term memory (LSTM), gated\ntransformers for reinforcement learning (GTrXL), and differentiable neural\ncomputers (DNCs) on control, long-term non-sequential recall, and 3D navigation\ntasks while using significantly fewer parameters.",
          "link": "http://arxiv.org/abs/2106.14117",
          "publishedOn": "2021-06-29T01:55:17.545Z",
          "wordCount": 562,
          "title": "Graph Convolutional Memory for Deep Reinforcement Learning. (arXiv:2106.14117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Ruiyuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaozhe Hu</a>",
          "description": "Polynomial functions have plenty of useful analytical properties, but they\nare rarely used as learning models because their function class is considered\nto be restricted. This work shows that when trained properly polynomial\nfunctions can be strong learning models. Particularly this work constructs\npolynomial feedforward neural networks using the product activation, a new\nactivation function constructed from multiplications. The new neural network is\na polynomial function and provides accurate control of its polynomial order. It\ncan be trained by standard training techniques such as batch normalization and\ndropout. This new feedforward network covers several previous polynomial models\nas special cases. Compared with common feedforward neural networks, the\npolynomial feedforward network has closed-form calculations of a few\ninteresting quantities, which are very useful in Bayesian learning. In a series\nof regression and classification tasks in the empirical study, the proposed\nmodel outperforms previous polynomial models.",
          "link": "http://arxiv.org/abs/2106.13834",
          "publishedOn": "2021-06-29T01:55:17.504Z",
          "wordCount": 590,
          "title": "Ladder Polynomial Neural Networks. (arXiv:2106.13834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1\">Omar Besbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_Y/0/1/0/all/0/1\">Yuri Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobel_I/0/1/0/all/0/1\">Ilan Lobel</a>",
          "description": "We study the problems of offline and online contextual optimization with\nfeedback information, where instead of observing the loss, we observe,\nafter-the-fact, the optimal action an oracle with full knowledge of the\nobjective function would have taken. We aim to minimize regret, which is\ndefined as the difference between our losses and the ones incurred by an\nall-knowing oracle. In the offline setting, the decision-maker has information\navailable from past periods and needs to make one decision, while in the online\nsetting, the decision-maker optimizes decisions dynamically over time based a\nnew set of feasible actions and contextual functions in each period. For the\noffline setting, we characterize the optimal minimax policy, establishing the\nperformance that can be achieved as a function of the underlying geometry of\nthe information induced by the data. In the online setting, we leverage this\ngeometric characterization to optimize the cumulative regret. We develop an\nalgorithm that yields the first regret bound for this problem that is\nlogarithmic in the time horizon.",
          "link": "http://arxiv.org/abs/2106.14015",
          "publishedOn": "2021-06-29T01:55:17.498Z",
          "wordCount": 603,
          "title": "Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Tobias Sebastian Finn</a>",
          "description": "Ensemble data from Earth system models has to be calibrated and\npost-processed. I propose a novel member-by-member post-processing approach\nwith neural networks. I bridge ideas from ensemble data assimilation with\nself-attention, resulting into the self-attentive ensemble transformer. Here,\ninteractions between ensemble members are represented as additive and dynamic\nself-attentive part. As proof-of-concept, global ECMWF ensemble forecasts are\nregressed to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate\nthat the ensemble transformer can calibrate the ensemble spread and extract\nadditional information from the ensemble. Furthermore, the ensemble transformer\ndirectly outputs multivariate and spatially-coherent ensemble members.\nTherefore, self-attention and the transformer technique can be a missing piece\nfor a member-by-member post-processing of ensemble data with neural networks.",
          "link": "http://arxiv.org/abs/2106.13924",
          "publishedOn": "2021-06-29T01:55:17.378Z",
          "wordCount": 581,
          "title": "Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chang-E Ren</a>",
          "description": "Broad learning system (BLS) has been proposed for a few years. It\ndemonstrates an effective learning capability for many classification and\nregression problems. However, BLS and its improved versions are mainly used to\ndeal with unsupervised, supervised and semi-supervised learning problems in a\nsingle domain. As far as we know, a little attention is paid to the\ncross-domain learning ability of BLS. Therefore, we introduce BLS into the\nfield of transfer learning and propose a novel algorithm called domain\nadaptation broad learning system based on locally linear embedding (DABLS-LLE).\nThe proposed algorithm can learn a robust classification model by using a small\npart of labeled data from the target domain and all labeled data from the\nsource domain. The proposed algorithm inherits the computational efficiency and\nlearning capability of BLS. Experiments on benchmark dataset\n(Office-Caltech-10) verify the effectiveness of our approach. The results show\nthat our approach can get better classification accuracy with less running time\nthan many existing transfer learning approaches. It shows that our approach can\nbring a new superiority for BLS.",
          "link": "http://arxiv.org/abs/2106.14367",
          "publishedOn": "2021-06-29T01:55:17.311Z",
          "wordCount": 603,
          "title": "Domain Adaptation Broad Learning System Based on Locally Linear Embedding. (arXiv:2106.14367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Surajit Chaudhuri</a>",
          "description": "Recent work has made significant progress in helping users to automate single\ndata preparation steps, such as string-transformations and table-manipulation\noperators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to\nautomate multiple such steps end-to-end, by synthesizing complex data pipelines\nwith both string transformations and table-manipulation operators. We propose a\nnovel \"by-target\" paradigm that allows users to easily specify the desired\npipeline, which is a significant departure from the traditional by-example\nparadigm. Using by-target, users would provide input tables (e.g., csv or json\nfiles), and point us to a \"target table\" (e.g., an existing database table or\nBI dashboard) to demonstrate how the output from the desired pipeline would\nschematically \"look like\". While the problem is seemingly underspecified, our\nunique insight is that implicit table constraints such as FDs and keys can be\nexploited to significantly constrain the space to make the problem tractable.\nWe develop an Auto-Pipeline system that learns to synthesize pipelines using\nreinforcement learning and search. Experiments on large numbers of real\npipelines crawled from GitHub suggest that Auto-Pipeline can successfully\nsynthesize 60-70% of these complex pipelines (up to 10 steps) in 10-20 seconds\non average.",
          "link": "http://arxiv.org/abs/2106.13861",
          "publishedOn": "2021-06-29T01:55:17.305Z",
          "wordCount": 624,
          "title": "AutoPipeline: Synthesize Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jothimurugan_K/0/1/0/all/0/1\">Kishor Jothimurugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Suguman Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1\">Rajeev Alur</a>",
          "description": "We study the problem of learning control policies for complex tasks given by\nlogical specifications. Recent approaches automatically generate a reward\nfunction from a given specification and use a suitable reinforcement learning\nalgorithm to learn a policy that maximizes the expected reward. These\napproaches, however, scale poorly to complex tasks that require high-level\nplanning. In this work, we develop a compositional learning approach, called\nDiRL, that interleaves high-level planning and reinforcement learning. First,\nDiRL encodes the specification as an abstract graph; intuitively, vertices and\nedges of the graph correspond to regions of the state space and simpler\nsub-tasks, respectively. Our approach then incorporates reinforcement learning\nto learn neural network policies for each edge (sub-task) within a\nDijkstra-style planning algorithm to compute a high-level plan in the graph. An\nevaluation of the proposed approach on a set of challenging control benchmarks\nwith continuous state and action spaces demonstrates that it outperforms\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.13906",
          "publishedOn": "2021-06-29T01:55:16.958Z",
          "wordCount": 582,
          "title": "Compositional Reinforcement Learning from Logical Specifications. (arXiv:2106.13906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:16.935Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1\">Desmond J. Higham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woldegeorgis_E/0/1/0/all/0/1\">Eliyas Woldegeorgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>",
          "description": "We develop and study new adversarial perturbations that enable an attacker to\ngain control over decisions in generic Artificial Intelligence (AI) systems\nincluding deep learning neural networks. In contrast to adversarial data\nmodification, the attack mechanism we consider here involves alterations to the\nAI system itself. Such a stealth attack could be conducted by a mischievous,\ncorrupt or disgruntled member of a software development team. It could also be\nmade by those wishing to exploit a \"democratization of AI\" agenda, where\nnetwork architectures and trained parameter sets are shared publicly. Building\non work by [Tyukin et al., International Joint Conference on Neural Networks,\n2020], we develop a range of new implementable attack strategies with\naccompanying analysis, showing that with high probability a stealth attack can\nbe made transparent, in the sense that system performance is unchanged on a\nfixed validation set which is unknown to the attacker, while evoking any\ndesired output on a trigger input of interest. The attacker only needs to have\nestimates of the size of the validation set and the spread of the AI's relevant\nlatent space. In the case of deep learning neural networks, we show that a one\nneuron attack is possible - a modification to the weights and bias associated\nwith a single neuron - revealing a vulnerability arising from\nover-parameterization. We illustrate these concepts in a realistic setting.\nGuided by the theory and computational results, we also propose strategies to\nguard against stealth attacks.",
          "link": "http://arxiv.org/abs/2106.13997",
          "publishedOn": "2021-06-29T01:55:16.889Z",
          "wordCount": 688,
          "title": "The Feasibility and Inevitability of Stealth Attacks. (arXiv:2106.13997v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kou_W/0/1/0/all/0/1\">Wenjun Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_D/0/1/0/all/0/1\">Dustin A. Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_A/0/1/0/all/0/1\">Alexandra J. Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnan_E/0/1/0/all/0/1\">Erica N. Donnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schauer_J/0/1/0/all/0/1\">Jacob M. Schauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandolfino_J/0/1/0/all/0/1\">John E. Pandolfino</a>",
          "description": "High-resolution manometry (HRM) is the primary procedure used to diagnose\nesophageal motility disorders. Its interpretation and classification includes\nan initial evaluation of swallow-level outcomes and then derivation of a\nstudy-level diagnosis based on Chicago Classification (CC), using a tree-like\nalgorithm. This diagnostic approach on motility disordered using HRM was\nmirrored using a multi-stage modeling framework developed using a combination\nof various machine learning approaches. Specifically, the framework includes\ndeep-learning models at the swallow-level stage and feature-based machine\nlearning models at the study-level stage. In the swallow-level stage, three\nmodels based on convolutional neural networks (CNNs) were developed to predict\nswallow type, swallow pressurization, and integrated relaxation pressure (IRP).\nAt the study-level stage, model selection from families of the\nexpert-knowledge-based rule models, xgboost models and artificial neural\nnetwork(ANN) models were conducted, with the latter two model designed and\naugmented with motivation from the export knowledge. A simple model-agnostic\nstrategy of model balancing motivated by Bayesian principles was utilized,\nwhich gave rise to model averaging weighted by precision scores. The averaged\n(blended) models and individual models were compared and evaluated, of which\nthe best performance on test dataset is 0.81 in top-1 prediction, 0.92 in top-2\npredictions. This is the first artificial-intelligence-style model to\nautomatically predict CC diagnosis of HRM study from raw multi-swallow data.\nMoreover, the proposed modeling framework could be easily extended to\nmulti-modal tasks, such as diagnosis of esophageal patients based on clinical\ndata from both HRM and functional luminal imaging probe panometry (FLIP).",
          "link": "http://arxiv.org/abs/2106.13869",
          "publishedOn": "2021-06-29T01:55:16.802Z",
          "wordCount": 693,
          "title": "A multi-stage machine learning model on diagnosis of esophageal manometry. (arXiv:2106.13869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13867",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jiameng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "We propose POLAR, a \\textbf{pol}ynomial \\textbf{ar}ithmetic framework that\nleverages polynomial overapproximations with interval remainders for\nbounded-time reachability analysis of neural network-controlled systems\n(NNCSs). Compared with existing arithmetic approaches that use standard Taylor\nmodels, our framework uses a novel approach to iteratively overapproximate the\nneuron output ranges layer-by-layer with a combination of Bernstein polynomial\ninterpolation for continuous activation functions and Taylor model arithmetic\nfor the other operations. This approach can overcome the main drawback in the\nstandard Taylor model arithmetic, i.e. its inability to handle functions that\ncannot be well approximated by Taylor polynomials, and significantly improve\nthe accuracy and efficiency of reachable states computation for NNCSs. To\nfurther tighten the overapproximation, our method keeps the Taylor model\nremainders symbolic under the linear mappings when estimating the output range\nof a neural network. We show that POLAR can be seamlessly integrated with\nexisting Taylor model flowpipe construction techniques, and demonstrate that\nPOLAR significantly outperforms the current state-of-the-art techniques on a\nsuite of benchmarks.",
          "link": "http://arxiv.org/abs/2106.13867",
          "publishedOn": "2021-06-29T01:55:16.795Z",
          "wordCount": 608,
          "title": "POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1\">Yihuang Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chiu_Y/0/1/0/all/0/1\">Yi-Wen Chiu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_M/0/1/0/all/0/1\">Ming-Yen Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Su_F/0/1/0/all/0/1\">Fang-yi Su</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Tai Huang</a>",
          "description": "Machine Learning (ML) and its applications have been transforming our lives\nbut it is also creating issues related to the development of fair, accountable,\ntransparent, and ethical Artificial Intelligence. As the ML models are not\nfully comprehensible yet, it is obvious that we still need humans to be part of\nalgorithmic decision-making processes. In this paper, we consider a ML\nframework that may accelerate model learning and improve its interpretability\nby incorporating human experts into the model learning loop. We propose a novel\nhuman-in-the-loop ML framework aimed at dealing with learning problems that the\ncost of data annotation is high and the lack of appropriate data to model the\nassociation between the target tasks and the input features. With an\napplication to precision dosing, our experimental results show that the\napproach can learn interpretable rules from data and may potentially lower\nexperts' workload by replacing data annotation with rule representation\nediting. The approach may also help remove algorithmic bias by introducing\nexperts' feedback into the iterative model learning process.",
          "link": "http://arxiv.org/abs/2106.14384",
          "publishedOn": "2021-06-29T01:55:16.783Z",
          "wordCount": 605,
          "title": "Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning. (arXiv:2106.14384v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yi Xiang Marcus Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_P/0/1/0/all/0/1\">Penny Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "Few-shot classifiers excel under limited training samples, making them useful\nin applications with sparsely user-provided labels. Their unique relative\nprediction setup offers opportunities for novel attacks, such as targeting\nsupport sets required to categorise unseen test samples, which are not\navailable in other machine learning setups. In this work, we propose a\ndetection strategy to identify adversarial support sets, aimed at destroying\nthe understanding of a few-shot classifier for a certain class. We achieve this\nby introducing the concept of self-similarity of a support set and by employing\nfiltering of supports. Our method is attack-agnostic, and we are the first to\nexplore adversarial detection for support sets of few-shot classifiers to the\nbest of our knowledge. Our evaluation of the miniImagenet (MI) and CUB datasets\nexhibits good attack detection performance despite conceptual simplicity,\nshowing high AUROC scores. We show that self-similarity and filtering for\nadversarial detection can be paired with other filtering functions,\nconstituting a generalisable concept.",
          "link": "http://arxiv.org/abs/2012.06330",
          "publishedOn": "2021-06-29T01:55:16.777Z",
          "wordCount": 651,
          "title": "Detection of Adversarial Supports in Few-shot Classifiers Using Self-Similarity and Filtering. (arXiv:2012.06330v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Modhe_N/0/1/0/all/0/1\">Nirbhay Modhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1\">Harish Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>",
          "description": "Model-based Reinforcement Learning (MBRL) algorithms have been traditionally\ndesigned with the goal of learning accurate dynamics of the environment. This\nintroduces a mismatch between the objectives of model-learning and the overall\nlearning problem of finding an optimal policy. Value-aware model learning, an\nalternative model-learning paradigm to maximum likelihood, proposes to inform\nmodel-learning through the value function of the learnt policy. While this\nparadigm is theoretically sound, it does not scale beyond toy settings. In this\nwork, we propose a novel value-aware objective that is an upper bound on the\nabsolute performance difference of a policy across two models. Further, we\npropose a general purpose algorithm that modifies the standard MBRL pipeline --\nenabling learning with value aware objectives. Our proposed objective, in\nconjunction with this algorithm, is the first successful instantiation of\nvalue-aware MBRL on challenging continuous control environments, outperforming\nprevious value-aware objectives and with competitive performance w.r.t.\nMLE-based MBRL approaches.",
          "link": "http://arxiv.org/abs/2106.14080",
          "publishedOn": "2021-06-29T01:55:16.755Z",
          "wordCount": 585,
          "title": "Model-Advantage Optimization for Model-Based Reinforcement Learning. (arXiv:2106.14080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:16.741Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13814",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Campos_E/0/1/0/all/0/1\">E. Campos</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rabinovich_D/0/1/0/all/0/1\">D. Rabinovich</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akshay_V/0/1/0/all/0/1\">V. Akshay</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Biamonte_J/0/1/0/all/0/1\">J. Biamonte</a>",
          "description": "Quantum Approximate Optimisation (QAOA) is the most studied gate based\nvariational quantum algorithm today. We train QAOA one layer at a time to\nmaximize overlap with an $n$ qubit target state. Doing so we discovered that\nsuch training always saturates -- called \\textit{training saturation} -- at\nsome depth $p^*$, meaning that past a certain depth, overlap can not be\nimproved by adding subsequent layers. We formulate necessary conditions for\nsaturation. Numerically, we find layerwise QAOA reaches its maximum overlap at\ndepth $p^*=n$. The addition of coherent dephasing errors to training removes\nsaturation, recovering robustness to layerwise training. This study sheds new\nlight on the performance limitations and prospects of QAOA.",
          "link": "http://arxiv.org/abs/2106.13814",
          "publishedOn": "2021-06-29T01:55:16.734Z",
          "wordCount": 554,
          "title": "Training Saturation in Layerwise Quantum Approximate Optimisation. (arXiv:2106.13814v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1\">Micha&#xeb;l Fanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardenet_R/0/1/0/all/0/1\">R&#xe9;mi Bardenet</a>",
          "description": "Determinantal Point Process (DPPs) are statistical models for repulsive point\npatterns. Both sampling and inference are tractable for DPPs, a rare feature\namong models with negative dependence that explains their popularity in machine\nlearning and spatial statistics. Parametric and nonparametric inference methods\nhave been proposed in the finite case, i.e. when the point patterns live in a\nfinite ground set. In the continuous case, only parametric methods have been\ninvestigated, while nonparametric maximum likelihood for DPPs -- an\noptimization problem over trace-class operators -- has remained an open\nquestion. In this paper, we show that a restricted version of this maximum\nlikelihood (MLE) problem falls within the scope of a recent representer theorem\nfor nonnegative functions in an RKHS. This leads to a finite-dimensional\nproblem, with strong statistical ties to the original MLE. Moreover, we\npropose, analyze, and demonstrate a fixed point algorithm to solve this\nfinite-dimensional problem. Finally, we also provide a controlled estimate of\nthe correlation kernel of the DPP, thus providing more interpretability.",
          "link": "http://arxiv.org/abs/2106.14210",
          "publishedOn": "2021-06-29T01:55:16.725Z",
          "wordCount": 600,
          "title": "Nonparametric estimation of continuous DPPs with kernel methods. (arXiv:2106.14210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fubing_M/0/1/0/all/0/1\">Mao Fubing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiwei_W/0/1/0/all/0/1\">Weng Weiwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "Learning from streaming tasks leads a model to catastrophically erase unique\nexperiences it absorbs from previous episodes. While regularization techniques\nsuch as LWF, SI, EWC have proven themselves as an effective avenue to overcome\nthis issue by constraining important parameters of old tasks from changing when\naccepting new concepts, these approaches do not exploit common information of\neach task which can be shared to existing neurons. As a result, they do not\nscale well to large-scale problems since the parameter importance variables\nquickly explode. An Inter-Task Synaptic Mapping (ISYANA) is proposed here to\nunderpin knowledge retention for continual learning. ISYANA combines\ntask-to-neuron relationship as well as concept-to-concept relationship such\nthat it prevents a neuron to embrace distinct concepts while merely accepting\nrelevant concept. Numerical study in the benchmark continual learning problems\nhas been carried out followed by comparison against prominent continual\nlearning algorithms. ISYANA exhibits competitive performance compared to state\nof the arts. Codes of ISYANA is made available in\n\\url{https://github.com/ContinualAL/ISYANAKBS}.",
          "link": "http://arxiv.org/abs/2106.13954",
          "publishedOn": "2021-06-29T01:55:16.709Z",
          "wordCount": 611,
          "title": "Continual Learning via Inter-Task Synaptic Mapping. (arXiv:2106.13954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogga_P/0/1/0/all/0/1\">Pradeep Dogga</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraman_A/0/1/0/all/0/1\">Anirudh Sivaraman</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1\">Shiv Kumar Saini</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Varghese_G/0/1/0/all/0/1\">George Varghese</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a> (2) ((1) UCLA, (2) Princeton University, (3) NYU, (4) Adobe Research, India)",
          "description": "A major difficulty in debugging distributed systems lies in manually\ndetermining which of the many available debugging tools to use and how to query\nits logs. Our own study of a production debugging workflow confirms the\nmagnitude of this burden. This paper explores whether a machine-learning model\ncan assist developers in distributed systems debugging. We present Revelio, a\ndebugging assistant which takes user reports and system logs as input, and\noutputs debugging queries that developers can use to find a bug's root cause.\nThe key challenges lie in (1) combining inputs of different types (e.g.,\nnatural language reports and quantitative logs) and (2) generalizing to unseen\nfaults. Revelio addresses these by employing deep neural networks to uniformly\nembed diverse input sources and potential queries into a high-dimensional\nvector space. In addition, it exploits observations from production systems to\nfactorize query generation into two computationally and statistically simpler\nlearning tasks. To evaluate Revelio, we built a testbed with multiple\ndistributed applications and debugging tools. By injecting faults and training\non logs and reports from 800 Mechanical Turkers, we show that Revelio includes\nthe most helpful query in its predicted list of top-3 relevant queries 96% of\nthe time. Our developer study confirms the utility of Revelio.",
          "link": "http://arxiv.org/abs/2106.14347",
          "publishedOn": "2021-06-29T01:55:16.703Z",
          "wordCount": 665,
          "title": "Revelio: ML-Generated Debugging Queries for Distributed Systems. (arXiv:2106.14347v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:16.691Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14320",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hajimohammadi_Z/0/1/0/all/0/1\">Zeinab Hajimohammadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Parand_K/0/1/0/all/0/1\">Kourosh Parand</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Various phenomena in biology, physics, and engineering are modeled by\ndifferential equations. These differential equations including partial\ndifferential equations and ordinary differential equations can be converted and\nrepresented as integral equations. In particular, Volterra Fredholm Hammerstein\nintegral equations are the main type of these integral equations and\nresearchers are interested in investigating and solving these equations. In\nthis paper, we propose Legendre Deep Neural Network (LDNN) for solving\nnonlinear Volterra Fredholm Hammerstein integral equations (VFHIEs). LDNN\nutilizes Legendre orthogonal polynomials as activation functions of the Deep\nstructure. We present how LDNN can be used to solve nonlinear VFHIEs. We show\nusing the Gaussian quadrature collocation method in combination with LDNN\nresults in a novel numerical solution for nonlinear VFHIEs. Several examples\nare given to verify the performance and accuracy of LDNN.",
          "link": "http://arxiv.org/abs/2106.14320",
          "publishedOn": "2021-06-29T01:55:16.685Z",
          "wordCount": 582,
          "title": "Legendre Deep Neural Network (LDNN) and its application for approximation of nonlinear Volterra Fredholm Hammerstein integral equations. (arXiv:2106.14320v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:16.671Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.03717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honnorat_N/0/1/0/all/0/1\">Nicolas Honnorat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Variation Autoencoder (VAE) has become a powerful tool in modeling the\nnon-linear generative process of data from a low-dimensional latent space.\nRecently, several studies have proposed to use VAE for unsupervised clustering\nby using mixture models to capture the multi-modal structure of latent\nrepresentations. This strategy, however, is ineffective when there are outlier\ndata samples whose latent representations are meaningless, yet contaminating\nthe estimation of key major clusters in the latent space. This exact problem\narises in the context of resting-state fMRI (rs-fMRI) analysis, where\nclustering major functional connectivity patterns is often hindered by heavy\nnoise of rs-fMRI and many minor clusters (rare connectivity patterns) of no\ninterest to analysis. In this paper we propose a novel generative process, in\nwhich we use a Gaussian-mixture to model a few major clusters in the data, and\nuse a non-informative uniform distribution to capture the remaining data. We\nembed this truncated Gaussian-Mixture model in a Variational AutoEncoder\nframework to obtain a general joint clustering and outlier detection approach,\ncalled tGM-VAE. We demonstrated the applicability of tGM-VAE on the MNIST\ndataset and further validated it in the context of rs-fMRI connectivity\nanalysis.",
          "link": "http://arxiv.org/abs/1902.03717",
          "publishedOn": "2021-06-29T01:55:16.665Z",
          "wordCount": 656,
          "title": "Truncated Gaussian-Mixture Variational AutoEncoder. (arXiv:1902.03717v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.",
          "link": "http://arxiv.org/abs/2105.06073",
          "publishedOn": "2021-06-29T01:55:16.660Z",
          "wordCount": 678,
          "title": "Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Topological data analysis, including persistent homology, has undergone\nsignificant development in recent years. However, one outstanding challenge is\nto build a coherent statistical inference procedure on persistent diagrams. The\npaired dependent data structure, which are the births and deaths in persistent\ndiagrams, adds complexity to statistical inference. In this paper, we present a\nnew lattice path representation for persistent diagrams. A new exact\nstatistical inference procedure is developed for lattice paths via\ncombinatorial enumerations. The proposed lattice path method is applied to\nstudy the topological characterization of the protein structures of the\nCOVID-19 virus. We demonstrate that there are topological changes during the\nconformational change of spike proteins, a necessary step in infecting host\ncells.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-06-29T01:55:16.652Z",
          "wordCount": 638,
          "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>",
          "description": "The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.",
          "link": "http://arxiv.org/abs/2012.02825",
          "publishedOn": "2021-06-29T01:55:16.623Z",
          "wordCount": 654,
          "title": "A Survey on Deep Learning for Human Mobility. (arXiv:2012.02825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangmeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>",
          "description": "In multi-party collaborative learning, the parameter server sends a global\nmodel to each data holder for local training and then aggregates committed\nmodels globally to achieve privacy protection. However, both the dragger issue\nof synchronous collaborative learning and the staleness issue of asynchronous\ncollaborative learning make collaborative learning inefficient in real-world\nheterogeneous environments. We propose a novel and efficient collaborative\nlearning framework named AdaptCL, which generates an adaptive sub-model\ndynamically from the global base model for each data holder, without any prior\ninformation about worker capability. All workers (data holders) achieve\napproximately identical update time as the fastest worker by equipping them\nwith capability-adapted pruned models. Thus the training process can be\ndramatically accelerated. Besides, we tailor the efficient pruned rate learning\nalgorithm and pruning approach for AdaptCL. Meanwhile, AdaptCL provides a\nmechanism for handling the trade-off between accuracy and time overhead and can\nbe combined with other techniques to accelerate training further. Empirical\nresults show that AdaptCL introduces little computing and communication\noverhead. AdaptCL achieves time savings of more than 41\\% on average and\nimproves accuracy in a low heterogeneous environment. In a highly heterogeneous\nenvironment, AdaptCL achieves a training speedup of 6.2x with a slight loss of\naccuracy.",
          "link": "http://arxiv.org/abs/2106.14126",
          "publishedOn": "2021-06-29T01:55:16.616Z",
          "wordCount": 641,
          "title": "AdaptCL: Efficient Collaborative Learning with Dynamic and Adaptive Pruning. (arXiv:2106.14126v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1\">Armin Moharrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1\">Khashayar Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_E/0/1/0/all/0/1\">Edmund Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1\">Stratis Ioannidis</a>",
          "description": "The mean squared error loss is widely used in many applications, including\nauto-encoders, multi-target regression, and matrix factorization, to name a\nfew. Despite computational advantages due to its differentiability, it is not\nrobust to outliers. In contrast, l_p norms are known to be robust, but cannot\nbe optimized via, e.g., stochastic gradient descent, as they are\nnon-differentiable. We propose an algorithm inspired by so-called model-based\noptimization (MBO) [35, 36], which replaces a non-convex objective with a\nconvex model function and alternates between optimizing the model function and\nupdating the solution. We apply this to robust regression, proposing SADM, a\nstochastic variant of the Online Alternating Direction Method of Multipliers\n(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges\nwith the rate O(log T/T). Finally, we demonstrate experimentally (a) the\nrobustness of l_p norms to outliers and (b) the efficiency of our proposed\nmodel-based algorithms in comparison with gradient methods on autoencoders and\nmulti-target regression.",
          "link": "http://arxiv.org/abs/2106.10759",
          "publishedOn": "2021-06-29T01:55:16.608Z",
          "wordCount": 617,
          "title": "Robust Regression via Model Based Methods. (arXiv:2106.10759v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1\">Tim De Ryck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>",
          "description": "Physics informed neural networks approximate solutions of PDEs by minimizing\npointwise residuals. We derive rigorous bounds on the error, incurred by PINNs\nin approximating the solutions of a large class of linear parabolic PDEs,\nnamely Kolmogorov equations that include the heat equation and Black-Scholes\nequation of option pricing, as examples. We construct neural networks, whose\nPINN residual (generalization error) can be made as small as desired. We also\nprove that the total $L^2$-error can be bounded by the generalization error,\nwhich in turn is bounded in terms of the training error, provided that a\nsufficient number of randomly chosen training (collocation) points is used.\nMoreover, we prove that the size of the PINNs and the number of training\nsamples only grow polynomially with the underlying dimension, enabling PINNs to\novercome the curse of dimensionality in this context. These results enable us\nto provide a comprehensive error analysis for PINNs in approximating Kolmogorov\nPDEs.",
          "link": "http://arxiv.org/abs/2106.14473",
          "publishedOn": "2021-06-29T01:55:16.603Z",
          "wordCount": 600,
          "title": "Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Marc Vicuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khannouz_M/0/1/0/all/0/1\">Martin Khannouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1\">Gregory Kiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_Y/0/1/0/all/0/1\">Yohan Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1\">Tristan Glatard</a>",
          "description": "Mondrian Forests are a powerful data stream classification method, but their\nlarge memory footprint makes them ill-suited for low-resource platforms such as\nconnected objects. We explored using reduced-precision floating-point\nrepresentations to lower memory consumption and evaluated its effect on\nclassification performance. We applied the Mondrian Forest implementation\nprovided by OrpailleCC, a C++ collection of data stream algorithms, to two\ncanonical datasets in human activity recognition: Recofit and Banos \\emph{et\nal}. Results show that the precision of floating-point values used by tree\nnodes can be reduced from 64 bits to 8 bits with no significant difference in\nF1 score. In some cases, reduced precision was shown to improve classification\nperformance, presumably due to its regularization effect. We conclude that\nnumerical precision is a relevant hyperparameter in the Mondrian Forest, and\nthat commonly-used double precision values may not be necessary for optimal\nperformance. Future work will evaluate the generalizability of these findings\nto other data stream classifiers.",
          "link": "http://arxiv.org/abs/2106.14340",
          "publishedOn": "2021-06-29T01:55:16.597Z",
          "wordCount": 611,
          "title": "Reducing numerical precision preserves classification accuracy in Mondrian Forests. (arXiv:2106.14340v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:16.567Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>",
          "description": "Supervised learning, while deployed in real-life scenarios, often encounters\ninstances of unknown classes. Conventional algorithms for training a supervised\nlearning model do not provide an option to detect such instances, so they\nmiss-classify such instances with 100% probability. Open Set Recognition (OSR)\nand Non-Exhaustive Learning (NEL) are potential solutions to overcome this\nproblem. Most existing methods of OSR first classify members of existing\nclasses and then identify instances of new classes. However, many of the\nexisting methods of OSR only makes a binary decision, i.e., they only identify\nthe existence of the unknown class. Hence, such methods cannot distinguish test\ninstances belonging to incremental unseen classes. On the other hand, the\nmajority of NEL methods often make a parametric assumption over the data\ndistribution, which either fail to return good results, due to the reason that\nreal-life complex datasets may not follow a well-known data distribution. In\nthis paper, we propose a new online non-exhaustive learning model, namely,\nNon-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to\naddress these issues. Our proposed model synthesizes Gaussian mixture based\nlatent representation over a deep generative model, such as GAN, for\nincremental detection of instances of emerging classes in the test data.\nExtensive experimental results on several benchmark datasets show that\nNE-GM-GAN significantly outperforms the state-of-the-art methods in detecting\ninstances of novel classes in streaming data.",
          "link": "http://arxiv.org/abs/2106.14344",
          "publishedOn": "2021-06-29T01:55:16.551Z",
          "wordCount": 653,
          "title": "Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10358",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1\">Taku Shoji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1\">Noboru Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1\">Toshihisa Tanaka</a>",
          "description": "Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but\nit requires expertise and experience to identify abnormalities. It is thus\ncrucial to develop automated models for the detection of abnormalities in EEGs\nrelated to epilepsy. This paper describes the development of a novel class of\ncompact convolutional neural networks (CNNs) for detecting abnormal patterns\nand electrodes in EEGs for epilepsy. The designed model is inspired by a CNN\ndeveloped for brain-computer interfacing called multichannel EEGNet (mEEGNet).\nUnlike the EEGNet, the proposed model, mEEGNet, has the same number of\nelectrode inputs and outputs to detect abnormal patterns. The mEEGNet was\nevaluated with a clinical dataset consisting of 29 cases of juvenile and\nchildhood absence epilepsy labeled by a clinical expert. The labels were given\nto paroxysmal discharges visually observed in both ictal (seizure) and\ninterictal (nonseizure) durations. Results showed that the mEEGNet detected\nabnormalities with the area under the curve, F1-values, and sensitivity\nequivalent to or higher than those of existing CNNs. Moreover, the number of\nparameters is much smaller than other CNN models. To our knowledge, the dataset\nof absence epilepsy validated with machine learning through this research is\nthe largest in the literature.",
          "link": "http://arxiv.org/abs/2105.10358",
          "publishedOn": "2021-06-29T01:55:16.545Z",
          "wordCount": 666,
          "title": "Automated Detection of Abnormalities from an EEG Recording of Epilepsy Patients With a Compact Convolutional Neural Network. (arXiv:2105.10358v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1\">Rei Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Mineto Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Most edge AI focuses on prediction tasks on resource-limited edge devices\nwhile the training is done at server machines. However, retraining or\ncustomizing a model is required at edge devices as the model is becoming\noutdated due to environmental changes over time. To follow such a concept\ndrift, a neural-network based on-device learning approach is recently proposed,\nso that edge devices train incoming data at runtime to update their model. In\nthis case, since a training is done at distributed edge devices, the issue is\nthat only a limited amount of training data can be used for each edge device.\nTo address this issue, one approach is a cooperative learning or federated\nlearning, where edge devices exchange their trained results and update their\nmodel by using those collected from the other devices. In this paper, as an\non-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme\nLearning Machine) to sequentially train a model based on recent samples and\ncombine it with autoencoder for anomaly detection. We extend it for an\non-device federated learning so that edge devices can exchange their trained\nresults and update their model by using those collected from the other edge\ndevices. This cooperative model update is one-shot while it can be repeatedly\napplied to synchronize their model. Our approach is evaluated with anomaly\ndetection tasks generated from a driving dataset of cars, a human activity\ndataset, and MNIST dataset. The results demonstrate that the proposed on-device\nfederated learning can produce a merged model by integrating trained results\nfrom multiple edge devices as accurately as traditional backpropagation based\nneural networks and a traditional federated learning approach with lower\ncomputation or communication cost.",
          "link": "http://arxiv.org/abs/2002.12301",
          "publishedOn": "2021-06-29T01:55:16.531Z",
          "wordCount": 786,
          "title": "An On-Device Federated Learning Approach for Cooperative Model Update between Edge Devices. (arXiv:2002.12301v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiajing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenxuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yangkang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yaxin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Graph neural networks (GNNs) constitute a class of deep learning methods for\ngraph data. They have wide applications in chemistry and biology, such as\nmolecular property prediction, reaction prediction and drug-target interaction\nprediction. Despite the interest, GNN-based modeling is challenging as it\nrequires graph data pre-processing and modeling in addition to programming and\ndeep learning. Here we present DGL-LifeSci, an open-source package for deep\nlearning on graphs in life science. DGL-LifeSci is a python toolkit based on\nRDKit, PyTorch and Deep Graph Library (DGL). DGL-LifeSci allows GNN-based\nmodeling on custom datasets for molecular property prediction, reaction\nprediction and molecule generation. With its command-line interfaces, users can\nperform modeling without any background in programming and deep learning. We\ntest the command-line interfaces using standard benchmarks MoleculeNet, USPTO,\nand ZINC. Compared with previous implementations, DGL-LifeSci achieves a speed\nup by up to 6x. For modeling flexibility, DGL-LifeSci provides well-optimized\nmodules for various stages of the modeling pipeline. In addition, DGL-LifeSci\nprovides pre-trained models for reproducing the test experiment results and\napplying models without training. The code is distributed under an Apache-2.0\nLicense and is freely accessible at https://github.com/awslabs/dgl-lifesci.",
          "link": "http://arxiv.org/abs/2106.14232",
          "publishedOn": "2021-06-29T01:55:16.516Z",
          "wordCount": 637,
          "title": "DGL-LifeSci: An Open-Source Toolkit for Deep Learning on Graphs in Life Science. (arXiv:2106.14232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1\">Konstantin Salomatin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">Jeffrey D. Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talanine_K/0/1/0/all/0/1\">Kirill Talanine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalal_O/0/1/0/all/0/1\">Onkar Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1\">Gungor Polatkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smoot_S/0/1/0/all/0/1\">Sara Smoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>",
          "description": "One of the most well-established applications of machine learning is in\ndeciding what content to show website visitors. When observation data comes\nfrom high-velocity, user-generated data streams, machine learning methods\nperform a balancing act between model complexity, training time, and\ncomputational costs. Furthermore, when model freshness is critical, the\ntraining of models becomes time-constrained. Parallelized batch offline\ntraining, although horizontally scalable, is often not time-considerate or\ncost-effective. In this paper, we propose Lambda Learner, a new framework for\ntraining models by incremental updates in response to mini-batches from data\nstreams. We show that the resulting model of our framework closely estimates a\nperiodically updated model trained on offline data and outperforms it when\nmodel updates are time-sensitive. We provide theoretical proof that the\nincremental learning updates improve the loss-function over a stale batch\nmodel. We present a large-scale deployment on the sponsored content platform\nfor a large social network, serving hundreds of millions of users across\ndifferent channels (e.g., desktop, mobile). We address challenges and\ncomplexities from both algorithms and infrastructure perspectives, and\nillustrate the system details for computation, storage, and streaming\nproduction of training data.",
          "link": "http://arxiv.org/abs/2010.05154",
          "publishedOn": "2021-06-29T01:55:16.476Z",
          "wordCount": 680,
          "title": "Lambda Learner: Fast Incremental Learning on Data Streams. (arXiv:2010.05154v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Dropout is a powerful and widely used technique to regularize the training of\ndeep neural networks. In this paper, we introduce a simple regularization\nstrategy upon dropout in model training, namely R-Drop, which forces the output\ndistributions of different sub models generated by dropout to be consistent\nwith each other. Specifically, for each training sample, R-Drop minimizes the\nbidirectional KL-divergence between the output distributions of two sub models\nsampled by dropout. Theoretical analysis reveals that R-Drop reduces the\nfreedom of the model parameters and complements dropout. Experiments on\n$\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total),\nincluding neural machine translation, abstractive summarization, language\nunderstanding, language modeling, and image classification, show that R-Drop is\nuniversally effective. In particular, it yields substantial improvements when\napplied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large,\nand BART, and achieves state-of-the-art (SOTA) performances with the vanilla\nTransformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU)\nand WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing\nmodels trained with extra large-scale data and expert-designed advanced\nvariants of Transformer models. Our code is available at\nGitHub{\\url{https://github.com/dropreg/R-Drop}}.",
          "link": "http://arxiv.org/abs/2106.14448",
          "publishedOn": "2021-06-29T01:55:16.470Z",
          "wordCount": 616,
          "title": "R-Drop: Regularized Dropout for Neural Networks. (arXiv:2106.14448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Randomized smoothing has been successfully applied to classification tasks on\nhigh-dimensional inputs, such as images, to obtain models that are provably\nrobust against adversarial perturbations of the input. We extend this technique\nto produce provable robustness for functions that map inputs into an arbitrary\nmetric space rather than discrete classes. Such functions are used in many\nmachine learning problems like image reconstruction, dimensionality reduction,\nfacial recognition, etc. Our robustness certificates guarantee that the change\nin the output of the smoothed model as measured by the distance metric remains\nsmall for any norm-bounded perturbation of the input. We can certify robustness\nunder a variety of different output metrics, such as total variation distance,\nJaccard distance, perceptual metrics, etc. In our experiments, we apply our\nprocedure to create certifiably robust models with disparate output spaces --\nfrom sets to images -- and show that it yields meaningful certificates without\nsignificantly degrading the performance of the base model. The code for our\nexperiments is available at: https://github.com/aounon/center-smoothing.",
          "link": "http://arxiv.org/abs/2102.09701",
          "publishedOn": "2021-06-29T01:55:16.459Z",
          "wordCount": 620,
          "title": "Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs. (arXiv:2102.09701v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Saliency methods are a popular class of feature attribution tools that aim to\ncapture a model's predictive reasoning by identifying \"important\" pixels in an\ninput image. However, the development and adoption of saliency methods are\ncurrently hindered by the lack of access to underlying model reasoning, which\nprevents accurate method evaluation. In this work, we design a synthetic\nevaluation framework, SMERF, that allows us to perform ground-truth-based\nevaluation of saliency methods while controlling the underlying complexity of\nmodel reasoning. Experimental evaluations via SMERF reveal significant\nlimitations in existing saliency methods, especially given the relative\nsimplicity of SMERF's synthetic evaluation tasks. Moreover, the SMERF\nbenchmarking suite represents a useful tool in the development of new saliency\nmethods to potentially overcome these limitations.",
          "link": "http://arxiv.org/abs/2105.06506",
          "publishedOn": "2021-06-29T01:55:16.381Z",
          "wordCount": 573,
          "title": "Sanity Simulations for Saliency Methods. (arXiv:2105.06506v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:16.365Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1\">Jo&#xe3;o Bento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1\">Pedro Saleiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_A/0/1/0/all/0/1\">Andr&#xe9; F. Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1\">M&#xe1;rio A.T. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Although recurrent neural networks (RNNs) are state-of-the-art in numerous\nsequential decision-making tasks, there has been little research on explaining\ntheir predictions. In this work, we present TimeSHAP, a model-agnostic\nrecurrent explainer that builds upon KernelSHAP and extends it to the\nsequential domain. TimeSHAP computes feature-, timestep-, and cell-level\nattributions. As sequences may be arbitrarily long, we further propose a\npruning method that is shown to dramatically decrease both its computational\ncost and the variance of its attributions. We use TimeSHAP to explain the\npredictions of a real-world bank account takeover fraud detection RNN model,\nand draw key insights from its explanations: i) the model identifies important\nfeatures and events aligned with what fraud analysts consider cues for account\ntakeover; ii) positive predicted sequences can be pruned to only 10% of the\noriginal length, as older events have residual attribution values; iii) the\nmost recent input event of positive predictions only contributes on average to\n41% of the model's score; iv) notably high attribution to client's age,\nsuggesting a potential discriminatory reasoning, later confirmed as higher\nfalse positive rates for older clients.",
          "link": "http://arxiv.org/abs/2012.00073",
          "publishedOn": "2021-06-29T01:55:16.359Z",
          "wordCount": 653,
          "title": "TimeSHAP: Explaining Recurrent Models through Sequence Perturbations. (arXiv:2012.00073v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasse_M/0/1/0/all/0/1\">Maxime Gasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasset_D/0/1/0/all/0/1\">Damien Grasset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudron_G/0/1/0/all/0/1\">Guillaume Gaudron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Learning efficiently a causal model of the environment is a key challenge of\nmodel-based RL agents operating in POMDPs. We consider here a scenario where\nthe learning agent has the ability to collect online experiences through direct\ninteractions with the environment (interventional data), but has also access to\na large collection of offline experiences, obtained by observing another agent\ninteracting with the environment (observational data). A key ingredient, that\nmakes this situation non-trivial, is that we allow the observed agent to\ninteract with the environment based on hidden information, which is not\nobserved by the learning agent. We then ask the following questions: can the\nonline and offline experiences be safely combined for learning a causal model ?\nAnd can we expect the offline experiences to improve the agent's performances ?\nTo answer these questions, we import ideas from the well-established causal\nframework of do-calculus, and we express model-based reinforcement learning as\na causal inference problem. Then, we propose a general yet simple methodology\nfor leveraging offline data during learning. In a nutshell, the method relies\non learning a latent-based causal transition model that explains both the\ninterventional and observational regimes, and then using the recovered latent\nvariable to infer the standard POMDP transition model via deconfounding. We\nprove our method is correct and efficient in the sense that it attains better\ngeneralization guarantees due to the offline data (in the asymptotic case), and\nwe illustrate its effectiveness empirically on synthetic toy problems. Our\ncontribution aims at bridging the gap between the fields of reinforcement\nlearning and causality.",
          "link": "http://arxiv.org/abs/2106.14421",
          "publishedOn": "2021-06-29T01:55:16.353Z",
          "wordCount": 687,
          "title": "Causal Reinforcement Learning using Observational and Interventional Data. (arXiv:2106.14421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:16.347Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1\">Kanhaiya Gupta</a>",
          "description": "In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.",
          "link": "http://arxiv.org/abs/2106.14257",
          "publishedOn": "2021-06-29T01:55:16.341Z",
          "wordCount": 528,
          "title": "Use of Machine Learning Technique to maximize the signal over background for $H \\rightarrow \\tau \\tau$. (arXiv:2106.14257v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1\">Luis Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "A major obstacle to achieving global convergence in distributed and federated\nlearning is the misalignment of gradients across clients, or mini-batches due\nto heterogeneity and stochasticity of the distributed data. One way to\nalleviate this problem is to encourage the alignment of gradients across\ndifferent clients throughout training. Our analysis reveals that this goal can\nbe accomplished by utilizing the right optimization method that replicates the\nimplicit regularization effect of SGD, leading to gradient alignment as well as\nimprovements in test accuracies. Since the existence of this regularization in\nSGD completely relies on the sequential use of different mini-batches during\ntraining, it is inherently absent when training with large mini-batches. To\nobtain the generalization benefits of this regularization while increasing\nparallelism, we propose a novel GradAlign algorithm that induces the same\nimplicit regularization while allowing the use of arbitrarily large batches in\neach update. We experimentally validate the benefit of our algorithm in\ndifferent distributed and federated learning settings.",
          "link": "http://arxiv.org/abs/2106.13897",
          "publishedOn": "2021-06-29T01:55:16.327Z",
          "wordCount": 592,
          "title": "Implicit Gradient Alignment in Distributed and Federated Learning. (arXiv:2106.13897v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buyl_M/0/1/0/all/0/1\">Maarten Buyl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>",
          "description": "Learning and reasoning over graphs is increasingly done by means of\nprobabilistic models, e.g. exponential random graph models, graph embedding\nmodels, and graph neural networks. When graphs are modeling relations between\npeople, however, they will inevitably reflect biases, prejudices, and other\nforms of inequity and inequality. An important challenge is thus to design\naccurate graph modeling approaches while guaranteeing fairness according to the\nspecific notion of fairness that the problem requires. Yet, past work on the\ntopic remains scarce, is limited to debiasing specific graph modeling methods,\nand often aims to ensure fairness in an indirect manner.\n\nWe propose a generic approach applicable to most probabilistic graph modeling\napproaches. Specifically, we first define the class of fair graph models\ncorresponding to a chosen set of fairness criteria. Given this, we propose a\nfairness regularizer defined as the KL-divergence between the graph model and\nits I-projection onto the set of fair models. We demonstrate that using this\nfairness regularizer in combination with existing graph modeling approaches\nefficiently trades-off fairness with accuracy, whereas the state-of-the-art\nmodels can only make this trade-off for the fairness criterion that they were\nspecifically designed for.",
          "link": "http://arxiv.org/abs/2103.01846",
          "publishedOn": "2021-06-29T01:55:16.321Z",
          "wordCount": 655,
          "title": "The KL-Divergence between a Graph Model and its Fair I-Projection as a Fairness Regularizer. (arXiv:2103.01846v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:16.313Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.308Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1\">Peter Torelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1\">Jeremy Holleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1\">Nat Jeffries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1\">Csaba Kiraly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1\">Pietro Montino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1\">David Kanter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sebastian Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1\">Danilo Pau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1\">Antonio Torrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Peter Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1\">Jay Cordaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1\">Giuseppe Di Guglielmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1\">Stephen Gibellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Videet Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Honson Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1\">Niu Wenxu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1\">Xu Xuesong</a>",
          "description": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.",
          "link": "http://arxiv.org/abs/2106.07597",
          "publishedOn": "2021-06-29T01:55:16.297Z",
          "wordCount": 629,
          "title": "MLPerf Tiny Benchmark. (arXiv:2106.07597v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.07272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n\nThe book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n\nThe chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.",
          "link": "http://arxiv.org/abs/1904.07272",
          "publishedOn": "2021-06-29T01:55:16.281Z",
          "wordCount": 748,
          "title": "Introduction to Multi-Armed Bandits. (arXiv:1904.07272v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:16.275Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:16.264Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1\">Waddah Saeed</a>",
          "description": "Short Message Service (SMS) is a very popular service used for communication\nby mobile users. However, this popular service can be abused by executing\nillegal activities and influencing security risks. Nowadays, many automatic\nmachine learning (AutoML) tools exist which can help domain experts and lay\nusers to build high-quality ML models with little or no machine learning\nknowledge. In this work, a classification performance comparison was conducted\nbetween three automatic ML tools for SMS spam message filtering. These tools\nare mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization\nTool (TPOT) AutoML. Experimental results showed that ensemble models achieved\nthe best classification performance. The Stacked Ensemble model, which was\nbuilt using H2O AutoML, achieved the best performance in terms of Log Loss\n(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There\nis a 19.05\\% improvement in Log Loss with respect to TPOT AutoML and 5.56\\%\nimprovement with respect to mljar-supervised AutoML. The satisfactory filtering\nperformance achieved with AutoML tools provides a potential application for\nAutoML tools to automatically determine the best ML model that can perform best\nfor SMS spam message filtering.",
          "link": "http://arxiv.org/abs/2106.08671",
          "publishedOn": "2021-06-29T01:55:16.250Z",
          "wordCount": 633,
          "title": "Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:16.244Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.238Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:16.232Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandak_S/0/1/0/all/0/1\">Siddharth Chandak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek S. Borkar</a>",
          "description": "Using a martingale concentration inequality, concentration bounds `from time\n$n_0$ on' are derived for stochastic approximation algorithms with contractive\nmaps and both martingale difference and Markov noises. These are applied to\nreinforcement learning algorithms, in particular to asynchronous Q-learning and\nTD(0).",
          "link": "http://arxiv.org/abs/2106.14308",
          "publishedOn": "2021-06-29T01:55:16.227Z",
          "wordCount": 481,
          "title": "Concentration of Contractive Stochastic Approximation and Reinforcement Learning. (arXiv:2106.14308v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1\">Boris Muzellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Kernel mean embeddings are a popular tool that consists in representing\nprobability measures by their infinite-dimensional mean embeddings in a\nreproducing kernel Hilbert space. When the kernel is characteristic, mean\nembeddings can be used to define a distance between probability measures, known\nas the maximum mean discrepancy (MMD). A well-known advantage of mean\nembeddings and MMD is their low computational cost and low sample complexity.\nHowever, kernel mean embeddings have had limited applications to problems that\nconsist in optimizing distributions, due to the difficulty of characterizing\nwhich Hilbert space vectors correspond to a probability distribution. In this\nnote, we propose to leverage the kernel sums-of-squares parameterization of\npositive functions of Marteau-Ferey et al. [2020] to fit distributions in the\nMMD geometry. First, we show that when the kernel is characteristic,\ndistributions with a kernel sum-of-squares density are dense. Then, we provide\nalgorithms to optimize such distributions in the finite-sample setting, which\nwe illustrate in a density fitting numerical experiment.",
          "link": "http://arxiv.org/abs/2106.09994",
          "publishedOn": "2021-06-29T01:55:16.221Z",
          "wordCount": 610,
          "title": "A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10293",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sharir_O/0/1/0/all/0/1\">Or Sharir</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We establish a direct connection between general tensor networks and deep\nfeed-forward artificial neural networks. The core of our results is the\nconstruction of neural-network layers that efficiently perform tensor\ncontractions, and that use commonly adopted non-linear activation functions.\nThe resulting deep networks feature a number of edges that closely matches the\ncontraction complexity of the tensor networks to be approximated. In the\ncontext of many-body quantum states, this result establishes that\nneural-network states have strictly the same or higher expressive power than\npractically usable variational tensor networks. As an example, we show that all\nmatrix product states can be efficiently written as neural-network states with\na number of edges polynomial in the bond dimension and depth logarithmic in the\nsystem size. The opposite instead does not hold true, and our results imply\nthat there exist quantum states that are not efficiently expressible in terms\nof matrix product states or practically usable PEPS, but that are instead\nefficiently expressible with neural network states.",
          "link": "http://arxiv.org/abs/2103.10293",
          "publishedOn": "2021-06-29T01:55:16.210Z",
          "wordCount": 625,
          "title": "Neural tensor contractions and the expressive power of deep neural quantum states. (arXiv:2103.10293v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1\">Pouya Bashivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1\">Reza Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adam Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1\">Mojtaba Faramarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1\">Touraj Laleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake Aaron Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>",
          "description": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths.",
          "link": "http://arxiv.org/abs/2006.04621",
          "publishedOn": "2021-06-29T01:55:16.143Z",
          "wordCount": 630,
          "title": "Adversarial Feature Desensitization. (arXiv:2006.04621v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17236",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1\">Zichang He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Fabrication process variations can significantly influence the performance\nand yield of nano-scale electronic and photonic circuits. Stochastic spectral\nmethods have achieved great success in quantifying the impact of process\nvariations, but they suffer from the curse of dimensionality. Recently,\nlow-rank tensor methods have been developed to mitigate this issue, but two\nfundamental challenges remain open: how to automatically determine the tensor\nrank and how to adaptively pick the informative simulation samples. This paper\nproposes a novel tensor regression method to address these two challenges. We\nuse a $\\ell_{q}/ \\ell_{2}$ group-sparsity regularization to determine the\ntensor rank. The resulting optimization problem can be efficiently solved via\nan alternating minimization solver. We also propose a two-stage adaptive\nsampling method to reduce the simulation cost. Our method considers both\nexploration and exploitation via the estimated Voronoi cell volume and\nnonlinearity measurement respectively. The proposed model is verified with\nsynthetic and some realistic circuit benchmarks, on which our method can well\ncapture the uncertainty caused by 19 to 100 random variables with only 100 to\n600 simulation samples.",
          "link": "http://arxiv.org/abs/2103.17236",
          "publishedOn": "2021-06-29T01:55:16.137Z",
          "wordCount": 646,
          "title": "High-Dimensional Uncertainty Quantification via Tensor Regression with Rank Determination and Adaptive Sampling. (arXiv:2103.17236v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:16.131Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00771",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mandralis_I/0/1/0/all/0/1\">Ioannis Mandralis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Weber_P/0/1/0/all/0/1\">Pascal Weber</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Novati_G/0/1/0/all/0/1\">Guido Novati</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koumoutsakos_P/0/1/0/all/0/1\">Petros Koumoutsakos</a>",
          "description": "Swimming organisms can escape their predators by creating and harnessing\nunsteady flow fields through their body motions. Stochastic optimization and\nflow simulations have identified escape patterns that are consistent with those\nobserved in natural larval swimmers. However, these patterns have been limited\nby the specification of a particular cost function and depend on a prescribed\nfunctional form of the body motion. Here, we deploy reinforcement learning to\ndiscover swimmer escape patterns for larval fish under energy constraints. The\nidentified patterns include the C-start mechanism, in addition to more\nenergetically efficient escapes. We find that maximizing distance with limited\nenergy requires swimming via short bursts of accelerating motion interlinked\nwith phases of gliding. The present, data efficient, reinforcement learning\nalgorithm results in an array of patterns that reveal practical flow\noptimization principles for efficient swimming and the methodology can be\ntransferred to the control of aquatic robotic devices operating under energy\nconstraints.",
          "link": "http://arxiv.org/abs/2105.00771",
          "publishedOn": "2021-06-29T01:55:16.126Z",
          "wordCount": 613,
          "title": "Learning swimming escape patterns for larval fish under energy constraints. (arXiv:2105.00771v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In this rejoinder, we aim to address two broad issues that cover most\ncomments made in the discussion. First, we discuss some theoretical aspects of\nour work and comment on how this work might impact the theoretical foundation\nof privacy-preserving data analysis. Taking a practical viewpoint, we next\ndiscuss how f-differential privacy (f-DP) and Gaussian differential privacy\n(GDP) can make a difference in a range of applications.",
          "link": "http://arxiv.org/abs/2104.01987",
          "publishedOn": "2021-06-29T01:55:16.098Z",
          "wordCount": 555,
          "title": "Rejoinder: Gaussian Differential Privacy. (arXiv:2104.01987v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "A surrogate model based hyperparameter tuning approach for deep learning is\npresented. This article demonstrates how the architecture-level parameters\n(hyperparameters) of deep learning models that were implemented in\nKeras/tensorflow can be optimized. The implementation of the tuning procedure\nis 100% accessible from R, the software environment for statistical computing.\nWith a few lines of code, existing R packages (tfruns and SPOT) can be combined\nto perform hyperparameter tuning. An elementary hyperparameter tuning task\n(neural network and the MNIST data) is used to exemplify this approach",
          "link": "http://arxiv.org/abs/2105.14625",
          "publishedOn": "2021-06-29T01:55:16.092Z",
          "wordCount": 552,
          "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT. (arXiv:2105.14625v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06958",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jutten_C/0/1/0/all/0/1\">Christian Jutten</a>",
          "description": "The extraction of nonstationary signals from blind and semi-blind\nmultivariate observations is a recurrent problem. Numerous algorithms have been\ndeveloped for this problem, which are based on the exact or approximate joint\ndiagonalization of second or higher order cumulant matrices/tensors of\nmultichannel data. While a great body of research has been dedicated to joint\ndiagonalization algorithms, the selection of the diagonalized matrix/tensor set\nremains highly problem-specific. Herein, various methods for nonstationarity\nidentification are reviewed and a new general framework based on hypothesis\ntesting is proposed, which results in a classification/clustering perspective\nto semi-blind source separation of nonstationary components. The proposed\nmethod is applied to noninvasive fetal ECG extraction, as case study.",
          "link": "http://arxiv.org/abs/2105.06958",
          "publishedOn": "2021-06-29T01:55:16.086Z",
          "wordCount": 566,
          "title": "A Hypothesis Testing Approach to Nonstationary Source Separation. (arXiv:2105.06958v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1\">Amrith Setlur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1\">Oscar Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "We categorize meta-learning evaluation into two settings:\n$\\textit{in-distribution}$ [ID], in which the train and test tasks are sampled\n$\\textit{iid}$ from the same underlying task distribution, and\n$\\textit{out-of-distribution}$ [OOD], in which they are not. While most\nmeta-learning theory and some FSL applications follow the ID setting, we\nidentify that most existing few-shot classification benchmarks instead reflect\nOOD evaluation, as they use disjoint sets of train (base) and test (novel)\nclasses for task generation. This discrepancy is problematic because -- as we\nshow on numerous benchmarks -- meta-learning methods that perform better on\nexisting OOD datasets may perform significantly worse in the ID setting. In\naddition, in the OOD setting, even though current FSL benchmarks seem\nbefitting, our study highlights concerns in 1) reliably performing model\nselection for a given meta-learning method, and 2) consistently comparing the\nperformance of different methods. To address these concerns, we provide\nsuggestions on how to construct FSL benchmarks to allow for ID evaluation as\nwell as more reliable OOD evaluation. Our work aims to inform the meta-learning\ncommunity about the importance and distinction of ID vs. OOD evaluation, as\nwell as the subtleties of OOD evaluation with current benchmarks.",
          "link": "http://arxiv.org/abs/2102.11503",
          "publishedOn": "2021-06-29T01:55:16.080Z",
          "wordCount": 652,
          "title": "Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution. (arXiv:2102.11503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13865",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1\">Hsuan-Tung Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lederman_J/0/1/0/all/0/1\">Joshua Lederman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_T/0/1/0/all/0/1\">Thomas Ferreira de Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chaoran Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shastri_B/0/1/0/all/0/1\">Bhavin Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenbluth_D/0/1/0/all/0/1\">David Rosenbluth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prucnal_P/0/1/0/all/0/1\">Paul Prucnal</a>",
          "description": "Machine learning (ML) methods are ubiquitous in wireless communication\nsystems and have proven powerful for applications including radio-frequency\n(RF) fingerprinting, automatic modulation classification, and cognitive radio.\nHowever, the large size of ML models can make them difficult to implement on\nedge devices for latency-sensitive downstream tasks. In wireless communication\nsystems, ML data processing at a sub-millisecond scale will enable real-time\nnetwork monitoring to improve security and prevent infiltration. In addition,\ncompact and integratable hardware platforms which can implement ML models at\nthe chip scale will find much broader application to wireless communication\nnetworks. Toward real-time wireless signal classification at the edge, we\npropose a novel compact deep network that consists of a\nphotonic-hardware-inspired recurrent neural network model in combination with a\nsimplified convolutional classifier, and we demonstrate its application to the\nidentification of RF emitters by their random transmissions. With the proposed\nmodel, we achieve 96.32% classification accuracy over a set of 30 identical\nZigBee devices when using 50 times fewer training parameters than an existing\nstate-of-the-art CNN classifier. Thanks to the large reduction in network size,\nwe demonstrate real-time RF fingerprinting with 0.219 ms latency using a\nsmall-scale FPGA board, the PYNQ-Z1.",
          "link": "http://arxiv.org/abs/2106.13865",
          "publishedOn": "2021-06-29T01:55:16.064Z",
          "wordCount": 658,
          "title": "A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge. (arXiv:2106.13865v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1\">Neil Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n\nWe prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.08769",
          "publishedOn": "2021-06-29T01:55:16.051Z",
          "wordCount": 635,
          "title": "Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tack_J/0/1/0/all/0/1\">Jihoon Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jongheon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Adversarial training (AT) is currently one of the most successful methods to\nobtain the adversarial robustness of deep neural networks. However, the\nphenomenon of robust overfitting, i.e., the robustness starts to decrease\nsignificantly during AT, has been problematic, not only making practitioners\nconsider a bag of tricks for a successful training, e.g., early stopping, but\nalso incurring a significant generalization gap in the robustness. In this\npaper, we propose an effective regularization technique that prevents robust\noverfitting by optimizing an auxiliary 'consistency' regularization loss during\nAT. Specifically, it forces the predictive distributions after attacking from\ntwo different augmentations of the same instance to be similar with each other.\nOur experimental results demonstrate that such a simple regularization\ntechnique brings significant improvements in the test robust accuracy of a wide\nrange of AT methods. More remarkably, we also show that our method could\nsignificantly help the model to generalize its robustness against unseen\nadversaries, e.g., other types or larger perturbations compared to those used\nduring training. Code is available at\nhttps://github.com/alinlab/consistency-adversarial.",
          "link": "http://arxiv.org/abs/2103.04623",
          "publishedOn": "2021-06-29T01:55:16.022Z",
          "wordCount": 637,
          "title": "Consistency Regularization for Adversarial Robustness. (arXiv:2103.04623v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oberst_M/0/1/0/all/0/1\">Michael Oberst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1\">Nikolaj Thams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jonas Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>",
          "description": "We propose a method for learning linear models whose predictive performance\nis robust to causal interventions on unobserved variables, when noisy proxies\nof those variables are available. Our approach takes the form of a\nregularization term that trades off between in-distribution performance and\nrobustness to interventions. Under the assumption of a linear structural causal\nmodel, we show that a single proxy can be used to create estimators that are\nprediction optimal under interventions of bounded strength. This strength\ndepends on the magnitude of the measurement noise in the proxy, which is, in\ngeneral, not identifiable. In the case of two proxy variables, we propose a\nmodified estimator that is prediction optimal under interventions up to a known\nstrength. We further show how to extend these estimators to scenarios where\nadditional information about the \"test time\" intervention is available during\ntraining. We evaluate our theoretical findings in synthetic experiments and\nusing real data of hourly pollution levels across several cities in China.",
          "link": "http://arxiv.org/abs/2103.02477",
          "publishedOn": "2021-06-29T01:55:16.003Z",
          "wordCount": 628,
          "title": "Regularizing towards Causal Invariance: Linear Models with Proxies. (arXiv:2103.02477v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimakopoulou_M/0/1/0/all/0/1\">Maria Dimakopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhimei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "During online decision making in Multi-Armed Bandits (MAB), one needs to\nconduct inference on the true mean reward of each arm based on data collected\nso far at each step. However, since the arms are adaptively selected--thereby\nyielding non-iid data--conducting inference accurately is not straightforward.\nIn particular, sample averaging, which is used in the family of UCB and\nThompson sampling (TS) algorithms, does not provide a good choice as it suffers\nfrom bias and a lack of good statistical properties (e.g. asymptotic\nnormality). Our thesis in this paper is that more sophisticated inference\nschemes that take into account the adaptive nature of the sequentially\ncollected data can unlock further performance gains, even though both UCB and\nTS type algorithms are optimal in the worst case. In particular, we propose a\nvariant of TS-style algorithms--which we call doubly adaptive TS--that\nleverages recent advances in causal inference and adaptively reweights the\nterms of a doubly robust estimator on the true mean reward of each arm. Through\n20 synthetic domain experiments and a semi-synthetic experiment based on data\nfrom an A/B test of a web service, we demonstrate that using an adaptive\ninferential scheme (while still retaining the exploration efficacy of TS)\nprovides clear benefits in online decision making: the proposed DATS algorithm\nhas superior empirical performance to existing baselines (UCB and TS) in terms\nof regret and sample complexity in identifying the best arm. In addition, we\nalso provide a finite-time regret bound of doubly adaptive TS that matches (up\nto log factors) those of UCB and TS algorithms, thereby establishing that its\nimproved practical benefits do not come at the expense of worst-case\nsuboptimality.",
          "link": "http://arxiv.org/abs/2102.13202",
          "publishedOn": "2021-06-29T01:55:15.996Z",
          "wordCount": 733,
          "title": "Online Multi-Armed Bandits with Adaptive Inference. (arXiv:2102.13202v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuning You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Self-supervised learning on graph-structured data has drawn recent interest\nfor learning generalizable, transferable and robust representations from\nunlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged\nwith promising representation learning performance. Unfortunately, unlike its\ncounterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data\naugmentations, which have to be manually picked per dataset, by either rules of\nthumb or trial-and-errors, owing to the diverse nature of graph data. That\nsignificantly limits the more general applicability of GraphCL. Aiming to fill\nin this crucial gap, this paper proposes a unified bi-level optimization\nframework to automatically, adaptively and dynamically select data\naugmentations when performing GraphCL on specific graph data. The general\nframework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as\nmin-max optimization. The selections of augmentations made by JOAO are shown to\nbe in general aligned with previous \"best practices\" observed from handcrafted\ntuning: yet now being automated, more flexible and versatile. Moreover, we\npropose a new augmentation-aware projection head mechanism, which will route\noutput features through different projection heads corresponding to different\naugmentations chosen at each training step. Extensive experiments demonstrate\nthat JOAO performs on par with or sometimes better than the state-of-the-art\ncompetitors including GraphCL, on multiple graph datasets of various scales and\ntypes, yet without resorting to any laborious dataset-specific tuning on\naugmentation selection. We release the code at\nhttps://github.com/Shen-Lab/GraphCL_Automated.",
          "link": "http://arxiv.org/abs/2106.07594",
          "publishedOn": "2021-06-29T01:55:15.970Z",
          "wordCount": 676,
          "title": "Graph Contrastive Learning Automated. (arXiv:2106.07594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02876",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schell_A/0/1/0/all/0/1\">Alexander Schell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>",
          "description": "We study the classical problem of recovering a multidimensional source\nprocess from observations of nonlinear mixtures of this process. Assuming\nstatistical independence of the coordinate processes of the source, we show\nthat this recovery is possible for many popular models of stochastic processes\n(up to order and monotone scaling of their coordinates) if the mixture is given\nby a sufficiently differentiable, invertible function. Key to our approach is\nthe combination of tools from stochastic analysis and recent contrastive\nlearning approaches to nonlinear ICA. This yields a scalable method with widely\napplicable theoretical guarantees for which our experiments indicate good\nperformance.",
          "link": "http://arxiv.org/abs/2102.02876",
          "publishedOn": "2021-06-29T01:55:15.939Z",
          "wordCount": 561,
          "title": "Nonlinear Independent Component Analysis for Continuous-Time Signals. (arXiv:2102.02876v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:15.934Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mengying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinliang Wu</a>",
          "description": "In representation learning on the graph-structured data, under heterophily\n(or low homophily), many popular GNNs may fail to capture long-range\ndependencies, which leads to their performance degradation. To solve the\nabove-mentioned issue, we propose a graph convolutional networks with structure\nlearning (GCN-SL), and furthermore, the proposed approach can be applied to\nnode classification. The proposed GCN-SL contains two improvements:\ncorresponding to node features and edges, respectively. In the aspect of node\nfeatures, we propose an efficient-spectral-clustering (ESC) and an ESC with\nanchors (ESC-ANCH) algorithms to efficiently aggregate feature representations\nfrom all similar nodes. In the aspect of edges, we build a re-connected\nadjacency matrix by using a special data preprocessing technique and similarity\nlearning, and the re-connected adjacency matrix can be optimized directly along\nwith GCN-SL parameters. Considering that the original adjacency matrix may\nprovide misleading information for aggregation in GCN, especially the graphs\nbeing with a low level of homophily. The proposed GCN-SL can aggregate feature\nrepresentations from nearby nodes via re-connected adjacency matrix and is\napplied to graphs with various levels of homophily. Experimental results on a\nwide range of benchmark datasets illustrate that the proposed GCN-SL\noutperforms the stateof-the-art GNN counterparts.",
          "link": "http://arxiv.org/abs/2105.13795",
          "publishedOn": "2021-06-29T01:55:15.919Z",
          "wordCount": 657,
          "title": "GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily. (arXiv:2105.13795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">WenYong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakria/0/1/0/all/0/1\">Zakria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Ting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1\">Waqar Ali</a>",
          "description": "The widespread significance of Android IoT devices is due to its flexibility\nand hardware support features which revolutionized the digital world by\nintroducing exciting applications almost in all walks of daily life, such as\nhealthcare, smart cities, smart environments, safety, remote sensing, and many\nmore. Such versatile applicability gives incentive for more malware attacks. In\nthis paper, we propose a framework which continuously aggregates multiple user\ntrained models on non-overlapping data into single model. Specifically for\nmalware detection task, (i) we propose a novel user (local) neural network\n(LNN) which trains on local distribution and (ii) then to assure the model\nauthenticity and quality, we propose a novel smart contract which enable\naggregation process over blokchain platform. The LNN model analyzes various\nstatic and dynamic features of both malware and benign whereas the smart\ncontract verifies the malicious applications both for uploading and downloading\nprocesses in the network using stored aggregated features of local models. In\nthis way, the proposed model not only improves malware detection accuracy using\ndecentralized model network but also model efficacy with blockchain. We\nevaluate our approach with three state-of-the-art models and performed deep\nanalyses of extracted features of the relative model.",
          "link": "http://arxiv.org/abs/2102.13376",
          "publishedOn": "2021-06-29T01:55:15.913Z",
          "wordCount": 671,
          "title": "Collective Intelligence: Decentralized Learning for Android Malware Detection in IoT with Blockchain. (arXiv:2102.13376v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoneyama_R/0/1/0/all/0/1\">Reo Yoneyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Chiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "We propose a unified approach to data-driven source-filter modeling using a\nsingle neural network for developing a neural vocoder capable of generating\nhigh-quality synthetic speech waveforms while retaining flexibility of the\nsource-filter model to control their voice characteristics. Our proposed\nnetwork called unified source-filter generative adversarial networks (uSFGAN)\nis developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the\nneural vocoders based on a single neural network, into a source excitation\ngeneration network and a vocal tract resonance filtering network by\nadditionally implementing a regularization loss. Moreover, inspired by neural\nsource filter (NSF), only a sinusoidal waveform is additionally used as the\nsimplest clue to generate a periodic source excitation waveform while\nminimizing the effect of approximations in the source filter model. The\nexperimental results demonstrate that uSFGAN outperforms conventional neural\nvocoders, such as QPPWG and NSF in both speech quality and pitch\ncontrollability.",
          "link": "http://arxiv.org/abs/2104.04668",
          "publishedOn": "2021-06-29T01:55:15.879Z",
          "wordCount": 632,
          "title": "Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN. (arXiv:2104.04668v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:15.726Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1\">Emadeldeen Eldele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1\">Mohamed Ragab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Learning decent representations from unlabeled time-series data with temporal\ndynamics is a very challenging task. In this paper, we propose an unsupervised\nTime-Series representation learning framework via Temporal and Contextual\nContrasting (TS-TCC), to learn time-series representation from unlabeled data.\nFirst, the raw time-series data are transformed into two different yet\ncorrelated views by using weak and strong augmentations. Second, we propose a\nnovel temporal contrasting module to learn robust temporal representations by\ndesigning a tough cross-view prediction task. Last, to further learn\ndiscriminative representations, we propose a contextual contrasting module\nbuilt upon the contexts from the temporal contrasting module. It attempts to\nmaximize the similarity among different contexts of the same sample while\nminimizing similarity among contexts of different samples. Experiments have\nbeen carried out on three real-world time-series datasets. The results manifest\nthat training a linear classifier on top of the features learned by our\nproposed TS-TCC performs comparably with the supervised training. Additionally,\nour proposed TS-TCC shows high efficiency in few-labeled data and transfer\nlearning scenarios. The code is publicly available at\nhttps://github.com/emadeldeen24/TS-TCC.",
          "link": "http://arxiv.org/abs/2106.14112",
          "publishedOn": "2021-06-29T01:55:15.720Z",
          "wordCount": 628,
          "title": "Time-Series Representation Learning via Temporal and Contextual Contrasting. (arXiv:2106.14112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1\">Veda C. Storey</a>",
          "description": "Both conceptual modeling and machine learning have long been recognized as\nimportant areas of research. With the increasing emphasis on digitizing and\nprocessing large amounts of data for business and other applications, it would\nbe helpful to consider how these areas of research can complement each other.\nTo understand how they can be paired, we provide an overview of machine\nlearning foundations and development cycle. We then examine how conceptual\nmodeling can be applied to machine learning and propose a framework for\nincorporating conceptual modeling into data science projects. The framework is\nillustrated by applying it to a healthcare application. For the inverse\npairing, machine learning can impact conceptual modeling through text and rule\nmining, as well as knowledge graphs. The pairing of conceptual modeling and\nmachine learning in this this way should help lay the foundations for future\nresearch.",
          "link": "http://arxiv.org/abs/2106.14251",
          "publishedOn": "2021-06-29T01:55:15.703Z",
          "wordCount": 579,
          "title": "Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis…",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:15.690Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:15.682Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:15.675Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "The learning efficiency and generalization ability of an intelligent agent\ncan be greatly improved by utilizing a useful set of skills. However, the\ndesign of robot skills can often be intractable in real-world applications due\nto the prohibitive amount of effort and expertise that it requires. In this\nwork, we introduce Skill Learning In Diversified Environments (SLIDE), a method\nto discover generalizable skills via automated generation of a diverse set of\ntasks. As opposed to prior work on unsupervised discovery of skills which\nincentivizes the skills to produce different outcomes in the same environment,\nour method pairs each skill with a unique task produced by a trainable task\ngenerator. To encourage generalizable skills to emerge, our method trains each\nskill to specialize in the paired task and maximizes the diversity of the\ngenerated tasks. A task discriminator defined on the robot behaviors in the\ngenerated tasks is jointly trained to estimate the evidence lower bound of the\ndiversity objective. The learned skills can then be composed in a hierarchical\nreinforcement learning algorithm to solve unseen target tasks. We demonstrate\nthat the proposed method can effectively learn a variety of robot skills in two\ntabletop manipulation domains. Our results suggest that the learned skills can\neffectively improve the robot's performance in various unseen target tasks\ncompared to existing reinforcement learning and skill learning methods.",
          "link": "http://arxiv.org/abs/2106.13935",
          "publishedOn": "2021-06-29T01:55:15.661Z",
          "wordCount": 664,
          "title": "Discovering Generalizable Skills via Automated Generation of Diverse Tasks. (arXiv:2106.13935v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1\">Alkis Kalavasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavropoulos_K/0/1/0/all/0/1\">Konstantinos Stavropoulos</a>",
          "description": "We consider the problem of learning the true ordering of a set of\nalternatives from largely incomplete and noisy rankings. We introduce a natural\ngeneralization of both the classical Mallows model of ranking distributions and\nthe extensively studied model of noisy pairwise comparisons. Our selective\nMallows model outputs a noisy ranking on any given subset of alternatives,\nbased on an underlying Mallows distribution. Assuming a sequence of subsets\nwhere each pair of alternatives appears frequently enough, we obtain strong\nasymptotically tight upper and lower bounds on the sample complexity of\nlearning the underlying complete ranking and the (identities and the) ranking\nof the top-k alternatives from selective Mallows rankings. Moreover, building\non the work of (Braverman and Mossel, 2009), we show how to efficiently compute\nthe maximum likelihood complete ranking from selective Mallows rankings.",
          "link": "http://arxiv.org/abs/2011.00810",
          "publishedOn": "2021-06-29T01:55:15.655Z",
          "wordCount": 637,
          "title": "Aggregating Incomplete and Noisy Rankings. (arXiv:2011.00810v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:15.649Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15421",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1\">Arkajyoti Saha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1\">Sumanta Basu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Datta_A/0/1/0/all/0/1\">Abhirup Datta</a>",
          "description": "Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n\nWe propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n\nWe establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n\nWe empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.",
          "link": "http://arxiv.org/abs/2007.15421",
          "publishedOn": "2021-06-29T01:55:15.643Z",
          "wordCount": 740,
          "title": "Random Forests for dependent data. (arXiv:2007.15421v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02373",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbin Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1\">Tam Le</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Estimating mutual information is an important statistics and machine learning\nproblem. To estimate the mutual information from data, a common practice is\npreparing a set of paired samples $\\{(\\mathbf{x}_i,\\mathbf{y}_i)\\}_{i=1}^n\n\\stackrel{\\mathrm{i.i.d.}}{\\sim} p(\\mathbf{x},\\mathbf{y})$. However, in many\nsituations, it is difficult to obtain a large number of data pairs. To address\nthis problem, we propose the semi-supervised Squared-loss Mutual Information\n(SMI) estimation method using a small number of paired samples and the\navailable unpaired ones. We first represent SMI through the density ratio\nfunction, where the expectation is approximated by the samples from marginals\nand its assignment parameters. The objective is formulated using the optimal\ntransport problem and quadratic programming. Then, we introduce the\nLeast-Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algorithm for\nefficient optimization. Through experiments, we first demonstrate that the\nproposed method can estimate the SMI without a large number of paired samples.\nThen, we show the effectiveness of the proposed LSMI-Sinkhorn algorithm on\nvarious types of machine learning problems such as image matching and photo\nalbum summarization. Code can be found at\nhttps://github.com/csyanbin/LSMI-Sinkhorn.",
          "link": "http://arxiv.org/abs/1909.02373",
          "publishedOn": "2021-06-29T01:55:15.637Z",
          "wordCount": 642,
          "title": "LSMI-Sinkhorn: Semi-supervised Mutual Information Estimation with Optimal Transport. (arXiv:1909.02373v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>",
          "description": "We study the best-arm identification problem with fixed confidence when\ncontextual (covariate) information is available in stochastic bandits. Although\nwe can use contextual information in each round, we are interested in the\nmarginalized mean reward over the contextual distribution. Our goal is to\nidentify the best arm with a minimal number of samplings under a given value of\nthe error rate. We show the instance-specific sample complexity lower bounds\nfor the problem. Then, we propose a context-aware version of the\n\"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the\nset of optimal allocations and prove that the expected number of arm draws\nmatches the lower bound asymptotically. We demonstrate that contextual\ninformation can be used to improve the efficiency of the identification of the\nbest marginalized mean reward compared with the results of Garivier & Kaufmann\n(2016). We experimentally confirm that context information contributes to\nfaster best-arm identification.",
          "link": "http://arxiv.org/abs/2106.14077",
          "publishedOn": "2021-06-29T01:55:15.620Z",
          "wordCount": 596,
          "title": "The Role of Contextual Information in Best Arm Identification. (arXiv:2106.14077v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternative\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-06-29T01:55:15.614Z",
          "wordCount": 735,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschaikowski_M/0/1/0/all/0/1\">Max Tschaikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teschl_G/0/1/0/all/0/1\">Gerald Teschl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Continuous-depth neural models, where the derivative of the model's hidden\nstate is defined by a neural network, have enabled strong sequential data\nprocessing capabilities. However, these models rely on advanced numerical\ndifferential equation (DE) solvers resulting in a significant overhead both in\nterms of computational cost and model complexity. In this paper, we present a\nnew family of models, termed Closed-form Continuous-depth (CfC) networks, that\nare simple to describe and at least one order of magnitude faster while\nexhibiting equally strong modeling abilities compared to their ODE-based\ncounterparts. The models are hereby derived from the analytical closed-form\nsolution of an expressive subset of time-continuous models, thus alleviating\nthe need for complex DE solvers all together. In our experimental evaluations,\nwe demonstrate that CfC networks outperform advanced, recurrent models over a\ndiverse set of time-series prediction tasks, including those with long-term\ndependencies and irregularly sampled data. We believe our findings open new\nopportunities to train and deploy rich, continuous neural models in\nresource-constrained settings, which demand both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.13898",
          "publishedOn": "2021-06-29T01:55:15.608Z",
          "wordCount": 615,
          "title": "Closed-form Continuous-Depth Models. (arXiv:2106.13898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14122",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1\">Joseph Salmon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "The widespread use of machine learning algorithms calls for automatic change\ndetection algorithms to monitor their behavior over time. As a machine learning\nalgorithm learns from a continuous, possibly evolving, stream of data, it is\ndesirable and often critical to supplement it with a companion change detection\nalgorithm to facilitate its monitoring and control. We present a generic\nscore-based change detection method that can detect a change in any number of\ncomponents of a machine learning model trained via empirical risk minimization.\nThis proposed statistical hypothesis test can be readily implemented for such\nmodels designed within a differentiable programming framework. We establish the\nconsistency of the hypothesis test and show how to calibrate it to achieve a\nprescribed false alarm rate. We illustrate the versatility of the approach on\nsynthetic and real data.",
          "link": "http://arxiv.org/abs/2106.14122",
          "publishedOn": "2021-06-29T01:55:15.603Z",
          "wordCount": 563,
          "title": "Score-Based Change Detection for Gradient-Based Learning Machines. (arXiv:2106.14122v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:15.597Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:15.581Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muravev_N/0/1/0/all/0/1\">Nikita Muravev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "We propose a novel approach of randomized smoothing over multiplicative\nparameters. Using this method we construct certifiably robust classifiers with\nrespect to a gamma-correction perturbation and compare the result with\nclassifiers obtained via Gaussian smoothing. To the best of our knowledge it is\nthe first work concerning certified robustness against the multiplicative\ngamma-correction transformation.",
          "link": "http://arxiv.org/abs/2106.14432",
          "publishedOn": "2021-06-29T01:55:15.575Z",
          "wordCount": 480,
          "title": "Certified Robustness via Randomized Smoothing over Multiplicative Parameters. (arXiv:2106.14432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:15.569Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngnam Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jineon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongmin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_Y/0/1/0/all/0/1\">Yugeun Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seewoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jonghun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_C/0/1/0/all/0/1\">Chan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jaewe Heo</a>",
          "description": "Like many other domains in Artificial Intelligence (AI), there are specific\ntasks in the field of AI in Education (AIEd) for which labels are scarce and\nexpensive, such as predicting exam score or review correctness. A common way of\ncircumventing label-scarce problems is pre-training a model to learn\nrepresentations of the contents of learning items. However, such methods fail\nto utilize the full range of student interaction data available and do not\nmodel student learning behavior. To this end, we propose Assessment Modeling, a\nclass of fundamental pre-training tasks for general interactive educational\nsystems. An assessment is a feature of student-system interactions which can\nserve as a pedagogical evaluation. Examples include the correctness and\ntimeliness of a student's answer. Assessment Modeling is the prediction of\nassessments conditioned on the surrounding context of interactions. Although it\nis natural to pre-train on interactive features available in large amounts,\nlimiting the prediction targets to assessments focuses the tasks' relevance to\nthe label-scarce educational problems and reduces less-relevant noise. While\nthe effectiveness of different combinations of assessments is open for\nexploration, we suggest Assessment Modeling as a first-order guiding principle\nfor selecting proper pre-training tasks for label-scarce educational problems.",
          "link": "http://arxiv.org/abs/2002.05505",
          "publishedOn": "2021-06-29T01:55:15.563Z",
          "wordCount": 717,
          "title": "Assessment Modeling: Fundamental Pre-training Tasks for Interactive Educational Systems. (arXiv:2002.05505v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:15.557Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11037",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "Low rank tensor approximation is a fundamental tool in modern machine\nlearning and data science. In this paper, we study the characterization,\nperturbation analysis, and an efficient sampling strategy for two primary\ntensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact\ntensor CUR decompositions for low multilinear rank tensors. We also present\ntheoretical error bounds of the tensor CUR approximations when (adversarial or\nGaussian) noise appears. Moreover, we show that low cost uniform sampling is\nsufficient for tensor CUR approximations if the tensor has an incoherent\nstructure. Empirical performance evaluations, with both synthetic and\nreal-world datasets, establish the speed advantage of the tensor CUR\napproximations over other state-of-the-art low multilinear rank tensor\napproximations.",
          "link": "http://arxiv.org/abs/2103.11037",
          "publishedOn": "2021-06-29T01:55:15.540Z",
          "wordCount": 590,
          "title": "Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions. (arXiv:2103.11037v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>",
          "description": "Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.",
          "link": "http://arxiv.org/abs/2105.07674",
          "publishedOn": "2021-06-29T01:55:15.533Z",
          "wordCount": 572,
          "title": "Continual Learning with Echo State Networks. (arXiv:2105.07674v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1\">David H. Wolpert</a>",
          "description": "The important recent book by G. Schurz appreciates that the no-free-lunch\ntheorems (NFL) have major implications for the problem of (meta) induction.\nHere I review the NFL theorems, emphasizing that they do not only concern the\ncase where there is a uniform prior -- they prove that there are \"as many\npriors\" (loosely speaking) for which any induction algorithm $A$\nout-generalizes some induction algorithm $B$ as vice-versa. Importantly though,\nin addition to the NFL theorems, there are many \\textit{free lunch} theorems.\nIn particular, the NFL theorems can only be used to compare the\n\\textit{marginal} expected performance of an induction algorithm $A$ with the\nmarginal expected performance of an induction algorithm $B$. There is a rich\nset of free lunches which instead concern the statistical correlations among\nthe generalization errors of induction algorithms. As I describe, the\nmeta-induction algorithms that Schurz advocate as a \"solution to Hume's\nproblem\" are just an example of such a free lunch based on correlations among\nthe generalization errors of induction algorithms. I end by pointing out that\nthe prior that Schurz advocates, which is uniform over bit frequencies rather\nthan bit patterns, is contradicted by thousands of experiments in statistical\nphysics and by the great success of the maximum entropy procedure in inductive\ninference.",
          "link": "http://arxiv.org/abs/2103.11956",
          "publishedOn": "2021-06-29T01:55:15.528Z",
          "wordCount": 664,
          "title": "The Implications of the No-Free-Lunch Theorems for Meta-induction. (arXiv:2103.11956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "In many important graph data processing applications the acquired information\nincludes both node features and observations of the graph topology. Graph\nneural networks (GNNs) are designed to exploit both sources of evidence but\nthey do not optimally trade-off their utility and integrate them in a manner\nthat is also universal. Here, universality refers to independence on homophily\nor heterophily graph assumptions. We address these issues by introducing a new\nGeneralized PageRank (GPR) GNN architecture that adaptively learns the GPR\nweights so as to jointly optimize node feature and topological information\nextraction, regardless of the extent to which the node labels are homophilic or\nheterophilic. Learned GPR weights automatically adjust to the node label\npattern, irrelevant on the type of initialization, and thereby guarantee\nexcellent learning performance for label patterns that are usually hard to\nhandle. Furthermore, they allow one to avoid feature over-smoothing, a process\nwhich renders feature information nondiscriminative, without requiring the\nnetwork to be shallow. Our accompanying theoretical analysis of the GPR-GNN\nmethod is facilitated by novel synthetic benchmark datasets generated by the\nso-called contextual stochastic block model. We also compare the performance of\nour GNN architecture with that of several state-of-the-art GNNs on the problem\nof node-classification, using well-known benchmark homophilic and heterophilic\ndatasets. The results demonstrate that GPR-GNN offers significant performance\nimprovement compared to existing techniques on both synthetic and benchmark\ndata.",
          "link": "http://arxiv.org/abs/2006.07988",
          "publishedOn": "2021-06-29T01:55:15.522Z",
          "wordCount": 716,
          "title": "Adaptive Universal Generalized PageRank Graph Neural Network. (arXiv:2006.07988v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12909",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Narita_Y/0/1/0/all/0/1\">Yusuke Narita</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yata_K/0/1/0/all/0/1\">Kohei Yata</a>",
          "description": "Algorithms produce a growing portion of decisions and recommendations both in\npolicy and business. Such algorithmic decisions are natural experiments\n(conditionally quasi-randomly assigned instruments) since the algorithms make\ndecisions based only on observable input variables. We use this observation to\ndevelop a treatment-effect estimator for a class of stochastic and\ndeterministic decision-making algorithms. Our estimator is shown to be\nconsistent and asymptotically normal for well-defined causal effects. A key\nspecial case of our estimator is a multidimensional regression discontinuity\ndesign. We apply our estimator to evaluate the effect of the Coronavirus Aid,\nRelief, and Economic Security (CARES) Act, where more than \\$175 billion worth\nof relief funding is allocated to hospitals via an algorithmic rule. Our\nestimates suggest that the relief funding has little effect on COVID-19-related\nhospital activity levels. Naive OLS and IV estimates exhibit substantial\nselection bias.",
          "link": "http://arxiv.org/abs/2104.12909",
          "publishedOn": "2021-06-29T01:55:15.515Z",
          "wordCount": 642,
          "title": "Algorithm is Experiment: Machine Learning, Market Design, and Policy Eligibility Rules. (arXiv:2104.12909v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1\">Medina Andresel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1\">Csaba Domokos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1\">Daria Stepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Kien Tran</a>",
          "description": "Recently, low-dimensional vector space representations of knowledge graphs\n(KGs) have been applied to find answers to conjunctive queries (CQs) over\nincomplete KGs. However, the current methods only focus on inductive reasoning,\ni.e. answering CQs by predicting facts based on patterns learned from the data,\nand lack the ability of deductive reasoning by applying external domain\nknowledge. Such (expert or commonsense) domain knowledge is an invaluable\nresource which can be used to advance machine intelligence. To address this\nshortcoming, we introduce a neural-symbolic method for ontology-mediated CQ\nanswering over incomplete KGs that operates in the embedding space. More\nspecifically, we propose various data augmentation strategies to generate\ntraining queries using query-rewriting based methods and then exploit a novel\nloss function for training the model. The experimental results demonstrate the\neffectiveness of our training strategies and the new loss function, i.e., our\nmethod significantly outperforms the baseline in the settings that require both\ninductive and deductive reasoning.",
          "link": "http://arxiv.org/abs/2106.14052",
          "publishedOn": "2021-06-29T01:55:15.493Z",
          "wordCount": 589,
          "title": "A Neural-symbolic Approach for Ontology-mediated Query Answering. (arXiv:2106.14052v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:15.485Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:15.473Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:15.467Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:15.451Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14238",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1\">James D. Wilson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Jihui Lee</a>",
          "description": "We consider the problem of interpretable network representation learning for\nsamples of network-valued data. We propose the Principal Component Analysis for\nNetworks (PCAN) algorithm to identify statistically meaningful low-dimensional\nrepresentations of a network sample via subgraph count statistics. The PCAN\nprocedure provides an interpretable framework for which one can readily\nvisualize, explore, and formulate predictive models for network samples. We\nfurthermore introduce a fast sampling-based algorithm, sPCAN, which is\nsignificantly more computationally efficient than its counterpart, but still\nenjoys advantages of interpretability. We investigate the relationship between\nthese two methods and analyze their large-sample properties under the common\nregime where the sample of networks is a collection of kernel-based random\ngraphs. We show that under this regime, the embeddings of the sPCAN method\nenjoy a central limit theorem and moreover that the population level embeddings\nof PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,\ncluster, and classify observations in network samples arising in nature,\nincluding functional connectivity network samples and dynamic networks\ndescribing the political co-voting habits of the U.S. Senate. Our analyses\nreveal that our proposed algorithm provides informative and discriminatory\nfeatures describing the networks in each sample. The PCAN and sPCAN methods\nbuild on the current literature of network representation learning and set the\nstage for a new line of research in interpretable learning on network-valued\ndata. Publicly available software for the PCAN and sPCAN methods are available\nat https://www.github.com/jihuilee/.",
          "link": "http://arxiv.org/abs/2106.14238",
          "publishedOn": "2021-06-29T01:55:15.445Z",
          "wordCount": 678,
          "title": "Interpretable Network Representation Learning with Principal Component Analysis. (arXiv:2106.14238v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:15.437Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1\">Andr&#xe9; Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghoussi_Y/0/1/0/all/0/1\">Yassine Baghoussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Carlos Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_Moreira_J/0/1/0/all/0/1\">Jo&#xe3;o Mendes-Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arantes_M/0/1/0/all/0/1\">Miguel Arantes</a>",
          "description": "Forecasting accuracy is reliant on the quality of available past data. Data\ndisruptions can adversely affect the quality of the generated model (e.g.\nunexpected events such as out-of-stock products when forecasting demand). We\naddress this problem by pastcasting: predicting how data should have been in\nthe past to explain the future better. We propose Pastprop-LSTM, a data-centric\nbackpropagation algorithm that assigns part of the responsibility for errors to\nthe training data and changes it accordingly. We test three variants of\nPastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta\nAnomaly Benchmark. Empirical evaluation indicates that the proposed method can\nimprove forecasting accuracy, especially when the prediction errors of standard\nLSTM are high. It also demonstrates the potential of the algorithm on datasets\ncontaining anomalies.",
          "link": "http://arxiv.org/abs/2106.13881",
          "publishedOn": "2021-06-29T01:55:15.430Z",
          "wordCount": 561,
          "title": "Pastprop-RNN: improved predictions of the future by correcting the past. (arXiv:2106.13881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:15.424Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:15.400Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Alvaro Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1\">Simon M. Lucas</a>",
          "description": "A large part of the interest in model-based reinforcement learning derives\nfrom the potential utility to acquire a forward model capable of strategic long\nterm decision making. Assuming that an agent succeeds in learning a useful\npredictive model, it still requires a mechanism to harness it to generate and\nselect among competing simulated plans. In this paper, we explore this theme\ncombining evolutionary algorithmic planning techniques with models learned via\ndeep learning and variational inference. We demonstrate the approach with an\nagent that reliably performs online planning in a set of visual navigation\ntasks.",
          "link": "http://arxiv.org/abs/2106.13911",
          "publishedOn": "2021-06-29T01:55:15.394Z",
          "wordCount": 548,
          "title": "Predictive Control Using Learned State Space Models via Rolling Horizon Evolution. (arXiv:2106.13911v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianfeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zuowei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijun Zhang</a>",
          "description": "This paper establishes the optimal approximation error characterization of\ndeep ReLU networks for smooth functions in terms of both width and depth\nsimultaneously. To that end, we first prove that multivariate polynomials can\nbe approximated by deep ReLU networks of width $\\mathcal{O}(N)$ and depth\n$\\mathcal{O}(L)$ with an approximation error $\\mathcal{O}(N^{-L})$. Through\nlocal Taylor expansions and their deep ReLU network approximations, we show\nthat deep ReLU networks of width $\\mathcal{O}(N\\ln N)$ and depth\n$\\mathcal{O}(L\\ln L)$ can approximate $f\\in C^s([0,1]^d)$ with a nearly optimal\napproximation error $\\mathcal{O}(\\|f\\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our\nestimate is non-asymptotic in the sense that it is valid for arbitrary width\nand depth specified by $N\\in\\mathbb{N}^+$ and $L\\in\\mathbb{N}^+$, respectively.",
          "link": "http://arxiv.org/abs/2001.03040",
          "publishedOn": "2021-06-28T01:57:57.979Z",
          "wordCount": 600,
          "title": "Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mehdi Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Charles Tapley Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermue_L/0/1/0/all/0/1\">Laurent Vermue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Asja Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>",
          "description": "The heterogeneity in recently published knowledge graph embedding models'\nimplementations, training, and evaluation has made fair and thorough\ncomparisons difficult. In order to assess the reproducibility of previously\npublished results, we re-implemented and evaluated 21 interaction models in the\nPyKEEN software package. Here, we outline which results could be reproduced\nwith their reported hyper-parameters, which could only be reproduced with\nalternate hyper-parameters, and which could not be reproduced at all as well as\nprovide insight as to why this might be the case.\n\nWe then performed a large-scale benchmarking on four datasets with several\nthousands of experiments and 24,804 GPU hours of computation time. We present\ninsights gained as to best practices, best configurations for each model, and\nwhere improvements could be made over previously published best configurations.\nOur results highlight that the combination of model architecture, training\napproach, loss function, and the explicit modeling of inverse relations is\ncrucial for a model's performances, and not only determined by the model\narchitecture. We provide evidence that several architectures can obtain results\ncompetitive to the state-of-the-art when configured carefully. We have made all\ncode, experimental configurations, results, and analyses that lead to our\ninterpretations available at https://github.com/pykeen/pykeen and\nhttps://github.com/pykeen/benchmarking",
          "link": "http://arxiv.org/abs/2006.13365",
          "publishedOn": "2021-06-28T01:57:57.973Z",
          "wordCount": 719,
          "title": "Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework. (arXiv:2006.13365v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farid_A/0/1/0/all/0/1\">Alec Farid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veer_S/0/1/0/all/0/1\">Sushant Veer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Anirudha Majumdar</a>",
          "description": "Our goal is to perform out-of-distribution (OOD) detection, i.e., to detect\nwhen a robot is operating in environments that are drawn from a different\ndistribution than the environments used to train the robot. We leverage\nProbably Approximately Correct (PAC)-Bayes theory in order to train a policy\nwith a guaranteed bound on performance on the training distribution. Our key\nidea for OOD detection then relies on the following intuition: violation of the\nperformance bound on test environments provides evidence that the robot is\noperating OOD. We formalize this via statistical techniques based on p-values\nand concentration inequalities. The resulting approach (i) provides guaranteed\nconfidence bounds on OOD detection, and (ii) is task-driven and sensitive only\nto changes that impact the robot's performance. We demonstrate our approach on\na simulated example of grasping objects with unfamiliar poses or shapes. We\nalso present both simulation and hardware experiments for a drone performing\nvision-based obstacle avoidance in unfamiliar environments (including wind\ndisturbances and different obstacle densities). Our examples demonstrate that\nwe can perform task-driven OOD detection within just a handful of trials.\nComparisons with baselines also demonstrate the advantages of our approach in\nterms of providing statistical guarantees and being insensitive to\ntask-irrelevant distribution shifts.",
          "link": "http://arxiv.org/abs/2106.13703",
          "publishedOn": "2021-06-28T01:57:57.965Z",
          "wordCount": 637,
          "title": "Task-Driven Out-of-Distribution Detection with Statistical Guarantees for Robot Learning. (arXiv:2106.13703v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13781",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1\">Yuejiao Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Stochastic nested optimization, including stochastic compositional, min-max\nand bilevel optimization, is gaining popularity in many machine learning\napplications. While the three problems share the nested structure, existing\nworks often treat them separately, and thus develop problem-specific algorithms\nand their analyses. Among various exciting developments, simple SGD-type\nupdates (potentially on multiple variables) are still prevalent in solving this\nclass of nested problems, but they are believed to have slower convergence rate\ncompared to that of the non-nested problems. This paper unifies several\nSGD-type updates for stochastic nested problems into a single SGD approach that\nwe term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging\nthe hidden smoothness of the problem, this paper presents a tighter analysis of\nALSET for stochastic nested problems. Under the new analysis, to achieve an\n$\\epsilon$-stationary point of the nested problem, it requires ${\\cal\nO}(\\epsilon^{-2})$ samples. Under certain regularity conditions, applying our\nresults to stochastic compositional, min-max and reinforcement learning\nproblems either improves or matches the best-known sample complexity in the\nrespective cases. Our results explain why simple SGD-type algorithms in\nstochastic nested problems all work very well in practice without the need for\nfurther modifications.",
          "link": "http://arxiv.org/abs/2106.13781",
          "publishedOn": "2021-06-28T01:57:57.960Z",
          "wordCount": 638,
          "title": "Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems. (arXiv:2106.13781v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2003.12319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andreoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porte_X/0/1/0/all/0/1\">Xavier Porte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chretien_S/0/1/0/all/0/1\">St&#xe9;phane Chr&#xe9;tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larger_L/0/1/0/all/0/1\">Laurent Larger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_D/0/1/0/all/0/1\">Daniel Brunner</a>",
          "description": "A high efficiency hardware integration of neural networks benefits from\nrealizing nonlinearity, network connectivity and learning fully in a physical\nsubstrate. Multiple systems have recently implemented some or all of these\noperations, yet the focus was placed on addressing technological challenges.\nFundamental questions regarding learning in hardware neural networks remain\nlargely unexplored. Noise in particular is unavoidable in such architectures,\nand here we investigate its interaction with a learning algorithm using an\nopto-electronic recurrent neural network. We find that noise strongly modifies\nthe system's path during convergence, and surprisingly fully decorrelates the\nfinal readout weight matrices. This highlights the importance of understanding\narchitecture, noise and learning algorithm as interacting players, and\ntherefore identifies the need for mathematical tools for noisy, analogue system\noptimization.",
          "link": "http://arxiv.org/abs/2003.12319",
          "publishedOn": "2021-06-28T01:57:57.953Z",
          "wordCount": 613,
          "title": "Boolean learning under noise-perturbations in hardware neural networks. (arXiv:2003.12319v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiding Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_V/0/1/0/all/0/1\">Vaishnavh Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_C/0/1/0/all/0/1\">Christina Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>",
          "description": "We empirically show that the test error of deep networks can be estimated by\nsimply training the same architecture on the same training set but with a\ndifferent run of Stochastic Gradient Descent (SGD), and measuring the\ndisagreement rate between the two networks on unlabeled test data. This builds\non -- and is a stronger version of -- the observation in Nakkiran & Bansal '20,\nwhich requires the second run to be on an altogether fresh training set. We\nfurther theoretically show that this peculiar phenomenon arises from the\n\\emph{well-calibrated} nature of \\emph{ensembles} of SGD-trained models. This\nfinding not only provides a simple empirical measure to directly predict the\ntest error using unlabeled test data, but also establishes a new conceptual\nconnection between generalization and calibration.",
          "link": "http://arxiv.org/abs/2106.13799",
          "publishedOn": "2021-06-28T01:57:57.932Z",
          "wordCount": 562,
          "title": "Assessing Generalization of SGD via Disagreement. (arXiv:2106.13799v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13755",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Angiuli_A/0/1/0/all/0/1\">Andrea Angiuli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fouque_J/0/1/0/all/0/1\">Jean-Pierre Fouque</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1\">Mathieu Lauriere</a>",
          "description": "Mean field games (MFG) and mean field control problems (MFC) are frameworks\nto study Nash equilibria or social optima in games with a continuum of agents.\nThese problems can be used to approximate competitive or cooperative games with\na large finite number of agents and have found a broad range of applications,\nin particular in economics. In recent years, the question of learning in MFG\nand MFC has garnered interest, both as a way to compute solutions and as a way\nto model how large populations of learners converge to an equilibrium. Of\nparticular interest is the setting where the agents do not know the model,\nwhich leads to the development of reinforcement learning (RL) methods. After\nreviewing the literature on this topic, we present a two timescale approach\nwith RL for MFG and MFC, which relies on a unified Q-learning algorithm. The\nmain novelty of this method is to simultaneously update an action-value\nfunction and a distribution but with different rates, in a model-free fashion.\nDepending on the ratio of the two learning rates, the algorithm learns either\nthe MFG or the MFC solution. To illustrate this method, we apply it to a mean\nfield problem of accumulated consumption in finite horizon with HARA utility\nfunction, and to a trader's optimal liquidation problem.",
          "link": "http://arxiv.org/abs/2106.13755",
          "publishedOn": "2021-06-28T01:57:57.924Z",
          "wordCount": 652,
          "title": "Reinforcement Learning for Mean Field Games, with Applications to Economics. (arXiv:2106.13755v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kavouras_I/0/1/0/all/0/1\">Ioannis Kavouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaselimia_M/0/1/0/all/0/1\">Maria Kaselimia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardis_E/0/1/0/all/0/1\">Emmanuel Sardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>",
          "description": "In this work we investigate the short-term variations in air quality\nemissions, attributed to the prevention measures, applied in different cities,\nto mitigate the COVID-19 spread. In particular, we emphasize on the\nconcentration effects regarding specific pollutant gases, such as carbon\nmonoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2).\nThe assessment of the impact of lockdown on air quality focused on four\nEuropean Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant\nfactors were obtained using global satellite observations. The level of the\nemployed prevention measures is employed using the Oxford COVID-19 Government\nResponse Tracker. The second part of the analysis employed a variety of machine\nlearning tools, utilized for estimating the concentration of each pollutant,\ntwo days ahead. The results showed that a weak to moderate correlation exists\nbetween the corresponding measures and the pollutant factors and that it is\npossible to create models which can predict the behaviour of the pollutant\ngases under daily human activities.",
          "link": "http://arxiv.org/abs/2106.13750",
          "publishedOn": "2021-06-28T01:57:57.919Z",
          "wordCount": 648,
          "title": "Assessing the Lockdown Effects on Air Quality during COVID-19 Era. (arXiv:2106.13750v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1\">Federico Errica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1\">Alessio Micheli</a>",
          "description": "We introduce the Graph Mixture Density Networks, a new family of machine\nlearning models that can fit multimodal output distributions conditioned on\ngraphs of arbitrary topology. By combining ideas from mixture models and graph\nrepresentation learning, we address a broader class of challenging conditional\ndensity estimation problems that rely on structured data. In this respect, we\nevaluate our method on a new benchmark application that leverages random graphs\nfor stochastic epidemic simulations. We show a significant improvement in the\nlikelihood of epidemic outcomes when taking into account both multimodality and\nstructure. The empirical analysis is complemented by two real-world regression\ntasks showing the effectiveness of our approach in modeling the output\nprediction uncertainty. Graph Mixture Density Networks open appealing research\nopportunities in the study of structure-dependent phenomena that exhibit\nnon-trivial conditional output distributions.",
          "link": "http://arxiv.org/abs/2012.03085",
          "publishedOn": "2021-06-28T01:57:57.911Z",
          "wordCount": 613,
          "title": "Graph Mixture Density Networks. (arXiv:2012.03085v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13727",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Fuhg_J/0/1/0/all/0/1\">Jan Niklas Fuhg</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fau_A/0/1/0/all/0/1\">Am&#xe9;lie Fau</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bouklas_N/0/1/0/all/0/1\">Nikolaos Bouklas</a>",
          "description": "Temporally and spatially dependent uncertain parameters are regularly\nencountered in engineering applications. Commonly these uncertainties are\naccounted for using random fields and processes which require knowledge about\nthe appearing probability distributions functions which is not readily\navailable. In these cases non-probabilistic approaches such as interval\nanalysis and fuzzy set theory are helpful uncertainty measures. Partial\ndifferential equations involving fuzzy and interval fields are traditionally\nsolved using the finite element method where the input fields are sampled using\nsome basis function expansion methods. This approach however is problematic, as\nit is reliant on knowledge about the spatial correlation fields. In this work\nwe utilize physics-informed neural networks (PINNs) to solve interval and fuzzy\npartial differential equations. The resulting network structures termed\ninterval physics-informed neural networks (iPINNs) and fuzzy physics-informed\nneural networks (fPINNs) show promising results for obtaining bounded solutions\nof equations involving spatially uncertain parameter fields. In contrast to\nfinite element approaches, no correlation length specification of the input\nfields as well as no averaging via Monte-Carlo simulations are necessary. In\nfact, information about the input interval fields is obtained directly as a\nbyproduct of the presented solution scheme. Furthermore, all major advantages\nof PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse\nproblem set-up.",
          "link": "http://arxiv.org/abs/2106.13727",
          "publishedOn": "2021-06-28T01:57:57.905Z",
          "wordCount": 662,
          "title": "Interval and fuzzy physics-informed neural networks for uncertain fields. (arXiv:2106.13727v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chengshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Cong Shen</a>",
          "description": "We study a new stochastic multi-player multi-armed bandits (MP-MAB) problem,\nwhere the reward distribution changes if a collision occurs on the arm.\nExisting literature always assumes a zero reward for involved players if\ncollision happens, but for applications such as cognitive radio, the more\nrealistic scenario is that collision reduces the mean reward but not\nnecessarily to zero. We focus on the more practical no-sensing setting where\nplayers do not perceive collisions directly, and propose the Error-Correction\nCollision Communication (EC3) algorithm that models implicit communication as a\nreliable communication over noisy channel problem, for which random coding\nerror exponent is used to establish the optimal regret that no communication\nprotocol can beat. Finally, optimizing the tradeoff between code length and\ndecoding error rate leads to a regret that approaches the centralized MP-MAB\nregret, which represents a natural lower bound. Experiments with practical\nerror-correction codes on both synthetic and real-world datasets demonstrate\nthe superiority of EC3. In particular, the results show that the choice of\ncoding schemes has a profound impact on the regret performance.",
          "link": "http://arxiv.org/abs/2106.13669",
          "publishedOn": "2021-06-28T01:57:57.891Z",
          "wordCount": 625,
          "title": "Multi-player Multi-armed Bandits with Collision-Dependent Reward Distributions. (arXiv:2106.13669v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korine_R/0/1/0/all/0/1\">Ron Korine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_D/0/1/0/all/0/1\">Danny Hendler</a>",
          "description": "Numerous metamorphic and polymorphic malicious variants are generated\nautomatically on a daily basis by mutation engines that transform the code of a\nmalicious program while retaining its functionality, in order to evade\nsignature-based detection. These automatic processes have greatly increased the\nnumber of malware variants, deeming their fully-manual analysis impossible.\nMalware classification is the task of determining to which family a new\nmalicious variant belongs. Variants of the same malware family show similar\nbehavioral patterns. Thus, classifying newly discovered malicious programs and\napplications helps assess the risks they pose. Moreover, malware classification\nfacilitates determining which of the newly discovered variants should undergo\nmanual analysis by a security expert, in order to determine whether they belong\nto a new family (e.g., one whose members exploit a zero-day vulnerability) or\nare simply the result of a concept drift within a known malicious family. This\nmotivated intense research in recent years on devising high-accuracy automatic\ntools for malware classification. In this work, we present DAEMON - a novel\ndataset-agnostic malware classifier. A key property of DAEMON is that the type\nof features it uses and the manner in which they are mined facilitate\nunderstanding the distinctive behavior of malware families, making its\nclassification decisions explainable. We've optimized DAEMON using a\nlarge-scale dataset of x86 binaries, belonging to a mix of several malware\nfamilies targeting computers running Windows. We then re-trained it and applied\nit, without any algorithmic change, feature re-engineering or parameter tuning,\nto two other large-scale datasets of malicious Android applications consisting\nof numerous malware families. DAEMON obtained highly accurate classification\nresults on all datasets, establishing that it is also platform-agnostic.",
          "link": "http://arxiv.org/abs/2008.01855",
          "publishedOn": "2021-06-28T01:57:57.885Z",
          "wordCount": 729,
          "title": "DAEMON: Dataset-Agnostic Explainable Malware Classification Using Multi-Stage Feature Mining. (arXiv:2008.01855v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05888",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Ankush Chakrabarty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benosman_M/0/1/0/all/0/1\">Mouhacine Benosman</a>",
          "description": "Data generated from dynamical systems with unknown dynamics enable the\nlearning of state observers that are: robust to modeling error, computationally\ntractable to design, and capable of operating with guaranteed performance. In\nthis paper, a modular design methodology is formulated, that consists of three\ndesign phases: (i) an initial robust observer design that enables one to learn\nthe dynamics without allowing the state estimation error to diverge (hence,\nsafe); (ii) a learning phase wherein the unmodeled components are estimated\nusing Bayesian optimization and Gaussian processes; and, (iii) a re-design\nphase that leverages the learned dynamics to improve convergence rate of the\nstate estimation error. The potential of our proposed learning-based observer\nis demonstrated on a benchmark nonlinear system. Additionally, certificates of\nguaranteed estimation performance are provided.",
          "link": "http://arxiv.org/abs/2005.05888",
          "publishedOn": "2021-06-28T01:57:57.879Z",
          "wordCount": 595,
          "title": "Safe Learning-based Observers for Unknown Nonlinear Systems using Bayesian Optimization. (arXiv:2005.05888v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>",
          "description": "Recent advances in image synthesis enables one to translate images by\nlearning the mapping between a source domain and a target domain. Existing\nmethods tend to learn the distributions by training a model on a variety of\ndatasets, with results evaluated largely in a subjective manner. Relatively few\nworks in this area, however, study the potential use of semantic image\ntranslation methods for image recognition tasks. In this paper, we explore the\nuse of Single Image Texture Translation (SITT) for data augmentation. We first\npropose a lightweight model for translating texture to images based on a single\ninput of source texture, allowing for fast training and testing. Based on SITT,\nwe then explore the use of augmented data in long-tailed and few-shot image\nclassification tasks. We find the proposed method is capable of translating\ninput data into a target domain, leading to consistent improved image\nrecognition performance. Finally, we examine how SITT and related image\ntranslation methods can provide a basis for a data-efficient, augmentation\nengineering approach to model training.",
          "link": "http://arxiv.org/abs/2106.13804",
          "publishedOn": "2021-06-28T01:57:57.872Z",
          "wordCount": 612,
          "title": "Single Image Texture Translation for Data Augmentation. (arXiv:2106.13804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>",
          "description": "Genetic mutations can cause disease by disrupting normal gene function.\nIdentifying the disease-causing mutations from millions of genetic variants\nwithin an individual patient is a challenging problem. Computational methods\nwhich can prioritize disease-causing mutations have, therefore, enormous\napplications. It is well-known that genes function through a complex regulatory\nnetwork. However, existing variant effect prediction models only consider a\nvariant in isolation. In contrast, we propose VEGN, which models variant effect\nprediction using a graph neural network (GNN) that operates on a heterogeneous\ngraph with genes and variants. The graph is created by assigning variants to\ngenes and connecting genes with an gene-gene interaction network. In this\ncontext, we explore an approach where a gene-gene graph is given and another\nwhere VEGN learns the gene-gene graph and therefore operates both on given and\nlearnt edges. The graph neural network is trained to aggregate information\nbetween genes, and between genes and variants. Variants can exchange\ninformation via the genes they connect to. This approach improves the\nperformance of existing state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.13642",
          "publishedOn": "2021-06-28T01:57:57.866Z",
          "wordCount": 618,
          "title": "VEGN: Variant Effect Prediction with Graph Neural Networks. (arXiv:2106.13642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1\">Less Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1\">Nestor Demeure</a>",
          "description": "As optimizers are critical to the performances of neural networks, every year\na large number of papers innovating on the subject are published. However,\nwhile most of these publications provide incremental improvements to existing\nalgorithms, they tend to be presented as new optimizers rather than composable\nalgorithms. Thus, many worthwhile improvements are rarely seen out of their\ninitial publication. Taking advantage of this untapped potential, we introduce\nRanger21, a new optimizer which combines AdamW with eight components, carefully\nselected after reviewing and testing ideas from the literature. We found that\nthe resulting optimizer provides significantly improved validation accuracy and\ntraining speed, smoother training curves, and is even able to train a ResNet50\non ImageNet2012 without Batch Normalization layers. A problem on which AdamW\nstays systematically stuck in a bad initial state.",
          "link": "http://arxiv.org/abs/2106.13731",
          "publishedOn": "2021-06-28T01:57:57.858Z",
          "wordCount": 564,
          "title": "Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1\">Jochen Stiasny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misyris_G/0/1/0/all/0/1\">Georgios S. Misyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1\">Spyros Chatzivasileiadis</a>",
          "description": "Solving the ordinary differential equations that govern the power system is\nan indispensable part in transient stability analysis. However, the\ntraditionally applied methods either carry a significant computational burden,\nrequire model simplifications, or use overly conservative surrogate models.\nNeural networks can circumvent these limitations but are faced with high\ndemands on the used datasets. Furthermore, they are agnostic to the underlying\ngoverning equations. Physics-informed neural network tackle this problem and we\nexplore their advantages and challenges in this paper. We illustrate the\nfindings on the Kundur two-area system and highlight possible pathways forward\nin developing this method further.",
          "link": "http://arxiv.org/abs/2106.13638",
          "publishedOn": "2021-06-28T01:57:57.842Z",
          "wordCount": 531,
          "title": "Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Bayesian neural networks utilize probabilistic layers that capture\nuncertainty over weights and activations, and are trained using Bayesian\ninference. Since these probabilistic layers are designed to be drop-in\nreplacement of their deterministic counter parts, Bayesian neural networks\nprovide a direct and natural way to extend conventional deep neural networks to\nsupport probabilistic deep learning. However, it is nontrivial to understand,\ndesign and train Bayesian neural networks due to their complexities. We discuss\nthe essentials of Bayesian neural networks including duality (deep neural\nnetworks, probabilistic models), approximate Bayesian inference, Bayesian\npriors, Bayesian posteriors, and deep variational learning. We use TensorFlow\nProbability APIs and code examples for illustration. The main problem with\nBayesian neural networks is that the architecture of deep neural networks makes\nit quite redundant, and costly, to account for uncertainty for a large number\nof successive layers. Hybrid Bayesian neural networks, which use few\nprobabilistic layers judicially positioned in the networks, provide a practical\nsolution.",
          "link": "http://arxiv.org/abs/2106.13594",
          "publishedOn": "2021-06-28T01:57:57.835Z",
          "wordCount": 577,
          "title": "Bayesian Neural Networks: Essentials. (arXiv:2106.13594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaeili_B/0/1/0/all/0/1\">Babak Esmaeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wick_M/0/1/0/all/0/1\">Michael Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tristan_J/0/1/0/all/0/1\">Jean-Baptiste Tristan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>",
          "description": "In this paper, we propose conjugate energy-based models (CEBMs), a new class\nof energy-based models that define a joint density over data and latent\nvariables. The joint density of a CEBM decomposes into an intractable\ndistribution over data and a tractable posterior over latent variables. CEBMs\nhave similar use cases as variational autoencoders, in the sense that they\nlearn an unsupervised mapping from data to latent variables. However, these\nmodels omit a generator network, which allows them to learn more flexible\nnotions of similarity between data points. Our experiments demonstrate that\nconjugate EBMs achieve competitive results in terms of image modelling,\npredictive power of latent space, and out-of-domain detection on a variety of\ndatasets.",
          "link": "http://arxiv.org/abs/2106.13798",
          "publishedOn": "2021-06-28T01:57:57.828Z",
          "wordCount": 543,
          "title": "Conjugate Energy-Based Models. (arXiv:2106.13798v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scieur_D/0/1/0/all/0/1\">Damien Scieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsung Kim</a>",
          "description": "This paper considers classification problems with hierarchically organized\nclasses. We force the classifier (hyperplane) of each class to belong to a\nsphere manifold, whose center is the classifier of its super-class. Then,\nindividual sphere manifolds are connected based on their hierarchical\nrelations. Our technique replaces the last layer of a neural network by\ncombining a spherical fully-connected layer with a hierarchical layer. This\nregularization is shown to improve the performance of widely used deep neural\nnetwork architectures (ResNet and DenseNet) on publicly available datasets\n(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).",
          "link": "http://arxiv.org/abs/2106.13549",
          "publishedOn": "2021-06-28T01:57:57.814Z",
          "wordCount": 524,
          "title": "Connecting Sphere Manifolds Hierarchically for Regularization. (arXiv:2106.13549v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiwei Steven Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>",
          "description": "Providing privacy protection has been one of the primary motivations of\nFederated Learning (FL). Recently, there has been a line of work on\nincorporating the formal privacy notion of differential privacy with FL. To\nguarantee the client-level differential privacy in FL algorithms, the clients'\ntransmitted model updates have to be clipped before adding privacy noise. Such\nclipping operation is substantially different from its counterpart of gradient\nclipping in the centralized differentially private SGD and has not been\nwell-understood. In this paper, we first empirically demonstrate that the\nclipped FedAvg can perform surprisingly well even with substantial data\nheterogeneity when training neural networks, which is partly because the\nclients' updates become similar for several popular deep architectures. Based\non this key observation, we provide the convergence analysis of a differential\nprivate (DP) FedAvg algorithm and highlight the relationship between clipping\nbias and the distribution of the clients' updates. To the best of our\nknowledge, this is the first work that rigorously investigates theoretical and\nempirical issues regarding the clipping operation in FL algorithms.",
          "link": "http://arxiv.org/abs/2106.13673",
          "publishedOn": "2021-06-28T01:57:57.807Z",
          "wordCount": 620,
          "title": "Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy. (arXiv:2106.13673v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "In this paper, we propose a residual echo suppression method using a UNet\nneural network that directly maps the outputs of a linear acoustic echo\ncanceler to the desired signal in the spectral domain. This system embeds a\ndesign parameter that allows a tunable tradeoff between the desired-signal\ndistortion and residual echo suppression in double-talk scenarios. The system\nemploys 136 thousand parameters, and requires 1.6 Giga floating-point\noperations per second and 10 Mega-bytes of memory. The implementation satisfies\nboth the timing requirements of the AEC challenge and the computational and\nmemory limitations of on-device applications. Experiments are conducted with\n161~h of data from the AEC challenge database and from real independent\nrecordings. We demonstrate the performance of the proposed system in real-life\nconditions and compare it with two competing methods regarding echo suppression\nand desired-signal distortion, generalization to various environments, and\nrobustness to high echo levels.",
          "link": "http://arxiv.org/abs/2106.13531",
          "publishedOn": "2021-06-28T01:57:57.800Z",
          "wordCount": 608,
          "title": "Deep Residual Echo Suppression with A Tunable Tradeoff Between Signal Distortion and Echo Suppression. (arXiv:2106.13531v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kates_B/0/1/0/all/0/1\">Brandon Kates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentch_J/0/1/0/all/0/1\">Jeff Mentch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharkar_A/0/1/0/all/0/1\">Anant Kharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>",
          "description": "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.",
          "link": "http://arxiv.org/abs/2106.13743",
          "publishedOn": "2021-06-28T01:57:57.792Z",
          "wordCount": 677,
          "title": "Privileged Zero-Shot AutoML. (arXiv:2106.13743v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1\">Francesco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>",
          "description": "We introduce a novel architecture for graph networks which is equivariant to\nany transformation in the coordinate embeddings that preserves the distance\nbetween neighbouring nodes. In particular, it is equivariant to the Euclidean\nand conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance\nproperties, the proposed model is extremely more data efficient with respect to\nclassical graph architectures and also intrinsically equipped with a better\ninductive bias. We show that, learning on a minimal amount of data, the\narchitecture we propose can perfectly generalise to unseen data in a synthetic\nproblem, while much more training data are required from a standard model to\nreach comparable performance.",
          "link": "http://arxiv.org/abs/2106.13786",
          "publishedOn": "2021-06-28T01:57:57.784Z",
          "wordCount": 554,
          "title": "Data efficiency in graph networks through equivariance. (arXiv:2106.13786v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Longbing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiguo Gong</a>",
          "description": "The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.",
          "link": "http://arxiv.org/abs/2106.13732",
          "publishedOn": "2021-06-28T01:57:57.771Z",
          "wordCount": 695,
          "title": "Recurrent Coupled Topic Modeling over Sequential Documents. (arXiv:2106.13732v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Building classifiers on multiple domains is a practical problem in the real\nlife. Instead of building classifiers one by one, multi-domain learning (MDL)\nsimultaneously builds classifiers on multiple domains. MDL utilizes the\ninformation shared among the domains to improve the performance. As a\nsupervised learning problem, the labeling effort is still high in MDL problems.\nUsually, this high labeling cost issue could be relieved by using active\nlearning. Thus, it is natural to utilize active learning to reduce the labeling\neffort in MDL, and we refer this setting as multi-domain active learning\n(MDAL). However, there are only few works which are built on this setting. And\nwhen the researches have to face this problem, there is no off-the-shelf\nsolutions. Under this circumstance, combining the current multi-domain learning\nmodels and single-domain active learning strategies might be a preliminary\nsolution for MDAL problem. To find out the potential of this preliminary\nsolution, a comparative study over 5 models and 4 selection strategies is made\nin this paper. To the best of our knowledge, this is the first work provides\nthe formal definition of MDAL. Besides, this is the first comparative work for\nMDAL problem. From the results, the Multinomial Adversarial Networks (MAN)\nmodel with a simple best vs second best (BvSB) uncertainty strategy shows its\nsuperiority in most cases. We take this combination as our off-the-shelf\nrecommendation for the MDAL problem.",
          "link": "http://arxiv.org/abs/2106.13516",
          "publishedOn": "2021-06-28T01:57:57.760Z",
          "wordCount": 657,
          "title": "Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13327",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Nandy_A/0/1/0/all/0/1\">Aditya Nandy</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Although the tailored metal active sites and porous architectures of MOFs\nhold great promise for engineering challenges ranging from gas separations to\ncatalysis, a lack of understanding of how to improve their stability limits\ntheir use in practice. To overcome this limitation, we extract thousands of\npublished reports of the key aspects of MOF stability necessary for their\npractical application: the ability to withstand high temperatures without\ndegrading and the capacity to be activated by removal of solvent molecules.\nFrom nearly 4,000 manuscripts, we use natural language processing and automated\nimage analysis to obtain over 2,000 solvent-removal stability measures and\n3,000 thermal degradation temperatures. We analyze the relationships between\nstability properties and the chemical and geometric structures in this set to\nidentify limits of prior heuristics derived from smaller sets of MOFs. By\ntraining predictive machine learning (ML, i.e., Gaussian process and artificial\nneural network) models to encode the structure-property relationships with\ngraph- and pore-structure-based representations, we are able to make\npredictions of stability orders of magnitude faster than conventional\nphysics-based modeling or experiment. Interpretation of important features in\nML models provides insights that we use to identify strategies to engineer\nincreased stability into typically unstable 3d-containing MOFs that are\nfrequently targeted for catalytic applications. We expect our approach to\naccelerate the time to discovery of stable, practical MOF materials for a wide\nrange of applications.",
          "link": "http://arxiv.org/abs/2106.13327",
          "publishedOn": "2021-06-28T01:57:57.470Z",
          "wordCount": 681,
          "title": "Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks. (arXiv:2106.13327v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zexuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1\">Paolo Barucca</a>",
          "description": "Time series forecasting based on deep architectures has been gaining\npopularity in recent years due to their ability to model complex non-linear\ntemporal dynamics. The recurrent neural network is one such model capable of\nhandling variable-length input and output. In this paper, we leverage recent\nadvances in deep generative models and the concept of state space models to\npropose a stochastic adaptation of the recurrent neural network for\nmultistep-ahead time series forecasting, which is trained with stochastic\ngradient variational Bayes. In our model design, the transition function of the\nrecurrent neural network, which determines the evolution of the hidden states,\nis stochastic rather than deterministic as in a regular recurrent neural\nnetwork; this is achieved by incorporating a latent random variable into the\ntransition process which captures the stochasticity of the temporal dynamics.\nOur model preserves the architectural workings of a recurrent neural network\nfor which all relevant information is encapsulated in its hidden states, and\nthis flexibility allows our model to be easily integrated into any deep\narchitecture for sequential modelling. We test our model on a wide range of\ndatasets from finance to healthcare; results show that the stochastic recurrent\nneural network consistently outperforms its deterministic counterpart.",
          "link": "http://arxiv.org/abs/2104.12311",
          "publishedOn": "2021-06-28T01:57:57.449Z",
          "wordCount": 665,
          "title": "Stochastic Recurrent Neural Network for Multistep Time Series Forecasting. (arXiv:2104.12311v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutt_F/0/1/0/all/0/1\">Florina Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Subhajit Das</a>",
          "description": "Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.",
          "link": "http://arxiv.org/abs/2106.13411",
          "publishedOn": "2021-06-28T01:57:57.431Z",
          "wordCount": 646,
          "title": "Fine-grained Geolocation Prediction of Tweets with Human Machine Collaboration. (arXiv:2106.13411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Young D. Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jagmohan Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Various incremental learning (IL) approaches have been proposed to help deep\nlearning models learn new tasks/classes continuously without forgetting what\nwas learned previously (i.e., avoid catastrophic forgetting). With the growing\nnumber of deployed audio sensing applications that need to dynamically\nincorporate new tasks and changing input distribution from users, the ability\nof IL on-device becomes essential for both efficiency and user privacy.\n\nHowever, prior works suffer from high computational costs and storage demands\nwhich hinders the deployment of IL on-device. In this work, to overcome these\nlimitations, we develop an end-to-end and on-device IL framework, FastICARL,\nthat incorporates an exemplar-based IL and quantization in the context of\naudio-based applications. We first employ k-nearest-neighbor to reduce the\nlatency of IL. Then, we jointly utilize a quantization technique to decrease\nthe storage requirements of IL. We implement FastICARL on two types of mobile\ndevices and demonstrate that FastICARL remarkably decreases the IL time up to\n78-92% and the storage requirements by 2-4 times without sacrificing its\nperformance. FastICARL enables complete on-device IL, ensuring user privacy as\nthe user data does not need to leave the device.",
          "link": "http://arxiv.org/abs/2106.07268",
          "publishedOn": "2021-06-28T01:57:57.407Z",
          "wordCount": 661,
          "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications. (arXiv:2106.07268v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaglione_J/0/1/0/all/0/1\">Jean-Rapha&#xeb;l Gaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rajarshi Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>",
          "description": "We address the problem of inferring descriptions of system behavior using\nLinear Temporal Logic (LTL) from a finite set of positive and negative\nexamples. Most of the existing approaches for solving such a task rely on\npredefined templates for guiding the structure of the inferred formula. The\napproaches that can infer arbitrary LTL formulas, on the other hand, are not\nrobust to noise in the data. To alleviate such limitations, we devise two\nalgorithms for inferring concise LTL formulas even in the presence of noise.\nOur first algorithm infers minimal LTL formulas by reducing the inference\nproblem to a problem in maximum satisfiability and then using off-the-shelf\nMaxSAT solvers to find a solution. To the best of our knowledge, we are the\nfirst to incorporate the usage of MaxSAT solvers for inferring formulas in LTL.\nOur second learning algorithm relies on the first algorithm to derive a\ndecision tree over LTL formulas based on a decision tree learning algorithm. We\nhave implemented both our algorithms and verified that our algorithms are\nefficient in extracting concise LTL descriptions even in the presence of noise.",
          "link": "http://arxiv.org/abs/2104.15083",
          "publishedOn": "2021-06-28T01:57:57.400Z",
          "wordCount": 660,
          "title": "Learning Linear Temporal Properties from Noisy Data: A MaxSAT Approach. (arXiv:2104.15083v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zniyed_Y/0/1/0/all/0/1\">Yassine Zniyed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usevich_K/0/1/0/all/0/1\">Konstantin Usevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_S/0/1/0/all/0/1\">Sebastian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brie_D/0/1/0/all/0/1\">David Brie</a>",
          "description": "Activation functions (AFs) are an important part of the design of neural\nnetworks (NNs), and their choice plays a predominant role in the performance of\na NN. In this work, we are particularly interested in the estimation of\nflexible activation functions using tensor-based solutions, where the AFs are\nexpressed as a weighted sum of predefined basis functions. To do so, we propose\na new learning algorithm which solves a constrained coupled matrix-tensor\nfactorization (CMTF) problem. This technique fuses the first and zeroth order\ninformation of the NN, where the first-order information is contained in a\nJacobian tensor, following a constrained canonical polyadic decomposition\n(CPD). The proposed algorithm can handle different decomposition bases. The\ngoal of this method is to compress large pretrained NN models, by replacing\nsubnetworks, {\\em i.e.,} one or multiple layers of the original network, by a\nnew flexible layer. The approach is applied to a pretrained convolutional\nneural network (CNN) used for character classification.",
          "link": "http://arxiv.org/abs/2106.13542",
          "publishedOn": "2021-06-28T01:57:57.385Z",
          "wordCount": 601,
          "title": "Tensor-based framework for training flexible neural networks. (arXiv:2106.13542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01839",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatalic_A/0/1/0/all/0/1\">Antoine Chatalic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schellekens_V/0/1/0/all/0/1\">Vincent Schellekens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacques_L/0/1/0/all/0/1\">Laurent Jacques</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1\">Philip Schniter</a>",
          "description": "This article considers \"compressive learning,\" an approach to large-scale\nmachine learning where datasets are massively compressed before learning (e.g.,\nclustering, classification, or regression) is performed. In particular, a\n\"sketch\" is first constructed by computing carefully chosen nonlinear random\nfeatures (e.g., random Fourier features) and averaging them over the whole\ndataset. Parameters are then learned from the sketch, without access to the\noriginal dataset. This article surveys the current state-of-the-art in\ncompressive learning, including the main concepts and algorithms, their\nconnections with established signal-processing methods, existing theoretical\nguarantees -- on both information preservation and privacy preservation, and\nimportant open problems.",
          "link": "http://arxiv.org/abs/2008.01839",
          "publishedOn": "2021-06-28T01:57:57.360Z",
          "wordCount": 567,
          "title": "Sketching Datasets for Large-Scale Learning (long version). (arXiv:2008.01839v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyejin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seiyun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1\">Kwang-Sung Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1\">Jungseul Ok</a>",
          "description": "Structured stochastic multi-armed bandits provide accelerated regret rates\nover the standard unstructured bandit problems. Most structured bandits,\nhowever, assume the knowledge of the structural parameter such as Lipschitz\ncontinuity, which is often not available. To cope with the latent structural\nparameter, we consider a transfer learning setting in which an agent must learn\nto transfer the structural information from the prior tasks to the next task,\nwhich is inspired by practical problems such as rate adaptation in wireless\nlink. We propose a novel framework to provably and accurately estimate the\nLipschitz constant based on previous tasks and fully exploit it for the new\ntask at hand. We analyze the efficiency of the proposed framework in two folds:\n(i) the sample complexity of our estimator matches with the\ninformation-theoretic fundamental limit; and (ii) our regret bound on the new\ntask is close to that of the oracle algorithm with the full knowledge of the\nLipschitz constant under mild assumptions. Our analysis reveals a set of useful\ninsights on transfer learning for latent Lipschitzconstants such as the\nfundamental challenge a learner faces. Our numerical evaluations confirm our\ntheoretical findings and show the superiority of the proposed framework\ncompared to baselines.",
          "link": "http://arxiv.org/abs/2102.02472",
          "publishedOn": "2021-06-28T01:57:57.354Z",
          "wordCount": 662,
          "title": "Transfer Learning in Bandits with Latent Continuity. (arXiv:2102.02472v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abels_A/0/1/0/all/0/1\">Axel Abels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenaerts_T/0/1/0/all/0/1\">Tom Lenaerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trianni_V/0/1/0/all/0/1\">Vito Trianni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1\">Ann Now&#xe9;</a>",
          "description": "Quite some real-world problems can be formulated as decision-making problems\nwherein one must repeatedly make an appropriate choice from a set of\nalternatives. Expert judgements, whether human or artificial, can help in\ntaking correct decisions, especially when exploration of alternative solutions\nis costly. As expert opinions might deviate, the problem of finding the right\nalternative can be approached as a collective decision making problem (CDM).\nCurrent state-of-the-art approaches to solve CDM are limited by the quality of\nthe best expert in the group, and perform poorly if experts are not qualified\nor if they are overly biased, thus potentially derailing the decision-making\nprocess. In this paper, we propose a new algorithmic approach based on\ncontextual multi-armed bandit problems (CMAB) to identify and counteract such\nbiased expertises. We explore homogeneous, heterogeneous and polarised expert\ngroups and show that this approach is able to effectively exploit the\ncollective expertise, irrespective of whether the provided advice is directly\nconducive to good performance, outperforming state-of-the-art methods,\nespecially when the quality of the provided expertise degrades. Our novel\nCMAB-inspired approach achieves a higher final performance and does so while\nconverging more rapidly than previous adaptive algorithms, especially when\nheterogeneous expertise is readily available.",
          "link": "http://arxiv.org/abs/2106.13539",
          "publishedOn": "2021-06-28T01:57:57.348Z",
          "wordCount": 630,
          "title": "Dealing with Expert Bias in Collective Decision-Making. (arXiv:2106.13539v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07428",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1\">Rodrigo dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>",
          "description": "Audio Event Detection (AED) Systems capture audio from the environment and\nemploy some deep learning algorithms for detecting the presence of a specific\nsound of interest. In this paper, we evaluate deep learning-based AED systems\nagainst evasion attacks through adversarial examples. We run multiple security\ncritical AED tasks, implemented as CNNs classifiers, and then generate audio\nadversarial examples using two different types of noise, namely background and\nwhite noise, that can be used by the adversary to evade detection. We also\nexamine the robustness of existing third-party AED capable devices, such as\nNest devices manufactured by Google, which run their own black-box deep\nlearning models.\n\nWe show that an adversary can focus on audio adversarial inputs to cause AED\nsystems to misclassify, similarly to what has been previously done by works\nfocusing on adversarial examples from the image domain. We then, seek to\nimprove classifiers' robustness through countermeasures to the attacks. We\nemploy adversarial training and a custom denoising technique. We show that\nthese countermeasures, when applied to audio input, can be successful, either\nin isolation or in combination, generating relevant increases of nearly fifty\npercent in the performance of the classifiers when these are under attack.",
          "link": "http://arxiv.org/abs/2106.07428",
          "publishedOn": "2021-06-28T01:57:57.340Z",
          "wordCount": 656,
          "title": "Audio Attacks and Defenses against AED Systems -- A Practical Study. (arXiv:2106.07428v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1\">Tianle Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zongliang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkhodary_K/0/1/0/all/0/1\">Khalil I. Elkhodary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xu Guo</a>",
          "description": "In this paper, a mechanistic data-driven approach is proposed to accelerate\nstructural topology optimization, employing an in-house developed finite\nelement convolutional neural network (FE-CNN). Our approach can be divided into\ntwo stages: offline training, and online optimization. During offline training,\na mapping function is built between high and low resolution representations of\na given design domain. The mapping is expressed by a FE-CNN, which targets a\ncommon objective function value (e.g., structural compliance) across design\ndomains of differing resolutions. During online optimization, an arbitrary\ndesign domain of high resolution is reduced to low resolution through the\ntrained mapping function. The original high-resolution domain is thus designed\nby computations performed on only the low-resolution version, followed by an\ninverse mapping back to the high-resolution domain. Numerical examples\ndemonstrate that this approach can accelerate optimization by up to an order of\nmagnitude in computational time. Our proposed approach therefore shows great\npotential to overcome the curse-of-dimensionality incurred by density-based\nstructural topology optimization. The limitation of our present approach is\nalso discussed.",
          "link": "http://arxiv.org/abs/2106.13652",
          "publishedOn": "2021-06-28T01:57:57.334Z",
          "wordCount": 622,
          "title": "A mechanistic-based data-driven approach to accelerate structural topology optimization through finite element convolutional neural network (FE-CNN). (arXiv:2106.13652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13683",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1\">Peipei Tang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1\">Chengjing Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>",
          "description": "In this paper, we introduce a proximal-proximal majorization-minimization\n(PPMM) algorithm for nonconvex tuning-free robust regression problems. The\nbasic idea is to apply the proximal majorization-minimization algorithm to\nsolve the nonconvex problem with the inner subproblems solved by a sparse\nsemismooth Newton (SSN) method based proximal point algorithm (PPA). We must\nemphasize that the main difficulty in the design of the algorithm lies in how\nto overcome the singular difficulty of the inner subproblem. Furthermore, we\nalso prove that the PPMM algorithm converges to a d-stationary point. Due to\nthe Kurdyka-Lojasiewicz (KL) property of the problem, we present the\nconvergence rate of the PPMM algorithm. Numerical experiments demonstrate that\nour proposed algorithm outperforms the existing state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2106.13683",
          "publishedOn": "2021-06-28T01:57:57.317Z",
          "wordCount": 572,
          "title": "A proximal-proximal majorization-minimization algorithm for nonconvex tuning-free robust regression problems. (arXiv:2106.13683v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Susheel Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Cong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>",
          "description": "Self-supervised learning of graph neural networks (GNN) is in great need\nbecause of the widespread label scarcity issue in real-world graph/network\ndata. Graph contrastive learning (GCL), by training GNNs to maximize the\ncorrespondence between the representations of the same graph in its different\naugmented forms, may yield robust and transferable GNNs even without using\nlabels. However, GNNs trained by traditional GCL often risk capturing redundant\ngraph features and thus may be brittle and provide sub-par performance in\ndownstream tasks. Here, we propose a novel principle, termed adversarial-GCL\n(AD-GCL), which enables GNNs to avoid capturing redundant information during\nthe training by optimizing adversarial graph augmentation strategies used in\nGCL. We pair AD-GCL with theoretical explanations and design a practical\ninstantiation based on trainable edge-dropping graph augmentation. We\nexperimentally validate AD-GCL by comparing with the state-of-the-art GCL\nmethods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in\ntransfer, and $3\\%$ in semi-supervised learning settings overall with 18\ndifferent benchmark datasets for the tasks of molecule property regression and\nclassification, and social network classification.",
          "link": "http://arxiv.org/abs/2106.05819",
          "publishedOn": "2021-06-28T01:57:57.310Z",
          "wordCount": 641,
          "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning. (arXiv:2106.05819v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13790",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dhulipala_S/0/1/0/all/0/1\">S. L. N. Dhulipala</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shields_M/0/1/0/all/0/1\">M. D. Shields</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spencer_B/0/1/0/all/0/1\">B. W. Spencer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bolisetti_C/0/1/0/all/0/1\">C. Bolisetti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Slaughter_A/0/1/0/all/0/1\">A. E. Slaughter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laboure_V/0/1/0/all/0/1\">V. M. Laboure</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chakroborty_P/0/1/0/all/0/1\">P. Chakroborty</a>",
          "description": "While multifidelity modeling provides a cost-effective way to conduct\nuncertainty quantification with computationally expensive models, much greater\nefficiency can be achieved by adaptively deciding the number of required\nhigh-fidelity (HF) simulations, depending on the type and complexity of the\nproblem and the desired accuracy in the results. We propose a framework for\nactive learning with multifidelity modeling emphasizing the efficient\nestimation of rare events. Our framework works by fusing a low-fidelity (LF)\nprediction with an HF-inferred correction, filtering the corrected LF\nprediction to decide whether to call the high-fidelity model, and for enhanced\nsubsequent accuracy, adapting the correction for the LF prediction after every\nHF model call. The framework does not make any assumptions as to the LF model\ntype or its correlations with the HF model. In addition, for improved\nrobustness when estimating smaller failure probabilities, we propose using\ndynamic active learning functions that decide when to call the HF model. We\ndemonstrate our framework using several academic case studies and two finite\nelement (FE) model case studies: estimating Navier-Stokes velocities using the\nStokes approximation and estimating stresses in a transversely isotropic model\nsubjected to displacements via a coarsely meshed isotropic model. Across these\ncase studies, not only did the proposed framework estimate the failure\nprobabilities accurately, but compared with either Monte Carlo or a standard\nvariance reduction method, it also required only a small fraction of the calls\nto the HF model.",
          "link": "http://arxiv.org/abs/2106.13790",
          "publishedOn": "2021-06-28T01:57:57.302Z",
          "wordCount": 686,
          "title": "Active Learning with Multifidelity Modeling for Efficient Rare Event Simulation. (arXiv:2106.13790v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13559",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esteve_A/0/1/0/all/0/1\">Anna Esteve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramos_D/0/1/0/all/0/1\">David Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Recently, bladder cancer has been significantly increased in terms of\nincidence and mortality. Currently, two subtypes are known based on tumour\ngrowth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).\nIn this work, we focus on the MIBC subtype because it is of the worst prognosis\nand can spread to adjacent organs. We present a self-learning framework to\ngrade bladder cancer from histological images stained via immunohistochemical\ntechniques. Specifically, we propose a novel Deep Convolutional Embedded\nAttention Clustering (DCEAC) which allows classifying histological patches into\ndifferent severity levels of the disease, according to the patterns established\nin the literature. The proposed DCEAC model follows a two-step fully\nunsupervised learning methodology to discern between non-tumour, mild and\ninfiltrative patterns from high-resolution samples of 512x512 pixels. Our\nsystem outperforms previous clustering-based methods by including a\nconvolutional attention module, which allows refining the features of the\nlatent space before the classification stage. The proposed network exceeds\nstate-of-the-art approaches by 2-3% across different metrics, achieving a final\naverage accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported\nclass activation maps evidence that our model is able to learn by itself the\nsame patterns that clinicians consider relevant, without incurring prior\nannotation steps. This fact supposes a breakthrough in muscle-invasive bladder\ncancer grading which bridges the gap with respect to train the model on\nlabelled data.",
          "link": "http://arxiv.org/abs/2106.13559",
          "publishedOn": "2021-06-28T01:57:57.294Z",
          "wordCount": 686,
          "title": "A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images. (arXiv:2106.13559v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12133",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1\">Le Thi Khanh Hien</a>, <a href=\"http://arxiv.org/find/math/1/au:+Phan_D/0/1/0/all/0/1\">Duy Nhat Phan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gillis_N/0/1/0/all/0/1\">Nicolas Gillis</a>",
          "description": "In this paper, we introduce TITAN, a novel inerTIal block majorizaTion\nminimizAtioN framework for non-smooth non-convex optimization problems. To the\nbest of our knowledge, TITAN is the first framework of block-coordinate update\nmethod that relies on the majorization-minimization framework while embedding\ninertial force to each step of the block updates. The inertial force is\nobtained via an extrapolation operator that subsumes heavy-ball and\nNesterov-type accelerations for block proximal gradient methods as special\ncases. By choosing various surrogate functions, such as proximal, Lipschitz\ngradient, Bregman, quadratic, and composite surrogate functions, and by varying\nthe extrapolation operator, TITAN produces a rich set of inertial\nblock-coordinate update methods. We study sub-sequential convergence as well as\nglobal convergence for the generated sequence of TITAN. We illustrate the\neffectiveness of TITAN on two important machine learning problems, namely\nsparse non-negative matrix factorization and matrix completion.",
          "link": "http://arxiv.org/abs/2010.12133",
          "publishedOn": "2021-06-28T01:57:57.286Z",
          "wordCount": 607,
          "title": "An Inertial Block Majorization Minimization Framework for Nonsmooth Nonconvex Optimization. (arXiv:2010.12133v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1\">Axel Parmentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1\">Thibaut Vidal</a>",
          "description": "Counterfactual explanations are usually generated through heuristics that are\nsensitive to the search's initial conditions. The absence of guarantees of\nperformance and robustness hinders trustworthiness. In this paper, we take a\ndisciplined approach towards counterfactual explanations for tree ensembles. We\nadvocate for a model-based search aiming at \"optimal\" explanations and propose\nefficient mixed-integer programming approaches. We show that isolation forests\ncan be modeled within our framework to focus the search on plausible\nexplanations with a low outlier score. We provide comprehensive coverage of\nadditional constraints that model important objectives, heterogeneous data\ntypes, structural constraints on the feature space, along with resource and\nactionability restrictions. Our experimental analyses demonstrate that the\nproposed search approach requires a computational effort that is orders of\nmagnitude smaller than previous mathematical programming algorithms. It scales\nup to large data sets and tree ensembles, where it provides, within seconds,\nsystematic explanations grounded on well-defined models solved to optimality.",
          "link": "http://arxiv.org/abs/2106.06631",
          "publishedOn": "2021-06-28T01:57:57.258Z",
          "wordCount": 630,
          "title": "Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rommel_C/0/1/0/all/0/1\">C&#xe9;dric Rommel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1\">Thomas Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>",
          "description": "Data augmentation is a key element of deep learning pipelines, as it informs\nthe network during training about transformations of the input data that keep\nthe label unchanged. Manually finding adequate augmentation methods and\nparameters for a given pipeline is however rapidly cumbersome. In particular,\nwhile intuition can guide this decision for images, the design and choice of\naugmentation policies remains unclear for more complex types of data, such as\nneuroscience signals. Moreover, label independent strategies might not be\nsuitable for such structured data and class-dependent augmentations might be\nnecessary. This idea has been surprisingly unexplored in the literature, while\nit is quite intuitive: changing the color of a car image does not change the\nobject class to be predicted, but doing the same to the picture of an orange\ndoes. This paper aims to increase the generalization power added through\nclass-wise data augmentation. Yet, as seeking transformations depending on the\nclass largely increases the complexity of the task, using gradient-free\noptimization techniques as done by most existing automatic approaches becomes\nintractable for real-world datasets. For this reason we propose to use\ndifferentiable data augmentation amenable to gradient-based learning. EEG\nsignals are a perfect example of data for which good augmentation policies are\nmostly unknown. In this work, we demonstrate the relevance of our approach on\nthe clinically relevant sleep staging classification task, for which we also\npropose differentiable transformations.",
          "link": "http://arxiv.org/abs/2106.13695",
          "publishedOn": "2021-06-28T01:57:57.252Z",
          "wordCount": 660,
          "title": "CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals. (arXiv:2106.13695v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13493",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kabeli_O/0/1/0/all/0/1\">Ori Kabeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Buye Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>",
          "description": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.",
          "link": "http://arxiv.org/abs/2106.13493",
          "publishedOn": "2021-06-28T01:57:57.243Z",
          "wordCount": 618,
          "title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation. (arXiv:2106.13493v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yao-Yuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1\">Cyrus Rashtchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "Adversarial robustness has emerged as a desirable property for neural\nnetworks. Prior work shows that robust networks perform well in some\nout-of-distribution generalization tasks, such as transfer learning and outlier\ndetection. We uncover a different kind of out-of-distribution generalization\nproperty of such networks, and find that they also do well in a task that we\ncall nearest category generalization (NCG) - given an out-of-distribution\ninput, they tend to predict the same label as that of the closest training\nexample. We empirically show that this happens even when the\nout-of-distribution inputs lie outside the robustness radius of the training\ndata, which suggests that these networks may generalize better along unseen\ndirections on the natural image manifold than arbitrary unseen directions. We\nexamine how performance changes when we change the robustness regions during\ntraining. We then design experiments to investigate the connection between\nout-of-distribution detection and nearest category generalization. Taken\ntogether, our work provides evidence that robust neural networks may resemble\nnearest neighbor classifiers in their behavior on out-of-distribution data. The\ncode is available at\nhttps://github.com/yangarbiter/nearest-category-generalization",
          "link": "http://arxiv.org/abs/2011.08485",
          "publishedOn": "2021-06-28T01:57:57.235Z",
          "wordCount": 645,
          "title": "Robustness and Generalization to Nearest Categories. (arXiv:2011.08485v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1\">Dakota Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jisung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kojaku_S/0/1/0/all/0/1\">Sadamori Kojaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costas_R/0/1/0/all/0/1\">Rodrigo Costas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woo-Sung Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milojevic_S/0/1/0/all/0/1\">Sta&#x161;a Milojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>",
          "description": "Human mobility drives major societal phenomena including epidemics,\neconomies, and innovation. Historically, mobility was constrained by geographic\ndistance, however, in the globalizing world, language, culture, and history are\nincreasingly important. We propose using the neural embedding model word2vec\nfor studying mobility and capturing its complexity. Word2ec is shown to be\nmathematically equivalent to the gravity model of mobility, and using three\nhuman trajectory datasets, we demonstrate that it encodes nuanced relationships\nbetween locations into a vector-space, providing a measure of effective\ndistance that outperforms baselines. Focusing on the case of scientific\nmobility, we show that embeddings uncover cultural, linguistic, and\nhierarchical relationships at multiple levels of granularity. Connecting neural\nembeddings to the gravity model opens up new avenues for the study of mobility.",
          "link": "http://arxiv.org/abs/2012.02785",
          "publishedOn": "2021-06-28T01:57:57.225Z",
          "wordCount": 616,
          "title": "Unsupervised embedding of trajectories captures the latent structure of mobility. (arXiv:2012.02785v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "We study the problem of Online Convex Optimization (OCO) with memory, which\nallows loss functions to depend on past decisions and thus captures temporal\neffects of learning problems. In this paper, we introduce dynamic policy regret\nas the performance measure to design algorithms robust to non-stationary\nenvironments, which competes algorithms' decisions with a sequence of changing\ncomparators. We propose a novel algorithm for OCO with memory that provably\nenjoys an optimal dynamic policy regret. The key technical challenge is how to\ncontrol the switching cost, the cumulative movements of player's decisions,\nwhich is neatly addressed by a novel decomposition of dynamic policy regret and\nan appropriate meta-expert structure. Furthermore, we apply the results to the\nproblem of online non-stochastic control, i.e., controlling a linear dynamical\nsystem with adversarial disturbance and convex loss functions. We derive a\nnovel gradient-based controller with dynamic policy regret guarantees, which is\nthe first controller competitive to a sequence of changing policies.",
          "link": "http://arxiv.org/abs/2102.03758",
          "publishedOn": "2021-06-28T01:57:57.207Z",
          "wordCount": 617,
          "title": "Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander G. Ororbia</a>",
          "description": "In this article, we propose a novel form of unsupervised learning, continual\ncompetitive memory (CCM), as well as a computational framework to unify related\nneural models that operate under the principles of competition. The resulting\nneural system is shown to offer an effective approach for combating\ncatastrophic forgetting in online continual classification problems. We\ndemonstrate that the proposed CCM system not only outperforms other competitive\nlearning neural models but also yields performance that is competitive with\nseveral modern, state-of-the-art lifelong learning approaches on benchmarks\nsuch as Split MNIST and Split NotMNIST. CCM yields a promising path forward for\nacquiring representations that are robust to interference from data streams,\nespecially when the task is unknown to the model and must be inferred without\nexternal guidance.",
          "link": "http://arxiv.org/abs/2106.13300",
          "publishedOn": "2021-06-28T01:57:57.199Z",
          "wordCount": 555,
          "title": "Continual Competitive Memory: A Neural System for Online Task-Free Lifelong Learning. (arXiv:2106.13300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.",
          "link": "http://arxiv.org/abs/2105.15082",
          "publishedOn": "2021-06-28T01:57:57.192Z",
          "wordCount": 689,
          "title": "Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trappolini_G/0/1/0/all/0/1\">Giovanni Trappolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1\">Luca Moschella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>",
          "description": "In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13679",
          "publishedOn": "2021-06-28T01:57:57.183Z",
          "wordCount": 657,
          "title": "Shape registration in the time of transformers. (arXiv:2106.13679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_j/0/1/0/all/0/1\">jinhong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1\">Hyunjung Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongwuk Lee</a>",
          "description": "Session-based recommendation aims at predicting the next item given a\nsequence of previous items consumed in the session, e.g., on e-commerce or\nmultimedia streaming services. Specifically, session data exhibits some unique\ncharacteristics, i.e., session consistency and sequential dependency over items\nwithin the session, repeated item consumption, and session timeliness. In this\npaper, we propose simple-yet-effective linear models for considering the\nholistic aspects of the sessions. The comprehensive nature of our models helps\nimprove the quality of session-based recommendation. More importantly, it\nprovides a generalized framework for reflecting different perspectives of\nsession data. Furthermore, since our models can be solved by closed-form\nsolutions, they are highly scalable. Experimental results demonstrate that the\nproposed linear models show competitive or state-of-the-art performance in\nvarious metrics on several real-world datasets.",
          "link": "http://arxiv.org/abs/2103.16104",
          "publishedOn": "2021-06-28T01:57:57.173Z",
          "wordCount": 599,
          "title": "Session-aware Linear Item-Item Models for Session-based Recommendation. (arXiv:2103.16104v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12525",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "Obtaining a useful estimate of an object from highly incomplete imaging\nmeasurements remains a holy grail of imaging science. Deep learning methods\nhave shown promise in learning object priors or constraints to improve the\nconditioning of an ill-posed imaging inverse problem. In this study, a\nframework for estimating an object of interest that is semantically related to\na known prior image, is proposed. An optimization problem is formulated in the\ndisentangled latent space of a style-based generative model, and semantically\nmeaningful constraints are imposed using the disentangled latent representation\nof the prior image. Stable recovery from incomplete measurements with the help\nof a prior image is theoretically analyzed. Numerical experiments demonstrating\nthe superior performance of our approach as compared to related methods are\npresented.",
          "link": "http://arxiv.org/abs/2102.12525",
          "publishedOn": "2021-06-28T01:57:57.154Z",
          "wordCount": 603,
          "title": "Prior Image-Constrained Reconstruction using Style-Based Generative Models. (arXiv:2102.12525v2 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13092",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Caragea_A/0/1/0/all/0/1\">A. Caragea</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1\">D.G. Lee</a>, <a href=\"http://arxiv.org/find/math/1/au:+Maly_J/0/1/0/all/0/1\">J. Maly</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pfander_G/0/1/0/all/0/1\">G. Pfander</a>, <a href=\"http://arxiv.org/find/math/1/au:+Voigtlaender_F/0/1/0/all/0/1\">F. Voigtlaender</a>",
          "description": "Until recently, applications of neural networks in machine learning have\nalmost exclusively relied on real-valued networks. It was recently observed,\nhowever, that complex-valued neural networks (CVNNs) exhibit superior\nperformance in applications in which the input is naturally complex-valued,\nsuch as MRI fingerprinting. While the mathematical theory of real-valued\nnetworks has, by now, reached some level of maturity, this is far from true for\ncomplex-valued networks. In this paper, we analyze the expressivity of\ncomplex-valued networks by providing explicit quantitative error bounds for\napproximating $C^n$ functions on compact subsets of $\\mathbb{C}^d$ by\ncomplex-valued neural networks that employ the modReLU activation function,\ngiven by $\\sigma(z) = \\mathrm{ReLU}(|z| - 1) \\, \\mathrm{sgn} (z)$, which is one\nof the most popular complex activation functions used in practice. We show that\nthe derived approximation rates are optimal (up to log factors) in the class of\nmodReLU networks with weights of moderate growth.",
          "link": "http://arxiv.org/abs/2102.13092",
          "publishedOn": "2021-06-28T01:57:57.147Z",
          "wordCount": 603,
          "title": "Quantitative approximation results for complex-valued neural networks. (arXiv:2102.13092v2 [math.FA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07006",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kollias_G/0/1/0/all/0/1\">George Kollias</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalev_A/0/1/0/all/0/1\">Amir Kalev</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wei_K/0/1/0/all/0/1\">Ken X. Wei</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose a new quantum state reconstruction method that combines ideas from\ncompressed sensing, non-convex optimization, and acceleration methods. The\nalgorithm, called Momentum-Inspired Factored Gradient Descent (\\texttt{MiFGD}),\nextends the applicability of quantum tomography for larger systems. Despite\nbeing a non-convex method, \\texttt{MiFGD} converges \\emph{provably} to the true\ndensity matrix at a linear rate, in the absence of experimental and statistical\nnoise, and under common assumptions. With this manuscript, we present the\nmethod, prove its convergence property and provide Frobenius norm bound\nguarantees with respect to the true density matrix. From a practical point of\nview, we benchmark the algorithm performance with respect to other existing\nmethods, in both synthetic and real experiments performed on an IBM's quantum\nprocessing unit. We find that the proposed algorithm performs orders of\nmagnitude faster than state of the art approaches, with the same or better\naccuracy. In both synthetic and real experiments, we observed accurate and\nrobust reconstruction, despite experimental and statistical noise in the\ntomographic data. Finally, we provide a ready-to-use code for state tomography\nof multi-qubit systems.",
          "link": "http://arxiv.org/abs/2104.07006",
          "publishedOn": "2021-06-28T01:57:57.141Z",
          "wordCount": 656,
          "title": "Fast quantum state reconstruction via accelerated non-convex programming. (arXiv:2104.07006v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durasov_N/0/1/0/all/0/1\">Nikita Durasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>",
          "description": "Deep neural networks have amply demonstrated their prowess but estimating the\nreliability of their predictions remains challenging. Deep Ensembles are widely\nconsidered as being one of the best methods for generating uncertainty\nestimates but are very expensive to train and evaluate. MC-Dropout is another\npopular alternative, which is less expensive, but also less reliable. Our\ncentral intuition is that there is a continuous spectrum of ensemble-like\nmodels of which MC-Dropout and Deep Ensembles are extreme examples. The first\nuses an effectively infinite number of highly correlated models while the\nsecond relies on a finite number of independent models.\n\nTo combine the benefits of both, we introduce Masksembles. Instead of\nrandomly dropping parts of the network as in MC-dropout, Masksemble relies on a\nfixed number of binary masks, which are parameterized in a way that allows to\nchange correlations between individual models. Namely, by controlling the\noverlap between the masks and their density one can choose the optimal\nconfiguration for the task at hand. This leads to a simple and easy to\nimplement method with performance on par with Ensembles at a fraction of the\ncost. We experimentally validate Masksembles on two widely used datasets,\nCIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.08334",
          "publishedOn": "2021-06-28T01:57:57.134Z",
          "wordCount": 672,
          "title": "Masksembles for Uncertainty Estimation. (arXiv:2012.08334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Caomingzhe Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fushuan Wen</a>",
          "description": "Non-intrusive load monitoring (NILM) is essential for understanding\ncustomer's power consumption patterns and may find wide applications like\ncarbon emission reduction and energy conservation. The training of NILM models\nrequires massive load data containing different types of appliances. However,\ninadequate load data and the risk of power consumer privacy breaches may be\nencountered by local data owners during the NILM model training. To prevent\nsuch potential risks, a novel NILM method named Fed-NILM which is based on\nFederated Learning (FL) is proposed in this paper. In Fed-NILM, local model\nparameters instead of local load data are shared among multiple data owners.\nThe global model is obtained by weighted averaging the parameters. Experiments\nbased on two measured load datasets are conducted to explore the generalization\nability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained\nNILMs and the centrally-trained NILM is conducted. The experimental results\nshow that Fed-NILM has superior performance in scalability and convergence.\nFed-NILM outperforms locally-trained NILMs operated by local data owners and\napproximates the centrally-trained NILM which is trained on the entire load\ndataset without privacy protection. The proposed Fed-NILM significantly\nimproves the co-modeling capabilities of local data owners while protecting\npower consumers' privacy.",
          "link": "http://arxiv.org/abs/2105.11085",
          "publishedOn": "2021-06-28T01:57:57.126Z",
          "wordCount": 664,
          "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection. (arXiv:2105.11085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12756",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Beck_E/0/1/0/all/0/1\">Edgar Beck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockelmann_C/0/1/0/all/0/1\">Carsten Bockelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekorsy_A/0/1/0/all/0/1\">Armin Dekorsy</a>",
          "description": "Following the great success of Machine Learning (ML), especially Deep Neural\nNetworks (DNNs), in many research domains in 2010s, several ML-based approaches\nwere proposed for detection in large inverse linear problems, e.g., massive\nMIMO systems. The main motivation behind is that the complexity of Maximum\nA-Posteriori (MAP) detection grows exponentially with system dimensions.\nInstead of using DNNs, essentially being a black-box, we take a slightly\ndifferent approach and introduce a probabilistic Continuous relaxation of\ndisCrete variables to MAP detection. Enabling close approximation and\ncontinuous optimization, we derive an iterative detection algorithm: Concrete\nMAP Detection (CMD). Furthermore, extending CMD by the idea of deep unfolding\ninto CMDNet, we allow for (online) optimization of a small number of parameters\nto different working points while limiting complexity. In contrast to recent\nDNN-based approaches, we select the optimization criterion and output of CMDNet\nbased on information theory and are thus able to learn approximate\nprobabilities of the individual optimal detector. This is crucial for soft\ndecoding in today's communication systems. Numerical simulation results in MIMO\nsystems reveal CMDNet to feature a promising accuracy complexity trade-off\ncompared to State of the Art. Notably, we demonstrate CMDNet's soft outputs to\nbe reliable for decoders.",
          "link": "http://arxiv.org/abs/2102.12756",
          "publishedOn": "2021-06-28T01:57:57.120Z",
          "wordCount": 683,
          "title": "Learning a Probabilistic Relaxation of Discrete Variables for Soft Detection with Low Complexity: CMDNet. (arXiv:2102.12756v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05397",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stankewitz_B/0/1/0/all/0/1\">Bernhard Stankewitz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1\">Nicole M&#xfc;cke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>",
          "description": "Optimization was recently shown to control the inductive bias in a learning\nprocess, a property referred to as implicit, or iterative regularization. The\nestimator obtained iteratively minimizing the training error can generalise\nwell with no need of further penalties or constraints. In this paper, we\ninvestigate this phenomenon in the context of linear models with smooth loss\nfunctions. In particular, we investigate and propose a proof technique\ncombining ideas from inexact optimization and probability theory, specifically\ngradient concentration. The proof is easy to follow and allows to obtain sharp\nlearning bounds. More generally, it highlights a way to develop optimization\nresults into learning guarantees.",
          "link": "http://arxiv.org/abs/2106.05397",
          "publishedOn": "2021-06-28T01:57:57.100Z",
          "wordCount": 559,
          "title": "From inexact optimization to learning via gradient concentration. (arXiv:2106.05397v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1\">Shahar Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinkas_B/0/1/0/all/0/1\">Benny Pinkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_C/0/1/0/all/0/1\">Carsten Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_C/0/1/0/all/0/1\">Chaya Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshet_J/0/1/0/all/0/1\">Joseph Keshet</a>",
          "description": "We present a framework that allows to certify the fairness degree of a model\nbased on an interactive and privacy-preserving test. The framework verifies any\ntrained model, regardless of its training process and architecture. Thus, it\nallows us to evaluate any deep learning model on multiple fairness definitions\nempirically. We tackle two scenarios, where either the test data is privately\navailable only to the tester or is publicly known in advance, even to the model\ncreator. We investigate the soundness of the proposed approach using\ntheoretical analysis and present statistical guarantees for the interactive\ntest. Finally, we provide a cryptographic technique to automate fairness\ntesting and certified inference with only black-box access to the model at hand\nwhile hiding the participants' sensitive data.",
          "link": "http://arxiv.org/abs/2009.01534",
          "publishedOn": "2021-06-28T01:57:57.083Z",
          "wordCount": 617,
          "title": "Fairness in the Eyes of the Data: Certifying Machine-Learning Models. (arXiv:2009.01534v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Appleby_G/0/1/0/all/0/1\">Gabriel Appleby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espadoto_M/0/1/0/all/0/1\">Mateus Espadoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goree_S/0/1/0/all/0/1\">Samuel Goree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1\">Alexandru Telea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_E/0/1/0/all/0/1\">Erik W Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Remco Chang</a>",
          "description": "Projection algorithms such as t-SNE or UMAP are useful for the visualization\nof high dimensional data, but depend on hyperparameters which must be tuned\ncarefully. Unfortunately, iteratively recomputing projections to find the\noptimal hyperparameter value is computationally intensive and unintuitive due\nto the stochastic nature of these methods. In this paper we propose HyperNP, a\nscalable method that allows for real-time interactive hyperparameter\nexploration of projection methods by training neural network approximations.\nHyperNP can be trained on a fraction of the total data instances and\nhyperparameter configurations and can compute projections for new data and\nhyperparameters at interactive speeds. HyperNP is compact in size and fast to\ncompute, thus allowing it to be embedded in lightweight visualization systems\nsuch as web browsers. We evaluate the performance of the HyperNP across three\ndatasets in terms of performance and speed. The results suggest that HyperNP is\naccurate, scalable, interactive, and appropriate for use in real-world\nsettings.",
          "link": "http://arxiv.org/abs/2106.13777",
          "publishedOn": "2021-06-28T01:57:57.070Z",
          "wordCount": 595,
          "title": "HyperNP: Interactive Visual Exploration of Multidimensional Projection Hyperparameters. (arXiv:2106.13777v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.02470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sutter_T/0/1/0/all/0/1\">Thomas M. Sutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1\">Imant Daunhawer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1\">Julia E. Vogt</a>",
          "description": "Multiple data types naturally co-occur when describing real-world phenomena\nand learning from them is a long-standing goal in machine learning research.\nHowever, existing self-supervised generative models approximating an ELBO are\nnot able to fulfill all desired requirements of multimodal models: their\nposterior approximation functions lead to a trade-off between the semantic\ncoherence and the ability to learn the joint data distribution. We propose a\nnew, generalized ELBO formulation for multimodal data that overcomes these\nlimitations. The new objective encompasses two previous methods as special\ncases and combines their benefits without compromises. In extensive\nexperiments, we demonstrate the advantage of the proposed method compared to\nstate-of-the-art models in self-supervised, generative learning tasks.",
          "link": "http://arxiv.org/abs/2105.02470",
          "publishedOn": "2021-06-28T01:57:57.063Z",
          "wordCount": 566,
          "title": "Generalized Multimodal ELBO. (arXiv:2105.02470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_L/0/1/0/all/0/1\">Lance Kaplan</a>",
          "description": "Inductive Logic Programming (ILP) systems learn generalised, interpretable\nrules in a data-efficient manner utilising existing background knowledge.\nHowever, current ILP systems require training examples to be specified in a\nstructured logical format. Neural networks learn from unstructured data,\nalthough their learned models may be difficult to interpret and are vulnerable\nto data perturbations at run-time. This paper introduces a hybrid\nneural-symbolic learning framework, called NSL, that learns interpretable rules\nfrom labelled unstructured data. NSL combines pre-trained neural networks for\nfeature extraction with FastLAS, a state-of-the-art ILP system for rule\nlearning under the answer set semantics. Features extracted by the neural\ncomponents define the structured context of labelled examples and the\nconfidence of the neural predictions determines the level of noise of the\nexamples. Using the scoring function of FastLAS, NSL searches for short,\ninterpretable rules that generalise over such noisy examples. We evaluate our\nframework on propositional and first-order classification tasks using the MNIST\ndataset as raw data. Specifically, we demonstrate that NSL is able to learn\nrobust rules from perturbed MNIST data and achieve comparable or superior\naccuracy when compared to neural network and random forest baselines whilst\nbeing more general and interpretable.",
          "link": "http://arxiv.org/abs/2012.05023",
          "publishedOn": "2021-06-28T01:57:57.025Z",
          "wordCount": 667,
          "title": "NSL: Hybrid Interpretable Learning From Noisy Raw Data. (arXiv:2012.05023v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1\">Beren Millidge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1\">Alexander Tschantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Anil Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1\">Christopher Buckley</a>",
          "description": "The exploration-exploitation trade-off is central to the description of\nadaptive behaviour in fields ranging from machine learning, to biology, to\neconomics. While many approaches have been taken, one approach to solving this\ntrade-off has been to equip or propose that agents possess an intrinsic\n'exploratory drive' which is often implemented in terms of maximizing the\nagents information gain about the world -- an approach which has been widely\nstudied in machine learning and cognitive science. In this paper we\nmathematically investigate the nature and meaning of such approaches and\ndemonstrate that this combination of utility maximizing and information-seeking\nbehaviour arises from the minimization of an entirely difference class of\nobjectives we call divergence objectives. We propose a dichotomy in the\nobjective functions underlying adaptive behaviour between \\emph{evidence}\nobjectives, which correspond to well-known reward or utility maximizing\nobjectives in the literature, and \\emph{divergence} objectives which instead\nseek to minimize the divergence between the agent's expected and desired\nfutures, and argue that this new class of divergence objectives could form the\nmathematical foundation for a much richer understanding of the exploratory\ncomponents of adaptive and intelligent action, beyond simply greedy utility\nmaximization.",
          "link": "http://arxiv.org/abs/2103.06859",
          "publishedOn": "2021-06-28T01:57:57.018Z",
          "wordCount": 694,
          "title": "Understanding the Origin of Information-Seeking Exploration in Probabilistic Objectives for Control. (arXiv:2103.06859v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13682",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1\">Zoe Guan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1\">Giovanni Parmigiani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Braun_D/0/1/0/all/0/1\">Danielle Braun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trippa_L/0/1/0/all/0/1\">Lorenzo Trippa</a>",
          "description": "Family history is a major risk factor for many types of cancer. Mendelian\nrisk prediction models translate family histories into cancer risk predictions\nbased on knowledge of cancer susceptibility genes. These models are widely used\nin clinical practice to help identify high-risk individuals. Mendelian models\nleverage the entire family history, but they rely on many assumptions about\ncancer susceptibility genes that are either unrealistic or challenging to\nvalidate due to low mutation prevalence. Training more flexible models, such as\nneural networks, on large databases of pedigrees can potentially lead to\naccuracy gains. In this paper, we develop a framework to apply neural networks\nto family history data and investigate their ability to learn inherited\nsusceptibility to cancer. While there is an extensive literature on neural\nnetworks and their state-of-the-art performance in many tasks, there is little\nwork applying them to family history data. We propose adaptations of\nfully-connected neural networks and convolutional neural networks to pedigrees.\nIn data simulated under Mendelian inheritance, we demonstrate that our proposed\nneural network models are able to achieve nearly optimal prediction\nperformance. Moreover, when the observed family history includes misreported\ncancer diagnoses, neural networks are able to outperform the Mendelian BRCAPRO\nmodel embedding the correct inheritance laws. Using a large dataset of over\n200,000 family histories, the Risk Service cohort, we train prediction models\nfor future risk of breast cancer. We validate the models using data from the\nCancer Genetics Network.",
          "link": "http://arxiv.org/abs/2106.13682",
          "publishedOn": "2021-06-28T01:57:56.949Z",
          "wordCount": 669,
          "title": "Prediction of Hereditary Cancers Using Neural Networks. (arXiv:2106.13682v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2007.11752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Ting-Wu Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1\">Diana Marculescu</a>",
          "description": "Slimmable neural networks provide a flexible trade-off front between\nprediction error and computational requirement (such as the number of\nfloating-point operations or FLOPs) with the same storage requirement as a\nsingle model. They are useful for reducing maintenance overhead for deploying\nmodels to devices with different memory constraints and are useful for\noptimizing the efficiency of a system with many CNNs. However, existing\nslimmable network approaches either do not optimize layer-wise widths or\noptimize the shared-weights and layer-wise widths independently, thereby\nleaving significant room for improvement by joint width and weight\noptimization. In this work, we propose a general framework to enable joint\noptimization for both width configurations and weights of slimmable networks.\nOur framework subsumes conventional and NAS-based slimmable methods as special\ncases and provides flexibility to improve over existing methods. From a\npractical standpoint, we propose Joslim, an algorithm that jointly optimizes\nboth the widths and weights for slimmable nets, which outperforms existing\nmethods for optimizing slimmable networks across various networks, datasets,\nand objectives. Quantitatively, improvements up to 1.7% and 8% in top-1\naccuracy on the ImageNet dataset can be attained for MobileNetV2 considering\nFLOPs and memory footprint, respectively. Our results highlight the potential\nof optimizing the channel counts for different layers jointly with the weights\nfor slimmable networks. Code available at https://github.com/cmu-enyac/Joslim.",
          "link": "http://arxiv.org/abs/2007.11752",
          "publishedOn": "2021-06-28T01:57:56.934Z",
          "wordCount": 730,
          "title": "Joslim: Joint Widths and Weights Optimization for Slimmable Neural Networks. (arXiv:2007.11752v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loveland_D/0/1/0/all/0/1\">Donald Loveland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shusen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiszpanski_A/0/1/0/all/0/1\">Anna Hiszpanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yong Han</a>",
          "description": "Graph neural network (GNN) explanations have largely been facilitated through\npost-hoc introspection. While this has been deemed successful, many post-hoc\nexplanation methods have been shown to fail in capturing a model's learned\nrepresentation. Due to this problem, it is worthwhile to consider how one might\ntrain a model so that it is more amenable to post-hoc analysis. Given the\nsuccess of adversarial training in the computer vision domain to train models\nwith more reliable representations, we propose a similar training paradigm for\nGNNs and analyze the respective impact on a model's explanations. In instances\nwithout ground truth labels, we also determine how well an explanation method\nis utilizing a model's learned representation through a new metric and\ndemonstrate adversarial training can help better extract domain-relevant\ninsights in chemistry.",
          "link": "http://arxiv.org/abs/2106.13427",
          "publishedOn": "2021-06-28T01:57:56.911Z",
          "wordCount": 577,
          "title": "Reliable Graph Neural Network Explanations Through Adversarial Training. (arXiv:2106.13427v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>",
          "description": "Graphs have been widely used in data mining and machine learning due to their\nunique representation of real-world objects and their interactions. As graphs\nare getting bigger and bigger nowadays, it is common to see their subgraphs\nseparately collected and stored in multiple local systems. Therefore, it is\nnatural to consider the subgraph federated learning setting, where each local\nsystem holding a small subgraph that may be biased from the distribution of the\nwhole graph. Hence, the subgraph federated learning aims to collaboratively\ntrain a powerful and generalizable graph mining model without directly sharing\ntheir graph data. In this work, towards the novel yet realistic setting of\nsubgraph federated learning, we propose two major techniques: (1) FedSage,\nwhich trains a GraphSage model based on FedAvg to integrate node features, link\nstructures, and task labels on multiple local subgraphs; (2) FedSage+, which\ntrains a missing neighbor generator along FedSage to deal with missing links\nacross local subgraphs. Empirical results on four real-world graph datasets\nwith synthesized subgraph federated learning settings demonstrate the\neffectiveness and efficiency of our proposed techniques. At the same time,\nconsistent theoretical implications are made towards their generalization\nability on the global graphs.",
          "link": "http://arxiv.org/abs/2106.13430",
          "publishedOn": "2021-06-28T01:57:56.893Z",
          "wordCount": 632,
          "title": "Subgraph Federated Learning with Missing Neighbor Generation. (arXiv:2106.13430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>",
          "description": "Breakthroughs in machine learning in the last decade have led to `digital\nintelligence', i.e. machine learning models capable of learning from vast\namounts of labeled data to perform several digital tasks such as speech\nrecognition, face recognition, machine translation and so on. The goal of this\nthesis is to make progress towards designing algorithms capable of `physical\nintelligence', i.e. building intelligent autonomous navigation agents capable\nof learning to perform complex navigation tasks in the physical world involving\nvisual perception, natural language understanding, reasoning, planning, and\nsequential decision making. Despite several advances in classical navigation\nmethods in the last few decades, current navigation agents struggle at\nlong-term semantic navigation tasks. In the first part of the thesis, we\ndiscuss our work on short-term navigation using end-to-end reinforcement\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\nlanguage grounding, and reasoning. In the second part, we present a new class\nof navigation methods based on modular learning and structured explicit map\nrepresentations, which leverage the strengths of both classical and end-to-end\nlearning methods, to tackle long-term navigation tasks. We show that these\nmethods are able to effectively tackle challenges such as localization,\nmapping, long-term planning, exploration and learning semantic priors. These\nmodular learning methods are capable of long-term spatial and semantic\nunderstanding and achieve state-of-the-art results on various navigation tasks.",
          "link": "http://arxiv.org/abs/2106.13415",
          "publishedOn": "2021-06-28T01:57:56.885Z",
          "wordCount": 671,
          "title": "Building Intelligent Autonomous Navigation Agents. (arXiv:2106.13415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1\">Kamalika Chaudhuri</a>",
          "description": "Normalizing flows are a class of flexible deep generative models that offer\neasy likelihood computation. Despite their empirical success, there is little\ntheoretical understanding of their expressiveness. In this work, we study\nresidual flows, a class of normalizing flows composed of Lipschitz residual\nblocks. We prove residual flows are universal approximators in maximum mean\ndiscrepancy. We provide upper bounds on the number of residual blocks to\nachieve approximation under different assumptions.",
          "link": "http://arxiv.org/abs/2103.05793",
          "publishedOn": "2021-06-28T01:57:56.876Z",
          "wordCount": 532,
          "title": "Universal Approximation of Residual Flows in Maximum Mean Discrepancy. (arXiv:2103.05793v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>",
          "description": "Inspired by a new coded computation algorithm for invertible functions, we\npropose Coded-InvNet a new approach to design resilient prediction serving\nsystems that can gracefully handle stragglers or node failures. Coded-InvNet\nleverages recent findings in the deep learning literature such as invertible\nneural networks, Manifold Mixup, and domain translation algorithms, identifying\ninteresting research directions that span across machine learning and systems.\nOur experimental results show that Coded-InvNet can outperform existing\napproaches, especially when the compute resource overhead is as low as 10%. For\ninstance, without knowing which of the ten workers is going to fail, our\nalgorithm can design a backup task so that it can correctly recover the missing\nprediction result with an accuracy of 85.9%, significantly outperforming the\nprevious SOTA by 32.5%.",
          "link": "http://arxiv.org/abs/2106.06445",
          "publishedOn": "2021-06-28T01:57:56.870Z",
          "wordCount": 569,
          "title": "Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03326",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1\">Marco Mondelli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1\">Christos Thrampoulidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1\">Ramji Venkataramanan</a>",
          "description": "We study the problem of recovering an unknown signal $\\boldsymbol x$ given\nmeasurements obtained from a generalized linear model with a Gaussian sensing\nmatrix. Two popular solutions are based on a linear estimator $\\hat{\\boldsymbol\nx}^{\\rm L}$ and a spectral estimator $\\hat{\\boldsymbol x}^{\\rm s}$. The former\nis a data-dependent linear combination of the columns of the measurement\nmatrix, and its analysis is quite simple. The latter is the principal\neigenvector of a data-dependent matrix, and a recent line of work has studied\nits performance. In this paper, we show how to optimally combine\n$\\hat{\\boldsymbol x}^{\\rm L}$ and $\\hat{\\boldsymbol x}^{\\rm s}$. At the heart\nof our analysis is the exact characterization of the joint empirical\ndistribution of $(\\boldsymbol x, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol\nx}^{\\rm s})$ in the high-dimensional limit. This allows us to compute the\nBayes-optimal combination of $\\hat{\\boldsymbol x}^{\\rm L}$ and\n$\\hat{\\boldsymbol x}^{\\rm s}$, given the limiting distribution of the signal\n$\\boldsymbol x$. When the distribution of the signal is Gaussian, then the\nBayes-optimal combination has the form $\\theta\\hat{\\boldsymbol x}^{\\rm\nL}+\\hat{\\boldsymbol x}^{\\rm s}$ and we derive the optimal combination\ncoefficient. In order to establish the limiting distribution of $(\\boldsymbol\nx, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol x}^{\\rm s})$, we design and\nanalyze an Approximate Message Passing (AMP) algorithm whose iterates give\n$\\hat{\\boldsymbol x}^{\\rm L}$ and approach $\\hat{\\boldsymbol x}^{\\rm s}$.\nNumerical simulations demonstrate the improvement of the proposed combination\nwith respect to the two methods considered separately.",
          "link": "http://arxiv.org/abs/2008.03326",
          "publishedOn": "2021-06-28T01:57:56.851Z",
          "wordCount": 713,
          "title": "Optimal Combination of Linear and Spectral Estimators for Generalized Linear Models. (arXiv:2008.03326v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfke_D/0/1/0/all/0/1\">Dominik Alfke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gondos_M/0/1/0/all/0/1\">Miriam Gondos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroche_L/0/1/0/all/0/1\">Lucile Peroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1\">Martin Stoll</a>",
          "description": "Time series data play an important role in many applications and their\nanalysis reveals crucial information for understanding the underlying\nprocesses. Among the many time series learning tasks of great importance, we\nhere focus on semi-supervised learning based on a graph representation of the\ndata. Two main aspects are involved in this task. A suitable distance measure\nto evaluate the similarities between time series, and a learning method to make\npredictions based on these distances. However, the relationship between the two\naspects has never been studied systematically in the context of graph-based\nlearning. We describe four different distance measures, including (Soft) DTW\nand MPDist, a distance measure based on the Matrix Profile, as well as four\nsuccessful semi-supervised learning methods, including the graph Allen--Cahn\nmethod and a Graph Convolutional Neural Network. We then compare the\nperformance of the algorithms on binary classification data sets. In our\nfindings we compare the chosen graph-based methods using all distance measures\nand observe that the results vary strongly with respect to the accuracy. As\npredicted by the ``no free lunch'' theorem, no clear best combination to employ\nin all cases is found. Our study provides a reproducible framework for future\nwork in the direction of semi-supervised learning for time series with a focus\non graph representations.",
          "link": "http://arxiv.org/abs/2104.08153",
          "publishedOn": "2021-06-28T01:57:56.841Z",
          "wordCount": 683,
          "title": "An Empirical Study of Graph-Based Approaches for Semi-Supervised Time Series Classification. (arXiv:2104.08153v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>",
          "description": "Although many techniques have been applied to matrix factorization (MF), they\nmay not fully exploit the feature structure. In this paper, we incorporate the\ngrouping effect into MF and propose a novel method called Robust Matrix\nFactorization with Grouping effect (GRMF). The grouping effect is a\ngeneralization of the sparsity effect, which conducts denoising by clustering\nsimilar values around multiple centers instead of just around 0. Compared with\nexisting algorithms, the proposed GRMF can automatically learn the grouping\nstructure and sparsity in MF without prior knowledge, by introducing a\nnaturally adjustable non-convex regularization to achieve simultaneous sparsity\nand grouping effect. Specifically, GRMF uses an efficient alternating\nminimization framework to perform MF, in which the original non-convex problem\nis first converted into a convex problem through Difference-of-Convex (DC)\nprogramming, and then solved by Alternating Direction Method of Multipliers\n(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix\nFactorization (NMF) settings. Extensive experiments have been conducted using\nreal-world data sets with outliers and contaminated noise, where the\nexperimental results show that GRMF has promoted performance and robustness,\ncompared to five benchmark algorithms.",
          "link": "http://arxiv.org/abs/2106.13681",
          "publishedOn": "2021-06-28T01:57:56.814Z",
          "wordCount": 626,
          "title": "Robust Matrix Factorization with Grouping Effect. (arXiv:2106.13681v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shokry_A/0/1/0/all/0/1\">Ahmed Shokry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torki_M/0/1/0/all/0/1\">Marwan Torki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1\">Moustafa Youssef</a>",
          "description": "Recent years have witnessed fast growth in outdoor location-based services.\nWhile GPS is considered a ubiquitous localization system, it is not supported\nby low-end phones, requires direct line of sight to the satellites, and can\ndrain the phone battery quickly.\n\nIn this paper, we propose DeepLoc: a deep learning-based outdoor localization\nsystem that obtains GPS-like localization accuracy without its limitations. In\nparticular, DeepLoc leverages the ubiquitous cellular signals received from the\ndifferent cell towers heard by the mobile device as hints to localize it. To do\nthat, crowd-sensed geo-tagged received signal strength information coming from\ndifferent cell towers is used to train a deep model that is used to infer the\nuser's position. As part of DeepLoc design, we introduce modules to address a\nnumber of practical challenges including scaling the data collection to large\nareas, handling the inherent noise in the cellular signal and geo-tagged data,\nas well as providing enough data that is required for deep learning models with\nlow-overhead.\n\nWe implemented DeepLoc on different Android devices. Evaluation results in\nrealistic urban and rural environments show that DeepLoc can achieve a median\nlocalization accuracy within 18.8m in urban areas and within 15.7m in rural\nareas. This accuracy outperforms the state-of-the-art cellular-based systems by\nmore than 470% and comes with 330% savings in power compared to the GPS. This\nhighlights the promise of DeepLoc as a ubiquitous accurate and low-overhead\nlocalization system.",
          "link": "http://arxiv.org/abs/2106.13632",
          "publishedOn": "2021-06-28T01:57:56.808Z",
          "wordCount": 690,
          "title": "DeepLoc: A Ubiquitous Accurate and Low-Overhead Outdoor Cellular Localization System. (arXiv:2106.13632v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Ting-Kuei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1\">Fernando Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>",
          "description": "In this paper, we present a perception-action-communication loop design using\nVision-based Graph Aggregation and Inference (VGAI). This multi-agent\ndecentralized learning-to-control framework maps raw visual observations to\nagent actions, aided by local communication among neighboring agents. Our\nframework is implemented by a cascade of a convolutional and a graph neural\nnetwork (CNN / GNN), addressing agent-level visual perception and feature\nlearning, as well as swarm-level communication, local information aggregation\nand agent action inference, respectively. By jointly training the CNN and GNN,\nimage features and communication messages are learned in conjunction to better\naddress the specific task. We use imitation learning to train the VGAI\ncontroller in an offline phase, relying on a centralized expert controller.\nThis results in a learned VGAI controller that can be deployed in a distributed\nmanner for online execution. Additionally, the controller exhibits good scaling\nproperties, with training in smaller teams and application in larger teams.\nThrough a multi-agent flocking application, we demonstrate that VGAI yields\nperformance comparable to or better than other decentralized controllers, using\nonly the visual input modality and without accessing precise location or motion\nstate information.",
          "link": "http://arxiv.org/abs/2106.13358",
          "publishedOn": "2021-06-28T01:57:56.797Z",
          "wordCount": 639,
          "title": "Scalable Perception-Action-Communication Loops with Convolutional and Graph Neural Networks. (arXiv:2106.13358v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_R/0/1/0/all/0/1\">Rohit Kumar Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rachit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Sandeep Kumar Shukla</a>",
          "description": "The rise in the adoption of blockchain technology has led to increased\nillegal activities by cyber-criminals costing billions of dollars. Many machine\nlearning algorithms are applied to detect such illegal behavior. These\nalgorithms are often trained on the transaction behavior and, in some cases,\ntrained on the vulnerabilities that exist in the system. In our approach, we\nstudy the feasibility of using metadata such as Domain Name (DN) associated\nwith the account in the blockchain and identify whether an account should be\ntagged malicious or not. Here, we leverage the temporal aspects attached to the\nDNs. Our results identify 144930 DNs that show malicious behavior, and out of\nthese, 54114 DNs show persistent malicious behavior over time. Nonetheless,\nnone of these identified malicious DNs were reported in new officially tagged\nmalicious blockchain DNs.",
          "link": "http://arxiv.org/abs/2106.13420",
          "publishedOn": "2021-06-28T01:57:56.777Z",
          "wordCount": 581,
          "title": "Identifying malicious accounts in Blockchains using Domain Names and associated temporal properties. (arXiv:2106.13420v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zouzias_A/0/1/0/all/0/1\">Anastasios Zouzias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalaitzidis_K/0/1/0/all/0/1\">Kleovoulos Kalaitzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grot_B/0/1/0/all/0/1\">Boris Grot</a>",
          "description": "Recent years have seen stagnating improvements to branch predictor (BP)\nefficacy and a dearth of fresh ideas in branch predictor design, calling for\nfresh thinking in this area. This paper argues that looking at BP from the\nviewpoint of Reinforcement Learning (RL) facilitates systematic reasoning\nabout, and exploration of, BP designs. We describe how to apply the RL\nformulation to branch predictors, show that existing predictors can be\nsuccinctly expressed in this formulation, and study two RL-based variants of\nconventional BPs.",
          "link": "http://arxiv.org/abs/2106.13429",
          "publishedOn": "2021-06-28T01:57:56.767Z",
          "wordCount": 533,
          "title": "Branch Prediction as a Reinforcement Learning Problem: Why, How and Case Studies. (arXiv:2106.13429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1\">Hannes Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Geoff Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1\">Phil Bachman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachet_R/0/1/0/all/0/1\">Remi Tachet</a>",
          "description": "Recent contrastive representation learning methods rely on estimating mutual\ninformation (MI) between multiple views of an underlying context. E.g., we can\nderive multiple views of a given image by applying data augmentation, or we can\nsplit a sequence into views comprising the past and future of some step in the\nsequence. Contrastive lower bounds on MI are easy to optimize, but have a\nstrong underestimation bias when estimating large amounts of MI. We propose\ndecomposing the full MI estimation problem into a sum of smaller estimation\nproblems by splitting one of the views into progressively more informed\nsubviews and by applying the chain rule on MI between the decomposed views.\nThis expression contains a sum of unconditional and conditional MI terms, each\nmeasuring modest chunks of the total MI, which facilitates approximation via\ncontrastive bounds. To maximize the sum, we formulate a contrastive lower bound\non the conditional MI which can be approximated efficiently. We refer to our\ngeneral approach as Decomposed Estimation of Mutual Information (DEMI). We show\nthat DEMI can capture a larger amount of MI than standard non-decomposed\ncontrastive bounds in a synthetic setting, and learns better representations in\na vision domain and for dialogue generation.",
          "link": "http://arxiv.org/abs/2106.13401",
          "publishedOn": "2021-06-28T01:57:56.759Z",
          "wordCount": 640,
          "title": "Decomposed Mutual Information Estimation for Contrastive Representation Learning. (arXiv:2106.13401v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bizhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yijin Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guojie Luo</a>",
          "description": "In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art\nalgorithms for analyzing non-euclidean graph data. By applying deep-learning to\nextract high-level representations from graph structures, GNNs achieve\nextraordinary accuracy and great generalization ability in various tasks.\nHowever, with the ever-increasing graph sizes, more and more complicated GNN\nlayers, and higher feature dimensions, the computational complexity of GNNs\ngrows exponentially. How to inference GNNs in real time has become a\nchallenging problem, especially for some resource-limited edge-computing\nplatforms.\n\nTo tackle this challenge, we propose BlockGNN, a software-hardware co-design\napproach to realize efficient GNN acceleration. At the algorithm level, we\npropose to leverage block-circulant weight matrices to greatly reduce the\ncomplexity of various GNN models. At the hardware design level, we propose a\npipelined CirCore architecture, which supports efficient block-circulant\nmatrices computation. Basing on CirCore, we present a novel BlockGNN\naccelerator to compute various GNNs with low latency. Moreover, to determine\nthe optimal configurations for diverse deployed tasks, we also introduce a\nperformance and resource model that helps choose the optimal hardware\nparameters automatically. Comprehensive experiments on the ZC706 FPGA platform\ndemonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\\times$\nspeedup compared to the baseline HyGCN architecture and $111.9\\times$ energy\nreduction compared to the Intel Xeon CPU platform.",
          "link": "http://arxiv.org/abs/2104.06214",
          "publishedOn": "2021-06-28T01:57:56.745Z",
          "wordCount": 682,
          "title": "BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. (arXiv:2104.06214v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "State-of-the-art deep-learning-based voice activity detectors (VADs) are\noften trained with anechoic data. However, real acoustic environments are\ngenerally reverberant, which causes the performance to significantly\ndeteriorate. To mitigate this mismatch between training data and real data, we\nsimulate an augmented training set that contains nearly five million\nutterances. This extension comprises of anechoic utterances and their\nreverberant modifications, generated by convolutions of the anechoic utterances\nwith a variety of room impulse responses (RIRs). We consider five different\nmodels to generate RIRs, and five different VADs that are trained with the\naugmented training set. We test all trained systems in three different real\nreverberant environments. Experimental results show $20\\%$ increase on average\nin accuracy, precision and recall for all detectors and response models,\ncompared to anechoic training. Furthermore, one of the RIR models consistently\nyields better performance than the other models, for all the tested VADs.\nAdditionally, one of the VADs consistently outperformed the other VADs in all\nexperiments.",
          "link": "http://arxiv.org/abs/2106.13511",
          "publishedOn": "2021-06-28T01:57:56.726Z",
          "wordCount": 621,
          "title": "Evaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments. (arXiv:2106.13511v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rai_K/0/1/0/all/0/1\">Khushwant Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hojatpanah_F/0/1/0/all/0/1\">Farnam Hojatpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajaei_F/0/1/0/all/0/1\">Firouz Badrkhani Ajaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grolinger_K/0/1/0/all/0/1\">Katarina Grolinger</a>",
          "description": "High-impedance faults (HIF) are difficult to detect because of their low\ncurrent amplitude and highly diverse characteristics. In recent years, machine\nlearning (ML) has been gaining popularity in HIF detection because ML\ntechniques learn patterns from data and successfully detect HIFs. However, as\nthese methods are based on supervised learning, they fail to reliably detect\nany scenario, fault or non-fault, not present in the training data.\nConsequently, this paper takes advantage of unsupervised learning and proposes\na convolutional autoencoder framework for HIF detection (CAE-HIFD). Contrary to\nthe conventional autoencoders that learn from normal behavior, the\nconvolutional autoencoder (CAE) in CAE-HIFD learns only from the HIF signals\neliminating the need for presence of diverse non-HIF scenarios in the CAE\ntraining. CAE distinguishes HIFs from non-HIF operating conditions by employing\ncross-correlation. To discriminate HIFs from transient disturbances such as\ncapacitor or load switching, CAE-HIFD uses kurtosis, a statistical measure of\nthe probability distribution shape. The performance evaluation studies\nconducted using the IEEE 13-node test feeder indicate that the CAE-HIFD\nreliably detects HIFs, outperforms the state-of-the-art HIF detection\ntechniques, and is robust against noise.",
          "link": "http://arxiv.org/abs/2106.13276",
          "publishedOn": "2021-06-28T01:57:56.679Z",
          "wordCount": 624,
          "title": "Deep Learning for High-Impedance Fault Detection: Convolutional Autoencoders. (arXiv:2106.13276v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mojto_M/0/1/0/all/0/1\">Martin Mojto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubusky_K/0/1/0/all/0/1\">Karol &#x13d;ubu&#x161;k&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fikar_M/0/1/0/all/0/1\">Miroslav Fikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulen_R/0/1/0/all/0/1\">Radoslav Paulen</a>",
          "description": "Inferential (or soft) sensors are used in industry to infer the values of\nimprecisely and rarely measured (or completely unmeasured) variables from\nvariables measured online (e.g., pressures, temperatures). The main challenge,\nakin to classical model overfitting, in designing an effective inferential\nsensor is the selection of a correct structure of the sensor. The sensor\nstructure is represented by the number of inputs to the sensor, which\ncorrespond to the variables measured online and their (simple) combinations.\nThis work is focused on the design of inferential sensors for product\ncomposition of an industrial distillation column in two oil refinery units, a\nFluid Catalytic Cracking unit and a Vacuum Gasoil Hydrogenation unit. As the\nfirst design step, we use several well-known data pre-treatment (gross error\ndetection) methods and compare the ability of these approaches to indicate\nsystematic errors and outliers in the available industrial data. We then study\neffectiveness of various methods for design of the inferential sensors taking\ninto account the complexity and accuracy of the resulting model. The\neffectiveness analysis indicates that the improvements achieved over the\ncurrent inferential sensors are up to 19 %.",
          "link": "http://arxiv.org/abs/2106.13503",
          "publishedOn": "2021-06-28T01:57:56.673Z",
          "wordCount": 620,
          "title": "Data-based Design of Inferential Sensors for Petrochemical Industry. (arXiv:2106.13503v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.09734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Na Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yuan-Hai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huajun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu-Ting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiu_N/0/1/0/all/0/1\">Naihua Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Nai-Yang Deng</a>",
          "description": "Considering the classification problem, we summarize the nonparallel support\nvector machines with the nonparallel hyperplanes to two types of frameworks.\nThe first type constructs the hyperplanes separately. It solves a series of\nsmall optimization problems to obtain a series of hyperplanes, but is hard to\nmeasure the loss of each sample. The other type constructs all the hyperplanes\nsimultaneously, and it solves one big optimization problem with the ascertained\nloss of each sample. We give the characteristics of each framework and compare\nthem carefully. In addition, based on the second framework, we construct a\nmax-min distance-based nonparallel support vector machine for multiclass\nclassification problem, called NSVM. It constructs hyperplanes with large\ndistance margin by solving an optimization problem. Experimental results on\nbenchmark data sets show the advantages of our NSVM.",
          "link": "http://arxiv.org/abs/1910.09734",
          "publishedOn": "2021-06-28T01:57:56.661Z",
          "wordCount": 616,
          "title": "Single and Union Non-parallel Support Vector Machine Frameworks. (arXiv:1910.09734v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McGovern_H/0/1/0/all/0/1\">Hope McGovern</a>",
          "description": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.",
          "link": "http://arxiv.org/abs/2106.13382",
          "publishedOn": "2021-06-28T01:57:56.648Z",
          "wordCount": 569,
          "title": "A Source-Criticism Debiasing Method for GloVe Embeddings. (arXiv:2106.13382v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>",
          "description": "We propose a nonlinear acoustic echo cancellation system, which aims to model\nthe echo path from the far-end signal to the near-end microphone in two parts.\nInspired by the physical behavior of modern hands-free devices, we first\nintroduce a novel neural network architecture that is specifically designed to\nmodel the nonlinear distortions these devices induce between receiving and\nplaying the far-end signal. To account for variations between devices, we\nconstruct this network with trainable memory length and nonlinear activation\nfunctions that are not parameterized in advance, but are rather optimized\nduring the training stage using the training data. Second, the network is\nsucceeded by a standard adaptive linear filter that constantly tracks the echo\npath between the loudspeaker output and the microphone. During training, the\nnetwork and filter are jointly optimized to learn the network parameters. This\nsystem requires 17 thousand parameters that consume 500 Million floating-point\noperations per second and 40 Kilo-bytes of memory. It also satisfies hands-free\ncommunication timing requirements on a standard neural processor, which renders\nit adequate for embedding on hands-free communication devices. Using 280 hours\nof real and synthetic data, experiments show advantageous performance compared\nto competing methods.",
          "link": "http://arxiv.org/abs/2106.13754",
          "publishedOn": "2021-06-28T01:57:56.620Z",
          "wordCount": 633,
          "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning. (arXiv:2106.13754v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13724",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Rezaie_M/0/1/0/all/0/1\">Mehdi Rezaie</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ross_A/0/1/0/all/0/1\">Ashley J. Ross</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Seo_H/0/1/0/all/0/1\">Hee-Jong Seo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mueller_E/0/1/0/all/0/1\">Eva-Maria Mueller</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Percival_W/0/1/0/all/0/1\">Will J. Percival</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Merz_G/0/1/0/all/0/1\">Grant Merz</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Katebi_R/0/1/0/all/0/1\">Reza Katebi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bunescu_R/0/1/0/all/0/1\">Razvan C. Bunescu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bautista_J/0/1/0/all/0/1\">Julian Bautista</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Brownstein_J/0/1/0/all/0/1\">Joel R. Brownstein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Burtin_E/0/1/0/all/0/1\">Etienne Burtin</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dawson_K/0/1/0/all/0/1\">Kyle Dawson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gil_Marin_H/0/1/0/all/0/1\">H&#xe9;ctor Gil-Mar&#xed;n</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hou_J/0/1/0/all/0/1\">Jiamin Hou</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lyke_E/0/1/0/all/0/1\">Eleanor B. Lyke</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Macorra_A/0/1/0/all/0/1\">Axel de la Macorra</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rossi_G/0/1/0/all/0/1\">Graziano Rossi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schneider_D/0/1/0/all/0/1\">Donald P. Schneider</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zarrouk_P/0/1/0/all/0/1\">Pauline Zarrouk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhao_G/0/1/0/all/0/1\">Gong-Bo Zhao</a>",
          "description": "We investigate the large-scale clustering of the final spectroscopic sample\nof quasars from the recently completed extended Baryon Oscillation\nSpectroscopic Survey (eBOSS). The sample contains $343708$ objects in the\nredshift range $0.8<z<2.2$ and $72667$ objects with redshifts $2.2<z<3.5$,\ncovering an effective area of $4699~{\\rm deg}^{2}$. We develop a neural\nnetwork-based approach to mitigate spurious fluctuations in the density field\ncaused by spatial variations in the quality of the imaging data used to select\ntargets for follow-up spectroscopy. Simulations are used with the same angular\nand radial distributions as the real data to estimate covariance matrices,\nperform error analyses, and assess residual systematic uncertainties. We\nmeasure the mean density contrast and cross-correlations of the eBOSS quasars\nagainst maps of potential sources of imaging systematics to address algorithm\neffectiveness, finding that the neural network-based approach outperforms\nstandard linear regression. Stellar density is one of the most important\nsources of spurious fluctuations, and a new template constructed using data\nfrom the Gaia spacecraft provides the best match to the observed quasar\nclustering. The end-product from this work is a new value-added quasar\ncatalogue with the improved weights to correct for nonlinear imaging systematic\neffects, which will be made public. Our quasar catalogue is used to measure the\nlocal-type primordial non-Gaussianity in our companion paper, Mueller et al. in\npreparation.",
          "link": "http://arxiv.org/abs/2106.13724",
          "publishedOn": "2021-06-28T01:57:56.611Z",
          "wordCount": 755,
          "title": "Primordial non-Gaussianity from the Completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey I: Catalogue Preparation and Systematic Mitigation. (arXiv:2106.13724v1 [astro-ph.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "Although the optimization objectives for learning neural networks are highly\nnon-convex, gradient-based methods have been wildly successful at learning\nneural networks in practice. This juxtaposition has led to a number of recent\nstudies on provable guarantees for neural networks trained by gradient descent.\nUnfortunately, the techniques in these works are often highly specific to the\nproblem studied in each setting, relying on different assumptions on the\ndistribution, optimization parameters, and network architectures, making it\ndifficult to generalize across different settings. In this work, we propose a\nunified non-convex optimization framework for the analysis of neural network\ntraining. We introduce the notions of proxy convexity and proxy\nPolyak-Lojasiewicz (PL) inequalities, which are satisfied if the original\nobjective function induces a proxy objective function that is implicitly\nminimized when using gradient methods. We show that stochastic gradient descent\n(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads\nto efficient guarantees for proxy objective functions. We further show that\nmany existing guarantees for neural networks trained by gradient descent can be\nunified through proxy convexity and proxy PL inequalities.",
          "link": "http://arxiv.org/abs/2106.13792",
          "publishedOn": "2021-06-28T01:57:56.601Z",
          "wordCount": 633,
          "title": "Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent. (arXiv:2106.13792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berdugo_B/0/1/0/all/0/1\">Baruch Berdugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>",
          "description": "We address voice activity detection in acoustic environments of transients\nand stationary noises, which often occur in real life scenarios. We exploit\nunique spatial patterns of speech and non-speech audio frames by independently\nlearning their underlying geometric structure. This process is done through a\ndeep encoder-decoder based neural network architecture. This structure involves\nan encoder that maps spectral features with temporal information to their\nlow-dimensional representations, which are generated by applying the diffusion\nmaps method. The encoder feeds a decoder that maps the embedded data back into\nthe high-dimensional space. A deep neural network, which is trained to separate\nspeech from non-speech frames, is obtained by concatenating the decoder to the\nencoder, resembling the known Diffusion nets architecture. Experimental results\nshow enhanced performance compared to competing voice activity detection\nmethods. The improvement is achieved in both accuracy, robustness and\ngeneralization ability. Our model performs in a real-time manner and can be\nintegrated into audio-based communication systems. We also present a batch\nalgorithm which obtains an even higher accuracy for off-line applications.",
          "link": "http://arxiv.org/abs/2106.13763",
          "publishedOn": "2021-06-28T01:57:56.595Z",
          "wordCount": 639,
          "title": "Voice Activity Detection for Transient Noisy Environment Based on Diffusion Nets. (arXiv:2106.13763v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1\">Hilal Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_A/0/1/0/all/0/1\">Alireza Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1\">Omid Javidbakht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>",
          "description": "We study adaptive methods for differentially private convex optimization,\nproposing and analyzing differentially private variants of a Stochastic\nGradient Descent (SGD) algorithm with adaptive stepsizes, as well as the\nAdaGrad algorithm. We provide upper bounds on the regret of both algorithms and\nshow that the bounds are (worst-case) optimal. As a consequence of our\ndevelopment, we show that our private versions of AdaGrad outperform adaptive\nSGD, which in turn outperforms traditional SGD in scenarios with non-isotropic\ngradients where (non-private) Adagrad provably outperforms SGD. The major\nchallenge is that the isotropic noise typically added for privacy dominates the\nsignal in gradient geometry for high-dimensional problems; approaches to this\nthat effectively optimize over lower-dimensional subspaces simply ignore the\nactual problems that varying gradient geometries introduce. In contrast, we\nstudy non-isotropic clipping and noise addition, developing a principled\ntheoretical approach; the consequent procedures also enjoy significantly\nstronger empirical performance than prior approaches.",
          "link": "http://arxiv.org/abs/2106.13756",
          "publishedOn": "2021-06-28T01:57:56.585Z",
          "wordCount": 607,
          "title": "Private Adaptive Gradient Methods for Convex Optimization. (arXiv:2106.13756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNeil_M/0/1/0/all/0/1\">Maxwell McNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdanov_P/0/1/0/all/0/1\">Petko Bogdanov</a>",
          "description": "Temporal graph signals are multivariate time series with individual\ncomponents associated with nodes of a fixed graph structure. Data of this kind\narises in many domains including activity of social network users, sensor\nnetwork readings over time, and time course gene expression within the\ninteraction network of a model organism. Traditional matrix decomposition\nmethods applied to such data fall short of exploiting structural regularities\nencoded in the underlying graph and also in the temporal patterns of the\nsignal. How can we take into account such structure to obtain a succinct and\ninterpretable representation of temporal graph signals?\n\nWe propose a general, dictionary-based framework for temporal graph signal\ndecomposition (TGSD). The key idea is to learn a low-rank, joint encoding of\nthe data via a combination of graph and time dictionaries. We propose a highly\nscalable decomposition algorithm for both complete and incomplete data, and\ndemonstrate its advantage for matrix decomposition, imputation of missing\nvalues, temporal interpolation, clustering, period estimation, and rank\nestimation in synthetic and real-world data ranging from traffic patterns to\nsocial media activity. Our framework achieves 28% reduction in RMSE compared to\nbaselines for temporal interpolation when as many as 75% of the observations\nare missing. It scales best among baselines taking under 20 seconds on 3.5\nmillion data points and produces the most parsimonious models. To the best of\nour knowledge, TGSD is the first framework to jointly model graph signals by\ntemporal and graph dictionaries.",
          "link": "http://arxiv.org/abs/2106.13517",
          "publishedOn": "2021-06-28T01:57:56.574Z",
          "wordCount": 699,
          "title": "Temporal Graph Signal Decomposition. (arXiv:2106.13517v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenglei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Sidan Du</a>",
          "description": "Regularization plays a vital role in machine learning optimization. One novel\nregularization method called flooding makes the training loss fluctuate around\nthe flooding level. It intends to make the model continue to random walk until\nit comes to a flat loss landscape to enhance generalization. However, the\nhyper-parameter flooding level of the flooding method fails to be selected\nproperly and uniformly. We propose a novel method called Jitter to improve it.\nJitter is essentially a kind of random loss function. Before training, we\nrandomly sample the Jitter Point from a specific probability distribution. The\nflooding level should be replaced by Jitter point to obtain a new target\nfunction and train the model accordingly. As Jitter point acting as a random\nfactor, we actually add some randomness to the loss function, which is\nconsistent with the fact that there exists innumerable random behaviors in the\nlearning process of the machine learning model and is supposed to make the\nmodel more robust. In addition, Jitter performs random walk randomly which\ndivides the loss curve into small intervals and then flipping them over,\nideally making the loss curve much flatter and enhancing generalization\nability. Moreover, Jitter can be a domain-, task-, and model-independent\nregularization method and train the model effectively after the training error\nreduces to zero. Our experimental results show that Jitter method can improve\nmodel performance more significantly than the previous flooding method and make\nthe test loss curve descend twice.",
          "link": "http://arxiv.org/abs/2106.13749",
          "publishedOn": "2021-06-28T01:57:56.567Z",
          "wordCount": 665,
          "title": "Jitter: Random Jittering Loss Function. (arXiv:2106.13749v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1\">Erik Hemberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1\">Una-May O&#x27;Reilly</a>",
          "description": "Generative adversary networks (GANs) suffer from training pathologies such as\ninstability and mode collapse, which mainly arise from a lack of diversity in\ntheir adversarial interactions. Co-evolutionary GAN (CoE-GAN) training\nalgorithms have shown to be resilient to these pathologies. This article\nintroduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity\nby using different loss functions during the training. Experimental analysis on\nMNIST and CelebA demonstrated that Mustangs trains statistically more accurate\ngenerators.",
          "link": "http://arxiv.org/abs/2106.13590",
          "publishedOn": "2021-06-28T01:57:56.471Z",
          "wordCount": 536,
          "title": "Fostering Diversity in Spatial Evolutionary Generative Adversarial Networks. (arXiv:2106.13590v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13746",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miao_N/0/1/0/all/0/1\">Ning Miao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1\">Emile Mathieu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>",
          "description": "We introduce a simple and effective method for learning VAEs with\ncontrollable inductive biases by using an intermediary set of latent variables.\nThis allows us to overcome the limitations of the standard Gaussian prior\nassumption. In particular, it allows us to impose desired properties like\nsparsity or clustering on learned representations, and incorporate prior\ninformation into the learned model. Our approach, which we refer to as the\nIntermediary Latent Space VAE (InteL-VAE), is based around controlling the\nstochasticity of the encoding process with the intermediary latent variables,\nbefore deterministically mapping them forward to our target latent\nrepresentation, from which reconstruction is performed. This allows us to\nmaintain all the advantages of the traditional VAE framework, while\nincorporating desired prior information, inductive biases, and even topological\ninformation through the latent mapping. We show that this, in turn, allows\nInteL-VAEs to learn both better generative models and representations.",
          "link": "http://arxiv.org/abs/2106.13746",
          "publishedOn": "2021-06-28T01:57:56.452Z",
          "wordCount": 586,
          "title": "InteL-VAEs: Adding Inductive Biases to Variational Auto-Encoders via Intermediary Latents. (arXiv:2106.13746v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>",
          "description": "We propose a theoretical approach towards the training numerical stability of\nVariational AutoEncoders (VAE). Our work is motivated by recent studies\nempowering VAEs to reach state of the art generative results on complex image\ndatasets. These very deep VAE architectures, as well as VAEs using more complex\noutput distributions, highlight a tendency to haphazardly produce high training\ngradients as well as NaN losses. The empirical fixes proposed to train them\ndespite their limitations are neither fully theoretically grounded nor\ngenerally sufficient in practice. Building on this, we localize the source of\nthe problem at the interface between the model's neural networks and their\noutput probabilistic distributions. We explain a common source of instability\nstemming from an incautious formulation of the encoded Normal distribution's\nvariance, and apply the same approach on other, less obvious sources. We show\nthat by implementing small changes to the way we parameterize the Normal\ndistributions on which they rely, VAEs can securely be trained.",
          "link": "http://arxiv.org/abs/2106.13739",
          "publishedOn": "2021-06-28T01:57:56.430Z",
          "wordCount": 583,
          "title": "Re-parameterizing VAEs for stability. (arXiv:2106.13739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13689",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wahab_N/0/1/0/all/0/1\">Noorul Wahab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miligy_I/0/1/0/all/0/1\">Islam M Miligy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dodd_K/0/1/0/all/0/1\">Katherine Dodd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahota_H/0/1/0/all/0/1\">Harvir Sahota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toss_M/0/1/0/all/0/1\">Michael Toss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wenqi Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Young Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hadjigeorghiou_G/0/1/0/all/0/1\">Giorgos Hadjigeorghiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhalerao_A/0/1/0/all/0/1\">Abhir Bhalerao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lashen_A/0/1/0/all/0/1\">Ayat Lashen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ibrahim_A/0/1/0/all/0/1\">Asmaa Ibrahim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katayama_A/0/1/0/all/0/1\">Ayaka Katayama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebili_H/0/1/0/all/0/1\">Henry O Ebili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkin_M/0/1/0/all/0/1\">Matthew Parkin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorell_T/0/1/0/all/0/1\">Tom Sorell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hero_E/0/1/0/all/0/1\">Emily Hero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldaly_H/0/1/0/all/0/1\">Hesham Eldaly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsang_Y/0/1/0/all/0/1\">Yee Wah Tsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kishore Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rakha_E/0/1/0/all/0/1\">Emad Rakha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>",
          "description": "Recent advances in whole slide imaging (WSI) technology have led to the\ndevelopment of a myriad of computer vision and artificial intelligence (AI)\nbased diagnostic, prognostic, and predictive algorithms. Computational\nPathology (CPath) offers an integrated solution to utilize information embedded\nin pathology WSIs beyond what we obtain through visual assessment. For\nautomated analysis of WSIs and validation of machine learning (ML) models,\nannotations at the slide, tissue and cellular levels are required. The\nannotation of important visual constructs in pathology images is an important\ncomponent of CPath projects. Improper annotations can result in algorithms\nwhich are hard to interpret and can potentially produce inaccurate and\ninconsistent results. Despite the crucial role of annotations in CPath\nprojects, there are no well-defined guidelines or best practices on how\nannotations should be carried out. In this paper, we address this shortcoming\nby presenting the experience and best practices acquired during the execution\nof a large-scale annotation exercise involving a multidisciplinary team of\npathologists, ML experts and researchers as part of the Pathology image data\nLake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a\nreal-world case study along with examples of different types of annotations,\ndiagnostic algorithm, annotation data dictionary and annotation constructs. The\nanalyses reported in this work highlight best practice recommendations that can\nbe used as annotation guidelines over the lifecycle of a CPath project.",
          "link": "http://arxiv.org/abs/2106.13689",
          "publishedOn": "2021-06-28T01:57:56.412Z",
          "wordCount": 729,
          "title": "Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations. (arXiv:2106.13689v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallouedec_Q/0/1/0/all/0/1\">Quentin Gallou&#xe9;dec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazin_N/0/1/0/all/0/1\">Nicolas Cazin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1\">Emmanuel Dellandr&#xe9;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "This technical report presents panda-gym, a set Reinforcement Learning (RL)\nenvironments for the Franka Emika Panda robot integrated with OpenAI Gym. Five\ntasks are included: reach, push, slide, pick & place and stack. They all follow\na Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To\nfoster open-research, we chose to use the open-source physics engine PyBullet.\nThe implementation chosen for this package allows to define very easily new\ntasks or new robots. This report also presents a baseline of results obtained\nwith state-of-the-art model-free off-policy algorithms. panda-gym is\nopen-source at https://github.com/qgallouedec/panda-gym.",
          "link": "http://arxiv.org/abs/2106.13687",
          "publishedOn": "2021-06-28T01:57:56.281Z",
          "wordCount": 535,
          "title": "Multi-Goal Reinforcement Learning environments for simulated Franka Emika Panda robot. (arXiv:2106.13687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alguacil_A/0/1/0/all/0/1\">Antonio Alguacil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_G/0/1/0/all/0/1\">Gon&#xe7;alves Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauerheim_M/0/1/0/all/0/1\">Michael Bauerheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1\">Marc C. Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_S/0/1/0/all/0/1\">St&#xe9;phane Moreau</a>",
          "description": "Accurate modeling of boundary conditions is crucial in computational physics.\nThe ever increasing use of neural networks as surrogates for physics-related\nproblems calls for an improved understanding of boundary condition treatment,\nand its influence on the network accuracy. In this paper, several strategies to\nimpose boundary conditions (namely padding, improved spatial context, and\nexplicit encoding of physical boundaries) are investigated in the context of\nfully convolutional networks applied to recurrent tasks. These strategies are\nevaluated on two spatio-temporal evolving problems modeled by partial\ndifferential equations: the 2D propagation of acoustic waves (hyperbolic PDE)\nand the heat equation (parabolic PDE). Results reveal a high sensitivity of\nboth accuracy and stability on the boundary implementation in such recurrent\ntasks. It is then demonstrated that the choice of the optimal padding strategy\nis directly linked to the data semantics. Furthermore, the inclusion of\nadditional input spatial context or explicit physics-based rules allows a\nbetter handling of boundaries in particular for large number of recurrences,\nresulting in more robust and stable neural networks, while facilitating the\ndesign and versatility of such networks.",
          "link": "http://arxiv.org/abs/2106.11160",
          "publishedOn": "2021-06-28T01:57:56.274Z",
          "wordCount": 651,
          "title": "Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics. (arXiv:2106.11160v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13551",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_G/0/1/0/all/0/1\">Gabriel Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amor_R/0/1/0/all/0/1\">Roc&#xed;o del Amor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colomer_A/0/1/0/all/0/1\">Adri&#xe1;n Colomer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verdu_Monedero_R/0/1/0/all/0/1\">Rafael Verd&#xfa;-Monedero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morales_Sanchez_J/0/1/0/all/0/1\">Juan Morales-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>",
          "description": "Glaucoma is one of the leading causes of blindness worldwide and Optical\nCoherence Tomography (OCT) is the quintessential imaging technique for its\ndetection. Unlike most of the state-of-the-art studies focused on glaucoma\ndetection, in this paper, we propose, for the first time, a novel framework for\nglaucoma grading using raw circumpapillary B-scans. In particular, we set out a\nnew OCT-based hybrid network which combines hand-driven and deep learning\nalgorithms. An OCT-specific descriptor is proposed to extract hand-crafted\nfeatures related to the retinal nerve fibre layer (RNFL). In parallel, an\ninnovative CNN is developed using skip-connections to include tailored residual\nand attention modules to refine the automatic features of the latent space. The\nproposed architecture is used as a backbone to conduct a novel few-shot\nlearning based on static and dynamic prototypical networks. The k-shot paradigm\nis redefined giving rise to a supervised end-to-end system which provides\nsubstantial improvements discriminating between healthy, early and advanced\nglaucoma samples. The training and evaluation processes of the dynamic\nprototypical network are addressed from two fused databases acquired via\nHeidelberg Spectralis system. Validation and testing results reach a\ncategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.\nBesides, the high performance reported by the proposed model for glaucoma\ndetection deserves a special mention. The findings from the class activation\nmaps are directly in line with the clinicians' opinion since the heatmaps\npointed out the RNFL as the most relevant structure for glaucoma diagnosis.",
          "link": "http://arxiv.org/abs/2106.13551",
          "publishedOn": "2021-06-28T01:57:56.261Z",
          "wordCount": 708,
          "title": "Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks. (arXiv:2106.13551v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13318",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Frigo_O/0/1/0/all/0/1\">Oriel Frigo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>",
          "description": "Deep learning based molecular graph generation and optimization has recently\nbeen attracting attention due to its great potential for de novo drug design.\nOn the one hand, recent models are able to efficiently learn a given graph\ndistribution, and many approaches have proven very effective to produce a\nmolecule that maximizes a given score. On the other hand, it was shown by\nprevious studies that generated optimized molecules are often unrealistic, even\nwith the inclusion of mechanics to enforce similarity to a dataset of real drug\nmolecules. In this work we use a hybrid approach, where the dataset\ndistribution is learned using an autoregressive model while the score\noptimization is done using the Metropolis algorithm, biased toward the learned\ndistribution. We show that the resulting method, that we call learned realism\nsampling (LRS), produces empirically more realistic molecules and outperforms\nall recent baselines in the task of molecule optimization with similarity\nconstraints.",
          "link": "http://arxiv.org/abs/2106.13318",
          "publishedOn": "2021-06-28T01:57:56.255Z",
          "wordCount": 602,
          "title": "Realistic molecule optimization on a learned graph manifold. (arXiv:2106.13318v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We consider a binary classification problem when the data comes from a\nmixture of two isotropic distributions satisfying concentration and\nanti-concentration properties enjoyed by log-concave distributions among\nothers. We show that there exists a universal constant $C_{\\mathrm{err}}>0$\nsuch that if a pseudolabeler $\\boldsymbol{\\beta}_{\\mathrm{pl}}$ can achieve\nclassification error at most $C_{\\mathrm{err}}$, then for any $\\varepsilon>0$,\nan iterative self-training algorithm initialized at $\\boldsymbol{\\beta}_0 :=\n\\boldsymbol{\\beta}_{\\mathrm{pl}}$ using pseudolabels $\\hat y =\n\\mathrm{sgn}(\\langle \\boldsymbol{\\beta}_t, \\mathbf{x}\\rangle)$ and using at\nmost $\\tilde O(d/\\varepsilon^2)$ unlabeled examples suffices to learn the\nBayes-optimal classifier up to $\\varepsilon$ error, where $d$ is the ambient\ndimension. That is, self-training converts weak learners to strong learners\nusing only unlabeled examples. We additionally show that by running gradient\ndescent on the logistic loss one can obtain a pseudolabeler\n$\\boldsymbol{\\beta}_{\\mathrm{pl}}$ with classification error $C_{\\mathrm{err}}$\nusing only $O(d)$ labeled examples (i.e., independent of $\\varepsilon$).\nTogether our results imply that mixture models can be learned to within\n$\\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled\nexamples and $\\tilde O(d/\\varepsilon^2)$ unlabeled examples by way of a\nsemi-supervised self-training algorithm.",
          "link": "http://arxiv.org/abs/2106.13805",
          "publishedOn": "2021-06-28T01:57:56.249Z",
          "wordCount": 622,
          "title": "Self-training Converts Weak Learners to Strong Learners in Mixture Models. (arXiv:2106.13805v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1\">Alex Oesterling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Angikar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haoyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1\">Rui Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baig_Y/0/1/0/all/0/1\">Yasa Baig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1\">Lesia Semenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>",
          "description": "We present our entry into the 2021 3C Shared Task Citation Context\nClassification based on Purpose competition. The goal of the competition is to\nclassify a citation in a scientific article based on its purpose. This task is\nimportant because it could potentially lead to more comprehensive ways of\nsummarizing the purpose and uses of scientific articles, but it is also\ndifficult, mainly due to the limited amount of available training data in which\nthe purposes of each citation have been hand-labeled, along with the\nsubjectivity of these labels. Our entry in the competition is a multi-task\nmodel that combines multiple modules designed to handle the problem from\ndifferent perspectives, including hand-generated linguistic features, TF-IDF\nfeatures, and an LSTM-with-attention model. We also provide an ablation study\nand feature analysis whose insights could lead to future work.",
          "link": "http://arxiv.org/abs/2106.13275",
          "publishedOn": "2021-06-28T01:57:56.242Z",
          "wordCount": 579,
          "title": "Multitask Learning for Citation Purpose Classification. (arXiv:2106.13275v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1\">Peter Varsanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1\">Rachit Pabreja</a>",
          "description": "While predictive policing has become increasingly common in assisting with\ndecisions in the criminal justice system, the use of these results is still\ncontroversial. Some software based on deep learning lacks accuracy (e.g., in\nF-1), and many decision processes are not transparent causing doubt about\ndecision bias, such as perceived racial, age, and gender disparities. This\npaper addresses bias issues with post-hoc explanations to provide a trustable\nprediction of whether a person will receive future criminal charges given one's\nprevious criminal records by learning temporal behavior patterns over twenty\nyears. Bi-LSTM relieves the vanishing gradient problem, and attentional\nmechanisms allows learning and interpretation of feature importance. Our\napproach shows consistent and reliable prediction precision and recall on a\nreal-life dataset. Our analysis of the importance of each input feature shows\nthe critical causal impact on decision-making, suggesting that criminal\nhistories are statistically significant factors, while identifiers, such as\nrace, gender, and age, are not. Finally, our algorithm indicates that a suspect\ntends to gradually rather than suddenly increase crime severity level over\ntime.",
          "link": "http://arxiv.org/abs/2106.13456",
          "publishedOn": "2021-06-28T01:57:56.217Z",
          "wordCount": 610,
          "title": "Deep Interpretable Criminal Charge Prediction and Algorithmic Bias. (arXiv:2106.13456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09474",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1\">Joseph Aylett-Bullock</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1\">Simon Badger</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1\">Ryan Moodie</a>",
          "description": "Machine learning technology has the potential to dramatically optimise event\ngeneration and simulations. We continue to investigate the use of neural\nnetworks to approximate matrix elements for high-multiplicity scattering\nprocesses. We focus on the case of loop-induced diphoton production through\ngluon fusion and develop a realistic simulation method that can be applied to\nhadron collider observables. Neural networks are trained using the one-loop\namplitudes implemented in the NJet C++ library and interfaced to the Sherpa\nMonte Carlo event generator where we perform a detailed study for $2\\to3$ and\n$2\\to4$ scattering problems. We also consider how the trained networks perform\nwhen varying the kinematic cuts effecting the phase space and the reliability\nof the neural network simulations.",
          "link": "http://arxiv.org/abs/2106.09474",
          "publishedOn": "2021-06-28T01:57:56.202Z",
          "wordCount": 588,
          "title": "Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sadia Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urner_R/0/1/0/all/0/1\">Ruth Urner</a>",
          "description": "The phenomenon of adversarial examples in deep learning models has caused\nsubstantial concern over their reliability. While many deep neural networks\nhave shown impressive performance in terms of predictive accuracy, it has been\nshown that in many instances an imperceptible perturbation can falsely flip the\nnetwork's prediction. Most research has then focused on developing defenses\nagainst adversarial attacks or learning under a worst-case adversarial loss. In\nthis work, we take a step back and aim to provide a framework for determining\nwhether a model's label change under small perturbation is justified (and when\nit is not). We carefully argue that adversarial robustness should be defined as\na locally adaptive measure complying with the underlying distribution. We then\nsuggest a definition for an adaptive robust loss, derive an empirical version\nof it, and develop a resulting data-augmentation framework. We prove that our\nadaptive data-augmentation maintains consistency of 1-nearest neighbor\nclassification under deterministic labels and provide illustrative empirical\nevaluations.",
          "link": "http://arxiv.org/abs/2106.13326",
          "publishedOn": "2021-06-28T01:57:56.195Z",
          "wordCount": 598,
          "title": "On the (Un-)Avoidability of Adversarial Examples. (arXiv:2106.13326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1\">Raghu Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jessica Lizeth Borja Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1\">Suresh Guttikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1\">Andr&#xe9; Biedenkapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1\">Jan Ole von Hartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "We present \\emph{MDP Playground}, an efficient testbed for Reinforcement\nLearning (RL) agents with \\textit{orthogonal} dimensions that can be controlled\nindependently to challenge agents in different ways and obtain varying degrees\nof hardness in generated environments. We consider and allow control over a\nwide variety of dimensions, including \\textit{delayed rewards},\n\\textit{rewardable sequences}, \\textit{density of rewards},\n\\textit{stochasticity}, \\textit{image representations}, \\textit{irrelevant\nfeatures}, \\textit{time unit}, \\textit{action range} and more. We define a\nparameterised collection of fast-to-run toy environments in \\textit{OpenAI Gym}\nby varying these dimensions and propose to use these for the initial design and\ndevelopment of agents. We also provide wrappers that inject these dimensions\ninto complex environments from \\textit{Atari} and \\textit{Mujoco} to allow for\nevaluating agent robustness. We further provide various example use-cases and\ninstructions on how to use \\textit{MDP Playground} to design and debug agents.\nWe believe that \\textit{MDP Playground} is a valuable testbed for researchers\ndesigning new, adaptive and intelligent RL agents and those wanting to unit\ntest their agents.",
          "link": "http://arxiv.org/abs/1909.07750",
          "publishedOn": "2021-06-28T01:57:56.178Z",
          "wordCount": 673,
          "title": "MDP Playground: A Design and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shulby_C/0/1/0/all/0/1\">Christopher Shulby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_F/0/1/0/all/0/1\">Frederico Santos de Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Aluisio</a>",
          "description": "Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.",
          "link": "http://arxiv.org/abs/2005.05144",
          "publishedOn": "2021-06-28T01:57:56.172Z",
          "wordCount": 620,
          "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese. (arXiv:2005.05144v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "Hypergraphs are used to model higher-order interactions amongst agents and\nthere exist many practically relevant instances of hypergraph datasets. To\nenable efficient processing of hypergraph-structured data, several hypergraph\nneural network platforms have been proposed for learning hypergraph properties\nand structure, with a special focus on node classification. However, almost all\nexisting methods use heuristic propagation rules and offer suboptimal\nperformance on many datasets. We propose AllSet, a new hypergraph neural\nnetwork paradigm that represents a highly general framework for (hyper)graph\nneural networks and for the first time implements hypergraph neural network\nlayers as compositions of two multiset functions that can be efficiently\nlearned for each task and each dataset. Furthermore, AllSet draws on new\nconnections between hypergraph neural networks and recent advances in deep\nlearning of multiset functions. In particular, the proposed architecture\nutilizes Deep Sets and Set Transformer architectures that allow for significant\nmodeling flexibility and offer high expressive power. To evaluate the\nperformance of AllSet, we conduct the most extensive experiments to date\ninvolving ten known benchmarking datasets and three newly curated datasets that\nrepresent significant challenges for hypergraph node classification. The\nresults demonstrate that AllSet has the unique ability to consistently either\nmatch or outperform all other hypergraph neural networks across the tested\ndatasets. Our implementation and dataset will be released upon acceptance.",
          "link": "http://arxiv.org/abs/2106.13264",
          "publishedOn": "2021-06-28T01:57:56.165Z",
          "wordCount": 655,
          "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks. (arXiv:2106.13264v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustering federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.",
          "link": "http://arxiv.org/abs/2106.13423",
          "publishedOn": "2021-06-28T01:57:56.158Z",
          "wordCount": 664,
          "title": "Federated Graph Classification over Non-IID Graphs. (arXiv:2106.13423v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1\">Noah Golowich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We consider the problem of online classification under a privacy constraint.\nIn this setting a learner observes sequentially a stream of labelled examples\n$(x_t, y_t)$, for $1 \\leq t \\leq T$, and returns at each iteration $t$ a\nhypothesis $h_t$ which is used to predict the label of each new example $x_t$.\nThe learner's performance is measured by her regret against a known hypothesis\nclass $\\mathcal{H}$. We require that the algorithm satisfies the following\nprivacy constraint: the sequence $h_1, \\ldots, h_T$ of hypotheses output by the\nalgorithm needs to be an $(\\epsilon, \\delta)$-differentially private function\nof the whole input sequence $(x_1, y_1), \\ldots, (x_T, y_T)$. We provide the\nfirst non-trivial regret bound for the realizable setting. Specifically, we\nshow that if the class $\\mathcal{H}$ has constant Littlestone dimension then,\ngiven an oblivious sequence of labelled examples, there is a private learner\nthat makes in expectation at most $O(\\log T)$ mistakes -- comparable to the\noptimal mistake bound in the non-private case, up to a logarithmic factor.\nMoreover, for general values of the Littlestone dimension $d$, the same mistake\nbound holds but with a doubly-exponential in $d$ factor. A recent line of work\nhas demonstrated a strong connection between classes that are online learnable\nand those that are differentially-private learnable. Our results strengthen\nthis connection and show that an online learning algorithm can in fact be\ndirectly privatized (in the realizable setting). We also discuss an adaptive\nsetting and provide a sublinear regret bound of $O(\\sqrt{T})$.",
          "link": "http://arxiv.org/abs/2106.13513",
          "publishedOn": "2021-06-28T01:57:56.152Z",
          "wordCount": 679,
          "title": "Littlestone Classes are Privately Online Learnable. (arXiv:2106.13513v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nooraiepour_A/0/1/0/all/0/1\">Alireza Nooraiepour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1\">Waheed U. Bajwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandayam_N/0/1/0/all/0/1\">Narayan B. Mandayam</a>",
          "description": "The fundamental task of classification given a limited number of training\ndata samples is considered for physical systems with known parametric\nstatistical models. The standalone learning-based and statistical model-based\nclassifiers face major challenges towards the fulfillment of the classification\ntask using a small training set. Specifically, classifiers that solely rely on\nthe physics-based statistical models usually suffer from their inability to\nproperly tune the underlying unobservable parameters, which leads to a\nmismatched representation of the system's behaviors. Learning-based\nclassifiers, on the other hand, typically rely on a large number of training\ndata from the underlying physical process, which might not be feasible in most\npractical scenarios. In this paper, a hybrid classification method -- termed\nHyPhyLearn -- is proposed that exploits both the physics-based statistical\nmodels and the learning-based classifiers. The proposed solution is based on\nthe conjecture that HyPhyLearn would alleviate the challenges associated with\nthe individual approaches of learning-based and statistical model-based\nclassifiers by fusing their respective strengths. The proposed hybrid approach\nfirst estimates the unobservable model parameters using the available\n(suboptimal) statistical estimation procedures, and subsequently use the\nphysics-based statistical models to generate synthetic data. Then, the training\ndata samples are incorporated with the synthetic data in a learning-based\nclassifier that is based on domain-adversarial training of neural networks.\nSpecifically, in order to address the mismatch problem, the classifier learns a\nmapping from the training data and the synthetic data to a common feature\nspace. Simultaneously, the classifier is trained to find discriminative\nfeatures within this space in order to fulfill the classification task.",
          "link": "http://arxiv.org/abs/2106.13436",
          "publishedOn": "2021-06-28T01:57:56.131Z",
          "wordCount": 715,
          "title": "A hybrid model-based and learning-based approach for classification using limited number of training samples. (arXiv:2106.13436v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13274",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1\">Dmitry Ivlev</a>",
          "description": "Purpose of this research is to forecast the development of sand bodies in\nproductive sediments based on well log data and seismic attributes. The object\nof the study is the productive intervals of Achimov sedimentary complex in the\npart of oil field located in Western Siberia. The research shows a\ntechnological stack of machine learning algorithms, methods for enriching the\nsource data with synthetic ones and algorithms for creating new features. The\nresult was the model of regression relationship between the values of natural\nradioactivity of rocks and seismic wave field attributes with an acceptable\nprediction quality. Acceptable quality of the forecast is confirmed both by\nmodel cross validation, and by the data obtained following the results of new\nwell.",
          "link": "http://arxiv.org/abs/2106.13274",
          "publishedOn": "2021-06-28T01:57:56.124Z",
          "wordCount": 587,
          "title": "Prediction of geophysical properties of rocks on rare well data and attributes of seismic waves by machine learning methods on the example of the Achimov formation. (arXiv:2106.13274v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rachit Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_T/0/1/0/all/0/1\">Tanmay Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1\">Sandeep Kumar Shukla</a>",
          "description": "Smart Contracts (SCs) in Ethereum can automate tasks and provide different\nfunctionalities to a user. Such automation is enabled by the `Turing-complete'\nnature of the programming language (Solidity) in which SCs are written. This\nalso opens up different vulnerabilities and bugs in SCs that malicious actors\nexploit to carry out malicious or illegal activities on the cryptocurrency\nplatform. In this work, we study the correlation between malicious activities\nand the vulnerabilities present in SCs and find that some malicious activities\nare correlated with certain types of vulnerabilities. We then develop and study\nthe feasibility of a scoring mechanism that corresponds to the severity of the\nvulnerabilities present in SCs to determine if it is a relevant feature to\nidentify suspicious SCs. We analyze the utility of severity score towards\ndetection of suspicious SCs using unsupervised machine learning (ML) algorithms\nacross different temporal granularities and identify behavioral changes. In our\nexperiments with on-chain SCs, we were able to find a total of 1094 benign SCs\nacross different granularities which behave similar to malicious SCs, with the\ninclusion of the smart contract vulnerability scores in the feature set.",
          "link": "http://arxiv.org/abs/2106.13422",
          "publishedOn": "2021-06-28T01:57:56.117Z",
          "wordCount": 636,
          "title": "Vulnerability and Transaction behavior based detection of Malicious Smart Contracts. (arXiv:2106.13422v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
          "link": "http://arxiv.org/abs/2007.15779",
          "publishedOn": "2021-06-28T01:57:56.105Z",
          "wordCount": 708,
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Katie Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1\">Gregory Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Deep reinforcement learning algorithms require large and diverse datasets in\norder to learn successful policies for perception-based mobile navigation.\nHowever, gathering such datasets with a single robot can be prohibitively\nexpensive. Collecting data with multiple different robotic platforms with\npossibly different dynamics is a more scalable approach to large-scale data\ncollection. But how can deep reinforcement learning algorithms leverage such\nheterogeneous datasets? In this work, we propose a deep reinforcement learning\nalgorithm with hierarchically integrated models (HInt). At training time, HInt\nlearns separate perception and dynamics models, and at test time, HInt\nintegrates the two models in a hierarchical manner and plans actions with the\nintegrated model. This method of planning with hierarchically integrated models\nallows the algorithm to train on datasets gathered by a variety of different\nplatforms, while respecting the physical capabilities of the deployment robot\nat test time. Our mobile navigation experiments show that HInt outperforms\nconventional hierarchical policies and single-source approaches.",
          "link": "http://arxiv.org/abs/2106.13280",
          "publishedOn": "2021-06-28T01:57:56.092Z",
          "wordCount": 587,
          "title": "Multi-Robot Deep Reinforcement Learning for Mobile Navigation. (arXiv:2106.13280v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Recently, transformers have shown great superiority in solving computer\nvision tasks by modeling images as a sequence of manually-split patches with\nself-attention mechanism. However, current architectures of vision transformers\n(ViTs) are simply inherited from natural language processing (NLP) tasks and\nhave not been sufficiently investigated and optimized. In this paper, we make a\nfurther step by examining the intrinsic structure of transformers for vision\ntasks and propose an architecture search method, dubbed ViTAS, to search for\nthe optimal architecture with similar hardware budgets. Concretely, we design a\nnew effective yet efficient weight sharing paradigm for ViTs, such that\narchitectures with different token embedding, sequence size, number of heads,\nwidth, and depth can be derived from a single super-transformer. Moreover, to\ncater for the variance of distinct architectures, we introduce \\textit{private}\nclass token and self-attention maps in the super-transformer. In addition, to\nadapt the searching for different budgets, we propose to search the sampling\nprobability of identity operation. Experimental results show that our ViTAS\nattains excellent results compared to existing pure transformer architectures.\nFor example, with $1.3$G FLOPs budget, our searched architecture achieves\n$74.7\\%$ top-$1$ accuracy on ImageNet and is $2.5\\%$ superior than the current\nbaseline ViT architecture. Code is available at\n\\url{https://github.com/xiusu/ViTAS}.",
          "link": "http://arxiv.org/abs/2106.13700",
          "publishedOn": "2021-06-28T01:57:56.072Z",
          "wordCount": 646,
          "title": "Vision Transformer Architecture Search. (arXiv:2106.13700v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1\">George Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">Jasmeet Judge</a>",
          "description": "Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, Domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%\nmore weeks with highest cosine similarity than the other NNs during test years.\nThe DgNN and Sequential NN were more robust during periods of abnormal crop\nprogress, though estimating the Silking-Grainfill transition was difficult for\nall methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.",
          "link": "http://arxiv.org/abs/2106.13323",
          "publishedOn": "2021-06-28T01:57:56.059Z",
          "wordCount": 679,
          "title": "Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rui Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekhor_S/0/1/0/all/0/1\">Shlomo Bekhor</a>",
          "description": "This paper derives the generalized extreme value (GEV) model with implicit\navailability/perception (IAP) of alternatives and proposes a variational\nautoencoder (VAE) approach for choice set generation and implicit perception of\nalternatives. Specifically, the cross-nested logit (CNL) model with IAP is\nderived as an example of IAP-GEV models. The VAE approach is adapted to model\nthe choice set generation process, in which the likelihood of perceiving chosen\nalternatives in the choice set is maximized. The VAE approach for route choice\nset generation is exemplified using a real dataset. IAP- CNL model estimated\nhas the best performance in terms of goodness-of-fit and prediction\nperformance, compared to multinomial logit models and conventional choice set\ngeneration methods.",
          "link": "http://arxiv.org/abs/2106.13319",
          "publishedOn": "2021-06-28T01:57:56.042Z",
          "wordCount": 568,
          "title": "A variational autoencoder approach for choice set generation and implicit perception of alternatives in choice modeling. (arXiv:2106.13319v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Shan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masegosa_A/0/1/0/all/0/1\">Andr&#xe9;s R. Masegosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1\">Stephan S. Lorenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1\">Yevgeny Seldin</a>",
          "description": "We present a new second-order oracle bound for the expected risk of a\nweighted majority vote. The bound is based on a novel parametric form of the\nChebyshev-Cantelli inequality (a.k.a.\\ one-sided Chebyshev's), which is\namenable to efficient minimization. The new form resolves the optimization\nchallenge faced by prior oracle bounds based on the Chebyshev-Cantelli\ninequality, the C-bounds [Germain et al., 2015], and, at the same time, it\nimproves on the oracle bound based on second order Markov's inequality\nintroduced by Masegosa et al. [2020]. We also derive the PAC-Bayes-Bennett\ninequality, which we use for empirical estimation of the oracle bound. The\nPAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality by\nSeldin et al. [2012]. We provide an empirical evaluation demonstrating that the\nnew bounds can improve on the work by Masegosa et al. [2020]. Both the\nparametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett\ninequality may be of independent interest for the study of concentration of\nmeasure in other domains.",
          "link": "http://arxiv.org/abs/2106.13624",
          "publishedOn": "2021-06-28T01:57:56.035Z",
          "wordCount": 606,
          "title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote. (arXiv:2106.13624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingyang Hong</a>",
          "description": "This paper proposes a multi-task learning network with phoneme-aware and\nchannel-wise attentive learning strategies for text-dependent Speaker\nVerification (SV). In the proposed structure, the frame-level multi-task\nlearning along with the segment-level adversarial learning is adopted for\nspeaker embedding extraction. The phoneme-aware attentive pooling is exploited\non frame-level features in the main network for speaker classifier, with the\ncorresponding posterior probability for the phoneme distribution in the\nauxiliary subnet. Further, the introduction of Squeeze and Excitation\n(SE-block) performs dynamic channel-wise feature recalibration, which improves\nthe representational ability. The proposed method exploits speaker\nidiosyncrasies associated with pass-phrases, and is further improved by the\nphoneme-aware attentive pooling and SE-block from temporal and channel-wise\naspects, respectively. The experiments conducted on RSR2015 Part 1 database\nconfirm that the proposed system achieves outstanding results for textdependent\nSV.",
          "link": "http://arxiv.org/abs/2106.13514",
          "publishedOn": "2021-06-28T01:57:56.011Z",
          "wordCount": 572,
          "title": "Phoneme-aware and Channel-wise Attentive Learning for Text DependentSpeaker Verification. (arXiv:2106.13514v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahinpei_A/0/1/0/all/0/1\">Anita Mahinpei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Justin Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lage_I/0/1/0/all/0/1\">Isaac Lage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1\">Finale Doshi-Velez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weiwei Pan</a>",
          "description": "Machine learning models that incorporate concept learning as an intermediate\nstep in their decision making process can match the performance of black-box\npredictive models while retaining the ability to explain outcomes in human\nunderstandable terms. However, we demonstrate that the concept representations\nlearned by these models encode information beyond the pre-defined concepts, and\nthat natural mitigation strategies do not fully work, rendering the\ninterpretation of the downstream prediction misleading. We describe the\nmechanism underlying the information leakage and suggest recourse for\nmitigating its effects.",
          "link": "http://arxiv.org/abs/2106.13314",
          "publishedOn": "2021-06-28T01:57:56.005Z",
          "wordCount": 519,
          "title": "Promises and Pitfalls of Black-Box Concept Learning Models. (arXiv:2106.13314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wanqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lizhong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>",
          "description": "Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.",
          "link": "http://arxiv.org/abs/2106.13393",
          "publishedOn": "2021-06-28T01:57:55.995Z",
          "wordCount": 702,
          "title": "Interpreting Depression From Question-wise Long-term Video Recording of SDS Evaluation. (arXiv:2106.13393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyde_N/0/1/0/all/0/1\">Nicholas Gyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1\">Ashish Kapoor</a>",
          "description": "The ability to perform causal and counterfactual reasoning are central\nproperties of human intelligence. Decision-making systems that can perform\nthese types of reasoning have the potential to be more generalizable and\ninterpretable. Simulations have helped advance the state-of-the-art in this\ndomain, by providing the ability to systematically vary parameters (e.g.,\nconfounders) and generate examples of the outcomes in the case of\ncounterfactual scenarios. However, simulating complex temporal causal events in\nmulti-agent scenarios, such as those that exist in driving and vehicle\nnavigation, is challenging. To help address this, we present a high-fidelity\nsimulation environment that is designed for developing algorithms for causal\ndiscovery and counterfactual reasoning in the safety-critical context. A core\ncomponent of our work is to introduce \\textit{agency}, such that it is simple\nto define and create complex scenarios using high-level definitions. The\nvehicles then operate with agency to complete these objectives, meaning\nlow-level behaviors need only be controlled if necessary. We perform\nexperiments with three state-of-the-art methods to create baselines and\nhighlight the affordances of this environment. Finally, we highlight challenges\nand opportunities for future work.",
          "link": "http://arxiv.org/abs/2106.13364",
          "publishedOn": "2021-06-28T01:57:55.987Z",
          "wordCount": 636,
          "title": "CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning. (arXiv:2106.13364v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13434",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1\">Reka A. Kovacs</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1\">Oktay Gunluk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1\">Raphael A. Hauser</a>",
          "description": "Binary matrix factorisation is an essential tool for identifying discrete\npatterns in binary data. In this paper we consider the rank-k binary matrix\nfactorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m\nbinary matrix X with possibly missing entries and need to find two binary\nmatrices A and B of dimension n x k and k x m respectively, which minimise the\ndistance between X and the Boolean product of A and B in the squared Frobenius\ndistance. We present a compact and two exponential size integer programs (IPs)\nfor k-BMF and show that the compact IP has a weak LP relaxation, while the\nexponential size LPs have a stronger equivalent LP relaxation. We introduce a\nnew objective function, which differs from the traditional squared Frobenius\nobjective in attributing a weight to zero entries of the input matrix that is\nproportional to the number of times the zero is erroneously covered in a rank-k\nfactorisation. For one of the exponential size IPs we describe a computational\napproach based on column generation. Experimental results on synthetic and real\nword datasets suggest that our integer programming approach is competitive\nagainst available methods for k-BMF and provides accurate low-error\nfactorisations.",
          "link": "http://arxiv.org/abs/2106.13434",
          "publishedOn": "2021-06-28T01:57:55.979Z",
          "wordCount": 643,
          "title": "Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jessica Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "In situations where explanations of black-box models may be useful, the\nfairness of the black-box is also often a relevant concern. However, the link\nbetween the fairness of the black-box model and the behavior of explanations\nfor the black-box is unclear. We focus on explanations applied to tabular\ndatasets, suggesting that explanations do not necessarily preserve the fairness\nproperties of the black-box algorithm. In other words, explanation algorithms\ncan ignore or obscure critical relevant properties, creating incorrect or\nmisleading explanations. More broadly, we propose future research directions\nfor evaluating and generating explanations such that they are informative and\nrelevant from a fairness perspective.",
          "link": "http://arxiv.org/abs/2106.13346",
          "publishedOn": "2021-06-28T01:57:55.961Z",
          "wordCount": 558,
          "title": "What will it take to generate fairness-preserving explanations?. (arXiv:2106.13346v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>",
          "description": "One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.13272",
          "publishedOn": "2021-06-28T01:57:55.949Z",
          "wordCount": 699,
          "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers. (arXiv:2106.13272v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1\">Ivana Bala&#x17e;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>",
          "description": "Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.",
          "link": "http://arxiv.org/abs/2106.13353",
          "publishedOn": "2021-06-28T01:57:55.943Z",
          "wordCount": 592,
          "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. (arXiv:2106.13353v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suryanarayanan_P/0/1/0/all/0/1\">Parthasarathy Suryanarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Prithwish Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1\">Piyush Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bore_K/0/1/0/all/0/1\">Kibichii Bore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogallo_W/0/1/0/all/0/1\">William Ogallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rachita Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghalwash_M/0/1/0/all/0/1\">Mohamed Ghalwash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buleje_I/0/1/0/all/0/1\">Italo Buleje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1\">Sekou Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahatma_S/0/1/0/all/0/1\">Shilpa Mahatma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1\">Pablo Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianying Hu</a>",
          "description": "In this work we introduce Disease Progression Modeling workbench 360 (DPM360)\nopensource clinical informatics framework for collaborative research and\ndelivery of healthcare AI. DPM360, when fully developed, will manage the entire\nmodeling life cycle, from data analysis (e.g., cohort identification) to\nmachine learning algorithm development and prototyping. DPM360 augments the\nadvantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS)\nprovided by the widely-adopted OHDSI initiative with a powerful machine\nlearning training framework, and a mechanism for rapid prototyping through\nautomatic deployment of models as containerized services to a cloud\nenvironment.",
          "link": "http://arxiv.org/abs/2106.13265",
          "publishedOn": "2021-06-28T01:57:55.919Z",
          "wordCount": 544,
          "title": "Disease Progression Modeling Workbench 360. (arXiv:2106.13265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1\">Hadi Mansourifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1\">Dana Alsagheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1\">Reza Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weidong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>",
          "description": "With high prevalence of offensive language against the minorities in social\nmedia, counter hate speech generation is considered as an automatic way to\ntackle this challenge. The counter hate speeches are supposed to appear as a\nthird voice to educate people and keep the social red lines bold without\nlimiting the freedom of speech principles. The counter hate speech generation\nis based on the optimistic assumption that, any attempt to intervene the hate\nspeeches in social media can play a positive role in this context. Beyond that,\nprevious works ignored to investigate the sequence of comments before and after\ncounter speech. To the best of our knowledge, no attempt has been made to\nmeasure the counter hate speech impact from statistical point of view. In this\npaper, we take the first step in this direction by measuring the counter hate\nspeech impact on the next comments in terms of Google Perspective Scores.\nFurthermore, our experiments show that, counter hate speech can cause negative\nimpacts, a phenomena which is called aggression in social media.",
          "link": "http://arxiv.org/abs/2106.13238",
          "publishedOn": "2021-06-28T01:57:55.907Z",
          "wordCount": 617,
          "title": "Hate Speech Detection in Clubhouse. (arXiv:2106.13238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashby_M/0/1/0/all/0/1\">Michael Hunter Ashby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilbrey_J/0/1/0/all/0/1\">Jenna A. Bilbrey</a>",
          "description": "We apply a temporal edge prediction model for weighted dynamic graphs to\npredict time-dependent changes in molecular structure. Each molecule is\nrepresented as a complete graph in which each atom is a vertex and all vertex\npairs are connected by an edge weighted by the Euclidean distance between atom\npairs. We ingest a sequence of complete molecular graphs into a dynamic graph\nneural network (GNN) to predict the graph at the next time step. Our dynamic\nGNN predicts atom-to-atom distances with a mean absolute error of 0.017 \\r{A},\nwhich is considered ``chemically accurate'' for molecular simulations. We also\nexplored the transferability of a trained network to new molecular systems and\nfound that finetuning with less than 10% of the total trajectory provides a\nmean absolute error of the same order of magnitude as that when training from\nscratch on the full molecular trajectory.",
          "link": "http://arxiv.org/abs/2106.13277",
          "publishedOn": "2021-06-28T01:57:55.901Z",
          "wordCount": 590,
          "title": "Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks. (arXiv:2106.13277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spieler_J/0/1/0/all/0/1\">Jonathan Spieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1\">Nico Potyka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1\">Steffen Staab</a>",
          "description": "Gradual argumentation frameworks represent arguments and their relationships\nin a weighted graph. Their graphical structure and intuitive semantics makes\nthem a potentially interesting tool for interpretable machine learning. It has\nbeen noted recently that their mechanics are closely related to neural\nnetworks, which allows learning their weights from data by standard deep\nlearning frameworks. As a first proof of concept, we propose a genetic\nalgorithm to simultaneously learn the structure of argumentative classification\nmodels. To obtain a well interpretable model, the fitness function balances\nsparseness and accuracy of the classifier. We discuss our algorithm and present\nfirst experimental results on standard benchmarks from the UCI machine learning\nrepository. Our prototype learns argumentative classification models that are\ncomparable to decision trees in terms of learning performance and\ninterpretability.",
          "link": "http://arxiv.org/abs/2106.13585",
          "publishedOn": "2021-06-28T01:57:55.884Z",
          "wordCount": 558,
          "title": "Learning Gradual Argumentation Frameworks using Genetic Algorithms. (arXiv:2106.13585v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13706",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hagen_A/0/1/0/all/0/1\">Alex Hagen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jackson_S/0/1/0/all/0/1\">Shane Jackson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kahn_J/0/1/0/all/0/1\">James Kahn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Strube_J/0/1/0/all/0/1\">Jan Strube</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Haide_I/0/1/0/all/0/1\">Isabel Haide</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pazdernik_K/0/1/0/all/0/1\">Karl Pazdernik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hainje_C/0/1/0/all/0/1\">Connor Hainje</a>",
          "description": "Statistical testing is widespread and critical for a variety of scientific\ndisciplines. The advent of machine learning and the increase of computing power\nhas increased the interest in the analysis and statistical testing of\nmultidimensional data. We extend the powerful Kolmogorov-Smirnov two sample\ntest to a high dimensional form in a similar manner to Fasano (Fasano, 1987).\nWe call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide\nthree novel contributions therewith: we develop an analytical equation for the\nsignificance of a given ddKS score, we provide an algorithm for computation of\nddKS on modern computing hardware that is of constant time complexity for small\nsample sizes and dimensions, and we provide two approximate calculations of\nddKS: one that reduces the time complexity to linear at larger sample sizes,\nand another that reduces the time complexity to linear with increasing\ndimension. We perform power analysis of ddKS and its approximations on a corpus\nof datasets and compare to other common high dimensional two sample tests and\ndistances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test\nperforms well for all datasets, dimensions, and sizes tested, whereas the other\ntests and distances fail to reject the null hypothesis on at least one dataset.\nWe therefore conclude that ddKS is a powerful multidimensional two sample test\nfor general use, and can be calculated in a fast and efficient manner using our\nparallel or approximate methods. Open source implementations of all methods\ndescribed in this work are located at https://github.com/pnnl/ddks.",
          "link": "http://arxiv.org/abs/2106.13706",
          "publishedOn": "2021-06-28T01:57:55.839Z",
          "wordCount": 699,
          "title": "Accelerated Computation of a High Dimensional Kolmogorov-Smirnov Distance. (arXiv:2106.13706v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Clayton Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1\">Bianca Picchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1\">Jovan Pantelic</a>",
          "description": "Machine learning for building energy prediction has exploded in popularity in\nrecent years, yet understanding its limitations and potential for improvement\nare lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition\nwas the largest building energy meter machine learning competition ever held\nwith 4,370 participants who submitted 39,403 predictions. The test data set\nincluded two years of hourly electricity, hot water, chilled water, and steam\nreadings from 2,380 meters in 1,448 buildings at 16 locations. This paper\nanalyzes the various sources and types of residual model error from an\naggregation of the competition's top 50 solutions. This analysis reveals the\nlimitations for machine learning using the standard model inputs of historical\nmeter, weather, and basic building metadata. The types of error are classified\naccording to the amount of time errors occur in each instance, abrupt versus\ngradual behavior, the magnitude of error, and whether the error existed on\nsingle buildings or several buildings at once from a single location. The\nresults show machine learning models have errors within a range of\nacceptability on 79.1% of the test data. Lower magnitude model errors occur in\n16.1% of the test data. These discrepancies can likely be addressed through\nadditional training data sources or innovations in machine learning. Higher\nmagnitude errors occur in 4.8% of the test data and are unlikely to be\naccurately predicted regardless of innovation. There is a diversity of error\nbehavior depending on the energy meter type (electricity prediction models have\nunacceptable error in under 10% of test data, while hot water is over 60%) and\nbuilding use type (public service less than 14%, while technology/science is\njust over 46%).",
          "link": "http://arxiv.org/abs/2106.13475",
          "publishedOn": "2021-06-28T01:57:55.796Z",
          "wordCount": 724,
          "title": "Limitations of machine learning for building energy prediction: ASHRAE Great Energy Predictor III Kaggle competition error analysis. (arXiv:2106.13475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1\">Kristofer Bouchard</a>",
          "description": "Many modern time-series datasets contain large numbers of output response\nvariables sampled for prolonged periods of time. For example, in neuroscience,\nthe activities of 100s-1000's of neurons are recorded during behaviors and in\nresponse to sensory stimuli. Multi-output Gaussian process models leverage the\nnonparametric nature of Gaussian processes to capture structure across multiple\noutputs. However, this class of models typically assumes that the correlations\nbetween the output response variables are invariant in the input space.\nStochastic linear mixing models (SLMM) assume the mixture coefficients depend\non input, making them more flexible and effective to capture complex output\ndependence. However, currently, the inference for SLMMs is intractable for\nlarge datasets, making them inapplicable to several modern time-series\nproblems. In this paper, we propose a new regression framework, the orthogonal\nstochastic linear mixing model (OSLMM) that introduces an orthogonal constraint\namongst the mixing coefficients. This constraint reduces the computational\nburden of inference while retaining the capability to handle complex output\ndependence. We provide Markov chain Monte Carlo inference procedures for both\nSLMM and OSLMM and demonstrate superior model scalability and reduced\nprediction error of OSLMM compared with state-of-the-art methods on several\nreal-world applications. In neurophysiology recordings, we use the inferred\nlatent functions for compact visualization of population responses to auditory\nstimuli, and demonstrate superior results compared to a competing method\n(GPFA). Together, these results demonstrate that OSLMM will be useful for the\nanalysis of diverse, large-scale time-series datasets.",
          "link": "http://arxiv.org/abs/2106.13379",
          "publishedOn": "2021-06-28T01:57:55.788Z",
          "wordCount": 676,
          "title": "Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model. (arXiv:2106.13379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13361",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Penwarden_M/0/1/0/all/0/1\">Michael Penwarden</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narayan_A/0/1/0/all/0/1\">Akil Narayan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kirby_R/0/1/0/all/0/1\">Robert M. Kirby</a>",
          "description": "Multifidelity simulation methodologies are often used in an attempt to\njudiciously combine low-fidelity and high-fidelity simulation results in an\naccuracy-increasing, cost-saving way. Candidates for this approach are\nsimulation methodologies for which there are fidelity differences connected\nwith significant computational cost differences. Physics-informed Neural\nNetworks (PINNs) are candidates for these types of approaches due to the\nsignificant difference in training times required when different fidelities\n(expressed in terms of architecture width and depth as well as optimization\ncriteria) are employed. In this paper, we propose a particular multifidelity\napproach applied to PINNs that exploits low-rank structure. We demonstrate that\nwidth, depth, and optimization criteria can be used as parameters related to\nmodel fidelity, and show numerical justification of cost differences in\ntraining due to fidelity parameter choices. We test our multifidelity scheme on\nvarious canonical forward PDE models that have been presented in the emerging\nPINNs literature.",
          "link": "http://arxiv.org/abs/2106.13361",
          "publishedOn": "2021-06-28T01:57:55.781Z",
          "wordCount": 579,
          "title": "Multifidelity Modeling for Physics-Informed Neural Networks (PINNs). (arXiv:2106.13361v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.",
          "link": "http://arxiv.org/abs/2106.13239",
          "publishedOn": "2021-06-28T01:57:55.744Z",
          "wordCount": 623,
          "title": "Federated Noisy Client Learning. (arXiv:2106.13239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>",
          "description": "Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.",
          "link": "http://arxiv.org/abs/2106.13301",
          "publishedOn": "2021-06-28T01:57:55.721Z",
          "wordCount": 644,
          "title": "Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1\">Alexandre Drouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>",
          "description": "This article introduces byteSteady -- a fast model for classification using\nbyte-level n-gram embeddings. byteSteady assumes that each input comes as a\nsequence of bytes. A representation vector is produced using the averaged\nembedding vectors of byte-level n-grams, with a pre-defined set of n. The\nhashing trick is used to reduce the number of embedding vectors. This input\nrepresentation vector is then fed into a linear classifier. A straightforward\napplication of byteSteady is text classification. We also apply byteSteady to\none type of non-language data -- DNA sequences for gene classification. For\nboth problems we achieved competitive classification results against strong\nbaselines, suggesting that byteSteady can be applied to both language and\nnon-language data. Furthermore, we find that simple compression using Huffman\ncoding does not significantly impact the results, which offers an\naccuracy-speed trade-off previously unexplored in machine learning.",
          "link": "http://arxiv.org/abs/2106.13302",
          "publishedOn": "2021-06-28T01:57:55.663Z",
          "wordCount": 571,
          "title": "byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings. (arXiv:2106.13302v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Juyang Weng</a>",
          "description": "This paper raises a rarely reported practice in Artificial Intelligence (AI)\ncalled Post Selection Using Test Sets (PSUTS). Consequently, the popular\nerror-backprop methodology in deep learning lacks an acceptable generalization\npower. All AI methods fall into two broad schools, connectionist and symbolic.\nThe PSUTS fall into two kinds, machine PSUTS and human PSUTS. The connectionist\nschool received criticisms for its \"scruffiness\" due to a huge number of\nnetwork parameters and now the worse machine PSUTS; but the seemingly \"clean\"\nsymbolic school seems more brittle because of a weaker generalization power\nusing human PSUTS. This paper formally defines what PSUTS is, analyzes why\nerror-backprop methods with random initial weights suffer from severe local\nminima, why PSUTS violates well-established research ethics, and how every\npaper that used PSUTS should have at least transparently reported PSUTS. For\nimproved transparency in future publications, this paper proposes a new\nstandard for performance evaluation of AI, called developmental errors for all\nnetworks trained, along with Three Learning Conditions: (1) an incremental\nlearning architecture, (2) a training experience and (3) a limited amount of\ncomputational resources. Developmental Networks avoid PSUTS and are not\n\"scruffy\" because they drive Emergent Turing Machines and are optimal in the\nsense of maximum-likelihood across lifetime.",
          "link": "http://arxiv.org/abs/2106.13233",
          "publishedOn": "2021-06-28T01:57:55.645Z",
          "wordCount": 669,
          "title": "Post Selections Using Test Sets (PSUTS) and How Developmental Networks Avoid Them. (arXiv:2106.13233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1\">Gavin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Adam Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1\">Jonathan Ullman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1\">Lydia Zakynthinou</a>",
          "description": "We present two sample-efficient differentially private mean estimators for\n$d$-dimensional (sub)Gaussian distributions with unknown covariance.\nInformally, given $n \\gtrsim d/\\alpha^2$ samples from such a distribution with\nmean $\\mu$ and covariance $\\Sigma$, our estimators output $\\tilde\\mu$ such that\n$\\| \\tilde\\mu - \\mu \\|_{\\Sigma} \\leq \\alpha$, where $\\| \\cdot \\|_{\\Sigma}$ is\nthe Mahalanobis distance. All previous estimators with the same guarantee\neither require strong a priori bounds on the covariance matrix or require\n$\\Omega(d^{3/2})$ samples.\n\nEach of our estimators is based on a simple, general approach to designing\ndifferentially private mechanisms, but with novel technical steps to make the\nestimator private and sample-efficient. Our first estimator samples a point\nwith approximately maximum Tukey depth using the exponential mechanism, but\nrestricted to the set of points of large Tukey depth. Proving that this\nmechanism is private requires a novel analysis. Our second estimator perturbs\nthe empirical mean of the data set with noise calibrated to the empirical\ncovariance, without releasing the covariance itself. Its sample complexity\nguarantees hold more generally for subgaussian distributions, albeit with a\nslightly worse dependence on the privacy parameter. For both estimators,\ncareful preprocessing of the data is required to satisfy differential privacy.",
          "link": "http://arxiv.org/abs/2106.13329",
          "publishedOn": "2021-06-28T01:57:55.585Z",
          "wordCount": 627,
          "title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation. (arXiv:2106.13329v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1\">Erwin Coumans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jie Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>",
          "description": "We focus on the problem of developing energy efficient controllers for\nquadrupedal robots. Animals can actively switch gaits at different speeds to\nlower their energy consumption. In this paper, we devise a hierarchical\nlearning framework, in which distinctive locomotion gaits and natural gait\ntransitions emerge automatically with a simple reward of energy minimization.\nWe use reinforcement learning to train a high-level gait policy that specifies\ngait patterns of each foot, while the low-level whole-body controller optimizes\nthe motor commands so that the robot can walk at a desired velocity using that\ngait pattern. We test our learning framework on a quadruped robot and\ndemonstrate automatic gait transitions, from walking to trotting and to\nfly-trotting, as the robot increases its speed. We show that the learned\nhierarchical controller consumes much less energy across a wide range of\nlocomotion speed than baseline controllers.",
          "link": "http://arxiv.org/abs/2104.04644",
          "publishedOn": "2021-06-25T02:00:47.701Z",
          "wordCount": 608,
          "title": "Fast and Efficient Locomotion via Learned Gait Transitions. (arXiv:2104.04644v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hiranandani_G/0/1/0/all/0/1\">Gaurush Hiranandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_J/0/1/0/all/0/1\">Jatin Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1\">Harikrishna Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1\">Mahdi Milani Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1\">Oluwasanmi Koyejo</a>",
          "description": "We consider learning to optimize a classification metric defined by a\nblack-box function of the confusion matrix. Such black-box learning settings\nare ubiquitous, for example, when the learner only has query access to the\nmetric of interest, or in noisy-label and domain adaptation applications where\nthe learner must evaluate the metric via performance evaluation using a small\nvalidation sample. Our approach is to adaptively learn example weights on the\ntraining dataset such that the resulting weighted objective best approximates\nthe metric on the validation sample. We show how to model and estimate the\nexample weights and use them to iteratively post-shift a pre-trained class\nprobability estimator to construct a classifier. We also analyze the resulting\nprocedure's statistical properties. Experiments on various label noise, domain\nshift, and fair classification setups confirm that our proposal compares\nfavorably to the state-of-the-art baselines for each application.",
          "link": "http://arxiv.org/abs/2102.09492",
          "publishedOn": "2021-06-25T02:00:47.695Z",
          "wordCount": 629,
          "title": "Optimizing Black-box Metrics with Iterative Example Weighting. (arXiv:2102.09492v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-25T02:00:47.678Z",
          "wordCount": 745,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02336",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1\">Zhixin Song</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1\">Youle Wang</a>",
          "description": "Singular value decomposition is central to many problems in engineering and\nscientific fields. Several quantum algorithms have been proposed to determine\nthe singular values and their associated singular vectors of a given matrix.\nAlthough these algorithms are promising, the required quantum subroutines and\nresources are too costly on near-term quantum devices. In this work, we propose\na variational quantum algorithm for singular value decomposition (VQSVD). By\nexploiting the variational principles for singular values and the Ky Fan\nTheorem, we design a novel loss function such that two quantum neural networks\n(or parameterized quantum circuits) could be trained to learn the singular\nvectors and output the corresponding singular values. Furthermore, we conduct\nnumerical simulations of VQSVD for random matrices as well as its applications\nin image compression of handwritten digits. Finally, we discuss the\napplications of our algorithm in recommendation systems and polar\ndecomposition. Our work explores new avenues for quantum information processing\nbeyond the conventional protocols that only works for Hermitian data, and\nreveals the capability of matrix decomposition on near-term quantum devices.",
          "link": "http://arxiv.org/abs/2006.02336",
          "publishedOn": "2021-06-25T02:00:47.602Z",
          "wordCount": 630,
          "title": "Variational Quantum Singular Value Decomposition. (arXiv:2006.02336v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.04883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Vector-valued learning, where the output space admits a vector-valued\nstructure, is an important problem that covers a broad family of important\ndomains, e.g. multi-label learning and multi-class classification. Using local\nRademacher complexity and unlabeled data, we derive novel data-dependent excess\nrisk bounds for learning vector-valued functions in both the kernel space and\nlinear space. The derived bounds are much sharper than existing ones, where\nconvergence rates are improved from $\\mathcal{O}(1/\\sqrt{n})$ to\n$\\mathcal{O}(1/\\sqrt{n+u}),$ and $\\mathcal{O}(1/n)$ in special cases. Motivated\nby our theoretical analysis, we propose a unified framework for learning\nvector-valued functions, incorporating both local Rademacher complexity and\nLaplacian regularization. Empirical results on a wide number of benchmark\ndatasets show that the proposed algorithm significantly outperforms baseline\nmethods, which coincides with our theoretical findings.",
          "link": "http://arxiv.org/abs/1909.04883",
          "publishedOn": "2021-06-25T02:00:47.597Z",
          "wordCount": 591,
          "title": "Semi-supervised Vector-valued Learning: From Theory to Algorithm. (arXiv:1909.04883v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>",
          "description": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
          "link": "http://arxiv.org/abs/2106.13219",
          "publishedOn": "2021-06-25T02:00:47.592Z",
          "wordCount": 596,
          "title": "Towards Understanding and Mitigating Social Biases in Language Models. (arXiv:2106.13219v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1810.03730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1\">Marian-Andrei Rizoiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lexing Xie</a>",
          "description": "In this paper, we develop an efficient nonparametric Bayesian estimation of\nthe kernel function of Hawkes processes. The non-parametric Bayesian approach\nis important because it provides flexible Hawkes kernels and quantifies their\nuncertainty. Our method is based on the cluster representation of Hawkes\nprocesses. Utilizing the stationarity of the Hawkes process, we efficiently\nsample random branching structures and thus, we split the Hawkes process into\nclusters of Poisson processes. We derive two algorithms -- a block Gibbs\nsampler and a maximum a posteriori estimator based on expectation maximization\n-- and we show that our methods have a linear time complexity, both\ntheoretically and empirically. On synthetic data, we show our methods to be\nable to infer flexible Hawkes triggering kernels. On two large-scale Twitter\ndiffusion datasets, we show that our methods outperform the current\nstate-of-the-art in goodness-of-fit and that the time complexity is linear in\nthe size of the dataset. We also observe that on diffusions related to online\nvideos, the learned kernels reflect the perceived longevity for different\ncontent types such as music or pets videos.",
          "link": "http://arxiv.org/abs/1810.03730",
          "publishedOn": "2021-06-25T02:00:47.587Z",
          "wordCount": 657,
          "title": "Efficient Non-parametric Bayesian Hawkes Processes. (arXiv:1810.03730v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>",
          "description": "Inductive Logic Programming (ILP) aims to learn generalised, interpretable\nhypotheses in a data-efficient manner. However, current ILP systems require\ntraining examples to be specified in a structured logical form. This paper\nintroduces a neural-symbolic learning framework, called Feed-Forward\nNeural-Symbolic Learner (FF-NSL), that integrates state-of-the-art ILP systems\nbased on the Answer Set semantics, with neural networks, in order to learn\ninterpretable hypotheses from labelled unstructured data. FF-NSL uses a\npre-trained neural network to extract symbolic facts from unstructured data and\nan ILP system to learn a hypothesis that performs a downstream classification\ntask. In order to evaluate the applicability of our approach to real-world\napplications, the framework is evaluated on tasks where distributional shifts\nare introduced to unstructured input data, for which pre-trained neural\nnetworks are likely to predict incorrectly and with high confidence.\nExperimental results show that FF-NSL outperforms baseline approaches such as a\nrandom forest and deep neural networks by learning more accurate and\ninterpretable hypotheses with fewer examples.",
          "link": "http://arxiv.org/abs/2106.13103",
          "publishedOn": "2021-06-25T02:00:47.568Z",
          "wordCount": 586,
          "title": "FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1\">Jia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>",
          "description": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
          "link": "http://arxiv.org/abs/2106.13230",
          "publishedOn": "2021-06-25T02:00:47.562Z",
          "wordCount": 606,
          "title": "Video Swin Transformer. (arXiv:2106.13230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramzi_Z/0/1/0/all/0/1\">Zaccharie Ramzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannel_F/0/1/0/all/0/1\">Florian Mannel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starck_J/0/1/0/all/0/1\">Jean-Luc Starck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciuciu_P/0/1/0/all/0/1\">Philippe Ciuciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1\">Thomas Moreau</a>",
          "description": "In recent years, implicit deep learning has emerged as a method to increase\nthe depth of deep neural networks. While their training is memory-efficient,\nthey are still significantly slower to train than their explicit counterparts.\nIn Deep Equilibrium Models (DEQs), the training is performed as a bi-level\nproblem, and its computational complexity is partially driven by the iterative\ninversion of a huge Jacobian matrix. In this paper, we propose a novel strategy\nto tackle this computational bottleneck from which many bi-level problems\nsuffer. The main idea is to use the quasi-Newton matrices from the forward pass\nto efficiently approximate the inverse Jacobian matrix in the direction needed\nfor the gradient computation. We provide a theorem that motivates using our\nmethod with the original forward algorithms. In addition, by modifying these\nforward algorithms, we further provide theoretical guarantees that our method\nasymptotically estimates the true implicit gradient. We empirically study this\napproach in many settings, ranging from hyperparameter optimization to large\nMultiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the\ncomputational cost of the backward pass by up to two orders of magnitude. All\nthis is achieved while retaining the excellent performance of the original\nmodels in hyperparameter optimization and on CIFAR, and giving encouraging and\ncompetitive results on ImageNet.",
          "link": "http://arxiv.org/abs/2106.00553",
          "publishedOn": "2021-06-25T02:00:47.557Z",
          "wordCount": 682,
          "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07879",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Subhani_M/0/1/0/all/0/1\">Moeez M. Subhani</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Anjum_A/0/1/0/all/0/1\">Ashiq Anjum</a>",
          "description": "Clinical predictions using clinical data by computational methods are common\nin bioinformatics. However, clinical predictions using information from\ngenomics datasets as well is not a frequently observed phenomenon in research.\nPrecision medicine research requires information from all available datasets to\nprovide intelligent clinical solutions. In this paper, we have attempted to\ncreate a prediction model which uses information from both clinical and\ngenomics datasets. We have demonstrated multiclass disease predictions based on\ncombined clinical and genomics datasets using machine learning methods. We have\ncreated an integrated dataset, using a clinical (ClinVar) and a genomics (gene\nexpression) dataset, and trained it using instance-based learner to predict\nclinical diseases. We have used an innovative but simple way for multiclass\nclassification, where the number of output classes is as high as 75. We have\nused Principal Component Analysis for feature selection. The classifier\npredicted diseases with 73\\% accuracy on the integrated dataset. The results\nwere consistent and competent when compared with other classification models.\nThe results show that genomics information can be reliably included in datasets\nfor clinical predictions and it can prove to be valuable in clinical\ndiagnostics and precision medicine.",
          "link": "http://arxiv.org/abs/2006.07879",
          "publishedOn": "2021-06-25T02:00:47.552Z",
          "wordCount": 653,
          "title": "Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets. (arXiv:2006.07879v1 [q-bio.GN] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>",
          "description": "Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.",
          "link": "http://arxiv.org/abs/2010.07987",
          "publishedOn": "2021-06-25T02:00:47.547Z",
          "wordCount": 632,
          "title": "Empirical Study of Transformers for Source Code. (arXiv:2010.07987v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neumeier_M/0/1/0/all/0/1\">Marion Neumeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollkuhn_A/0/1/0/all/0/1\">Andreas Tollk&#xfc;hn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berberich_T/0/1/0/all/0/1\">Thomas Berberich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>",
          "description": "This paper introduces the Descriptive Variational Autoencoder (DVAE), an\nunsupervised and end-to-end trainable neural network for predicting vehicle\ntrajectories that provides partial interpretability. The novel approach is\nbased on the architecture and objective of common variational autoencoders. By\nintroducing expert knowledge within the decoder part of the autoencoder, the\nencoder learns to extract latent parameters that provide a graspable meaning in\nhuman terms. Such an interpretable latent space enables the validation by\nexpert defined rule sets. The evaluation of the DVAE is performed using the\npublicly available highD dataset for highway traffic scenarios. In comparison\nto a conventional variational autoencoder with equivalent complexity, the\nproposed model provides a similar prediction accuracy but with the great\nadvantage of having an interpretable latent space. For crucial decision making\nand assessing trustworthiness of a prediction this property is highly\ndesirable.",
          "link": "http://arxiv.org/abs/2103.13726",
          "publishedOn": "2021-06-25T02:00:47.533Z",
          "wordCount": 607,
          "title": "Variational Autoencoder-Based Vehicle Trajectory Prediction with an Interpretable Latent Space. (arXiv:2103.13726v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1\">Nicolai Pogrebnyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1\">Shohreh Shaghaghian</a>",
          "description": "Transfer learning methods, and in particular domain adaptation, help exploit\nlabeled data in one domain to improve the performance of a certain task in\nanother domain. However, it is still not clear what factors affect the success\nof domain adaptation. This paper models adaptation success and selection of the\nmost suitable source domains among several candidates in text similarity. We\nuse descriptive domain information and cross-domain similarity metrics as\npredictive features. While mostly positive, the results also point to some\ndomains where adaptation success was difficult to predict.",
          "link": "http://arxiv.org/abs/2106.04641",
          "publishedOn": "2021-06-25T02:00:47.527Z",
          "wordCount": 543,
          "title": "Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1\">Michael Laskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "We introduce a framework that abstracts Reinforcement Learning (RL) as a\nsequence modeling problem. This allows us to draw upon the simplicity and\nscalability of the Transformer architecture, and associated advances in\nlanguage modeling such as GPT-x and BERT. In particular, we present Decision\nTransformer, an architecture that casts the problem of RL as conditional\nsequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal\nactions by leveraging a causally masked Transformer. By conditioning an\nautoregressive model on the desired return (reward), past states, and actions,\nour Decision Transformer model can generate future actions that achieve the\ndesired return. Despite its simplicity, Decision Transformer matches or exceeds\nthe performance of state-of-the-art model-free offline RL baselines on Atari,\nOpenAI Gym, and Key-to-Door tasks.",
          "link": "http://arxiv.org/abs/2106.01345",
          "publishedOn": "2021-06-25T02:00:47.522Z",
          "wordCount": 603,
          "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling. (arXiv:2106.01345v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hambly_B/0/1/0/all/0/1\">Ben Hambly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huining Yang</a>",
          "description": "We explore reinforcement learning methods for finding the optimal policy in\nthe linear quadratic regulator (LQR) problem. In particular, we consider the\nconvergence of policy gradient methods in the setting of known and unknown\nparameters. We are able to produce a global linear convergence guarantee for\nthis approach in the setting of finite time horizon and stochastic state\ndynamics under weak assumptions. The convergence of a projected policy gradient\nmethod is also established in order to handle problems with constraints. We\nillustrate the performance of the algorithm with two examples. The first\nexample is the optimal liquidation of a holding in an asset. We show results\nfor the case where we assume a model for the underlying dynamics and where we\napply the method to the data directly. The empirical evidence suggests that the\npolicy gradient method can learn the global optimal solution for a larger class\nof stochastic systems containing the LQR framework and that it is more robust\nwith respect to model mis-specification when compared to a model-based\napproach. The second example is an LQR system in a higher dimensional setting\nwith synthetic data.",
          "link": "http://arxiv.org/abs/2011.10300",
          "publishedOn": "2021-06-25T02:00:47.517Z",
          "wordCount": 665,
          "title": "Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon. (arXiv:2011.10300v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1\">Rachid Guerraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinot_R/0/1/0/all/0/1\">Rafa&#xeb;l Pinot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1\">S&#xe9;bastien Rouault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephan_J/0/1/0/all/0/1\">John Stephan</a>",
          "description": "This paper addresses the problem of combining Byzantine resilience with\nprivacy in machine learning (ML). Specifically, we study if a distributed\nimplementation of the renowned Stochastic Gradient Descent (SGD) learning\nalgorithm is feasible with both differential privacy (DP) and\n$(\\alpha,f)$-Byzantine resilience. To the best of our knowledge, this is the\nfirst work to tackle this problem from a theoretical point of view. A key\nfinding of our analyses is that the classical approaches to these two\n(seemingly) orthogonal issues are incompatible. More precisely, we show that a\ndirect composition of these techniques makes the guarantees of the resulting\nSGD algorithm depend unfavourably upon the number of parameters of the ML\nmodel, making the training of large models practically infeasible. We validate\nour theoretical results through numerical experiments on publicly-available\ndatasets; showing that it is impractical to ensure DP and Byzantine resilience\nsimultaneously.",
          "link": "http://arxiv.org/abs/2102.08166",
          "publishedOn": "2021-06-25T02:00:47.512Z",
          "wordCount": 629,
          "title": "Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?. (arXiv:2102.08166v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Delaney_E/0/1/0/all/0/1\">Eoin Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greene_D/0/1/0/all/0/1\">Derek Greene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_M/0/1/0/all/0/1\">Mark T. Keane</a>",
          "description": "In recent years, there has been a rapidly expanding focus on explaining the\npredictions made by black-box AI systems that handle image and tabular data.\nHowever, considerably less attention has been paid to explaining the\npredictions of opaque AI systems handling time series data. In this paper, we\nadvance a novel model-agnostic, case-based technique -- Native Guide -- that\ngenerates counterfactual explanations for time series classifiers. Given a\nquery time series, $T_{q}$, for which a black-box classification system\npredicts class, $c$, a counterfactual time series explanation shows how $T_{q}$\ncould change, such that the system predicts an alternative class, $c'$. The\nproposed instance-based technique adapts existing counterfactual instances in\nthe case-base by highlighting and modifying discriminative areas of the time\nseries that underlie the classification. Quantitative and qualitative results\nfrom two comparative experiments indicate that Native Guide generates\nplausible, proximal, sparse and diverse explanations that are better than those\nproduced by key benchmark counterfactual methods.",
          "link": "http://arxiv.org/abs/2009.13211",
          "publishedOn": "2021-06-25T02:00:47.498Z",
          "wordCount": 615,
          "title": "Instance-based Counterfactual Explanations for Time Series Classification. (arXiv:2009.13211v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sangwoong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1\">Yung-Kyun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1\">Frank Chongwoo Park</a>",
          "description": "Likelihood is a standard estimate for outlier detection. The specific role of\nthe normalization constraint is to ensure that the out-of-distribution (OOD)\nregime has a small likelihood when samples are learned using maximum\nlikelihood. Because autoencoders do not possess such a process of\nnormalization, they often fail to recognize outliers even when they are\nobviously OOD. We propose the Normalized Autoencoder (NAE), a normalized\nprobabilistic model constructed from an autoencoder. The probability density of\nNAE is defined using the reconstruction error of an autoencoder, which is\ndifferently defined in the conventional energy-based model. In our model,\nnormalization is enforced by suppressing the reconstruction of negative\nsamples, significantly improving the outlier detection performance. Our\nexperimental results confirm the efficacy of NAE, both in detecting outliers\nand in generating in-distribution samples.",
          "link": "http://arxiv.org/abs/2105.05735",
          "publishedOn": "2021-06-25T02:00:47.492Z",
          "wordCount": 611,
          "title": "Autoencoding Under Normalization Constraints. (arXiv:2105.05735v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02609",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Pananjady_A/0/1/0/all/0/1\">Ashwin Pananjady</a>, <a href=\"http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1\">Richard J. Samworth</a>",
          "description": "Motivated by models for multiway comparison data, we consider the problem of\nestimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from\nnoisy observations collected on a uniform lattice, but where the design points\nhave been permuted along each dimension. While the univariate and bivariate\nversions of this problem have received significant attention, our focus is on\nthe multivariate case $d \\geq 3$. We study both the minimax risk of estimation\n(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified\nby the adaptivity index) to a family of piecewise constant functions. We\nprovide a computationally efficient Mirsky partition estimator that is minimax\noptimal while also achieving the smallest adaptivity index possible for\npolynomial time procedures. Thus, from a worst-case perspective and in sharp\ncontrast to the bivariate case, the latent permutations in the model do not\nintroduce significant computational difficulties over and above vanilla\nisotonic regression. On the other hand, the fundamental limits of adaptation\nare significantly different with and without unknown permutations: Assuming a\nhardness conjecture from average-case complexity theory, a\nstatistical-computational gap manifests in the former case. In a complementary\ndirection, we show that natural modifications of existing estimators fail to\nsatisfy at least one of the desiderata of optimal worst-case statistical\nperformance, computational efficiency, and fast adaptation. Along the way to\nshowing our results, we improve adaptation results in the special case $d = 2$\nand establish some properties of estimators for vanilla isotonic regression,\nboth of which may be of independent interest.",
          "link": "http://arxiv.org/abs/2009.02609",
          "publishedOn": "2021-06-25T02:00:47.487Z",
          "wordCount": 725,
          "title": "Isotonic regression with unknown permutations: Statistics, computation, and adaptation. (arXiv:2009.02609v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sater_R/0/1/0/all/0/1\">Raed Abdel Sater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "Internet of Things (IoT) sensors in smart buildings are becoming increasingly\nubiquitous, making buildings more livable, energy efficient, and sustainable.\nThese devices sense the environment and generate multivariate temporal data of\nparamount importance for detecting anomalies and improving the prediction of\nenergy usage in smart buildings. However, detecting these anomalies in\ncentralized systems is often plagued by a huge delay in response time. To\novercome this issue, we formulate the anomaly detection problem in a federated\nlearning setting by leveraging the multi-task learning paradigm, which aims at\nsolving multiple tasks simultaneously while taking advantage of the\nsimilarities and differences across tasks. We propose a novel privacy-by-design\nfederated learning model using a stacked long short-time memory (LSTM) model,\nand we demonstrate that it is more than twice as fast during training\nconvergence compared to the centralized LSTM. The effectiveness of our\nfederated learning approach is demonstrated on three real-world datasets\ngenerated by the IoT production system at General Electric Current smart\nbuilding, achieving state-of-the-art performance compared to baseline methods\nin both classification and regression tasks. Our experimental results\ndemonstrate the effectiveness of the proposed framework in reducing the overall\ntraining cost without compromising the prediction performance.",
          "link": "http://arxiv.org/abs/2010.10293",
          "publishedOn": "2021-06-25T02:00:47.480Z",
          "wordCount": 664,
          "title": "A Federated Learning Approach to Anomaly Detection in Smart Buildings. (arXiv:2010.10293v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.01849",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bauch_J/0/1/0/all/0/1\">Jonathan Bauch</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1\">Pini Zilber</a>",
          "description": "We present a new, simple and computationally efficient iterative method for\nlow rank matrix completion. Our method is inspired by the class of\nfactorization-type iterative algorithms, but substantially differs from them in\nthe way the problem is cast. Precisely, given a target rank $r$, instead of\noptimizing on the manifold of rank $r$ matrices, we allow our interim estimated\nmatrix to have a specific over-parametrized rank $2r$ structure. Our algorithm,\ndenoted R2RILS for rank $2r$ iterative least squares, has low memory\nrequirements, and at each iteration it solves a computationally cheap sparse\nleast-squares problem. We motivate our algorithm by its theoretical analysis\nfor the simplified case of a rank-1 matrix. Empirically, R2RILS is able to\nrecover ill conditioned low rank matrices from very few observations -- near\nthe information limit, and it is stable to additive noise.",
          "link": "http://arxiv.org/abs/2002.01849",
          "publishedOn": "2021-06-25T02:00:47.466Z",
          "wordCount": 602,
          "title": "Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries. (arXiv:2002.01849v2 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13202",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Park_J/0/1/0/all/0/1\">Ju An Park</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Thomas_K/0/1/0/all/0/1\">Kathryn E. Thomas</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deglint_J/0/1/0/all/0/1\">Jason L. Deglint</a>",
          "description": "Warming oceans due to climate change are leading to increased numbers of\nectoparasitic copepods, also known as sea lice, which can cause significant\necological loss to wild salmon populations and major economic loss to\naquaculture sites. The main transport mechanism driving the spread of sea lice\npopulations are near-surface ocean currents. Present strategies to estimate the\ndistribution of sea lice larvae are computationally complex and limit\nfull-scale analysis. Motivated to address this challenge, we propose SALT: Sea\nlice Adaptive Lattice Tracking approach for efficient estimation of sea lice\ndispersion and distribution in space and time. Specifically, an adaptive\nspatial mesh is generated by merging nodes in the lattice graph of the Ocean\nModel based on local ocean properties, thus enabling highly efficient graph\nrepresentation. SALT demonstrates improved efficiency while maintaining\nconsistent results with the standard method, using near-surface current data\nfor Hardangerfjord, Norway. The proposed SALT technique shows promise for\nenhancing proactive aquaculture management through predictive modelling of sea\nlice infestation pressure maps in a changing climate.",
          "link": "http://arxiv.org/abs/2106.13202",
          "publishedOn": "2021-06-25T02:00:47.455Z",
          "wordCount": 631,
          "title": "SALT: Sea lice Adaptive Lattice Tracking -- An Unsupervised Approach to Generate an Improved Ocean Model. (arXiv:2106.13202v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08393",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadhu_S/0/1/0/all/0/1\">Samik Sadhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Che-Wei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mallidi_S/0/1/0/all/0/1\">Sri Harish Mallidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Minhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maas_R/0/1/0/all/0/1\">Roland Maas</a>",
          "description": "Wav2vec-C introduces a novel representation learning technique combining\nelements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized\nrepresentations from partially masked speech encoding using a contrastive loss\nin a way similar to Wav2vec 2.0. However, the quantization process is\nregularized by an additional consistency network that learns to reconstruct the\ninput features to the wav2vec 2.0 network from the quantized representations in\na way similar to a VQ-VAE model. The proposed self-supervised model is trained\non 10k hours of unlabeled data and subsequently used as the speech encoder in a\nRNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one\nof only a few studies of self-supervised learning on speech tasks with a large\nvolume of real far-field labeled data. The Wav2vec-C encoded representations\nachieves, on average, twice the error reduction over baseline and a higher\ncodebook utilization in comparison to wav2vec 2.0",
          "link": "http://arxiv.org/abs/2103.08393",
          "publishedOn": "2021-06-25T02:00:47.438Z",
          "wordCount": 630,
          "title": "Wav2vec-C: A Self-supervised Model for Speech Representation Learning. (arXiv:2103.08393v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1\">Elizaveta Rebrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Hang Tang</a>",
          "description": "We introduce and investigate matrix approximation by decomposition into a sum\nof radial basis function (RBF) components. An RBF component is a generalization\nof the outer product between a pair of vectors, where an RBF function replaces\nthe scalar multiplication between individual vector elements. Even though the\nRBF functions are positive definite, the summation across components is not\nrestricted to convex combinations and allows us to compute the decomposition\nfor any real matrix that is not necessarily symmetric or positive definite. We\nformulate the problem of seeking such a decomposition as an optimization\nproblem with a nonlinear and non-convex loss function. Several modern versions\nof the gradient descent method, including their scalable stochastic\ncounterparts, are used to solve this problem. We provide extensive empirical\nevidence of the effectiveness of the RBF decomposition and that of the\ngradient-based fitting algorithm. While being conceptually motivated by\nsingular value decomposition (SVD), our proposed nonlinear counterpart\noutperforms SVD by drastically reducing the memory required to approximate a\ndata matrix with the same L2 error for a wide range of matrix types. For\nexample, it leads to 2 to 6 times memory save for Gaussian noise, graph\nadjacency matrices, and kernel matrices. Moreover, this proximity-based\ndecomposition can offer additional interpretability in applications that\ninvolve, e.g., capturing the inner low-dimensional structure of the data,\nretaining graph connectivity structure, and preserving the acutance of images.",
          "link": "http://arxiv.org/abs/2106.02018",
          "publishedOn": "2021-06-25T02:00:47.425Z",
          "wordCount": 676,
          "title": "Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Heejong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>",
          "description": "Current deep learning approaches for diffusion MRI modeling circumvent the\nneed for densely-sampled diffusion-weighted images (DWIs) by directly\npredicting microstructural indices from sparsely-sampled DWIs. However, they\nimplicitly make unrealistic assumptions of static $q$-space sampling during\ntraining and reconstruction. Further, such approaches can restrict downstream\nusage of variably sampled DWIs for usages including the estimation of\nmicrostructural indices or tractography. We propose a generative adversarial\ntranslation framework for high-quality DWI synthesis with arbitrary $q$-space\nsampling given commonly acquired structural images (e.g., B0, T1, T2). Our\ntranslation network linearly modulates its internal representations conditioned\non continuous $q$-space information, thus removing the need for fixed sampling\nschemes. Moreover, this approach enables downstream estimation of high-quality\nmicrostructural maps from arbitrarily subsampled DWIs, which may be\nparticularly important in cases with sparsely sampled DWIs. Across several\nrecent methodologies, the proposed approach yields improved DWI synthesis\naccuracy and fidelity with enhanced downstream utility as quantified by the\naccuracy of scalar microstructure indices estimated from the synthesized\nimages. Code is available at\nhttps://github.com/mengweiren/q-space-conditioned-dwi-synthesis.",
          "link": "http://arxiv.org/abs/2106.13188",
          "publishedOn": "2021-06-25T02:00:47.419Z",
          "wordCount": 645,
          "title": "Q-space Conditioned Translation Networks for Directional Synthesis of Diffusion Weighted Images from Multi-modal Structural MRI. (arXiv:2106.13188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10745",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1\">Adrian Celaya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1\">Jonas A. Actor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1\">Rajarajeswari Muthusivarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1\">Evan Gates</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Caroline Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1\">Dawid Schellingerhout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1\">Beatrice Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1\">David Fuentes</a>",
          "description": "Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.",
          "link": "http://arxiv.org/abs/2104.10745",
          "publishedOn": "2021-06-25T02:00:47.411Z",
          "wordCount": 586,
          "title": "PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gordillo_C/0/1/0/all/0/1\">Camilo Gordillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1\">Joakim Bergdahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollmar_K/0/1/0/all/0/1\">Konrad Tollmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1\">Linus Gissl&#xe9;n</a>",
          "description": "As modern games continue growing both in size and complexity, it has become\nmore challenging to ensure that all the relevant content is tested and that any\npotential issue is properly identified and fixed. Attempting to maximize\ntesting coverage using only human participants, however, results in a tedious\nand hard to orchestrate process which normally slows down the development\ncycle. Complementing playtesting via autonomous agents has shown great promise\naccelerating and simplifying this process. This paper addresses the problem of\nautomatically exploring and testing a given scenario using reinforcement\nlearning agents trained to maximize game state coverage. Each of these agents\nis rewarded based on the novelty of its actions, thus encouraging a curious and\nexploratory behaviour on a complex 3D scenario where previously proposed\nexploration techniques perform poorly. The curious agents are able to learn the\ncomplex navigation mechanics required to reach the different areas around the\nmap, thus providing the necessary data to identify potential issues. Moreover,\nthe paper also explores different visualization strategies and evaluates how to\nmake better use of the collected data to drive design decisions and to\nrecognize possible problems and oversights.",
          "link": "http://arxiv.org/abs/2103.13798",
          "publishedOn": "2021-06-25T02:00:47.406Z",
          "wordCount": 647,
          "title": "Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents. (arXiv:2103.13798v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yifan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Liyao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>",
          "description": "Federated learning is emerging as a machine learning technique that trains a\nmodel across multiple decentralized parties. It is renowned for preserving\nprivacy as the data never leaves the computational devices, and recent\napproaches further enhance its privacy by hiding messages transferred in\nencryption. However, we found that despite the efforts, federated learning\nremains privacy-threatening, due to its interactive nature across different\nparties. In this paper, we analyze the privacy threats in industrial-level\nfederated learning frameworks with secure computation, and reveal such threats\nwidely exist in typical machine learning models such as linear regression,\nlogistic regression and decision tree. For the linear and logistic regression,\nwe show through theoretical analysis that it is possible for the attacker to\ninvert the entire private input of the victim, given very few information. For\nthe decision tree model, we launch an attack to infer the range of victim's\nprivate inputs. All attacks are evaluated on popular federated learning\nframeworks and real-world datasets.",
          "link": "http://arxiv.org/abs/2106.13076",
          "publishedOn": "2021-06-25T02:00:47.389Z",
          "wordCount": 601,
          "title": "Privacy Threats Analysis to Secure Federated Learning. (arXiv:2106.13076v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiyue Yin</a>",
          "description": "Model-based reinforcement learning (MBRL) is believed to have higher sample\nefficiency compared with model-free reinforcement learning (MFRL). However,\nMBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is\nthe phenomenon that the performance of the algorithm falls into the local\noptimum instead of increasing when the interaction step with the environment\nincreases, which means more data can not bring better performance. In this\npaper, we find that the trajectory reward estimation error is the main reason\nthat causes dynamics bottleneck dilemma through theoretical analysis. We give\nan upper bound of the trajectory reward estimation error and point out that\nincreasing the agent's exploration ability is the key to reduce trajectory\nreward estimation error, thereby alleviating dynamics bottleneck dilemma.\nMotivated by this, a model-based control method combined with exploration named\nMOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We\nconduct experiments on several complex continuous control benchmark tasks. The\nresults verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma\nand have higher sample efficiency than previous MBRL and MFRL algorithms.",
          "link": "http://arxiv.org/abs/2010.12914",
          "publishedOn": "2021-06-25T02:00:47.357Z",
          "wordCount": 649,
          "title": "Planning with Exploration: Addressing Dynamics Bottleneck in Model-based Reinforcement Learning. (arXiv:2010.12914v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-06-25T02:00:47.351Z",
          "wordCount": 596,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+George_Y/0/1/0/all/0/1\">Yasmeen George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karunasekera_S/0/1/0/all/0/1\">Shanika Karunasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1\">Aaron Harwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "A key challenge in mining social media data streams is to identify events\nwhich are actively discussed by a group of people in a specific local or global\narea. Such events are useful for early warning for accident, protest, election\nor breaking news. However, neither the list of events nor the resolution of\nboth event time and space is fixed or known beforehand. In this work, we\npropose an online spatio-temporal event detection system using social media\nthat is able to detect events at different time and space resolutions. First,\nto address the challenge related to the unknown spatial resolution of events, a\nquad-tree method is exploited in order to split the geographical space into\nmultiscale regions based on the density of social media data. Then, a\nstatistical unsupervised approach is performed that involves Poisson\ndistribution and a smoothing method for highlighting regions with unexpected\ndensity of social posts. Further, event duration is precisely estimated by\nmerging events happening in the same region at consecutive time intervals. A\npost processing stage is introduced to filter out events that are spam, fake or\nwrong. Finally, we incorporate simple semantics by using social media entities\nto assess the integrity, and accuracy of detected events. The proposed method\nis evaluated using different social media datasets: Twitter and Flickr for\ndifferent cities: Melbourne, London, Paris and New York. To verify the\neffectiveness of the proposed method, we compare our results with two baseline\nalgorithms based on fixed split of geographical space and clustering method.\nFor performance evaluation, we manually compute recall and precision. We also\npropose a new quality measure named strength index, which automatically\nmeasures how accurate the reported event is.",
          "link": "http://arxiv.org/abs/2106.13121",
          "publishedOn": "2021-06-25T02:00:47.336Z",
          "wordCount": 729,
          "title": "Real-time Spatio-temporal Event Detection on Geotagged Social Media. (arXiv:2106.13121v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1\">Celestine Mendler-D&#xfc;nner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>",
          "description": "When reasoning about strategic behavior in a machine learning context it is\ntempting to combine standard microfoundations of rational agents with the\nstatistical decision theory underlying classification. In this work, we argue\nthat a direct combination of these standard ingredients leads to brittle\nsolution concepts of limited descriptive and prescriptive value. First, we show\nthat rational agents with perfect information produce discontinuities in the\naggregate response to a decision rule that we often do not observe empirically.\nSecond, when any positive fraction of agents is not perfectly strategic,\ndesirable stable points -- where the classifier is optimal for the data it\nentails -- cease to exist. Third, optimal decision rules under standard\nmicrofoundations maximize a measure of negative externality known as social\nburden within a broad class of possible assumptions about agent behavior.\n\nRecognizing these limitations we explore alternatives to standard\nmicrofoundations for binary classification. We start by describing a set of\ndesiderata that help navigate the space of possible assumptions about how\nagents respond to a decision rule. In particular, we analyze a natural\nconstraint on feature manipulations, and discuss properties that are sufficient\nto guarantee the robust existence of stable points. Building on these insights,\nwe then propose the noisy response model. Inspired by smoothed analysis and\nempirical observations, noisy response incorporates imperfection in the agent\nresponses, which we show mitigates the limitations of standard\nmicrofoundations. Our model retains analytical tractability, leads to more\nrobust insights about stable points, and imposes a lower social burden at\noptimality.",
          "link": "http://arxiv.org/abs/2106.12705",
          "publishedOn": "2021-06-25T02:00:47.330Z",
          "wordCount": 695,
          "title": "Alternative Microfoundations for Strategic Classification. (arXiv:2106.12705v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>",
          "description": "We analyze the process of creating word embedding feature representations\ndesigned for a learning task when annotated data is scarce, for example, in\ndepressive language detection from Tweets. We start with a rich word embedding\npre-trained from a large general dataset, which is then augmented with\nembeddings learned from a much smaller and more specific domain dataset through\na simple non-linear mapping mechanism. We also experimented with several other\nmore sophisticated methods of such mapping including, several auto-encoder\nbased and custom loss-function based methods that learn embedding\nrepresentations through gradually learning to be close to the words of similar\nsemantics and distant to dissimilar semantics. Our strengthened representations\nbetter capture the semantics of the depression domain, as it combines the\nsemantics learned from the specific domain coupled with word coverage from the\ngeneral language. We also present a comparative performance analyses of our\nword embedding representations with a simple bag-of-words model, well known\nsentiment and psycholinguistic lexicons, and a general pre-trained word\nembedding. When used as feature representations for several different machine\nlearning methods, including deep learning models in a depressive Tweets\nidentification task, we show that our augmented word embedding representations\nachieve a significantly better F1 score than the others, specially when applied\nto a high quality dataset. Also, we present several data ablation tests which\nconfirm the efficacy of our augmentation techniques.",
          "link": "http://arxiv.org/abs/2106.12797",
          "publishedOn": "2021-06-25T02:00:47.325Z",
          "wordCount": 707,
          "title": "A comprehensive empirical analysis on cross-domain semantic enrichment for detection of depressive language. (arXiv:2106.12797v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allmendinger_R/0/1/0/all/0/1\">Richard Allmendinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Ibanez_M/0/1/0/all/0/1\">Manuel L&#xf3;pez-Ib&#xe1;&#xf1;ez</a>",
          "description": "Safe learning and optimization deals with learning and optimization problems\nthat avoid, as much as possible, the evaluation of non-safe input points, which\nare solutions, policies, or strategies that cause an irrecoverable loss (e.g.,\nbreakage of a machine or equipment, or life threat). Although a comprehensive\nsurvey of safe reinforcement learning algorithms was published in 2015, a\nnumber of new algorithms have been proposed thereafter, and related works in\nactive learning and in optimization were not considered. This paper reviews\nthose algorithms from a number of domains including reinforcement learning,\nGaussian process regression and classification, evolutionary algorithms, and\nactive learning. We provide the fundamental concepts on which the reviewed\nalgorithms are based and a characterization of the individual algorithms. We\nconclude by explaining how the algorithms are connected and suggestions for\nfuture research.",
          "link": "http://arxiv.org/abs/2101.09505",
          "publishedOn": "2021-06-25T02:00:47.287Z",
          "wordCount": 672,
          "title": "Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art. (arXiv:2101.09505v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>",
          "description": "In this work, we propose FastDPM, a unified framework for fast sampling in\ndiffusion probabilistic models. FastDPM generalizes previous methods and gives\nrise to new algorithms with improved sample quality. We systematically\ninvestigate the fast sampling methods under this framework across different\ndomains, on different datasets, and with different amount of conditional\ninformation provided for generation. We find the performance of a particular\nmethod depends on data domains (e.g., image or audio), the trade-off between\nsampling speed and sample quality, and the amount of conditional information.\nWe further provide insights and recipes on the choice of methods for\npractitioners.",
          "link": "http://arxiv.org/abs/2106.00132",
          "publishedOn": "2021-06-25T02:00:47.272Z",
          "wordCount": 544,
          "title": "On Fast Sampling of Diffusion Probabilistic Models. (arXiv:2106.00132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We propose a reparametrization scheme to address the challenges of applying\ndifferentially private SGD on large neural networks, which are 1) the huge\nmemory cost of storing individual gradients, 2) the added noise suffering\nnotorious dimensional dependence. Specifically, we reparametrize each weight\nmatrix with two \\emph{gradient-carrier} matrices of small dimension and a\n\\emph{residual weight} matrix. We argue that such reparametrization keeps the\nforward/backward process unchanged while enabling us to compute the projected\ngradient without computing the gradient itself. To learn with differential\nprivacy, we design \\emph{reparametrized gradient perturbation (RGP)} that\nperturbs the gradients on gradient-carrier matrices and reconstructs an update\nfor the original weight from the noisy gradients. Importantly, we use\nhistorical updates to find the gradient-carrier matrices, whose optimality is\nrigorously justified under linear regression and empirically verified with deep\nlearning tasks. RGP significantly reduces the memory cost and improves the\nutility. For example, we are the first able to apply differential privacy on\nthe BERT model and achieve an average accuracy of $83.9\\%$ on four downstream\ntasks with $\\epsilon=8$, which is within $5\\%$ loss compared to the non-private\nbaseline but enjoys much lower privacy leakage risk.",
          "link": "http://arxiv.org/abs/2106.09352",
          "publishedOn": "2021-06-25T02:00:47.261Z",
          "wordCount": 661,
          "title": "Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-25T02:00:47.230Z",
          "wordCount": 645,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yunhao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuno_T/0/1/0/all/0/1\">Tadashi Kozuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1\">Mark Rowland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1\">R&#xe9;mi Munos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1\">Michal Valko</a>",
          "description": "Model-agnostic meta-reinforcement learning requires estimating the Hessian\nmatrix of value functions. This is challenging from an implementation\nperspective, as repeatedly differentiating policy gradient estimates may lead\nto biased Hessian estimates. In this work, we provide a unifying framework for\nestimating higher-order derivatives of value functions, based on off-policy\nevaluation. Our framework interprets a number of prior approaches as special\ncases and elucidates the bias and variance trade-off of Hessian estimates. This\nframework also opens the door to a new family of estimates, which can be easily\nimplemented with auto-differentiation libraries, and lead to performance gains\nin practice.",
          "link": "http://arxiv.org/abs/2106.13125",
          "publishedOn": "2021-06-25T02:00:47.219Z",
          "wordCount": 531,
          "title": "Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation. (arXiv:2106.13125v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agudelo_Espana_D/0/1/0/all/0/1\">Diego Agudelo-Espa&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemmour_Y/0/1/0/all/0/1\">Yassine Nemmour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia-Jie Zhu</a>",
          "description": "Random features is a powerful universal function approximator that inherits\nthe theoretical rigor of kernel methods and can scale up to modern learning\ntasks. This paper views uncertain system models as unknown or uncertain smooth\nfunctions in universal reproducing kernel Hilbert spaces. By directly\napproximating the one-step dynamics function using random features with\nuncertain parameters, which are equivalent to a shallow Bayesian neural\nnetwork, we then view the whole dynamical system as a multi-layer neural\nnetwork. Exploiting the structure of Hamiltonian dynamics, we show that finding\nworst-case dynamics realizations using Pontryagin's minimum principle is\nequivalent to performing the Frank-Wolfe algorithm on the deep net. Various\nnumerical experiments on dynamics learning showcase the capacity of our\nmodeling methodology.",
          "link": "http://arxiv.org/abs/2106.13066",
          "publishedOn": "2021-06-25T02:00:47.188Z",
          "wordCount": 565,
          "title": "Shallow Representation is Deep: Learning Uncertainty-aware and Worst-case Random Feature Dynamics. (arXiv:2106.13066v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13109",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1\">Chenru Duan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chen_S/0/1/0/all/0/1\">Shuxin Chen</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Taylor_M/0/1/0/all/0/1\">Michael G. Taylor</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kulik_H/0/1/0/all/0/1\">Heather J. Kulik</a>",
          "description": "Computational virtual high-throughput screening (VHTS) with density\nfunctional theory (DFT) and machine-learning (ML)-acceleration is essential in\nrapid materials discovery. By necessity, efficient DFT-based workflows are\ncarried out with a single density functional approximation (DFA). Nevertheless,\nproperties evaluated with different DFAs can be expected to disagree for the\ncases with challenging electronic structure (e.g., open shell transition metal\ncomplexes, TMCs) for which rapid screening is most needed and accurate\nbenchmarks are often unavailable. To quantify the effect of DFA bias, we\nintroduce an approach to rapidly obtain property predictions from 23\nrepresentative DFAs spanning multiple families and \"rungs\" (e.g., semi-local to\ndouble hybrid) and basis sets on over 2,000 TMCs. Although computed properties\n(e.g., spin-state ordering and frontier orbital gap) naturally differ by DFA,\nhigh linear correlations persist across all DFAs. We train independent ML\nmodels for each DFA and observe convergent trends in feature importance; these\nfeatures thus provide DFA-invariant, universal design rules. We devise a\nstrategy to train ML models informed by all 23 DFAs and use them to predict\nproperties (e.g., spin-splitting energy) of over 182k TMCs. By requiring\nconsensus of the ANN-predicted DFA properties, we improve correspondence of\nthese computational lead compounds with literature-mined, experimental\ncompounds over the single-DFA approach typically employed. Both feature\nanalysis and consensus-based ML provide efficient, alternative paths to\novercome accuracy limitations of practical DFT.",
          "link": "http://arxiv.org/abs/2106.13109",
          "publishedOn": "2021-06-25T02:00:47.183Z",
          "wordCount": 680,
          "title": "Machine learning to tame divergent density functional approximations: a new path to consensus materials design principles. (arXiv:2106.13109v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xueyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingcai Guo</a>",
          "description": "We study the recent emerging personalized federated learning (PFL) that aims\nat dealing with the challenging problem of Non-I.I.D. data in the federated\nlearning (FL) setting. The key difference between PFL and conventional FL lies\nin the training target, of which the personalized models in PFL usually pursue\na trade-off between personalization (i.e., usually from local models) and\ngeneralization (i.e., usually from the global model) on trained models.\nConventional FL methods can hardly meet this target because of their both\nwell-developed global and local models. The prevalent PFL approaches usually\nmaintain a global model to guide the training process of local models and\ntransfer a proper degree of generalization to them. However, the sole global\nmodel can only provide one direction of generalization and may even transfer\nnegative effects to some local models when rich statistical diversity exists\nacross multiple local datasets. Based on our observation, most real or\nsynthetic data distributions usually tend to be clustered to some degree, of\nwhich we argue different directions of generalization can facilitate the PFL.\nIn this paper, we propose a novel concept called clustered generalization to\nhandle the challenge of statistical heterogeneity in FL. Specifically, we\nmaintain multiple global (generalized) models in the server to associate with\nthe corresponding amount of local model clusters in clients, and further\nformulate the PFL as a bi-level optimization problem that can be solved\nefficiently and robustly. We also conduct detailed theoretical analysis and\nprovide the convergence guarantee for the smooth non-convex objectives.\nExperimental results on both synthetic and real datasets show that our approach\nsurpasses the state-of-the-art by a significant margin.",
          "link": "http://arxiv.org/abs/2106.13044",
          "publishedOn": "2021-06-25T02:00:47.172Z",
          "wordCount": 697,
          "title": "Personalized Federated Learning with Clustered Generalization. (arXiv:2106.13044v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andr&#xe9; Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1\">Diana Borsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shaobo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aygun_E/0/1/0/all/0/1\">Eser Ayg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamel_P/0/1/0/all/0/1\">Philippe Hamel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1\">Daniel Toyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunt_J/0/1/0/all/0/1\">Jonathan Hunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourad_S/0/1/0/all/0/1\">Shibl Mourad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1\">David Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "The ability to combine known skills to create new ones may be crucial in the\nsolution of complex reinforcement learning problems that unfold over extended\nperiods. We argue that a robust way of combining skills is to define and\nmanipulate them in the space of pseudo-rewards (or \"cumulants\"). Based on this\npremise, we propose a framework for combining skills using the formalism of\noptions. We show that every deterministic option can be unambiguously\nrepresented as a cumulant defined in an extended domain. Building on this\ninsight and on previous results on transfer learning, we show how to\napproximate options whose cumulants are linear combinations of the cumulants of\nknown options. This means that, once we have learned options associated with a\nset of cumulants, we can instantaneously synthesise options induced by any\nlinear combination of them, without any learning involved. We describe how this\nframework provides a hierarchical interface to the environment whose abstract\nactions correspond to combinations of basic skills. We demonstrate the\npractical benefits of our approach in a resource management problem and a\nnavigation task involving a quadrupedal simulated robot.",
          "link": "http://arxiv.org/abs/2106.13105",
          "publishedOn": "2021-06-25T02:00:47.167Z",
          "wordCount": 636,
          "title": "The Option Keyboard: Combining Skills in Reinforcement Learning. (arXiv:2106.13105v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13041",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>",
          "description": "Understanding the 3D world from 2D projected natural images is a fundamental\nchallenge in computer vision and graphics. Recently, an unsupervised learning\napproach has garnered considerable attention owing to its advantages in data\ncollection. However, to mitigate training limitations, typical methods need to\nimpose assumptions for viewpoint distribution (e.g., a dataset containing\nvarious viewpoint images) or object shape (e.g., symmetric objects). These\nassumptions often restrict applications; for instance, the application to\nnon-rigid objects or images captured from similar viewpoints (e.g., flower or\nbird images) remains a challenge. To complement these approaches, we propose\naperture rendering generative adversarial networks (AR-GANs), which equip\naperture rendering on top of GANs, and adopt focus cues to learn the depth and\ndepth-of-field (DoF) effect of unlabeled natural images. To address the\nambiguities triggered by unsupervised setting (i.e., ambiguities between smooth\ntexture and out-of-focus blurs, and between foreground and background blurs),\nwe develop DoF mixture learning, which enables the generator to learn real\nimage distribution while generating diverse DoF images. In addition, we devise\na center focus prior to guiding the learning direction. In the experiments, we\ndemonstrate the effectiveness of AR-GANs in various datasets, such as flower,\nbird, and face images, demonstrate their portability by incorporating them into\nother 3D representation learning GANs, and validate their applicability in\nshallow DoF rendering.",
          "link": "http://arxiv.org/abs/2106.13041",
          "publishedOn": "2021-06-25T02:00:47.161Z",
          "wordCount": 689,
          "title": "Unsupervised Learning of Depth and Depth-of-Field Effect from Natural Images with Aperture Rendering Generative Adversarial Networks. (arXiv:2106.13041v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13081",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadiya_S/0/1/0/all/0/1\">S Sadiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alhanai_T/0/1/0/all/0/1\">T Alhanai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghassemi_M/0/1/0/all/0/1\">MM Ghassemi</a>",
          "description": "Electroencephalography (EEG) has countless applications across many of\nfields. However, EEG applications are limited by low signal-to-noise ratios.\nMultiple types of artifacts contribute to the noisiness of EEG, and many\ntechniques have been proposed to detect and correct these artifacts. These\ntechniques range from simply detecting and rejecting artifact ridden segments,\nto extracting the noise component from the EEG signal. In this paper we review\na variety of recent and classical techniques for EEG data artifact detection\nand correction with a focus on the last half-decade. We compare the strengths\nand weaknesses of the approaches and conclude with proposed future directions\nfor the field.",
          "link": "http://arxiv.org/abs/2106.13081",
          "publishedOn": "2021-06-25T02:00:47.145Z",
          "wordCount": 563,
          "title": "Artifact Detection and Correction in EEG data: A Review. (arXiv:2106.13081v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plawinski_J/0/1/0/all/0/1\">Jason Plawinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1\">Sajanth Subramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Readie_A/0/1/0/all/0/1\">Aimee Readie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ligozio_G/0/1/0/all/0/1\">Gregory Ligozio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohlssen_D/0/1/0/all/0/1\">David Ohlssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baillie_M/0/1/0/all/0/1\">Mark Baillie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coroller_T/0/1/0/all/0/1\">Thibaud Coroller</a>",
          "description": "Sharing data from clinical studies can facilitate innovative data-driven\nresearch and ultimately lead to better public health. However, sharing\nbiomedical data can put sensitive personal information at risk. This is usually\nsolved by anonymization, which is a slow and expensive process. An alternative\nto anonymization is sharing a synthetic dataset that bears a behaviour similar\nto the real data but preserves privacy. As part of the collaboration between\nNovartis and the Oxford Big Data Institute, we generate a synthetic dataset\nbased on COSENTYX (secukinumab) Ankylosing Spondylitis (AS) clinical study. We\napply an Auxiliary Classifier GAN (ac-GAN) to generate synthetic magnetic\nresonance images (MRIs) of vertebral units (VUs). The images are conditioned on\nthe VU location (cervical, thoracic and lumbar). In this paper, we present a\nmethod for generating a synthetic dataset and conduct an in-depth analysis on\nits properties of along three key metrics: image fidelity, sample diversity and\ndataset privacy.",
          "link": "http://arxiv.org/abs/2106.13199",
          "publishedOn": "2021-06-25T02:00:47.140Z",
          "wordCount": 606,
          "title": "A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs. (arXiv:2106.13199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1\">Christopher J. Anders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_D/0/1/0/all/0/1\">David Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>",
          "description": "Deep Neural Networks (DNNs) are known to be strong predictors, but their\nprediction strategies can rarely be understood. With recent advances in\nExplainable Artificial Intelligence, approaches are available to explore the\nreasoning behind those complex models' predictions. One class of approaches are\npost-hoc attribution methods, among which Layer-wise Relevance Propagation\n(LRP) shows high performance. However, the attempt at understanding a DNN's\nreasoning often stops at the attributions obtained for individual samples in\ninput space, leaving the potential for deeper quantitative analyses untouched.\nAs a manual analysis without the right tools is often unnecessarily labor\nintensive, we introduce three software packages targeted at scientists to\nexplore model reasoning using attribution approaches and beyond: (1) Zennit - a\nhighly customizable and intuitive attribution framework implementing LRP and\nrelated approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly\nconstruct quantitative analysis pipelines for dataset-wide analyses of\nexplanations, and (3) ViRelAy - a web-application to interactively explore\ndata, attributions, and analysis results.",
          "link": "http://arxiv.org/abs/2106.13200",
          "publishedOn": "2021-06-25T02:00:47.135Z",
          "wordCount": 613,
          "title": "Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy. (arXiv:2106.13200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12996",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhro Ghosh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1\">Philippe Rigollet</a>",
          "description": "Motivated by cutting-edge applications like cryo-electron microscopy\n(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an\nunknown signal from repeated measurements of its images under the latent action\nof a group of isometries and additive noise of magnitude $\\sigma$. Despite\nsignificant interest, a clear picture for understanding rates of estimation in\nthis model has emerged only recently, particularly in the high-noise regime\n$\\sigma \\gg 1$ that is highly relevant in applications. Recent investigations\nhave revealed a remarkable asymptotic sample complexity of order $\\sigma^6$ for\ncertain signals whose Fourier transforms have full support, in stark contrast\nto the traditional $\\sigma^2$ that arise in regular models. Often prohibitively\nlarge in practice, these results have prompted the investigation of variations\naround the MRA model where better sample complexity may be achieved. In this\npaper, we show that \\emph{sparse} signals exhibit an intermediate $\\sigma^4$\nsample complexity even in the classical MRA model. Our results explore and\nexploit connections of the MRA estimation problem with two classical topics in\napplied mathematics: the \\textit{beltway problem} from combinatorial\noptimization, and \\textit{uniform uncertainty principles} from harmonic\nanalysis.",
          "link": "http://arxiv.org/abs/2106.12996",
          "publishedOn": "2021-06-25T02:00:47.129Z",
          "wordCount": 629,
          "title": "Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Leveraging the framework of Optimal Transport, we introduce a new family of\ngenerative autoencoders with a learnable prior, called Symmetric Wasserstein\nAutoencoders (SWAEs). We propose to symmetrically match the joint distributions\nof the observed data and the latent representation induced by the encoder and\nthe decoder. The resulting algorithm jointly optimizes the modelling losses in\nboth the data and the latent spaces with the loss in the data space leading to\nthe denoising effect. With the symmetric treatment of the data and the latent\nrepresentation, the algorithm implicitly preserves the local structure of the\ndata in the latent space. To further improve the quality of the latent\nrepresentation, we incorporate a reconstruction loss into the objective, which\nsignificantly benefits both the generation and reconstruction. We empirically\nshow the superior performance of SWAEs over the state-of-the-art generative\nautoencoders in terms of classification, reconstruction, and generation.",
          "link": "http://arxiv.org/abs/2106.13024",
          "publishedOn": "2021-06-25T02:00:47.123Z",
          "wordCount": 575,
          "title": "Symmetric Wasserstein Autoencoders. (arXiv:2106.13024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morrison_K/0/1/0/all/0/1\">Katelyn Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilby_B/0/1/0/all/0/1\">Benjamin Gilby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipchak_C/0/1/0/all/0/1\">Colton Lipchak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattioli_A/0/1/0/all/0/1\">Adam Mattioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>",
          "description": "Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.",
          "link": "http://arxiv.org/abs/2106.13122",
          "publishedOn": "2021-06-25T02:00:47.107Z",
          "wordCount": 593,
          "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers. (arXiv:2106.13122v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13082",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Rosenbaum_R/0/1/0/all/0/1\">Robert Rosenbaum</a>",
          "description": "In this manuscript, I review and extend recent work on the relationship\nbetween predictive coding and backpropagation for training artificial neural\nnetworks on supervised learning tasks. I also discuss some implications of\nthese results for the interpretation of predictive coding and deep neural\nnetworks as models of biological learning and I describe a repository of\nfunctions, Torch2PC, for performing predictive coding with PyTorch neural\nnetwork models.",
          "link": "http://arxiv.org/abs/2106.13082",
          "publishedOn": "2021-06-25T02:00:47.102Z",
          "wordCount": 502,
          "title": "On the relationship between predictive coding and backpropagation. (arXiv:2106.13082v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Terrance Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_A/0/1/0/all/0/1\">Anna Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1\">Michal Muszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_R/0/1/0/all/0/1\">Ryo Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brent_D/0/1/0/all/0/1\">David Brent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>",
          "description": "Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.",
          "link": "http://arxiv.org/abs/2106.13213",
          "publishedOn": "2021-06-25T02:00:47.092Z",
          "wordCount": 656,
          "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data. (arXiv:2106.13213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13194",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bubnova_A/0/1/0/all/0/1\">Anna V. Bubnova</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deeva_I/0/1/0/all/0/1\">Irina Deeva</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>",
          "description": "This paper describes a new library for learning Bayesian networks from data\ncontaining discrete and continuous variables (mixed data). In addition to the\nclassical learning methods on discretized data, this library proposes its\nalgorithm that allows structural learning and parameters learning from mixed\ndata without discretization since data discretization leads to information\nloss. This algorithm based on mixed MI score function for structural learning,\nand also linear regression and Gaussian distribution approximation for\nparameters learning. The library also offers two algorithms for enumerating\ngraph structures - the greedy Hill-Climbing algorithm and the evolutionary\nalgorithm. Thus the key capabilities of the proposed library are as follows:\n(1) structural and parameters learning of a Bayesian network on discretized\ndata, (2) structural and parameters learning of a Bayesian network on mixed\ndata using the MI mixed score function and Gaussian approximation, (3)\nlaunching learning algorithms on one of two algorithms for enumerating graph\nstructures - Hill-Climbing and the evolutionary algorithm. Since the need for\nmixed data representation comes from practical necessity, the advantages of our\nimplementations are evaluated in the context of solving approximation and gap\nrecovery problems on synthetic data and real datasets.",
          "link": "http://arxiv.org/abs/2106.13194",
          "publishedOn": "2021-06-25T02:00:47.087Z",
          "wordCount": 627,
          "title": "MIxBN: library for learning Bayesian networks from mixed data. (arXiv:2106.13194v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tirinzoni_A/0/1/0/all/0/1\">Andrea Tirinzoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1\">Matteo Pirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>",
          "description": "We derive a novel asymptotic problem-dependent lower-bound for regret\nminimization in finite-horizon tabular Markov Decision Processes (MDPs). While,\nsimilar to prior work (e.g., for ergodic MDPs), the lower-bound is the solution\nto an optimization problem, our derivation reveals the need for an additional\nconstraint on the visitation distribution over state-action pairs that\nexplicitly accounts for the dynamics of the MDP. We provide a characterization\nof our lower-bound through a series of examples illustrating how different MDPs\nmay have significantly different complexity. 1) We first consider a \"difficult\"\nMDP instance, where the novel constraint based on the dynamics leads to a\nlarger lower-bound (i.e., a larger regret) compared to the classical analysis.\n2) We then show that our lower-bound recovers results previously derived for\nspecific MDP instances. 3) Finally, we show that, in certain \"simple\" MDPs, the\nlower bound is considerably smaller than in the general case and it does not\nscale with the minimum action gap at all. We show that this last result is\nattainable (up to $poly(H)$ terms, where $H$ is the horizon) by providing a\nregret upper-bound based on policy gaps for an optimistic algorithm.",
          "link": "http://arxiv.org/abs/2106.13013",
          "publishedOn": "2021-06-25T02:00:47.072Z",
          "wordCount": 619,
          "title": "A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs. (arXiv:2106.13013v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pogodin_R/0/1/0/all/0/1\">Roman Pogodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_Y/0/1/0/all/0/1\">Yash Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1\">Timothy P. Lillicrap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latham_P/0/1/0/all/0/1\">Peter E. Latham</a>",
          "description": "Convolutional networks are ubiquitous in deep learning. They are particularly\nuseful for images, as they reduce the number of parameters, reduce training\ntime, and increase accuracy. However, as a model of the brain they are\nseriously problematic, since they require weight sharing - something real\nneurons simply cannot do. Consequently, while neurons in the brain can be\nlocally connected (one of the features of convolutional networks), they cannot\nbe convolutional. Locally connected but non-convolutional networks, however,\nsignificantly underperform convolutional ones. This is troublesome for studies\nthat use convolutional networks to explain activity in the visual system. Here\nwe study plausible alternatives to weight sharing that aim at the same\nregularization principle, which is to make each neuron within a pool react\nsimilarly to identical inputs. The most natural way to do that is by showing\nthe network multiple translations of the same image, akin to saccades in animal\nvision. However, this approach requires many translations, and doesn't remove\nthe performance gap. We propose instead to add lateral connectivity to a\nlocally connected network, and allow learning via Hebbian plasticity. This\nrequires the network to pause occasionally for a sleep-like phase of \"weight\nsharing\". This method enables locally connected networks to achieve nearly\nconvolutional performance on ImageNet, thus supporting convolutional networks\nas a model of the visual stream.",
          "link": "http://arxiv.org/abs/2106.13031",
          "publishedOn": "2021-06-25T02:00:47.066Z",
          "wordCount": 655,
          "title": "Towards Biologically Plausible Convolutional Networks. (arXiv:2106.13031v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianlong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>",
          "description": "The adoption of electronic health records (EHR) has become universal during\nthe past decade, which has afforded in-depth data-based research. By learning\nfrom the large amount of healthcare data, various data-driven models have been\nbuilt to predict future events for different medical tasks, such as auto\ndiagnosis and heart-attack prediction. Although EHR is abundant, the population\nthat satisfies specific criteria for learning population-specific tasks is\nscarce, making it challenging to train data-hungry deep learning models. This\nstudy presents the Claim Pre-Training (Claim-PT) framework, a generic\npre-training model that first trains on the entire pediatric claims dataset,\nfollowed by a discriminative fine-tuning on each population-specific task. The\nsemantic meaning of medical events can be captured in the pre-training stage,\nand the effective knowledge transfer is completed through the task-aware\nfine-tuning stage. The fine-tuning process requires minimal parameter\nmodification without changing the model architecture, which mitigates the data\nscarcity issue and helps train the deep learning model adequately on small\npatient cohorts. We conducted experiments on a real-world claims dataset with\nmore than one million patient records. Experimental results on two downstream\ntasks demonstrated the effectiveness of our method: our general task-agnostic\npre-training framework outperformed tailored task-specific models, achieving\nmore than 10\\% higher in model performance as compared to baselines. In\naddition, our framework showed a great generalizability potential to transfer\nlearned knowledge from one institution to another, paving the way for future\nhealthcare model pre-training across institutions.",
          "link": "http://arxiv.org/abs/2106.13095",
          "publishedOn": "2021-06-25T02:00:47.059Z",
          "wordCount": 675,
          "title": "Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks. (arXiv:2106.13095v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1\">Mohammad Babaeizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1\">Mohammad Taghi Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1\">Dumitru Erhan</a>",
          "description": "An agent that is capable of predicting what happens next can perform a\nvariety of tasks through planning with no additional training. Furthermore,\nsuch an agent can internally represent the complex dynamics of the real-world\nand therefore can acquire a representation useful for a variety of visual\nperception tasks. This makes predicting the future frames of a video,\nconditioned on the observed past and potentially future actions, an interesting\ntask which remains exceptionally challenging despite many recent advances.\nExisting video prediction models have shown promising results on simple narrow\nbenchmarks but they generate low quality predictions on real-life datasets with\nmore complicated dynamics or broader domain. There is a growing body of\nevidence that underfitting on the training data is one of the primary causes\nfor the low quality predictions. In this paper, we argue that the inefficient\nuse of parameters in the current video models is the main reason for\nunderfitting. Therefore, we introduce a new architecture, named FitVid, which\nis capable of severe overfitting on the common benchmarks while having similar\nparameter count as the current state-of-the-art models. We analyze the\nconsequences of overfitting, illustrating how it can produce unexpected\noutcomes such as generating high quality output by repeating the training data,\nand how it can be mitigated using existing image augmentation techniques. As a\nresult, FitVid outperforms the current state-of-the-art models across four\ndifferent video prediction benchmarks on four different metrics.",
          "link": "http://arxiv.org/abs/2106.13195",
          "publishedOn": "2021-06-25T02:00:47.052Z",
          "wordCount": 675,
          "title": "FitVid: Overfitting in Pixel-Level Video Prediction. (arXiv:2106.13195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feretzakis_G/0/1/0/all/0/1\">Georgios Feretzakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlis_G/0/1/0/all/0/1\">George Karlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loupelis_E/0/1/0/all/0/1\">Evangelos Loupelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalles_D/0/1/0/all/0/1\">Dimitris Kalles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakou_R/0/1/0/all/0/1\">Rea Chatzikyriakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trakas_N/0/1/0/all/0/1\">Nikolaos Trakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakou_E/0/1/0/all/0/1\">Eugenia Karakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakagianni_A/0/1/0/all/0/1\">Aikaterini Sakagianni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelves_L/0/1/0/all/0/1\">Lazaros Tzelves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petropoulou_S/0/1/0/all/0/1\">Stavroula Petropoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tika_A/0/1/0/all/0/1\">Aikaterini Tika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalainas_I/0/1/0/all/0/1\">Ilias Dalainas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaldis_V/0/1/0/all/0/1\">Vasileios Kaldis</a>",
          "description": "Introduction: One of the most important tasks in the Emergency Department\n(ED) is to promptly identify the patients who will benefit from hospital\nadmission. Machine Learning (ML) techniques show promise as diagnostic aids in\nhealthcare. Material and methods: We investigated the following features\nseeking to investigate their performance in predicting hospital admission:\nserum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,\nC-Reactive Protein, Complete Blood Count with differential, Activated Partial\nThromboplastin Time, D Dimer, International Normalized Ratio, age, gender,\ntriage disposition to ED unit and ambulance utilization. A total of 3,204 ED\nvisits were analyzed. Results: The proposed algorithms generated models which\ndemonstrated acceptable performance in predicting hospital admission of ED\npatients. The range of F-measure and ROC Area values of all eight evaluated\nalgorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The\nmain advantages of this tool include easy access, availability, yes/no result,\nand low cost. The clinical implications of our approach might facilitate a\nshift from traditional clinical decision-making to a more sophisticated model.\nConclusion: Developing robust prognostic models with the utilization of common\nbiomarkers is a project that might shape the future of emergency medicine. Our\nfindings warrant confirmation with implementation in pragmatic ED trials.",
          "link": "http://arxiv.org/abs/2106.12921",
          "publishedOn": "2021-06-25T02:00:47.047Z",
          "wordCount": 661,
          "title": "Using machine learning techniques to predict hospital admission at the emergency department. (arXiv:2106.12921v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12961",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Yang_L/0/1/0/all/0/1\">Liping Yang</a>",
          "description": "In recent years, Bitcoin price prediction has attracted the interest of\nresearchers and investors. However, the accuracy of previous studies is not\nwell enough. Machine learning and deep learning methods have been proved to\nhave strong prediction ability in this area. This paper proposed a method\ncombined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning\nmethod called long short-term memory (LSTM) to research the problem of next-day\nBitcoin price forecast.",
          "link": "http://arxiv.org/abs/2106.12961",
          "publishedOn": "2021-06-25T02:00:47.032Z",
          "wordCount": 503,
          "title": "Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods. (arXiv:2106.12961v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12954",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Ziqing Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "End-to-end optimization capability offers neural image compression (NIC)\nsuperior lossy compression performance. However, distinct models are required\nto be trained to reach different points in the rate-distortion (R-D) space. In\nthis paper, we consider the problem of R-D characteristic analysis and modeling\nfor NIC. We make efforts to formulate the essential mathematical functions to\ndescribe the R-D behavior of NIC using deep network and statistical modeling.\nThus continuous bit-rate points could be elegantly realized by leveraging such\nmodel via a single trained network. In this regard, we propose a plugin-in\nmodule to learn the relationship between the target bit-rate and the binary\nrepresentation for the latent variable of auto-encoder. Furthermore, we model\nthe rate and distortion characteristic of NIC as a function of the coding\nparameter $\\lambda$ respectively. Our experiments show our proposed method is\neasy to adopt and obtains competitive coding performance with fixed-rate coding\napproaches, which would benefit the practical deployment of NIC. In addition,\nthe proposed model could be applied to NIC rate control with limited bit-rate\nerror using a single network.",
          "link": "http://arxiv.org/abs/2106.12954",
          "publishedOn": "2021-06-25T02:00:47.022Z",
          "wordCount": 630,
          "title": "Rate Distortion Characteristic Modeling for Neural Image Compression. (arXiv:2106.12954v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13035",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zanetti_A/0/1/0/all/0/1\">Andrea Zanetti</a>",
          "description": "Pre-trained language models like Ernie or Bert are currently used in many\napplications. These models come with a set of pre-trained weights typically\nobtained in unsupervised/self-supervised modality on a huge amount of data.\nAfter that, they are fine-tuned on a specific task. Applications then use these\nmodels for inference, and often some additional constraints apply, like low\npower-budget or low latency between input and output. The main avenue to meet\nthese additional requirements for the inference settings, is to use low\nprecision computation (e.g. INT8 rather than FP32), but this comes with a cost\nof deteriorating the functional performance (e.g. accuracy) of the model. Some\napproaches have been developed to tackle the problem and go beyond the\nlimitations of the PTO (Post-Training Quantization), more specifically the QAT\n(Quantization Aware Training, see [4]) is a procedure that interferes with the\ntraining process in order to make it affected (or simply disturbed) by the\nquantization phase during the training itself. Besides QAT, recently\nIntel-Habana Labs have proposed an additional and more direct way to make the\ntraining results more robust to subsequent quantization which uses a\nregularizer, therefore changing the loss function that drives the training\nprocedure. But their proposal does not work out-of-the-box for pre-trained\nmodels like Ernie, for example. In this short paper we show why this is not\nhappening (for the Ernie case) and we propose a very basic way to deal with it,\nsharing as well some initial results (increase in final INT8 accuracy) that\nmight be of interest to practitioners willing to use Ernie in their\napplications, in low precision regime.",
          "link": "http://arxiv.org/abs/2106.13035",
          "publishedOn": "2021-06-25T02:00:47.016Z",
          "wordCount": 701,
          "title": "Quantization Aware Training, ERNIE and Kurtosis Regularizer: a short empirical study. (arXiv:2106.13035v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13055",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lorsung_C/0/1/0/all/0/1\">Cooper Lorsung</a>",
          "description": "Neural Linear Models (NLM) are deep Bayesian models that produce predictive\nuncertainty by learning features from the data and then performing Bayesian\nlinear regression over these features. Despite their popularity, few works have\nfocused on formally evaluating the predictive uncertainties of these models.\nFurthermore, existing works point out the difficulties of encoding domain\nknowledge in models like NLMs, making them unsuitable for applications where\ninterpretability is required. In this work, we show that traditional training\nprocedures for NLMs can drastically underestimate uncertainty in data-scarce\nregions. We identify the underlying reasons for this behavior and propose a\nnovel training method that can both capture useful predictive uncertainties as\nwell as allow for incorporation of domain knowledge.",
          "link": "http://arxiv.org/abs/2106.13055",
          "publishedOn": "2021-06-25T02:00:47.010Z",
          "wordCount": 547,
          "title": "Understanding Uncertainty in Bayesian Deep Learning. (arXiv:2106.13055v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1\">Stefanos Leonardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1\">Georgios Piliouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spendlove_K/0/1/0/all/0/1\">Kelly Spendlove</a>,",
          "description": "The interplay between exploration and exploitation in competitive multi-agent\nlearning is still far from being well understood. Motivated by this, we study\nsmooth Q-learning, a prototypical learning model that explicitly captures the\nbalance between game rewards and exploration costs. We show that Q-learning\nalways converges to the unique quantal-response equilibrium (QRE), the standard\nsolution concept for games under bounded rationality, in weighted zero-sum\npolymatrix games with heterogeneous learning agents using positive exploration\nrates. Complementing recent results about convergence in weighted potential\ngames, we show that fast convergence of Q-learning in competitive settings is\nobtained regardless of the number of agents and without any need for parameter\nfine-tuning. As showcased by our experiments in network zero-sum games, these\ntheoretical results provide the necessary guarantees for an algorithmic\napproach to the currently open problem of equilibrium selection in competitive\nmulti-agent settings.",
          "link": "http://arxiv.org/abs/2106.12928",
          "publishedOn": "2021-06-25T02:00:47.000Z",
          "wordCount": 601,
          "title": "Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality. (arXiv:2106.12928v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1\">James Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbster_M/0/1/0/all/0/1\">Mark Herbster</a>",
          "description": "We address the problem of sequential prediction with expert advice in a\nnon-stationary environment with long-term memory guarantees in the sense of\nBousquet and Warmuth [4]. We give a linear-time algorithm that improves on the\nbest known regret bounds [26]. This algorithm incorporates a relative entropy\nprojection step. This projection is advantageous over previous weight-sharing\napproaches in that weight updates may come with implicit costs as in for\nexample portfolio optimization. We give an algorithm to compute this projection\nstep in linear time, which may be of independent interest.",
          "link": "http://arxiv.org/abs/2106.13021",
          "publishedOn": "2021-06-25T02:00:46.983Z",
          "wordCount": 516,
          "title": "Improved Regret Bounds for Tracking Experts with Memory. (arXiv:2106.13021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roster_K/0/1/0/all/0/1\">Kirstin Roster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_F/0/1/0/all/0/1\">Francisco A. Rodrigues</a>",
          "description": "Due to a lack of treatments and universal vaccine, early forecasts of Dengue\nare an important tool for disease control. Neural networks are powerful\npredictive models that have made contributions to many areas of public health.\nIn this systematic review, we provide an introduction to the neural networks\nrelevant to Dengue forecasting and review their applications in the literature.\nThe objective is to help inform model design for future work. Following the\nPRISMA guidelines, we conduct a systematic search of studies that use neural\nnetworks to forecast Dengue in human populations. We summarize the relative\nperformance of neural networks and comparator models, model architectures and\nhyper-parameters, as well as choices of input features. Nineteen papers were\nincluded. Most studies implement shallow neural networks using historical\nDengue incidence and meteorological input features. Prediction horizons tend to\nbe short. Building on the strengths of neural networks, most studies use\ngranular observations at the city or sub-national level. Performance of neural\nnetworks relative to comparators such as Support Vector Machines varies across\nstudy contexts. The studies suggest that neural networks can provide good\npredictions of Dengue and should be included in the set of candidate models.\nThe use of convolutional, recurrent, or deep networks is relatively unexplored\nbut offers promising avenues for further research, as does the use of a broader\nset of input features such as social media or mobile phone data.",
          "link": "http://arxiv.org/abs/2106.12905",
          "publishedOn": "2021-06-25T02:00:46.967Z",
          "wordCount": 670,
          "title": "Neural Networks for Dengue Prediction: A Systematic Review. (arXiv:2106.12905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12981",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cairoli_F/0/1/0/all/0/1\">Francesca Cairoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1\">Ginevra Carbone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1\">Luca Bortolussi</a>",
          "description": "Markov Population Models are a widespread formalism used to model the\ndynamics of complex systems, with applications in Systems Biology and many\nother fields. The associated Markov stochastic process in continuous time is\noften analyzed by simulation, which can be costly for large or stiff systems,\nparticularly when a massive number of simulations has to be performed (e.g. in\na multi-scale model). A strategy to reduce computational load is to abstract\nthe population model, replacing it with a simpler stochastic model, faster to\nsimulate. Here we pursue this idea, building on previous works and constructing\na generator capable of producing stochastic trajectories in continuous space\nand discrete time. This generator is learned automatically from simulations of\nthe original model in a Generative Adversarial setting. Compared to previous\nworks, which rely on deep neural networks and Dirichlet processes, we explore\nthe use of state of the art generative models, which are flexible enough to\nlearn a full trajectory rather than a single transition kernel.",
          "link": "http://arxiv.org/abs/2106.12981",
          "publishedOn": "2021-06-25T02:00:46.958Z",
          "wordCount": 593,
          "title": "Abstraction of Markov Population Dynamics via Generative Adversarial Nets. (arXiv:2106.12981v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martins_F/0/1/0/all/0/1\">Felipe B. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1\">Mateus G. Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braga_P/0/1/0/all/0/1\">Pedro H. M. Braga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1\">Edna S. Barros</a>",
          "description": "Reinforcement learning is an active research area with a vast number of\napplications in robotics, and the RoboCup competition is an interesting\nenvironment for studying and evaluating reinforcement learning methods. A known\ndifficulty in applying reinforcement learning to robotics is the high number of\nexperience samples required, being the use of simulated environments for\ntraining the agents followed by transfer learning to real-world (sim-to-real) a\nviable path. This article introduces an open-source simulator for the IEEE Very\nSmall Size Soccer and the Small Size League optimized for reinforcement\nlearning experiments. We also propose a framework for creating OpenAI Gym\nenvironments with a set of benchmarks tasks for evaluating single-agent and\nmulti-agent robot soccer skills. We then demonstrate the learning capabilities\nof two state-of-the-art reinforcement learning methods as well as their\nlimitations in certain scenarios introduced in this framework. We believe this\nwill make it easier for more teams to compete in these categories using\nend-to-end reinforcement learning approaches and further develop this research\narea.",
          "link": "http://arxiv.org/abs/2106.12895",
          "publishedOn": "2021-06-25T02:00:46.948Z",
          "wordCount": 623,
          "title": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer. (arXiv:2106.12895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barumerli_R/0/1/0/all/0/1\">Roberto Barumerli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_D/0/1/0/all/0/1\">Daniele Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geronazzo_M/0/1/0/all/0/1\">Michele Geronazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avanzini_F/0/1/0/all/0/1\">Federico Avanzini</a>",
          "description": "This paper introduces a shoebox room simulator able to systematically\ngenerate synthetic datasets of binaural room impulse responses (BRIRs) given an\narbitrary set of head-related transfer functions (HRTFs). The evaluation of\nmachine hearing algorithms frequently requires BRIR datasets in order to\nsimulate the acoustics of any environment. However, currently available\nsolutions typically consider only HRTFs measured on dummy heads, which poorly\ncharacterize the high variability in spatial sound perception. Our solution\nallows to integrate a room impulse response (RIR) simulator with different HRTF\nsets represented in Spatially Oriented Format for Acoustics (SOFA). The source\ncode and the compiled binaries for different operating systems allow to both\nadvanced and non-expert users to benefit from our toolbox, see\nhttps://github.com/spatialaudiotools/sofamyroom/ .",
          "link": "http://arxiv.org/abs/2106.12992",
          "publishedOn": "2021-06-25T02:00:46.931Z",
          "wordCount": 587,
          "title": "SofaMyRoom: a fast and multiplatform \"shoebox\" room simulator for binaural room impulse response dataset generation. (arXiv:2106.12992v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soni_B/0/1/0/all/0/1\">Badal Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakuria_D/0/1/0/all/0/1\">Debangan Thakuria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_N/0/1/0/all/0/1\">Nilutpal Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Navarun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boro_B/0/1/0/all/0/1\">Bhaskarananda Boro</a>",
          "description": "Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users' penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users' watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.",
          "link": "http://arxiv.org/abs/2106.12970",
          "publishedOn": "2021-06-25T02:00:46.924Z",
          "wordCount": 644,
          "title": "RikoNet: A Novel Anime Recommendation Engine. (arXiv:2106.12970v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12923",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1\">Jun-Kun Wang</a>",
          "description": "In the first part of this dissertation research, we develop a modular\nframework that can serve as a recipe for constructing and analyzing iterative\nalgorithms for convex optimization. Specifically, our work casts optimization\nas iteratively playing a two-player zero-sum game. Many existing optimization\nalgorithms including Frank-Wolfe and Nesterov's acceleration methods can be\nrecovered from the game by pitting two online learners with appropriate\nstrategies against each other. Furthermore, the sum of the weighted average\nregrets of the players in the game implies the convergence rate. As a result,\nour approach provides simple alternative proofs to these algorithms. Moreover,\nwe demonstrate that our approach of optimization as iteratively playing a game\nleads to three new fast Frank-Wolfe-like algorithms for some constraint sets,\nwhich further shows that our framework is indeed generic, modular, and\neasy-to-use.\n\nIn the second part, we develop a modular analysis of provable acceleration\nvia Polyak's momentum for certain problems, which include solving the classical\nstrongly quadratic convex problems, training a wide ReLU network under the\nneural tangent kernel regime, and training a deep linear network with an\northogonal initialization. We develop a meta theorem and show that when\napplying Polyak's momentum for these problems, the induced dynamics exhibit a\nform where we can directly apply our meta theorem.\n\nIn the last part of the dissertation, we show another advantage of the use of\nPolyak's momentum -- it facilitates fast saddle point escape in smooth\nnon-convex optimization. This result, together with those of the second part,\nsheds new light on Polyak's momentum in modern non-convex optimization and deep\nlearning.",
          "link": "http://arxiv.org/abs/2106.12923",
          "publishedOn": "2021-06-25T02:00:46.916Z",
          "wordCount": 709,
          "title": "Understanding Modern Techniques in Optimization: Frank-Wolfe, Nesterov's Momentum, and Polyak's Momentum. (arXiv:2106.12923v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1\">Stephan Sloth Lorenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Mads Nielsen</a>",
          "description": "The information bottleneck (IB) principle has been suggested as a way to\nanalyze deep neural networks. The learning dynamics are studied by inspecting\nthe mutual information (MI) between the hidden layers and the input and output.\nNotably, separate fitting and compression phases during training have been\nreported. This led to some controversy including claims that the observations\nare not reproducible and strongly dependent on the type of activation function\nused as well as on the way the MI is estimated. Our study confirms that\ndifferent ways of binning when computing the MI lead to qualitatively different\nresults, either supporting or refusing IB conjectures. To resolve the\ncontroversy, we study the IB principle in settings where MI is non-trivial and\ncan be computed exactly. We monitor the dynamics of quantized neural networks,\nthat is, we discretize the whole deep learning system so that no approximation\nis required when computing the MI. This allows us to quantify the information\nflow without measurement errors. In this setting, we observed a fitting phase\nfor all layers and a compression phase for the output layer in all experiments;\nthe compression in the hidden layers was dependent on the type of activation\nfunction. Our study shows that the initial IB results were not artifacts of\nbinning when computing the MI. However, the critical claim that the compression\nphase may not be observed for some networks also holds true.",
          "link": "http://arxiv.org/abs/2106.12912",
          "publishedOn": "2021-06-25T02:00:46.910Z",
          "wordCount": 662,
          "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks. (arXiv:2106.12912v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solbiati_A/0/1/0/all/0/1\">Alessandro Solbiati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damaskinos_G/0/1/0/all/0/1\">Georgios Damaskinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Shivani Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_S/0/1/0/all/0/1\">Shubham Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cali_J/0/1/0/all/0/1\">Jacques Cali</a>",
          "description": "Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.",
          "link": "http://arxiv.org/abs/2106.12978",
          "publishedOn": "2021-06-25T02:00:46.903Z",
          "wordCount": 519,
          "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings. (arXiv:2106.12978v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qingqing Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guojie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kunqing Xie</a>",
          "description": "Spatial-temporal forecasting has attracted tremendous attention in a wide\nrange of applications, and traffic flow prediction is a canonical and typical\nexample. The complex and long-range spatial-temporal correlations of traffic\nflow bring it to a most intractable challenge. Existing works typically utilize\nshallow graph convolution networks (GNNs) and temporal extracting modules to\nmodel spatial and temporal dependencies respectively. However, the\nrepresentation ability of such models is limited due to: (1) shallow GNNs are\nincapable to capture long-range spatial correlations, (2) only spatial\nconnections are considered and a mass of semantic connections are ignored,\nwhich are of great importance for a comprehensive understanding of traffic\nnetworks. To this end, we propose Spatial-Temporal Graph Ordinary Differential\nEquation Networks (STGODE). Specifically, we capture spatial-temporal dynamics\nthrough a tensor-based ordinary differential equation (ODE), as a result,\ndeeper networks can be constructed and spatial-temporal features are utilized\nsynchronously. To understand the network more comprehensively, semantical\nadjacency matrix is considered in our model, and a well-design temporal\ndialated convolution structure is used to capture long term temporal\ndependencies. We evaluate our model on multiple real-world traffic datasets and\nsuperior performance is achieved over state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.12931",
          "publishedOn": "2021-06-25T02:00:46.897Z",
          "wordCount": 620,
          "title": "Spatial-Temporal Graph ODE Networks for Traffic Flow Forecasting. (arXiv:2106.12931v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rathee_M/0/1/0/all/0/1\">Mandeep Rathee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funke_T/0/1/0/all/0/1\">Thorben Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosla_M/0/1/0/all/0/1\">Megha Khosla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Graph neural networks (GNNs) have achieved great success on various tasks and\nfields that require relational modeling. GNNs aggregate node features using the\ngraph structure as inductive biases resulting in flexible and powerful models.\nHowever, GNNs remain hard to interpret as the interplay between node features\nand graph structure is only implicitly learned. In this paper, we propose a\nnovel method called Kedge for explicitly sparsifying the underlying graph by\nremoving unnecessary neighbors. Our key idea is based on a tractable method for\nsparsification using the Hard Kumaraswamy distribution that can be used in\nconjugation with any GNN model. Kedge learns edge masks in a modular fashion\ntrained with any GNN allowing for gradient based optimization in an end-to-end\nfashion. We demonstrate through extensive experiments that our model Kedge can\nprune a large proportion of the edges with only a minor effect on the test\naccuracy. Specifically, in the PubMed dataset, Kedge learns to drop more than\n80% of the edges with an accuracy drop of merely 2% showing that graph\nstructure has only a small contribution in comparison to node features.\nFinally, we also show that Kedge effectively counters the over-smoothing\nphenomena in deep GNNs by maintaining good task performance with increasing GNN\nlayers.",
          "link": "http://arxiv.org/abs/2106.12920",
          "publishedOn": "2021-06-25T02:00:46.881Z",
          "wordCount": 641,
          "title": "Learnt Sparsification for Interpretable Graph Neural Networks. (arXiv:2106.12920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Abraham_K/0/1/0/all/0/1\">Kweku Abraham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naulet_Z/0/1/0/all/0/1\">Zacharie Naulet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1\">Elisabeth Gassiat</a>",
          "description": "We study the frontier between learnable and unlearnable hidden Markov models\n(HMMs). HMMs are flexible tools for clustering dependent data coming from\nunknown populations. The model parameters are known to be identifiable as soon\nas the clusters are distinct and the hidden chain is ergodic with a full rank\ntransition matrix. In the limit as any one of these conditions fails, it\nbecomes impossible to identify parameters. For a chain with two hidden states\nwe prove nonasymptotic minimax upper and lower bounds, matching up to\nconstants, which exhibit thresholds at which the parameters become learnable.",
          "link": "http://arxiv.org/abs/2106.12936",
          "publishedOn": "2021-06-25T02:00:46.876Z",
          "wordCount": 537,
          "title": "Fundamental limits for learning hidden Markov model parameters. (arXiv:2106.12936v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-06-25T02:00:46.870Z",
          "wordCount": 620,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raahil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokora_K/0/1/0/all/0/1\">Kamil Pokora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzerg_A/0/1/0/all/0/1\">Abdelhamid Ezzerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimkov_V/0/1/0/all/0/1\">Viacheslav Klimkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huybrechts_G/0/1/0/all/0/1\">Goeric Huybrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putrycz_B/0/1/0/all/0/1\">Bartosz Putrycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korzekwa_D/0/1/0/all/0/1\">Daniel Korzekwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merritt_T/0/1/0/all/0/1\">Thomas Merritt</a>",
          "description": "Whilst recent neural text-to-speech (TTS) approaches produce high-quality\nspeech, they typically require a large amount of recordings from the target\nspeaker. In previous work, a 3-step method was proposed to generate\nhigh-quality TTS while greatly reducing the amount of data required for\ntraining. However, we have observed a ceiling effect in the level of\nnaturalness achievable for highly expressive voices when using this approach.\nIn this paper, we present a method for building highly expressive TTS voices\nwith as little as 15 minutes of speech data from the target speaker. Compared\nto the current state-of-the-art approach, our proposed improvements close the\ngap to recordings by 23.3% for naturalness of speech and by 16.3% for speaker\nsimilarity. Further, we match the naturalness and speaker similarity of a\nTacotron2-based full-data (~10 hours) model using only 15 minutes of target\nspeaker data, whereas with 30 minutes or more, we significantly outperform it.\nThe following improvements are proposed: 1) changing from an autoregressive,\nattention-based TTS model to a non-autoregressive model replacing attention\nwith an external duration model and 2) an additional Conditional Generative\nAdversarial Network (cGAN) based fine-tuning step.",
          "link": "http://arxiv.org/abs/2106.12896",
          "publishedOn": "2021-06-25T02:00:46.856Z",
          "wordCount": 647,
          "title": "Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech. (arXiv:2106.12896v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12974",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1\">Sujie Li</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1\">Jiang Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1\">Pan Zhang</a>",
          "description": "Modeling the joint distribution of high-dimensional data is a central task in\nunsupervised machine learning. In recent years, many interests have been\nattracted to developing learning models based on tensor networks, which have\nadvantages of theoretical understandings of the expressive power using\nentanglement properties, and as a bridge connecting the classical computation\nand the quantum computation. Despite the great potential, however, existing\ntensor-network-based unsupervised models only work as a proof of principle, as\ntheir performances are much worse than the standard models such as the\nrestricted Boltzmann machines and neural networks. In this work, we present the\nAutoregressive Matrix Product States (AMPS), a tensor-network-based model\ncombining the matrix product states from quantum many-body physics and the\nautoregressive models from machine learning. The model enjoys exact calculation\nof normalized probability and unbiased sampling, as well as a clear theoretical\nunderstanding of expressive power. We demonstrate the performance of our model\nusing two applications, the generative modeling on synthetic and real-world\ndata, and the reinforcement learning in statistical physics. Using extensive\nnumerical experiments, we show that the proposed model significantly\noutperforms the existing tensor-network-based models and the restricted\nBoltzmann machines, and is competitive with the state-of-the-art neural network\nmodels.",
          "link": "http://arxiv.org/abs/2106.12974",
          "publishedOn": "2021-06-25T02:00:46.851Z",
          "wordCount": 651,
          "title": "Tensor networks for unsupervised machine learning. (arXiv:2106.12974v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12985",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1\">Kamaladdin Fataliyev</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1\">Aneesh Chivukula</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1\">Mukesh Prasad</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Stock market movements are influenced by public and private information\nshared through news articles, company reports, and social media discussions.\nAnalyzing these vast sources of data can give market participants an edge to\nmake profit. However, the majority of the studies in the literature are based\non traditional approaches that come short in analyzing unstructured, vast\ntextual data. In this study, we provide a review on the immense amount of\nexisting literature of text-based stock market analysis. We present input data\ntypes and cover main textual data sources and variations. Feature\nrepresentation techniques are then presented. Then, we cover the analysis\ntechniques and create a taxonomy of the main stock market forecast models.\nImportantly, we discuss representative work in each category of the taxonomy,\nanalyzing their respective contributions. Finally, this paper shows the\nfindings on unaddressed open problems and gives suggestions for future work.\nThe aim of this study is to survey the main stock market analysis models, text\nrepresentation techniques for financial market prediction, shortcomings of\nexisting techniques, and propose promising directions for future research.",
          "link": "http://arxiv.org/abs/2106.12985",
          "publishedOn": "2021-06-25T02:00:46.845Z",
          "wordCount": 611,
          "title": "Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bertoin_D/0/1/0/all/0/1\">David Bertoin</a> (ISAE-SUPAERO), <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a> (UT1, TSE), <a href=\"http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1\">S&#xe9;bastien Gerchinovitz</a> (IMT), <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (CNRS, IRIT)",
          "description": "In theory, the choice of ReLU (0) in [0, 1] for a neural network has a\nnegligible influence both on backpropagation and training. Yet, in the real\nworld, 32 bits default precision combined with the size of deep learning\nproblems makes it a hyperparameter of training methods. We investigate the\nimportance of the value of ReLU (0) for several precision levels (16, 32, 64\nbits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST,\nCIFAR10, SVHN). We observe considerable variations of backpropagation outputs\nwhich occur around half of the time in 32 bits precision. The effect disappears\nwith double precision, while it is systematic at 16 bits. For vanilla SGD\ntraining, the choice ReLU (0) = 0 seems to be the most efficient. We also\nevidence that reconditioning approaches as batch-norm or ADAM tend to buffer\nthe influence of ReLU (0)'s value. Overall, the message we want to convey is\nthat algorithmic differentiation of nonsmooth problems potentially hides\nparameters that could be tuned advantageously.",
          "link": "http://arxiv.org/abs/2106.12915",
          "publishedOn": "2021-06-25T02:00:46.839Z",
          "wordCount": 600,
          "title": "Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossam_M/0/1/0/all/0/1\">Mahmoud Hossam</a>",
          "description": "Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.",
          "link": "http://arxiv.org/abs/2106.12942",
          "publishedOn": "2021-06-25T02:00:46.834Z",
          "wordCount": 715,
          "title": "High Performance Hyperspectral Image Classification using Graphics Processing Units. (arXiv:2106.12942v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12751",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1\">Xuanqing Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "Partition-based methods are increasingly-used in extreme multi-label\nclassification (XMC) problems due to their scalability to large output spaces\n(e.g., millions or more). However, existing methods partition the large label\nspace into mutually exclusive clusters, which is sub-optimal when labels have\nmulti-modality and rich semantics. For instance, the label \"Apple\" can be the\nfruit or the brand name, which leads to the following research question: can we\ndisentangle these multi-modal labels with non-exclusive clustering tailored for\ndownstream XMC tasks? In this paper, we show that the label assignment problem\nin partition-based XMC can be formulated as an optimization problem, with the\nobjective of maximizing precision rates. This leads to an efficient algorithm\nto form flexible and overlapped label clusters, and a method that can\nalternatively optimizes the cluster assignments and the model parameters for\npartition-based XMC. Experimental results on synthetic and real datasets show\nthat our method can successfully disentangle multi-modal labels, leading to\nstate-of-the-art (SOTA) results on four XMC benchmarks.",
          "link": "http://arxiv.org/abs/2106.12751",
          "publishedOn": "2021-06-25T02:00:46.827Z",
          "wordCount": 595,
          "title": "Label Disentanglement in Partition-based Extreme Multilabel Classification. (arXiv:2106.12751v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wegmann_M/0/1/0/all/0/1\">Marc Wegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zipperling_D/0/1/0/all/0/1\">Domenique Zipperling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillenbrand_J/0/1/0/all/0/1\">Jonas Hillenbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>",
          "description": "Data analysis plays an indispensable role for value creation in industry.\nCluster analysis in this context is able to explore given datasets with little\nor no prior knowledge and to identify unknown patterns. As (big) data\ncomplexity increases in the dimensions volume, variety, and velocity, this\nbecomes even more important. Many tools for cluster analysis have been\ndeveloped from early on and the variety of different clustering algorithms is\nhuge. As the selection of the right clustering procedure is crucial to the\nresults of the data analysis, users are in need for support on their journey of\nextracting knowledge from raw data. Thus, the objective of this paper lies in\nthe identification of a systematic selection logic for clustering algorithms\nand corresponding validation concepts. The goal is to enable potential users to\nchoose an algorithm that fits best to their needs and the properties of their\nunderlying data clustering problem. Moreover, users are supported in selecting\nthe right validation concepts to make sense of the clustering results. Based on\na comprehensive literature review, this paper provides assessment criteria for\nclustering method evaluation and validation concept selection. The criteria are\napplied to several common algorithms and the selection process of an algorithm\nis supported by the introduction of pseudocode-based routines that consider the\nunderlying data structure.",
          "link": "http://arxiv.org/abs/2106.12792",
          "publishedOn": "2021-06-25T02:00:46.816Z",
          "wordCount": 658,
          "title": "A review of systematic selection of clustering algorithms and their evaluation. (arXiv:2106.12792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_T/0/1/0/all/0/1\">Thomas Viehmann</a>",
          "description": "With the rise of machine learning and deep learning based applications in\npractice, monitoring, i.e. verifying that these operate within specification,\nhas become an important practical problem. An important aspect of this\nmonitoring is to check whether the inputs (or intermediates) have strayed from\nthe distribution they were validated for, which can void the performance\nassurances obtained during testing.\n\nThere are two common approaches for this. The, perhaps, more classical one is\noutlier detection or novelty detection, where, for a single input we ask\nwhether it is an outlier, i.e. exceedingly unlikely to have originated from a\nreference distribution. The second, perhaps more recent approach, is to\nconsider a larger number of inputs and compare its distribution to a reference\ndistribution (e.g. sampled during testing). This is done under the label drift\ndetection.\n\nIn this work, we bridge the gap between outlier detection and drift detection\nthrough comparing a given number of inputs to an automatically chosen part of\nthe reference distribution.",
          "link": "http://arxiv.org/abs/2106.12893",
          "publishedOn": "2021-06-25T02:00:46.789Z",
          "wordCount": 609,
          "title": "Partial Wasserstein and Maximum Mean Discrepancy distances for bridging the gap between outlier detection and drift detection. (arXiv:2106.12893v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1\">Samy Bengio</a>",
          "description": "Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.",
          "link": "http://arxiv.org/abs/2106.02795",
          "publishedOn": "2021-06-25T02:00:46.778Z",
          "wordCount": 598,
          "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding. (arXiv:2106.02795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haixu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiehui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the \\textit{long-term forecasting} problem of time\nseries. Prior Transformer-based models adopt various self-attention mechanisms\nto discover the long-range dependencies. However, intricate temporal patterns\nof the long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Towards these challenges, we propose Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We go\nbeyond the pre-processing convention of series decomposition and renovate it as\na basic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease.",
          "link": "http://arxiv.org/abs/2106.13008",
          "publishedOn": "2021-06-25T02:00:46.763Z",
          "wordCount": 624,
          "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1\">Wesley J. Maddox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1\">Maximilian Balandat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshy_E/0/1/0/all/0/1\">Eytan Bakshy</a>",
          "description": "Bayesian Optimization is a sample-efficient black-box optimization procedure\nthat is typically applied to problems with a small number of independent\nobjectives. However, in practice we often wish to optimize objectives defined\nover many correlated outcomes (or ``tasks\"). For example, scientists may want\nto optimize the coverage of a cell tower network across a dense grid of\nlocations. Similarly, engineers may seek to balance the performance of a robot\nacross dozens of different environments via constrained or robust optimization.\nHowever, the Gaussian Process (GP) models typically used as probabilistic\nsurrogates for multi-task Bayesian Optimization scale poorly with the number of\noutcomes, greatly limiting applicability. We devise an efficient technique for\nexact multi-task GP sampling that combines exploiting Kronecker structure in\nthe covariance matrices with Matheron's identity, allowing us to perform\nBayesian Optimization using exact multi-task GP models with tens of thousands\nof correlated outputs. In doing so, we achieve substantial improvements in\nsample efficiency compared to existing approaches that only model aggregate\nfunctions of the outcomes. We demonstrate how this unlocks a new class of\napplications for Bayesian Optimization across a range of tasks in science and\nengineering, including optimizing interference patterns of an optical\ninterferometer with more than 65,000 outputs.",
          "link": "http://arxiv.org/abs/2106.12997",
          "publishedOn": "2021-06-25T02:00:46.752Z",
          "wordCount": 634,
          "title": "Bayesian Optimization with High-Dimensional Outputs. (arXiv:2106.12997v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zengyi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuchu Fan</a>",
          "description": "We study constrained reinforcement learning (CRL) from a novel perspective by\nsetting constraints directly on state density functions, rather than the value\nfunctions considered by previous works. State density has a clear physical and\nmathematical interpretation, and is able to express a wide variety of\nconstraints such as resource limits and safety requirements. Density\nconstraints can also avoid the time-consuming process of designing and tuning\ncost functions required by value function-based constraints to encode system\nspecifications. We leverage the duality between density functions and Q\nfunctions to develop an effective algorithm to solve the density constrained RL\nproblem optimally and the constrains are guaranteed to be satisfied. We prove\nthat the proposed algorithm converges to a near-optimal solution with a bounded\nerror even when the policy update is imperfect. We use a set of comprehensive\nexperiments to demonstrate the advantages of our approach over state-of-the-art\nCRL methods, with a wide range of density constrained tasks as well as standard\nCRL benchmarks such as Safety-Gym.",
          "link": "http://arxiv.org/abs/2106.12764",
          "publishedOn": "2021-06-25T02:00:46.744Z",
          "wordCount": 596,
          "title": "Density Constrained Reinforcement Learning. (arXiv:2106.12764v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_B/0/1/0/all/0/1\">Bryan E. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenz_J/0/1/0/all/0/1\">Juan A. Saenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonnewald_M/0/1/0/all/0/1\">Maike Sonnewald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_D/0/1/0/all/0/1\">Daniel Livescu</a>",
          "description": "The advent of big data has vast potential for discovery in natural phenomena\nranging from climate science to medicine, but overwhelming complexity stymies\ninsight. Existing theory is often not able to succinctly describe salient\nphenomena, and progress has largely relied on ad hoc definitions of dynamical\nregimes to guide and focus exploration. We present a formal definition in which\nthe identification of dynamical regimes is formulated as an optimization\nproblem, and we propose an intelligible objective function. Furthermore, we\npropose an unsupervised learning framework which eliminates the need for a\npriori knowledge and ad hoc definitions; instead, the user need only choose\nappropriate clustering and dimensionality reduction algorithms, and this choice\ncan be guided using our proposed objective function. We illustrate its\napplicability with example problems drawn from ocean dynamics, tumor\nangiogenesis, and turbulent boundary layers. Our method is a step towards\nunbiased data exploration that allows serendipitous discovery within dynamical\nsystems, with the potential to propel the physical sciences forward.",
          "link": "http://arxiv.org/abs/2106.12963",
          "publishedOn": "2021-06-25T02:00:46.739Z",
          "wordCount": 621,
          "title": "Objective discovery of dominant dynamical processes with intelligible machine learning. (arXiv:2106.12963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evensen_S/0/1/0/all/0/1\">Sara Evensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>",
          "description": "Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.",
          "link": "http://arxiv.org/abs/2106.12767",
          "publishedOn": "2021-06-25T02:00:46.712Z",
          "wordCount": 660,
          "title": "TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration. (arXiv:2106.12767v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "In federated learning (FL), model training is distributed over clients and\nlocal models are aggregated by a central server. The performance of uploaded\nmodels in such situations can vary widely due to imbalanced data distributions,\npotential demands on privacy protections, and quality of transmissions. In this\npaper, we aim to minimize FL training delay over wireless channels, constrained\nby overall training performance as well as each client's differential privacy\n(DP) requirement. We solve this problem in the framework of multi-agent\nmulti-armed bandit (MAMAB) to deal with the situation where there are multiple\nclients confornting different unknown transmission environments, e.g., channel\nfading and interferences. Specifically, we first transform the long-term\nconstraints on both training performance and each client's DP into a virtual\nqueue based on the Lyapunov drift technique. Then, we convert the MAMAB to a\nmax-min bipartite matching problem at each communication round, by estimating\nrewards with the upper confidence bound (UCB) approach. More importantly, we\npropose two efficient solutions to this matching problem, i.e., modified\nHungarian algorithm and greedy matching with a better alternative (GMBA), in\nwhich the first one can achieve the optimal solution with a high complexity\nwhile the second one approaches a better trade-off by enabling a verified\nlow-complexity with little performance loss. In addition, we develop an upper\nbound on the expected regret of this MAMAB based FL framework, which shows a\nlinear growth over the logarithm of communication rounds, justifying its\ntheoretical feasibility. Extensive experimental results are conducted to\nvalidate the effectiveness of our proposed algorithms, and the impacts of\nvarious parameters on the FL performance over wireless edge networks are also\ndiscussed.",
          "link": "http://arxiv.org/abs/2106.13039",
          "publishedOn": "2021-06-25T02:00:46.696Z",
          "wordCount": 720,
          "title": "Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dymond_J/0/1/0/all/0/1\">Jack Dymond</a>",
          "description": "When machine learning models encounter data which is out of the distribution\non which they were trained they have a tendency to behave poorly, most\nprominently over-confidence in erroneous predictions. Such behaviours will have\ndisastrous effects on real-world machine learning systems. In this field\ngraceful degradation refers to the optimisation of model performance as it\nencounters this out-of-distribution data. This work presents a definition and\ndiscussion of graceful degradation and where it can be applied in deployed\nvisual systems. Following this a survey of relevant areas is undertaken,\nnovelly splitting the graceful degradation problem into active and passive\napproaches. In passive approaches, graceful degradation is handled and achieved\nby the model in a self-contained manner, in active approaches the model is\nupdated upon encountering epistemic uncertainties. This work communicates the\nimportance of the problem and aims to prompt the development of machine\nlearning strategies that are aware of graceful degradation.",
          "link": "http://arxiv.org/abs/2106.11119",
          "publishedOn": "2021-06-25T02:00:46.682Z",
          "wordCount": 592,
          "title": "Graceful Degradation and Related Fields. (arXiv:2106.11119v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13086",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanhao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshimura_N/0/1/0/all/0/1\">Natsue Yoshimura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koike_Y/0/1/0/all/0/1\">Yasuharu Koike</a>",
          "description": "The Partial Least Square Regression (PLSR) algorithm exhibits exceptional\ncompetence for predicting continuous variables from inter-correlated brain\nrecordings in brain-computer interfaces, which achieved successful prediction\nfrom epidural electrocorticography of macaques to three-dimensional continuous\nhand trajectories recently. Nevertheless, PLSR is in essence formulated based\non the least square criterion, thus, being non-robust with respect to\ncomplicated noises consequently. The aim of the present study is to propose a\nrobust version of PLSR. To this end, the maximum correntropy criterion is\nadopted to structure a new robust variant of PLSR, namely Partial Maximum\nCorrentropy Regression (PMCR). Half-quadratic optimization technique is\nutilized to calculate the robust latent variables. We assess the proposed PMCR\non a synthetic example and the public Neurotycho dataset. Compared with the\nconventional PLSR and the state-of-the-art variant, PMCR realized superior\nprediction competence on three different performance indicators with\ncontaminated training set. The proposed PMCR was demonstrated as an effective\napproach for robust decoding from noisy brain measurements, which could reduce\nthe performance degradation resulting from adverse noises, thus, improving the\ndecoding robustness of brain-computer interfaces.",
          "link": "http://arxiv.org/abs/2106.13086",
          "publishedOn": "2021-06-25T02:00:46.676Z",
          "wordCount": 626,
          "title": "Partial Maximum Correntropy Regression for Robust Trajectory Decoding from Noisy Epidural Electrocorticographic Signals. (arXiv:2106.13086v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2009.04899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingyuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun-Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimoukha_I/0/1/0/all/0/1\">Imad Jaimoukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>",
          "description": "In this paper, we aim to address the problem of solving a non-convex\noptimization problem over an intersection of multiple variable sets. This kind\nof problems is typically solved by using an alternating minimization (AM)\nstrategy which splits the overall problem into a set of sub-problems\ncorresponding to each variable, and then iteratively performs minimization over\neach sub-problem using a fixed updating rule. However, due to the intrinsic\nnon-convexity of the overall problem, the optimization can usually be trapped\ninto bad local minimum even when each sub-problem can be globally optimized at\neach iteration. To tackle this problem, we propose a meta-learning based Global\nScope Optimization (GSO) method. It adaptively generates optimizers for\nsub-problems via meta-learners and constantly updates these meta-learners with\nrespect to the global loss information of the overall problem. Therefore, the\nsub-problems are optimized with the objective of minimizing the global loss\nspecifically. We evaluate the proposed model on a number of simulations,\nincluding solving bi-linear inverse problems: matrix completion, and non-linear\nproblems: Gaussian mixture models. The experimental results show that our\nproposed approach outperforms AM-based methods in standard settings, and is\nable to achieve effective optimization in some challenging cases while other\nmethods would typically fail.",
          "link": "http://arxiv.org/abs/2009.04899",
          "publishedOn": "2021-06-25T02:00:46.653Z",
          "wordCount": 691,
          "title": "Meta-learning for Multi-variable Non-convex Optimization Problems: Iterating Non-optimums Makes Optimum Possible. (arXiv:2009.04899v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12639",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1\">Robin Schmucker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salinas_D/0/1/0/all/0/1\">David Salinas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>",
          "description": "Hyperparameter optimization (HPO) is increasingly used to automatically tune\nthe predictive performance (e.g., accuracy) of machine learning models.\nHowever, in a plethora of real-world applications, accuracy is only one of the\nmultiple -- often conflicting -- performance criteria, necessitating the\nadoption of a multi-objective (MO) perspective. While the literature on MO\noptimization is rich, few prior studies have focused on HPO. In this paper, we\npropose algorithms that extend asynchronous successive halving (ASHA) to the MO\nsetting. Considering multiple evaluation metrics, we assess the performance of\nthese methods on three real world tasks: (i) Neural architecture search, (ii)\nalgorithmic fairness and (iii) language model optimization. Our empirical\nanalysis shows that MO ASHA enables to perform MO HPO at scale. Further, we\nobserve that that taking the entire Pareto front into account for candidate\nselection consistently outperforms multi-fidelity HPO based on MO scalarization\nin terms of wall-clock time. Our algorithms (to be open-sourced) establish new\nbaselines for future research in the area.",
          "link": "http://arxiv.org/abs/2106.12639",
          "publishedOn": "2021-06-25T02:00:46.633Z",
          "wordCount": 590,
          "title": "Multi-objective Asynchronous Successive Halving. (arXiv:2106.12639v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuning Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1\">Anusha Nagabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "The ability to plan into the future while utilizing only raw high-dimensional\nobservations, such as images, can provide autonomous agents with broad\ncapabilities. Visual model-based reinforcement learning (RL) methods that plan\nfuture actions directly have shown impressive results on tasks that require\nonly short-horizon reasoning, however, these methods struggle on temporally\nextended tasks. We argue that it is easier to solve long-horizon tasks by\nplanning sequences of states rather than just actions, as the effects of\nactions greatly compound over time and are harder to optimize. To achieve this,\nwe draw on the idea of collocation, which has shown good results on\nlong-horizon tasks in optimal control literature, and adapt it to the\nimage-based setting by utilizing learned latent state space models. The\nresulting latent collocation method (LatCo) optimizes trajectories of latent\nstates, which improves over previously proposed shooting methods for visual\nmodel-based RL on tasks with sparse rewards and long-term goals. Videos and\ncode at https://orybkin.github.io/latco/.",
          "link": "http://arxiv.org/abs/2106.13229",
          "publishedOn": "2021-06-25T02:00:46.581Z",
          "wordCount": 611,
          "title": "Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-25T02:00:46.576Z",
          "wordCount": 631,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "This work studies the statistical limits of uniform convergence for offline\npolicy evaluation (OPE) problems with model-based methods (for episodic MDP)\nand provides a unified framework towards optimal learning for several\nwell-motivated offline tasks. Uniform OPE\n$\\sup_\\Pi|Q^\\pi-\\hat{Q}^\\pi|<\\epsilon$ is a stronger measure than the\npoint-wise OPE and ensures offline learning when $\\Pi$ contains all policies\n(the global class). In this paper, we establish an $\\Omega(H^2\nS/d_m\\epsilon^2)$ lower bound (over model-based family) for the global uniform\nOPE and our main result establishes an upper bound of\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ for the \\emph{local} uniform convergence that\napplies to all \\emph{near-empirically optimal} policies for the MDPs with\n\\emph{stationary} transition. Here $d_m$ is the minimal marginal state-action\nprobability. Critically, the highlight in achieving the optimal rate\n$\\tilde{O}(H^2/d_m\\epsilon^2)$ is our design of \\emph{singleton absorbing MDP},\nwhich is a new sharp analysis tool that works with the model-based approach. We\ngeneralize such a model-based framework to the new settings: offline\ntask-agnostic and the offline reward-free with optimal complexity\n$\\tilde{O}(H^2\\log(K)/d_m\\epsilon^2)$ ($K$ is the number of tasks) and\n$\\tilde{O}(H^2S/d_m\\epsilon^2)$ respectively. These results provide a unified\nsolution for simultaneously solving different offline RL problems.",
          "link": "http://arxiv.org/abs/2105.06029",
          "publishedOn": "2021-06-25T02:00:46.570Z",
          "wordCount": 665,
          "title": "Optimal Uniform OPE and Model-based Offline Reinforcement Learning in Time-Homogeneous, Reward-Free and Task-Agnostic Settings. (arXiv:2105.06029v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1\">Cameron Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_M/0/1/0/all/0/1\">Michael Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1\">Tim Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1\">Matthew Riemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1\">Gerald Tesauro</a>",
          "description": "The difficulty of deterministic planning increases exponentially with\nsearch-tree depth. Black-box planning presents an even greater challenge, since\nplanners must operate without an explicit model of the domain. Heuristics can\nmake search more efficient, but goal-aware heuristics for black-box planning\nusually rely on goal counting, which is often quite uninformative. In this\nwork, we show how to overcome this limitation by discovering macro-actions that\nmake the goal-count heuristic more accurate. Our approach searches for\nmacro-actions with focused effects (i.e. macros that modify only a small number\nof state variables), which align well with the assumptions made by the\ngoal-count heuristic. Focused macros dramatically improve black-box planning\nefficiency across a wide range of planning domains, sometimes beating even\nstate-of-the-art planners with access to a full domain model.",
          "link": "http://arxiv.org/abs/2004.13242",
          "publishedOn": "2021-06-25T02:00:46.565Z",
          "wordCount": 620,
          "title": "Efficient Black-Box Planning Using Macro-Actions with Focused Effects. (arXiv:2004.13242v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1\">Dimiter Dobrev</a>",
          "description": "We will reduce the task of creating AI to the task of finding an appropriate\nlanguage for description of the world. This will not be a programing language\nbecause programing languages describe only computable functions, while our\nlanguage will describe a somewhat broader class of functions. Another\nspecificity of this language will be that the description will consist of\nseparate modules. This will enable us look for the description of the world\nautomatically such that we discover it module after module. Our approach to the\ncreation of this new language will be to start with a particular world and\nwrite the description of that particular world. The point is that the language\nwhich can describe this particular world will be appropriate for describing any\nworld.",
          "link": "http://arxiv.org/abs/2010.16243",
          "publishedOn": "2021-06-25T02:00:46.547Z",
          "wordCount": 581,
          "title": "Language for Description of Worlds. (arXiv:2010.16243v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Howe Seng Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sin_K/0/1/0/all/0/1\">Kai Ling Sin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kelly Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nicole Ka Hei Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_X/0/1/0/all/0/1\">Xin Yu Liew</a>",
          "description": "The intention of this research is to study and design an automated\nagriculture commodity price prediction system with novel machine learning\ntechniques. Due to the increasing large amounts historical data of agricultural\ncommodity prices and the need of performing accurate prediction of price\nfluctuations, the solution has largely shifted from statistical methods to\nmachine learning area. However, the selection of proper set from historical\ndata for forecasting still has limited consideration. On the other hand, when\nimplementing machine learning techniques, finding a suitable model with optimal\nparameters for global solution, nonlinearity and avoiding curse of\ndimensionality are still biggest challenges, therefore machine learning\nstrategies study are needed. In this research, we propose a web-based automated\nsystem to predict agriculture commodity price. In the two series experiments,\nfive popular machine learning algorithms, ARIMA, SVR, Prophet, XGBoost and LSTM\nhave been compared with large historical datasets in Malaysia and the most\noptimal algorithm, LSTM model with an average of 0.304 mean-square error has\nbeen selected as the prediction engine of the proposed system.",
          "link": "http://arxiv.org/abs/2106.12747",
          "publishedOn": "2021-06-25T02:00:46.541Z",
          "wordCount": 628,
          "title": "Automated Agriculture Commodity Price Prediction System with Machine Learning Techniques. (arXiv:2106.12747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12864",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Johann Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_M/0/1/0/all/0/1\">Mingtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+BasheerBennamoun/0/1/0/all/0/1\">BasheerBennamoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyuan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Juan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_L/0/1/0/all/0/1\">Lin Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1\">Syed Afaq Ali Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>",
          "description": "The astounding success made by artificial intelligence (AI) in healthcare and\nother fields proves that AI can achieve human-like performance. However,\nsuccess always comes with challenges. Deep learning algorithms are\ndata-dependent and require large datasets for training. The lack of data in the\nmedical imaging field creates a bottleneck for the application of deep learning\nto medical image analysis. Medical image acquisition, annotation, and analysis\nare costly, and their usage is constrained by ethical restrictions. They also\nrequire many resources, such as human expertise and funding. That makes it\ndifficult for non-medical researchers to have access to useful and large\nmedical data. Thus, as comprehensive as possible, this paper provides a\ncollection of medical image datasets with their associated challenges for deep\nlearning research. We have collected information of around three hundred\ndatasets and challenges mainly reported between 2013 and 2020 and categorized\nthem into four categories: head & neck, chest & abdomen, pathology & blood, and\n``others''. Our paper has three purposes: 1) to provide a most up to date and\ncomplete list that can be used as a universal reference to easily find the\ndatasets for clinical image analysis, 2) to guide researchers on the\nmethodology to test and evaluate their methods' performance and robustness on\nrelevant datasets, 3) to provide a ``route'' to relevant algorithms for the\nrelevant medical topics, and challenge leaderboards.",
          "link": "http://arxiv.org/abs/2106.12864",
          "publishedOn": "2021-06-25T02:00:46.535Z",
          "wordCount": 708,
          "title": "A Systematic Collection of Medical Image Datasets for Deep Learning. (arXiv:2106.12864v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Understanding and explaining the mistakes made by trained models is critical\nto many machine learning objectives, such as improving robustness, addressing\nconcept drift, and mitigating biases. However, this is often an ad hoc process\nthat involves manually looking at the model's mistakes on many test samples and\nguessing at the underlying reasons for those incorrect predictions. In this\npaper, we propose a systematic approach, conceptual explanation scores (CES),\nthat explains why a classifier makes a mistake on a particular test sample(s)\nin terms of human-understandable concepts (e.g. this zebra is misclassified as\na dog because of faint stripes). We base CES on two prior ideas: counterfactual\nexplanations and concept activation vectors, and validate our approach on\nwell-known pretrained models, showing that it explains the models' mistakes\nmeaningfully. We also train new models with intentional and known spurious\ncorrelations, which CES successfully identifies from a single misclassified\ntest sample. The code for CES is publicly available and can easily be applied\nto new models.",
          "link": "http://arxiv.org/abs/2106.12723",
          "publishedOn": "2021-06-25T02:00:46.529Z",
          "wordCount": 595,
          "title": "Meaningfully Explaining a Model's Mistakes. (arXiv:2106.12723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1\">Thai Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "Accurate models of robot dynamics are critical for safe and stable control\nand generalization to novel operational conditions. Hand-designed models,\nhowever, may be insufficiently accurate, even after careful parameter tuning.\nThis motivates the use of machine learning techniques to approximate the robot\ndynamics over a training set of state-control trajectories. The dynamics of\nmany robots, including ground, aerial, and underwater vehicles, are described\nin terms of their SE(3) pose and generalized velocity, and satisfy conservation\nof energy principles. This paper proposes a Hamiltonian formulation over the\nSE(3) manifold of the structure of a neural ordinary differential equation\n(ODE) network to approximate the dynamics of a rigid body. In contrast to a\nblack-box ODE network, our formulation guarantees total energy conservation by\nconstruction. We develop energy shaping and damping injection control for the\nlearned, potentially under-actuated SE(3) Hamiltonian dynamics to enable a\nunified approach for stabilization and trajectory tracking with various\nplatforms, including pendulum, rigid-body, and quadrotor systems.",
          "link": "http://arxiv.org/abs/2106.12782",
          "publishedOn": "2021-06-25T02:00:46.524Z",
          "wordCount": 611,
          "title": "Hamiltonian-based Neural ODE Networks on the SE(3) Manifold For Dynamics Learning and Control. (arXiv:2106.12782v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12819",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1\">Yang Qian</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Variational quantum algorithms (VQAs) have the potential of utilizing\nnear-term quantum machines to gain certain computational advantages over\nclassical methods. Nevertheless, modern VQAs suffer from cumbersome\ncomputational overhead, hampered by the tradition of employing a solitary\nquantum processor to handle large-volume data. As such, to better exert the\nsuperiority of VQAs, it is of great significance to improve their runtime\nefficiency. Here we devise an efficient distributed optimization scheme, called\nQUDIO, to address this issue. Specifically, in QUDIO, a classical central\nserver partitions the learning problem into multiple subproblems and allocate\nthem to multiple local nodes where each of them consists of a quantum processor\nand a classical optimizer. During the training procedure, all local nodes\nproceed parallel optimization and the classical server synchronizes\noptimization information among local nodes timely. In doing so, we prove a\nsublinear convergence rate of QUDIO in terms of the number of global iteration\nunder the ideal scenario, while the system imperfection may incur divergent\noptimization. Numerical results on standard benchmarks demonstrate that QUDIO\ncan surprisingly achieve a superlinear runtime speedup with respect to the\nnumber of local nodes. Our proposal can be readily mixed with other advanced\nVQAs-based techniques to narrow the gap between the state of the art and\napplications with quantum advantage.",
          "link": "http://arxiv.org/abs/2106.12819",
          "publishedOn": "2021-06-25T02:00:46.508Z",
          "wordCount": 640,
          "title": "Accelerating variational quantum algorithms with multiple quantum processors. (arXiv:2106.12819v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1\">Ibrahim Alabdulmohsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "We present a scalable post-processing algorithm for debiasing trained models,\nincluding deep neural networks (DNNs), which we prove to be near-optimal by\nbounding its excess Bayes risk. We empirically validate its advantages on\nstandard benchmark datasets across both classical algorithms as well as modern\nDNN architectures and demonstrate that it outperforms previous post-processing\nmethods while performing on par with in-processing. In addition, we show that\nthe proposed algorithm is particularly effective for models trained at scale\nwhere post-processing is a natural and practical choice.",
          "link": "http://arxiv.org/abs/2106.12887",
          "publishedOn": "2021-06-25T02:00:46.503Z",
          "wordCount": 535,
          "title": "A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models. (arXiv:2106.12887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huh_D/0/1/0/all/0/1\">Dom Huh</a>",
          "description": "Shared feature spaces for actor-critic methods aims to capture generalized\nlatent representations to be used by the policy and value function with the\nhopes for a more stable and sample-efficient optimization. However, such a\nparadigm present a number of challenges in practice, as parameters generating a\nshared representation must learn off two distinct objectives, resulting in\ncompeting updates and learning perturbations. In this paper, we present a novel\nfeature-sharing framework to address these difficulties by introducing the mix\nand mask mechanisms and the distributional scalarization technique. These\nmechanisms behaves dynamically to couple and decouple connected latent features\nvariably between the policy and value function, while the distributional\nscalarization standardizes the two objectives using a probabilistic standpoint.\nFrom our experimental results, we demonstrate significant performance\nimprovements compared to alternative methods using separate networks and\nnetworks with a shared backbone.",
          "link": "http://arxiv.org/abs/2106.13037",
          "publishedOn": "2021-06-25T02:00:46.498Z",
          "wordCount": 559,
          "title": "Mix and Mask Actor-Critic Methods. (arXiv:2106.13037v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1\">Tushar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_U/0/1/0/all/0/1\">Umang Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rupali Patil</a>",
          "description": "Anticipating the quantity of new associated or affirmed cases with novel\ncoronavirus ailment 2019 (COVID-19) is critical in the counteraction and\ncontrol of the COVID-19 flare-up. The new associated cases with COVID-19\ninformation were gathered from 20 January 2020 to 21 July 2020. We filtered out\nthe countries which are converging and used those for training the network. We\nutilized the SARIMAX, Linear regression model to anticipate new suspected\nCOVID-19 cases for the countries which did not converge yet. We predict the\ncurve of non-converged countries with the help of proposed Statistical SARIMAX\nmodel (SSM). We present new information investigation-based forecast results\nthat can assist governments with planning their future activities and help\nclinical administrations to be more ready for what's to come. Our framework can\nforesee peak corona cases with an R-Squared value of 0.986 utilizing linear\nregression and fall of this pandemic at various levels for countries like\nIndia, US, and Brazil. We found that considering more countries for training\ndegrades the prediction process as constraints vary from nation to nation.\nThus, we expect that the outcomes referenced in this work will help individuals\nto better understand the possibilities of this pandemic.",
          "link": "http://arxiv.org/abs/2106.12888",
          "publishedOn": "2021-06-25T02:00:46.493Z",
          "wordCount": 693,
          "title": "COVID-19 cases prediction using regression and novel SSM model for non-converged countries. (arXiv:2106.12888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jokic_P/0/1/0/all/0/1\">Petar Jokic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarkhish_E/0/1/0/all/0/1\">Erfan Azarkhish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonetti_A/0/1/0/all/0/1\">Andrea Bonetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_M/0/1/0/all/0/1\">Marc Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emery_S/0/1/0/all/0/1\">Stephane Emery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>",
          "description": "Implementing embedded neural network processing at the edge requires\nefficient hardware acceleration that couples high computational performance\nwith low power consumption. Driven by the rapid evolution of network\narchitectures and their algorithmic features, accelerator designs are\nconstantly updated and improved. To evaluate and compare hardware design\nchoices, designers can refer to a myriad of accelerator implementations in the\nliterature. Surveys provide an overview of these works but are often limited to\nsystem-level and benchmark-specific performance metrics, making it difficult to\nquantitatively compare the individual effect of each utilized optimization\ntechnique. This complicates the evaluation of optimizations for new accelerator\ndesigns, slowing-down the research progress. This work provides a survey of\nneural network accelerator optimization approaches that have been used in\nrecent works and reports their individual effects on edge processing\nperformance. It presents the list of optimizations and their quantitative\neffects as a construction kit, allowing to assess the design choices for each\nbuilding block separately. Reported optimizations range from up to 10'000x\nmemory savings to 33x energy reductions, providing chip designers an overview\nof design choices for implementing efficient low power neural network\naccelerators.",
          "link": "http://arxiv.org/abs/2106.12810",
          "publishedOn": "2021-06-25T02:00:46.488Z",
          "wordCount": 628,
          "title": "A Construction Kit for Efficient Low Power Neural Network Accelerator Designs. (arXiv:2106.12810v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hengxu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>",
          "description": "Successful quantitative investment usually relies on precise predictions of\nthe future movement of the stock price. Recently, machine learning based\nsolutions have shown their capacity to give more accurate stock prediction and\nbecome indispensable components in modern quantitative investment systems.\nHowever, the i.i.d. assumption behind existing methods is inconsistent with the\nexistence of diverse trading patterns in the stock market, which inevitably\nlimits their ability to achieve better stock prediction performance. In this\npaper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to\nempower existing stock prediction models with the ability to model multiple\nstock trading patterns. Essentially, TRA is a lightweight module that consists\nof a set of independent predictors for learning multiple patterns as well as a\nrouter to dispatch samples to different predictors. Nevertheless, the lack of\nexplicit pattern identifiers makes it quite challenging to train an effective\nTRA-based model. To tackle this challenge, we further design a learning\nalgorithm based on Optimal Transport (OT) to obtain the optimal sample to\npredictor assignment and effectively optimize the router with such assignment\nthrough an auxiliary loss term. Experiments on the real-world stock ranking\ntask show that compared to the state-of-the-art baselines, e.g., Attention LSTM\nand Transformer, the proposed method can improve information coefficient (IC)\nfrom 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used\nin this work are publicly available: https://github.com/microsoft/qlib.",
          "link": "http://arxiv.org/abs/2106.12950",
          "publishedOn": "2021-06-25T02:00:46.472Z",
          "wordCount": 686,
          "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. (arXiv:2106.12950v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12929",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bedrunka_M/0/1/0/all/0/1\">Mario Christopher Bedrunka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wilde_D/0/1/0/all/0/1\">Dominik Wilde</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kliemank_M/0/1/0/all/0/1\">Martin Kliemank</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reith_D/0/1/0/all/0/1\">Dirk Reith</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Foysi_H/0/1/0/all/0/1\">Holger Foysi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kramer_A/0/1/0/all/0/1\">Andreas Kr&#xe4;mer</a>",
          "description": "The lattice Boltzmann method (LBM) is an efficient simulation technique for\ncomputational fluid mechanics and beyond. It is based on a simple\nstream-and-collide algorithm on Cartesian grids, which is easily compatible\nwith modern machine learning architectures. While it is becoming increasingly\nclear that deep learning can provide a decisive stimulus for classical\nsimulation techniques, recent studies have not addressed possible connections\nbetween machine learning and LBM. Here, we introduce Lettuce, a PyTorch-based\nLBM code with a threefold aim. Lettuce enables GPU accelerated calculations\nwith minimal source code, facilitates rapid prototyping of LBM models, and\nenables integrating LBM simulations with PyTorch's deep learning and automatic\ndifferentiation facility. As a proof of concept for combining machine learning\nwith the LBM, a neural collision model is developed, trained on a doubly\nperiodic shear layer and then transferred to a different flow, a decaying\nturbulence. We also exemplify the added benefit of PyTorch's automatic\ndifferentiation framework in flow control and optimization. To this end, the\nspectrum of a forced isotropic turbulence is maintained without further\nconstraining the velocity field. The source code is freely available from\nhttps://github.com/lettucecfd/lettuce.",
          "link": "http://arxiv.org/abs/2106.12929",
          "publishedOn": "2021-06-25T02:00:46.467Z",
          "wordCount": 622,
          "title": "Lettuce: PyTorch-based Lattice Boltzmann Framework. (arXiv:2106.12929v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nishant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanfeld_P/0/1/0/all/0/1\">Pia Hanfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1\">Michael Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bussmann_M/0/1/0/all/0/1\">Michael Bussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1\">Stefan Gumhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmannn_N/0/1/0/all/0/1\">Nico Hoffmannn</a>",
          "description": "Normalizing flows are prominent deep generative models that provide tractable\nprobability distributions and efficient density estimation. However, they are\nwell known to fail while detecting Out-of-Distribution (OOD) inputs as they\ndirectly encode the local features of the input representations in their latent\nspace. In this paper, we solve this overconfidence issue of normalizing flows\nby demonstrating that flows, if extended by an attention mechanism, can\nreliably detect outliers including adversarial attacks. Our approach does not\nrequire outlier data for training and we showcase the efficiency of our method\nfor OOD detection by reporting state-of-the-art performance in diverse\nexperimental settings. Code available at\nhttps://github.com/ComputationalRadiationPhysics/InFlow .",
          "link": "http://arxiv.org/abs/2106.12894",
          "publishedOn": "2021-06-25T02:00:46.453Z",
          "wordCount": 546,
          "title": "InFlow: Robust outlier detection utilizing Normalizing Flows. (arXiv:2106.12894v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhiheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minxian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1\">Maria Alejandra Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buyya_R/0/1/0/all/0/1\">Rajkumar Buyya</a>",
          "description": "Containerization is a lightweight application virtualization technology,\nproviding high environmental consistency, operating system distribution\nportability, and resource isolation. Existing mainstream cloud service\nproviders have prevalently adopted container technologies in their distributed\nsystem infrastructures for automated application management. To handle the\nautomation of deployment, maintenance, autoscaling, and networking of\ncontainerized applications, container orchestration is proposed as an essential\nresearch problem. However, the highly dynamic and diverse feature of cloud\nworkloads and environments considerably raises the complexity of orchestration\nmechanisms. Machine learning algorithms are accordingly employed by container\norchestration systems for behavior modelling and prediction of\nmulti-dimensional performance metrics. Such insights could further improve the\nquality of resource provisioning decisions in response to the changing\nworkloads under complex environments. In this paper, we present a comprehensive\nliterature review of existing machine learning-based container orchestration\napproaches. Detailed taxonomies are proposed to classify the current researches\nby their common features. Moreover, the evolution of machine learning-based\ncontainer orchestration technologies from the year 2016 to 2021 has been\ndesigned based on objectives and metrics. A comparative analysis of the\nreviewed techniques is conducted according to the proposed taxonomies, with\nemphasis on their key characteristics. Finally, various open research\nchallenges and potential future directions are highlighted.",
          "link": "http://arxiv.org/abs/2106.12739",
          "publishedOn": "2021-06-25T02:00:46.447Z",
          "wordCount": 652,
          "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions. (arXiv:2106.12739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12758",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dhadphale_J/0/1/0/all/0/1\">Jayesh Dhadphale</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Unni_V/0/1/0/all/0/1\">Vishnu R. Unni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saha_A/0/1/0/all/0/1\">Abhishek Saha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sujith_R/0/1/0/all/0/1\">R. I. Sujith</a>",
          "description": "In reacting flow systems, thermoacoustic instability characterized by high\namplitude pressure fluctuations, is driven by a positive coupling between the\nunsteady heat release rate and the acoustic field of the combustor. When the\nunderlying flow is turbulent, as a control parameter of the system is varied\nand the system approach thermoacoustic instability, the acoustic pressure\noscillations synchronize with heat release rate oscillations. Consequently,\nduring the onset of thermoacoustic instability in turbulent combustors, the\nsystem dynamics transition from chaotic oscillations to periodic oscillations\nvia a state of intermittency. Thermoacoustic systems are traditionally modeled\nby coupling the model for the unsteady heat source and the acoustic subsystem,\neach estimated independently. The response of the unsteady heat source, the\nflame, to acoustic fluctuations are characterized by introducing external\nunsteady forcing. This necessitates a powerful excitation module to obtain the\nnonlinear response of the flame to acoustic perturbations. Instead of\ncharacterizing individual subsystems, we introduce a neural ordinary\ndifferential equation (neural ODE) framework to model the thermoacoustic system\nas a whole. The neural ODE model for the thermoacoustic system uses time series\nof the heat release rate and the pressure fluctuations, measured simultaneously\nwithout introducing any external perturbations, to model their coupled\ninteraction. Further, we use the parameters of neural ODE to define an anomaly\nmeasure that represents the proximity of system dynamics to limit cycle\noscillations and thus provide an early warning signal for the onset of\nthermoacoustic instability.",
          "link": "http://arxiv.org/abs/2106.12758",
          "publishedOn": "2021-06-25T02:00:46.441Z",
          "wordCount": 682,
          "title": "Neural ODE to model and prognose thermoacoustic instability. (arXiv:2106.12758v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1\">Polina Kirichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dushyant Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_N/0/1/0/all/0/1\">Nir Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huiyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "Learning new tasks continuously without forgetting on a constantly changing\ndata distribution is essential for real-world problems but extremely\nchallenging for modern deep learning. In this work we propose HCL, a Hybrid\ngenerative-discriminative approach to Continual Learning for classification. We\nmodel the distribution of each task and each class with a normalizing flow. The\nflow is used to learn the data distribution, perform classification, identify\ntask changes, and avoid forgetting, all leveraging the invertibility and exact\nlikelihood which are uniquely enabled by the normalizing flow model. We use the\ngenerative capabilities of the flow to avoid catastrophic forgetting through\ngenerative replay and a novel functional regularization technique. For task\nidentification, we use state-of-the-art anomaly detection techniques based on\nmeasuring the typicality of the model's statistics. We demonstrate the strong\nperformance of HCL on a range of continual learning benchmarks such as\nsplit-MNIST, split-CIFAR, and SVHN-MNIST.",
          "link": "http://arxiv.org/abs/2106.12772",
          "publishedOn": "2021-06-25T02:00:46.425Z",
          "wordCount": 589,
          "title": "Task-agnostic Continual Learning with Hybrid Probabilistic Models. (arXiv:2106.12772v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shechner_M/0/1/0/all/0/1\">Moshe Shechner</a>",
          "description": "We study the problem of differentially private clustering under\ninput-stability assumptions. Despite the ever-growing volume of works on\ndifferential privacy in general and differentially private clustering in\nparticular, only three works (Nissim et al. 2007, Wang et al. 2015, Huang et\nal. 2018) looked at the problem of privately clustering \"nice\" k-means\ninstances, all three relying on the sample-and-aggregate framework and all\nthree measuring utility in terms of Wasserstein distance between the true\ncluster centers and the centers returned by the private algorithm. In this work\nwe improve upon this line of works on multiple axes. We present a far simpler\nalgorithm for clustering stable inputs (not relying on the sample-and-aggregate\nframework), and analyze its utility in both the Wasserstein distance and the\nk-means cost. Moreover, our algorithm has straight-forward analogues for \"nice\"\nk-median instances and for the local-model of differential privacy.",
          "link": "http://arxiv.org/abs/2106.12959",
          "publishedOn": "2021-06-25T02:00:46.406Z",
          "wordCount": 603,
          "title": "Differentially Private Algorithms for Clustering with Stability Assumptions. (arXiv:2106.12959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadip Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1\">Swapna Sourav Rout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Sudeep Choudhary</a>",
          "description": "Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.",
          "link": "http://arxiv.org/abs/2106.12871",
          "publishedOn": "2021-06-25T02:00:46.389Z",
          "wordCount": 617,
          "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection. (arXiv:2106.12871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Feiwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yanli Shao</a>",
          "description": "The recurrent network architecture is a widely used model in sequence\nmodeling, but its serial dependency hinders the computation parallelization,\nwhich makes the operation inefficient. The same problem was encountered in\nserial adder at the early stage of digital electronics. In this paper, we\ndiscuss the similarities between recurrent neural network (RNN) and serial\nadder. Inspired by carry-lookahead adder, we introduce carry-lookahead module\nto RNN, which makes it possible for RNN to run in parallel. Then, we design the\nmethod of parallel RNN computation, and finally Carry-lookahead RNN (CL-RNN) is\nproposed. CL-RNN takes advantages in parallelism and flexible receptive field.\nThrough a comprehensive set of tests, we verify that CL-RNN can perform better\nthan existing typical RNNs in sequence modeling tasks which are specially\ndesigned for RNNs.",
          "link": "http://arxiv.org/abs/2106.12901",
          "publishedOn": "2021-06-25T02:00:46.382Z",
          "wordCount": 564,
          "title": "Recurrent Neural Network from Adder's Perspective: Carry-lookahead RNN. (arXiv:2106.12901v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lingam_V/0/1/0/all/0/1\">Vijay Lingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragesh_R/0/1/0/all/0/1\">Rahul Ragesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Arun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellamanickam_S/0/1/0/all/0/1\">Sundararajan Sellamanickam</a>",
          "description": "Graph Neural Networks (GNNs) have shown excellent performance on graphs that\nexhibit strong homophily with respect to the node labels i.e. connected nodes\nhave same labels. However, they perform poorly on heterophilic graphs. Recent\napproaches have typically modified aggregation schemes, designed adaptive graph\nfilters, etc. to address this limitation. In spite of this, the performance on\nheterophilic graphs can still be poor. We propose a simple alternative method\nthat exploits Truncated Singular Value Decomposition (TSVD) of topological\nstructure and node features. Our approach achieves up to ~30% improvement in\nperformance over state-of-the-art methods on heterophilic graphs. This work is\nan early investigation into methods that differ from aggregation based\napproaches. Our experimental results suggest that it might be important to\nexplore other alternatives to aggregation methods for heterophilic setting.",
          "link": "http://arxiv.org/abs/2106.12807",
          "publishedOn": "2021-06-25T02:00:46.362Z",
          "wordCount": 577,
          "title": "Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs. (arXiv:2106.12807v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziyadidegan_S/0/1/0/all/0/1\">Samira Ziyadidegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1\">Moein Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesarakli_H/0/1/0/all/0/1\">Homa Pesarakli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javid_A/0/1/0/all/0/1\">Amir Hossein Javid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erraguntla_M/0/1/0/all/0/1\">Madhav Erraguntla</a>",
          "description": "The COVID-19 disease spreads swiftly, and nearly three months after the first\npositive case was confirmed in China, Coronavirus started to spread all over\nthe United States. Some states and counties reported high number of positive\ncases and deaths, while some reported lower COVID-19 related cases and\nmortality. In this paper, the factors that could affect the risk of COVID-19\ninfection and mortality were analyzed in county level. An innovative method by\nusing K-means clustering and several classification models is utilized to\ndetermine the most critical factors. Results showed that mean temperature,\npercent of people below poverty, percent of adults with obesity, air pressure,\npopulation density, wind speed, longitude, and percent of uninsured people were\nthe most significant attributes",
          "link": "http://arxiv.org/abs/2106.12766",
          "publishedOn": "2021-06-25T02:00:46.344Z",
          "wordCount": 627,
          "title": "Factors affecting the COVID-19 risk in the US counties: an innovative approach by combining unsupervised and supervised learning. (arXiv:2106.12766v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1\">J&#xfc;rgen Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munzner_T/0/1/0/all/0/1\">Tamara Munzner</a>",
          "description": "Graph neural networks (GNNs) are a class of powerful machine learning tools\nthat model node relations for making predictions of nodes or links. GNN\ndevelopers rely on quantitative metrics of the predictions to evaluate a GNN,\nbut similar to many other neural networks, it is difficult for them to\nunderstand if the GNN truly learns characteristics of a graph as expected. We\npropose an approach to corresponding an input graph to its node embedding (aka\nlatent space), a common component of GNNs that is later used for prediction. We\nabstract the data and tasks, and develop an interactive multi-view interface\ncalled CorGIE to instantiate the abstraction. As the key function in CorGIE, we\npropose the K-hop graph layout to show topological neighbors in hops and their\nclustering structure. To evaluate the functionality and usability of CorGIE, we\npresent how to use CorGIE in two usage scenarios, and conduct a case study with\ntwo GNN experts.",
          "link": "http://arxiv.org/abs/2106.12839",
          "publishedOn": "2021-06-25T02:00:46.338Z",
          "wordCount": 596,
          "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding. (arXiv:2106.12839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00543",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Junyu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>",
          "description": "We posit a new mechanism for cooperation in multi-agent reinforcement\nlearning (MARL) based upon any nonlinear function of the team's long-term\nstate-action occupancy measure, i.e., a \\emph{general utility}. This subsumes\nthe cumulative return but also allows one to incorporate risk-sensitivity,\nexploration, and priors. % We derive the {\\bf D}ecentralized {\\bf S}hadow\nReward {\\bf A}ctor-{\\bf C}ritic (DSAC) in which agents alternate between policy\nevaluation (critic), weighted averaging with neighbors (information mixing),\nand local gradient updates for their policy parameters (actor). DSAC augments\nthe classic critic step by requiring agents to (i) estimate their local\noccupancy measure in order to (ii) estimate the derivative of the local utility\nwith respect to their occupancy measure, i.e., the \"shadow reward\". DSAC\nconverges to $\\epsilon$-stationarity in $\\mathcal{O}(1/\\epsilon^{2.5})$\n(Theorem \\ref{theorem:final}) or faster $\\mathcal{O}(1/\\epsilon^{2})$\n(Corollary \\ref{corollary:communication}) steps with high probability,\ndepending on the amount of communications. We further establish the\nnon-existence of spurious stationary points for this problem, that is, DSAC\nfinds the globally optimal policy (Corollary \\ref{corollary:global}).\nExperiments demonstrate the merits of goals beyond the cumulative return in\ncooperative MARL.",
          "link": "http://arxiv.org/abs/2106.00543",
          "publishedOn": "2021-06-25T02:00:46.318Z",
          "wordCount": 639,
          "title": "MARL with General Utilities via Decentralized Shadow Reward Actor-Critic. (arXiv:2106.00543v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Li Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>",
          "description": "Graph neural networks (GNNs) emerged recently as a standard toolkit for\nlearning from data on graphs. Current GNN designing works depend on immense\nhuman expertise to explore different message-passing mechanisms, and require\nmanual enumeration to determine the proper message-passing depth. Inspired by\nthe strong searching capability of neural architecture search (NAS) in CNN,\nthis paper proposes Graph Neural Architecture Search (GNAS) with novel-designed\nsearch space. The GNAS can automatically learn better architecture with the\noptimal depth of message passing on the graph. Specifically, we design Graph\nNeural Architecture Paradigm (GAP) with tree-topology computation procedure and\ntwo types of fine-grained atomic operations (feature filtering and neighbor\naggregation) from message-passing mechanism to construct powerful graph network\nsearch space. Feature filtering performs adaptive feature selection, and\nneighbor aggregation captures structural information and calculates neighbors'\nstatistics. Experiments show that our GNAS can search for better GNNs with\nmultiple message-passing mechanisms and optimal message-passing depth. The\nsearched network achieves remarkable improvement over state-of-the-art manual\ndesigned and search-based GNNs on five large-scale datasets at three classical\ngraph tasks. Codes can be found at https://github.com/phython96/GNAS-MP.",
          "link": "http://arxiv.org/abs/2103.14282",
          "publishedOn": "2021-06-25T02:00:46.312Z",
          "wordCount": 672,
          "title": "Rethinking Graph Neural Architecture Search from Message-passing. (arXiv:2103.14282v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.03376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Persand_K/0/1/0/all/0/1\">Kaveena Persand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>",
          "description": "The computation and memory needed for Convolutional Neural Network (CNN)\ninference can be reduced by pruning weights from the trained network. Pruning\nis guided by a pruning saliency, which heuristically approximates the change in\nthe loss function associated with the removal of specific weights. Many pruning\nsignals have been proposed, but the performance of each heuristic depends on\nthe particular trained network. This leaves the data scientist with a difficult\nchoice. When using any one saliency metric for the entire pruning process, we\nrun the risk of the metric assumptions being invalidated, leading to poor\ndecisions being made by the metric. Ideally we could combine the best aspects\nof different saliency metrics. However, despite an extensive literature review,\nwe are unable to find any prior work on composing different saliency metrics.\nThe chief difficulty lies in combining the numerical output of different\nsaliency metrics, which are not directly comparable.\n\nWe propose a method to compose several primitive pruning saliencies, to\nexploit the cases where each saliency measure does well. Our experiments show\nthat the composition of saliencies avoids many poor pruning choices identified\nby individual saliencies. In most cases our method finds better selections than\neven the best individual pruning saliency.",
          "link": "http://arxiv.org/abs/2004.03376",
          "publishedOn": "2021-06-25T02:00:46.306Z",
          "wordCount": 690,
          "title": "Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle. (arXiv:2004.03376v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.01956",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gneiting_T/0/1/0/all/0/1\">Tilmann Gneiting</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Walz_E/0/1/0/all/0/1\">Eva-Maria Walz</a>",
          "description": "Throughout science and technology, receiver operating characteristic (ROC)\ncurves and associated area under the curve (AUC) measures constitute powerful\ntools for assessing the predictive abilities of features, markers and tests in\nbinary classification problems. Despite its immense popularity, ROC analysis\nhas been subject to a fundamental restriction, in that it applies to\ndichotomous (yes or no) outcomes only. Here we introduce ROC movies and\nuniversal ROC (UROC) curves that apply to just any linearly ordered outcome,\nalong with an associated coefficient of predictive ability (CPA) measure. CPA\nequals the area under the UROC curve, and admits appealing interpretations in\nterms of probabilities and rank based covariances. For binary outcomes CPA\nequals AUC, and for pairwise distinct outcomes CPA relates linearly to\nSpearman's coefficient, in the same way that the C index relates linearly to\nKendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the\ntools of classical ROC analysis, and are bound to supersede them in a wealth of\napplications. Their usage is illustrated in data examples from biomedicine and\nmeteorology, where rank based measures yield new insights in the WeatherBench\ncomparison of the predictive performance of convolutional neural networks and\nphysical-numerical models for weather prediction.",
          "link": "http://arxiv.org/abs/1912.01956",
          "publishedOn": "2021-06-25T02:00:46.288Z",
          "wordCount": 671,
          "title": "Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). (arXiv:1912.01956v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.01171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lubin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Weili Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>",
          "description": "Multiple convolutional neural network (CNN) classifiers have been proposed\nfor electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,\nCNN models have been found vulnerable to universal adversarial perturbations\n(UAPs), which are small and example-independent, yet powerful enough to degrade\nthe performance of a CNN model, when added to a benign example. This paper\nproposes a novel total loss minimization (TLM) approach to generate UAPs for\nEEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on\nthree popular CNN classifiers for both target and non-target attacks. We also\nverified the transferability of UAPs in EEG-based BCI systems. To our\nknowledge, this is the first study on UAPs of CNN classifiers in EEG-based\nBCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a\npotentially critical security concern of BCIs.",
          "link": "http://arxiv.org/abs/1912.01171",
          "publishedOn": "2021-06-25T02:00:46.268Z",
          "wordCount": 636,
          "title": "Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs. (arXiv:1912.01171v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habetler_T/0/1/0/all/0/1\">Thomas G. Habetler</a>",
          "description": "The rapid development of artificial intelligence and deep learning has\nprovided many opportunities to further enhance the safety, stability, and\naccuracy of industrial Cyber-Physical Systems (CPS). As indispensable\ncomponents to many mission-critical CPS assets and equipment, mechanical\nbearings need to be monitored to identify any trace of abnormal conditions.\nMost of the data-driven approaches applied to bearing fault diagnosis\nup-to-date are trained using a large amount of fault data collected a priori.\nIn many practical applications, however, it can be unsafe and time-consuming to\ncollect sufficient data samples for each fault category, making it challenging\nto train a robust classifier. In this paper, we propose a few-shot learning\nframework for bearing fault diagnosis based on model-agnostic meta-learning\n(MAML), which targets for training an effective fault classifier using limited\ndata. In addition, it can leverage the training data and learn to identify new\nfault scenarios more efficiently. Case studies on the generalization to new\nartificial faults show that the proposed framework achieves an overall accuracy\nup to 25% higher than a Siamese network-based benchmark study. Finally, the\nrobustness and the generalization capability of the proposed framework are\nfurther validated by applying it to identify real bearing damages using data\nfrom artificial damages, which compares favorably against 6 state-of-the-art\nfew-shot learning algorithms using consistent test environments.",
          "link": "http://arxiv.org/abs/2007.12851",
          "publishedOn": "2021-06-25T02:00:46.240Z",
          "wordCount": 700,
          "title": "Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning. (arXiv:2007.12851v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cadavid_J/0/1/0/all/0/1\">Juan Pablo Usuga Cadavid</a> (LAMIH, ENSAM), <a href=\"http://arxiv.org/find/cs/1/au:+Lamouri_S/0/1/0/all/0/1\">Samir Lamouri</a> (LAMIH, ENSAM), <a href=\"http://arxiv.org/find/cs/1/au:+Grabot_B/0/1/0/all/0/1\">Bernard Grabot</a> (LGP, ENIT), <a href=\"http://arxiv.org/find/cs/1/au:+Fortin_A/0/1/0/all/0/1\">Arnaud Fortin</a>",
          "description": "Proper Production Planning and Control (PPC) is capital to have an edge over\ncompetitors, reduce costs and respect delivery dates. With regard to PPC,\nMachine Learning (ML) provides new opportunities to make intelligent decisions\nbased on data. Therefore, this communication provides an initial systematic\nreview of publications on ML applied in PPC. The research objective of this\nstudy is twofold: firstly, it aims to identify techniques and tools allowing to\napply ML in PPC, and secondly, it reviews the characteristics of Industry 4.0\n(I4.0) in recent research papers. Concerning the second objective, seven\ncharacteristics of I4.0 are used in the analysis framework, from which two of\nthem are proposed by the authors. Additionally, the addressed domains of\nML-aided PPC in scientific literature are identified. Finally, results are\nanalyzed and gaps that may motivate further research are highlighted.",
          "link": "http://arxiv.org/abs/2106.12916",
          "publishedOn": "2021-06-25T02:00:46.235Z",
          "wordCount": 609,
          "title": "L'Apprentissage Automatique dans la planification et le contr{\\^o}le de la production : un {\\'e}tat de l'art. (arXiv:2106.12916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>",
          "description": "Since the first coronavirus case was identified in the U.S. on Jan. 21, more\nthan 1 million people in the U.S. have confirmed cases of COVID-19. This\ninfectious respiratory disease has spread rapidly across more than 3000\ncounties and 50 states in the U.S. and have exhibited evolutionary clustering\nand complex triggering patterns. It is essential to understand the complex\nspacetime intertwined propagation of this disease so that accurate prediction\nor smart external intervention can be carried out. In this paper, we model the\npropagation of the COVID-19 as spatio-temporal point processes and propose a\ngenerative and intensity-free model to track the spread of the disease. We\nfurther adopt a generative adversarial imitation learning framework to learn\nthe model parameters. In comparison with the traditional likelihood-based\nlearning methods, this imitation learning framework does not need to prespecify\nan intensity function, which alleviates the model-misspecification. Moreover,\nthe adversarial learning procedure bypasses the difficult-to-evaluate integral\ninvolved in the likelihood evaluation, which makes the model inference more\nscalable with the data and variables. We showcase the dynamic learning\nperformance on the COVID-19 confirmed cases in the U.S. and evaluate the social\ndistancing policy based on the learned generative model.",
          "link": "http://arxiv.org/abs/2106.13097",
          "publishedOn": "2021-06-25T02:00:46.219Z",
          "wordCount": 684,
          "title": "Understanding the Spread of COVID-19 Epidemic: A Spatio-Temporal Point Process View. (arXiv:2106.13097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13067",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Johnstone_P/0/1/0/all/0/1\">Patrick R. Johnstone</a>, <a href=\"http://arxiv.org/find/math/1/au:+Eckstein_J/0/1/0/all/0/1\">Jonathan Eckstein</a>, <a href=\"http://arxiv.org/find/math/1/au:+Flynn_T/0/1/0/all/0/1\">Thomas Flynn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>",
          "description": "We present a new, stochastic variant of the projective splitting (PS) family\nof algorithms for monotone inclusion problems. It can solve min-max and\nnoncooperative game formulations arising in applications such as robust ML\nwithout the convergence issues associated with gradient descent-ascent, the\ncurrent de facto standard approach in such situations. Our proposal is the\nfirst version of PS able to use stochastic (as opposed to deterministic)\ngradient oracles. It is also the first stochastic method that can solve min-max\ngames while easily handling multiple constraints and nonsmooth regularizers via\nprojection and proximal operators. We close with numerical experiments on a\ndistributionally robust sparse logistic regression problem.",
          "link": "http://arxiv.org/abs/2106.13067",
          "publishedOn": "2021-06-25T02:00:46.208Z",
          "wordCount": 545,
          "title": "Stochastic Projective Splitting: Solving Saddle-Point Problems with Multiple Regularizers. (arXiv:2106.13067v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12933",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zilber_P/0/1/0/all/0/1\">Pini Zilber</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>",
          "description": "Low rank matrix recovery problems, including matrix completion and matrix\nsensing, appear in a broad range of applications. In this work we present GNMR\n-- an extremely simple iterative algorithm for low rank matrix recovery, based\non a Gauss-Newton linearization. On the theoretical front, we derive recovery\nguarantees for GNMR in both the matrix sensing and matrix completion settings.\nA key property of GNMR is that it implicitly keeps the factor matrices\napproximately balanced throughout its iterations. On the empirical front, we\nshow that for matrix completion with uniform sampling, GNMR performs better\nthan several popular methods, especially when given very few observations close\nto the information limit.",
          "link": "http://arxiv.org/abs/2106.12933",
          "publishedOn": "2021-06-25T02:00:46.203Z",
          "wordCount": 556,
          "title": "GNMR: A provable one-line algorithm for low rank matrix recovery. (arXiv:2106.12933v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_S/0/1/0/all/0/1\">Sam Zabdiel Sunder Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamakshi_V/0/1/0/all/0/1\">Vidhya Kamakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodhi_N/0/1/0/all/0/1\">Namrata Lodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">Narayanan C Krishnan</a>",
          "description": "A particular class of Explainable AI (XAI) methods provide saliency maps to\nhighlight part of the image a Convolutional Neural Network (CNN) model looks at\nto classify the image as a way to explain its working. These methods provide an\nintuitive way for users to understand predictions made by CNNs. Other than\nquantitative computational tests, the vast majority of evidence to highlight\nthat the methods are valuable is anecdotal. Given that humans would be the\nend-users of such methods, we devise three human subject experiments through\nwhich we gauge the effectiveness of these saliency-based explainability\nmethods.",
          "link": "http://arxiv.org/abs/2106.12773",
          "publishedOn": "2021-06-25T02:00:46.198Z",
          "wordCount": 540,
          "title": "Evaluation of Saliency-based Explainability Method. (arXiv:2106.12773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wickramanayake_S/0/1/0/all/0/1\">Sandareka Wickramanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wynne Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mong Li Lee</a>",
          "description": "Despite the remarkable performance, Deep Neural Networks (DNNs) behave as\nblack-boxes hindering user trust in Artificial Intelligence (AI) systems.\nResearch on opening black-box DNN can be broadly categorized into post-hoc\nmethods and inherently interpretable DNNs. While many surveys have been\nconducted on post-hoc interpretation methods, little effort is devoted to\ninherently interpretable DNNs. This paper provides a review of existing methods\nto develop DNNs with intrinsic interpretability, with a focus on Convolutional\nNeural Networks (CNNs). The aim is to understand the current progress towards\nfully interpretable DNNs that can cater to different interpretation\nrequirements. Finally, we identify gaps in current work and suggest potential\nresearch directions.",
          "link": "http://arxiv.org/abs/2106.13164",
          "publishedOn": "2021-06-25T02:00:46.193Z",
          "wordCount": 563,
          "title": "Towards Fully Interpretable Deep Neural Networks: Are We There Yet?. (arXiv:2106.13164v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandle_S/0/1/0/all/0/1\">Sebastian Br&#xe4;ndle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanussek_M/0/1/0/all/0/1\">Marc Hanussek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blohm_M/0/1/0/all/0/1\">Matthias Blohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kintz_M/0/1/0/all/0/1\">Maximilien Kintz</a>",
          "description": "Automated Machine Learning (AutoML) has gained increasing success on tabular\ndata in recent years. However, processing unstructured data like text is a\nchallenge and not widely supported by open-source AutoML tools. This work\ncompares three manually created text representations and text embeddings\nautomatically created by AutoML tools. Our benchmark includes four popular\nopen-source AutoML tools and eight datasets for text classification purposes.\nThe results show that straightforward text representations perform better than\nAutoML tools with automatically created text embeddings.",
          "link": "http://arxiv.org/abs/2106.12798",
          "publishedOn": "2021-06-25T02:00:46.188Z",
          "wordCount": 526,
          "title": "Evaluation of Representation Models for Text Classification with AutoML Tools. (arXiv:2106.12798v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12930",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pham_H/0/1/0/all/0/1\">Hieu H. Pham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia T. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha Q. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huynh_T/0/1/0/all/0/1\">Thang Q. Huynh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dao_M/0/1/0/all/0/1\">Minh Dao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">Van Vu</a>",
          "description": "Radiographs are used as the most important imaging tool for identifying spine\nanomalies in clinical practice. The evaluation of spinal bone lesions, however,\nis a challenging task for radiologists. This work aims at developing and\nevaluating a deep learning-based framework, named VinDr-SpineXR, for the\nclassification and localization of abnormalities from spine X-rays. First, we\nbuild a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,\neach of which is manually annotated by an experienced radiologist with bounding\nboxes around abnormal findings in 13 categories. Using this dataset, we then\ntrain a deep learning classifier to determine whether a spine scan is abnormal\nand a detector to localize 7 crucial findings amongst the total 13. The\nVinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,\nwhich is kept separate from the training set. It demonstrates an area under the\nreceiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,\n90.02%) for the image-level classification task and a mean average precision\n(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve\nas a proof of concept and set a baseline for future research in this direction.\nTo encourage advances, the dataset, codes, and trained deep learning models are\nmade publicly available.",
          "link": "http://arxiv.org/abs/2106.12930",
          "publishedOn": "2021-06-25T02:00:46.173Z",
          "wordCount": 702,
          "title": "VinDr-SpineXR: A deep learning framework for spinal lesions detection and classification from radiographs. (arXiv:2106.12930v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keltjens_B/0/1/0/all/0/1\">Benjamin Keltjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_T/0/1/0/all/0/1\">Tom van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido de Croon</a>",
          "description": "Self-supervised deep learning methods have leveraged stereo images for\ntraining monocular depth estimation. Although these methods show strong results\non outdoor datasets such as KITTI, they do not match performance of supervised\nmethods on indoor environments with camera rotation. Indoor, rotated scenes are\ncommon for less constrained applications and pose problems for two reasons:\nabundance of low texture regions and increased complexity of depth cues for\nimages under rotation. In an effort to extend self-supervised learning to more\ngeneralised environments we propose two additions. First, we propose a novel\nFilled Disparity Loss term that corrects for ambiguity of image reconstruction\nerror loss in textureless regions. Specifically, we interpolate disparity in\nuntextured regions, using the estimated disparity from surrounding textured\nareas, and use L1 loss to correct the original estimation. Our experiments show\nthat depth estimation is substantially improved on low-texture scenes, without\nany loss on textured scenes, when compared to Monodepth by Godard et al.\nSecondly, we show that training with an application's representative rotations,\nin both pitch and roll, is sufficient to significantly improve performance over\nthe entire range of expected rotation. We demonstrate that depth estimation is\nsuccessfully generalised as performance is not lost when evaluated on test sets\nwith no camera rotation. Together these developments enable a broader use of\nself-supervised learning of monocular depth estimation for complex\nenvironments.",
          "link": "http://arxiv.org/abs/2106.12958",
          "publishedOn": "2021-06-25T02:00:46.167Z",
          "wordCount": 664,
          "title": "Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated Scenes. (arXiv:2106.12958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xingyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiani Liu</a>",
          "description": "Sketching uses randomized Hash functions for dimensionality reduction and\nacceleration. The existing sketching methods, such as count sketch (CS), tensor\nsketch (TS), and higher-order count sketch (HCS), either suffer from low\naccuracy or slow speed in some tensor based applications. In this paper, the\nproposed fast count sketch (FCS) applies multiple shorter Hash functions based\nCS to the vector form of the input tensor, which is more accurate than TS since\nthe spatial information of the input tensor can be preserved more sufficiently.\nWhen the input tensor admits CANDECOMP/PARAFAC decomposition (CPD), FCS can\naccelerate CS and HCS by using fast Fourier transform, which exhibits a\ncomputational complexity asymptotically identical to TS for low-order tensors.\nThe effectiveness of FCS is validated by CPD, tensor regression network\ncompression, and Kronecker product compression. Experimental results show its\nsuperior performance in terms of approximation accuracy and computational\nefficiency.",
          "link": "http://arxiv.org/abs/2106.13062",
          "publishedOn": "2021-06-25T02:00:46.162Z",
          "wordCount": 568,
          "title": "Efficient Tensor Contraction via Fast Count Sketch. (arXiv:2106.13062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>",
          "description": "Designing novel protein sequences for a desired 3D topological fold is a\nfundamental yet non-trivial task in protein engineering. Challenges exist due\nto the complex sequence--fold relationship, as well as the difficulties to\ncapture the diversity of the sequences (therefore structures and functions)\nwithin a fold. To overcome these challenges, we propose Fold2Seq, a novel\ntransformer-based generative framework for designing protein sequences\nconditioned on a specific target fold. To model the complex sequence--structure\nrelationship, Fold2Seq jointly learns a sequence embedding using a transformer\nand a fold embedding from the density of secondary structural elements in 3D\nvoxels. On test sets with single, high-resolution and complete structure inputs\nfor individual folds, our experiments demonstrate improved or comparable\nperformance of Fold2Seq in terms of speed, coverage, and reliability for\nsequence design, when compared to existing state-of-the-art methods that\ninclude data-driven deep generative models and physics-based RosettaDesign. The\nunique advantages of fold-based Fold2Seq, in comparison to a structure-based\ndeep model and RosettaDesign, become more evident on three additional\nreal-world challenges originating from low-quality, incomplete, or ambiguous\ninput structures. Source code and data are available at\nhttps://github.com/IBM/fold2seq.",
          "link": "http://arxiv.org/abs/2106.13058",
          "publishedOn": "2021-06-25T02:00:46.147Z",
          "wordCount": 630,
          "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. (arXiv:2106.13058v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Ee Fey Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">ZhiYuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1\">Wei Xiang Lim</a>",
          "description": "The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.",
          "link": "http://arxiv.org/abs/2106.12736",
          "publishedOn": "2021-06-25T02:00:46.141Z",
          "wordCount": 680,
          "title": "Frequency Domain Convolutional Neural Network: Accelerated CNN for Large Diabetic Retinopathy Image Classification. (arXiv:2106.12736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12987",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Satone_V/0/1/0/all/0/1\">Vipul Satone</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Desai_D/0/1/0/all/0/1\">Dhruv Desai</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mehta_D/0/1/0/all/0/1\">Dhagash Mehta</a>",
          "description": "Identifying similar mutual funds with respect to the underlying portfolios\nhas found many applications in financial services ranging from fund recommender\nsystems, competitors analysis, portfolio analytics, marketing and sales, etc.\nThe traditional methods are either qualitative, and hence prone to biases and\noften not reproducible, or, are known not to capture all the nuances\n(non-linearities) among the portfolios from the raw data. We propose a\nradically new approach to identify similar funds based on the weighted\nbipartite network representation of funds and their underlying assets data\nusing a sophisticated machine learning method called Node2Vec which learns an\nembedded low-dimensional representation of the network. We call the embedding\n\\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network\nrepresentation of the funds-assets network in its original form that identifies\nstructural similarity among portfolios as opposed to merely portfolio overlaps.",
          "link": "http://arxiv.org/abs/2106.12987",
          "publishedOn": "2021-06-25T02:00:46.123Z",
          "wordCount": 589,
          "title": "Fund2Vec: Mutual Funds Similarity using Graph Learning. (arXiv:2106.12987v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woosub Jung</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yizhou Feng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sabbir Ahmed Khan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1\">Chunsheng Xin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Danella Zhao</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gang Zhou</a> (1) ((1) William &amp; Mary, (2) Old Dominion University)",
          "description": "As the number of IoT devices has increased rapidly, IoT botnets have\nexploited the vulnerabilities of IoT devices. However, it is still challenging\nto detect the initial intrusion on IoT devices prior to massive attacks. Recent\nstudies have utilized power side-channel information to characterize this\nintrusion behavior on IoT devices but still lack real-time detection\napproaches. This study aimed to design an online intrusion detection system\ncalled DeepAuditor for IoT devices via power auditing. To realize the real-time\nsystem, we first proposed a lightweight power auditing device called Power\nAuditor. With the Power Auditor, we developed a Distributed CNN classifier for\nonline inference in our laboratory setting. In order to protect data leakage\nand reduce networking redundancy, we also proposed a privacy-preserved\ninference protocol via Packed Homomorphic Encryption and a sliding window\nprotocol in our system. The classification accuracy and processing time were\nmeasured in our laboratory settings. We also demonstrated that the distributed\nCNN design is secure against any distributed components. Overall, the\nmeasurements were shown to the feasibility of our real-time distributed system\nfor intrusion detection on IoT devices.",
          "link": "http://arxiv.org/abs/2106.12753",
          "publishedOn": "2021-06-25T02:00:46.105Z",
          "wordCount": 649,
          "title": "DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing. (arXiv:2106.12753v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Anwesh Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>",
          "description": "In certain situations, Neural Networks (NN) are trained upon data that obey\nunderlying physical symmetries. However, it is not guaranteed that NNs will\nobey the underlying symmetry unless embedded in the network structure. In this\nwork, we explore a special kind of symmetry where functions are invariant with\nrespect to involutory linear/affine transformations up to parity $p=\\pm 1$. We\ndevelop mathematical theorems and propose NN architectures that ensure\ninvariance and universal approximation properties. Numerical experiments\nindicate that the proposed models outperform baseline networks while respecting\nthe imposed symmetry. An adaption of our technique to convolutional NN\nclassification tasks for datasets with inherent horizontal/vertical reflection\nsymmetry has also been proposed.",
          "link": "http://arxiv.org/abs/2106.12891",
          "publishedOn": "2021-06-25T02:00:46.099Z",
          "wordCount": 547,
          "title": "Encoding Involutory Invariance in Neural Networks. (arXiv:2106.12891v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1\">Crist&#xf3;bal Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nishant A. Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_A/0/1/0/all/0/1\">Ali Mortazavi</a>",
          "description": "Much of the work in online learning focuses on the study of sublinear upper\nbounds on the regret. In this work, we initiate the study of best-case lower\nbounds in online convex optimization, wherein we bound the largest improvement\nan algorithm can obtain relative to the single best action in hindsight. This\nproblem is motivated by the goal of better understanding the adaptivity of a\nlearning algorithm. Another motivation comes from fairness: it is known that\nbest-case lower bounds are instrumental in obtaining algorithms for\ndecision-theoretic online learning (DTOL) that satisfy a notion of group\nfairness. Our contributions are a general method to provide best-case lower\nbounds in Follow The Regularized Leader (FTRL) algorithms with time-varying\nregularizers, which we use to show that best-case lower bounds are of the same\norder as existing upper regret bounds: this includes situations with a fixed\nlearning rate, decreasing learning rates, timeless methods, and adaptive\ngradient methods. In stark contrast, we show that the linearized version of\nFTRL can attain negative linear regret. Finally, in DTOL with two experts and\nbinary predictions, we fully characterize the best-case sequences, which\nprovides a finer understanding of the best-case lower bounds.",
          "link": "http://arxiv.org/abs/2106.12688",
          "publishedOn": "2021-06-25T02:00:46.064Z",
          "wordCount": 622,
          "title": "Best-Case Lower Bounds in Online Learning. (arXiv:2106.12688v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weg_B/0/1/0/all/0/1\">Bram van de Weg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greve_L/0/1/0/all/0/1\">Lars Greve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosic_B/0/1/0/all/0/1\">Bojana Rosic</a>",
          "description": "To incorporate prior knowledge as well as measurement uncertainties in the\ntraditional long short term memory (LSTM) neural networks, an efficient sparse\nBayesian training algorithm is introduced to the network architecture. The\nproposed scheme automatically determines relevant neural connections and adapts\naccordingly, in contrast to the classical LSTM solution. Due to its\nflexibility, the new LSTM scheme is less prone to overfitting, and hence can\napproximate time dependent solutions by use of a smaller data set. On a\nstructural nonlinear finite element application we show that the\nself-regulating framework does not require prior knowledge of a suitable\nnetwork architecture and size, while ensuring satisfying accuracy at reasonable\ncomputational cost.",
          "link": "http://arxiv.org/abs/2106.12694",
          "publishedOn": "2021-06-25T02:00:46.048Z",
          "wordCount": 538,
          "title": "Long short-term relevance learning. (arXiv:2106.12694v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12619",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lee_K/0/1/0/all/0/1\">Kookjin Lee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Trask_N/0/1/0/all/0/1\">Nathaniel A. Trask</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1\">Panos Stinis</a>",
          "description": "Forecasting of time-series data requires imposition of inductive biases to\nobtain predictive extrapolation, and recent works have imposed\nHamiltonian/Lagrangian form to preserve structure for systems with reversible\ndynamics. In this work we present a novel parameterization of dissipative\nbrackets from metriplectic dynamical systems appropriate for learning\nirreversible dynamics with unknown a priori model form. The process learns\ngeneralized Casimirs for energy and entropy guaranteed to be conserved and\nnondecreasing, respectively. Furthermore, for the case of added thermal noise,\nwe guarantee exact preservation of a fluctuation-dissipation theorem, ensuring\nthermodynamic consistency. We provide benchmarks for dissipative systems\ndemonstrating learned dynamics are more robust and generalize better than\neither \"black-box\" or penalty-based approaches.",
          "link": "http://arxiv.org/abs/2106.12619",
          "publishedOn": "2021-06-25T02:00:46.032Z",
          "wordCount": 545,
          "title": "Machine learning structure preserving brackets for forecasting irreversible processes. (arXiv:2106.12619v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanchu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Existing bias mitigation methods for DNN models primarily work on learning\ndebiased encoders. This process not only requires a lot of instance-level\nannotations for sensitive attributes, it also does not guarantee that all\nfairness sensitive information has been removed from the encoder. To address\nthese limitations, we explore the following research question: Can we reduce\nthe discrimination of DNN models by only debiasing the classification head,\neven with biased representations as inputs? To this end, we propose a new\nmitigation technique, namely, Representation Neutralization for Fairness (RNF)\nthat achieves fairness by debiasing only the task-specific classification head\nof DNN models. To this end, we leverage samples with the same ground-truth\nlabel but different sensitive attributes, and use their neutralized\nrepresentations to train the classification head of the DNN model. The key idea\nof RNF is to discourage the classification head from capturing spurious\ncorrelation between fairness sensitive information in encoder representations\nwith specific class labels. To address low-resource settings with no access to\nsensitive attribute annotations, we leverage a bias-amplified model to generate\nproxy annotations for sensitive attributes. Experimental results over several\nbenchmark datasets demonstrate our RNF framework to effectively reduce\ndiscrimination of DNN models with minimal degradation in task-specific\nperformance.",
          "link": "http://arxiv.org/abs/2106.12674",
          "publishedOn": "2021-06-25T02:00:45.997Z",
          "wordCount": 641,
          "title": "Fairness via Representation Neutralization. (arXiv:2106.12674v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Luhuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Andrew Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_L/0/1/0/all/0/1\">Lauren Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John Cunningham</a>",
          "description": "We examine the general problem of inter-domain Gaussian Processes (GPs):\nproblems where the GP realization and the noisy observations of that\nrealization lie on different domains. When the mapping between those domains is\nlinear, such as integration or differentiation, inference is still closed form.\nHowever, many of the scaling and approximation techniques that our community\nhas developed do not apply to this setting. In this work, we introduce the\nhierarchical inducing point GP (HIP-GP), a scalable inter-domain GP inference\nmethod that enables us to improve the approximation accuracy by increasing the\nnumber of inducing points to the millions. HIP-GP, which relies on inducing\npoints with grid structure and a stationary kernel assumption, is suitable for\nlow-dimensional problems. In developing HIP-GP, we introduce (1) a fast\nwhitening strategy, and (2) a novel preconditioner for conjugate gradients\nwhich can be helpful in general GP settings. Our code is available at https:\n//github.com/cunningham-lab/hipgp.",
          "link": "http://arxiv.org/abs/2103.00393",
          "publishedOn": "2021-06-25T02:00:45.992Z",
          "wordCount": 620,
          "title": "Hierarchical Inducing Point Gaussian Process for Inter-domain Observations. (arXiv:2103.00393v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1\">Samuel Ainsworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1\">Kendall Lowrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Siddhartha Srinivasa</a>",
          "description": "We study the estimation of policy gradients for continuous-time systems with\nknown dynamics. By reframing policy learning in continuous-time, we show that\nit is possible construct a more efficient and accurate gradient estimator. The\nstandard back-propagation through time estimator (BPTT) computes exact\ngradients for a crude discretization of the continuous-time system. In\ncontrast, we approximate continuous-time gradients in the original system. With\nthe explicit goal of estimating continuous-time gradients, we are able to\ndiscretize adaptively and construct a more efficient policy gradient estimator\nwhich we call the Continuous-Time Policy Gradient (CTPG). We show that\nreplacing BPTT policy gradients with more efficient CTPG estimates results in\nfaster and more robust learning in a variety of control tasks and simulators.",
          "link": "http://arxiv.org/abs/2012.06684",
          "publishedOn": "2021-06-25T02:00:45.965Z",
          "wordCount": 587,
          "title": "Faster Policy Learning with Continuous-Time Gradients. (arXiv:2012.06684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anjali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Shamanth R Nayak K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>",
          "description": "Explainability techniques for Graph Neural Networks still have a long way to\ngo compared to explanations available for both neural and decision decision\ntree-based models trained on tabular data. Using a task that straddles both\ngraphs and tabular data, namely Entity Matching, we comment on key aspects of\nexplainability that are missing in GNN model explanations.",
          "link": "http://arxiv.org/abs/2106.12665",
          "publishedOn": "2021-06-25T02:00:45.959Z",
          "wordCount": 500,
          "title": "Reimagining GNN Explanations with ideas from Tabular Data. (arXiv:2106.12665v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>",
          "description": "Recommender systems -- and especially matrix factorization-based\ncollaborative filtering algorithms -- play a crucial role in mediating our\naccess to online information. We show that such algorithms induce a particular\nkind of stereotyping: if preferences for a \\textit{set} of items are\nanti-correlated in the general user population, then those items may not be\nrecommended together to a user, regardless of that user's preferences and\nratings history. First, we introduce a notion of \\textit{joint accessibility},\nwhich measures the extent to which a set of items can jointly be accessed by\nusers. We then study joint accessibility under the standard factorization-based\ncollaborative filtering framework, and provide theoretical necessary and\nsufficient conditions when joint accessibility is violated. Moreover, we show\nthat these conditions can easily be violated when the users are represented by\na single feature vector. To improve joint accessibility, we further propose an\nalternative modelling fix, which is designed to capture the diverse multiple\ninterests of each user using a multi-vector representation. We conduct\nextensive experiments on real and simulated datasets, demonstrating the\nstereotyping problem with standard single-vector matrix factorization models.",
          "link": "http://arxiv.org/abs/2106.12622",
          "publishedOn": "2021-06-25T02:00:45.954Z",
          "wordCount": 615,
          "title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems. (arXiv:2106.12622v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1\">Tasnim Mohiuddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>",
          "description": "Transfer learning has yielded state-of-the-art (SoTA) results in many\nsupervised NLP tasks. However, annotated data for every target task in every\ntarget language is rare, especially for low-resource languages. We propose\nUXLA, a novel unsupervised data augmentation framework for zero-resource\ntransfer learning scenarios. In particular, UXLA aims to solve cross-lingual\nadaptation problems from a source language task distribution to an unknown\ntarget language task distribution, assuming no training label in the target\nlanguage. At its core, UXLA performs simultaneous self-training with data\naugmentation and unsupervised sample selection. To show its effectiveness, we\nconduct extensive experiments on three diverse zero-resource cross-lingual\ntransfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the\nbaselines by a good margin. With an in-depth framework dissection, we\ndemonstrate the cumulative contributions of different components to its\nsuccess.",
          "link": "http://arxiv.org/abs/2004.13240",
          "publishedOn": "2021-06-25T02:00:45.948Z",
          "wordCount": 611,
          "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for {Zero-Resource} Cross-Lingual NLP. (arXiv:2004.13240v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_R/0/1/0/all/0/1\">Ritik Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwah_R/0/1/0/all/0/1\">Rishika Kushwah</a>",
          "description": "The reliance of humans over machines has never been so high such that from\nobject classification in photographs to adding sound to silent movies\neverything can be performed with the help of deep learning and machine learning\nalgorithms. Likewise, Handwritten text recognition is one of the significant\nareas of research and development with a streaming number of possibilities that\ncould be attained. Handwriting recognition (HWR), also known as Handwritten\nText Recognition (HTR), is the ability of a computer to receive and interpret\nintelligible handwritten input from sources such as paper documents,\nphotographs, touch-screens and other devices [1]. Apparently, in this paper, we\nhave performed handwritten digit recognition with the help of MNIST datasets\nusing Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\nConvolution Neural Network (CNN) models. Our main objective is to compare the\naccuracy of the models stated above along with their execution time to get the\nbest possible model for digit recognition.",
          "link": "http://arxiv.org/abs/2106.12614",
          "publishedOn": "2021-06-25T02:00:45.942Z",
          "wordCount": 620,
          "title": "Handwritten Digit Recognition using Machine and Deep Learning Algorithms. (arXiv:2106.12614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianhao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliu Liu</a>",
          "description": "Although neural networks are widely used, it remains challenging to formally\nverify the safety and robustness of neural networks in real-world applications.\nExisting methods are designed to verify the network before use, which is\nlimited to relatively simple specifications and fixed networks. These methods\nare not ready to be applied to real-world problems with complex and/or\ndynamically changing specifications and networks. To effectively handle\ndynamically changing specifications and networks, the verification needs to be\nperformed online when these changes take place. However, it is still\nchallenging to run existing verification algorithms online. Our key insight is\nthat we can leverage the temporal dependencies of these changes to accelerate\nthe verification process, e.g., by warm starting new online verification using\nprevious verified results. This paper establishes a novel framework for\nscalable online verification to solve real-world verification problems with\ndynamically changing specifications and/or networks, known as domain shift and\nweight shift respectively. We propose three types of techniques (branch\nmanagement, perturbation tolerance analysis, and incremental computation) to\naccelerate the online verification of deep neural networks. Experiment results\nshow that our online verification algorithm is up to two orders of magnitude\nfaster than existing verification algorithms, and thus can scale to real-world\napplications.",
          "link": "http://arxiv.org/abs/2106.12732",
          "publishedOn": "2021-06-25T02:00:45.926Z",
          "wordCount": 636,
          "title": "Online Verification of Deep Neural Networks under Domain or Weight Shift. (arXiv:2106.12732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Continuous deep learning architectures enable learning of flexible\nprobabilistic models for predictive modeling as neural ordinary differential\nequations (ODEs), and for generative modeling as continuous normalizing flows.\nIn this work, we design a framework to decipher the internal dynamics of these\ncontinuous depth models by pruning their network architectures. Our empirical\nresults suggest that pruning improves generalization for neural ODEs in\ngenerative modeling. Moreover, pruning finds minimal and efficient neural ODE\nrepresentations with up to 98\\% less parameters compared to the original\nnetwork, without loss of accuracy. Finally, we show that by applying pruning we\ncan obtain insightful information about the design of better neural ODEs.We\nhope our results will invigorate further research into the performance-size\ntrade-offs of modern continuous-depth models.",
          "link": "http://arxiv.org/abs/2106.12718",
          "publishedOn": "2021-06-25T02:00:45.921Z",
          "wordCount": 548,
          "title": "Sparse Flows: Pruning Continuous-depth Models. (arXiv:2106.12718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12594",
          "author": "<a href=\"http://arxiv.org/find/gr-qc/1/au:+Dax_M/0/1/0/all/0/1\">Maximilian Dax</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Green_S/0/1/0/all/0/1\">Stephen R. Green</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Gair_J/0/1/0/all/0/1\">Jonathan Gair</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Macke_J/0/1/0/all/0/1\">Jakob H. Macke</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Buonanno_A/0/1/0/all/0/1\">Alessandra Buonanno</a>, <a href=\"http://arxiv.org/find/gr-qc/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "We demonstrate unprecedented accuracy for rapid gravitational-wave parameter\nestimation with deep learning. Using neural networks as surrogates for Bayesian\nposterior distributions, we analyze eight gravitational-wave events from the\nfirst LIGO-Virgo Gravitational-Wave Transient Catalog and find very close\nquantitative agreement with standard inference codes, but with inference times\nreduced from O(day) to a minute per event. Our networks are trained using\nsimulated data, including an estimate of the detector-noise characteristics\nnear the event. This encodes the signal and noise models within millions of\nneural-network parameters, and enables inference for any observed data\nconsistent with the training distribution, accounting for noise nonstationarity\nfrom event to event. Our algorithm -- called \"DINGO\" -- sets a new standard in\nfast-and-accurate inference of physical parameters of detected\ngravitational-wave events, which should enable real-time data analysis without\nsacrificing accuracy.",
          "link": "http://arxiv.org/abs/2106.12594",
          "publishedOn": "2021-06-25T02:00:45.915Z",
          "wordCount": 592,
          "title": "Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v1 [gr-qc])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1\">Saeed Mohammadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_V/0/1/0/all/0/1\">Vitor H.Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamare_R/0/1/0/all/0/1\">Rodrigo C. de Lamare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukrer_O/0/1/0/all/0/1\">Osman Kukrer</a>",
          "description": "In this paper, a novel and robust algorithm is proposed for adaptive\nbeamforming based on the idea of reconstructing the autocorrelation sequence\n(ACS) of a random process from a set of measured data. This is obtained from\nthe first column and the first row of the sample covariance matrix (SCM) after\naveraging along its diagonals. Then, the power spectrum of the correlation\nsequence is estimated using the discrete Fourier transform (DFT). The DFT\ncoefficients corresponding to the angles within the noise-plus-interference\nregion are used to reconstruct the noise-plus-interference covariance matrix\n(NPICM), while the desired signal covariance matrix (DSCM) is estimated by\nidentifying and removing the noise-plus-interference component from the SCM. In\nparticular, the spatial power spectrum of the estimated received signal is\nutilized to compute the correlation sequence corresponding to the\nnoise-plus-interference in which the dominant DFT coefficient of the\nnoise-plus-interference is captured. A key advantage of the proposed adaptive\nbeamforming is that only little prior information is required. Specifically, an\nimprecise knowledge of the array geometry and of the angular sectors in which\nthe interferences are located is needed. Simulation results demonstrate that\ncompared with previous reconstruction-based beamformers, the proposed approach\ncan achieve better overall performance in the case of multiple mismatches over\na very large range of input signal-to-noise ratios.",
          "link": "http://arxiv.org/abs/2106.12663",
          "publishedOn": "2021-06-25T02:00:45.909Z",
          "wordCount": 663,
          "title": "Study of Robust Adaptive Beamforming Based on Low-Complexity DFT Spatial Sampling. (arXiv:2106.12663v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12627",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">Hsin-Yuan Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kueng_R/0/1/0/all/0/1\">Richard Kueng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Torlai_G/0/1/0/all/0/1\">Giacomo Torlai</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albert_V/0/1/0/all/0/1\">Victor V. Albert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Preskill_J/0/1/0/all/0/1\">John Preskill</a>",
          "description": "Classical machine learning (ML) provides a potentially powerful approach to\nsolving challenging quantum many-body problems in physics and chemistry.\nHowever, the advantages of ML over more traditional methods have not been\nfirmly established. In this work, we prove that classical ML algorithms can\nefficiently predict ground state properties of gapped Hamiltonians in finite\nspatial dimensions, after learning from data obtained by measuring other\nHamiltonians in the same quantum phase of matter. In contrast, under widely\naccepted complexity theory assumptions, classical algorithms that do not learn\nfrom data cannot achieve the same guarantee. We also prove that classical ML\nalgorithms can efficiently classify a wide range of quantum phases of matter.\nOur arguments are based on the concept of a classical shadow, a succinct\nclassical description of a many-body quantum state that can be constructed in\nfeasible quantum experiments and be used to predict many properties of the\nstate. Extensive numerical experiments corroborate our theoretical results in a\nvariety of scenarios, including Rydberg atom systems, 2D random Heisenberg\nmodels, symmetry-protected topological phases, and topologically ordered\nphases.",
          "link": "http://arxiv.org/abs/2106.12627",
          "publishedOn": "2021-06-25T02:00:45.903Z",
          "wordCount": 625,
          "title": "Provably efficient machine learning for quantum many-body problems. (arXiv:2106.12627v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "In temporal difference (TD) learning, off-policy sampling is known to be more\npractical than on-policy sampling, and by decoupling learning from data\ncollection, it enables data reuse. It is known that policy evaluation\n(including multi-step off-policy importance sampling) has the interpretation of\nsolving a generalized Bellman equation. In this paper, we derive finite-sample\nbounds for any general off-policy TD-like stochastic approximation algorithm\nthat solves for the fixed-point of this generalized Bellman operator. Our key\nstep is to show that the generalized Bellman operator is simultaneously a\ncontraction mapping with respect to a weighted $\\ell_p$-norm for each $p$ in\n$[1,\\infty)$, with a common contraction factor.\n\nOff-policy TD-learning is known to suffer from high variance due to the\nproduct of importance sampling ratios. A number of algorithms (e.g.\n$Q^\\pi(\\lambda)$, Tree-Backup$(\\lambda)$, Retrace$(\\lambda)$, and $Q$-trace)\nhave been proposed in the literature to address this issue. Our results\nimmediately imply finite-sample bounds of these algorithms. In particular, we\nprovide first-known finite-sample guarantees for $Q^\\pi(\\lambda)$,\nTree-Backup$(\\lambda)$, and Retrace$(\\lambda)$, and improve the best known\nbounds of $Q$-trace in [19]. Moreover, we show the bias-variance trade-offs in\neach of these algorithms.",
          "link": "http://arxiv.org/abs/2106.12729",
          "publishedOn": "2021-06-25T02:00:45.888Z",
          "wordCount": 627,
          "title": "Finite-Sample Analysis of Off-Policy TD-Learning via Generalized Bellman Operators. (arXiv:2106.12729v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pashine_S/0/1/0/all/0/1\">Samay Pashine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandiya_S/0/1/0/all/0/1\">Sagar Mandiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Praveen Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_R/0/1/0/all/0/1\">Rashid Sheikh</a>",
          "description": "Deep Learning as a field has been successfully used to solve a plethora of\ncomplex problems, the likes of which we could not have imagined a few decades\nback. But as many benefits as it brings, there are still ways in which it can\nbe used to bring harm to our society. Deep fakes have been proven to be one\nsuch problem, and now more than ever, when any individual can create a fake\nimage or video simply using an application on the smartphone, there need to be\nsome countermeasures, with which we can detect if the image or video is a fake\nor real and dispose of the problem threatening the trustworthiness of online\ninformation. Although the Deep fakes created by neural networks, may seem to be\nas real as a real image or video, it still leaves behind spatial and temporal\ntraces or signatures after moderation, these signatures while being invisible\nto a human eye can be detected with the help of a neural network trained to\nspecialize in Deep fake detection. In this paper, we analyze several such\nstates of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\nNet) and compare them against each other, to find an optimal solution for\nvarious scenarios like real-time deep fake detection to be deployed in online\nsocial media platforms where the classification should be made as fast as\npossible or for a small news agency where the classification need not be in\nreal-time but requires utmost accuracy.",
          "link": "http://arxiv.org/abs/2106.12605",
          "publishedOn": "2021-06-25T02:00:45.883Z",
          "wordCount": 713,
          "title": "Deep Fake Detection: Survey of Facial Manipulation Detection Solutions. (arXiv:2106.12605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12704",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mizota_Y/0/1/0/all/0/1\">Yusuke Mizota</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hamada_N/0/1/0/all/0/1\">Naoki Hamada</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ichiki_S/0/1/0/all/0/1\">Shunsuke Ichiki</a>",
          "description": "A multi-objective optimization problem is $C^r$ weakly simplicial if there\nexists a $C^r$ surjection from a simplex onto the Pareto set/front such that\nthe image of each subsimplex is the Pareto set/front of a subproblem, where\n$0\\leq r\\leq \\infty$. This property is helpful to compute a parametric-surface\napproximation of the entire Pareto set and Pareto front. It is known that all\nunconstrained strongly convex $C^r$ problems are $C^{r-1}$ weakly simplicial\nfor $1\\leq r \\leq \\infty$. In this paper, we show that all unconstrained\nstrongly convex problems are $C^0$ weakly simplicial. The usefulness of this\ntheorem is demonstrated in a sparse modeling application: we reformulate the\nelastic net as a non-differentiable multi-objective strongly convex problem and\napproximate its Pareto set (the set of all trained models with different\nhyper-parameters) and Pareto front (the set of performance metrics of the\ntrained models) by using a B\\'ezier simplex fitting method, which accelerates\nhyper-parameter search.",
          "link": "http://arxiv.org/abs/2106.12704",
          "publishedOn": "2021-06-25T02:00:45.847Z",
          "wordCount": 602,
          "title": "All unconstrained strongly convex problems are weakly simplicial. (arXiv:2106.12704v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Anurag Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1\">Seth Cooper</a>",
          "description": "Variational autoencoders (VAEs) have been used in prior works for generating\nand blending levels from different games. To add controllability to these\nmodels, conditional VAEs (CVAEs) were recently shown capable of generating\noutput that can be modified using labels specifying desired content, albeit\nworking with segments of levels and platformers exclusively. We expand these\nworks by using CVAEs for generating whole platformer and dungeon levels, and\nblending levels across these genres. We show that CVAEs can reliably control\ndoor placement in dungeons and progression direction in platformer levels.\nThus, by using appropriate labels, our approach can generate whole dungeons and\nplatformer levels of interconnected rooms and segments respectively as well as\nlevels that blend dungeons and platformers. We demonstrate our approach using\nThe Legend of Zelda, Metroid, Mega Man and Lode Runner.",
          "link": "http://arxiv.org/abs/2106.12692",
          "publishedOn": "2021-06-25T02:00:45.831Z",
          "wordCount": 562,
          "title": "Dungeon and Platformer Level Blending and Generation using Conditional VAEs. (arXiv:2106.12692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1\">Vladimir Aliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "Normalizing flows are a powerful class of generative models demonstrating\nstrong performance in several speech and vision problems. In contrast to other\ngenerative models, normalizing flows have tractable likelihoods and allow for\nstable training. However, they have to be carefully designed to represent\ninvertible functions with efficient Jacobian determinant calculation. In\npractice, these requirements lead to overparameterized and sophisticated\narchitectures that are inferior to alternative feed-forward models in terms of\ninference time and memory consumption. In this work, we investigate whether one\ncan distill knowledge from flow-based models to more efficient alternatives. We\nprovide a positive answer to this question by proposing a simple distillation\napproach and demonstrating its effectiveness on state-of-the-art conditional\nflow-based models for image super-resolution and speech synthesis.",
          "link": "http://arxiv.org/abs/2106.12699",
          "publishedOn": "2021-06-25T02:00:45.816Z",
          "wordCount": 549,
          "title": "Distilling the Knowledge from Normalizing Flows. (arXiv:2106.12699v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wei-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daniel Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hsiang-Fu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Choon-Hui Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1\">Kedarnath Kolluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shandilya_N/0/1/0/all/0/1\">Nikhil Shandilya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ievgrafov_V/0/1/0/all/0/1\">Vyacheslav Ievgrafov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Japinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>",
          "description": "We consider the problem of semantic matching in product search: given a\ncustomer query, retrieve all semantically related products from a huge catalog\nof size 100 million, or more. Because of large catalog spaces and real-time\nlatency constraints, semantic matching algorithms not only desire high recall\nbut also need to have low latency. Conventional lexical matching approaches\n(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but\nfail to capture behavioral signals between queries and products. In contrast,\nembedding-based models learn semantic representations from customer behavior\ndata, but the performance is often limited by shallow neural encoders due to\nlatency constraints. Semantic product search can be viewed as an eXtreme\nMulti-label Classification (XMC) problem, where customer queries are input\ninstances and products are output labels. In this paper, we aim to improve\nsemantic product search by using tree-based XMC models where inference time\ncomplexity is logarithmic in the number of products. We consider hierarchical\nlinear models with n-gram features for fast real-time inference.\nQuantitatively, our method maintains a low latency of 1.25 milliseconds per\nquery and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a\ncompeting embedding-based DSSM model. Our model is robust to weight pruning\nwith varying thresholds, which can flexibly meet different system requirements\nfor online deployments. Qualitatively, our method can retrieve products that\nare complementary to existing product search system and add diversity to the\nmatch set.",
          "link": "http://arxiv.org/abs/2106.12657",
          "publishedOn": "2021-06-25T02:00:45.800Z",
          "wordCount": 695,
          "title": "Extreme Multi-label Learning for Semantic Matching in Product Search. (arXiv:2106.12657v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianlong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>",
          "description": "The claims data, containing medical codes, services information, and incurred\nexpenditure, can be a good resource for estimating an individual's health\ncondition and medical risk level. In this study, we developed Transformer-based\nMultimodal AutoEncoder (TMAE), an unsupervised learning framework that can\nlearn efficient patient representation by encoding meaningful information from\nthe claims data. TMAE is motivated by the practical needs in healthcare to\nstratify patients into different risk levels for improving care delivery and\nmanagement. Compared to previous approaches, TMAE is able to 1) model\ninpatient, outpatient, and medication claims collectively, 2) handle irregular\ntime intervals between medical events, 3) alleviate the sparsity issue of the\nrare medical codes, and 4) incorporate medical expenditure information. We\ntrained TMAE using a real-world pediatric claims dataset containing more than\n600,000 patients and compared its performance with various approaches in two\nclustering tasks. Experimental results demonstrate that TMAE has superior\nperformance compared to all baselines. Multiple downstream applications are\nalso conducted to illustrate the effectiveness of our framework. The promising\nresults confirm that the TMAE framework is scalable to large claims data and is\nable to generate efficient patient embeddings for risk stratification and\nanalysis.",
          "link": "http://arxiv.org/abs/2106.12658",
          "publishedOn": "2021-06-25T02:00:45.777Z",
          "wordCount": 634,
          "title": "Transformer-based unsupervised patient representation learning based on medical claims for risk stratification and analysis. (arXiv:2106.12658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ibayashi_H/0/1/0/all/0/1\">Hikaru Ibayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamaguchi_T/0/1/0/all/0/1\">Takuo Hamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaizum_M/0/1/0/all/0/1\">Masaaki Imaizum</a>",
          "description": "Toward achieving robust and defensive neural networks, the robustness against\nthe weight parameters perturbations, i.e., sharpness, attracts attention in\nrecent years (Sun et al., 2020). However, sharpness is known to remain a\ncritical issue, \"scale-sensitivity.\" In this paper, we propose a novel\nsharpness measure, Minimum Sharpness. It is known that NNs have a specific\nscale transformation that constitutes equivalent classes where functional\nproperties are completely identical, and at the same time, their sharpness\ncould change unlimitedly. We define our sharpness through a minimization\nproblem over the equivalent NNs being invariant to the scale transformation. We\nalso develop an efficient and exact technique to make the sharpness tractable,\nwhich reduces the heavy computational costs involved with Hessian. In the\nexperiment, we observed that our sharpness has a valid correlation with the\ngeneralization of NNs and runs with less computational cost than existing\nsharpness measures.",
          "link": "http://arxiv.org/abs/2106.12612",
          "publishedOn": "2021-06-25T02:00:45.761Z",
          "wordCount": 587,
          "title": "Minimum sharpness: Scale-invariant parameter-robustness of neural networks. (arXiv:2106.12612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-06-25T02:00:45.755Z",
          "wordCount": 628,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherapanamjeri_Y/0/1/0/all/0/1\">Yeshwanth Cherapanamjeri</a>",
          "description": "We consider the phenomenon of adversarial examples in ReLU networks with\nindependent gaussian parameters. For networks of constant depth and with a\nlarge range of widths (for instance, it suffices if the width of each layer is\npolynomial in that of any other layer), small perturbations of input vectors\nlead to large changes of outputs. This generalizes results of Daniely and\nSchacham (2020) for networks of rapidly decreasing width and of Bubeck et al\n(2021) for two-layer networks. The proof shows that adversarial examples arise\nin these networks because the functions that they compute are very close to\nlinear. Bottleneck layers in the network play a key role: the minimal width up\nto some point in the network determines scales and sensitivities of mappings\ncomputed up to that point. The main result is for networks with constant depth,\nbut we also show that some constraint on depth is necessary for a result of\nthis kind, because there are suitably deep networks that, with constant\nprobability, compute a function that is close to constant.",
          "link": "http://arxiv.org/abs/2106.12611",
          "publishedOn": "2021-06-25T02:00:45.750Z",
          "wordCount": 610,
          "title": "Adversarial Examples in Multi-Layer Random ReLU Networks. (arXiv:2106.12611v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1\">Hayden S. Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdin_M/0/1/0/all/0/1\">Marah Abdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedigo_B/0/1/0/all/0/1\">Benjamin D. Pedigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1\">Shweti Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyzinski_V/0/1/0/all/0/1\">Vince Lyzinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Amitabh Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_P/0/1/0/all/0/1\">Piali~Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Christopher M. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey E. Priebe</a>",
          "description": "In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.",
          "link": "http://arxiv.org/abs/2106.12621",
          "publishedOn": "2021-06-25T02:00:45.729Z",
          "wordCount": 616,
          "title": "Leveraging semantically similar queries for ranking via combining representations. (arXiv:2106.12621v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1\">Niloofar Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_W/0/1/0/all/0/1\">Weston Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Derrick Liu</a>",
          "description": "Monitoring network traffic to identify content, services, and applications is\nan active research topic in network traffic control systems. While modern\nfirewalls provide the capability to decrypt packets, this is not appealing for\nprivacy advocates. Hence, identifying any information from encrypted traffic is\na challenging task. Nonetheless, previous work has identified machine learning\nmethods that may enable application and service identification. The process\ninvolves high level feature extraction from network packet data then training a\nrobust machine learning classifier for traffic identification. We propose a\nclassification technique using an ensemble of deep learning architectures on\npacket, payload, and inter-arrival time sequences. To our knowledge, this is\nthe first time such deep learning architectures have been applied to the Server\nName Indication (SNI) classification problem. Our ensemble model beats the\nstate of the art machine learning methods and our up-to-date model can be found\non github: \\url{https://github.com/niloofarbayat/NetworkClassification}",
          "link": "http://arxiv.org/abs/2106.12693",
          "publishedOn": "2021-06-25T02:00:45.666Z",
          "wordCount": 584,
          "title": "Deep Learning for Network Traffic Classification. (arXiv:2106.12693v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uniyal_A/0/1/0/all/0/1\">Archit Uniyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotti_S/0/1/0/all/0/1\">Sasikanth Kotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1\">Patrik Joslin Kenfack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1\">Andrew Trask</a>",
          "description": "Recent advances in differentially private deep learning have demonstrated\nthat application of differential privacy, specifically the DP-SGD algorithm,\nhas a disparate impact on different sub-groups in the population, which leads\nto a significantly high drop-in model utility for sub-populations that are\nunder-represented (minorities), compared to well-represented ones. In this\nwork, we aim to compare PATE, another mechanism for training deep learning\nmodels using differential privacy, with DP-SGD in terms of fairness. We show\nthat PATE does have a disparate impact too, however, it is much less severe\nthan DP-SGD. We draw insights from this observation on what might be promising\ndirections in achieving better fairness-privacy trade-offs.",
          "link": "http://arxiv.org/abs/2106.12576",
          "publishedOn": "2021-06-25T02:00:45.661Z",
          "wordCount": 562,
          "title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?. (arXiv:2106.12576v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1\">Ting-hao</a> (Kenneth) <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1\">Rebecca J. Passonneau</a>",
          "description": "Atomic clauses are fundamental text units for understanding complex\nsentences. Identifying the atomic sentences within complex sentences is\nimportant for applications such as summarization, argument mining, discourse\nanalysis, discourse parsing, and question answering. Previous work mainly\nrelies on rule-based methods dependent on parsing. We propose a new task to\ndecompose each complex sentence into simple sentences derived from the tensed\nclauses in the source, and a novel problem formulation as a graph edit task.\nOur neural model learns to Accept, Break, Copy or Drop elements of a graph that\ncombines word adjacency and grammatical dependencies. The full processing\npipeline includes modules for graph construction, graph editing, and sentence\ngeneration from the output graph. We introduce DeSSE, a new dataset designed to\ntrain and evaluate complex sentence decomposition, and MinWiki, a subset of\nMinWikiSplit. ABCD achieves comparable performance as two parsing baselines on\nMinWiki. On DeSSE, which has a more even balance of complex sentence types, our\nmodel achieves higher accuracy on the number of atomic sentences than an\nencoder-decoder baseline. Results include a detailed error analysis.",
          "link": "http://arxiv.org/abs/2106.12027",
          "publishedOn": "2021-06-24T01:51:45.769Z",
          "wordCount": 649,
          "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1\">Thomas S. A. Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>",
          "description": "One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.",
          "link": "http://arxiv.org/abs/2106.12447",
          "publishedOn": "2021-06-24T01:51:45.764Z",
          "wordCount": 688,
          "title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Many learning-based 3D shape semantic segmentation methods assign labels to\nshape atoms (e.g. points in a point cloud or faces in a mesh) with a\nsingle-pass approach trained in an end-to-end fashion. Such methods achieve\nimpressive performance but require large amounts of labeled training data. This\nparadigm entangles two separable subproblems: (1) decomposing a shape into\nregions and (2) assigning semantic labels to these regions. We claim that\ndisentangling these subproblems reduces the labeled data burden: (1) region\ndecomposition requires no semantic labels and could be performed in an\nunsupervised fashion, and (2) labeling shape regions instead of atoms results\nin a smaller search space and should be learnable with less labeled training\ndata. In this paper, we investigate this second claim by presenting the\nNeurally-Guided Shape Parser (NGSP), a method that learns how to assign\nsemantic labels to regions of an over-segmented 3D shape. We solve this problem\nvia MAP inference, modeling the posterior probability of a labeling assignment\nconditioned on an input shape. We employ a Monte Carlo importance sampling\napproach guided by a neural proposal network, a search-based approach made\nfeasible by assuming the input shape is decomposed into discrete regions. We\nevaluate NGSP on the task of hierarchical semantic segmentation on manufactured\n3D shapes from PartNet. We find that NGSP delivers significant performance\nimprovements over baselines that learn to label shape atoms and then aggregate\npredictions for each shape region, especially in low-data regimes. Finally, we\ndemonstrate that NGSP is robust to region granularity, as it maintains strong\nsegmentation performance even as the regions undergo significant corruption.",
          "link": "http://arxiv.org/abs/2106.12026",
          "publishedOn": "2021-06-24T01:51:45.758Z",
          "wordCount": 719,
          "title": "The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>",
          "description": "Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to address such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the proposed weak predictors, their probabilities of sampling\nbetter architectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework produces\ncoarse-to-fine iteration to refine the ranking of sampling space gradually.\nExtensive experiments demonstrate that our method costs fewer samples to find\ntop-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as\nachieves the state-of-the-art ImageNet performance on the NASNet search space.\nIn particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,\nWeakNAS outperforms all of them with notable margins, e.g., requiring at least\n7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also\nabsorb them for further performance boost. We further strike the new SOTA\nresult of 81.3% in the ImageNet MobileNet Search Space. The code is available\nat https://github.com/VITA-Group/WeakNAS.",
          "link": "http://arxiv.org/abs/2102.10490",
          "publishedOn": "2021-06-24T01:51:45.753Z",
          "wordCount": 746,
          "title": "Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>",
          "description": "This paper studies regret minimization in multi-armed bandits, a classical\nonline learning problem. To develop more statistically-efficient algorithms, we\npropose to use the assumption of a random-effect model. In this model, the mean\nrewards of arms are drawn independently from an unknown distribution, whose\nparameters we estimate. We provide an estimator of the arm means in this model\nand also analyze its uncertainty. Based on these results, we design a UCB\nalgorithm, which we call ReUCB. We analyze ReUCB and prove a Bayes regret bound\non its $n$-round regret, which matches an existing lower bound. Our experiments\nshow that ReUCB can outperform Thompson sampling in various scenarios, without\nassuming that the prior distribution of arm means is known.",
          "link": "http://arxiv.org/abs/2106.12200",
          "publishedOn": "2021-06-24T01:51:45.748Z",
          "wordCount": 539,
          "title": "Random Effect Bandits. (arXiv:2106.12200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.01593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vial_D/0/1/0/all/0/1\">Daniel Vial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Advait Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "We propose two algorithms that use linear function approximation (LFA) for\nstochastic shortest path (SSP) and bound their regret over $K$ episodes. When\nall stationary policies are proper, our first algorithm obtains sublinear\nregret ($K^{3/4}$), is computationally efficient, and uses stationary policies.\nThis is the first LFA algorithm with these three properties, to the best of our\nknowledge. Our second algorithm improves the regret to $\\sqrt{K}$ when the\nfeature vectors satisfy certain assumptions. Both algorithms are special cases\nof a more general one, which has $\\sqrt{K}$ regret for general features given\naccess to a certain computation oracle. These algorithms and regret bounds are\nthe first for SSP with function approximation.",
          "link": "http://arxiv.org/abs/2105.01593",
          "publishedOn": "2021-06-24T01:51:45.741Z",
          "wordCount": 594,
          "title": "Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation. (arXiv:2105.01593v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "An innovations sequence of a time series is a sequence of independent and\nidentically distributed random variables with which the original time series\nhas a causal representation. The innovation at a time is statistically\nindependent of the prior history of the time series. As such, it represents the\nnew information contained at present but not in the past. Because of its simple\nprobability structure, an innovations sequence is the most efficient signature\nof the original. Unlike the principle or independent analysis (PCA/ICA)\nrepresentations, an innovations sequence preserves not only the complete\nstatistical properties but also the temporal order of the original time series.\nAn long-standing open problem is to find a computationally tractable way to\nextract an innovations sequence of non-Gaussian processes. This paper presents\na deep learning approach, referred to as Innovations Autoencoder (IAE), that\nextracts innovations sequences using a causal convolutional neural network. An\napplication of IAE to nonparametric anomaly detection with unknown anomaly and\nanomaly-free models is also presented.",
          "link": "http://arxiv.org/abs/2106.12382",
          "publishedOn": "2021-06-24T01:51:45.726Z",
          "wordCount": 595,
          "title": "Innovations Autoencoder and its Application in Real-Time Anomaly Detection. (arXiv:2106.12382v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1\">Abdullatif Albaseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1\">Mohamed Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>",
          "description": "In Federated edge learning (FEEL), energy-constrained devices at the network\nedge consume significant energy when training and uploading their local machine\nlearning models, leading to a decrease in their lifetime. This work proposes\nnovel solutions for energy-efficient FEEL by jointly considering local training\ndata, available computation, and communications resources, and deadline\nconstraints of FEEL rounds to reduce energy consumption. This paper considers a\nsystem model where the edge server is equipped with multiple antennas employing\nbeamforming techniques to communicate with the local users through orthogonal\nchannels. Specifically, we consider a problem that aims to find the optimal\nuser's resources, including the fine-grained selection of relevant training\nsamples, bandwidth, transmission power, beamforming weights, and processing\nspeed with the goal of minimizing the total energy consumption given a deadline\nconstraint on the communication rounds of FEEL. Then, we devise tractable\nsolutions by first proposing a novel fine-grained training algorithm that\nexcludes less relevant training samples and effectively chooses only the\nsamples that improve the model's performance. After that, we derive closed-form\nsolutions, followed by a Golden-Section-based iterative algorithm to find the\noptimal computation and communication resources that minimize energy\nconsumption. Experiments using MNIST and CIFAR-10 datasets demonstrate that our\nproposed algorithms considerably outperform the state-of-the-art solutions as\nenergy consumption decreases by 79% for MNIST and 73% for CIFAR-10 datasets.",
          "link": "http://arxiv.org/abs/2106.12561",
          "publishedOn": "2021-06-24T01:51:45.721Z",
          "wordCount": 653,
          "title": "Fine-Grained Data Selection for Improved Energy Efficiency of Federated Edge Learning. (arXiv:2106.12561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhengqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.",
          "link": "http://arxiv.org/abs/2106.12378",
          "publishedOn": "2021-06-24T01:51:45.707Z",
          "wordCount": 577,
          "title": "Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12190",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1\">Mostafa Rahmani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "The idea of Innovation Search, which was initially proposed for data\nclustering, was recently used for outlier detection. In the application of\nInnovation Search for outlier detection, the directions of innovation were\nutilized to measure the innovation of the data points. We study the Innovation\nValues computed by the Innovation Search algorithm under a quadratic cost\nfunction and it is proved that Innovation Values with the new cost function are\nequivalent to Leverage Scores. This interesting connection is utilized to\nestablish several theoretical guarantees for a Leverage Score based robust PCA\nmethod and to design a new robust PCA method. The theoretical results include\nperformance guarantees with different models for the distribution of outliers\nand the distribution of inliers. In addition, we demonstrate the robustness of\nthe algorithms against the presence of noise. The numerical and theoretical\nstudies indicate that while the presented approach is fast and closed-form, it\ncan outperform most of the existing algorithms.",
          "link": "http://arxiv.org/abs/2106.12190",
          "publishedOn": "2021-06-24T01:51:45.703Z",
          "wordCount": 610,
          "title": "Closed-Form, Provable, and Robust PCA via Leverage Statistics and Innovation Search. (arXiv:2106.12190v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>",
          "description": "Commonsense reasoning is one of the key problems in natural language\nprocessing, but the relative scarcity of labeled data holds back the progress\nfor languages other than English. Pretrained cross-lingual models are a source\nof powerful language-agnostic representations, yet their inherent reasoning\ncapabilities are still actively studied. In this work, we design a simple\napproach to commonsense reasoning which trains a linear classifier with weights\nof multi-head attention as features. To evaluate this approach, we create a\nmultilingual Winograd Schema corpus by processing several datasets from prior\nwork within a standardized pipeline and measure cross-lingual generalization\nability in terms of out-of-sample performance. The method performs\ncompetitively with recent supervised and unsupervised approaches for\ncommonsense reasoning, even when applied to other languages in a zero-shot\nmanner. Also, we demonstrate that most of the performance is given by the same\nsmall subset of attention heads for all studied languages, which provides\nevidence of universal reasoning capabilities in multilingual encoders.",
          "link": "http://arxiv.org/abs/2106.12066",
          "publishedOn": "2021-06-24T01:51:45.698Z",
          "wordCount": 622,
          "title": "It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostina_V/0/1/0/all/0/1\">Victoria Kostina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "This paper considers quantized distributed optimization algorithms in the\nparameter server framework of distributed training. We introduce the principle\nwe call Differential Quantization (DQ) that prescribes that the past\nquantization errors should be compensated in such a way as to direct the\ndescent trajectory of a quantized algorithm towards that of its unquantized\ncounterpart. Assuming that the objective function is smooth and strongly\nconvex, we prove that in the limit of large problem dimension, Differentially\nQuantized Gradient Descent (DQ-GD) attains a linear contraction factor of\n$\\max\\{\\sigma_{\\mathrm{GD}}, 2^{-R}\\}$, where $\\sigma_{\\mathrm{GD}}$ is the\ncontraction factor of unquantized gradient descent (GD). Thus at any\n$R\\geq\\log_2 1 /\\sigma_{\\mathrm{GD}}$ bits, the contraction factor of DQ-GD is\nthe same as that of unquantized GD, i.e., there is no loss due to quantization.\nWe show a converse demonstrating that no quantized gradient descent algorithm\ncan converge faster than $\\max\\{\\sigma_{\\mathrm{GD}}, 2^{-R}\\}$. In contrast,\nnaively quantized GD where the worker directly quantizes the gradient barely\nattains $\\sigma_{\\mathrm{GD}} + 2^{-R}$. The principle of differential\nquantization continues to apply to gradient methods with momentum such as\nNesterov's accelerated gradient descent, and Polyak's heavy ball method. For\nthese algorithms as well, if the rate is above a certain threshold, there is no\nloss in contraction factor obtained by the differentially quantized algorithm\ncompared to its unquantized counterpart, and furthermore, the differentially\nquantized heavy ball method attains the optimal contraction achievable among\nall (even unquantized) gradient methods. Experimental results on both simulated\nand real-world least-squares problems validate our theoretical analysis.",
          "link": "http://arxiv.org/abs/2002.02508",
          "publishedOn": "2021-06-24T01:51:45.693Z",
          "wordCount": 725,
          "title": "Differentially Quantized Gradient Methods. (arXiv:2002.02508v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.08271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1\">Ning Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
          "link": "http://arxiv.org/abs/2003.08271",
          "publishedOn": "2021-06-24T01:51:45.687Z",
          "wordCount": 603,
          "title": "Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarma_A/0/1/0/all/0/1\">Anup Sarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sonali Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaipan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Mahmut T Kandemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1\">Chita R Das</a>",
          "description": "Recurrent Neural Networks (RNNs), more specifically their Long Short-Term\nMemory (LSTM) variants, have been widely used as a deep learning tool for\ntackling sequence-based learning tasks in text and speech. Training of such\nLSTM applications is computationally intensive due to the recurrent nature of\nhidden state computation that repeats for each time step. While sparsity in\nDeep Neural Nets has been widely seen as an opportunity for reducing\ncomputation time in both training and inference phases, the usage of non-ReLU\nactivation in LSTM RNNs renders the opportunities for such dynamic sparsity\nassociated with neuron activation and gradient values to be limited or\nnon-existent. In this work, we identify dropout induced sparsity for LSTMs as a\nsuitable mode of computation reduction. Dropout is a widely used regularization\nmechanism, which randomly drops computed neuron values during each iteration of\ntraining. We propose to structure dropout patterns, by dropping out the same\nset of physical neurons within a batch, resulting in column (row) level hidden\nstate sparsity, which are well amenable to computation reduction at run-time in\ngeneral-purpose SIMD hardware as well as systolic arrays. We conduct our\nexperiments for three representative NLP tasks: language modelling on the PTB\ndataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi\ndatasets, and named entity recognition sequence labelling using the CoNLL-2003\nshared task. We demonstrate that our proposed approach can be used to translate\ndropout-based computation reduction into reduced training time, with\nimprovement ranging from 1.23x to 1.64x, without sacrificing the target metric.",
          "link": "http://arxiv.org/abs/2106.12089",
          "publishedOn": "2021-06-24T01:51:45.682Z",
          "wordCount": 697,
          "title": "Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training. (arXiv:2106.12089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Divyansh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Shuvam Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1\">Chris Cundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "In many sequential decision-making problems (e.g., robotics control, game\nplaying, sequential prediction), human or expert data is available containing\nuseful information about the task. However, imitation learning (IL) from a\nsmall amount of expert data can be challenging in high-dimensional environments\nwith complex dynamics. Behavioral cloning is a simple method that is widely\nused due to its simplicity of implementation and stable convergence but doesn't\nutilize any information involving the environment's dynamics. Many existing\nmethods that exploit dynamics information are difficult to train in practice\ndue to an adversarial optimization process over reward and policy approximators\nor biased, high variance gradient estimators. We introduce a method for\ndynamics-aware IL which avoids adversarial training by learning a single\nQ-function, implicitly representing both reward and policy. On standard\nbenchmarks, the implicitly learned rewards show a high positive correlation\nwith the ground-truth rewards, illustrating our method can also be used for\ninverse reinforcement learning (IRL). Our method, Inverse soft-Q learning\n(IQ-Learn) obtains state-of-the-art results in offline and online imitation\nlearning settings, surpassing existing methods both in the number of required\nenvironment interactions and scalability in high-dimensional spaces.",
          "link": "http://arxiv.org/abs/2106.12142",
          "publishedOn": "2021-06-24T01:51:45.677Z",
          "wordCount": 616,
          "title": "IQ-Learn: Inverse soft-Q Learning for Imitation. (arXiv:2106.12142v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1\">Valentina Zantedeschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viallard_P/0/1/0/all/0/1\">Paul Viallard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morvant_E/0/1/0/all/0/1\">Emilie Morvant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1\">R&#xe9;mi Emonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1\">Amaury Habrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1\">Pascal Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>",
          "description": "We investigate a stochastic counterpart of majority votes over finite\nensembles of classifiers, and study its generalization properties. While our\napproach holds for arbitrary distributions, we instantiate it with Dirichlet\ndistributions: this allows for a closed-form and differentiable expression for\nthe expected risk, which then turns the generalization bound into a tractable\ntraining objective. The resulting stochastic majority vote learning algorithm\nachieves state-of-the-art accuracy and benefits from (non-vacuous) tight\ngeneralization bounds, in a series of numerical experiments when compared to\ncompeting algorithms which also minimize PAC-Bayes objectives -- both with\nuninformed (data-independent) and informed (data-dependent) priors.",
          "link": "http://arxiv.org/abs/2106.12535",
          "publishedOn": "2021-06-24T01:51:45.672Z",
          "wordCount": 544,
          "title": "Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound. (arXiv:2106.12535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1\">Andreea Iana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "In today's academic publishing model, especially in Computer Science,\nconferences commonly constitute the main platforms for releasing the latest\npeer-reviewed advancements in their respective fields. However, choosing a\nsuitable academic venue for publishing one's research can represent a\nchallenging task considering the plethora of available conferences,\nparticularly for those at the start of their academic careers, or for those\nseeking to publish outside of their usual domain. In this paper, we propose\nGraphConfRec, a conference recommender system which combines SciGraph and graph\nneural networks, to infer suggestions based not only on title and abstract, but\nalso on co-authorship and citation relationships. GraphConfRec achieves a\nrecall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention\nnetwork-based recommendation model. A user study with 25 subjects supports the\npositive results.",
          "link": "http://arxiv.org/abs/2106.12340",
          "publishedOn": "2021-06-24T01:51:45.656Z",
          "wordCount": 577,
          "title": "GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumdar_E/0/1/0/all/0/1\">Eric Mazumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1\">S. Shankar Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "As predictive models are deployed into the real world, they must increasingly\ncontend with strategic behavior. A growing body of work on strategic\nclassification treats this problem as a Stackelberg game: the decision-maker\n\"leads\" in the game by deploying a model, and the strategic agents \"follow\" by\nplaying their best response to the deployed model. Importantly, in this\nframing, the burden of learning is placed solely on the decision-maker, while\nthe agents' best responses are implicitly treated as instantaneous. In this\nwork, we argue that the order of play in strategic classification is\nfundamentally determined by the relative frequencies at which the\ndecision-maker and the agents adapt to each other's actions. In particular, by\ngeneralizing the standard model to allow both players to learn over time, we\nshow that a decision-maker that makes updates faster than the agents can\nreverse the order of play, meaning that the agents lead and the decision-maker\nfollows. We observe in standard learning settings that such a role reversal can\nbe desirable for both the decision-maker and the strategic agents. Finally, we\nshow that a decision-maker with the freedom to choose their update frequency\ncan induce learning dynamics that converge to Stackelberg equilibria with\neither order of play.",
          "link": "http://arxiv.org/abs/2106.12529",
          "publishedOn": "2021-06-24T01:51:45.642Z",
          "wordCount": 642,
          "title": "Who Leads and Who Follows in Strategic Classification?. (arXiv:2106.12529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "We investigate the adversarial robustness of CNNs from the perspective of\nchannel-wise activations. By comparing \\textit{non-robust} (normally trained)\nand \\textit{robustified} (adversarially trained) models, we observe that\nadversarial training (AT) robustifies CNNs by aligning the channel-wise\nactivations of adversarial data with those of their natural counterparts.\nHowever, the channels that are \\textit{negatively-relevant} (NR) to predictions\nare still over-activated when processing adversarial data. Besides, we also\nobserve that AT does not result in similar robustness for all classes. For the\nrobust classes, channels with larger activation magnitudes are usually more\n\\textit{positively-relevant} (PR) to predictions, but this alignment does not\nhold for the non-robust classes. Given these observations, we hypothesize that\nsuppressing NR channels and aligning PR ones with their relevances further\nenhances the robustness of CNNs under AT. To examine this hypothesis, we\nintroduce a novel mechanism, i.e., \\underline{C}hannel-wise\n\\underline{I}mportance-based \\underline{F}eature \\underline{S}election (CIFS).\nThe CIFS manipulates channels' activations of certain layers by generating\nnon-negative multipliers to these channels based on their relevances to\npredictions. Extensive experiments on benchmark datasets including CIFAR10 and\nSVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying\nCNNs.",
          "link": "http://arxiv.org/abs/2102.05311",
          "publishedOn": "2021-06-24T01:51:45.620Z",
          "wordCount": 660,
          "title": "CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection. (arXiv:2102.05311v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shanqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Junjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Licheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "It is challenging learning from demonstrated observation-only trajectories in\na non-time-aligned environment because most imitation learning methods aim to\nimitate experts by following the demonstration step-by-step. However, aligned\ndemonstrations are seldom obtainable in real-world scenarios. In this work, we\npropose a new imitation learning approach called Hierarchical Imitation\nLearning from Observation(HILONet), which adopts a hierarchical structure to\nchoose feasible sub-goals from demonstrated observations dynamically. Our\nmethod can solve all kinds of tasks by achieving these sub-goals, whether it\nhas a single goal position or not. We also present three different ways to\nincrease sample efficiency in the hierarchical structure. We conduct extensive\nexperiments using several environments. The results show the improvement in\nboth performance and learning efficiency.",
          "link": "http://arxiv.org/abs/2011.02671",
          "publishedOn": "2021-06-24T01:51:45.607Z",
          "wordCount": 602,
          "title": "HILONet: Hierarchical Imitation Learning from Non-Aligned Observations. (arXiv:2011.02671v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1\">Apratim Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>",
          "description": "Accurate prediction of pedestrian and bicyclist paths is integral to the\ndevelopment of reliable autonomous vehicles in dense urban environments. The\ninteractions between vehicle and pedestrian or bicyclist have a significant\nimpact on the trajectories of traffic participants e.g. stopping or turning to\navoid collisions. Although recent datasets and trajectory prediction approaches\nhave fostered the development of autonomous vehicles yet the amount of\nvehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,\nwe propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In\nparticular, our dataset caters more diverse and complex interactions in dense\nurban scenarios compared to the existing datasets. To address the challenges in\npredicting future trajectories with dense interactions, we develop a joint\ninference model that learns an expressive multi-modal shared latent space\nacross agents in the urban scene. This enables our Joint-$\\beta$-cVAE approach\nto better model the distribution of future trajectories. We achieve state of\nthe art results on the nuScenes and Euro-PVI datasets demonstrating the\nimportance of capturing interactions between ego-vehicle and pedestrians\n(bicyclists) for accurate predictions.",
          "link": "http://arxiv.org/abs/2106.12442",
          "publishedOn": "2021-06-24T01:51:45.580Z",
          "wordCount": 622,
          "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>",
          "description": "We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$\nin the presence of malicious noise of Valiant~(1985). This is a challenging\nnoise model and only until recently has near-optimal noise tolerance bound been\nestablished under the mild condition that the unlabeled data distribution is\nisotropic log-concave. However, it remains unsettled how to obtain the optimal\nsample complexity simultaneously. In this work, we present a new analysis for\nthe algorithm of Awasthi~et~al.~(2017) and show that it essentially achieves\nthe near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best\nknown result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation\nof a matrix Chernoff-type inequality to bound the spectrum of an empirical\ncovariance matrix for well-behaved distributions, in conjunction with a careful\nexploration of the localization schemes of Awasthi~et~al.~(2017). We further\nextend the algorithm and analysis to the more general and stronger nasty noise\nmodel of Bshouty~et~al.~(2002), showing that it is still possible to achieve\nnear-optimal noise tolerance and sample complexity in polynomial time.",
          "link": "http://arxiv.org/abs/2102.06247",
          "publishedOn": "2021-06-24T01:51:45.014Z",
          "wordCount": 632,
          "title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise. (arXiv:2102.06247v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhouxing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "Recently, bound propagation based certified robust training methods have been\nproposed for training neural networks with certifiable robustness guarantees.\nDespite that state-of-the-art (SOTA) methods including interval bound\npropagation (IBP) and CROWN-IBP have per-batch training complexity similar to\nstandard neural network training, they usually use a long warmup schedule with\nhundreds or thousands epochs to reach SOTA performance and are thus still\ncostly. In this paper, we identify two important issues in existing methods,\nnamely exploded bounds at initialization, and the imbalance in ReLU activation\nstates. These two issues make certified training difficult and unstable, and\nthereby long warmup schedules were needed in prior works. To mitigate these\nissues and conduct certified training with shorter warmup, we propose three\nimprovements: 1) We derive a new weight initialization method for IBP training;\n2) We propose to fully add Batch Normalization (BN) to each layer in the model,\nsince we find BN can reduce the imbalance in ReLU activation states; 3) We also\ndesign regularization to explicitly tighten certified bounds and balance ReLU\nactivation states. In our experiments, we are able to obtain 65.03% verified\nerror on CIFAR-10 ($\\epsilon=\\frac{8}{255}$) and 82.36% verified error on\nTinyImageNet ($\\epsilon=\\frac{1}{255}$) using very short training schedules\n(160 and 80 total epochs, respectively), outperforming literature SOTA trained\nwith hundreds or thousands epochs under the same network architecture.",
          "link": "http://arxiv.org/abs/2103.17268",
          "publishedOn": "2021-06-24T01:51:45.008Z",
          "wordCount": 695,
          "title": "Fast Certified Robust Training with Short Warmup. (arXiv:2103.17268v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengfei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruoxi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuhao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>",
          "description": "Deep neural networks(DNNs) is vulnerable to be attacked by adversarial\nexamples. Black-box attack is the most threatening attack. At present,\nblack-box attack methods mainly adopt gradient-based iterative attack methods,\nwhich usually limit the relationship between the iteration step size, the\nnumber of iterations, and the maximum perturbation. In this paper, we propose a\nnew gradient iteration framework, which redefines the relationship between the\nabove three. Under this framework, we easily improve the attack success rate of\nDI-TI-MIM. In addition, we propose a gradient iterative attack method based on\ninput dropout, which can be well combined with our framework. We further\npropose a multi dropout rate version of this method. Experimental results show\nthat our best method can achieve attack success rate of 96.2\\% for defense\nmodel on average, which is higher than the state-of-the-art gradient-based\nattacks.",
          "link": "http://arxiv.org/abs/2106.01617",
          "publishedOn": "2021-06-24T01:51:44.984Z",
          "wordCount": 606,
          "title": "Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>",
          "description": "Data subset selection from a large number of training instances has been a\nsuccessful approach toward efficient and cost-effective machine learning.\nHowever, models trained on a smaller subset may show poor generalization\nability. In this paper, our goal is to design an algorithm for selecting a\nsubset of the training data, so that the model can be trained quickly, without\nsignificantly sacrificing on accuracy. More specifically, we focus on data\nsubset selection for L2 regularized regression problems and provide a novel\nproblem formulation which seeks to minimize the training loss with respect to\nboth the trainable parameters and the subset of training data, subject to error\nbounds on the validation set. We tackle this problem using several technical\ninnovations. First, we represent this problem with simplified constraints using\nthe dual of the original training problem and show that the objective of this\nnew representation is a monotone and alpha-submodular function, for a wide\nvariety of modeling choices. Such properties lead us to develop SELCON, an\nefficient majorization-minimization algorithm for data subset selection, that\nadmits an approximation guarantee even when the training provides an imperfect\nestimate of the trained model. Finally, our experiments on several datasets\nshow that SELCON trades off accuracy and efficiency more effectively than the\ncurrent state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.12491",
          "publishedOn": "2021-06-24T01:51:44.978Z",
          "wordCount": 651,
          "title": "Training Data Subset Selection for Regression with Controlled Generalization Error. (arXiv:2106.12491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14543",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1\">Tobias Haug</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kim_M/0/1/0/all/0/1\">M.S. Kim</a>",
          "description": "Variational quantum algorithms (VQAs) promise efficient use of near-term\nquantum computers. However, training VQAs often requires an extensive amount of\ntime and suffers from the barren plateau problem where the magnitude of the\ngradients vanishes with increasing number of qubits. Here, we show how to\noptimally train VQAs for learning quantum states. Parameterized quantum\ncircuits can form Gaussian kernels, which we use to derive adaptive learning\nrates for gradient ascent. We introduce the generalized quantum natural\ngradient that features stability and optimized movement in parameter space.\nBoth methods together outperform other optimization routines in training VQAs.\nOur methods also excel at numerically optimizing driving protocols for quantum\ncontrol problems. The gradients of the VQA do not vanish when the fidelity\nbetween the initial state and the state to be learned is bounded from below. We\nidentify a VQA for quantum simulation with such a constraint that thus can be\ntrained free of barren plateaus. Finally, we propose the application of\nGaussian kernels for quantum machine learning.",
          "link": "http://arxiv.org/abs/2104.14543",
          "publishedOn": "2021-06-24T01:51:44.972Z",
          "wordCount": 633,
          "title": "Optimal training of variational quantum algorithms without barren plateaus. (arXiv:2104.14543v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1\">Cristian Bodnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1\">Fabrizio Frasca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1\">Nina Otter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael Bronstein</a>",
          "description": "Graph Neural Networks (GNNs) are limited in their expressive power, struggle\nwith long-range interactions and lack a principled way to model higher-order\nstructures. These problems can be attributed to the strong coupling between the\ncomputational graph and the input graph structure. The recently proposed\nMessage Passing Simplicial Networks naturally decouple these elements by\nperforming message passing on the clique complex of the graph. Nevertheless,\nthese models are severely constrained by the rigid combinatorial structure of\nSimplicial Complexes (SCs). In this work, we extend recent theoretical results\non SCs to regular Cell Complexes, topological objects that flexibly subsume SCs\nand graphs. We show that this generalisation provides a powerful set of graph\n``lifting'' transformations, each leading to a unique hierarchical message\npassing procedure. The resulting methods, which we collectively call CW\nNetworks (CWNs), are strictly more powerful than the WL test and, in certain\ncases, not less powerful than the 3-WL test. In particular, we demonstrate the\neffectiveness of one such scheme, based on rings, when applied to molecular\ngraph problems. The proposed architecture benefits from provably larger\nexpressivity than commonly used GNNs, principled modelling of higher-order\nsignals and from compressing the distances between nodes. We demonstrate that\nour model achieves state-of-the-art results on a variety of molecular datasets.",
          "link": "http://arxiv.org/abs/2106.12575",
          "publishedOn": "2021-06-24T01:51:44.967Z",
          "wordCount": 648,
          "title": "Weisfeiler and Lehman Go Cellular: CW Networks. (arXiv:2106.12575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dinglan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "The attention module, which is a crucial component in Transformer, cannot\nscale efficiently to long sequences due to its quadratic complexity. Many works\nfocus on approximating the dot-then-exponentiate softmax function in the\noriginal attention, leading to sub-quadratic or even linear-complexity\nTransformer architectures. However, we show that these methods cannot be\napplied to more powerful attention modules that go beyond the\ndot-then-exponentiate style, e.g., Transformers with relative positional\nencoding (RPE). Since in many state-of-the-art models, relative positional\nencoding is used as default, designing efficient Transformers that can\nincorporate RPE is appealing. In this paper, we propose a novel way to\naccelerate attention calculation for Transformers with RPE on top of the\nkernelized attention. Based upon the observation that relative positional\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\nattention with RPE can be calculated efficiently using Fast Fourier Transform\n(FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity.\nInterestingly, we further demonstrate that properly using relative positional\nencoding can mitigate the training instability problem of vanilla kernelized\nattention. On a wide range of tasks, we empirically show that our models can be\ntrained from scratch without any optimization issues. The learned model\nperforms better than many efficient Transformer variants and is faster than\nstandard Transformer in the long-sequence regime.",
          "link": "http://arxiv.org/abs/2106.12566",
          "publishedOn": "2021-06-24T01:51:44.952Z",
          "wordCount": 670,
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14958",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Davila_Pena_L/0/1/0/all/0/1\">L. Davila-Pena</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garcia_Jurado_I/0/1/0/all/0/1\">Ignacio Garc&#xed;a-Jurado</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casas_Mendez_B/0/1/0/all/0/1\">B. Casas-M&#xe9;ndez</a>",
          "description": "This paper deals with an important subject in classification problems\naddressed by machine learning techniques: the evaluation of the influence of\neach of the features on the classification of individuals. Specifically, a\nmeasure of that influence is introduced using the Shapley value of cooperative\ngames. In addition, an axiomatic characterisation of the proposed measure is\nprovided based on properties of efficiency and balanced contributions.\nFurthermore, some experiments have been designed in order to validate the\nappropriate performance of such measure. Finally, the methodology introduced is\napplied to a sample of COVID-19 patients to study the influence of certain\ndemographic or risk factors on various events of interest related to the\nevolution of the disease.",
          "link": "http://arxiv.org/abs/2104.14958",
          "publishedOn": "2021-06-24T01:51:44.947Z",
          "wordCount": 629,
          "title": "Assessment of the influence of features on a classification problem: an application to COVID-19 patients. (arXiv:2104.14958v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaghoubi_V/0/1/0/all/0/1\">Vahid Yaghoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paepegem_W/0/1/0/all/0/1\">Wim Van Paepegem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersemans_M/0/1/0/all/0/1\">Mathias Kersemans</a>",
          "description": "Achieving a high prediction rate is a crucial task in fault detection.\nAlthough various classification procedures are available, none of them can give\nhigh accuracy in all applications. Therefore, in this paper, a novel\nmulti-classifier fusion approach is developed to boost the performance of the\nindividual classifiers. This is acquired by using Dempster-Shafer theory (DST).\nHowever, in cases with conflicting evidences, the DST may give\ncounter-intuitive results. In this regard, a preprocessing technique based on a\nnew metric is devised in order to measure and mitigate the conflict between the\nevidences. To evaluate and validate the effectiveness of the proposed approach,\nthe method is applied to 15 benchmarks datasets from UCI and KEEL. Further, it\nis applied for classifying polycrystalline Nickel alloy first-stage turbine\nblades based on their broadband vibrational response. Through statistical\nanalysis with different noise levels, and by comparing with four\nstate-of-the-art fusion techniques, it is shown that that the proposed method\nimproves the classification accuracy and outperforms the individual\nclassifiers.",
          "link": "http://arxiv.org/abs/2012.02481",
          "publishedOn": "2021-06-24T01:51:44.942Z",
          "wordCount": 639,
          "title": "A novel multi-classifier information fusion based on Dempster-Shafer theory: application to vibration-based fault detection. (arXiv:2012.02481v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.",
          "link": "http://arxiv.org/abs/2104.06392",
          "publishedOn": "2021-06-24T01:51:44.936Z",
          "wordCount": 739,
          "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">MohammadJavad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-06-24T01:51:44.930Z",
          "wordCount": 559,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>",
          "description": "We study {\\em online} active learning of homogeneous halfspaces in\n$\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy\nlabel is constrained to be at most $\\nu$. Our main contribution is a\nPerceptron-like online active learning algorithm that runs in polynomial time,\nand under the conditions that the marginal distribution is isotropic\nlog-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the\ntarget error rate, our algorithm PAC learns the underlying halfspace with\nnear-optimal label complexity of $\\tilde{O}\\big(d \\cdot\npolylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$. Prior to this work, existing online\nalgorithms designed for tolerating the adversarial noise are subject to either\nlabel complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise\ntolerance, or restrictive marginal distributions. With the additional prior\nknowledge that the underlying halfspace is $s$-sparse, we obtain\nattribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot polylog(d,\n\\frac{1}{\\epsilon}) \\big)$ and sample complexity of\n$\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot polylog(d) \\big)$. As an immediate\ncorollary, we show that under the agnostic model where no assumption is made on\nthe noise rate $\\nu$, our active learner achieves an error rate of $O(OPT) +\n\\epsilon$ with the same running time and label and sample complexity, where\n$OPT$ is the best possible error rate achievable by any homogeneous halfspace.",
          "link": "http://arxiv.org/abs/2012.10793",
          "publishedOn": "2021-06-24T01:51:44.890Z",
          "wordCount": 698,
          "title": "On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise. (arXiv:2012.10793v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12041",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hough_A/0/1/0/all/0/1\">Alana Hough</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wong_T/0/1/0/all/0/1\">Tony E. Wong</a>",
          "description": "Climate models are critical tools for developing strategies to manage the\nrisks posed by sea-level rise to coastal communities. While these models are\nnecessary for understanding climate risks, there is a level of uncertainty\ninherent in each parameter in the models. This model parametric uncertainty\nleads to uncertainty in future climate risks. Consequently, there is a need to\nunderstand how those parameter uncertainties impact our assessment of future\nclimate risks and the efficacy of strategies to manage them. Here, we use\nrandom forests to examine the parametric drivers of future climate risk and how\nthe relative importances of those drivers change over time. We find that the\nequilibrium climate sensitivity and a factor that scales the effect of aerosols\non radiative forcing are consistently the most important climate model\nparametric uncertainties throughout the 2020 to 2150 interval for both low and\nhigh radiative forcing scenarios. The near-term hazards of high-end sea-level\nrise are driven primarily by thermal expansion, while the longer-term hazards\nare associated with mass loss from the Antarctic and Greenland ice sheets. Our\nresults highlight the practical importance of considering time-evolving\nparametric uncertainties when developing strategies to manage future climate\nrisks.",
          "link": "http://arxiv.org/abs/2106.12041",
          "publishedOn": "2021-06-24T01:51:44.883Z",
          "wordCount": 636,
          "title": "Analysis of the Evolution of Parametric Drivers of High-End Sea-Level Hazards. (arXiv:2106.12041v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sungjoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1\">Youngjung Uh</a>",
          "description": "Generative adversarial networks (GANs) synthesize realistic images from\nrandom latent vectors. Although manipulating the latent vectors controls the\nsynthesized outputs, editing real images with GANs suffers from i)\ntime-consuming optimization for projecting real images to the latent vectors,\nii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the\nintermediate latent space has spatial dimensions, and a spatially variant\nmodulation replaces AdaIN. It makes the embedding through an encoder more\naccurate than existing optimization-based methods while maintaining the\nproperties of GANs. Experimental results demonstrate that our method\nsignificantly outperforms state-of-the-art models in various image manipulation\ntasks such as local editing and image interpolation. Last but not least,\nconventional editing methods on GANs are still valid on our StyleMapGAN. Source\ncode is available at https://github.com/naver-ai/StyleMapGAN.",
          "link": "http://arxiv.org/abs/2104.14754",
          "publishedOn": "2021-06-24T01:51:44.878Z",
          "wordCount": 608,
          "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07405",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1\">Wu Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>",
          "description": "Natural-gradient descent on structured parameter spaces (e.g., low-rank\ncovariances) is computationally challenging due to complicated inverse\nFisher-matrix computations. We address this issue for optimization, inference,\nand search problems by using \\emph{local-parameter coordinates}. Our method\ngeneralizes an existing evolutionary-strategy method, recovers Newton and\nRiemannian-gradient methods as special cases, and also yields new tractable\nnatural-gradient algorithms for learning flexible covariance structures of\nGaussian and Wishart-based distributions via \\emph{matrix groups}. We show\nresults on a range of applications on deep learning, variational inference, and\nevolution strategies. Our work opens a new direction for scalable structured\ngeometric methods via local parameterizations.",
          "link": "http://arxiv.org/abs/2102.07405",
          "publishedOn": "2021-06-24T01:51:44.872Z",
          "wordCount": 585,
          "title": "Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1\">Alvaro Gomariz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1\">Tiziano Portenier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1\">Patrick M. Helbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1\">Stephan Isringhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1\">Ute Suessbier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1\">C&#xe9;sar Nombela-Arrieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>",
          "description": "Fluorescence microscopy allows for a detailed inspection of cells, cellular\nnetworks, and anatomical landmarks by staining with a variety of\ncarefully-selected markers visualized as color channels. Quantitative\ncharacterization of structures in acquired images often relies on automatic\nimage analysis methods. Despite the success of deep learning methods in other\nvision applications, their potential for fluorescence image analysis remains\nunderexploited. One reason lies in the considerable workload required to train\naccurate models, which are normally specific for a given combination of\nmarkers, and therefore applicable to a very restricted number of experimental\nsettings. We herein propose Marker Sampling and Excite, a neural network\napproach with a modality sampling strategy and a novel attention module that\ntogether enable (i) flexible training with heterogeneous datasets with\ncombinations of markers and (ii) successful utility of learned models on\narbitrary subsets of markers prospectively. We show that our single neural\nnetwork solution performs comparably to an upper bound scenario where an\nensemble of many networks is na\\\"ively trained for each possible marker\ncombination separately. In addition, we demonstrate the feasibility of this\nframework in high-throughput biological analysis by revising a recent\nquantitative characterization of bone marrow vasculature in 3D confocal\nmicroscopy datasets and further confirm the validity of our approach on an\nadditional, significantly different dataset of microvessels in fetal liver\ntissues. Not only can our work substantially ameliorate the use of deep\nlearning in fluorescence microscopy analysis, but it can also be utilized in\nother fields with incomplete data acquisitions and missing modalities.",
          "link": "http://arxiv.org/abs/2008.12380",
          "publishedOn": "2021-06-24T01:51:44.867Z",
          "wordCount": 758,
          "title": "Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In this paper, we design a novel Bregman gradient policy optimization\nframework for reinforcement learning based on Bregman divergences and momentum\ntechniques. Specifically, we propose a Bregman gradient policy optimization\n(BGPO) algorithm based on the basic momentum technique and mirror descent\niteration. At the same time, we present an accelerated Bregman gradient policy\noptimization (VR-BGPO) algorithm based on a momentum variance-reduced\ntechnique. Moreover, we introduce a convergence analysis framework for our\nBregman gradient policy optimization under the nonconvex setting. Specifically,\nwe prove that BGPO achieves the sample complexity of $\\tilde{O}(\\epsilon^{-4})$\nfor finding $\\epsilon$-stationary point only requiring one trajectory at each\niteration, and VR-BGPO reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point, which\nalso only requires one trajectory at each iteration. In particular, by using\ndifferent Bregman divergences, our methods unify many existing policy\noptimization algorithms and their new variants such as the existing\n(variance-reduced) policy gradient algorithms and (variance-reduced) natural\npolicy gradient algorithms. Extensive experimental results on multiple\nreinforcement learning tasks demonstrate the efficiency of our new algorithms.",
          "link": "http://arxiv.org/abs/2106.12112",
          "publishedOn": "2021-06-24T01:51:44.852Z",
          "wordCount": 606,
          "title": "Bregman Gradient Policy Optimization. (arXiv:2106.12112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuhang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1\">Carlee Joe-Wong</a>",
          "description": "We study the problem of clustering nodes in a dynamic graph, where the\nconnections between nodes and nodes' cluster memberships may change over time,\ne.g., due to community migration. We first propose a dynamic stochastic block\nmodel that captures these changes, and a simple decay-based clustering\nalgorithm that clusters nodes based on weighted connections between them, where\nthe weight decreases at a fixed rate over time. This decay rate can then be\ninterpreted as signifying the importance of including historical connection\ninformation in the clustering. However, the optimal decay rate may differ for\nclusters with different rates of turnover. We characterize the optimal decay\nrate for each cluster and propose a clustering method that achieves almost\nexact recovery of the true clusters. We then demonstrate the efficacy of our\nclustering algorithm with optimized decay rates on simulated graph data.\nRecurrent neural networks (RNNs), a popular algorithm for sequence learning,\nuse a similar decay-based method, and we use this insight to propose two new\nRNN-GCN (graph convolutional network) architectures for semi-supervised graph\nclustering. We finally demonstrate that the proposed architectures perform well\non real data compared to state-of-the-art graph clustering algorithms.",
          "link": "http://arxiv.org/abs/2012.08740",
          "publishedOn": "2021-06-24T01:51:44.835Z",
          "wordCount": 661,
          "title": "Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks. (arXiv:2012.08740v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07900",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1\">M Brandon Westover</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
          "link": "http://arxiv.org/abs/2106.07900",
          "publishedOn": "2021-06-24T01:51:44.828Z",
          "wordCount": 658,
          "title": "Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04806",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alexander Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard S. Zemel</a>",
          "description": "Sketch drawings capture the salient information of visual concepts. Previous\nwork has shown that neural networks are capable of producing sketches of\nnatural objects drawn from a small number of classes. While earlier approaches\nfocus on generation quality or retrieval, we explore properties of image\nrepresentations learned by training a model to produce sketches of images. We\nshow that this generative, class-agnostic model produces informative embeddings\nof images from novel examples, classes, and even novel datasets in a few-shot\nsetting. Additionally, we find that these learned representations exhibit\ninteresting structure and compositionality.",
          "link": "http://arxiv.org/abs/2009.04806",
          "publishedOn": "2021-06-24T01:51:44.821Z",
          "wordCount": 589,
          "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shih_S/0/1/0/all/0/1\">Sheng-Min Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_P/0/1/0/all/0/1\">Pin-Ju Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>",
          "description": "Attribution methods have been shown as promising approaches for identifying\nkey features that led to learned model predictions. While most existing\nattribution methods rely on a baseline input for performing feature\nperturbations, limited research has been conducted to address the baseline\nselection issues. Poor choices of baselines limit the ability of one-vs-one\n(1-vs-1) explanations for multi-class classifiers, which means the attribution\nmethods were not able to explain why an input belongs to its original class but\nnot the other specified target class. 1-vs-1 explanation is crucial when\ncertain classes are more similar than others, e.g. two bird types among\nmultiple animals, by focusing on key differentiating features rather than\nshared features across classes. In this paper, we present GAN-based Model\nEXplainability (GANMEX), a novel approach applying Generative Adversarial\nNetworks (GAN) by incorporating the to-be-explained classifier as part of the\nadversarial networks. Our approach effectively selects the counterfactual\nbaseline as the closest realistic sample belong to the target class, which\nallows attribution methods to provide true 1-vs-1 explanations. We showed that\nGANMEX baselines improved the saliency maps and led to stronger performance on\nperturbation-based evaluation metrics over the existing baselines. Existing\nattribution results are known for being insensitive to model randomization, and\nwe demonstrated that GANMEX baselines led to better outcome under the cascading\nrandomization of the model.",
          "link": "http://arxiv.org/abs/2011.06015",
          "publishedOn": "2021-06-24T01:51:44.816Z",
          "wordCount": 698,
          "title": "GANMEX: One-vs-One Attributions Guided by GAN-based Counterfactual Explanation Baselines. (arXiv:2011.06015v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Shreyas Malakarjun Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1\">Constantine Dovrolis</a>",
          "description": "Methods that sparsify a network at initialization are important in practice\nbecause they greatly improve the efficiency of both learning and inference. Our\nwork is based on a recently proposed decomposition of the Neural Tangent Kernel\n(NTK) that has decoupled the dynamics of the training process into a\ndata-dependent component and an architecture-dependent kernel - the latter\nreferred to as Path Kernel. That work has shown how to design sparse neural\nnetworks for faster convergence, without any training data, using the\nSynflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in\nterms of convergence, for a given network density, it results in sub-networks\nwith \"bottleneck\" (narrow) layers - leading to poor performance as compared to\nother data-agnostic methods that use the same number of parameters. Then we\npropose a new method to construct sparse networks, without any training data,\nreferred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic\nnetwork formation method based on biased random walks that only depends on the\ninitial weights. It has similar path kernel properties as Synflow-L2 but it\ngenerates much wider layers, resulting in better generalization and\nperformance. PHEW achieves significant improvements over the data-independent\nSynFlow and SynFlow-L2 methods at a wide range of network densities.",
          "link": "http://arxiv.org/abs/2010.11354",
          "publishedOn": "2021-06-24T01:51:44.795Z",
          "wordCount": 685,
          "title": "PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data. (arXiv:2010.11354v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14567",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1\">Tatjana Chavdarova</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pagliardini_M/0/1/0/all/0/1\">Matteo Pagliardini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fleuret_F/0/1/0/all/0/1\">Francois Fleuret</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Generative Adversarial Networks are notoriously challenging to train. The\nunderlying minmax optimization is highly susceptible to the variance of the\nstochastic gradient and the rotational component of the associated game vector\nfield. To tackle these challenges, we propose the Lookahead algorithm for\nminmax optimization, originally developed for single objective minimization\nonly. The backtracking step of our Lookahead-minmax naturally handles the\nrotational game dynamics, a property which was identified to be key for\nenabling gradient ascent descent methods to converge on challenging examples\noften analyzed in the literature. Moreover, it implicitly handles high variance\nwithout using large mini-batches, known to be essential for reaching state of\nthe art performance. Experimental results on MNIST, SVHN, CIFAR-10, and\nImageNet demonstrate a clear advantage of combining Lookahead-minmax with Adam\nor extragradient, in terms of performance and improved stability, for\nnegligible memory and computational cost. Using 30-fold fewer parameters and\n16-fold smaller minibatches we outperform the reported performance of the\nclass-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the\nclass labels, bringing state-of-the-art GAN training within reach of common\ncomputational resources.",
          "link": "http://arxiv.org/abs/2006.14567",
          "publishedOn": "2021-06-24T01:51:44.784Z",
          "wordCount": 638,
          "title": "Taming GANs with Lookahead-Minmax. (arXiv:2006.14567v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09258",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1\">Conor Durkan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1\">Iain Murray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>",
          "description": "Score-based diffusion models synthesize samples by reversing a stochastic\nprocess that diffuses data to noise, and are trained by minimizing a weighted\ncombination of score matching losses. The log-likelihood of score-based models\ncan be tractably computed through a connection to continuous normalizing flows,\nbut log-likelihood is not directly optimized by the weighted combination of\nscore matching losses. We show that for a specific weighting scheme, the\nobjective upper bounds the negative log-likelihood, thus enabling approximate\nmaximum likelihood training of score-based models. We empirically observe that\nmaximum likelihood training consistently improves the likelihood of score-based\nmodels across multiple datasets, stochastic processes, and model architectures.\nOur best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on\nCIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these\ntasks.",
          "link": "http://arxiv.org/abs/2101.09258",
          "publishedOn": "2021-06-24T01:51:44.779Z",
          "wordCount": 583,
          "title": "Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xueyuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquier_T/0/1/0/all/0/1\">Thomas Pasquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Ding Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_J/0/1/0/all/0/1\">Junghwan Rhee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mickens_J/0/1/0/all/0/1\">James Mickens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Margo Seltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>",
          "description": "Many users implicitly assume that software can only be exploited after it is\ninstalled. However, recent supply-chain attacks demonstrate that application\nintegrity must be ensured during installation itself. We introduce SIGL, a new\ntool for detecting malicious behavior during software installation. SIGL\ncollects traces of system call activity, building a data provenance graph that\nit analyzes using a novel autoencoder architecture with a graph long short-term\nmemory network (graph LSTM) for the encoder and a standard multilayer\nperceptron for the decoder. SIGL flags suspicious installations as well as the\nspecific installation-time processes that are likely to be malicious. Using a\ntest corpus of 625 malicious installers containing real-world malware, we\ndemonstrate that SIGL has a detection accuracy of 96%, outperforming similar\nsystems from industry and academia by up to 87% in precision and recall and 45%\nin accuracy. We also demonstrate that SIGL can pinpoint the processes most\nlikely to have triggered malicious behavior, works on different audit platforms\nand operating systems, and is robust to training data contamination and\nadversarial attack. It can be used with application-specific models, even in\nthe presence of new software versions, as well as application-agnostic\nmeta-models that encompass a wide range of applications and installers.",
          "link": "http://arxiv.org/abs/2008.11533",
          "publishedOn": "2021-06-24T01:51:44.754Z",
          "wordCount": 692,
          "title": "SIGL: Securing Software Installations Through Deep Graph Learning. (arXiv:2008.11533v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>",
          "description": "Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution\ncomprises the IsoMax loss for training and the entropic score for\nout-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in\nreplacement because swapping the SoftMax loss with the IsoMax loss requires no\nchanges in the model's architecture or training procedures/hyperparameters. In\nthis paper, we propose to perform what we call an isometrization of the\ndistances used in the IsoMax loss. Additionally, we propose to replace the\nentropic score with the minimum distance score. Our experiments showed that\nthese simple modifications increase out-of-distribution detection performance\nwhile keeping the solution seamless.",
          "link": "http://arxiv.org/abs/2105.14399",
          "publishedOn": "2021-06-24T01:51:44.749Z",
          "wordCount": 610,
          "title": "Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konobeev_M/0/1/0/all/0/1\">Mikhail Konobeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-wei Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mladenov_M/0/1/0/all/0/1\">Martin Mladenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1\">Craig Boutilier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesvari</a>",
          "description": "Efficient exploration in bandits is a fundamental online learning problem. We\npropose a variant of Thompson sampling that learns to explore better as it\ninteracts with bandit instances drawn from an unknown prior. The algorithm\nmeta-learns the prior and thus we call it MetaTS. We propose several efficient\nimplementations of MetaTS and analyze it in Gaussian bandits. Our analysis\nshows the benefit of meta-learning and is of a broader interest, because we\nderive a novel prior-dependent Bayes regret bound for Thompson sampling. Our\ntheory is complemented by empirical evaluation, which shows that MetaTS quickly\nadapts to the unknown prior.",
          "link": "http://arxiv.org/abs/2102.06129",
          "publishedOn": "2021-06-24T01:51:44.744Z",
          "wordCount": 569,
          "title": "Meta-Thompson Sampling. (arXiv:2102.06129v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shmileva_E/0/1/0/all/0/1\">Elena Shmileva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarzhan_V/0/1/0/all/0/1\">Viktor Sarzhan</a>",
          "description": "This work is devoted to the clustering of check-in sequences from a geosocial\nnetwork. We used the mixture Markov chain process as a mathematical model for\ntime-dependent types of data. For clustering, we adjusted the\nExpectation-Maximization (EM) algorithm. As a result, we obtained highly\ndetailed communities (clusters) of users of the now defunct geosocial network,\nWeeplaces.",
          "link": "http://arxiv.org/abs/2106.12039",
          "publishedOn": "2021-06-24T01:51:44.729Z",
          "wordCount": 503,
          "title": "Clustering of check-in sequences using the mixture Markov chain process. (arXiv:2106.12039v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08721",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1\">Md Tamjidul Hoque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1\">Mahdi Abdelguerfi</a>",
          "description": "In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast\ndominant wave periods in oceanic waters. First, we use the data collected from\nCDIP buoys and apply various data filtering methods. The data filtering methods\nallow us to obtain a high-quality dataset for training and validation purposes.\nWe then extract various wave-based features like wave heights, periods,\nskewness, kurtosis, etc., and atmospheric features like humidity, pressure, and\nair temperature for the buoys. Afterward, we train algorithms that use LightGBM\nand Extra Trees through a hv-block cross-validation scheme to forecast dominant\nwave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,\nand 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,\nExtra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,\n15-day ahead, and 30 day ahead prediction. In case of the test dataset,\nLightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and\n30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day\nahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both\ntraining and the test dataset suggests that the machine learning models\ndeveloped in this paper are robust. Since the LightGBM algorithm outperforms ET\nfor all the windows tested, it is taken as the final algorithm. Note that the\nperformance of both methods does not decrease significantly as the forecast\nhorizon increases. Likewise, the proposed method outperforms the numerical\napproaches included in this paper in the test dataset. For 1 day ahead\nprediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,\n0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre\nfor Medium-range Weather Forecasts (ECMWF) model, which outperforms all the\nother methods in the test dataset.",
          "link": "http://arxiv.org/abs/2105.08721",
          "publishedOn": "2021-06-24T01:51:44.724Z",
          "wordCount": 779,
          "title": "A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v3 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12472",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Ziatdinov_M/0/1/0/all/0/1\">Maxim Ziatdinov</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wong_C/0/1/0/all/0/1\">Chun Yin Wong</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kalinin_S/0/1/0/all/0/1\">Sergei V. Kalinin</a>",
          "description": "Recent advances in scanning tunneling and transmission electron microscopies\n(STM and STEM) have allowed routine generation of large volumes of imaging data\ncontaining information on the structure and functionality of materials. The\nexperimental data sets contain signatures of long-range phenomena such as\nphysical order parameter fields, polarization and strain gradients in STEM, or\nstanding electronic waves and carrier-mediated exchange interactions in STM,\nall superimposed onto scanning system distortions and gradual changes of\ncontrast due to drift and/or mis-tilt effects. Correspondingly, while the human\neye can readily identify certain patterns in the images such as lattice\nperiodicities, repeating structural elements, or microstructures, their\nautomatic extraction and classification are highly non-trivial and universal\npathways to accomplish such analyses are absent. We pose that the most\ndistinctive elements of the patterns observed in STM and (S)TEM images are\nsimilarity and (almost-) periodicity, behaviors stemming directly from the\nparsimony of elementary atomic structures, superimposed on the gradual changes\nreflective of order parameter distributions. However, the discovery of these\nelements via global Fourier methods is non-trivial due to variability and lack\nof ideal discrete translation symmetry. To address this problem, we develop\nshift-invariant variational autoencoders (shift-VAE) that allow disentangling\ncharacteristic repeating features in the images, their variations, and shifts\ninevitable for random sampling of image space. Shift-VAEs balance the\nuncertainty in the position of the object of interest with the uncertainty in\nshape reconstruction. This approach is illustrated for model 1D data, and\nfurther extended to synthetic and experimental STM and STEM 2D data.",
          "link": "http://arxiv.org/abs/2106.12472",
          "publishedOn": "2021-06-24T01:51:44.718Z",
          "wordCount": 700,
          "title": "Finding simplicity: unsupervised discovery of features, patterns, and order parameters via shift-invariant variational autoencoders. (arXiv:2106.12472v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1\">Xiaoyu Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1\">Simon Leglaive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>",
          "description": "Dynamical variational auto-encoders (DVAEs) are a class of deep generative\nmodels with latent variables, dedicated to time series data modeling. DVAEs can\nbe considered as extensions of the variational autoencoder (VAE) that include\nthe modeling of temporal dependencies between successive observed and/or latent\nvectors in data sequences. Previous work has shown the interest of DVAEs and\ntheir better performance over the VAE for speech signals (spectrogram)\nmodeling. Independently, the VAE has been successfully applied to speech\nenhancement in noise, in an unsupervised noise-agnostic set-up that does not\nrequire the use of a parallel dataset of clean and noisy speech samples for\ntraining, but only requires clean speech signals. In this paper, we extend\nthose works to DVAE-based single-channel unsupervised speech enhancement, hence\nexploiting both speech signals unsupervised representation learning and\ndynamics modeling. We propose an unsupervised speech enhancement algorithm\nbased on the most general form of DVAEs, that we then adapt to three specific\nDVAE models to illustrate the versatility of the framework. More precisely, we\ncombine DVAE-based speech priors with a noise model based on nonnegative matrix\nfactorization, and we derive a variational expectation-maximization (VEM)\nalgorithm to perform speech enhancement. Experimental results show that the\nproposed approach based on DVAEs outperforms its VAE counterpart and a\nsupervised speech enhancement baseline.",
          "link": "http://arxiv.org/abs/2106.12271",
          "publishedOn": "2021-06-24T01:51:44.707Z",
          "wordCount": 652,
          "title": "Unsupervised Speech Enhancement using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1\">Gowri Somanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>",
          "description": "We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.",
          "link": "http://arxiv.org/abs/2011.10687",
          "publishedOn": "2021-06-24T01:51:44.693Z",
          "wordCount": 649,
          "title": "HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.08045",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Azhand_D/0/1/0/all/0/1\">Dr. Arash Azhand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rabe_D/0/1/0/all/0/1\">Dr. Sophie Rabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1\">Dr. Swantje M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sattler_I/0/1/0/all/0/1\">Igor Sattler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steinert_D/0/1/0/all/0/1\">Dr. Anika Steinert</a>",
          "description": "Despite its paramount importance for manifold use cases (e.g., in the health\ncare industry, sports, rehabilitation and fitness assessment), sufficiently\nvalid and reliable gait parameter measurement is still limited to high-tech\ngait laboratories mostly. Here, we demonstrate the excellent validity and\ntest-retest repeatability of a novel gait assessment system which is built upon\nmodern convolutional neural networks to extract three-dimensional skeleton\njoints from monocular frontal-view videos of walking humans. The validity study\nis based on a comparison to the GAITRite pressure-sensitive walkway system. All\nmeasured gait parameters (gait speed, cadence, step length and step time)\nshowed excellent concurrent validity for multiple walk trials at normal and\nfast gait speeds. The test-retest-repeatability is on the same level as the\nGAITRite system. In conclusion, we are convinced that our results can pave the\nway for cost, space and operationally effective gait analysis in broad\nmainstream applications. Most sensor-based systems are costly, must be operated\nby extensively trained personnel (e.g., motion capture systems) or - even if\nnot quite as costly - still possess considerable complexity (e.g., wearable\nsensors). In contrast, a video sufficient for the assessment method presented\nhere can be obtained by anyone, without much training, via a smartphone camera.",
          "link": "http://arxiv.org/abs/2008.08045",
          "publishedOn": "2021-06-24T01:51:44.688Z",
          "wordCount": 695,
          "title": "Algorithm Based on One Monocular Video Delivers Highly Valid and Reliable Gait Parameters. (arXiv:2008.08045v5 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1\">Rhea Sanjay Sukthanker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.",
          "link": "http://arxiv.org/abs/2106.03959",
          "publishedOn": "2021-06-24T01:51:44.683Z",
          "wordCount": 611,
          "title": "Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenghao Xu</a>",
          "description": "Multi-armed bandits (MAB) provide a principled online learning approach to\nattain the balance between exploration and exploitation. Due to the superior\nperformance and low feedback learning without the learning to act in multiple\nsituations, Multi-armed Bandits drawing widespread attention in applications\nranging such as recommender systems. Likewise, within the recommender system,\ncollaborative filtering (CF) is arguably the earliest and most influential\nmethod in the recommender system. Crucially, new users and an ever-changing\npool of recommended items are the challenges that recommender systems need to\naddress. For collaborative filtering, the classical method is training the\nmodel offline, then perform the online testing, but this approach can no longer\nhandle the dynamic changes in user preferences which is the so-called cold\nstart. So how to effectively recommend items to users in the absence of\neffective information? To address the aforementioned problems, a multi-armed\nbandit based collaborative filtering recommender system has been proposed,\nnamed BanditMF. BanditMF is designed to address two challenges in the\nmulti-armed bandits algorithm and collaborative filtering: (1) how to solve the\ncold start problem for collaborative filtering under the condition of scarcity\nof valid information, (2) how to solve the sub-optimal problem of bandit\nalgorithms in strong social relations domains caused by independently\nestimating unknown parameters associated with each user and ignoring\ncorrelations between users.",
          "link": "http://arxiv.org/abs/2106.10898",
          "publishedOn": "2021-06-24T01:51:44.678Z",
          "wordCount": 664,
          "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1\">Celestine Mendler-D&#xfc;nner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "An increasingly common setting in machine learning involves multiple parties,\neach with their own data, who want to jointly make predictions on future test\npoints. Agents wish to benefit from the collective expertise of the full set of\nagents to make better predictions than they would individually, but may not be\nwilling to release their data or model parameters. In this work, we explore a\ndecentralized mechanism to make collective predictions at test time, leveraging\neach agent's pre-trained model without relying on external validation, model\nretraining, or data pooling. Our approach takes inspiration from the literature\nin social science on human consensus-making. We analyze our mechanism\ntheoretically, showing that it converges to inverse meansquared-error (MSE)\nweighting in the large-sample limit. To compute error bars on the collective\npredictions we propose a decentralized Jackknife procedure that evaluates the\nsensitivity of our mechanism to a single agent's prediction. Empirically, we\ndemonstrate that our scheme effectively combines models with differing quality\nacross the input space. The proposed consensus prediction achieves significant\ngains over classical model averaging, and even outperforms weighted averaging\nschemes that have access to additional validation data.",
          "link": "http://arxiv.org/abs/2106.12012",
          "publishedOn": "2021-06-24T01:51:44.673Z",
          "wordCount": 618,
          "title": "Test-time Collective Prediction. (arXiv:2106.12012v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yapar_C/0/1/0/all/0/1\">&#xc7;a&#x11f;kan Yapar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1\">Ron Levie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caire_G/0/1/0/all/0/1\">Giuseppe Caire</a>",
          "description": "This paper deals with the problem of localization in a cellular network in a\ndense urban scenario. Global Navigation Satellite Systems typically perform\npoorly in urban environments, where the likelihood of line-of-sight conditions\nbetween the devices and the satellites is low, and thus alternative\nlocalization methods are required for good accuracy. We present a deep learning\nmethod for localization, based merely on pathloss, which does not require any\nincrease in computation complexity at the user devices with respect to the\ndevice standard operations, unlike methods that rely on time of arrival or\nangle of arrival information. In a wireless network, user devices scan the base\nstation beacon slots and identify the few strongest base station signals for\nhandover and user-base station association purposes. In the proposed method,\nthe user to be localized simply reports such received signal strengths to a\ncentral processing unit, which may be located in the cloud. For each base\nstation we have good approximation of the pathloss at every location in a dense\ngrid in the map. This approximation is provided by RadioUNet, a deep\nlearning-based simulator of pathloss functions in urban environment, that we\nhave previously proposed and published. Using the estimated pathloss radio maps\nof all base stations and the corresponding reported signal strengths, the\nproposed deep learning algorithm can extract a very accurate localization of\nthe user. The proposed method, called LocUNet, enjoys high robustness to\ninaccuracies in the estimated radio maps. We demonstrate this by numerical\nexperiments, which obtain state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.12556",
          "publishedOn": "2021-06-24T01:51:44.668Z",
          "wordCount": 694,
          "title": "Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach. (arXiv:2106.12556v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feurer_M/0/1/0/all/0/1\">Matthias Feurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1\">Jan N. van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadra_A/0/1/0/all/0/1\">Arlind Kadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gijsbers_P/0/1/0/all/0/1\">Pieter Gijsbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Andreas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1\">Joaquin Vanschoren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>",
          "description": "OpenML is an online platform for open science collaboration in machine\nlearning, used to share datasets and results of machine learning experiments.\nIn this paper we introduce OpenML-Python, a client API for Python, opening up\nthe OpenML platform for a wide range of Python-based tools. It provides easy\naccess to all datasets, tasks and experiments on OpenML from within Python. It\nalso provides functionality to conduct machine learning experiments, upload the\nresults to OpenML, and reproduce results which are stored on OpenML.\nFurthermore, it comes with a scikit-learn plugin and a plugin mechanism to\neasily integrate other machine learning libraries written in Python into the\nOpenML ecosystem. Source code and documentation is available at\nhttps://github.com/openml/openml-python/.",
          "link": "http://arxiv.org/abs/1911.02490",
          "publishedOn": "2021-06-24T01:51:44.653Z",
          "wordCount": 604,
          "title": "OpenML-Python: an extensible Python API for OpenML. (arXiv:1911.02490v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1\">Abhimitra Meka</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gengyan Li</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a> (1) ((1) ETH Zurich, (2) Google)",
          "description": "Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.",
          "link": "http://arxiv.org/abs/2104.05988",
          "publishedOn": "2021-06-24T01:51:44.648Z",
          "wordCount": 665,
          "title": "VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08817",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1\">Florian Graf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1\">Christoph D. Hofer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>",
          "description": "Minimizing cross-entropy over the softmax scores of a linear map composed\nwith a high-capacity encoder is arguably the most popular choice for training\nneural networks on supervised learning tasks. However, recent works show that\none can directly optimize the encoder instead, to obtain equally (or even more)\ndiscriminative representations via a supervised variant of a contrastive\nobjective. In this work, we address the question whether there are fundamental\ndifferences in the sought-for representation geometry in the output space of\nthe encoder at minimal loss. Specifically, we prove, under mild assumptions,\nthat both losses attain their minimum once the representations of each class\ncollapse to the vertices of a regular simplex, inscribed in a hypersphere. We\nprovide empirical evidence that this configuration is attained in practice and\nthat reaching a close-to-optimal state typically indicates good generalization\nperformance. Yet, the two losses show remarkably different optimization\nbehavior. The number of iterations required to perfectly fit to data scales\nsuperlinearly with the amount of randomly flipped labels for the supervised\ncontrastive loss. This is in contrast to the approximately linear scaling\npreviously reported for networks trained with cross-entropy.",
          "link": "http://arxiv.org/abs/2102.08817",
          "publishedOn": "2021-06-24T01:51:44.642Z",
          "wordCount": 635,
          "title": "Dissecting Supervised Constrastive Learning. (arXiv:2102.08817v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.00581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1\">Douglas Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>",
          "description": "Social learning is a key component of human and animal intelligence. By\ntaking cues from the behavior of experts in their environment, social learners\ncan acquire sophisticated behavior and rapidly adapt to new circumstances. This\npaper investigates whether independent reinforcement learning (RL) agents in a\nmulti-agent environment can learn to use social learning to improve their\nperformance. We find that in most circumstances, vanilla model-free RL agents\ndo not use social learning. We analyze the reasons for this deficiency, and\nshow that by imposing constraints on the training environment and introducing a\nmodel-based auxiliary loss we are able to obtain generalized social learning\npolicies which enable agents to: i) discover complex skills that are not\nlearned from single-agent training, and ii) adapt online to novel environments\nby taking cues from experts present in the new environment. In contrast, agents\ntrained with model-free RL or imitation learning generalize poorly and do not\nsucceed in the transfer tasks. By mixing multi-agent and solo training, we can\nobtain agents that use social learning to gain skills that they can deploy when\nalone, even out-performing agents trained alone from the start.",
          "link": "http://arxiv.org/abs/2010.00581",
          "publishedOn": "2021-06-24T01:51:44.637Z",
          "wordCount": 676,
          "title": "Emergent Social Learning via Multi-agent Reinforcement Learning. (arXiv:2010.00581v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "Imitation learning is an effective and safe technique to train robot policies\nin the real world because it does not depend on an expensive random exploration\nprocess. However, due to the lack of exploration, learning policies that\ngeneralize beyond the demonstrated behaviors is still an open challenge. We\npresent a novel imitation learning framework to enable robots to 1) learn\ncomplex real world manipulation tasks efficiently from a small number of human\ndemonstrations, and 2) synthesize new behaviors not contained in the collected\ndemonstrations. Our key insight is that multi-task domains often present a\nlatent structure, where demonstrated trajectories for different tasks intersect\nat common regions of the state space. We present Generalization Through\nImitation (GTI), a two-stage offline imitation learning algorithm that exploits\nthis intersecting structure to train goal-directed policies that generalize to\nunseen start and goal state combinations. In the first stage of GTI, we train a\nstochastic policy that leverages trajectory intersections to have the capacity\nto compose behaviors from different demonstration trajectories together. In the\nsecond stage of GTI, we collect a small set of rollouts from the unconditioned\nstochastic policy of the first stage, and train a goal-directed agent to\ngeneralize to novel start and goal configurations. We validate GTI in both\nsimulated domains and a challenging long-horizon robotic manipulation domain in\nthe real world. Additional results and videos are available at\nhttps://sites.google.com/view/gti2020/ .",
          "link": "http://arxiv.org/abs/2003.06085",
          "publishedOn": "2021-06-24T01:51:44.623Z",
          "wordCount": 709,
          "title": "Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. (arXiv:2003.06085v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1\">Dylan Slack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1\">Sophie Hilgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Hima Lakkaraju</a>",
          "description": "As machine learning models are increasingly used in critical decision-making\nsettings (e.g., healthcare, finance), there has been a growing emphasis on\ndeveloping methods to explain model predictions. Such \\textit{explanations} are\nused to understand and establish trust in models and are vital components in\nmachine learning pipelines. Though explanations are a critical piece in these\nsystems, there is little understanding about how they are vulnerable to\nmanipulation by adversaries. In this paper, we discuss how two broad classes of\nexplanations are vulnerable to manipulation. We demonstrate how adversaries can\ndesign biased models that manipulate model agnostic feature attribution methods\n(e.g., LIME \\& SHAP) and counterfactual explanations that hill-climb during the\ncounterfactual search (e.g., Wachter's Algorithm \\& DiCE) into\n\\textit{concealing} the model's biases. These vulnerabilities allow an\nadversary to deploy a biased model, yet explanations will not reveal this bias,\nthereby deceiving stakeholders into trusting the model. We evaluate the\nmanipulations on real world data sets, including COMPAS and Communities \\&\nCrime, and find explanations can be manipulated in practice.",
          "link": "http://arxiv.org/abs/2106.12563",
          "publishedOn": "2021-06-24T01:51:44.617Z",
          "wordCount": 605,
          "title": "Feature Attributions and Counterfactual Explanations Can Be Manipulated. (arXiv:2106.12563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Deligiannidis_S/0/1/0/all/0/1\">Stavros Deligiannidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesaritakis_C/0/1/0/all/0/1\">Charis Mesaritakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogris_A/0/1/0/all/0/1\">Adonis Bogris</a>",
          "description": "We investigate the complexity and performance of recurrent neural network\n(RNN) models as post-processing units for the compensation of fibre\nnonlinearities in digital coherent systems carrying polarization multiplexed\n16-QAM and 32-QAM signals. We evaluate three bi-directional RNN models, namely\nthe bi-LSTM, bi-GRU and bi-Vanilla-RNN and show that all of them are promising\nnonlinearity compensators especially in dispersion unmanaged systems. Our\nsimulations show that during inference the three models provide similar\ncompensation performance, therefore in real-life systems the simplest scheme\nbased on Vanilla-RNN units should be preferred. We compare bi-Vanilla-RNN with\nVolterra nonlinear equalizers and exhibit its superiority both in terms of\nperformance and complexity, thus highlighting that RNN processing is a very\npromising pathway for the upgrade of long-haul optical communication systems\nutilizing coherent detection.",
          "link": "http://arxiv.org/abs/2103.03832",
          "publishedOn": "2021-06-24T01:51:44.591Z",
          "wordCount": 601,
          "title": "Performance and Complexity Analysis of bi-directional Recurrent Neural Network Models vs. Volterra Nonlinear Equalizers in Digital Coherent Systems. (arXiv:2103.03832v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02869",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1\">Elif Keles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1\">Ahmet Topcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1\">Sachin Jambawalikar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>",
          "description": "Visual explanation methods have an important role in the prognosis of the\npatients where the annotated data is limited or unavailable. There have been\nseveral attempts to use gradient-based attribution methods to localize\npathology from medical scans without using segmentation labels. This research\ndirection has been impeded by the lack of robustness and reliability. These\nmethods are highly sensitive to the network parameters. In this study, we\nintroduce a robust visual explanation method to address this problem for\nmedical applications. We provide an innovative visual explanation algorithm for\ngeneral purpose and as an example application, we demonstrate its effectiveness\nfor quantifying lesions in the lungs caused by the Covid-19 with high accuracy\nand robustness without using dense segmentation labels. This approach overcomes\nthe drawbacks of commonly used Grad-CAM and its extended versions. The premise\nbehind our proposed strategy is that the information flow is minimized while\nensuring the classifier prediction stays similar. Our findings indicate that\nthe bottleneck condition provides a more stable severity estimation than the\nsimilar attribution methods.",
          "link": "http://arxiv.org/abs/2104.02869",
          "publishedOn": "2021-06-24T01:51:44.578Z",
          "wordCount": 697,
          "title": "Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>",
          "description": "Deep neural networks, while generalize well, are known to be sensitive to\nsmall adversarial perturbations. This phenomenon poses severe security threat\nand calls for in-depth investigation of the robustness of deep learning models.\nWith the emergence of neural networks for graph structured data, similar\ninvestigations are urged to understand their robustness. It has been found that\nadversarially perturbing the graph structure and/or node features may result in\na significant degradation of the model performance. In this work, we show from\na different angle that such fragility similarly occurs if the graph contains a\nfew bad-actor nodes, which compromise a trained graph neural network through\nflipping the connections to any targeted victim. Worse, the bad actors found\nfor one graph model severely compromise other models as well. We call the bad\nactors ``anchor nodes'' and propose an algorithm, named GUA, to identify them.\nThorough empirical investigations suggest an interesting finding that the\nanchor nodes often belong to the same class; and they also corroborate the\nintuitive trade-off between the number of anchor nodes and the attack success\nrate. For the dataset Cora which contains 2708 nodes, as few as six anchor\nnodes will result in an attack success rate higher than 80\\% for GCN and other\nthree models.",
          "link": "http://arxiv.org/abs/2002.04784",
          "publishedOn": "2021-06-24T01:51:44.572Z",
          "wordCount": 699,
          "title": "Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models. (arXiv:2002.04784v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06097",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1\">Yueming Lyu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>",
          "description": "Recent studies show a close connection between neural networks (NN) and\nkernel methods. However, most of these analyses (e.g., NTK) focus on the\ninfluence of (infinite) width instead of the depth of NN models. There remains\na gap between theory and practical network designs that benefit from the depth.\nThis paper first proposes a novel kernel family named Neural Optimization\nKernel (NOK). Our kernel is defined as the inner product between two $T$-step\nupdated functionals in RKHS w.r.t. a regularized optimization problem.\nTheoretically, we proved the monotonic descent property of our update rule for\nboth convex and non-convex problems, and a $O(1/T)$ convergence rate of our\nupdates for convex problems. Moreover, we propose a data-dependent structured\napproximation of our NOK, which builds the connection between training deep NNs\nand kernel methods associated with NOK. The resultant computational graph is a\nResNet-type finite width NN. Our structured approximation preserved the\nmonotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer\nNN performs $T$-step monotonic descent updates. Notably, we show our\n$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate\nw.r.t. a convex regularized problem, which explains the success of ReLU on\ntraining deep NN from a NN architecture optimization perspective. For the\nunsupervised learning and the shared parameter case, we show the equivalence of\ntraining structured NN with GD and performing functional gradient descent in\nRKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.\nFor finite NOKs, we prove generalization bounds. Remarkably, we show that\noverparameterized deep NN (NOK) can increase the expressive power to reduce\nempirical risk and reduce the generalization bound at the same time. Extensive\nexperiments verify the robustness of our structured NOK blocks.",
          "link": "http://arxiv.org/abs/2106.06097",
          "publishedOn": "2021-06-24T01:51:44.565Z",
          "wordCount": 736,
          "title": "Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1\">Mohammad Samragh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hossein Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1\">Aleksei Triastcyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarian_K/0/1/0/all/0/1\">Kambiz Azarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1\">Joseph Soriaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>",
          "description": "Splitting network computations between the edge device and a server enables\nlow edge-compute inference of neural networks but might expose sensitive\ninformation about the test query to the server. To address this problem,\nexisting techniques train the model to minimize information leakage for a given\nset of sensitive attributes. In practice, however, the test queries might\ncontain attributes that are not foreseen during training. We propose instead an\nunsupervised obfuscation method to discard the information irrelevant to the\nmain task. We formulate the problem via an information theoretical framework\nand derive an analytical solution for a given distortion to the model output.\nIn our method, the edge device runs the model up to a split layer determined\nbased on its computational capacity. It then obfuscates the obtained feature\nvector based on the first layer of the server model by removing the components\nin the null space as well as the low-energy components of the remaining signal.\nOur experimental results show that our method outperforms existing techniques\nin removing the information of the irrelevant attributes and maintaining the\naccuracy on the target label. We also show that our method reduces the\ncommunication cost and incurs only a small computational overhead.",
          "link": "http://arxiv.org/abs/2104.11413",
          "publishedOn": "2021-06-24T01:51:44.521Z",
          "wordCount": 670,
          "title": "Unsupervised Information Obfuscation for Split Inference of Neural Networks. (arXiv:2104.11413v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10492",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jeong_R/0/1/0/all/0/1\">Ryan Jeong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rolnick_D/0/1/0/all/0/1\">David Rolnick</a>",
          "description": "Assessing the complexity of functions computed by a neural network helps us\nunderstand how the network will learn and generalize. One natural measure of\ncomplexity is how the network distorts length - if the network takes a\nunit-length curve as input, what is the length of the resulting curve of\noutputs? It has been widely believed that this length grows exponentially in\nnetwork depth. We prove that in fact this is not the case: the expected length\ndistortion does not grow with depth, and indeed shrinks slightly, for ReLU\nnetworks with standard random initialization. We also generalize this result by\nproving upper bounds both for higher moments of the length distortion and for\nthe distortion of higher-dimensional volumes. These theoretical results are\ncorroborated by our experiments.",
          "link": "http://arxiv.org/abs/2102.10492",
          "publishedOn": "2021-06-24T01:51:44.505Z",
          "wordCount": 575,
          "title": "Deep ReLU Networks Preserve Expected Length. (arXiv:2102.10492v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1\">Laura D&#xf6;rr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1\">Felix Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1\">Alexander Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1\">Martin Pouls</a>",
          "description": "While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.",
          "link": "http://arxiv.org/abs/2104.09123",
          "publishedOn": "2021-06-24T01:51:44.491Z",
          "wordCount": 586,
          "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08583",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1\">Pujan Pokhrel</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1\">Md Tamjidul Hoque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1\">Mahdi Abdelguerfi</a>",
          "description": "This paper proposes a machine learning method based on the Extra Trees (ET)\nalgorithm for forecasting Significant Wave Heights in oceanic waters. To derive\nmultiple features from the CDIP buoys, which make point measurements, we first\nnowcast various parameters and then forecast them at 30-min intervals. The\nproposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root\nMean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for\none day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for\n14-day ahead prediction on the testing dataset. While other state-of-the-art\nmethods can only forecast up to 120 hours ahead, we extend it further to 14\ndays. Our proposed setup includes spectral features, hv-block cross-validation,\nand stringent QC criteria. The proposed algorithm performs significantly better\nthan the state-of-the-art methods commonly used for significant wave height\nforecasting for one-day ahead prediction. Moreover, the improved performance of\nthe proposed machine learning method compared to the numerical methods shows\nthat this performance can be extended to even longer periods allowing for early\nprediction of significant wave heights in oceanic waters.",
          "link": "http://arxiv.org/abs/2105.08583",
          "publishedOn": "2021-06-24T01:51:44.477Z",
          "wordCount": 644,
          "title": "Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v2 [physics.ao-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1\">Felix Gimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.",
          "link": "http://arxiv.org/abs/2103.03206",
          "publishedOn": "2021-06-24T01:51:44.471Z",
          "wordCount": 679,
          "title": "Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1\">Sergul Aydore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_W/0/1/0/all/0/1\">William Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1\">Michael Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melis_L/0/1/0/all/0/1\">Luca Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siva_A/0/1/0/all/0/1\">Ankit Siva</a>",
          "description": "We propose, implement, and evaluate a new algorithm for releasing answers to\nvery large numbers of statistical queries like $k$-way marginals, subject to\ndifferential privacy. Our algorithm makes adaptive use of a continuous\nrelaxation of the Projection Mechanism, which answers queries on the private\ndataset using simple perturbation, and then attempts to find the synthetic\ndataset that most closely matches the noisy answers. We use a continuous\nrelaxation of the synthetic dataset domain which makes the projection loss\ndifferentiable, and allows us to use efficient ML optimization techniques and\ntooling. Rather than answering all queries up front, we make judicious use of\nour privacy budget by iteratively and adaptively finding queries for which our\n(relaxed) synthetic data has high error, and then repeating the projection. We\nperform extensive experimental evaluations across a range of parameters and\ndatasets, and find that our method outperforms existing algorithms in many\ncases, especially when the privacy budget is small or the query class is large.",
          "link": "http://arxiv.org/abs/2103.06641",
          "publishedOn": "2021-06-24T01:51:44.466Z",
          "wordCount": 631,
          "title": "Differentially Private Query Release Through Adaptive Projection. (arXiv:2103.06641v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01350",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyu Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1\">Jiashan Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>",
          "description": "This paper primarily focuses on computing the Euclidean projection of a\nvector onto the $\\ell_{p}$ ball in which $p\\in(0,1)$. Such a problem emerges as\nthe core building block in statistical machine learning and signal processing\ntasks because of its ability to promote sparsity. However, efficient numerical\nalgorithms for finding the projections are still not available, particularly in\nlarge-scale optimization. To meet this challenge, we first derive the\nfirst-order necessary optimality conditions of this problem using Fr\\'echet\nnormal cone. Based on this characterization, we develop a novel numerical\napproach for computing the stationary point through solving a sequence of\nprojections onto the reweighted $\\ell_{1}$-balls. This method is practically\nsimple to implement and computationally efficient. Moreover, the proposed\nalgorithm is shown to converge uniquely under mild conditions and has a\nworst-case $O(1/\\sqrt{k})$ convergence rate. Numerical experiments demonstrate\nthe efficiency of our proposed algorithm.",
          "link": "http://arxiv.org/abs/2101.01350",
          "publishedOn": "2021-06-24T01:51:44.448Z",
          "wordCount": 643,
          "title": "Towards an efficient approach for the nonconvex $\\ell_p$-ball projection: algorithm and analysis. (arXiv:2101.01350v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1\">Sahil Suneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yunhui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yufan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laredo_J/0/1/0/all/0/1\">Jim Laredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1\">Alessandro Morari</a>",
          "description": "This work explores the signal awareness of AI models for source code\nunderstanding. Using a software vulnerability detection use case, we evaluate\nthe models' ability to capture the correct vulnerability signals to produce\ntheir predictions. Our prediction-preserving input minimization (P2IM) approach\nsystematically reduces the original source code to a minimal snippet which a\nmodel needs to maintain its prediction. The model's reliance on incorrect\nsignals is then uncovered when the vulnerability in the original code is\nmissing in the minimal snippet, both of which the model however predicts as\nbeing vulnerable. We measure the signal awareness of models using a new metric\nwe propose- Signal-aware Recall (SAR). We apply P2IM on three different neural\nnetwork architectures across multiple datasets. The results show a sharp drop\nin the model's Recall from the high 90s to sub-60s with the new metric,\nhighlighting that the models are presumably picking up a lot of noise or\ndataset nuances while learning their vulnerability detection logic. Although\nthe drop in model performance may be perceived as an adversarial attack, but\nthis isn't P2IM's objective. The idea is rather to uncover the signal-awareness\nof a black-box model in a data-driven manner via controlled queries. SAR's\npurpose is to measure the impact of task-agnostic model training, and not to\nsuggest a shortcoming in the Recall metric. The expectation, in fact, is for\nSAR to match Recall in the ideal scenario where the model truly captures\ntask-specific signals.",
          "link": "http://arxiv.org/abs/2011.14934",
          "publishedOn": "2021-06-24T01:51:44.442Z",
          "wordCount": 720,
          "title": "Probing Model Signal-Awareness via Prediction-Preserving Input Minimization. (arXiv:2011.14934v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12312",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Scognamiglio_S/0/1/0/all/0/1\">Salvatore Scognamiglio</a>",
          "description": "This paper introduces a neural network approach for fitting the Lee-Carter\nand the Poisson Lee-Carter model on multiple populations. We develop some\nneural networks that replicate the structure of the individual LC models and\nallow their joint fitting by analysing the mortality data of all the considered\npopulations simultaneously. The neural network architecture is specifically\ndesigned to calibrate each individual model using all available information\ninstead of using a population-specific subset of data as in the traditional\nestimation schemes. A large set of numerical experiments performed on all the\ncountries of the Human Mortality Database (HMD) shows the effectiveness of our\napproach. In particular, the resulting parameter estimates appear smooth and\nless sensitive to the random fluctuations often present in the mortality rates'\ndata, especially for low-population countries. In addition, the forecasting\nperformance results significantly improved as well.",
          "link": "http://arxiv.org/abs/2106.12312",
          "publishedOn": "2021-06-24T01:51:44.436Z",
          "wordCount": 573,
          "title": "Calibrating the Lee-Carter and the Poisson Lee-Carter models via Neural Networks. (arXiv:2106.12312v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1\">Christian Tomani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_S/0/1/0/all/0/1\">Sebastian Gruber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_M/0/1/0/all/0/1\">Muhammed Ebrar Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1\">Florian Buettner</a>",
          "description": "We address the problem of uncertainty calibration. While standard deep neural\nnetworks typically yield uncalibrated predictions, calibrated confidence scores\nthat are representative of the true likelihood of a prediction can be achieved\nusing post-hoc calibration methods. However, to date the focus of these\napproaches has been on in-domain calibration. Our contribution is two-fold.\nFirst, we show that existing post-hoc calibration methods yield highly\nover-confident predictions under domain shift. Second, we introduce a simple\nstrategy where perturbations are applied to samples in the validation set\nbefore performing the post-hoc calibration step. In extensive experiments, we\ndemonstrate that this perturbation step results in substantially better\ncalibration under domain shift on a wide range of architectures and modelling\ntasks.",
          "link": "http://arxiv.org/abs/2012.10988",
          "publishedOn": "2021-06-24T01:51:44.429Z",
          "wordCount": 604,
          "title": "Post-hoc Uncertainty Calibration for Domain Drift Scenarios. (arXiv:2012.10988v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12785",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider measurement-feedback control in linear dynamical systems from the\nperspective of regret minimization. Unlike most prior work in this area, we\nfocus on the problem of designing an online controller which competes with the\noptimal dynamic sequence of control actions selected in hindsight, instead of\nthe best controller in some specific class of controllers. This formulation of\nregret is attractive when the environment changes over time and no single\ncontroller achieves good performance over the entire time horizon. We show that\nin the measurement-feedback setting, unlike in the full-information setting,\nthere is no single offline controller which outperforms every other offline\ncontroller on every disturbance, and propose a new $H_2$-optimal offline\ncontroller as a benchmark for the online controller to compete against. We show\nthat the corresponding regret-optimal online controller can be found via a\nnovel reduction to the classical Nehari problem from robust control and present\na tight data-dependent bound on its regret.",
          "link": "http://arxiv.org/abs/2011.12785",
          "publishedOn": "2021-06-24T01:51:44.423Z",
          "wordCount": 614,
          "title": "Regret-optimal measurement-feedback control. (arXiv:2011.12785v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "We develop an algorithm for sequential adaptation of a classifier that is\ntrained for a source domain to generalize in an unannotated target domain. We\nconsider that the model has been trained on the source domain annotated data\nand then it needs to be adapted using the target domain unannotated data when\nthe source domain data is not accessible. We align the distributions of the\nsource and the target domains in a discriminative embedding space via an\nintermediate internal distribution. This distribution is estimated using the\nsource data representations in the embedding. We conduct experiments on four\nbenchmarks to demonstrate the method is effective and compares favorably\nagainst existing methods.",
          "link": "http://arxiv.org/abs/2007.00197",
          "publishedOn": "2021-06-24T01:51:44.407Z",
          "wordCount": 593,
          "title": "Sequential Model Adaptation Using Domain Agnostic Internal Distributions. (arXiv:2007.00197v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1\">Seyed Saeed Changiz Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fred X. Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1\">Mohammad Salameh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Keith Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shuo Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1\">Shangling Jui</a>",
          "description": "Despite the empirical success of neural architecture search (NAS) in deep\nlearning applications, the optimality, reproducibility and cost of NAS schemes\nremain hard to assess. In this paper, we propose Generative Adversarial NAS\n(GA-NAS) with theoretically provable convergence guarantees, promoting\nstability and reproducibility in neural architecture search. Inspired by\nimportance sampling, GA-NAS iteratively fits a generator to previously\ndiscovered top architectures, thus increasingly focusing on important parts of\na large search space. Furthermore, we propose an efficient adversarial learning\napproach, where the generator is trained by reinforcement learning based on\nrewards provided by a discriminator, thus being able to explore the search\nspace without evaluating a large number of architectures. Extensive experiments\nshow that GA-NAS beats the best published results under several cases on three\npublic NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search\nconstraints and search spaces. We show that GA-NAS can be used to improve\nalready optimized baselines found by other NAS methods, including EfficientNet\nand ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in\ntheir original search space.",
          "link": "http://arxiv.org/abs/2105.09356",
          "publishedOn": "2021-06-24T01:51:44.402Z",
          "wordCount": 670,
          "title": "Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousselle_F/0/1/0/all/0/1\">Fabrice Rousselle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_J/0/1/0/all/0/1\">Jan Nov&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1\">Alexander Keller</a>",
          "description": "We present a real-time neural radiance caching method for path-traced global\nillumination. Our system is designed to handle fully dynamic scenes, and makes\nno assumptions about the lighting, geometry, and materials. The data-driven\nnature of our approach sidesteps many difficulties of caching algorithms, such\nas locating, interpolating, and updating cache points. Since pretraining neural\nnetworks to handle novel, dynamic scenes is a formidable generalization\nchallenge, we do away with pretraining and instead achieve generalization via\nadaptation, i.e. we opt for training the radiance cache while rendering. We\nemploy self-training to provide low-noise training targets and simulate\ninfinite-bounce transport by merely iterating few-bounce training updates. The\nupdates and cache queries incur a mild overhead -- about 2.6ms on full HD\nresolution -- thanks to a streaming implementation of the neural network that\nfully exploits modern hardware. We demonstrate significant noise reduction at\nthe cost of little induced bias, and report state-of-the-art, real-time\nperformance on a number of challenging scenarios.",
          "link": "http://arxiv.org/abs/2106.12372",
          "publishedOn": "2021-06-24T01:51:44.397Z",
          "wordCount": 600,
          "title": "Real-time Neural Radiance Caching for Path Tracing. (arXiv:2106.12372v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henning_C/0/1/0/all/0/1\">Christian Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervera_M/0/1/0/all/0/1\">Maria R. Cervera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1\">Johannes von Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traber_R/0/1/0/all/0/1\">Regina Traber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehret_B/0/1/0/all/0/1\">Benjamin Ehret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Seijin Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1\">Benjamin F. Grewe</a>",
          "description": "Learning a sequence of tasks without access to i.i.d. observations is a\nwidely studied form of continual learning (CL) that remains challenging. In\nprinciple, Bayesian learning directly applies to this setting, since recursive\nand one-off Bayesian updates yield the same result. In practice, however,\nrecursive updating often leads to poor trade-off solutions across tasks because\napproximate inference is necessary for most models of interest. Here, we\ndescribe an alternative Bayesian approach where task-conditioned parameter\ndistributions are continually inferred from data. We offer a practical deep\nlearning implementation of our framework based on probabilistic\ntask-conditioned hypernetworks, an approach we term \"posterior meta-replay\".\nExperiments on standard benchmarks show that our probabilistic hypernetworks\ncompress sequences of posterior parameter distributions with virtually no\nforgetting. We obtain considerable performance gains compared to existing\nBayesian CL methods, and identify task inference as our major limiting factor.\nThis limitation has several causes that are independent of the considered\nsequential setting, opening up new avenues for progress in CL.",
          "link": "http://arxiv.org/abs/2103.01133",
          "publishedOn": "2021-06-24T01:51:44.391Z",
          "wordCount": 633,
          "title": "Posterior Meta-Replay for Continual Learning. (arXiv:2103.01133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1\">Itamar Ciochetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1\">Emerson Bertolo</a>",
          "description": "The objective of this paper is to develop predictive models to classify\nBrazilian legal proceedings in three possible classes of status: (i) archived\nproceedings, (ii) active proceedings, and (iii) suspended proceedings. This\nproblem's resolution is intended to assist public and private institutions in\nmanaging large portfolios of legal proceedings, providing gains in scale and\nefficiency. In this paper, legal proceedings are made up of sequences of short\ntexts called \"motions.\" We combined several natural language processing (NLP)\nand machine learning techniques to solve the problem. Although working with\nPortuguese NLP, which can be challenging due to lack of resources, our\napproaches performed remarkably well in the classification task, achieving\nmaximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93\n(weighted). Furthermore, we could extract and interpret the patterns learned by\none of our models besides quantifying how those patterns relate to the\nclassification task. The interpretability step is important among machine\nlearning legal applications and gives us an exciting insight into how black-box\nmodels make decisions.",
          "link": "http://arxiv.org/abs/2003.11561",
          "publishedOn": "2021-06-24T01:51:44.385Z",
          "wordCount": 677,
          "title": "Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04290",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1\">Xingchen Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>",
          "description": "In many applications, it is desirable that a classifier not only makes\naccurate predictions, but also outputs calibrated posterior probabilities.\nHowever, many existing classifiers, especially deep neural network classifiers,\ntend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a\nmodel by learning a calibration map. Existing approaches mostly focus on\nconstructing calibration maps with low calibration errors, however, this\nquality is inadequate for a calibrator being useful. In this paper, we\nintroduce two constraints that are worth consideration in designing a\ncalibration map for post-hoc calibration. Then we present Meta-Cal, which is\nbuilt from a base calibrator and a ranking model. Under some mild assumptions,\ntwo high-probability bounds are given with respect to these constraints.\nEmpirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular\nnetwork architectures show our proposed method significantly outperforms the\ncurrent state of the art for post-hoc multi-class classification calibration.",
          "link": "http://arxiv.org/abs/2105.04290",
          "publishedOn": "2021-06-24T01:51:44.369Z",
          "wordCount": 592,
          "title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking. (arXiv:2105.04290v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1\">Ming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu-Dong Liu</a>",
          "description": "Detecting the newly emerging malware variants in real time is crucial for\nmitigating cyber risks and proactively blocking intrusions. In this paper, we\npropose MG-DVD, a novel detection framework based on dynamic heterogeneous\ngraph learning, to detect malware variants in real time. Particularly, MG-DVD\nfirst models the fine-grained execution event streams of malware variants into\ndynamic heterogeneous graphs and investigates real-world meta-graphs between\nmalware objects, which can effectively characterize more discriminative\nmalicious evolutionary patterns between malware and their variants. Then,\nMG-DVD presents two dynamic walk-based heterogeneous graph learning methods to\nlearn more comprehensive representations of malware variants, which\nsignificantly reduces the cost of the entire graph retraining. As a result,\nMG-DVD is equipped with the ability to detect malware variants in real time,\nand it presents better interpretability by introducing meaningful meta-graphs.\nComprehensive experiments on large-scale samples prove that our proposed MG-DVD\noutperforms state-of-the-art methods in detecting malware variants in terms of\neffectiveness and efficiency.",
          "link": "http://arxiv.org/abs/2106.12288",
          "publishedOn": "2021-06-24T01:51:44.363Z",
          "wordCount": 618,
          "title": "MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning. (arXiv:2106.12288v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.",
          "link": "http://arxiv.org/abs/2106.12300",
          "publishedOn": "2021-06-24T01:51:44.358Z",
          "wordCount": 632,
          "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1\">Justin Dulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1\">Gregory Rolwes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1\">Duke Pauli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1\">Nadia Shakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1\">Abby Stylianou</a>",
          "description": "Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.",
          "link": "http://arxiv.org/abs/2106.05748",
          "publishedOn": "2021-06-24T01:51:44.353Z",
          "wordCount": 630,
          "title": "Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15341",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kim_S/0/1/0/all/0/1\">Suyong Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ji_W/0/1/0/all/0/1\">Weiqi Ji</a>, <a href=\"http://arxiv.org/find/math/1/au:+Deng_S/0/1/0/all/0/1\">Sili Deng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1\">Yingbo Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rackauckas_C/0/1/0/all/0/1\">Christopher Rackauckas</a>",
          "description": "Neural Ordinary Differential Equations (ODE) are a promising approach to\nlearn dynamic models from time-series data in science and engineering\napplications. This work aims at learning Neural ODE for stiff systems, which\nare usually raised from chemical kinetic modeling in chemical and biological\nsystems. We first show the challenges of learning neural ODE in the classical\nstiff ODE systems of Robertson's problem and propose techniques to mitigate the\nchallenges associated with scale separations in stiff systems. We then present\nsuccessful demonstrations in stiff systems of Robertson's problem and an air\npollution problem. The demonstrations show that the usage of deep networks with\nrectified activations, proper scaling of the network outputs as well as loss\nfunctions, and stabilized gradient calculations are the key techniques enabling\nthe learning of stiff neural ODE. The success of learning stiff neural ODE\nopens up possibilities of using neural ODEs in applications with widely varying\ntime-scales, like chemical dynamics in energy conversion, environmental\nengineering, and the life sciences.",
          "link": "http://arxiv.org/abs/2103.15341",
          "publishedOn": "2021-06-24T01:51:44.347Z",
          "wordCount": 609,
          "title": "Stiff Neural Ordinary Differential Equations. (arXiv:2103.15341v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aviv_R/0/1/0/all/0/1\">Rotem Zamir Aviv</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_I/0/1/0/all/0/1\">Ido Hakimi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1\">Assaf Schuster</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1\">Kfir Y. Levy</a> (1 and 3) ((1) Department of Electrical and Computer Engineering, Technion, (2) Department of Computer Science, Technion, (3) A Viterbi Fellow)",
          "description": "We consider stochastic convex optimization problems, where several machines\nact asynchronously in parallel while sharing a common memory. We propose a\nrobust training method for the constrained setting and derive non asymptotic\nconvergence guarantees that do not depend on prior knowledge of update delays,\nobjective smoothness, and gradient variance. Conversely, existing methods for\nthis setting crucially rely on this prior knowledge, which render them\nunsuitable for essentially all shared-resources computational environments,\nsuch as clouds and data centers. Concretely, existing approaches are unable to\naccommodate changes in the delays which result from dynamic allocation of the\nmachines, while our method implicitly adapts to such changes.",
          "link": "http://arxiv.org/abs/2106.12261",
          "publishedOn": "2021-06-24T01:51:44.341Z",
          "wordCount": 569,
          "title": "Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays. (arXiv:2106.12261v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaocheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>",
          "description": "In this paper, we study the bandits with knapsacks (BwK) problem and develop\na primal-dual based algorithm that achieves a problem-dependent logarithmic\nregret bound. The BwK problem extends the multi-arm bandit (MAB) problem to\nmodel the resource consumption associated with playing each arm, and the\nexisting BwK literature has been mainly focused on deriving asymptotically\noptimal distribution-free regret bounds. We first study the primal and dual\nlinear programs underlying the BwK problem. From this primal-dual perspective,\nwe discover symmetry between arms and knapsacks, and then propose a new notion\nof sub-optimality measure for the BwK problem. The sub-optimality measure\nhighlights the important role of knapsacks in determining algorithm regret and\ninspires the design of our two-phase algorithm. In the first phase, the\nalgorithm identifies the optimal arms and the binding knapsacks, and in the\nsecond phase, it exhausts the binding knapsacks via playing the optimal arms\nthrough an adaptive procedure. Our regret upper bound involves the proposed\nsub-optimality measure and it has a logarithmic dependence on length of horizon\n$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the\nnumber of knapsacks). To the best of our knowledge, this is the first\nproblem-dependent logarithmic regret bound for solving the general BwK problem.",
          "link": "http://arxiv.org/abs/2102.06385",
          "publishedOn": "2021-06-24T01:51:44.326Z",
          "wordCount": 687,
          "title": "The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks. (arXiv:2102.06385v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06304",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Maurer_A/0/1/0/all/0/1\">Andreas Maurer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pontil_M/0/1/0/all/0/1\">Massimiliano Pontil</a>",
          "description": "We prove concentration inequalities for functions of independent random\nvariables {under} sub-gaussian and sub-exponential conditions. The utility of\nthe inequalities is demonstrated by an extension of the now classical method of\nRademacher complexities to Lipschitz function classes and unbounded\nsub-exponential distribution.",
          "link": "http://arxiv.org/abs/2102.06304",
          "publishedOn": "2021-06-24T01:51:44.319Z",
          "wordCount": 506,
          "title": "Some Hoeffding- and Bernstein-type Concentration Inequalities. (arXiv:2102.06304v4 [math.PR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1\">Sam Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>",
          "description": "Variational inference (VI) plays an essential role in approximate Bayesian\ninference due to its computational efficiency and broad applicability. Crucial\nto the performance of VI is the selection of the associated divergence measure,\nas VI approximates the intractable distribution by minimizing this divergence.\nIn this paper we propose a meta-learning algorithm to learn the divergence\nmetric suited for the task of interest, automating the design of VI methods. In\naddition, we learn the initialization of the variational parameters without\nadditional cost when our method is deployed in the few-shot learning scenarios.\nWe demonstrate our approach outperforms standard VI on Gaussian mixture\ndistribution approximation, Bayesian neural network regression, image\ngeneration with variational autoencoders and recommender systems with a partial\nvariational autoencoder.",
          "link": "http://arxiv.org/abs/2007.02912",
          "publishedOn": "2021-06-24T01:51:44.314Z",
          "wordCount": 588,
          "title": "Meta-Learning Divergences of Variational Inference. (arXiv:2007.02912v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1\">Sainyam Galhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1\">Romila Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1\">Babak Salimi</a>",
          "description": "There has been a recent resurgence of interest in explainable artificial\nintelligence (XAI) that aims to reduce the opaqueness of AI-based\ndecision-making systems, allowing humans to scrutinize and trust them. Prior\nwork in this context has focused on the attribution of responsibility for an\nalgorithm's decisions to its inputs wherein responsibility is typically\napproached as a purely associational concept. In this paper, we propose a\nprincipled causality-based approach for explaining black-box decision-making\nsystems that addresses limitations of existing methods in XAI. At the core of\nour framework lies probabilistic contrastive counterfactuals, a concept that\ncan be traced back to philosophical, cognitive, and social foundations of\ntheories on how humans generate and select explanations. We show how such\ncounterfactuals can quantify the direct and indirect influences of a variable\non decisions made by an algorithm, and provide actionable recourse for\nindividuals negatively affected by the algorithm's decision. Unlike prior work,\nour system, LEWIS: (1)can compute provably effective explanations and recourse\nat local, global and contextual levels (2)is designed to work with users with\nvarying levels of background knowledge of the underlying causal model and\n(3)makes no assumptions about the internals of an algorithmic system except for\nthe availability of its input-output data. We empirically evaluate LEWIS on\nthree real-world datasets and show that it generates human-understandable\nexplanations that improve upon state-of-the-art approaches in XAI, including\nthe popular LIME and SHAP. Experiments on synthetic data further demonstrate\nthe correctness of LEWIS's explanations and the scalability of its recourse\nalgorithm.",
          "link": "http://arxiv.org/abs/2103.11972",
          "publishedOn": "2021-06-24T01:51:44.308Z",
          "wordCount": 719,
          "title": "Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals. (arXiv:2103.11972v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trosser_F/0/1/0/all/0/1\">Fulya Tr&#xf6;sser</a> (MIAT INRA), <a href=\"http://arxiv.org/find/cs/1/au:+Givry_S/0/1/0/all/0/1\">Simon de Givry</a> (MIAT INRA), <a href=\"http://arxiv.org/find/cs/1/au:+Katsirelos_G/0/1/0/all/0/1\">George Katsirelos</a> (MIA-Paris)",
          "description": "Bayesian networks are probabilistic graphical models with a wide range of\napplication areas including gene regulatory networks inference, risk analysis\nand image processing. Learning the structure of a Bayesian network (BNSL) from\ndiscrete data is known to be an NP-hard task with a superexponential search\nspace of directed acyclic graphs. In this work, we propose a new polynomial\ntime algorithm for discovering a subset of all possible cluster cuts, a greedy\nalgorithm for approximately solving the resulting linear program, and a\ngeneralised arc consistency algorithm for the acyclicity constraint. We embed\nthese in the constraint programmingbased branch-and-bound solver CPBayes and\nshow that, despite being suboptimal, they improve performance by orders of\nmagnitude. The resulting solver also compares favourably with GOBNILP, a\nstate-of-the-art solver for the BNSL problem which solves an NP-hard problem to\ndiscover each cut and solves the linear program exactly.",
          "link": "http://arxiv.org/abs/2106.12269",
          "publishedOn": "2021-06-24T01:51:44.302Z",
          "wordCount": 604,
          "title": "Improved Acyclicity Reasoning for Bayesian Network Structure Learning with Constraint Programming. (arXiv:2106.12269v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1\">Miguel Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1\">Samuel S. Schoenholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andrea J. Liu</a>",
          "description": "We show that learning can be improved by using loss functions that evolve\ncyclically during training to emphasize one class at a time. In\nunderparameterized networks, such dynamical loss functions can lead to\nsuccessful training for networks that fail to find a deep minima of the\nstandard cross-entropy loss. In overparameterized networks, dynamical loss\nfunctions can lead to better generalization. Improvement arises from the\ninterplay of the changing loss landscape with the dynamics of the system as it\nevolves to minimize the loss. In particular, as the loss function oscillates,\ninstabilities develop in the form of bifurcation cascades, which we study using\nthe Hessian and Neural Tangent Kernel. Valleys in the landscape widen and\ndeepen, and then narrow and rise as the loss landscape changes during a cycle.\nAs the landscape narrows, the learning rate becomes too large and the network\nbecomes unstable and bounces around the valley. This process ultimately pushes\nthe system into deeper and wider regions of the loss landscape and is\ncharacterized by decreasing eigenvalues of the Hessian. This results in better\nregularized models with improved generalization performance.",
          "link": "http://arxiv.org/abs/2102.03793",
          "publishedOn": "2021-06-24T01:51:44.286Z",
          "wordCount": 668,
          "title": "Tilting the playing field: Dynamical loss functions for machine learning. (arXiv:2102.03793v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1\">Talia Konkle</a>",
          "description": "The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties",
          "link": "http://arxiv.org/abs/2006.07991",
          "publishedOn": "2021-06-24T01:51:44.281Z",
          "wordCount": 752,
          "title": "Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itoh_A/0/1/0/all/0/1\">Asami Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1\">Ryota Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimaoka_M/0/1/0/all/0/1\">Motomu Shimaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "Shift workers who are essential contributors to our society, face high risks\nof poor health and wellbeing. To help with their problems, we collected and\nanalyzed physiological and behavioral wearable sensor data from shift working\nnurses and doctors, as well as their behavioral questionnaire data and their\nself-reported daily health and wellbeing labels, including alertness,\nhappiness, energy, health, and stress. We found the similarities and\ndifferences between the responses of nurses and doctors. According to the\ndifferences in self-reported health and wellbeing labels between nurses and\ndoctors, and the correlations among their labels, we proposed a job-role based\nmultitask and multilabel deep learning model, where we modeled physiological\nand behavioral data for nurses and doctors simultaneously to predict\nparticipants' next day's multidimensional self-reported health and wellbeing\nstatus. Our model showed significantly better performances than baseline models\nand previous state-of-the-art models in the evaluations of binary/3-class\nclassification and regression prediction tasks. We also found features related\nto heart rate, sleep, and work shift contributed to shift workers' health and\nwellbeing.",
          "link": "http://arxiv.org/abs/2106.12081",
          "publishedOn": "2021-06-24T01:51:44.275Z",
          "wordCount": 622,
          "title": "Forecasting Health and Wellbeing for Shift Workers Using Job-role Based Deep Neural Network. (arXiv:2106.12081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haddad_M/0/1/0/all/0/1\">Maroun Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1\">Mohamed Bouguessa</a>",
          "description": "While representation learning has yielded a great success on many graph\nlearning tasks, there is little understanding behind the structures that are\nbeing captured by these embeddings. For example, we wonder if the topological\nfeatures, such as the Triangle Count, the Degree of the node and other\ncentrality measures are concretely encoded in the embeddings. Furthermore, we\nask if the presence of these structures in the embeddings is necessary for a\nbetter performance on the downstream tasks, such as clustering and\nclassification. To address these questions, we conduct an extensive empirical\nstudy over three classes of unsupervised graph embedding models and seven\ndifferent variants of Graph Autoencoders. Our results show that five\ntopological features: the Degree, the Local Clustering Score, the Betweenness\nCentrality, the Eigenvector Centrality, and Triangle Count are concretely\npreserved in the first layer of the graph autoencoder that employs the SUM\naggregation rule, under the condition that the model preserves the second-order\nproximity. We supplement further evidence for the presence of these features by\nrevealing a hierarchy in the distribution of the topological features in the\nembeddings of the aforementioned model. We also show that a model with such\nproperties can outperform other models on certain downstream tasks, especially\nwhen the preserved features are relevant to the task at hand. Finally, we\nevaluate the suitability of our findings through a test case study related to\nsocial influence prediction.",
          "link": "http://arxiv.org/abs/2106.12005",
          "publishedOn": "2021-06-24T01:51:44.270Z",
          "wordCount": 660,
          "title": "Exploring the Representational Power of Graph Autoencoder. (arXiv:2106.12005v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>",
          "description": "Transfer learning has become a common solution to address training data\nscarcity in practice. It trains a specified student model by reusing or\nfine-tuning early layers of a well-trained teacher model that is usually\npublicly available. However, besides utility improvement, the transferred\npublic knowledge also brings potential threats to model confidentiality, and\neven further raises other security and privacy issues.\n\nIn this paper, we present the first comprehensive investigation of the\nteacher model exposure threat in the transfer learning context, aiming to gain\na deeper insight into the tension between public knowledge and model\nconfidentiality. To this end, we propose a teacher model fingerprinting attack\nto infer the origin of a student model, i.e., the teacher model it transfers\nfrom. Specifically, we propose a novel optimization-based method to carefully\ngenerate queries to probe the student model to realize our attack. Unlike\nexisting model reverse engineering approaches, our proposed fingerprinting\nmethod neither relies on fine-grained model outputs, e.g., posteriors, nor\nauxiliary information of the model architecture or training dataset. We\nsystematically evaluate the effectiveness of our proposed attack. The empirical\nresults demonstrate that our attack can accurately identify the model origin\nwith few probing queries. Moreover, we show that the proposed attack can serve\nas a stepping stone to facilitating other attacks against machine learning\nmodels, such as model stealing.",
          "link": "http://arxiv.org/abs/2106.12478",
          "publishedOn": "2021-06-24T01:51:44.264Z",
          "wordCount": 652,
          "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning. (arXiv:2106.12478v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12511",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1\">Grant Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Paul P Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1\">Neal Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1\">Bryan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1\">Alan C. Kwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1\">Matthew J. Shun-Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1\">Kevin M. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1\">Joseph Ebinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1\">Florian Rader</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">David H. Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1\">Ingela Schnittger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1\">Euan A. Ashley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">James Y. Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Jignesh Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1\">Ronald Witteles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1\">Susan Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1\">David Ouyang</a>",
          "description": "Left ventricular hypertrophy (LVH) results from chronic remodeling caused by\na broad range of systemic and cardiovascular disease including hypertension,\naortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early\ndetection and characterization of LVH can significantly impact patient care but\nis limited by under-recognition of hypertrophy, measurement error and\nvariability, and difficulty differentiating etiologies of LVH. To overcome this\nchallenge, we present EchoNet-LVH - a deep learning workflow that automatically\nquantifies ventricular hypertrophy with precision equal to human experts and\npredicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model\naccurately measures intraventricular wall thickness (mean absolute error [MAE]\n1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI\n2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and\nclassifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic\ncardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets\nfrom independent domestic and international healthcare systems, EchoNet-LVH\naccurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)\nand detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy\n(AUC 0.89) on the domestic external validation site. Leveraging measurements\nacross multiple heart beats, our model can more accurately identify subtle\nchanges in LV geometry and its causal etiologies. Compared to human experts,\nEchoNet-LVH is fully automated, allowing for reproducible, precise\nmeasurements, and lays the foundation for precision diagnosis of cardiac\nhypertrophy. As a resource to promote further innovation, we also make publicly\navailable a large dataset of 23,212 annotated echocardiogram videos.",
          "link": "http://arxiv.org/abs/2106.12511",
          "publishedOn": "2021-06-24T01:51:44.249Z",
          "wordCount": 731,
          "title": "High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Kaichao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "This paper studies task adaptive pre-trained model selection, an\nunderexplored problem of assessing pre-trained models for the target task and\nselect best ones from the model zoo \\emph{without fine-tuning}. A few pilot\nworks addressed the problem in transferring supervised pre-trained models to\nclassification tasks, but they cannot handle emerging unsupervised pre-trained\nmodels or regression tasks. In pursuit of a practical assessment method, we\npropose to estimate the maximum value of label evidence given features\nextracted by pre-trained models. Unlike the maximum likelihood, the maximum\nevidence is \\emph{immune to over-fitting}, while its expensive computation can\nbe dramatically reduced by our carefully designed algorithm. The Logarithm of\nMaximum Evidence (LogME) can be used to assess pre-trained models for transfer\nlearning: a pre-trained model with a high LogME value is likely to have good\ntransfer performance. LogME is \\emph{fast, accurate, and general},\ncharacterizing itself as the first practical method for assessing pre-trained\nmodels. Compared with brute-force fine-tuning, LogME brings at most\n$3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory\nfootprint. It outperforms prior methods by a large margin in their setting and\nis applicable to new settings. It is general enough for diverse pre-trained\nmodels (supervised pre-trained and unsupervised pre-trained), downstream tasks\n(classification and regression), and modalities (vision and language). Code is\navailable at this repository:\n\\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.",
          "link": "http://arxiv.org/abs/2102.11005",
          "publishedOn": "2021-06-24T01:51:44.244Z",
          "wordCount": 701,
          "title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chang-Han Rhee</a>",
          "description": "The empirical success of deep learning is often attributed to SGD's\nmysterious ability to avoid sharp local minima in the loss landscape, as sharp\nminima are known to lead to poor generalization. Recently, empirical evidence\nof heavy-tailed gradient noise was reported in many deep learning tasks, and it\nwas shown in \\c{S}im\\c{s}ekli (2019a,b) that SGD can escape sharp local minima\nunder the presence of such heavy-tailed gradient noise, providing a partial\nsolution to the mystery. In this work, we analyze a popular variant of SGD\nwhere gradients are truncated above a fixed threshold. We show that it achieves\na stronger notion of avoiding sharp minima: it can effectively eliminate sharp\nlocal minima entirely from its training trajectory. We characterize the\ndynamics of truncated SGD driven by heavy-tailed noises. First, we show that\nthe truncation threshold and width of the attraction field dictate the order of\nthe first exit time from the associated local minimum. Moreover, when the\nobjective function satisfies appropriate structural conditions, we prove that\nas the learning rate decreases, the dynamics of heavy-tailed truncated SGD\nclosely resemble those of a continuous-time Markov chain that never visits any\nsharp minima. Real data experiments on deep learning confirm our theoretical\nprediction that heavy-tailed SGD with gradient clipping finds a \"flatter\" local\nminima and achieves better generalization.",
          "link": "http://arxiv.org/abs/2102.04297",
          "publishedOn": "2021-06-24T01:51:44.238Z",
          "wordCount": 713,
          "title": "Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise. (arXiv:2102.04297v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1\">Amy Widdicombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1\">Simon J. Julier</a>",
          "description": "Binarized Neural Networks (BNNs) have the potential to revolutionize the way\nthat deep learning is carried out in edge computing platforms. However, the\neffectiveness of interpretability methods on these networks has not been\nassessed.\n\nIn this paper, we compare the performance of several widely used saliency\nmap-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when\napplied to Binarized or Full Precision Neural Networks (FPNNs). We found that\nthe basic Gradient method produces very similar-looking maps for both types of\nnetwork. However, SmoothGrad produces significantly noisier maps for BNNs.\nGradCAM also produces saliency maps which differ between network types, with\nsome of the BNNs having seemingly nonsensical explanations. We comment on\npossible reasons for these differences in explanations and present it as an\nexample of why interpretability techniques should be tested on a wider range of\nnetwork types.",
          "link": "http://arxiv.org/abs/2106.12569",
          "publishedOn": "2021-06-24T01:51:44.232Z",
          "wordCount": 592,
          "title": "Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.05437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hashemizadeh_M/0/1/0/all/0/1\">Meraj Hashemizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Michelle Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">Jacob Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "Tensor Networks (TN) offer a powerful framework to efficiently represent very\nhigh-dimensional objects. TN have recently shown their potential for machine\nlearning applications and offer a unifying view of common tensor decomposition\nmodels such as Tucker, tensor train (TT) and tensor ring (TR). However,\nidentifying the best tensor network structure from data for a given task is\nchallenging. In this work, we leverage the TN formalism to develop a generic\nand efficient adaptive algorithm to jointly learn the structure and the\nparameters of a TN from data. Our method is based on a simple greedy approach\nstarting from a rank one tensor and successively identifying the most promising\ntensor network edges for small rank increments. Our algorithm can adaptively\nidentify TN structures with small number of parameters that effectively\noptimize any differentiable objective function. Experiments on tensor\ndecomposition, tensor completion and model compression tasks demonstrate the\neffectiveness of the proposed algorithm. In particular, our method outperforms\nthe state-of-the-art evolutionary topology search [Li and Sun, 2020] for tensor\ndecomposition of images (while being orders of magnitude faster) and finds\nefficient tensor network structures to compress neural networks outperforming\npopular TT based approaches [Novikov et al., 2015].",
          "link": "http://arxiv.org/abs/2008.05437",
          "publishedOn": "2021-06-24T01:51:44.226Z",
          "wordCount": 653,
          "title": "Adaptive Learning of Tensor Network Structures. (arXiv:2008.05437v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1\">Pawe&#x142; Wawrzy&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1\">Daniel Marczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "We propose a new method for unsupervised continual knowledge consolidation in\ngenerative models that relies on the partitioning of Variational Autoencoder's\nlatent space. Acquiring knowledge about new data samples without forgetting\nprevious ones is a critical problem of continual learning. Currently proposed\nmethods achieve this goal by extending the existing model while constraining\nits behavior not to degrade on the past data, which does not exploit the full\npotential of relations within the entire training dataset. In this work, we\nidentify this limitation and posit the goal of continual learning as a\nknowledge accumulation task. We solve it by continuously re-aligning latent\nspace partitions that we call bands which are representations of samples seen\nin different tasks, driven by the similarity of the information they contain.\nIn addition, we introduce a simple yet effective method for controlled\nforgetting of past data that improves the quality of reconstructions encoded in\nlatent bands and a latent space disentanglement technique that improves\nknowledge consolidation. On top of the standard continual learning evaluation\nbenchmarks, we evaluate our method on a new knowledge consolidation scenario\nand show that the proposed approach outperforms state-of-the-art by up to\ntwofold across all testing scenarios.",
          "link": "http://arxiv.org/abs/2106.12196",
          "publishedOn": "2021-06-24T01:51:44.211Z",
          "wordCount": 634,
          "title": "Multiband VAE: Latent Space Partitioning for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1\">Jan Brabec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1\">Jan Kohout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Martin Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajer_L/0/1/0/all/0/1\">Lukas Bajer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>",
          "description": "We address the problems of identifying malware in network telemetry logs and\nproviding \\emph{indicators of compromise} -- comprehensible explanations of\nbehavioral patterns that identify the threat. In our system, an array of\nspecialized detectors abstracts network-flow data into comprehensible\n\\emph{network events} in a first step. We develop a neural network that\nprocesses this sequence of events and identifies specific threats, malware\nfamilies and broad categories of malware. We then use the\n\\emph{integrated-gradients} method to highlight events that jointly constitute\nthe characteristic behavioral pattern of the threat. We compare network\narchitectures based on CNNs, LSTMs, and transformers, and explore the efficacy\nof unsupervised pre-training experimentally on large-scale telemetry data. We\ndemonstrate how this system detects njRAT and other malware based on behavioral\npatterns.",
          "link": "http://arxiv.org/abs/2106.12328",
          "publishedOn": "2021-06-24T01:51:44.205Z",
          "wordCount": 572,
          "title": "Learning Explainable Representations of Malware Behavior. (arXiv:2106.12328v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1\">Mats L. Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoning_J/0/1/0/all/0/1\">Julius Sch&#xf6;ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krumnack_U/0/1/0/all/0/1\">Ulf Krumnack</a>",
          "description": "Applying artificial neural networks (ANN) to specific tasks, researchers,\nprogrammers, and other specialists usually overshot the number of convolutional\nlayers in their designs. By implication, these ANNs hold too many parameters,\nwhich needed unnecessarily trained without impacting the result. The features,\na convolutional layer can process, are strictly limited by its receptive field.\nBy layer-wise analyzing the expansion of the receptive fields, we can reliably\npredict sequences of layers that will not contribute qualitatively to the\ninference in thegiven ANN architecture. Based on these analyses, we propose\ndesign strategies to resolve these inefficiencies, optimizing the\nexplainability and the computational performance of ANNs. Since neither the\nstrategies nor the analysis requires training of the actual model, these\ninsights allow for a very efficient design process of ANNs architectures which\nmight be automated in the future.",
          "link": "http://arxiv.org/abs/2106.12307",
          "publishedOn": "2021-06-24T01:51:44.186Z",
          "wordCount": 593,
          "title": "Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis. (arXiv:2106.12307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1\">Tiago de Souza Farias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1\">Jonas Maziero</a>",
          "description": "We introduce feature alignment, a technique for obtaining approximate\nreversibility in artificial neural networks. By means of feature extraction, we\ncan train a neural network to learn an estimated map for its reverse process\nfrom outputs to inputs. Combined with variational autoencoders, we can generate\nnew samples from the same statistics as the training data. Improvements of the\nresults are obtained by using concepts from generative adversarial networks.\nFinally, we show that the technique can be modified for training neural\nnetworks locally, saving computational memory resources. Applying these\ntechniques, we report results for three vision generative tasks: MNIST,\nCIFAR-10, and celebA.",
          "link": "http://arxiv.org/abs/2106.12562",
          "publishedOn": "2021-06-24T01:51:44.178Z",
          "wordCount": 540,
          "title": "Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>",
          "description": "We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.",
          "link": "http://arxiv.org/abs/2010.03802",
          "publishedOn": "2021-06-24T01:51:44.123Z",
          "wordCount": 662,
          "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12506",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce a new family of particle evolution samplers suitable for\nconstrained domains and non-Euclidean geometries. Stein Variational Mirror\nDescent and Mirrored Stein Variational Gradient Descent minimize the\nKullback-Leibler (KL) divergence to constrained target distributions by\nevolving particles in a dual space defined by a mirror map. Stein Variational\nNatural Gradient exploits non-Euclidean geometry to more efficiently minimize\nthe KL divergence to unconstrained targets. We derive these samplers from a new\nclass of mirrored Stein operators and adaptive kernels developed in this work.\nWe demonstrate that these new samplers yield accurate approximations to\ndistributions on the simplex, deliver valid confidence intervals in\npost-selection inference, and converge more rapidly than prior methods in\nlarge-scale unconstrained posterior inference. Finally, we establish the\nconvergence of our new procedures under verifiable conditions on the target\ndistribution.",
          "link": "http://arxiv.org/abs/2106.12506",
          "publishedOn": "2021-06-24T01:51:44.116Z",
          "wordCount": 561,
          "title": "Sampling with Mirrored Stein Operators. (arXiv:2106.12506v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1\">Arthur Drichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drury_V/0/1/0/all/0/1\">Vincent Drury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1\">Justus von Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1\">Ulrike Meyer</a>",
          "description": "Current popular phishing prevention techniques mainly utilize reactive\nblocklists, which leave a ``window of opportunity'' for attackers during which\nvictims are unprotected. One possible approach to shorten this window aims to\ndetect phishing attacks earlier, during website preparation, by monitoring\nCertificate Transparency (CT) logs. Previous attempts to work with CT log data\nfor phishing classification exist, however they lack evaluations on actual CT\nlog data. In this paper, we present a pipeline that facilitates such\nevaluations by addressing a number of problems when working with CT log data.\nThe pipeline includes dataset creation, training, and past or live\nclassification of CT logs. Its modular structure makes it possible to easily\nexchange classifiers or verification sources to support ground truth labeling\nefforts and classifier comparisons. We test the pipeline on a number of new and\nexisting classifiers, and find a general potential to improve classifiers for\nthis scenario in the future. We publish the source code of the pipeline and the\nused datasets along with this paper\n(https://gitlab.com/rwth-itsec/ctl-pipeline), thus making future research in\nthis direction more accessible.",
          "link": "http://arxiv.org/abs/2106.12343",
          "publishedOn": "2021-06-24T01:51:44.111Z",
          "wordCount": 657,
          "title": "Finding Phish in a Haystack: A Pipeline for Phishing Classification on Certificate Transparency Logs. (arXiv:2106.12343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khodaverdian_Z/0/1/0/all/0/1\">Zeinab Khodaverdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadr_H/0/1/0/all/0/1\">Hossein Sadr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalatpanah_S/0/1/0/all/0/1\">Seyed Ahmad Edalatpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solimandarabi_M/0/1/0/all/0/1\">Mojdeh Nazari Solimandarabi</a>",
          "description": "Cloud computing service models have experienced rapid growth and inefficient\nresource usage is known as one of the greatest causes of high energy\nconsumption in cloud data centers. Resource allocation in cloud data centers\naiming to reduce energy consumption has been conducted using live migration of\nVirtual Machines (VMs) and their consolidation into the small number of\nPhysical Machines (PMs). However, the selection of the appropriate VM for\nmigration is an important challenge. To solve this issue, VMs can be classified\naccording to the pattern of user requests into sensitive or insensitive classes\nto latency, and thereafter suitable VMs can be selected for migration. In this\npaper, the combination of Convolution Neural Network (CNN) and Gated Recurrent\nUnit (GRU) is utilized for the classification of VMs in the Microsoft Azure\ndataset. Due to the fact the majority of VMs in this dataset are labeled as\ninsensitive to latency, migration of more VMs in this group not only reduces\nenergy consumption but also decreases the violation of Service Level Agreements\n(SLA). Based on the empirical results, the proposed model obtained an accuracy\nof 95.18which clearly demonstrates the superiority of our proposed model\ncompared to other existing models.",
          "link": "http://arxiv.org/abs/2106.12178",
          "publishedOn": "2021-06-24T01:51:44.106Z",
          "wordCount": 655,
          "title": "Combination of Convolutional Neural Network and Gated Recurrent Unit for Energy Aware Resource Allocation. (arXiv:2106.12178v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiapeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1\">Etienne Denis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1\">William L. Hamilton</a>",
          "description": "Conventional representation learning algorithms for knowledge graphs (KG) map\neach entity to a unique embedding vector. Such a shallow lookup results in a\nlinear growth of memory consumption for storing the embedding matrix and incurs\nhigh computational costs when working with real-world KGs. Drawing parallels\nwith subword tokenization commonly used in NLP, we explore the landscape of\nmore parameter-efficient node embedding strategies with possibly sublinear\nmemory requirements. To this end, we propose NodePiece, an anchor-based\napproach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of\nsubword/sub-entity units is constructed from anchor nodes in a graph with known\nrelation types. Given such a fixed-size vocabulary, it is possible to bootstrap\nan encoding and embedding for any entity, including those unseen during\ntraining. Experiments show that NodePiece performs competitively in node\nclassification, link prediction, and relation prediction tasks while retaining\nless than 10% of explicit nodes in a graph as anchors and often having 10x\nfewer parameters.",
          "link": "http://arxiv.org/abs/2106.12144",
          "publishedOn": "2021-06-24T01:51:44.101Z",
          "wordCount": 600,
          "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1\">Gautam Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1\">Babak Hassibi</a>",
          "description": "We consider estimation and control in linear time-varying dynamical systems\nfrom the perspective of regret minimization. Unlike most prior work in this\narea, we focus on the problem of designing causal estimators and controllers\nwhich compete against a clairvoyant noncausal policy, instead of the best\npolicy selected in hindsight from some fixed parametric class. We show that the\nregret-optimal estimator and regret-optimal controller can be derived in\nstate-space form using operator-theoretic techniques from robust control and\npresent tight,data-dependent bounds on the regret incurred by our algorithms in\nterms of the energy of the disturbances. Our results can be viewed as extending\ntraditional robust estimation and control, which focuses on minimizing\nworst-case cost, to minimizing worst-case regret. We propose regret-optimal\nanalogs of Model-Predictive Control (MPC) and the Extended KalmanFilter (EKF)\nfor systems with nonlinear dynamics and present numerical experiments which\nshow that our regret-optimal algorithms can significantly outperform standard\napproaches to estimation and control.",
          "link": "http://arxiv.org/abs/2106.12097",
          "publishedOn": "2021-06-24T01:51:44.085Z",
          "wordCount": 581,
          "title": "Regret-optimal Estimation and Control. (arXiv:2106.12097v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1\">Takuhiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>",
          "description": "Generative adversarial networks (GANs) have gained considerable attention\nowing to their ability to reproduce images. However, they can recreate training\nimages faithfully despite image degradation in the form of blur, noise, and\ncompression, generating similarly degraded images. To solve this problem, the\nrecently proposed noise robust GAN (NR-GAN) provides a partial solution by\ndemonstrating the ability to learn a clean image generator directly from noisy\nimages using a two-generator model comprising image and noise generators.\nHowever, its application is limited to noise, which is relatively easy to\ndecompose owing to its additive and reversible characteristics, and its\napplication to irreversible image degradation, in the form of blur,\ncompression, and combination of all, remains a challenge. To address these\nproblems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that\ncan learn a clean image generator directly from degraded images without\nknowledge of degradation parameters (e.g., blur kernel types, noise amounts, or\nquality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator\nmodel composed of image, blur-kernel, noise, and quality-factor generators.\nHowever, in contrast to NR-GAN, to address irreversible characteristics, we\nintroduce masking architectures adjusting degradation strength values in a\ndata-driven manner using bypasses before and after degradation. Furthermore, to\nsuppress uncertainty caused by the combination of blur, noise, and compression,\nwe introduce adaptive consistency losses imposing consistency between\nirreversible degradation processes according to the degradation strengths. We\ndemonstrate the effectiveness of BNCR-GAN through large-scale comparative\nstudies on CIFAR-10 and a generality analysis on FFHQ. In addition, we\ndemonstrate the applicability of BNCR-GAN in image restoration.",
          "link": "http://arxiv.org/abs/2003.07849",
          "publishedOn": "2021-06-24T01:51:44.080Z",
          "wordCount": 740,
          "title": "Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "Reflecting on the last few years, the biggest breakthroughs in deep\nreinforcement learning (RL) have been in the discrete action domain. Robotic\nmanipulation, however, is inherently a continuous control environment, but\nthese continuous control reinforcement learning algorithms often depend on\nactor-critic methods that are sample-inefficient and inherently difficult to\ntrain, due to the joint optimisation of the actor and critic. To that end, we\nexplore how we can bring the stability of discrete action RL algorithms to the\nrobot manipulation domain. We extend the recently released ARM algorithm, by\nreplacing the continuous next-best pose agent with a discrete next-best pose\nagent. Discretisation of rotation is trivial given its bounded nature, while\ntranslation is inherently unbounded, making discretisation difficult. We\nformulate the translation prediction as the voxel prediction problem by\ndiscretising the 3D space; however, voxelisation of a large workspace is memory\nintensive and would not work with a high density of voxels, crucial to\nobtaining the resolution needed for robotic manipulation. We therefore propose\nto apply this voxel prediction in a coarse-to-fine manner by gradually\nincreasing the resolution. In each step, we extract the highest valued voxel as\nthe predicted location, which is then used as the centre of the\nhigher-resolution voxelisation in the next step. This coarse-to-fine prediction\nis applied over several steps, giving a near-lossless prediction of the\ntranslation. We show that our new coarse-to-fine algorithm is able to\naccomplish RLBench tasks much more efficiently than the continuous control\nequivalent, and even train some real-world tasks, tabular rasa, in less than 7\nminutes, with only 3 demonstrations. Moreover, we show that by moving to a\nvoxel representation, we are able to easily incorporate observations from\nmultiple cameras.",
          "link": "http://arxiv.org/abs/2106.12534",
          "publishedOn": "2021-06-24T01:51:44.075Z",
          "wordCount": 736,
          "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sunny Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianlong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fang Chen</a>",
          "description": "Imitation learning aims to extract knowledge from human experts'\ndemonstrations or artificially created agents in order to replicate their\nbehaviors. Its success has been demonstrated in areas such as video games,\nautonomous driving, robotic simulations and object manipulation. However, this\nreplicating process could be problematic, such as the performance is highly\ndependent on the demonstration quality, and most trained agents are limited to\nperform well in task-specific environments. In this survey, we provide a\nsystematic review on imitation learning. We first introduce the background\nknowledge from development history and preliminaries, followed by presenting\ndifferent taxonomies within Imitation Learning and key milestones of the field.\nWe then detail challenges in learning strategies and present research\nopportunities with learning policy from suboptimal demonstration, voice\ninstructions and other associated optimization schemes.",
          "link": "http://arxiv.org/abs/2106.12177",
          "publishedOn": "2021-06-24T01:51:44.069Z",
          "wordCount": 557,
          "title": "Imitation Learning: Progress, Taxonomies and Opportunities. (arXiv:2106.12177v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1\">Jackson A. Killian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Arpita Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sanket Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "Multi-action restless multi-armed bandits (RMABs) are a powerful framework\nfor constrained resource allocation in which $N$ independent processes are\nmanaged. However, previous work only study the offline setting where problem\ndynamics are known. We address this restrictive assumption, designing the first\nalgorithms for learning good policies for Multi-action RMABs online using\ncombinations of Lagrangian relaxation and Q-learning. Our first approach,\nMAIQL, extends a method for Q-learning the Whittle index in binary-action RMABs\nto the multi-action setting. We derive a generalized update rule and\nconvergence proof and establish that, under standard assumptions, MAIQL\nconverges to the asymptotically optimal multi-action RMAB policy as\n$t\\rightarrow{}\\infty$. However, MAIQL relies on learning Q-functions and\nindexes on two timescales which leads to slow convergence and requires problem\nstructure to perform well. Thus, we design a second algorithm, LPQL, which\nlearns the well-performing and more general Lagrange policy for multi-action\nRMABs by learning to minimize the Lagrange bound through a variant of\nQ-learning. To ensure fast convergence, we take an approximation strategy that\nenables learning on a single timescale, then give a guarantee relating the\napproximation's precision to an upper bound of LPQL's return as\n$t\\rightarrow{}\\infty$. Finally, we show that our approaches always outperform\nbaselines across multiple settings, including one derived from real-world\nmedication adherence data.",
          "link": "http://arxiv.org/abs/2106.12024",
          "publishedOn": "2021-06-24T01:51:44.053Z",
          "wordCount": 665,
          "title": "Q-Learning Lagrange Policies for Multi-Action Restless Bandits. (arXiv:2106.12024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "The transition matrix, denoting the transition relationship from clean labels\nto noisy labels, is essential to build statistically consistent classifiers in\nlabel-noise learning. Existing methods for estimating the transition matrix\nrely heavily on estimating the noisy class posterior. However, the estimation\nerror for noisy class posterior could be large due to the randomness of label\nnoise, which would lead the transition matrix to be poorly estimated.\nTherefore, in this paper, we aim to solve this problem by exploiting the\ndivide-and-conquer paradigm. Specifically, we introduce an intermediate class\nto avoid directly estimating the noisy class posterior. By this intermediate\nclass, the original transition matrix can then be factorized into the product\nof two easy-to-estimate transition matrices. We term the proposed method the\ndual-T estimator. Both theoretical analyses and empirical results illustrate\nthe effectiveness of the dual-T estimator for estimating transition matrices,\nleading to better classification performances.",
          "link": "http://arxiv.org/abs/2006.07805",
          "publishedOn": "2021-06-24T01:51:44.048Z",
          "wordCount": 632,
          "title": "Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. (arXiv:2006.07805v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>",
          "description": "Recently, the study on learned iterative shrinkage thresholding algorithm\n(LISTA) has attracted increasing attentions. A large number of experiments as\nwell as some theories have proved the high efficiency of LISTA for solving\nsparse coding problems. However, existing LISTA methods are all serial\nconnection. To address this issue, we propose a novel extragradient based LISTA\n(ELISTA), which has a residual structure and theoretical guarantees. In\nparticular, our algorithm can also provide the interpretability for Res-Net to\na certain extent. From a theoretical perspective, we prove that our method\nattains linear convergence. In practice, extensive empirical results verify the\nadvantages of our method.",
          "link": "http://arxiv.org/abs/2106.11970",
          "publishedOn": "2021-06-24T01:51:44.043Z",
          "wordCount": 561,
          "title": "Learned Interpretable Residual Extragradient ISTA for Sparse Coding. (arXiv:2106.11970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>",
          "description": "Machine learning algorithms train models from patterns of input data and\ntarget outputs, with the goal of predicting correct outputs for unseen test\ninputs. Here we demonstrate a problem of machine learning in vital application\nareas such as medical informatics or patent law that consists of the inclusion\nof measurements on which target outputs are deterministically defined in the\nrepresentations of input data. This leads to perfect, but circular predictions\nbased on a machine reconstruction of the known target definition, but fails on\nreal-world data where the defining measurements may not or only incompletely be\navailable. We present a circularity test that shows, for given datasets and\nblack-box machine learning models, whether the target functional definition can\nbe reconstructed and has been used in training. We argue that a transfer of\nresearch results to real-world applications requires to avoid circularity by\nseparating measurements that define target outcomes from data representations\nin machine learning.",
          "link": "http://arxiv.org/abs/2106.12417",
          "publishedOn": "2021-06-24T01:51:44.037Z",
          "wordCount": 593,
          "title": "False perfection in machine prediction: Detecting and assessing circularity problems in machine learning. (arXiv:2106.12417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Weituo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1\">Gregory Spell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>",
          "description": "The outbreak of COVID-19 Disease due to the novel coronavirus has caused a\nshortage of medical resources. To aid and accelerate the diagnosis process,\nautomatic diagnosis of COVID-19 via deep learning models has recently been\nexplored by researchers across the world. While different data-driven deep\nlearning models have been developed to mitigate the diagnosis of COVID-19, the\ndata itself is still scarce due to patient privacy concerns. Federated Learning\n(FL) is a natural solution because it allows different organizations to\ncooperatively learn an effective deep learning model without sharing raw data.\nHowever, recent studies show that FL still lacks privacy protection and may\ncause data leakage. We investigate this challenging problem by proposing a\nsimple yet effective algorithm, named \\textbf{F}ederated \\textbf{L}earning\n\\textbf{o}n Medical Datasets using \\textbf{P}artial Networks (FLOP), that\nshares only a partial model between the server and clients. Extensive\nexperiments on benchmark data and real-world healthcare tasks show that our\napproach achieves comparable or better performance while reducing the privacy\nand security risks. Of particular interest, we conduct experiments on the\nCOVID-19 dataset and find that our FLOP algorithm can allow different hospitals\nto collaboratively and effectively train a partially shared model without\nsharing local patients' data.",
          "link": "http://arxiv.org/abs/2102.05218",
          "publishedOn": "2021-06-24T01:51:44.032Z",
          "wordCount": 722,
          "title": "FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladu_A/0/1/0/all/0/1\">Adrian Vladu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>",
          "description": "The increasing computational requirements of deep neural networks (DNNs) have\nled to significant interest in obtaining DNN models that are sparse, yet\naccurate. Recent work has investigated the even harder case of sparse training,\nwhere the DNN weights are, for as much as possible, already sparse to reduce\ncomputational costs during training.\n\nExisting sparse training methods are mainly empirical and often have lower\naccuracy relative to the dense baseline. In this paper, we present a general\napproach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs,\ndemonstrate convergence for a variant of the algorithm, and show that AC/DC\noutperforms existing sparse training methods in accuracy at similar\ncomputational budgets; at high sparsity levels, AC/DC even outperforms existing\nmethods that rely on accurate pre-trained dense models. An important property\nof AC/DC is that it allows co-training of dense and sparse models, yielding\naccurate sparse-dense model pairs at the end of the training process. This is\nuseful in practice, where compressed variants may be desirable for deployment\nin resource-constrained settings without re-doing the entire training flow, and\nalso provides us with insights into the accuracy gap between dense and\ncompressed models.",
          "link": "http://arxiv.org/abs/2106.12379",
          "publishedOn": "2021-06-24T01:51:44.026Z",
          "wordCount": 621,
          "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks. (arXiv:2106.12379v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chzhen_E/0/1/0/all/0/1\">Evgenii Chzhen</a> (LMO, CELESTE), <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_C/0/1/0/all/0/1\">Christophe Giraud</a> (LMO, CELESTE), <a href=\"http://arxiv.org/find/cs/1/au:+Stoltz_G/0/1/0/all/0/1\">Gilles Stoltz</a> (LMO, CELESTE)",
          "description": "We provide a setting and a general approach to fair online learning with\nstochastic sensitive and non-sensitive contexts. The setting is a repeated game\nbetween the Player and Nature, where at each stage both pick actions based on\nthe contexts. Inspired by the notion of unawareness, we assume that the Player\ncan only access the non-sensitive context before making a decision, while we\ndiscuss both cases of Nature accessing the sensitive contexts and Nature\nunaware of the sensitive contexts. Adapting Blackwell's approachability theory\nto handle the case of an unknown contexts' distribution, we provide a general\nnecessary and sufficient condition for learning objectives to be compatible\nwith some fairness constraints. This condition is instantiated on (group-wise)\nno-regret and (group-wise) calibration objectives, and on demographic parity as\nan additional constraint. When the objective is not compatible with the\nconstraint, the provided framework permits to characterise the optimal\ntrade-off between the two.",
          "link": "http://arxiv.org/abs/2106.12242",
          "publishedOn": "2021-06-24T01:51:44.010Z",
          "wordCount": 607,
          "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability. (arXiv:2106.12242v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09056",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Padmanabha_G/0/1/0/all/0/1\">Govinda Anantha Padmanabha</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1\">Nicholas Zabaras</a>",
          "description": "Fine-scale simulation of complex systems governed by multiscale partial\ndifferential equations (PDEs) is computationally expensive and various\nmultiscale methods have been developed for addressing such problems. In\naddition, it is challenging to develop accurate surrogate and uncertainty\nquantification models for high-dimensional problems governed by stochastic\nmultiscale PDEs using limited training data. In this work to address these\nchallenges, we introduce a novel hybrid deep-learning and multiscale approach\nfor stochastic multiscale PDEs with limited training data. For demonstration\npurposes, we focus on a porous media flow problem. We use an image-to-image\nsupervised deep learning model to learn the mapping between the input\npermeability field and the multiscale basis functions. We introduce a Bayesian\napproach to this hybrid framework to allow us to perform uncertainty\nquantification and propagation tasks. The performance of this hybrid approach\nis evaluated with varying intrinsic dimensionality of the permeability field.\nNumerical results indicate that the hybrid network can efficiently predict well\nfor high-dimensional inputs.",
          "link": "http://arxiv.org/abs/2103.09056",
          "publishedOn": "2021-06-24T01:51:44.005Z",
          "wordCount": 629,
          "title": "A Bayesian Multiscale Deep Learning Framework for Flows in Random Media. (arXiv:2103.09056v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in an open-world setting. However, existing OOD detection\nsolutions can be brittle in the open world, facing various types of adversarial\nOOD inputs. While methods leveraging auxiliary OOD data have emerged, our\nanalysis on illuminative examples reveals a key insight that the majority of\nauxiliary OOD examples may not meaningfully improve or even hurt the decision\nboundary of the OOD detector, which is also observed in empirical results on\nreal data. In this paper, we provide a theoretically motivated method,\nAdversarial Training with informative Outlier Mining (ATOM), which improves the\nrobustness of OOD detection. We show that, by mining informative auxiliary OOD\ndata, one can significantly improve OOD detection performance, and somewhat\nsurprisingly, generalize to unseen adversarial attacks. ATOM achieves\nstate-of-the-art performance under a broad family of classic and adversarial\nOOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,\nATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,\nsurpassing the previous best baseline by a large margin.",
          "link": "http://arxiv.org/abs/2006.15207",
          "publishedOn": "2021-06-24T01:51:43.999Z",
          "wordCount": 660,
          "title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harilal_N/0/1/0/all/0/1\">Nidhin Harilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_U/0/1/0/all/0/1\">Udit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Auroop R. Ganguly</a>",
          "description": "Advances in neural architecture search, as well as explainability and\ninterpretability of connectionist architectures, have been reported in the\nrecent literature. However, our understanding of how to design Bayesian Deep\nLearning (BDL) hyperparameters, specifically, the depth, width and ensemble\nsize, for robust function mapping with uncertainty quantification, is still\nemerging. This paper attempts to further our understanding by mapping Bayesian\nconnectionist representations to polynomials of different orders with varying\nnoise types and ratios. We examine the noise-contaminated polynomials to search\nfor the combination of hyperparameters that can extract the underlying\npolynomial signals while quantifying uncertainties based on the noise\nattributes. Specifically, we attempt to study the question that an appropriate\nneural architecture and ensemble configuration can be found to detect a signal\nof any n-th order polynomial contaminated with noise having different\ndistributions and signal-to-noise (SNR) ratios and varying noise attributes.\nOur results suggest the possible existence of an optimal network depth as well\nas an optimal number of ensembles for prediction skills and uncertainty\nquantification, respectively. However, optimality is not discernible for width,\neven though the performance gain reduces with increasing width at high values\nof width. Our experiments and insights can be directional to understand\ntheoretical properties of BDL representations and to design practical\nsolutions.",
          "link": "http://arxiv.org/abs/2106.12532",
          "publishedOn": "2021-06-24T01:51:43.993Z",
          "wordCount": 650,
          "title": "Bayesian Deep Learning Hyperparameter Search for Robust Function Mapping to Polynomials with Noise. (arXiv:2106.12532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.",
          "link": "http://arxiv.org/abs/2106.12183",
          "publishedOn": "2021-06-24T01:51:43.979Z",
          "wordCount": 707,
          "title": "A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14152",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiujia Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>",
          "description": "End-to-end models with auto-regressive decoders have shown impressive results\nfor automatic speech recognition (ASR). These models formulate the\nsequence-level probability as a product of the conditional probabilities of all\nindividual tokens given their histories. However, the performance of locally\nnormalised models can be sub-optimal because of factors such as exposure bias.\nConsequently, the model distribution differs from the underlying data\ndistribution. In this paper, the residual energy-based model (R-EBM) is\nproposed to complement the auto-regressive ASR model to close the gap between\nthe two distributions. Meanwhile, R-EBMs can also be regarded as\nutterance-level confidence estimators, which may benefit many downstream tasks.\nExperiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word\nerror rates (WERs) by 8.2%/6.7% while improving areas under precision-recall\ncurves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.\nFurthermore, on a state-of-the-art model using self-supervised learning\n(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence\nestimation performance.",
          "link": "http://arxiv.org/abs/2103.14152",
          "publishedOn": "2021-06-24T01:51:43.973Z",
          "wordCount": 629,
          "title": "Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_D/0/1/0/all/0/1\">Deeparnab Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negahbani_M/0/1/0/all/0/1\">Maryam Negahbani</a>",
          "description": "We study data clustering problems with $\\ell_p$-norm objectives (e.g.\n$k$-Median and $k$-Means) in the context of individual fairness. The dataset\nconsists of $n$ points, and we want to find $k$ centers such that (a) the\nobjective is minimized, while (b) respecting the individual fairness constraint\nthat every point $v$ has a center within a distance at most $r(v)$, where\n$r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz\n[FORC 2020] introduced this concept and designed a clustering algorithm with\nprovable (approximate) fairness and objective guarantees for the $\\ell_\\infty$\nor $k$-Center objective. Mahabadi and Vakilian [ICML 2020] revisited this\nproblem to give a local-search algorithm for all $\\ell_p$-norms. Empirically,\ntheir algorithms outperform Jung et. al.'s by a large margin in terms of cost\n(for $k$-Median and $k$-Means), but they incur a reasonable loss in fairness.\nIn this paper, our main contribution is to use Linear Programming (LP)\ntechniques to obtain better algorithms for this problem, both in theory and in\npractice. We prove that by modifying known LP rounding techniques, one gets a\nworst-case guarantee on the objective which is much better than in MV20, and\nempirically, this objective is extremely close to the optimal. Furthermore, our\ntheoretical fairness guarantees are comparable with MV20 in theory, and\nempirically, we obtain noticeably fairer solutions. Although solving the LP\n{\\em exactly} might be prohibitive, we demonstrate that in practice, a simple\nsparsification technique drastically improves the run-time of our algorithm.",
          "link": "http://arxiv.org/abs/2106.12150",
          "publishedOn": "2021-06-24T01:51:43.969Z",
          "wordCount": 670,
          "title": "Better Algorithms for Individually Fair $k$-Clustering. (arXiv:2106.12150v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09461",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1\">Constantine Caramanis</a>",
          "description": "The goal of compressed sensing is to estimate a high dimensional vector from\nan underdetermined system of noisy linear equations. In analogy to classical\ncompressed sensing, here we assume a generative model as a prior, that is, we\nassume the vector is represented by a deep generative model $G: \\mathbb{R}^k\n\\rightarrow \\mathbb{R}^n$. Classical recovery approaches such as empirical risk\nminimization (ERM) are guaranteed to succeed when the measurement matrix is\nsub-Gaussian. However, when the measurement matrix and measurements are\nheavy-tailed or have outliers, recovery may fail dramatically. In this paper we\npropose an algorithm inspired by the Median-of-Means (MOM). Our algorithm\nguarantees recovery for heavy-tailed data, even in the presence of outliers.\nTheoretically, our results show our novel MOM-based algorithm enjoys the same\nsample complexity guarantees as ERM under sub-Gaussian assumptions. Our\nexperiments validate both aspects of our claims: other algorithms are indeed\nfragile and fail under heavy-tailed and/or corrupted data, while our approach\nexhibits the predicted robustness.",
          "link": "http://arxiv.org/abs/2006.09461",
          "publishedOn": "2021-06-24T01:51:43.962Z",
          "wordCount": 621,
          "title": "Robust Compressed Sensing using Generative Models. (arXiv:2006.09461v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_G/0/1/0/all/0/1\">Gang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>",
          "description": "Being able to efficiently and accurately select the top-$k$ elements with\ndifferential privacy is an integral component of various private data analysis\ntasks. In this paper, we present the oneshot Laplace mechanism, which\ngeneralizes the well-known Report Noisy Max mechanism to reporting noisy\ntop-$k$ elements. We show that the oneshot Laplace mechanism with a noise level\nof $\\widetilde{O}(\\sqrt{k}/\\eps)$ is approximately differentially private.\nCompared to the previous peeling approach of running Report Noisy Max $k$\ntimes, the oneshot Laplace mechanism only adds noises and computes the top $k$\nelements once, hence much more efficient for large $k$. In addition, our proof\nof privacy relies on a novel coupling technique that bypasses the use of\ncomposition theorems. Finally, we present a novel application of efficient\ntop-$k$ selection in the classical problem of ranking from pairwise\ncomparisons.",
          "link": "http://arxiv.org/abs/2105.08233",
          "publishedOn": "2021-06-24T01:51:43.957Z",
          "wordCount": 601,
          "title": "Oneshot Differentially Private Top-k Selection. (arXiv:2105.08233v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kshitij Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1\">Raunak Manekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">David Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1\">Felix Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>",
          "description": "Several deep learning methods for phase retrieval exist, but most of them\nfail on realistic data without precise support information. We propose a novel\nmethod based on single-instance deep generative prior that works well on\ncomplex-valued crystal data.",
          "link": "http://arxiv.org/abs/2106.04812",
          "publishedOn": "2021-06-24T01:51:43.951Z",
          "wordCount": 495,
          "title": "Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1\">Alex Hernandez-Garcia</a>",
          "description": "The renaissance of artificial neural networks was catalysed by the success of\nclassification models, tagged by the community with the broader term supervised\nlearning. The extraordinary results gave rise to a hype loaded with ambitious\npromises and overstatements. Soon the community realised that the success owed\nmuch to the availability of thousands of labelled examples and supervised\nlearning went, for many, from glory to shame: Some criticised deep learning as\na whole and others proclaimed that the way forward had to be alternatives to\nsupervised learning: predictive, unsupervised, semi-supervised and, more\nrecently, self-supervised learning. However, all these seem brand names, rather\nthan actual categories of a theoretically grounded taxonomy. Moreover, the call\nto banish supervised learning was motivated by the questionable claim that\nhumans learn with little or no supervision and are capable of robust\nout-of-distribution generalisation. Here, we review insights about learning and\nsupervision in nature, revisit the notion that learning and generalisation are\nnot possible without supervision or inductive biases and argue that we will\nmake better progress if we just call it by its name.",
          "link": "http://arxiv.org/abs/2012.02526",
          "publishedOn": "2021-06-24T01:51:43.937Z",
          "wordCount": 666,
          "title": "Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>",
          "description": "Automatic processing of bibliographic data becomes very important in digital\nlibraries, data science and machine learning due to its importance in keeping\npace with the significant increase of published papers every year from one side\nand to the inherent challenges from the other side. This processing has several\naspects including but not limited to I) Automatic extraction of references from\nPDF documents, II) Building an accurate citation graph, III) Author name\ndisambiguation, etc. Bibliographic data is heterogeneous by nature and occurs\nin both structured (e.g. citation graph) and unstructured (e.g. publications)\nformats. Therefore, it requires data science and machine learning techniques to\nbe processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on\nBibliographic Data Analysis and Processing.",
          "link": "http://arxiv.org/abs/2106.12320",
          "publishedOn": "2021-06-24T01:51:43.932Z",
          "wordCount": 572,
          "title": "BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borrageiro_G/0/1/0/all/0/1\">Gabriel Borrageiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firoozye_N/0/1/0/all/0/1\">Nick Firoozye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1\">Paolo Barucca</a>",
          "description": "We investigate the benefits of feature selection, nonlinear modelling and\nonline learning when forecasting in financial time series. We consider the\nsequential and continual learning sub-genres of online learning. The\nexperiments we conduct show that there is a benefit to online transfer\nlearning, in the form of radial basis function networks, beyond the sequential\nupdating of recursive least-squares models. We show that the radial basis\nfunction networks, which make use of clustering algorithms to construct a\nkernel Gram matrix, are more beneficial than treating each training vector as\nseparate basis functions, as occurs with kernel Ridge regression. We\ndemonstrate quantitative procedures to determine the very structure of the\nradial basis function networks. Finally, we conduct experiments on the log\nreturns of financial time series and show that the online learning models,\nparticularly the radial basis function networks, are able to outperform a\nrandom walk baseline, whereas the offline learning models struggle to do so.",
          "link": "http://arxiv.org/abs/2103.08414",
          "publishedOn": "2021-06-24T01:51:43.926Z",
          "wordCount": 624,
          "title": "Online Learning with Radial Basis Function Networks. (arXiv:2103.08414v2 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being ``fair'' with respect to Asians may\nrequire being ``unfair'' with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-06-24T01:51:43.921Z",
          "wordCount": 653,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.05961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shetty_M/0/1/0/all/0/1\">Manish Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1\">Chetan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1\">Nikitha Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1\">Nachiappan Nagappan</a>",
          "description": "The move from boxed products to services and the widespread adoption of cloud\ncomputing has had a huge impact on the software development life cycle and\nDevOps processes. Particularly, incident management has become critical for\ndeveloping and operating large-scale services. Prior work on incident\nmanagement has heavily focused on the challenges with incident triaging and\nde-duplication. In this work, we address the fundamental problem of structured\nknowledge extraction from service incidents. We have built SoftNER, a framework\nfor mining Knowledge Graphs from incident reports. First, we build a novel\nmulti-task learning based BiLSTM-CRF model which leverages not just the\nsemantic context but also the data-types for extracting factual information in\nthe form of named entities. Next, we present an approach to mine relations\nbetween the named entities for automatically constructing knowledge graphs. We\nhave deployed SoftNER at Microsoft, a major cloud service provider and have\nevaluated it on more than 2 months of cloud incidents. We show that the\nunsupervised machine learning pipeline has a high precision of 0.96. Our\nmulti-task learning based deep learning model also outperforms the\nstate-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,\nwe are able to build accurate models for applications such as incident triaging\nand recommending entities based on their relevance to incident titles.",
          "link": "http://arxiv.org/abs/2101.05961",
          "publishedOn": "2021-06-24T01:51:43.905Z",
          "wordCount": 691,
          "title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents. (arXiv:2101.05961v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Werling_K/0/1/0/all/0/1\">Keenon Werling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omens_D/0/1/0/all/0/1\">Dalton Omens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Exarchos_I/0/1/0/all/0/1\">Ioannis Exarchos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>",
          "description": "We present a fast and feature-complete differentiable physics engine, Nimble\n(nimblephysics.org), that supports Lagrangian dynamics and hard contact\nconstraints for articulated rigid body simulation. Our differentiable physics\nengine offers a complete set of features that are typically only available in\nnon-differentiable physics simulators commonly used by robotics applications.\nWe solve contact constraints precisely using linear complementarity problems\n(LCPs). We present efficient and novel analytical gradients through the LCP\nformulation of inelastic contact that exploit the sparsity of the LCP solution.\nWe support complex contact geometry, and gradients approximating\ncontinuous-time elastic collision. We also introduce a novel method to compute\ncomplementarity-aware gradients that help downstream optimization tasks avoid\nstalling in saddle points. We show that an implementation of this combination\nin an existing physics engine (DART) is capable of a 87x single-core speedup\nover finite-differencing in computing analytical Jacobians for a single\ntimestep, while preserving all the expressiveness of original DART.",
          "link": "http://arxiv.org/abs/2103.16021",
          "publishedOn": "2021-06-24T01:51:43.900Z",
          "wordCount": 643,
          "title": "Fast and Feature-Complete Differentiable Physics for Articulated Rigid Bodies with Contact. (arXiv:2103.16021v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08174",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1\">Davide Viviano</a>",
          "description": "This paper discusses experimental design to estimate welfare-maximizing\npolicies. We consider a setting where units are organized into large, finitely\nmany independent clusters and interact over unobserved dimensions within each\ncluster. The contribution of this paper is two-fold. First, we construct a test\nfor whether a welfare-improving treatment configuration exists and hence worth\nlearning by conducting a larger scale experiment. Second, we introduce an\nadaptive randomization procedure to estimate welfare-maximizing individual\ntreatment allocation rules valid under unobserved interference. We derive\nasymptotic properties of the marginal effects estimators and finite-sample\nregret guarantees of the policy. Finally, we illustrate the method's advantage\nin simulations calibrated to an existing experiment on information diffusion.",
          "link": "http://arxiv.org/abs/2011.08174",
          "publishedOn": "2021-06-24T01:51:43.894Z",
          "wordCount": 574,
          "title": "Policy choice in experiments with unknown interference. (arXiv:2011.08174v4 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05163",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mestav_K/0/1/0/all/0/1\">Kursat Rasim Mestav</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_L/0/1/0/all/0/1\">Lang Tong</a>",
          "description": "A deep learning approach is proposed to detect data and system anomalies\nusing high-resolution continuous point-on-wave (CPOW) or phasor measurements.\nBoth the anomaly and anomaly-free measurement models are assumed to have\nunknown temporal dependencies and probability distributions. Historical\ntraining samples are assumed for the anomaly-free model, while no training\nsamples are available for the anomaly measurements. By transforming the\nanomaly-free observations into uniform independent and identically distributed\nsequences via a generative adversarial network, the proposed approach deploys a\nuniformity test for anomaly detection at the sensor level. A distributed\ndetection scheme that combines sensor level detections at the control center is\nalso proposed that combines local detections to form more reliable detections.\nNumerical results demonstrate significant improvement over the state-of-the-art\nsolutions for various bad-data cases using real and synthetic CPOW and PMU data\nsets.",
          "link": "http://arxiv.org/abs/2012.05163",
          "publishedOn": "2021-06-24T01:51:43.814Z",
          "wordCount": 613,
          "title": "A Deep Learning Approach to Anomaly Sequence Detection for High-Resolution Monitoring of Power Systems. (arXiv:2012.05163v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1\">Sujay Khandagale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>",
          "description": "As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. Despite the widespread use of explainability\ntechniques, evaluating and comparing different feature attribution methods\nremains challenging: evaluations ideally require human studies, and empirical\nevaluation metrics are often computationally prohibitive on real-world\ndatasets. In this work, we address this issue by releasing XAI-Bench: a suite\nof synthetic datasets along with a library for benchmarking feature attribution\nalgorithms. Unlike real-world datasets, synthetic datasets allow the efficient\ncomputation of conditional expected values that are needed to evaluate\nground-truth Shapley values and other metrics. The synthetic datasets we\nrelease offer a wide variety of parameters that can be configured to simulate\nreal-world data. We demonstrate the power of our library by benchmarking\npopular explainability techniques across several evaluation metrics and\nidentifying failure modes for popular explainers. The efficiency of our library\nwill help bring new explainability methods from development to deployment.",
          "link": "http://arxiv.org/abs/2106.12543",
          "publishedOn": "2021-06-24T01:51:43.808Z",
          "wordCount": 600,
          "title": "Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>",
          "description": "We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.",
          "link": "http://arxiv.org/abs/2106.10928",
          "publishedOn": "2021-06-24T01:51:43.802Z",
          "wordCount": 640,
          "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNNs. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that the token-mixing MLP is a variant of the depthwise convolution\nwith a global reception field and spatial-specific configuration. But the\nglobal reception field and the spatial-specific property make token-mixing MLP\nprone to over-fitting. In this paper, we propose a novel pure MLP architecture,\nspatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only\ncontains channel-mixing MLP. We utilize a spatial-shift operation for\ncommunications between patches. It has a local reception field and is\nspatial-agnostic. It is parameter-free and efficient for computation. The\nproposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when\ntraining on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent\nperformance as ViT on ImageNet-1K dataset with considerably simpler\narchitecture and fewer FLOPs and parameters.",
          "link": "http://arxiv.org/abs/2106.07477",
          "publishedOn": "2021-06-24T01:51:43.786Z",
          "wordCount": 678,
          "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>",
          "description": "Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.",
          "link": "http://arxiv.org/abs/2106.12070",
          "publishedOn": "2021-06-24T01:51:43.781Z",
          "wordCount": 635,
          "title": "Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image--image) and CUB (image--text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-06-24T01:51:43.776Z",
          "wordCount": 589,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12147",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1\">Hyung Ju Hwang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Son_H/0/1/0/all/0/1\">Hwijae Son</a>",
          "description": "In this paper, we propose a novel conservative formulation for solving\nkinetic equations via neural networks. More precisely, we formulate the\nlearning problem as a constrained optimization problem with constraints that\nrepresent the physical conservation laws. The constraints are relaxed toward\nthe residual loss function by the Lagrangian duality. By imposing physical\nconservation properties of the solution as constraints of the learning problem,\nwe demonstrate far more accurate approximations of the solutions in terms of\nerrors and the conservation laws, for the kinetic Fokker-Planck equation and\nthe homogeneous Boltzmann equation.",
          "link": "http://arxiv.org/abs/2106.12147",
          "publishedOn": "2021-06-24T01:51:43.769Z",
          "wordCount": 528,
          "title": "Lagrangian dual framework for conservative neural network solutions of kinetic equations. (arXiv:2106.12147v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliee_H/0/1/0/all/0/1\">Hananeh Aliee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1\">Fabian J. Theis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Spurred by tremendous success in pattern matching and prediction tasks,\nresearchers increasingly resort to machine learning to aid original scientific\ndiscovery. Given large amounts of observational data about a system, can we\nuncover the rules that govern its evolution? Solving this task holds the great\npromise of fully understanding the causal interactions and being able to make\nreliable predictions about the system's behavior under interventions. We take a\nstep towards answering this question for time-series data generated from\nsystems of ordinary differential equations (ODEs). While the governing ODEs\nmight not be identifiable from data alone, we show that combining simple\nregularization schemes with flexible neural ODEs can robustly recover the\ndynamics and causal structures from time-series data. Our results on a variety\nof (non)-linear first and second order systems as well as real data validate\nour method. We conclude by showing that we can also make accurate predictions\nunder interventions on variables or the system itself.",
          "link": "http://arxiv.org/abs/2106.12430",
          "publishedOn": "2021-06-24T01:51:43.754Z",
          "wordCount": 589,
          "title": "Beyond Predictions in Neural ODEs: Identification and Interventions. (arXiv:2106.12430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1\">Israa Odeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1\">Mouhammd Alkasassbeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1\">Mohammad Alauthman</a>",
          "description": "Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in\ndiabetic patients. DR is a microvascular disease that affects the eye retina,\nwhich causes vessel blockage and therefore cuts the main source of nutrition\nfor the retina tissues. Treatment for this visual disorder is most effective\nwhen it is detected in its earliest stages, as severe DR can result in\nirreversible blindness. Nonetheless, DR identification requires the expertise\nof Ophthalmologists which is often expensive and time-consuming. Therefore,\nautomatic detection systems were introduced aiming to facilitate the\nidentification process, making it available globally in a time and\ncost-efficient manner. However, due to the limited reliable datasets and\nmedical records for this particular eye disease, the obtained predictions\naccuracies were relatively unsatisfying for eye specialists to rely on them as\ndiagnostic systems. Thus, we explored an ensemble-based learning strategy,\nmerging a substantial selection of well-known classification algorithms in one\nsophisticated diagnostic model. The proposed framework achieved the highest\naccuracy rates among all other common classification algorithms in the area. 4\nsubdatasets were generated to contain the top 5 and top 10 features of the\nMessidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies\nof 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original\ndataset respectively. The results imply the impressive performance of the\nsubdataset, which significantly conduces to a less complex classification\nprocess",
          "link": "http://arxiv.org/abs/2106.12545",
          "publishedOn": "2021-06-24T01:51:43.748Z",
          "wordCount": 669,
          "title": "Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12045",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1\">Bipin Kumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1\">Dev Niyogi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1\">Suryachandra Rao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gill_S/0/1/0/all/0/1\">Sukhpal Singh Gill</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chattopadhyay_R/0/1/0/all/0/1\">Rajib Chattopadhyay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nanjundiah_R/0/1/0/all/0/1\">Ravi S Nanjundiah</a>",
          "description": "The formation of precipitation in state-of-the-art weather and climate models\nis an important process. The understanding of its relationship with other\nvariables can lead to endless benefits, particularly for the world's monsoon\nregions dependent on rainfall as a support for livelihood. Various factors play\na crucial role in the formation of rainfall, and those physical processes are\nleading to significant biases in the operational weather forecasts. We use the\nUNET architecture of a deep convolutional neural network with residual learning\nas a proof of concept to learn global data-driven models of precipitation. The\nmodels are trained on reanalysis datasets projected on the cubed-sphere\nprojection to minimize errors due to spherical distortion. The results are\ncompared with the operational dynamical model used by the India Meteorological\nDepartment. The theoretical deep learning-based model shows doubling of the\ngrid point, as well as area averaged skill measured in Pearson correlation\ncoefficients relative to operational system. This study is a proof-of-concept\nshowing that residual learning-based UNET can unravel physical relationships to\ntarget precipitation, and those physical constraints can be used in the\ndynamical operational models towards improved precipitation forecasts. Our\nresults pave the way for the development of online, hybrid models in the\nfuture.",
          "link": "http://arxiv.org/abs/2106.12045",
          "publishedOn": "2021-06-24T01:51:43.743Z",
          "wordCount": 650,
          "title": "Deep learning for improved global precipitation in numerical weather prediction systems. (arXiv:2106.12045v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengchun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kettimuthu_R/0/1/0/all/0/1\">Rajkumar Kettimuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1\">Michael E. Papka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>",
          "description": "Supercomputer FCFS-based scheduling policies result in many transient idle\nnodes, a phenomenon that is only partially alleviated by backfill scheduling\nmethods that promote small jobs to run before large jobs. Here we describe how\nto realize a novel use for these otherwise wasted resources, namely, deep\nneural network (DNN) training. This important workload is easily organized as\nmany small fragments that can be configured dynamically to fit essentially any\nnode*time hole in a supercomputer's schedule. We describe how the task of\nrescaling suitable DNN training tasks to fit dynamically changing holes can be\nformulated as a deterministic mixed integer linear programming (MILP)-based\nresource allocation algorithm, and show that this MILP problem can be solved\nefficiently at run time. We show further how this MILP problem can be adapted\nto optimize for administrator- or user-defined metrics. We validate our method\nwith supercomputer scheduler logs and different DNN training scenarios, and\ndemonstrate efficiencies of up to 93% compared with running the same training\ntasks on dedicated nodes. Our method thus enables substantial supercomputer\nresources to be allocated to DNN training with no impact on other applications.",
          "link": "http://arxiv.org/abs/2106.12091",
          "publishedOn": "2021-06-24T01:51:43.736Z",
          "wordCount": 629,
          "title": "BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes. (arXiv:2106.12091v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1\">Reese Timothy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Self-training based unsupervised domain adaptation (UDA) has shown great\npotential to address the problem of domain shift, when applying a trained deep\nlearning model in a source domain to unlabeled target domains. However, while\nthe self-training UDA has demonstrated its effectiveness on discriminative\ntasks, such as classification and segmentation, via the reliable pseudo-label\nselection based on the softmax discrete histogram, the self-training UDA for\ngenerative tasks, such as image synthesis, is not fully investigated. In this\nwork, we propose a novel generative self-training (GST) UDA framework with\ncontinuous value prediction and regression objective for cross-domain image\nsynthesis. Specifically, we propose to filter the pseudo-label with an\nuncertainty mask, and quantify the predictive confidence of generated images\nwith practical variational Bayes learning. The fast test-time adaptation is\nachieved by a round-based alternative optimization scheme. We validated our\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\nproblem, where datasets in the source and target domains were acquired from\ndifferent scanners or centers. Extensive validations were carried out to verify\nour framework against popular adversarial training UDA methods. Results show\nthat our GST, with tagged MRI of test subjects in new target domains, improved\nthe synthesis quality by a large margin, compared with the adversarial training\nUDA methods.",
          "link": "http://arxiv.org/abs/2106.12499",
          "publishedOn": "2021-06-24T01:51:43.719Z",
          "wordCount": 671,
          "title": "Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1\">Serban Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>",
          "description": "Multi-source unsupervised domain adaptation (MUDA) is a recently explored\nlearning framework, where the goal is to address the challenge of labeled data\nscarcity in a target domain via transferring knowledge from multiple source\ndomains with annotated data. Since the source data is distributed, the privacy\nof source domains' data can be a natural concern. We benefit from the idea of\ndomain alignment in an embedding space to address the privacy concern for MUDA.\nOur method is based on aligning the sources and target distributions indirectly\nvia internally learned distributions, without communicating data samples\nbetween domains. We justify our approach theoretically and perform extensive\nexperiments to demonstrate that our method is effective and compares favorably\nagainst existing methods.",
          "link": "http://arxiv.org/abs/2106.12124",
          "publishedOn": "2021-06-24T01:51:43.713Z",
          "wordCount": 538,
          "title": "Secure Domain Adaptation with Multiple Sources. (arXiv:2106.12124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1\">Arthur Drichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faerber_N/0/1/0/all/0/1\">Nils Faerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1\">Ulrike Meyer</a>",
          "description": "Numerous malware families rely on domain generation algorithms (DGAs) to\nestablish a connection to their command and control (C2) server. Counteracting\nDGAs, several machine learning classifiers have been proposed enabling the\nidentification of the DGA that generated a specific domain name and thus\ntriggering targeted remediation measures. However, the proposed\nstate-of-the-art classifiers are based on deep learning models. The black box\nnature of these makes it difficult to evaluate their reasoning. The resulting\nlack of confidence makes the utilization of such models impracticable. In this\npaper, we propose EXPLAIN, a feature-based and contextless DGA multiclass\nclassifier. We comparatively evaluate several combinations of feature sets and\nhyperparameters for our approach against several state-of-the-art classifiers\nin a unified setting on the same real-world data. Our classifier achieves\ncompetitive results, is real-time capable, and its predictions are easier to\ntrace back to features than the predictions made by the DGA multiclass\nclassifiers proposed in related work.",
          "link": "http://arxiv.org/abs/2106.12336",
          "publishedOn": "2021-06-24T01:51:43.669Z",
          "wordCount": 616,
          "title": "First Step Towards EXPLAINable DGA Multiclass Classification. (arXiv:2106.12336v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1\">Marissa Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_K/0/1/0/all/0/1\">Kion Fallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozell_C/0/1/0/all/0/1\">Christopher Rozell</a>",
          "description": "Many machine learning techniques incorporate identity-preserving\ntransformations into their models to generalize their performance to previously\nunseen data. These transformations are typically selected from a set of\nfunctions that are known to maintain the identity of an input when applied\n(e.g., rotation, translation, flipping, and scaling). However, there are many\nnatural variations that cannot be labeled for supervision or defined through\nexamination of the data. As suggested by the manifold hypothesis, many of these\nnatural variations live on or near a low-dimensional, nonlinear manifold.\nSeveral techniques represent manifold variations through a set of learned Lie\ngroup operators that define directions of motion on the manifold. However\ntheses approaches are limited because they require transformation labels when\ntraining their models and they lack a method for determining which regions of\nthe manifold are appropriate for applying each specific operator. We address\nthese limitations by introducing a learning strategy that does not require\ntransformation labels and developing a method that learns the local regions\nwhere each operator is likely to be used while preserving the identity of\ninputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to\nlearn identity-preserving transformations on multi-class datasets.\nAdditionally, we train on CelebA to showcase our model's ability to learn\nsemantically meaningful transformations on complex datasets in an unsupervised\nmanner.",
          "link": "http://arxiv.org/abs/2106.12096",
          "publishedOn": "2021-06-24T01:51:43.652Z",
          "wordCount": 644,
          "title": "Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>",
          "description": "We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.",
          "link": "http://arxiv.org/abs/2106.12423",
          "publishedOn": "2021-06-24T01:51:43.645Z",
          "wordCount": 583,
          "title": "Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Information theory is of importance to machine learning, but the notation for\ninformation-theoretic quantities is sometimes opaque. The right notation can\nconvey valuable intuitions and concisely express new ideas. We propose such a\nnotation for machine learning users and expand it to include\ninformation-theoretic quantities between events (outcomes) and random\nvariables. We apply this notation to a popular information-theoretic\nacquisition function in Bayesian active learning which selects the most\ninformative (unlabelled) samples to be labelled by an expert. We demonstrate\nthe value of our notation when extending the acquisition function to the\ncore-set problem, which consists of selecting the most informative samples\n\\emph{given} the labels.",
          "link": "http://arxiv.org/abs/2106.12062",
          "publishedOn": "2021-06-24T01:51:43.618Z",
          "wordCount": 535,
          "title": "A Practical & Unified Notation for Information-Theoretic Quantities in ML. (arXiv:2106.12062v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qitian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "We introduce a conceptually simple yet effective model for self-supervised\nrepresentation learning with graph data. It follows the previous methods that\ngenerate two views of an input graph through data augmentation. However, unlike\ncontrastive methods that focus on instance-level discrimination, we optimize an\ninnovative feature-level objective inspired by classical Canonical Correlation\nAnalysis. Compared with other works, our approach requires none of the\nparameterized mutual information estimator, additional projector, asymmetric\nstructures, and most importantly, negative samples which can be costly. We show\nthat the new objective essentially 1) aims at discarding augmentation-variant\ninformation by learning invariant representations, and 2) can prevent\ndegenerated solutions by decorrelating features in different dimensions. Our\ntheoretical analysis further provides an understanding for the new objective\nwhich can be equivalently seen as an instantiation of the Information\nBottleneck Principle under the self-supervised setting. Despite its simplicity,\nour method performs competitively on seven public graph datasets.",
          "link": "http://arxiv.org/abs/2106.12484",
          "publishedOn": "2021-06-24T01:51:43.607Z",
          "wordCount": 583,
          "title": "From Canonical Correlation Analysis to Self-supervised Graph Neural Networks. (arXiv:2106.12484v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shao-Bo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Ding-Xuan Zhou</a>",
          "description": "Compared with avid research activities of deep convolutional neural networks\n(DCNNs) in practice, the study of theoretical behaviors of DCNNs lags heavily\nbehind. In particular, the universal consistency of DCNNs remains open. In this\npaper, we prove that implementing empirical risk minimization on DCNNs with\nexpansive convolution (with zero-padding) is strongly universally consistent.\nMotivated by the universal consistency, we conduct a series of experiments to\nshow that without any fully connected layers, DCNNs with expansive convolution\nperform not worse than the widely used deep neural networks with hybrid\nstructure containing contracting (without zero-padding) convolution layers and\nseveral fully connected layers.",
          "link": "http://arxiv.org/abs/2106.12498",
          "publishedOn": "2021-06-24T01:51:43.593Z",
          "wordCount": 537,
          "title": "Universal Consistency of Deep Convolutional Neural Networks. (arXiv:2106.12498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1\">Charaf Eddine Benarab</a>",
          "description": "Knowledge is acquired by humans through experience, and no boundary is set\nbetween the kinds of knowledge or skill levels we can achieve on different\ntasks at the same time. When it comes to Neural Networks, that is not the case,\nthe major breakthroughs in the field are extremely task and domain specific.\nVision and language are dealt with in separate manners, using separate methods\nand different datasets. In this work, we propose to use knowledge acquired by\nbenchmark Vision Models which are trained on ImageNet to help a much smaller\narchitecture learn to classify text. After transforming the textual data\ncontained in the IMDB dataset to gray scale images. An analysis of different\ndomains and the Transfer Learning method is carried out. Despite the challenge\nposed by the very different datasets, promising results are achieved. The main\ncontribution of this work is a novel approach which links large pretrained\nmodels on both language and vision to achieve state-of-the-art results in\ndifferent sub-fields from the original task. Without needing high compute\ncapacity resources. Specifically, Sentiment Analysis is achieved after\ntransferring knowledge between vision and language models. BERT embeddings are\ntransformed into grayscale images, these images are then used as training\nexamples for pretrained vision models such as VGG16 and ResNet\n\nIndex Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain\nAdaptation.",
          "link": "http://arxiv.org/abs/2106.12479",
          "publishedOn": "2021-06-24T01:51:43.588Z",
          "wordCount": 675,
          "title": "Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12228",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1\">Martin Jullum</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Redelmeier_A/0/1/0/all/0/1\">Annabelle Redelmeier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aas_K/0/1/0/all/0/1\">Kjersti Aas</a>",
          "description": "Shapley values has established itself as one of the most appropriate and\ntheoretically sound frameworks for explaining predictions from complex machine\nlearning models. The popularity of Shapley values in the explanation setting is\nprobably due to its unique theoretical properties. The main drawback with\nShapley values, however, is that its computational complexity grows\nexponentially in the number of input features, making it unfeasible in many\nreal world situations where there could be hundreds or thousands of features.\nFurthermore, with many (dependent) features, presenting/visualizing and\ninterpreting the computed Shapley values also becomes challenging. The present\npaper introduces groupShapley: a conceptually simple approach for dealing with\nthe aforementioned bottlenecks. The idea is to group the features, for example\nby type or dependence, and then compute and present Shapley values for these\ngroups instead of for all individual features. Reducing hundreds or thousands\nof features to half a dozen or so, makes precise computations practically\nfeasible and the presentation and knowledge extraction greatly simplified. We\nprove that under certain conditions, groupShapley is equivalent to summing the\nfeature-wise Shapley values within each feature group. Moreover, we provide a\nsimulation study exemplifying the differences when these conditions are not\nmet. We illustrate the usability of the approach in a real world car insurance\nexample, where groupShapley is used to provide simple and intuitive\nexplanations.",
          "link": "http://arxiv.org/abs/2106.12228",
          "publishedOn": "2021-06-24T01:51:43.567Z",
          "wordCount": 659,
          "title": "groupShapley: Efficient prediction explanation with Shapley values for feature groups. (arXiv:2106.12228v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>",
          "description": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled and unseen target domain, which is\nusually trained on data from both domains. Access to the source domain data at\nthe adaptation stage, however, is often limited, due to data storage or privacy\nissues. To alleviate this, in this work, we target source free UDA for\nsegmentation, and propose to adapt an ``off-the-shelf\" segmentation model\npre-trained in the source domain to the target domain, with an adaptive\nbatch-wise normalization statistics adaptation framework. Specifically, the\ndomain-specific low-order batch statistics, i.e., mean and variance, are\ngradually adapted with an exponential momentum decay scheme, while the\nconsistency of domain shareable high-order batch statistics, i.e., scaling and\nshifting parameters, is explicitly enforced by our optimization objective. The\ntransferability of each channel is adaptively measured first from which to\nbalance the contribution of each channel. Moreover, the proposed source free\nUDA framework is orthogonal to unsupervised learning methods, e.g.,\nself-entropy minimization, which can thus be simply added on top of our\nframework. Extensive experiments on the BraTS 2018 database show that our\nsource free UDA framework outperformed existing source-relaxed UDA methods for\nthe cross-subtype UDA segmentation task and yielded comparable results for the\ncross-modality UDA segmentation task, compared with a supervised UDA methods\nwith the source data.",
          "link": "http://arxiv.org/abs/2106.12497",
          "publishedOn": "2021-06-24T01:51:43.562Z",
          "wordCount": 673,
          "title": "Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12034",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yinglun Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1\">Ruoxi Jiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>",
          "description": "We study pure exploration in bandits, where the dimension of the feature\nrepresentation can be much larger than the number of arms. To overcome the\ncurse of dimensionality, we propose to adaptively embed the feature\nrepresentation of each arm into a lower-dimensional space and carefully deal\nwith the induced model misspecifications. Our approach is conceptually very\ndifferent from existing works that can either only handle low-dimensional\nlinear bandits or passively deal with model misspecifications. We showcase the\napplication of our approach to two pure exploration settings that were\npreviously under-studied: (1) the reward function belongs to a possibly\ninfinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward\nfunction is nonlinear and can be approximated by neural networks. Our main\nresults provide sample complexity guarantees that only depend on the effective\ndimension of the feature spaces in the kernel or neural representations.\nExtensive experiments conducted on both synthetic and real-world datasets\ndemonstrate the efficacy of our methods.",
          "link": "http://arxiv.org/abs/2106.12034",
          "publishedOn": "2021-06-24T01:51:43.556Z",
          "wordCount": 592,
          "title": "Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1\">Balamurali B T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1\">Hwan Ing Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Saumitra Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1\">Oon Hoe Teoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Sung Shin Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Khai Pin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1\">Dorien Herremans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jer Ming Chen</a>",
          "description": "Intelligent systems are transforming the world, as well as our healthcare\nsystem. We propose a deep learning-based cough sound classification model that\ncan distinguish between children with healthy versus pathological coughs such\nas asthma, upper respiratory tract infection (URTI), and lower respiratory\ntract infection (LRTI). In order to train a deep neural network model, we\ncollected a new dataset of cough sounds, labelled with clinician's diagnosis.\nThe chosen model is a bidirectional long-short term memory network (BiLSTM)\nbased on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting\ntrained model when trained for classifying two classes of coughs -- healthy or\npathology (in general or belonging to a specific respiratory pathology),\nreaches accuracy exceeding 84\\% when classifying cough to the label provided by\nthe physicians' diagnosis. In order to classify subject's respiratory pathology\ncondition, results of multiple cough epochs per subject were combined. The\nresulting prediction accuracy exceeds 91\\% for all three respiratory\npathologies. However, when the model is trained to classify and discriminate\namong the four classes of coughs, overall accuracy dropped: one class of\npathological coughs are often misclassified as other. However, if one consider\nthe healthy cough classified as healthy and pathological cough classified to\nhave some kind of pathologies, then the overall accuracy of four class model is\nabove 84\\%. A longitudinal study of MFCC feature space when comparing\npathological and recovered coughs collected from the same subjects revealed the\nfact that pathological cough irrespective of the underlying conditions occupy\nthe same feature space making it harder to differentiate only using MFCC\nfeatures.",
          "link": "http://arxiv.org/abs/2106.12174",
          "publishedOn": "2021-06-24T01:51:43.551Z",
          "wordCount": 722,
          "title": "Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chen Lv</a>",
          "description": "To further improve the learning efficiency and performance of reinforcement\nlearning (RL), in this paper we propose a novel uncertainty-aware model-based\nRL (UA-MBRL) framework, and then implement and validate it in autonomous\ndriving under various task scenarios. First, an action-conditioned ensemble\nmodel with the ability of uncertainty assessment is established as the virtual\nenvironment model. Then, a novel uncertainty-aware model-based RL framework is\ndeveloped based on the adaptive truncation approach, providing virtual\ninteractions between the agent and environment model, and improving RL's\ntraining efficiency and performance. The developed algorithms are then\nimplemented in end-to-end autonomous vehicle control tasks, validated and\ncompared with state-of-the-art methods under various driving scenarios. The\nvalidation results suggest that the proposed UA-MBRL method surpasses the\nexisting model-based and model-free RL approaches, in terms of learning\nefficiency and achieved performance. The results also demonstrate the good\nability of the proposed method with respect to the adaptiveness and robustness,\nunder various autonomous driving scenarios.",
          "link": "http://arxiv.org/abs/2106.12194",
          "publishedOn": "2021-06-24T01:51:43.546Z",
          "wordCount": 588,
          "title": "Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouillard_L/0/1/0/all/0/1\">Louis Rouillard</a> (PARIETAL, Inria, CEA), <a href=\"http://arxiv.org/find/cs/1/au:+Wassermann_D/0/1/0/all/0/1\">Demian Wassermann</a> (PARIETAL, Inria, CEA)",
          "description": "Frequently, population studies feature pyramidally-organized data represented\nusing Hierarchical Bayesian Models (HBM) enriched with plates. These models can\nbecome prohibitively large in settings such as neuroimaging, where a sample is\ncomposed of a functional MRI signal measured on 64 thousand brain locations,\nacross 4 measurement sessions, and at least tens of subjects. Even a reduced\nexample on a specific cortical region of 300 brain locations features around 1\nmillion parameters, hampering the usage of modern density estimation techniques\nsuch as Simulation-Based Inference (SBI). To infer parameter posterior\ndistributions in this challenging class of problems, we designed a novel\nmethodology that automatically produces a variational family dual to a target\nHBM. This variatonal family, represented as a neural network, consists in the\ncombination of an attention-based hierarchical encoder feeding summary\nstatistics to a set of normalizing flows. Our automatically-derived neural\nnetwork exploits exchangeability in the plate-enriched HBM and factorizes its\nparameter space. The resulting architecture reduces by orders of magnitude its\nparameterization with respect to that of a typical SBI representation, while\nmaintaining expressivity. Our method performs inference on the specified HBM in\nan amortized setup: once trained, it can readily be applied to a new data\nsample to compute the parameters' full posterior. We demonstrate the capability\nof our method on simulated data, as well as a challenging high-dimensional\nbrain parcellation experiment. We also open up several questions that lie at\nthe intersection between SBI techniques and structured Variational Inference.",
          "link": "http://arxiv.org/abs/2106.12248",
          "publishedOn": "2021-06-24T01:51:43.530Z",
          "wordCount": 701,
          "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models. (arXiv:2106.12248v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "In active learning, new labels are commonly acquired in batches. However,\ncommon acquisition functions are only meant for one-sample acquisition rounds\nat a time, and when their scores are used naively for batch acquisition, they\nresult in batches lacking diversity, which deteriorates performance. On the\nother hand, state-of-the-art batch acquisition functions are costly to compute.\nIn this paper, we present a novel class of stochastic acquisition functions\nthat extend one-sample acquisition functions to the batch setting by observing\nhow one-sample acquisition scores change as additional samples are acquired and\nmodelling this difference for additional batch samples. We simply acquire new\nsamples by sampling from the pool set using a Gibbs distribution based on the\nacquisition scores. Our acquisition functions are both vastly cheaper to\ncompute and out-perform other batch acquisition functions.",
          "link": "http://arxiv.org/abs/2106.12059",
          "publishedOn": "2021-06-24T01:51:43.512Z",
          "wordCount": 570,
          "title": "A Simple Baseline for Batch Active Learning with Stochastic Acquisition Functions. (arXiv:2106.12059v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuantao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jie Ding</a>",
          "description": "Multi-layer feedforward networks have been used to approximate a wide range\nof nonlinear functions. An important and fundamental problem is to understand\nthe learnability of a network model through its statistical risk, or the\nexpected prediction error on future data. To the best of our knowledge, the\nrate of convergence of neural networks shown by existing works is bounded by at\nmost the order of $n^{-1/4}$ for a sample size of $n$. In this paper, we show\nthat a class of variation-constrained neural networks, with arbitrary width,\ncan achieve near-parametric rate $n^{-1/2+\\delta}$ for an arbitrarily small\npositive constant $\\delta$. It is equivalent to $n^{-1 +2\\delta}$ under the\nmean squared error. This rate is also observed by numerical experiments. The\nresult indicates that the neural function space needed for approximating smooth\nfunctions may not be as large as what is often perceived. Our result also\nprovides insight to the phenomena that deep neural networks do not easily\nsuffer from overfitting when the number of neurons and learning parameters\nrapidly grow with $n$ or even surpass $n$. We also discuss the rate of\nconvergence regarding other network parameters, including the input dimension,\nnetwork layer, and coefficient norm.",
          "link": "http://arxiv.org/abs/2106.12068",
          "publishedOn": "2021-06-24T01:51:43.507Z",
          "wordCount": 636,
          "title": "The Rate of Convergence of Variation-Constrained Deep Neural Networks. (arXiv:2106.12068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12231",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1\">Luigi Carratino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vigogna_S/0/1/0/all/0/1\">Stefano Vigogna</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1\">Daniele Calandriello</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>",
          "description": "We introduce ParK, a new large-scale solver for kernel ridge regression. Our\napproach combines partitioning with random projections and iterative\noptimization to reduce space and time complexity while provably maintaining the\nsame statistical accuracy. In particular, constructing suitable partitions\ndirectly in the feature space rather than in the input space, we promote\northogonality between the local estimators, thus ensuring that key quantities\nsuch as local effective dimension and bias remain under control. We\ncharacterize the statistical-computational tradeoff of our model, and\ndemonstrate the effectiveness of our method by numerical experiments on\nlarge-scale datasets.",
          "link": "http://arxiv.org/abs/2106.12231",
          "publishedOn": "2021-06-24T01:51:43.491Z",
          "wordCount": 534,
          "title": "ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions. (arXiv:2106.12231v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1\">Kalvik Jakkala</a>",
          "description": "Gaussian processes are one of the dominant approaches in Bayesian learning.\nAlthough the approach has been applied to numerous problems with great success,\nit has a few fundamental limitations. Multiple methods in literature have\naddressed these limitations. However, there has not been a comprehensive survey\nof the topics as of yet. Most existing surveys focus on only one particular\nvariant of Gaussian processes and their derivatives. This survey details the\ncore motivations for using Gaussian processes, their mathematical formulations,\nlimitations, and research themes that have flourished over the years to address\nsaid limitations. Furthermore, one particular research area is Deep Gaussian\nProcesses (DGPs), it has improved substantially in the past decade. The\nsignificant publications that advanced the forefront of this research area are\noutlined in their survey. Finally, a brief discussion on open problems and\nresearch directions for future work is presented at the end.",
          "link": "http://arxiv.org/abs/2106.12135",
          "publishedOn": "2021-06-24T01:51:43.474Z",
          "wordCount": 572,
          "title": "Deep Gaussian Processes: A Survey. (arXiv:2106.12135v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>",
          "description": "Transfer learning is essential when sufficient data comes from the source\ndomain, with scarce labeled data from the target domain. We develop estimators\nthat achieve minimax linear risk for linear regression problems under\ndistribution shift. Our algorithms cover different transfer learning settings\nincluding covariate shift and model shift. We also consider when data are\ngenerated from either linear or general nonlinear models. We show that linear\nminimax estimators are within an absolute constant of the minimax risk even\namong nonlinear estimators for various source/target distributions.",
          "link": "http://arxiv.org/abs/2106.12108",
          "publishedOn": "2021-06-24T01:51:43.442Z",
          "wordCount": 517,
          "title": "Near-Optimal Linear Regression under Distribution Shift. (arXiv:2106.12108v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11950",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Behne_J/0/1/0/all/0/1\">Joshua K. Behne</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1\">Galen Reeves</a>",
          "description": "We study the problem of estimating a rank-one matrix from Gaussian\nobservations where different blocks of the matrix are observed under different\nnoise levels. This problem is motivated by applications in clustering and\ncommunity detection where latent variables can be partitioned into a fixed\nnumber of known groups (e.g., users and items) and the blocks of the matrix\ncorrespond to different types of pairwise interactions (e.g., user-user,\nuser-item, or item-item interactions). In the setting where the number of\nblocks is fixed while the number of variables tends to infinity, we prove\nasymptotically exact formulas for the minimum mean-squared error in estimating\nboth the matrix and the latent variables. These formulas describe the weak\nrecovery thresholds for the problem and reveal invariance properties with\nrespect to certain scalings of the noise variance. We also derive an\napproximate message passing algorithm and a gradient descent algorithm and show\nempirically that these algorithms achieve the information-theoretic limits in\ncertain regimes.",
          "link": "http://arxiv.org/abs/2106.11950",
          "publishedOn": "2021-06-23T01:48:42.476Z",
          "wordCount": 589,
          "title": "Rank-one matrix estimation with groupwise heteroskedasticity. (arXiv:2106.11950v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1906.04648",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tan_S/0/1/0/all/0/1\">Sandra S. Y. Tan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1\">Antonios Varvitsiotis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "We introduce a new framework for unifying and systematizing the performance\nanalysis of first-order black-box optimization algorithms for unconstrained\nconvex minimization. The low-cost iteration complexity enjoyed by first-order\nalgorithms renders them particularly relevant for applications in machine\nlearning and large-scale data analysis. Relying on sum-of-squares (SOS)\noptimization, we introduce a hierarchy of semidefinite programs that give\nincreasingly better convergence bounds for higher levels of the hierarchy.\nAlluding to the power of the SOS hierarchy, we show that the (dual of the)\nfirst level corresponds to the Performance Estimation Problem (PEP) introduced\nby Drori and Teboulle [Math. Program., 145(1):451--482, 2014], a powerful\nframework for determining convergence rates of first-order optimization\nalgorithms. Consequently, many results obtained within the PEP framework can be\nreinterpreted as degree-1 SOS proofs, and thus, the SOS framework provides a\npromising new approach for certifying improved rates of convergence by means of\nhigher-order SOS certificates. To determine analytical rate bounds, in this\nwork we use the first level of the SOS hierarchy and derive new result{s} for\nnoisy gradient descent with inexact line search methods (Armijo, Wolfe, and\nGoldstein).",
          "link": "http://arxiv.org/abs/1906.04648",
          "publishedOn": "2021-06-23T01:48:42.470Z",
          "wordCount": 698,
          "title": "Analysis of Optimization Algorithms via Sum-of-Squares. (arXiv:1906.04648v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1\">Grzegorz Ciesielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Dzikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1\">Mateusz Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1\">Piotr Pusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>",
          "description": "We investigate the possibility of forcing a self-supervised model trained\nusing a contrastive predictive loss to extract slowly varying latent\nrepresentations. Rather than producing individual predictions for each of the\nfuture representations, the model emits a sequence of predictions shorter than\nthat of the upcoming representations to which they will be aligned. In this\nway, the prediction network solves a simpler task of predicting the next\nsymbols, but not their exact timing, while the encoding network is trained to\nproduce piece-wise constant latent codes. We evaluate the model on a speech\ncoding task and demonstrate that the proposed Aligned Contrastive Predictive\nCoding (ACPC) leads to higher linear phone prediction accuracy and lower ABX\nerror rates, while being slightly faster to train due to the reduced number of\nprediction heads.",
          "link": "http://arxiv.org/abs/2104.11946",
          "publishedOn": "2021-06-23T01:48:42.386Z",
          "wordCount": 614,
          "title": "Aligned Contrastive Predictive Coding. (arXiv:2104.11946v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13945",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Abel_S/0/1/0/all/0/1\">Steve Abel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Blance_A/0/1/0/all/0/1\">Andrew Blance</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>",
          "description": "We perform an in-depth comparison of quantum annealing with several classical\noptimisation techniques, namely thermal annealing, Nelder-Mead, and gradient\ndescent. We begin with a direct study of the 2D Ising model on a quantum\nannealer, and compare its properties directly with those of the thermal 2D\nIsing model. These properties include an Ising-like phase transition that can\nbe induced by either a change in 'quantum-ness' of the theory, or by a scaling\nthe Ising couplings up or down. This behaviour is in accord with what is\nexpected from the physical understanding of the quantum system. We then go on\nto demonstrate the efficacy of the quantum annealer at minimising several\nincreasingly hard two dimensional potentials. For all the potentials we find\nthe general behaviour that Nelder-Mead and gradient descent methods are very\nsusceptible to becoming trapped in false minima, while the thermal anneal\nmethod is somewhat better at discovering the true minimum. However, and despite\ncurrent limitations on its size, the quantum annealer performs a minimisation\nvery markedly better than any of these classical techniques. A quantum anneal\ncan be designed so that the system almost never gets trapped in a false\nminimum, and rapidly and successfully minimises the potentials.",
          "link": "http://arxiv.org/abs/2105.13945",
          "publishedOn": "2021-06-23T01:48:42.369Z",
          "wordCount": 690,
          "title": "Quantum Optimisation of Complex Systems with a Quantum Annealer. (arXiv:2105.13945v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>",
          "description": "Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques. Also given their success, several legal\nproblems arise when working with these techniques.\n\nIn this article we consider a set of key questions in the area of generative\ndeep learning for the arts. Is it possible to use copyrighted works as training\nset for generative models? How do we legally store their copies in order to\nperform the training process? And then, who (if someone) will own the copyright\non the generated data? We try to answer these questions considering the law in\nforce in both US and EU and the future alternatives, trying to define a set of\nguidelines for artists and developers working on deep learning generated art.",
          "link": "http://arxiv.org/abs/2105.09266",
          "publishedOn": "2021-06-23T01:48:42.362Z",
          "wordCount": 630,
          "title": "Copyright in Generative Deep Learning. (arXiv:2105.09266v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zinc Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hutchin Huang</a>",
          "description": "Hateful content detection is one of the areas where deep learning can and\nshould make a significant difference. The Hateful Memes Challenge from Facebook\nhelps fulfill such potential by challenging the contestants to detect hateful\nspeech in multi-modal memes using deep learning algorithms. In this paper, we\nutilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved\nmodels' performance by adding training datasets generated from data\naugmentation. Enlarging the training data set helped us get a more than 2%\nboost in terms of AUROC with the Visual BERT model. Our approach achieved\n0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,\nwhich revealed remarkable progress.",
          "link": "http://arxiv.org/abs/2105.13132",
          "publishedOn": "2021-06-23T01:48:42.356Z",
          "wordCount": 591,
          "title": "Enhance Multimodal Model Performance with Data Augmentation: Facebook Hateful Meme Challenge Solution. (arXiv:2105.13132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11170",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xueyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Longhan Xie</a>",
          "description": "At present, people usually use some methods based on convolutional neural\nnetworks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have\nlimitations in perceiving global dependencies, which is not adequate for common\nEEG paradigms with a strong overall relationship. Regarding this issue, we\npropose a novel EEG decoding method that mainly relies on the attention\nmechanism. The EEG data is firstly preprocessed and spatially filtered. And\nthen, we apply attention transforming on the feature-channel dimension so that\nthe model can enhance more relevant spatial features. The most crucial step is\nto slice the data in the time dimension for attention transforming, and finally\nobtain a highly distinguishable representation. At this time, global averaging\npooling and a simple fully-connected layer are used to classify different\ncategories of EEG data. Experiments on two public datasets indicate that the\nstrategy of attention transforming effectively utilizes spatial and temporal\nfeatures. And we have reached the level of the state-of-the-art in\nmulti-classification of EEG, with fewer parameters. As far as we know, it is\nthe first time that a detailed and complete method based on the transformer\nidea has been proposed in this field. It has good potential to promote the\npracticality of brain-computer interface (BCI). The source code can be found\nat: \\textit{https://github.com/anranknight/EEG-Transformer}.",
          "link": "http://arxiv.org/abs/2106.11170",
          "publishedOn": "2021-06-23T01:48:42.350Z",
          "wordCount": 655,
          "title": "Transformer-based Spatial-Temporal Feature Learning for EEG Decoding. (arXiv:2106.11170v1 [eess.SP] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1\">Julian B&#xfc;chel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1\">Fynn Faber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1\">Dylan R. Muir</a>",
          "description": "Neuromorphic neural network processors, in the form of compute-in-memory\ncrossbar arrays of memristors, or in the form of subthreshold analog and\nmixed-signal ASICs, promise enormous advantages in compute density and energy\nefficiency for NN-based ML tasks. However, these technologies are prone to\ncomputational non-idealities, due to process variation and intrinsic device\nphysics. This degrades the task performance of networks deployed to the\nprocessor, by introducing parameter noise into the deployed model. While it is\npossible to calibrate each device, or train networks individually for each\nprocessor, these approaches are expensive and impractical for commercial\ndeployment. Alternative methods are therefore needed to train networks that are\ninherently robust against parameter variation, as a consequence of network\narchitecture and parameters. We present a new adversarial network optimisation\nalgorithm that attacks network parameters during training, and promotes robust\nperformance during inference in the face of parameter variation. Our approach\nintroduces a regularization term penalising the susceptibility of a network to\nweight perturbation. We compare against previous approaches for producing\nparameter insensitivity such as dropout, weight smoothing and introducing\nparameter noise during training. We show that our approach produces models that\nare more robust to targeted parameter variation, and equally robust to random\nparameter variation. Our approach finds minima in flatter locations in the\nweight-loss landscape compared with other approaches, highlighting that the\nnetworks found by our technique are less sensitive to parameter perturbation.\nOur work provides an approach to deploy neural network architectures to\ninference devices that suffer from computational non-idealities, with minimal\nloss of performance. ...",
          "link": "http://arxiv.org/abs/2106.05009",
          "publishedOn": "2021-06-23T01:48:42.334Z",
          "wordCount": 699,
          "title": "Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1\">Laura Manduchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1\">Kieran Chin-Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1\">Holger Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1\">Sven Wellmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1\">Julia E. Vogt</a>",
          "description": "Constrained clustering has gained significant attention in the field of\nmachine learning as it can leverage prior information on a growing amount of\nonly partially labeled data. Following recent advances in deep generative\nmodels, we propose a novel framework for constrained clustering that is\nintuitive, interpretable, and can be trained efficiently in the framework of\nstochastic gradient variational inference. By explicitly integrating domain\nknowledge in the form of probabilistic relations, our proposed model (DC-GMM)\nuncovers the underlying distribution of data conditioned on prior clustering\npreferences, expressed as pairwise constraints. These constraints guide the\nclustering process towards a desirable partition of the data by indicating\nwhich samples should or should not belong to the same cluster. We provide\nextensive experiments to demonstrate that DC-GMM shows superior clustering\nperformances and robustness compared to state-of-the-art deep constrained\nclustering methods on a wide range of data sets. We further demonstrate the\nusefulness of our approach on two challenging real-world applications.",
          "link": "http://arxiv.org/abs/2106.06385",
          "publishedOn": "2021-06-23T01:48:42.318Z",
          "wordCount": 605,
          "title": "Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07467",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1\">Michael Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1\">Evis Sala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1\">Leonardo Rundo</a>",
          "description": "Background: Colonoscopy remains the gold-standard screening for colorectal\ncancer. However, significant miss rates for polyps have been reported,\nparticularly when there are multiple small adenomas. This presents an\nopportunity to leverage computer-aided systems to support clinicians and reduce\nthe number of polyps missed.\n\nMethod: In this work we introduce the Focus U-Net, a novel dual\nattention-gated deep neural network, which combines efficient spatial and\nchannel-based attention into a single Focus Gate module to encourage selective\nlearning of polyp features. The Focus U-Net further incorporates short-range\nskip connections and deep supervision. Furthermore, we introduce the Hybrid\nFocal loss, a new compound loss function based on the Focal loss and Focal\nTversky loss, to handle class-imbalanced image segmentation. For our\nexperiments, we selected five public datasets containing images of polyps\nobtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,\nETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we\nuse the Dice similarity coefficient (DSC) and Intersection over Union (IoU)\nmetrics.\n\nResults: Our model achieves state-of-the-art results for both CVC-ClinicDB\nand Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When\nevaluated on a combination of five public polyp datasets, our model similarly\nachieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of\n0.809, a 14% and 15% improvement over the previous state-of-the-art results of\n0.768 and 0.702, respectively.\n\nConclusions: This study shows the potential for deep learning to provide fast\nand accurate polyp segmentation results for use during colonoscopy. The Focus\nU-Net may be adapted for future use in newer non-invasive screening and more\nbroadly to other biomedical image segmentation tasks involving class imbalance\nand requiring efficiency.",
          "link": "http://arxiv.org/abs/2105.07467",
          "publishedOn": "2021-06-23T01:48:42.310Z",
          "wordCount": 745,
          "title": "Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1\">Wei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>",
          "description": "Spiking Neural Networks (SNNs) have been attached great importance due to\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\nAs these chips are usually resource-constrained, the compression of SNNs is\nthus crucial along the road of practical use of SNNs. Most existing methods\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\nwhich ignore the difference between ANNs and SNNs, thus limiting the\nperformance of the pruned SNNs. Besides, these methods are only suitable for\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\noptimize network structure without retraining. Our key innovation is to\nredefine the gradient to a new synaptic parameter, allowing better exploration\nof network structures by taking full advantage of the competition between\npruning and regrowth of connections. The experimental results show that the\nproposed method achieves minimal loss of SNNs' performance on MNIST and\nCIFAR-10 dataset so far. Moreover, it reaches a $\\sim$3.5% accuracy loss under\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\ncapability in SNNs. Our work suggests that there exists extremely high\nredundancy in deep SNNs. Our codes are available at\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.",
          "link": "http://arxiv.org/abs/2105.04916",
          "publishedOn": "2021-06-23T01:48:42.302Z",
          "wordCount": 722,
          "title": "Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1\">Zaynah Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Daniel S. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Satvik Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jerry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1\">Ashwin Balakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1\">Marek Petrik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca D. Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>",
          "description": "The difficulty in specifying rewards for many real-world problems has led to\nan increased focus on learning rewards from human feedback, such as\ndemonstrations. However, there are often many different reward functions that\nexplain the human feedback, leaving agents with uncertainty over what the true\nreward function is. While most policy optimization approaches handle this\nuncertainty by optimizing for expected performance, many applications demand\nrisk-averse behavior. We derive a novel policy gradient-style robust\noptimization approach, PG-BROIL, that optimizes a soft-robust objective that\nbalances expected performance and risk. To the best of our knowledge, PG-BROIL\nis the first policy optimization algorithm robust to a distribution of reward\nhypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL\ncan produce a family of behaviors ranging from risk-neutral to risk-averse and\noutperforms state-of-the-art imitation learning algorithms when learning from\nambiguous demonstrations by hedging against uncertainty, rather than seeking to\nuniquely identify the demonstrator's reward function.",
          "link": "http://arxiv.org/abs/2106.06499",
          "publishedOn": "2021-06-23T01:48:42.292Z",
          "wordCount": 626,
          "title": "Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1\">Aaron Defazio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>",
          "description": "We propose a new stochastic gradient method that uses recorded past loss\nvalues to reduce the variance. Our method can be interpreted as a new\nstochastic variant of the Polyak Stepsize that converges globally without\nassuming interpolation. Our method introduces auxiliary variables, one for each\ndata point, that track the loss value for each data point. We provide a global\nconvergence theory for our method by showing that it can be interpreted as a\nspecial variant of online SGD. The new method only stores a single scalar per\ndata point, opening up new applications for variance reduction where memory is\nthe bottleneck.",
          "link": "http://arxiv.org/abs/2106.11851",
          "publishedOn": "2021-06-23T01:48:42.250Z",
          "wordCount": 556,
          "title": "Stochastic Polyak Stepsize with a Moving Target. (arXiv:2106.11851v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaherwar_P/0/1/0/all/0/1\">Prutha Gaherwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shraddha Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khengare_R/0/1/0/all/0/1\">Rahul Khengare</a>",
          "description": "With an increase in mobile and camera devices' popularity, digital content in\nthe form of images has increased drastically. As personal life is being\ncontinuously documented in pictures, the risk of losing it to eavesdroppers is\na matter of grave concern. Secondary storage is the most preferred medium for\nthe storage of personal and other images. Our work is concerned with the\nsecurity of such images. While encryption is the best way to ensure image\nsecurity, full encryption and decryption is a computationally-intensive\nprocess. Moreover, as cameras are getting better every day, image quality, and\nthus, the pixel density has increased considerably. The increased pixel density\nmakes encryption and decryption more expensive. We, therefore, delve into\nselective encryption and selective blurring based on the region of interest.\nInstead of encrypting or blurring the entire photograph, we only encode\nselected regions of the image. We present a comparative analysis of the partial\nand full encryption of the photos. This kind of encoding will help us lower the\nencryption overhead without compromising security. The applications utilizing\nthis technique will become more usable due to the reduction in the decryption\ntime. Additionally, blurred images being more readable than encrypted ones,\nallowed us to define the level of security. We leverage the machine learning\nalgorithms like Mask-RCNN (Region-based convolutional neural network) and YOLO\n(You Only Look Once) to select the region of interest. These algorithms have\nset new benchmarks for object recognition. We develop an end to end system to\ndemonstrate our idea of selective encryption.",
          "link": "http://arxiv.org/abs/2106.11770",
          "publishedOn": "2021-06-23T01:48:42.233Z",
          "wordCount": 689,
          "title": "SISA: Securing Images by Selective Alteration. (arXiv:2106.11770v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jordana_A/0/1/0/all/0/1\">Armand Jordana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpentier_J/0/1/0/all/0/1\">Justin Carpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Ludovic Righetti</a>",
          "description": "Modeling dynamical systems plays a crucial role in capturing and\nunderstanding complex physical phenomena. When physical models are not\nsufficiently accurate or hardly describable by analytical formulas, one can use\ngeneric function approximators such as neural networks to capture the system\ndynamics directly from sensor measurements. As for now, current methods to\nlearn the parameters of these neural networks are highly sensitive to the\ninherent instability of most dynamical systems of interest, which in turn\nprevents the study of very long sequences. In this work, we introduce a generic\nand scalable method based on multiple shooting to learn latent representations\nof indirectly observed dynamical systems. We achieve state-of-the-art\nperformances on systems observed directly from raw images. Further, we\ndemonstrate that our method is robust to noisy measurements and can handle\ncomplex dynamical systems, such as chaotic ones.",
          "link": "http://arxiv.org/abs/2106.11712",
          "publishedOn": "2021-06-23T01:48:42.226Z",
          "wordCount": 573,
          "title": "Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting. (arXiv:2106.11712v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1\">Kaitlin Maile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1\">Erwan Lecarpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1\">Herv&#xe9; Luga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1\">Dennis G. Wilson</a>",
          "description": "Differentiable Architecture Search (DARTS) is a recently proposed neural\narchitecture search (NAS) method based on a differentiable relaxation. Due to\nits success, numerous variants analyzing and improving parts of the DARTS\nframework have recently been proposed. By considering the problem as a\nconstrained bilevel optimization, we propose and analyze three improvements to\narchitectural weight competition, update scheduling, and regularization towards\ndiscretization. First, we introduce a new approach to the activation of\narchitecture weights, which prevents confounding competition within an edge and\nallows for fair comparison across edges to aid in discretization. Next, we\npropose a dynamic schedule based on per-minibatch network information to make\narchitecture updates more informed. Finally, we consider two regularizations,\nbased on proximity to discretization and the Alternating Directions Method of\nMultipliers (ADMM) algorithm, to promote early discretization. Our results show\nthat this new activation scheme reduces final architecture size and the\nregularizations improve reliability in search results while maintaining\ncomparable performance to state-of-the-art in NAS, especially when used with\nour new dynamic informed schedule.",
          "link": "http://arxiv.org/abs/2106.11655",
          "publishedOn": "2021-06-23T01:48:42.219Z",
          "wordCount": 602,
          "title": "On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-23T01:48:41.861Z",
          "wordCount": 544,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammerbacher_T/0/1/0/all/0/1\">Tom Hammerbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_Hegermann_M/0/1/0/all/0/1\">Markus Lange-Hegermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platz_G/0/1/0/all/0/1\">Gorden Platz</a>",
          "description": "Digitalization leads to data transparency for production systems that we can\nbenefit from with data-driven analysis methods like neural networks. For\nexample, automated anomaly detection enables saving resources and optimizing\nthe production. We study using rarely occurring information about labeled\nanomalies into Variational Autoencoder neural network structures to overcome\ninformation deficits of supervised and unsupervised approaches. This method\noutperforms all other models in terms of accuracy, precision, and recall. We\nevaluate the following methods: Principal Component Analysis, Isolation Forest,\nClassifying Neural Networks, and Variational Autoencoders on seven time series\ndatasets to find the best performing detection methods. We extend this idea to\ninclude more infrequently occurring meta information about production\nprocesses. This use of sparse labels, both of anomalies or production data,\nallows to harness any additional information available for increasing anomaly\ndetection performance.",
          "link": "http://arxiv.org/abs/2103.12998",
          "publishedOn": "2021-06-23T01:48:41.855Z",
          "wordCount": 605,
          "title": "Including Sparse Production Knowledge into Variational Autoencoders to Increase Anomaly Detection Reliability. (arXiv:2103.12998v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1\">Renkun Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1\">Kezhi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Conventional image classifiers are trained by randomly sampling mini-batches\nof images. To achieve state-of-the-art performance, practitioners use\nsophisticated data augmentation schemes to expand the amount of training data\navailable for sampling. In contrast, meta-learning algorithms sample support\ndata, query data, and tasks on each training step. In this complex sampling\nscenario, data augmentation can be used not only to expand the number of images\navailable per class, but also to generate entirely new classes/tasks. We\nsystematically dissect the meta-learning pipeline and investigate the distinct\nways in which data augmentation can be integrated at both the image and class\nlevels. Our proposed meta-specific data augmentation significantly improves the\nperformance of meta-learners on few-shot classification benchmarks.",
          "link": "http://arxiv.org/abs/2010.07092",
          "publishedOn": "2021-06-23T01:48:41.849Z",
          "wordCount": 583,
          "title": "Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>",
          "description": "Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.",
          "link": "http://arxiv.org/abs/2106.06804",
          "publishedOn": "2021-06-23T01:48:41.843Z",
          "wordCount": 622,
          "title": "Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Viraaj Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giallanza_T/0/1/0/all/0/1\">Tyler Giallanza</a>",
          "description": "Early detection of suicidal ideation in depressed individuals can allow for\nadequate medical attention and support, which in many cases is life-saving.\nRecent NLP research focuses on classifying, from a given piece of text, if an\nindividual is suicidal or clinically healthy. However, there have been no major\nattempts to differentiate between depression and suicidal ideation, which is an\nimportant clinical challenge. Due to the scarce availability of EHR data,\nsuicide notes, or other similar verified sources, web query data has emerged as\na promising alternative. Online sources, such as Reddit, allow for anonymity\nthat prompts honest disclosure of symptoms, making it a plausible source even\nin a clinical setting. However, these online datasets also result in lower\nperformance, which can be attributed to the inherent noise in web-scraped\nlabels, which necessitates a noise-removal process. Thus, we propose SDCNL, a\nsuicide versus depression classification method through a deep learning\napproach. We utilize online content from Reddit to train our algorithm, and to\nverify and correct noisy labels, we propose a novel unsupervised label\ncorrection method which, unlike previous work, does not require prior noise\ndistribution information. Our extensive experimentation with multiple deep word\nembedding models and classifiers display the strong performance of the method\nin anew, challenging classification application. We make our code and dataset\navailable at https://github.com/ayaanzhaque/SDCNL",
          "link": "http://arxiv.org/abs/2102.09427",
          "publishedOn": "2021-06-23T01:48:41.837Z",
          "wordCount": 685,
          "title": "Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction. (arXiv:2102.09427v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.",
          "link": "http://arxiv.org/abs/2104.08760",
          "publishedOn": "2021-06-23T01:48:41.830Z",
          "wordCount": 739,
          "title": "Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02601",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1\">James Kotary</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1\">Ferdinando Fioretto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1\">Pascal Van Hentenryck</a>",
          "description": "Optimization problems are ubiquitous in our societies and are present in\nalmost every segment of the economy. Most of these optimization problems are\nNP-hard and computationally demanding, often requiring approximate solutions\nfor large-scale instances. Machine learning frameworks that learn to\napproximate solutions to such hard optimization problems are a potentially\npromising avenue to address these difficulties, particularly when many closely\nrelated problem instances must be solved repeatedly. Supervised learning\nframeworks can train a model using the outputs of pre-solved instances.\nHowever, when the outputs are themselves approximations, when the optimization\nproblem has symmetric solutions, and/or when the solver uses randomization,\nsolutions to closely related instances may exhibit large differences and the\nlearning task can become inherently more difficult. This paper demonstrates\nthis critical challenge, connects the volatility of the training data to the\nability of a model to approximate it, and proposes a method for producing\n(exact or approximate) solutions to optimization problems that are more\namenable to supervised learning tasks. The effectiveness of the method is\ntested on hard non-linear nonconvex and discrete combinatorial problems.",
          "link": "http://arxiv.org/abs/2106.02601",
          "publishedOn": "2021-06-23T01:48:41.824Z",
          "wordCount": 628,
          "title": "Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1\">Hermawan Mulyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1\">Desmond Chan</a>",
          "description": "Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.",
          "link": "http://arxiv.org/abs/2105.10880",
          "publishedOn": "2021-06-23T01:48:41.816Z",
          "wordCount": 577,
          "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jingxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1\">Martin Shepperd</a>",
          "description": "Context: Software engineering researchers have undertaken many experiments\ninvestigating the potential of software defect prediction algorithms.\nUnfortunately, some widely used performance metrics are known to be\nproblematic, most notably F1, but nevertheless F1 is widely used.\n\nObjective: To investigate the potential impact of using F1 on the validity of\nthis large body of research.\n\nMethod: We undertook a systematic review to locate relevant experiments and\nthen extract all pairwise comparisons of defect prediction performance using F1\nand the un-biased Matthews correlation coefficient (MCC).\n\nResults: We found a total of 38 primary studies. These contain 12,471 pairs\nof results. Of these, 21.95% changed direction when the MCC metric is used\ninstead of the biased F1 metric. Unfortunately, we also found evidence\nsuggesting that F1 remains widely used in software defect prediction research.\n\nConclusions: We reiterate the concerns of statisticians that the F1 is a\nproblematic metric outside of an information retrieval context, since we are\nconcerned about both classes (defect-prone and not defect-prone units). This\ninappropriate usage has led to a substantial number (more than one fifth) of\nerroneous (in terms of direction) results. Therefore we urge researchers to (i)\nuse an unbiased metric and (ii) publish detailed results including confusion\nmatrices such that alternative analyses become possible.",
          "link": "http://arxiv.org/abs/2103.10201",
          "publishedOn": "2021-06-23T01:48:41.809Z",
          "wordCount": 721,
          "title": "The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">MohammadJavad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1\">Sheldon M Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyu Zhang</a>",
          "description": "We consider the problem of finding, through adaptive sampling, which of $n$\noptions (arms) has the largest mean. Our objective is to determine a rule which\nidentifies the best arm with a fixed minimum confidence using as few\nobservations as possible, i.e. this is a fixed-confidence (FC) best arm\nidentification (BAI) in multi-armed bandits. We study such problems under the\nBayesian setting with both Bernoulli and Gaussian arms. We propose to use the\nclassical \"vector at a time\" (VT) rule, which samples each remaining arm once\nin each round. We show how VT can be implemented and analyzed in our Bayesian\nsetting and be improved by early elimination. Our analysis show that these\nalgorithms guarantee an optimal strategy under the prior. We also propose and\nanalyze a variant of the classical \"play the winner\" (PW) algorithm. Numerical\nresults show that these rules compare favorably with state-of-art algorithms.",
          "link": "http://arxiv.org/abs/2106.06848",
          "publishedOn": "2021-06-23T01:48:41.767Z",
          "wordCount": 619,
          "title": "Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandits: Simple Sequential Elimination Algorithms. (arXiv:2106.06848v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Ting Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>",
          "description": "We consider an adversarially-trained version of the nonnegative matrix\nfactorization, a popular latent dimensionality reduction technique. In our\nformulation, an attacker adds an arbitrary matrix of bounded norm to the given\ndata matrix. We design efficient algorithms inspired by adversarial training to\noptimize for dictionary and coefficient matrices with enhanced generalization\nabilities. Extensive simulations on synthetic and benchmark datasets\ndemonstrate the superior predictive performance on matrix completion tasks of\nour proposed method compared to state-of-the-art competitors, including other\nvariants of adversarial nonnegative matrix factorization.",
          "link": "http://arxiv.org/abs/2104.04757",
          "publishedOn": "2021-06-23T01:48:41.757Z",
          "wordCount": 552,
          "title": "Adversarially-Trained Nonnegative Matrix Factorization. (arXiv:2104.04757v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joanne Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1\">Bishal Lamichhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Zeev_D/0/1/0/all/0/1\">Dror Ben-Zeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1\">Andrew Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1\">Akane Sano</a>",
          "description": "We aim to develop clustering models to obtain behavioral representations from\ncontinuous multimodal mobile sensing data towards relapse prediction tasks. The\nidentified clusters could represent different routine behavioral trends related\nto daily living of patients as well as atypical behavioral trends associated\nwith impending relapse.\n\nWe used the mobile sensing data obtained in the CrossCheck project for our\nanalysis. Continuous data from six different mobile sensing-based modalities\n(e.g. ambient light, sound/conversation, acceleration etc.) obtained from a\ntotal of 63 schizophrenia patients, each monitored for up to a year, were used\nfor the clustering models and relapse prediction evaluation. Two clustering\nmodels, Gaussian Mixture Model (GMM) and Partition Around Medoids (PAM), were\nused to obtain behavioral representations from the mobile sensing data. The\nfeatures obtained from the clustering models were used to train and evaluate a\npersonalized relapse prediction model using Balanced Random Forest. The\npersonalization was done by identifying optimal features for a given patient\nbased on a personalization subset consisting of other patients who are of\nsimilar age.\n\nThe clusters identified using the GMM and PAM models were found to represent\ndifferent behavioral patterns (such as clusters representing sedentary days,\nactive but with low communications days, etc.). Significant changes near the\nrelapse periods were seen in the obtained behavioral representation features\nfrom the clustering models. The clustering model based features, together with\nother features characterizing the mobile sensing data, resulted in an F2 score\nof 0.24 for the relapse prediction task in a leave-one-patient-out evaluation\nsetting. This obtained F2 score is significantly higher than a random\nclassification baseline with an average F2 score of 0.042.",
          "link": "http://arxiv.org/abs/2106.11487",
          "publishedOn": "2021-06-23T01:48:41.748Z",
          "wordCount": 709,
          "title": "Routine Clustering of Mobile Sensor Data Facilitates Psychotic Relapse Prediction in Schizophrenia Patients. (arXiv:2106.11487v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14602",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1\">Bhusan Chettri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1\">Rosa Gonz&#xe1;lez Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>",
          "description": "Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.",
          "link": "http://arxiv.org/abs/2103.14602",
          "publishedOn": "2021-06-23T01:48:41.733Z",
          "wordCount": 579,
          "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study reinforcement learning (RL) with linear function approximation.\nExisting algorithms for this problem only have high-probability regret and/or\nProbably Approximately Correct (PAC) sample complexity guarantees, which cannot\nguarantee the convergence to the optimal policy. In this paper, in order to\novercome the limitation of existing algorithms, we propose a new algorithm\ncalled FLUTE, which enjoys uniform-PAC convergence to the optimal policy with\nhigh probability. The uniform-PAC guarantee is the strongest possible guarantee\nfor reinforcement learning in the literature, which can directly imply both PAC\nand high probability regret bounds, making our algorithm superior to all\nexisting algorithms with linear function approximation. At the core of our\nalgorithm is a novel minimax value function estimator and a multi-level\npartition scheme to select the training samples from historical observations.\nBoth of these techniques are new and of independent interest.",
          "link": "http://arxiv.org/abs/2106.11612",
          "publishedOn": "2021-06-23T01:48:41.718Z",
          "wordCount": 582,
          "title": "Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1\">Justin Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>",
          "description": "Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.",
          "link": "http://arxiv.org/abs/2106.11437",
          "publishedOn": "2021-06-23T01:48:41.688Z",
          "wordCount": 683,
          "title": "Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11519",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1\">Christoph Dann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1\">Ayush Sekhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1\">Karthik Sridharan</a>",
          "description": "There have been many recent advances on provably efficient Reinforcement\nLearning (RL) in problems with rich observation spaces. However, all these\nworks share a strong realizability assumption about the optimal value function\nof the true MDP. Such realizability assumptions are often too strong to hold in\npractice. In this work, we consider the more realistic setting of agnostic RL\nwith rich observation spaces and a fixed class of policies $\\Pi$ that may not\ncontain any near-optimal policy. We provide an algorithm for this setting whose\nerror is bounded in terms of the rank $d$ of the underlying MDP. Specifically,\nour algorithm enjoys a sample complexity bound of $\\widetilde{O}\\left((H^{4d}\nK^{3d} \\log |\\Pi|)/\\epsilon^2\\right)$ where $H$ is the length of episodes, $K$\nis the number of actions and $\\epsilon>0$ is the desired sub-optimality. We\nalso provide a nearly matching lower bound for this agnostic setting that shows\nthat the exponential dependence on rank is unavoidable, without further\nassumptions.",
          "link": "http://arxiv.org/abs/2106.11519",
          "publishedOn": "2021-06-23T01:48:41.679Z",
          "wordCount": 600,
          "title": "Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations. (arXiv:2106.11519v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kruse_J/0/1/0/all/0/1\">Jakob Kruse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1\">Lynton Ardizzone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1\">Ullrich K&#xf6;the</a>",
          "description": "Recent work demonstrated that flow-based invertible neural networks are\npromising tools for solving ambiguous inverse problems. Following up on this,\nwe investigate how ten invertible architectures and related models fare on two\nintuitive, low-dimensional benchmark problems, obtaining the best results with\ncoupling layers and simple autoencoders. We hope that our initial efforts\ninspire other researchers to evaluate their invertible architectures in the\nsame setting and put forth additional benchmarks, so our evaluation may\neventually grow into an official community challenge.",
          "link": "http://arxiv.org/abs/2101.10763",
          "publishedOn": "2021-06-23T01:48:41.672Z",
          "wordCount": 558,
          "title": "Benchmarking Invertible Architectures on Inverse Problems. (arXiv:2101.10763v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Solanki_P/0/1/0/all/0/1\">Prashant Solanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1\">Aaron Harwood</a>",
          "description": "With the prevalence of online social networking sites (OSNs) and mobile\ndevices, people are increasingly reliant on a variety of OSNs for keeping in\ntouch with family and friends, and using it as a source of information. For\nexample, a user might utilise multiple OSNs for different purposes, such as\nusing Flickr to share holiday pictures with family and friends, and Twitter to\npost short messages about their thoughts. Identifying the same user across\nmultiple OSNs is an important task as this allows us to understand the usage\npatterns of users among different OSNs, make recommendations when a user\nregisters for a new OSN, and various other useful applications. To address this\nproblem, we proposed an algorithm based on the multilayer perceptron using\nvarious types of features, namely: (i) user profile, such as name, location,\ndescription; (ii) temporal distribution of user generated content; and (iii)\nembedding based on user name, real name and description. Using a Twitter and\nFlickr dataset of users and their posting activities, we perform an empirical\nstudy on how these features affect the performance of user identification\nacross the two OSNs and discuss our main findings based on the different\nfeatures.",
          "link": "http://arxiv.org/abs/2106.11815",
          "publishedOn": "2021-06-23T01:48:41.665Z",
          "wordCount": 656,
          "title": "User Identification across Social Networking Sites using User Profiles and Posting Patterns. (arXiv:2106.11815v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1\">Niall McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1\">Jesus Martinez del Rincon</a>",
          "description": "Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.",
          "link": "http://arxiv.org/abs/2106.11821",
          "publishedOn": "2021-06-23T01:48:41.659Z",
          "wordCount": 590,
          "title": "Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>",
          "description": "We study the stochastic shortest path problem with adversarial costs and\nknown transition, and show that the minimax regret is\n$\\widetilde{O}(\\sqrt{DT^\\star K})$ and $\\widetilde{O}(\\sqrt{DT^\\star SA K})$\nfor the full-information setting and the bandit feedback setting respectively,\nwhere $D$ is the diameter, $T^\\star$ is the expected hitting time of the\noptimal policy, $S$ is the number of states, $A$ is the number of actions, and\n$K$ is the number of episodes. Our results significantly improve upon the\nexisting work of (Rosenberg and Mansour, 2020) which only considers the\nfull-information setting and achieves suboptimal regret. Our work is also the\nfirst to consider bandit feedback with adversarial costs.\n\nOur algorithms are built on top of the Online Mirror Descent framework with a\nvariety of new techniques that might be of independent interest, including an\nimproved multi-scale expert algorithm, a reduction from general stochastic\nshortest path to a special loop-free case, a skewed occupancy measure space,\nand a novel correction term added to the cost estimators. Interestingly, the\nlast two elements reduce the variance of the learner via positive bias and the\nvariance of the optimal policy via negative bias respectively, and having them\nsimultaneously is critical for obtaining the optimal high-probability bound in\nthe bandit feedback setting.",
          "link": "http://arxiv.org/abs/2012.04053",
          "publishedOn": "2021-06-23T01:48:41.640Z",
          "wordCount": 679,
          "title": "Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition. (arXiv:2012.04053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Active Learning is essential for more label-efficient deep learning. Bayesian\nActive Learning has focused on BALD, which reduces model parameter uncertainty.\nHowever, we show that BALD gets stuck on out-of-distribution or junk data that\nis not relevant for the task. We examine a novel *Expected Predictive\nInformation Gain (EPIG)* to deal with distribution shifts of the pool set. EPIG\nreduces the uncertainty of *predictions* on an unlabelled *evaluation set*\nsampled from the test data distribution whose distribution might be different\nto the pool set distribution. Based on this, our new EPIG-BALD acquisition\nfunction for Bayesian Neural Networks selects samples to improve the\nperformance on the test data distribution instead of selecting samples that\nreduce model uncertainty everywhere, including for out-of-distribution regions\nwith low density in the test data distribution. Our method outperforms\nstate-of-the-art Bayesian active learning methods on high-dimensional datasets\nand avoids out-of-distribution junk data in cases where current\nstate-of-the-art methods fail.",
          "link": "http://arxiv.org/abs/2106.11719",
          "publishedOn": "2021-06-23T01:48:41.632Z",
          "wordCount": 589,
          "title": "Active Learning under Pool Set Distribution Shift and Noisy Data. (arXiv:2106.11719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11531",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>",
          "description": "Routing methods in capsule networks often learn a hierarchical relationship\nfor capsules in successive layers, but the intra-relation between capsules in\nthe same layer is less studied, while this intra-relation is a key factor for\nthe semantic understanding in text data. Therefore, in this paper, we introduce\na new capsule network with graph routing to learn both relationships, where\ncapsules in each layer are treated as the nodes of a graph. We investigate\nstrategies to yield adjacency and degree matrix with three different distances\nfrom a layer of capsules, and propose the graph routing mechanism between those\ncapsules. We validate our approach on five text classification datasets, and\nour findings suggest that the approach combining bottom-up routing and top-down\nattention performs the best. Such an approach demonstrates generalization\ncapability across datasets. Compared to the state-of-the-art routing methods,\nthe improvements in accuracy in the five datasets we used were 0.82, 0.39,\n0.07, 1.01, and 0.02, respectively.",
          "link": "http://arxiv.org/abs/2106.11531",
          "publishedOn": "2021-06-23T01:48:41.626Z",
          "wordCount": 592,
          "title": "Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lydia T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_F/0/1/0/all/0/1\">Feng Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1\">Horia Mania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We study two-sided matching markets in which one side of the market (the\nplayers) does not have a priori knowledge about its preferences for the other\nside (the arms) and is required to learn its preferences from experience. Also,\nwe assume the players have no direct means of communication. This model extends\nthe standard stochastic multi-armed bandit framework to a decentralized\nmultiple player setting with competition. We introduce a new algorithm for this\nsetting that, over a time horizon $T$, attains $\\mathcal{O}(\\log(T))$ stable\nregret when preferences of the arms over players are shared, and\n$\\mathcal{O}(\\log(T)^2)$ regret when there are no assumptions on the\npreferences on either side. Moreover, in the setting where a single player may\ndeviate, we show that the algorithm is incentive compatible whenever the arms'\npreferences are shared, but not necessarily so when preferences are fully\ngeneral.",
          "link": "http://arxiv.org/abs/2012.07348",
          "publishedOn": "2021-06-23T01:48:41.619Z",
          "wordCount": 637,
          "title": "Bandit Learning in Decentralized Matching Markets. (arXiv:2012.07348v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>",
          "description": "We resolve the long-standing \"impossible tuning\" issue for the classic expert\nproblem and show that, it is in fact possible to achieve regret\n$O\\left(\\sqrt{(\\ln d)\\sum_t \\ell_{t,i}^2}\\right)$ simultaneously for all expert\n$i$ in a $T$-round $d$-expert problem where $\\ell_{t,i}$ is the loss for expert\n$i$ in round $t$. Our algorithm is based on the Mirror Descent framework with a\ncorrection term and a weighted entropy regularizer. While natural, the\nalgorithm has not been studied before and requires a careful analysis. We also\ngeneralize the bound to $O\\left(\\sqrt{(\\ln d)\\sum_t\n(\\ell_{t,i}-m_{t,i})^2}\\right)$ for any prediction vector $m_t$ that the\nlearner receives, and recover or improve many existing results by choosing\ndifferent $m_t$. Furthermore, we use the same framework to create a master\nalgorithm that combines a set of base algorithms and learns the best one with\nlittle overhead. The new guarantee of our master allows us to derive many new\nresults for both the expert problem and more generally Online Linear\nOptimization.",
          "link": "http://arxiv.org/abs/2102.01046",
          "publishedOn": "2021-06-23T01:48:41.612Z",
          "wordCount": 625,
          "title": "Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications. (arXiv:2102.01046v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10399",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we present a novel neural network to generate high resolution\nimages. We replace the decoder of VAE with a discriminator while using the\nencoder as it is. The encoder is fed data from a normal distribution while the\ngenerator is fed from a gaussian distribution. The combination from both is\ngiven to a discriminator which tells whether the generated image is correct or\nnot. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA\ndataset. Our network beats the previous state of the art using MMD, SSIM, log\nlikelihood, reconstruction error, ELBO and KL divergence as the evaluation\nmetrics while generating much sharper images. This work is potentially very\nexciting as we are able to combine the advantages of generative models and\ninference models in a principled bayesian manner.",
          "link": "http://arxiv.org/abs/2008.10399",
          "publishedOn": "2021-06-23T01:48:41.592Z",
          "wordCount": 622,
          "title": "Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1\">Mohammad Mamouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1\">Gholamreza Salimi-Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Shishir Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1\">Abdelaali Hassaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1\">Dexter Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1\">Kazem Rahimi</a>",
          "description": "Electronic health records represent a holistic overview of patients'\ntrajectories. Their increasing availability has fueled new hopes to leverage\nthem and develop accurate risk prediction models for a wide range of diseases.\nGiven the complex interrelationships of medical records and patient outcomes,\ndeep learning models have shown clear merits in achieving this goal. However, a\nkey limitation of these models remains their capacity in processing long\nsequences. Capturing the whole history of medical encounters is expected to\nlead to more accurate predictions, but the inclusion of records collected for\ndecades and from multiple resources can inevitably exceed the receptive field\nof the existing deep learning architectures. This can result in missing\ncrucial, long-term dependencies. To address this gap, we present Hi-BEHRT, a\nhierarchical Transformer-based model that can significantly expand the\nreceptive field of Transformers and extract associations from much longer\nsequences. Using a multimodal large-scale linked longitudinal electronic health\nrecords, the Hi-BEHRT exceeds the state-of-the-art BEHRT 1% to 5% for area\nunder the receiver operating characteristic (AUROC) curve and 3% to 6% for area\nunder the precision recall (AUPRC) curve on average, and 3% to 6% (AUROC) and\n3% to 11% (AUPRC) for patients with long medical history for 5-year heart\nfailure, diabetes, chronic kidney disease, and stroke risk prediction.\nAdditionally, because pretraining for hierarchical Transformer is not\nwell-established, we provide an effective end-to-end contrastive pre-training\nstrategy for Hi-BEHRT using EHR, improving its transferability on predicting\nclinical events with relatively small training dataset.",
          "link": "http://arxiv.org/abs/2106.11360",
          "publishedOn": "2021-06-23T01:48:41.584Z",
          "wordCount": 696,
          "title": "Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. (arXiv:2106.11360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1\">Atsushi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1\">Hajime Asama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinkyoo Park</a>",
          "description": "We introduce the framework of continuous--depth graph neural networks (GNNs).\nGraph neural ordinary differential equations (GDEs) are formalized as the\ncounterpart to GNNs where the input-output relationship is determined by a\ncontinuum of GNN layers, blending discrete topological structures and\ndifferential equations. The proposed framework is shown to be compatible with\nvarious static and autoregressive GNN models. Results prove general\neffectiveness of GDEs: in static settings they offer computational advantages\nby incorporating numerical methods in their forward pass; in dynamic settings,\non the other hand, they are shown to improve performance by exploiting the\ngeometry of the underlying dynamics.",
          "link": "http://arxiv.org/abs/1911.07532",
          "publishedOn": "2021-06-23T01:48:41.576Z",
          "wordCount": 601,
          "title": "Graph Neural Ordinary Differential Equations. (arXiv:1911.07532v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1\">Nicole Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>",
          "description": "Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.",
          "link": "http://arxiv.org/abs/2101.11453",
          "publishedOn": "2021-06-23T01:48:41.568Z",
          "wordCount": 634,
          "title": "Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Trisha Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>",
          "description": "The advent of social media platforms has been a catalyst for the development\nof digital photography that engendered a boom in vision applications. With this\nmotivation, we introduce a large-scale dataset termed 'Photozilla', which\nincludes over 990k images belonging to 10 different photographic styles. The\ndataset is then used to train 3 classification models to automatically classify\nthe images into the relevant style which resulted in an accuracy of ~96%. With\nthe rapid evolution of digital photography, we have seen new types of\nphotography styles emerging at an exponential rate. On that account, we present\na novel Siamese-based network that uses the trained classification models as\nthe base architecture to adapt and classify unseen styles with only 25 training\nsamples. We report an accuracy of over 68% for identifying 10 other distinct\ntypes of photography styles. This dataset can be found at\nhttps://trisha025.github.io/Photozilla/",
          "link": "http://arxiv.org/abs/2106.11359",
          "publishedOn": "2021-06-23T01:48:41.559Z",
          "wordCount": 617,
          "title": "Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.12873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1\">Praneeth Narayanamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1\">Namrata Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1\">Aditya Ramamoorthy</a>",
          "description": "Subspace tracking (ST) with missing data (ST-miss) or outliers (Robust ST) or\nboth (Robust ST-miss) has been extensively studied in the last many years. This\nwork provides a new simple algorithm and guarantee for both ST with missing\ndata (ST-miss) and RST-miss. Unlike past work on this topic, the algorithm is\nmuch simpler (uses fewer parameters) and the guarantee does not make the\nartificial assumption of piecewise constant subspace change, although it still\nhandles that setting. Secondly, we extend our approach and its analysis to\nprovably solving these problems when the raw data is federated and when the\nover-air data communication modality is used for information exchange between\nthe $K$ peer nodes and the center.",
          "link": "http://arxiv.org/abs/2002.12873",
          "publishedOn": "2021-06-23T01:48:41.549Z",
          "wordCount": 616,
          "title": "Federated Over-Air Subspace Tracking from Incomplete and Corrupted Data. (arXiv:2002.12873v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.00482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>",
          "description": "In this paper, we study the problem of estimating smooth Generalized Linear\nModels (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.\nDifferent from its classical setting, our model allows the server to access\nsome additional public but unlabeled data. By using Stein's lemma and its\nvariants, we first show that there is an $(\\epsilon, \\delta)$-NLDP algorithm\nfor GLM (under some mild assumptions), if each data record is i.i.d sampled\nfrom some sub-Gaussian distribution with bounded $\\ell_1$-norm. Then with high\nprobability, the sample complexity of the public and private data, for the\nalgorithm to achieve an $\\alpha$ estimation error (in $\\ell_\\infty$-norm), is\n$O(p^2\\alpha^{-2})$ and ${O}(p^2\\alpha^{-2}\\epsilon^{-2})$, respectively, if\n$\\alpha$ is not too small ({\\em i.e.,} $\\alpha\\geq\n\\Omega(\\frac{1}{\\sqrt{p}})$), where $p$ is the dimensionality of the data. This\nis a significant improvement over the previously known quasi-polynomial (in\n$\\alpha$) or exponential (in $p$) complexity of GLM with no public data. Also,\nour algorithm can answer multiple (at most $\\exp(O(p))$) GLM queries with the\nsame sample complexities as in the one GLM query case with at least constant\nprobability. We then extend our idea to the non-linear regression problem and\nshow a similar phenomenon for it. Finally, we demonstrate the effectiveness of\nour algorithms through experiments on both synthetic and real world datasets.\nTo our best knowledge, this is the first paper showing the existence of\nefficient and effective algorithms for GLM and non-linear regression in the\nNLDP model with public unlabeled data.",
          "link": "http://arxiv.org/abs/1910.00482",
          "publishedOn": "2021-06-23T01:48:41.527Z",
          "wordCount": 728,
          "title": "Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data. (arXiv:1910.00482v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.01635",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Shustin_B/0/1/0/all/0/1\">Boris Shustin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Avron_H/0/1/0/all/0/1\">Haim Avron</a>",
          "description": "Optimization problems on the generalized Stiefel manifold (and products of\nit) are prevalent across science and engineering. For example, in computational\nscience they arise in the symmetric (generalized) eigenvalue problem, in\nnonlinear eigenvalue problems, and in electronic structures computations, to\nname a few problems. In statistics and machine learning, they arise, for\nexample, in various dimensionality reduction techniques such as canonical\ncorrelation analysis. In deep learning, regularization and improved stability\ncan be obtained by constraining some layers to have parameter matrices that\nbelong to the Stiefel manifold. Solving problems on the generalized Stiefel\nmanifold can be approached via the tools of Riemannian optimization. However,\nusing the standard geometric components for the generalized Stiefel manifold\nhas two possible shortcoming: computing some of the geometric components can be\ntoo expensive and converge can be rather slow in certain cases. Both\nshortcomings can be addressed using a technique called Riemannian\npreconditioning, which amounts to using geometric components derived using a\nprecoditioner that defines a Riemannian metric on the constraint manifold. In\nthis paper we develop the geometric components required to perform Riemannian\noptimization on the generalized Stiefel manifold equipped with a non-standard\nmetric, and illustrate theoretically and numerically the use of those\ncomponents and the effect of Riemannian preconditioning for solving\noptimization problems on the generalized Stiefel manifold.",
          "link": "http://arxiv.org/abs/1902.01635",
          "publishedOn": "2021-06-23T01:48:41.520Z",
          "wordCount": 678,
          "title": "Preconditioned Riemannian Optimization on the Generalized Stiefel Manifold. (arXiv:1902.01635v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1\">Yury Gorishniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1\">Ivan Rubachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1\">Valentin Khrulkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>",
          "description": "The necessity of deep learning for tabular data is still an unanswered\nquestion addressed by a large number of research efforts. The recent literature\non tabular DL proposes several deep architectures reported to be superior to\ntraditional \"shallow\" models like Gradient Boosted Decision Trees. However,\nsince existing works often use different benchmarks and tuning protocols, it is\nunclear if the proposed models universally outperform GBDT. Moreover, the\nmodels are often not compared to each other, therefore, it is challenging to\nidentify the best deep model for practitioners.\n\nIn this work, we start from a thorough review of the main families of DL\nmodels recently developed for tabular data. We carefully tune and evaluate them\non a wide range of datasets and reveal two significant findings. First, we show\nthat the choice between GBDT and DL models highly depends on data and there is\nstill no universally superior solution. Second, we demonstrate that a simple\nResNet-like architecture is a surprisingly effective baseline, which\noutperforms most of the sophisticated models from the DL literature. Finally,\nwe design a simple adaptation of the Transformer architecture for tabular data\nthat becomes a new strong DL baseline and reduces the gap between GBDT and DL\nmodels on datasets where GBDT dominates.",
          "link": "http://arxiv.org/abs/2106.11959",
          "publishedOn": "2021-06-23T01:48:41.510Z",
          "wordCount": 639,
          "title": "Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxin Yang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xitong Zhang</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qiang Guan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youzuo Lin</a> (1) ((1) Earth and Environmental Sciences Division, Los Alamos National Laboratory, (2) Department of Computer Science, Kent State University, (3) Department of Computational Mathematics, Science and Engineering, Michigan State University)",
          "description": "Deep learning and data-driven approaches have shown great potential in\nscientific domains. The promise of data-driven techniques relies on the\navailability of a large volume of high-quality training datasets. Due to the\nhigh cost of obtaining data through expensive physical experiments,\ninstruments, and simulations, data augmentation techniques for scientific\napplications have emerged as a new direction for obtaining scientific data\nrecently. However, existing data augmentation techniques originating from\ncomputer vision, yield physically unacceptable data samples that are not\nhelpful for the domain problems that we are interested in. In this paper, we\ndevelop new physics-informed data augmentation techniques based on\nconvolutional neural networks. Specifically, our generative models leverage\ndifferent physics knowledge (such as governing equations, observable\nperception, and physics phenomena) to improve the quality of the synthetic\ndata. To validate the effectiveness of our data augmentation techniques, we\napply them to solve a subsurface seismic full-waveform inversion using\nsimulated CO$_2$ leakage data. Our interest is to invert for subsurface\nvelocity models associated with very small CO$_2$ leakage. We validate the\nperformance of our methods using comprehensive numerical tests. Via comparison\nand analysis, we show that data-driven seismic imaging can be significantly\nenhanced by using our physics-informed data augmentation techniques.\nParticularly, the imaging quality has been improved by 15% in test scenarios of\ngeneral-sized leakage and 17% in small-sized leakage when using an augmented\ntraining set obtained with our techniques.",
          "link": "http://arxiv.org/abs/2106.11892",
          "publishedOn": "2021-06-23T01:48:41.503Z",
          "wordCount": 713,
          "title": "Making Invisible Visible: Data-Driven Seismic Inversion with Physics-Informed Data Augmentation. (arXiv:2106.11892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1\">Dilip Arumugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "Agents that learn to select optimal actions represent a prominent focus of\nthe sequential decision-making literature. In the face of a complex environment\nor constraints on time and resources, however, aiming to synthesize such an\noptimal policy can become infeasible. These scenarios give rise to an important\ntrade-off between the information an agent must acquire to learn and the\nsub-optimality of the resulting policy. While an agent designer has a\npreference for how this trade-off is resolved, existing approaches further\nrequire that the designer translate these preferences into a fixed learning\ntarget for the agent. In this work, leveraging rate-distortion theory, we\nautomate this process such that the designer need only express their\npreferences via a single hyperparameter and the agent is endowed with the\nability to compute its own learning targets that best achieve the desired\ntrade-off. We establish a general bound on expected discounted regret for an\nagent that decides what to learn in this manner along with computational\nexperiments that illustrate the expressiveness of designer preferences and even\nshow improvements over Thompson sampling in identifying an optimal policy.",
          "link": "http://arxiv.org/abs/2101.06197",
          "publishedOn": "2021-06-23T01:48:41.494Z",
          "wordCount": 648,
          "title": "Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1\">Orchid Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1\">Marzia Polito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.",
          "link": "http://arxiv.org/abs/2101.11058",
          "publishedOn": "2021-06-23T01:48:41.468Z",
          "wordCount": 634,
          "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.03979",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1\">Wei Biao Wu</a>",
          "description": "The stochastic gradient descent (SGD) algorithm is widely used for parameter\nestimation, especially for huge data sets and online learning. While this\nrecursive algorithm is popular for computation and memory efficiency,\nquantifying variability and randomness of the solutions has been rarely\nstudied. This paper aims at conducting statistical inference of SGD-based\nestimates in an online setting. In particular, we propose a fully online\nestimator for the covariance matrix of averaged SGD iterates (ASGD) only using\nthe iterates from SGD. We formally establish our online estimator's consistency\nand show that the convergence rate is comparable to offline counterparts. Based\non the classic asymptotic normality results of ASGD, we construct\nasymptotically valid confidence intervals for model parameters. Upon receiving\nnew observations, we can quickly update the covariance matrix estimate and the\nconfidence intervals. This approach fits in an online setting and takes full\nadvantage of SGD: efficiency in computation and memory.",
          "link": "http://arxiv.org/abs/2002.03979",
          "publishedOn": "2021-06-23T01:48:41.459Z",
          "wordCount": 608,
          "title": "Online Covariance Matrix Estimation in Stochastic Gradient Descent. (arXiv:2002.03979v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Redefined_A/0/1/0/all/0/1\">AI Redefined</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1\">Sai Krishna Gottipati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurandwad_S/0/1/0/all/0/1\">Sagar Kurandwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1\">Clod&#xe9;ric Mars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szriftgiser_G/0/1/0/all/0/1\">Gregory Szriftgiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1\">Fran&#xe7;ois Chabot</a>",
          "description": "Involving humans directly for the benefit of AI agents' training is getting\ntraction thanks to several advances in reinforcement learning and\nhuman-in-the-loop learning. Humans can provide rewards to the agent,\ndemonstrate tasks, design a curriculum, or act in the environment, but these\nbenefits also come with architectural, functional design and engineering\ncomplexities. We present Cogment, a unifying open-source framework that\nintroduces an actor formalism to support a variety of humans-agents\ncollaboration typologies and training approaches. It is also scalable out of\nthe box thanks to a distributed micro service architecture, and offers\nsolutions to the aforementioned complexities.",
          "link": "http://arxiv.org/abs/2106.11345",
          "publishedOn": "2021-06-23T01:48:41.451Z",
          "wordCount": 559,
          "title": "Cogment: Open Source Framework For Distributed Multi-actor Training, Deployment & Operations. (arXiv:2106.11345v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/1903.12561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shaokai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1\">Jan-Henrik Lambrechts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",
          "link": "http://arxiv.org/abs/1903.12561",
          "publishedOn": "2021-06-23T01:48:41.444Z",
          "wordCount": 666,
          "title": "Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moondra_J/0/1/0/all/0/1\">Jai Moondra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortagy_H/0/1/0/all/0/1\">Hassan Mortagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Swati Gupta</a>",
          "description": "Optimization algorithms such as projected Newton's method, FISTA, mirror\ndescent and its variants enjoy near-optimal regret bounds and convergence\nrates, but suffer from a computational bottleneck of computing \"projections''\nin potentially each iteration (e.g., $O(T^{1/2})$ regret of online mirror\ndescent). On the other hand, conditional gradient variants solve a linear\noptimization in each iteration, but result in suboptimal rates (e.g.,\n$O(T^{3/4})$ regret of online Frank-Wolfe). Motivated by this trade-off in\nruntime v/s convergence rates, we consider iterative projections of close-by\npoints over widely-prevalent submodular base polytopes $B(f)$. We develop a\ntoolkit to speed up the computation of projections using both discrete and\ncontinuous perspectives. We subsequently adapt the away-step Frank-Wolfe\nalgorithm to use this information and enable early termination. For the special\ncase of cardinality based submodular polytopes, we improve the runtime of\ncomputing certain Bregman projections by a factor of $\\Omega(n/\\log(n))$. Our\ntheoretical results show orders of magnitude reduction in runtime in\npreliminary computational experiments.",
          "link": "http://arxiv.org/abs/2106.11943",
          "publishedOn": "2021-06-23T01:48:41.436Z",
          "wordCount": 594,
          "title": "Reusing Combinatorial Structure: Faster Iterative Projections over Submodular Base Polytopes. (arXiv:2106.11943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.02684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jieming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengzhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>",
          "description": "Graph neural networks (GNNs) are designed for semi-supervised node\nclassification on graphs where only a small subset of nodes have class labels.\nHowever, under extreme cases when very few labels are available (e.g., 1\nlabeled node per class), GNNs suffer from severe result quality degradation.\nSeveral existing studies make an initial effort to ease this situation, but are\nstill far from satisfactory.\n\nIn this paper, on few-labeled graph data, we propose an effective framework\nABN that is readily applicable to both shallow and deep GNN architectures and\nsignificantly boosts classification accuracy. In particular, on a benchmark\ndataset Cora with only 1 labeled node per class, while the classic graph\nconvolutional network (GCN) only has 44.6% accuracy, an immediate instantiation\nof ABN over GCN achieves 62.5% accuracy; when applied to a deep architecture\nDAGNN, ABN improves accuracy from 59.8% to 66.4%, which is state of the art.\n\nABN obtains superior performance through three main algorithmic designs.\nFirst, it selects high-quality unlabeled nodes via an adaptive pseudo labeling\ntechnique, so as to adaptively enhance the training process of GNNs. Second,\nABN balances the labels of the selected nodes on real-world skewed graph data\nby pseudo label balancing. Finally, a negative sampling regularizer is designed\nfor ABN to further utilize the unlabeled nodes. The effectiveness of the three\ntechniques in ABN is well-validated by both theoretical and empirical analysis.\nExtensive experiments, comparing 12 existing approaches on 4 benchmark\ndatasets, demonstrate that ABN achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/1910.02684",
          "publishedOn": "2021-06-23T01:48:41.416Z",
          "wordCount": 707,
          "title": "Effective Semi-Supervised Node Classification on Few-Labeled Graph Data. (arXiv:1910.02684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1\">David Eriksson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce I-Jen Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daulton_S/0/1/0/all/0/1\">Sam Daulton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Peng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shicong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1\">Maximilian Balandat</a>",
          "description": "When tuning the architecture and hyperparameters of large machine learning\nmodels for on-device deployment, it is desirable to understand the optimal\ntrade-offs between on-device latency and model accuracy. In this work, we\nleverage recent methodological advances in Bayesian optimization over\nhigh-dimensional search spaces and multi-objective Bayesian optimization to\nefficiently explore these trade-offs for a production-scale on-device natural\nlanguage understanding model at Facebook.",
          "link": "http://arxiv.org/abs/2106.11890",
          "publishedOn": "2021-06-23T01:48:41.408Z",
          "wordCount": 519,
          "title": "Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. (arXiv:2106.11890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1\">Wilkie Olin-Ammentorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1\">Maxim Bazhenov</a>",
          "description": "In this work, we extend standard neural networks by building upon an\nassumption that neuronal activations correspond to the angle of a complex\nnumber lying on the unit circle, or 'phasor.' Each layer in such a network\nproduces new activations by taking a weighted superposition of the previous\nlayer's phases and calculating the new phase value. This generalized\narchitecture allows models to reach high accuracy and carries the singular\nadvantage that mathematically equivalent versions of the network can be\nexecuted with or without regard to a temporal variable. Importantly, the value\nof a phase angle in the temporal domain can be sparsely represented by a\nperiodically repeating series of delta functions or 'spikes'. We demonstrate\nthe atemporal training of a phasor network on standard deep learning tasks and\nshow that these networks can then be executed in either the traditional\natemporal domain or spiking temporal domain with no conversion step needed.\nThis provides a novel basis for constructing deep networkswhich operate via\ntemporal, spike-based calculations suitable for neuromorphic computing\nhardware.",
          "link": "http://arxiv.org/abs/2106.11908",
          "publishedOn": "2021-06-23T01:48:41.402Z",
          "wordCount": 610,
          "title": "Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks. (arXiv:2106.11908v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1\">I-Chung Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Recent advances in protecting node privacy on graph data and attacking graph\nneural networks (GNNs) gain much attention. The eye does not bring these two\nessential tasks together yet. Imagine an adversary can utilize the powerful\nGNNs to infer users' private labels in a social network. How can we\nadversarially defend against such privacy attacks while maintaining the utility\nof perturbed graphs? In this work, we propose a novel research task,\nadversarial defenses against GNN-based privacy attacks, and present a graph\nperturbation-based approach, NetFense, to achieve the goal. NetFense can\nsimultaneously keep graph data unnoticeability (i.e., having limited changes on\nthe graph structure), maintain the prediction confidence of targeted label\nclassification (i.e., preserving data utility), and reduce the prediction\nconfidence of private label classification (i.e., protecting the privacy of\nnodes). Experiments conducted on single- and multiple-target perturbations\nusing three real graph data exhibit that the perturbed graphs by NetFense can\neffectively maintain data utility (i.e., model unnoticeability) on targeted\nlabel classification and significantly decrease the prediction confidence of\nprivate label classification (i.e., privacy protection). Extensive studies also\nbring several insights, such as the flexibility of NetFense, preserving local\nneighborhoods in data unnoticeability, and better privacy protection for\nhigh-degree nodes.",
          "link": "http://arxiv.org/abs/2106.11865",
          "publishedOn": "2021-06-23T01:48:41.396Z",
          "wordCount": 670,
          "title": "NetFense: Adversarial Defenses against Privacy Attacks on Neural Networks for Graph Data. (arXiv:2106.11865v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1\">Pavel Izmailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholson_P/0/1/0/all/0/1\">Patrick Nicholson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_S/0/1/0/all/0/1\">Sanae Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "Approximate Bayesian inference for neural networks is considered a robust\nalternative to standard training, often providing good performance on\nout-of-distribution data. However, Bayesian neural networks (BNNs) with\nhigh-fidelity approximate inference via full-batch Hamiltonian Monte Carlo\nachieve poor generalization under covariate shift, even underperforming\nclassical estimation. We explain this surprising result, showing how a Bayesian\nmodel average can in fact be problematic under covariate shift, particularly in\ncases where linear dependencies in the input features cause a lack of posterior\ncontraction. We additionally show why the same issue does not affect many\napproximate inference procedures, or classical maximum a-posteriori (MAP)\ntraining. Finally, we propose novel priors that improve the robustness of BNNs\nto many sources of covariate shift.",
          "link": "http://arxiv.org/abs/2106.11905",
          "publishedOn": "2021-06-23T01:48:41.388Z",
          "wordCount": 553,
          "title": "Dangers of Bayesian Model Averaging under Covariate Shift. (arXiv:2106.11905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>",
          "description": "Temperature monitoring during the life time of heat-source components in\nengineering systems becomes essential to ensure the normal work and even the\nlong working life of the heat sources. However, prior methods, which mainly use\nthe interpolate estimation, require large amounts of temperature tensors for an\naccurate estimation. To solve this problem, this work develops a novel\nphysics-informed deep surrogate models for temperature field reconstruction.\nFirst, we defines the temperature field reconstruction task of heat-source\nsystems. Then, this work develops the deep surrogate model mapping for the\nproposed task. Finally, considering the physical properties of heat transfer,\nthis work proposes four different losses and joint learns the deep surrogate\nmodel with these losses. Experimental studies have conducted over typical\ntwo-dimensional heat-source systems to demonstrate the effectiveness and\nefficiency of the proposed physics-informed deep surrogate models for\ntemperature field reconstruction.",
          "link": "http://arxiv.org/abs/2106.11929",
          "publishedOn": "2021-06-23T01:48:41.368Z",
          "wordCount": 582,
          "title": "Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Y/0/1/0/all/0/1\">Yousef Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zechen Zhang</a>",
          "description": "The general method of graph coarsening or graph reduction has been a\nremarkably useful and ubiquitous tool in scientific computing and it is now\njust starting to have a similar impact in machine learning. The goal of this\npaper is to take a broad look into coarsening techniques that have been\nsuccessfully deployed in scientific computing and see how similar principles\nare finding their way in more recent applications related to machine learning.\nIn scientific computing, coarsening plays a central role in algebraic multigrid\nmethods as well as the related class of multilevel incomplete LU\nfactorizations. In machine learning, graph coarsening goes under various names,\ne.g., graph downsampling or graph reduction. Its goal in most cases is to\nreplace some original graph by one which has fewer nodes, but whose structure\nand characteristics are similar to those of the original graph. As will be\nseen, a common strategy in these methods is to rely on spectral properties to\ndefine the coarse graph.",
          "link": "http://arxiv.org/abs/2106.11863",
          "publishedOn": "2021-06-23T01:48:41.361Z",
          "wordCount": 601,
          "title": "Graph coarsening: From scientific computing to machine learning. (arXiv:2106.11863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1706.07180",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a> (PANAMA, DANTE), <a href=\"http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (DATASHAPE, LMO), <a href=\"http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a> (PANAMA, GIPSA-GAIA), <a href=\"http://arxiv.org/find/stat/1/au:+Traonmilin_Y/0/1/0/all/0/1\">Yann Traonmilin</a> (PANAMA, IMB)",
          "description": "We describe a general framework -- compressive statistical learning -- for\nresource-efficient large-scale learning: the training collection is compressed\nin one pass into a low-dimensional sketch (a vector of random empirical\ngeneralized moments) that captures the information relevant to the considered\nlearning task. A near-minimizer of the risk is computed from the sketch through\nthe solution of a nonlinear least squares problem. We investigate sufficient\nsketch sizes to control the generalization error of this procedure. The\nframework is illustrated on compressive PCA, compressive clustering, and\ncompressive Gaussian mixture Modeling with fixed known variance. The latter two\nare further developed in a companion paper.",
          "link": "http://arxiv.org/abs/1706.07180",
          "publishedOn": "2021-06-23T01:48:41.355Z",
          "wordCount": 666,
          "title": "Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11918",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pekpe_K/0/1/0/all/0/1\">Komi Midzodzi P&#xe9;kp&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zitouni_D/0/1/0/all/0/1\">Djamel Zitouni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gasso_G/0/1/0/all/0/1\">Gilles Gasso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dhifli_W/0/1/0/all/0/1\">Wajdi Dhifli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guinhouya_B/0/1/0/all/0/1\">Benjamin C. Guinhouya</a>",
          "description": "Common compartmental modeling for COVID-19 is based on a priori knowledge and\nnumerous assumptions. Additionally, they do not systematically incorporate\nasymptomatic cases. Our study aimed at providing a framework for data-driven\napproaches, by leveraging the strengths of the grey-box system theory or\ngrey-box identification, known for its robustness in problem solving under\npartial, incomplete, or uncertain data. Empirical data on confirmed cases and\ndeaths, extracted from an open source repository were used to develop the\nSEAIRD compartment model. Adjustments were made to fit current knowledge on the\nCOVID-19 behavior. The model was implemented and solved using an Ordinary\nDifferential Equation solver and an optimization tool. A cross-validation\ntechnique was applied, and the coefficient of determination $R^2$ was computed\nin order to evaluate the goodness-of-fit of the model. %to the data. Key\nepidemiological parameters were finally estimated and we provided the rationale\nfor the construction of SEAIRD model. When applied to Brazil's cases, SEAIRD\nproduced an excellent agreement to the data, with an %coefficient of\ndetermination $R^2$ $\\geq 90\\%$. The probability of COVID-19 transmission was\ngenerally high ($\\geq 95\\%$). On the basis of a 20-day modeling data, the\nincidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed\npersons in Brazil and France. Within the same time frame, the fatality rate of\nCOVID-19 was the highest in France (16.4\\%) followed by Brazil (6.9\\%), and the\nlowest in Russia ($\\leq 1\\%$). SEAIRD represents an asset for modeling\ninfectious diseases in their dynamical stable phase, especially for new viruses\nwhen pathophysiology knowledge is very limited.",
          "link": "http://arxiv.org/abs/2106.11918",
          "publishedOn": "2021-06-23T01:48:41.347Z",
          "wordCount": 763,
          "title": "From SIR to SEAIRD: a novel data-driven modeling approach based on the Grey-box System Theory to predict the dynamics of COVID-19. (arXiv:2106.11918v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11879",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cohen_A/0/1/0/all/0/1\">Alon Cohen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Daniely_A/0/1/0/all/0/1\">Amit Daniely</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drori_Y/0/1/0/all/0/1\">Yoel Drori</a>, <a href=\"http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schain_M/0/1/0/all/0/1\">Mariano Schain</a>",
          "description": "We consider stochastic optimization with delayed gradients where, at each\ntime step $t$, the algorithm makes an update using a stale stochastic gradient\nfrom step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts\nasynchronous distributed optimization where a central server receives gradient\nupdates computed by worker machines. These machines can experience computation\nand communication loads that might vary significantly over time. In the general\nnon-convex smooth optimization setting, we give a simple and efficient\nalgorithm that requires $O( \\sigma^2/\\epsilon^4 + \\tau/\\epsilon^2 )$ steps for\nfinding an $\\epsilon$-stationary point $x$, where $\\tau$ is the \\emph{average}\ndelay $\\smash{\\frac{1}{T}\\sum_{t=1}^T d_t}$ and $\\sigma^2$ is the variance of\nthe stochastic gradients. This improves over previous work, which showed that\nstochastic gradient decent achieves the same rate but with respect to the\n\\emph{maximal} delay $\\max_{t} d_t$, that can be significantly larger than the\naverage delay especially in heterogeneous distributed systems. Our experiments\ndemonstrate the efficacy and robustness of our algorithm in cases where the\ndelay distribution is skewed or heavy-tailed.",
          "link": "http://arxiv.org/abs/2106.11879",
          "publishedOn": "2021-06-23T01:48:41.338Z",
          "wordCount": 602,
          "title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays. (arXiv:2106.11879v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1\">Abraham George Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1\">Cynthia Terrones-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1\">Anne Kiil Berthelsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1\">Nora Jarrett Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1\">Lena Specht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1\">Ivan Richter Vogelius</a>",
          "description": "Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n\nSource code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.",
          "link": "http://arxiv.org/abs/2106.11942",
          "publishedOn": "2021-06-23T01:48:41.317Z",
          "wordCount": 618,
          "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiafan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "The success of deep reinforcement learning (DRL) is due to the power of\nlearning a representation that is suitable for the underlying exploration and\nexploitation task. However, existing provable reinforcement learning algorithms\nwith linear function approximation often assume the feature representation is\nknown and fixed. In order to understand how representation learning can improve\nthe efficiency of RL, we study representation learning for a class of low-rank\nMarkov Decision Processes (MDPs) where the transition kernel can be represented\nin a bilinear form. We propose a provably efficient algorithm called ReLEX that\ncan simultaneously learn the representation and perform exploration. We show\nthat ReLEX always performs no worse than a state-of-the-art algorithm without\nrepresentation learning, and will be strictly better in terms of sample\nefficiency if the function class of representations enjoys a certain mild\n\"coverage'' property over the whole state-action space.",
          "link": "http://arxiv.org/abs/2106.11935",
          "publishedOn": "2021-06-23T01:48:41.309Z",
          "wordCount": 589,
          "title": "Provably Efficient Representation Learning in Low-rank Markov Decision Processes. (arXiv:2106.11935v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimanov_D/0/1/0/all/0/1\">Daniel Dimanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaguer_Ballester_E/0/1/0/all/0/1\">Emili Balaguer-Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singleton_C/0/1/0/all/0/1\">Colin Singleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_S/0/1/0/all/0/1\">Shahin Rostami</a>",
          "description": "In this paper, we present a novel neuroevolutionary method to identify the\narchitecture and hyperparameters of convolutional autoencoders. Remarkably, we\nused a hypervolume indicator in the context of neural architecture search for\nautoencoders, for the first time to our current knowledge. Results show that\nimages were compressed by a factor of more than 10, while still retaining\nenough information to achieve image classification for the majority of the\ntasks. Thus, this new approach can be used to speed up the AutoML pipeline for\nimage compression.",
          "link": "http://arxiv.org/abs/2106.11914",
          "publishedOn": "2021-06-23T01:48:41.303Z",
          "wordCount": 531,
          "title": "MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders. (arXiv:2106.11914v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1\">D. J. Hand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1\">C. Anagnostopoulos</a>",
          "description": "The H-measure is a classifier performance measure which takes into account\nthe context of application without requiring a rigid value of relative\nmisclassification costs to be set. Since its introduction in 2009 it has become\nwidely adopted. This paper answers various queries which users have raised\nsince its introduction, including questions about its interpretation, the\nchoice of a weighting function, whether it is strictly proper, and its\ncoherence, and relates the measure to other work.",
          "link": "http://arxiv.org/abs/2106.11888",
          "publishedOn": "2021-06-23T01:48:41.293Z",
          "wordCount": 502,
          "title": "Notes on the H-measure of classifier performance. (arXiv:2106.11888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1\">Sarah M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohr_A/0/1/0/all/0/1\">Alexander von Rohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>",
          "description": "Reinforcement learning (RL) aims to find an optimal policy by interaction\nwith an environment. Consequently, learning complex behavior requires a vast\nnumber of samples, which can be prohibitive in practice. Nevertheless, instead\nof systematically reasoning and actively choosing informative samples, policy\ngradients for local search are often obtained from random perturbations. These\nrandom samples yield high variance estimates and hence are sub-optimal in terms\nof sample complexity. Actively selecting informative samples is at the core of\nBayesian optimization, which constructs a probabilistic surrogate of the\nobjective from past samples to reason about informative subsequent ones. In\nthis paper, we propose to join both worlds. We develop an algorithm utilizing a\nprobabilistic model of the objective function and its gradient. Based on the\nmodel, the algorithm decides where to query a noisy zeroth-order oracle to\nimprove the gradient estimates. The resulting algorithm is a novel type of\npolicy search method, which we compare to existing black-box algorithms. The\ncomparison reveals improved sample complexity and reduced variance in extensive\nempirical evaluations on synthetic objectives. Further, we highlight the\nbenefits of active sampling on popular RL benchmarks.",
          "link": "http://arxiv.org/abs/2106.11899",
          "publishedOn": "2021-06-23T01:48:41.286Z",
          "wordCount": 615,
          "title": "Local policy search with Bayesian optimization. (arXiv:2106.11899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Beining Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhizhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuofan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jian Peng</a>",
          "description": "We study deep reinforcement learning (RL) algorithms with delayed rewards. In\nmany real-world tasks, instant rewards are often not readily accessible or even\ndefined immediately after the agent performs actions. In this work, we first\nformally define the environment with delayed rewards and discuss the challenges\nraised due to the non-Markovian nature of such environments. Then, we introduce\na general off-policy RL framework with a new Q-function formulation that can\nhandle the delayed rewards with theoretical convergence guarantees. For\npractical tasks with high dimensional state spaces, we further introduce the\nHC-decomposition rule of the Q-function in our framework which naturally leads\nto an approximation scheme that helps boost the training efficiency and\nstability. We finally conduct extensive experiments to demonstrate the superior\nperformance of our algorithms over the existing work and their variants.",
          "link": "http://arxiv.org/abs/2106.11854",
          "publishedOn": "2021-06-23T01:48:41.278Z",
          "wordCount": 564,
          "title": "Off-Policy Reinforcement Learning with Delayed Rewards. (arXiv:2106.11854v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callcut_R/0/1/0/all/0/1\">Rachael Callcut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>",
          "description": "Multiple organ failure (MOF) is a severe syndrome with a high mortality rate\namong Intensive Care Unit (ICU) patients. Early and precise detection is\ncritical for clinicians to make timely decisions. An essential challenge in\napplying machine learning models to electronic health records (EHRs) is the\npervasiveness of missing values. Most existing imputation methods are involved\nin the data preprocessing phase, failing to capture the relationship between\ndata and outcome for downstream predictions. In this paper, we propose\nclassifier-guided generative adversarial imputation networks Classifier-GAIN)\nfor MOF prediction to bridge this gap, by incorporating both observed data and\nlabel information. Specifically, the classifier takes imputed values from the\ngenerator(imputer) to predict task outcomes and provides additional supervision\nsignals to the generator by joint training. The classifier-guide generator\nimputes missing values with label-awareness during training, improving the\nclassifier's performance during inference. We conduct extensive experiments\nshowing that our approach consistently outperforms classical and state-of-art\nneural baselines across a range of missing data scenarios and evaluation\nmetrics.",
          "link": "http://arxiv.org/abs/2106.11878",
          "publishedOn": "2021-06-23T01:48:41.247Z",
          "wordCount": 603,
          "title": "Multiple Organ Failure Prediction with Classifier-Guided Generative Adversarial Imputation Networks. (arXiv:2106.11878v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11849",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Agarwal_N/0/1/0/all/0/1\">Nikita Agarwal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeitler_J/0/1/0/all/0/1\">Jakob Zeitler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mastouri_A/0/1/0/all/0/1\">Afsaneh Mastouri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "Algorithmic recourse aims to provide actionable recommendations to\nindividuals to obtain a more favourable outcome from an automated\ndecision-making system. As it involves reasoning about interventions performed\nin the physical world, recourse is fundamentally a causal problem. Existing\nmethods compute the effect of recourse actions using a causal model learnt from\ndata under the assumption of no hidden confounding and modelling assumptions\nsuch as additive noise. Building on the seminal work of Balke and Pearl (1994),\nwe propose an alternative approach for discrete random variables which relaxes\nthese assumptions and allows for unobserved confounding and arbitrary\nstructural equations. The proposed approach only requires specification of the\ncausal graph and confounding structure and bounds the expected counterfactual\neffect of recourse actions. If the lower bound is above a certain threshold,\ni.e., on the other side of the decision boundary, recourse is guaranteed in\nexpectation.",
          "link": "http://arxiv.org/abs/2106.11849",
          "publishedOn": "2021-06-23T01:48:41.236Z",
          "wordCount": 600,
          "title": "Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. (arXiv:2106.11849v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>",
          "description": "Deep neural networks have reached very high accuracy on object detection but\ntheir success hinges on large amounts of labeled data. To reduce the dependency\non labels, various active-learning strategies have been proposed, typically\nbased on the confidence of the detector. However, these methods are biased\ntowards best-performing classes and can lead to acquired datasets that are not\ngood representatives of the data in the testing set. In this work, we propose a\nunified framework for active learning, that considers both the uncertainty and\nthe robustness of the detector, ensuring that the network performs accurately\nin all classes. Furthermore, our method is able to pseudo-label the very\nconfident predictions, suppressing a potential distribution drift while further\nboosting the performance of the model. Experiments show that our method\ncomprehensively outperforms a wide range of active-learning methods on PASCAL\nVOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%\nreduction in labeling cost.",
          "link": "http://arxiv.org/abs/2106.11921",
          "publishedOn": "2021-06-23T01:48:41.227Z",
          "wordCount": 601,
          "title": "Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11881",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Vlantis_P/0/1/0/all/0/1\">Panagiotis Vlantis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zavlanos_M/0/1/0/all/0/1\">Michael M. Zavlanos</a>",
          "description": "In this work, we consider the problem of learning a feed-forward neural\nnetwork (NN) controller to safely steer an arbitrarily shaped planar robot in a\ncompact and obstacle-occluded workspace. Unlike existing methods that depend\nstrongly on the density of data points close to the boundary of the safe state\nspace to train NN controllers with closed-loop safety guarantees, we propose an\napproach that lifts such assumptions on the data that are hard to satisfy in\npractice and instead allows for graceful safety violations, i.e., of a bounded\nmagnitude that can be spatially controlled. To do so, we employ reachability\nanalysis methods to encapsulate safety constraints in the training process.\nSpecifically, to obtain a computationally efficient over-approximation of the\nforward reachable set of the closed-loop system, we partition the robot's state\nspace into cells and adaptively subdivide the cells that contain states which\nmay escape the safe set under the trained control law. To do so, we first\ndesign appropriate under- and over-approximations of the robot's footprint to\nadaptively subdivide the configuration space into cells. Then, using the\noverlap between each cell's forward reachable set and the set of infeasible\nrobot configurations as a measure for safety violations, we introduce penalty\nterms into the loss function that penalize this overlap in the training\nprocess. As a result, our method can learn a safe vector field for the\nclosed-loop system and, at the same time, provide numerical worst-case bounds\non safety violation over the whole configuration space, defined by the overlap\nbetween the over-approximation of the forward reachable set of the closed-loop\nsystem and the set of unsafe states. Moreover, it can control the tradeoff\nbetween computational complexity and tightness of these bounds. Finally, we\nprovide a simulation study that verifies the efficacy of the proposed scheme.",
          "link": "http://arxiv.org/abs/2106.11881",
          "publishedOn": "2021-06-23T01:48:41.219Z",
          "wordCount": 742,
          "title": "Failing with Grace: Learning Neural Network Controllers that are Boundedly Unsafe. (arXiv:2106.11881v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1\">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bru_A/0/1/0/all/0/1\">Antonio Br&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuno_J/0/1/0/all/0/1\">Juan Carlos Nu&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Alvarez_J/0/1/0/all/0/1\">Jos&#xe9; Luis Gonz&#xe1;lez-&#xc1;lvarez</a>",
          "description": "Gender-based crime is one of the most concerning scourges of contemporary\nsociety. Governments worldwide have invested lots of economic and human\nresources to radically eliminate this threat. Despite these efforts, providing\naccurate predictions of the risk that a victim of gender violence has of being\nattacked again is still a very hard open problem. The development of new\nmethods for issuing accurate, fair and quick predictions would allow police\nforces to select the most appropriate measures to prevent recidivism. In this\nwork, we propose to apply Machine Learning (ML) techniques to create models\nthat accurately predict the recidivism risk of a gender-violence offender. The\nrelevance of the contribution of this work is threefold: (i) the proposed ML\nmethod outperforms the preexisting risk assessment algorithm based on classical\nstatistical techniques, (ii) the study has been conducted through an official\nspecific-purpose database with more than 40,000 reports of gender violence, and\n(iii) two new quality measures are proposed for assessing the effective police\nprotection that a model supplies and the overload in the invested resources\nthat it generates. Additionally, we propose a hybrid model that combines the\nstatistical prediction methods with the ML method, permitting authorities to\nimplement a smooth transition from the preexisting model to the ML-based model.\nThis hybrid nature enables a decision-making process to optimally balance\nbetween the efficiency of the police system and aggressiveness of the\nprotection measures taken.",
          "link": "http://arxiv.org/abs/2106.11847",
          "publishedOn": "2021-06-23T01:48:41.212Z",
          "wordCount": 712,
          "title": "Machine learning for risk assessment in gender-based crime. (arXiv:2106.11847v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising image regions,\nin each acquisition step. The problem is framed in an exploration-exploitation\nframework by combining an embedding based on Uniform Manifold Approximation to\nmodel representativeness with entropy as uncertainty measure to model\ninformativeness. We applied our proposed method to the challenging autonomous\ndriving data sets CamVid and Cityscapes and performed a quantitative comparison\nwith state-of-the-art methods. We find that our active learning method achieves\nbetter performance on CamVid compared to other methods, while on Cityscapes,\nthe performance lift was negligible.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-06-23T01:48:41.189Z",
          "wordCount": 565,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yingtao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>",
          "description": "Partial differential equations (PDEs) are concise and understandable\nrepresentations of domain knowledge, which are essential for deepening our\nunderstanding of physical processes and predicting future responses. However,\nthe PDEs of many real-world problems are uncertain, which calls for PDE\ndiscovery. We propose the symbolic genetic algorithm (SGA-PDE) to discover\nopen-form PDEs directly from data without prior knowledge about the equation\nstructure. SGA-PDE focuses on the representation and optimization of PDE.\nFirstly, SGA-PDE uses symbolic mathematics to realize the flexible\nrepresentation of any given PDE, transforms a PDE into a forest, and converts\neach function term into a binary tree. Secondly, SGA-PDE adopts a specially\ndesigned genetic algorithm to efficiently optimize the binary trees by\niteratively updating the tree topology and node attributes. The SGA-PDE is\ngradient-free, which is a desirable characteristic in PDE discovery since it is\ndifficult to obtain the gradient between the PDE loss and the PDE structure. In\nthe experiment, SGA-PDE not only successfully discovered nonlinear Burgers'\nequation, Korteweg-de Vries (KdV) equation, and Chafee-Infante equation, but\nalso handled PDEs with fractional structure and compound functions that cannot\nbe solved by conventional PDE discovery methods.",
          "link": "http://arxiv.org/abs/2106.11927",
          "publishedOn": "2021-06-23T01:48:41.183Z",
          "wordCount": 655,
          "title": "Any equation is a forest: Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE). (arXiv:2106.11927v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chitsazan_N/0/1/0/all/0/1\">Nima Chitsazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1\">Samuel Sharpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_D/0/1/0/all/0/1\">Dwipam Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qianyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasethupathy_K/0/1/0/all/0/1\">Karthik Rajasethupathy</a>",
          "description": "As financial services (FS) companies have experienced drastic technology\ndriven changes, the availability of new data streams provides the opportunity\nfor more comprehensive customer understanding. We propose Dynamic Customer\nEmbeddings (DCE), a framework that leverages customers' digital activity and a\nwide range of financial context to learn dense representations of customers in\nthe FS industry. Our method examines customer actions and pageviews within a\nmobile or web digital session, the sequencing of the sessions themselves, and\nsnapshots of common financial features across our organization at the time of\nlogin. We test our customer embeddings using real world data in three\nprediction problems: 1) the intent of a customer in their next digital session,\n2) the probability of a customer calling the call centers after a session, and\n3) the probability of a digital session to be fraudulent. DCE showed\nperformance lift in all three downstream problems.",
          "link": "http://arxiv.org/abs/2106.11880",
          "publishedOn": "2021-06-23T01:48:41.175Z",
          "wordCount": 592,
          "title": "Dynamic Customer Embeddings for Financial Service Applications. (arXiv:2106.11880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sordello_M/0/1/0/all/0/1\">Matteo Sordello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>",
          "description": "In this paper, we consider the framework of privacy amplification via\niteration, which is originally proposed by Feldman et al. and subsequently\nsimplified by Asoodeh et al. in their analysis via the contraction coefficient.\nThis line of work focuses on the study of the privacy guarantees obtained by\nthe projected noisy stochastic gradient descent (PNSGD) algorithm with hidden\nintermediate updates. A limitation in the existing literature is that only the\nearly stopped PNSGD has been studied, while no result has been proved on the\nmore widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has\nbeen yet proposed regarding how to decrease the injected noise when new data\nare received in an online fashion. In this work, we first prove a privacy\nguarantee for shuffled PNSGD, which is investigated asymptotically when the\nnoise is fixed for each sample size $n$ but reduced at a predetermined rate\nwhen $n$ increases, in order to achieve the convergence of privacy loss. We\nthen analyze the online setting and provide a faster decaying scheme for the\nmagnitude of the injected noise that also guarantees the convergence of privacy\nloss.",
          "link": "http://arxiv.org/abs/2106.11767",
          "publishedOn": "2021-06-23T01:48:41.169Z",
          "wordCount": 633,
          "title": "Privacy Amplification via Iteration for Shuffled and Online PNSGD. (arXiv:2106.11767v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khavari_B/0/1/0/all/0/1\">Behnoush Khavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "Tensor network methods have been a key ingredient of advances in condensed\nmatter physics and have recently sparked interest in the machine learning\ncommunity for their ability to compactly represent very high-dimensional\nobjects. Tensor network methods can for example be used to efficiently learn\nlinear models in exponentially large feature spaces [Stoudenmire and Schwab,\n2016]. In this work, we derive upper and lower bounds on the VC dimension and\npseudo-dimension of a large class of tensor network models for classification,\nregression and completion. Our upper bounds hold for linear models\nparameterized by arbitrary tensor network structures, and we derive lower\nbounds for common tensor decomposition models~(CP, Tensor Train, Tensor Ring\nand Tucker) showing the tightness of our general upper bound. These results are\nused to derive a generalization bound which can be applied to classification\nwith low rank matrices as well as linear classifiers based on any of the\ncommonly used tensor decomposition models. As a corollary of our results, we\nobtain a bound on the VC dimension of the matrix product state classifier\nintroduced in [Stoudenmire and Schwab, 2016] as a function of the so-called\nbond dimension~(i.e. tensor train rank), which answers an open problem listed\nby Cirac, Garre-Rubio and P\\'erez-Garc\\'ia in [Cirac et al., 2019].",
          "link": "http://arxiv.org/abs/2106.11827",
          "publishedOn": "2021-06-23T01:48:41.161Z",
          "wordCount": 639,
          "title": "Lower and Upper Bounds on the VC-Dimension of Tensor Network Models. (arXiv:2106.11827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>",
          "description": "Cross-silo federated learning (FL) is a distributed learning approach where\nclients train a global model cooperatively while keeping their local data\nprivate. Different from cross-device FL, clients in cross-silo FL are usually\norganizations or companies which may execute multiple cross-silo FL processes\nrepeatedly due to their time-varying local data sets, and aim to optimize their\nlong-term benefits by selfishly choosing their participation levels. While\nthere has been some work on incentivizing clients to join FL, the analysis of\nthe long-term selfish participation behaviors of clients in cross-silo FL\nremains largely unexplored. In this paper, we analyze the selfish participation\nbehaviors of heterogeneous clients in cross-silo FL. Specifically, we model the\nlong-term selfish participation behaviors of clients as an infinitely repeated\ngame, with the stage game being a selfish participation game in one cross-silo\nFL process (SPFL). For the stage game SPFL, we derive the unique Nash\nequilibrium (NE), and propose a distributed algorithm for each client to\ncalculate its equilibrium participation strategy. For the long-term\ninteractions among clients, we derive a cooperative strategy for clients which\nminimizes the number of free riders while increasing the amount of local data\nfor model training. We show that enforced by a punishment strategy, such a\ncooperative strategy is a SPNE of the infinitely repeated game, under which\nsome clients who are free riders at the NE of the stage game choose to be\n(partial) contributors. We further propose an algorithm to calculate the\noptimal SPNE which minimizes the number of free riders while maximizing the\namount of local data for model training. Simulation results show that our\nproposed cooperative strategy at the optimal SPNE can effectively reduce the\nnumber of free riders and increase the amount of local data for model training.",
          "link": "http://arxiv.org/abs/2106.11814",
          "publishedOn": "2021-06-23T01:48:41.141Z",
          "wordCount": 730,
          "title": "Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A Repeated Game Perspective. (arXiv:2106.11814v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mora_A/0/1/0/all/0/1\">A.M. Mora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esparcia_Alcazar_A/0/1/0/all/0/1\">A.I. Esparcia-Alc&#xe1;zar</a>",
          "description": "Volumen with the Late-Breaking Abstracts submitted to the Evo* 2021\nConference, held online from 7 to 9 of April 2021. These papers present ongoing\nresearch and preliminary results investigating on the application of different\napproaches of Bioinspired Methods (mainly Evolutionary Computation) to\ndifferent problems, most of them real world ones.",
          "link": "http://arxiv.org/abs/2106.11804",
          "publishedOn": "2021-06-23T01:48:41.134Z",
          "wordCount": 503,
          "title": "Evo* 2021 -- Late-Breaking Abstracts Volume. (arXiv:2106.11804v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ray Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1\">Tom Zahavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhongwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_M/0/1/0/all/0/1\">Matteo Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1\">Charles Blundell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1\">Hado van Hasselt</a>",
          "description": "Off-policy learning allows us to learn about possible policies of behavior\nfrom experience generated by a different behavior policy. Temporal difference\n(TD) learning algorithms can become unstable when combined with function\napproximation and off-policy sampling - this is known as the ''deadly triad''.\nEmphatic temporal difference (ETD($\\lambda$)) algorithm ensures convergence in\nthe linear case by appropriately weighting the TD($\\lambda$) updates. In this\npaper, we extend the use of emphatic methods to deep reinforcement learning\nagents. We show that naively adapting ETD($\\lambda$) to popular deep\nreinforcement learning algorithms, which use forward view multi-step returns,\nresults in poor performance. We then derive new emphatic algorithms for use in\nthe context of such algorithms, and we demonstrate that they provide noticeable\nbenefits in small problems designed to highlight the instability of TD methods.\nFinally, we observed improved performance when applying these algorithms at\nscale on classic Atari games from the Arcade Learning Environment.",
          "link": "http://arxiv.org/abs/2106.11779",
          "publishedOn": "2021-06-23T01:48:41.128Z",
          "wordCount": 602,
          "title": "Emphatic Algorithms for Deep Reinforcement Learning. (arXiv:2106.11779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11853",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lienen_J/0/1/0/all/0/1\">Julian Lienen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "Self-training is an effective approach to semi-supervised learning. The key\nidea is to let the learner itself iteratively generate \"pseudo-supervision\" for\nunlabeled instances based on its current hypothesis. In combination with\nconsistency regularization, pseudo-labeling has shown promising performance in\nvarious domains, for example in computer vision. To account for the\nhypothetical nature of the pseudo-labels, these are commonly provided in the\nform of probability distributions. Still, one may argue that even a probability\ndistribution represents an excessive level of informedness, as it suggests that\nthe learner precisely knows the ground-truth conditional probabilities. In our\napproach, we therefore allow the learner to label instances in the form of\ncredal sets, that is, sets of (candidate) probability distributions. Thanks to\nthis increased expressiveness, the learner is able to represent uncertainty and\na lack of knowledge in a more flexible and more faithful manner. To learn from\nweakly labeled data of that kind, we leverage methods that have recently been\nproposed in the realm of so-called superset learning. In an exhaustive\nempirical evaluation, we compare our methodology to state-of-the-art\nself-supervision approaches, showing competitive to superior performance\nespecially in low-label scenarios incorporating a high degree of uncertainty.",
          "link": "http://arxiv.org/abs/2106.11853",
          "publishedOn": "2021-06-23T01:48:41.121Z",
          "wordCount": 623,
          "title": "Credal Self-Supervised Learning. (arXiv:2106.11853v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Gustavo H. de Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>",
          "description": "A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven\nto be a state-of-the-art algorithm comparable to Logistic Regressors, Support\nVector Machines in a wide variety of tasks. Recently, its Python-based version,\ndenoted as OPFython, has been proposed to provide a more friendly framework and\na faster prototyping environment. Nevertheless, Python-based algorithms are\nslower than their counterpart C-based algorithms, impacting their performance\nwhen confronted with large amounts of data. Therefore, this paper proposed a\nsimple yet highly efficient speed up using the Numba package, which accelerates\nNumpy-based calculations and attempts to increase the algorithm's overall\nperformance. Experimental results showed that the proposed approach achieved\nbetter results than the na\\\"ive Python-based OPF and speeded up its distance\nmeasurement calculation.",
          "link": "http://arxiv.org/abs/2106.11828",
          "publishedOn": "2021-06-23T01:48:41.114Z",
          "wordCount": 558,
          "title": "Speeding Up OPFython with Numba. (arXiv:2106.11828v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1\">Albin Soutif--Cormerais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost Van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Twardowski</a>",
          "description": "In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features.\nThis is especially important when the number of classes per task is small.",
          "link": "http://arxiv.org/abs/2106.11930",
          "publishedOn": "2021-06-23T01:48:41.095Z",
          "wordCount": 618,
          "title": "On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1\">Vikramjit Mitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zifang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1\">Colin Lea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1\">Lauren Tooley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Sarah Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1\">Darren Botten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1\">Ashwini Palekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1\">Shrinath Thelapurath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1\">Panayiotis Georgiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1\">Jefferey Bigham</a>",
          "description": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.",
          "link": "http://arxiv.org/abs/2106.11759",
          "publishedOn": "2021-06-23T01:48:41.087Z",
          "wordCount": 671,
          "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuyang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homaifar_A/0/1/0/all/0/1\">Abdollah Homaifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1\">Mrinmoy Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girma_A/0/1/0/all/0/1\">Abenezer Girma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstel_E/0/1/0/all/0/1\">Edward Tunstel</a>",
          "description": "The non-stationary nature of data streams strongly challenges traditional\nmachine learning techniques. Although some solutions have been proposed to\nextend traditional machine learning techniques for handling data streams, these\napproaches either require an initial label set or rely on specialized design\nparameters. The overlap among classes and the labeling of data streams\nconstitute other major challenges for classifying data streams. In this paper,\nwe proposed a clustering-based data stream classification framework to handle\nnon-stationary data streams without utilizing an initial label set. A\ndensity-based stream clustering procedure is used to capture novel concepts\nwith a dynamic threshold and an effective active label querying strategy is\nintroduced to continuously learn the new concepts from the data streams. The\nsub-cluster structure of each cluster is explored to handle the overlap among\nclasses. Experimental results and quantitative comparison studies reveal that\nthe proposed method provides statistically better or comparable performance\nthan the existing methods.",
          "link": "http://arxiv.org/abs/2106.11823",
          "publishedOn": "2021-06-23T01:48:41.080Z",
          "wordCount": 593,
          "title": "A Clustering-based Framework for Classifying Data Streams. (arXiv:2106.11823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+BK_V/0/1/0/all/0/1\">Vanya BK</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1\">Balaji Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Aniket Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Devbrat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arvind Agarwal</a>",
          "description": "Explaining Graph Neural Networks predictions to end users of AI applications\nin easily understandable terms remains an unsolved problem. In particular, we\ndo not have well developed methods for automatically evaluating explanations,\nin ways that are closer to how users consume those explanations. Based on\nrecent application trends and our own experiences in real world problems, we\npropose automatic evaluation approaches for GNN Explanations.",
          "link": "http://arxiv.org/abs/2106.11864",
          "publishedOn": "2021-06-23T01:48:41.073Z",
          "wordCount": 512,
          "title": "Towards Automated Evaluation of Explanations in Graph Neural Networks. (arXiv:2106.11864v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11936",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tod_G/0/1/0/all/0/1\">Georges Tod</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1\">Gert-Jan Both</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1\">Remy Kusters</a>",
          "description": "Discovering the partial differential equations underlying a spatio-temporal\ndatasets from very limited observations is of paramount interest in many\nscientific fields. However, it remains an open question to know when model\ndiscovery algorithms based on sparse regression can actually recover the\nunderlying physical processes. We trace back the poor of performance of Lasso\nbased model discovery algorithms to its potential variable selection\ninconsistency: meaning that even if the true model is present in the library,\nit might not be selected. By first revisiting the irrepresentability condition\n(IRC) of the Lasso, we gain some insights of when this might occur. We then\nshow that the adaptive Lasso will have more chances of verifying the IRC than\nthe Lasso and propose to integrate it within a deep learning model discovery\nframework with stability selection and error control. Experimental results show\nwe can recover several nonlinear and chaotic canonical PDEs with a single set\nof hyperparameters from a very limited number of samples at high noise levels.",
          "link": "http://arxiv.org/abs/2106.11936",
          "publishedOn": "2021-06-23T01:48:41.066Z",
          "wordCount": 590,
          "title": "Sparsistent Model Discovery. (arXiv:2106.11936v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1\">Nikola Konstantinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>",
          "description": "Fairness-aware learning aims at constructing classifiers that not only make\naccurate predictions, but do not discriminate against specific groups. It is a\nfast-growing area of machine learning with far-reaching societal impact.\nHowever, existing fair learning methods are vulnerable to accidental or\nmalicious artifacts in the training data, which can cause them to unknowingly\nproduce unfair classifiers. In this work we address the problem of fair\nlearning from unreliable training data in the robust multisource setting, where\nthe available training data comes from multiple sources, a fraction of which\nmight be not representative of the true data distribution. We introduce FLEA, a\nfiltering-based algorithm that allows the learning system to identify and\nsuppress those data sources that would have a negative impact on fairness or\naccuracy if they were used for training. We show the effectiveness of our\napproach by a diverse range of experiments on multiple datasets. Additionally\nwe prove formally that, given enough data, FLEA protects the learner against\nunreliable data as long as the fraction of affected data sources is less than\nhalf.",
          "link": "http://arxiv.org/abs/2106.11732",
          "publishedOn": "2021-06-23T01:48:41.060Z",
          "wordCount": 606,
          "title": "FLEA: Provably Fair Multisource Learning from Unreliable Training Data. (arXiv:2106.11732v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen-Chi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Raskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1\">Luc De Raedt</a>",
          "description": "Model checking has been developed for verifying the behaviour of systems with\nstochastic and non-deterministic behavior. It is used to provide guarantees\nabout such systems. While most model checking methods focus on propositional\nmodels, various probabilistic planning and reinforcement frameworks deal with\nrelational domains, for instance, STRIPS planning and relational Markov\nDecision Processes. Using propositional model checking in relational settings\nrequires one to ground the model, which leads to the well known state explosion\nproblem and intractability. We present pCTL-REBEL, a lifted model checking\napproach for verifying pCTL properties on relational MDPs. It extends REBEL,\nthe relational Bellman update operator, which is a lifted value iteration\napproach for model-based relational reinforcement learning, toward relational\nmodel-checking. PCTL-REBEL is lifted, which means that rather than grounding,\nthe model exploits symmetries and reasons at an abstract relational level.\nTheoretically, we show that the pCTL model checking approach is decidable for\nrelational MDPs even for possibly infinite domains provided that the states\nhave a bounded size. Practically, we contribute algorithms and an\nimplementation of lifted relational model checking, and we show that the lifted\napproach improves the scalability of the model checking approach.",
          "link": "http://arxiv.org/abs/2106.11735",
          "publishedOn": "2021-06-23T01:48:41.041Z",
          "wordCount": 623,
          "title": "Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepti Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Maanak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Smriti Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosun_A/0/1/0/all/0/1\">Ali Saman Tosun</a>",
          "description": "The growth in Remote Patient Monitoring (RPM) services using wearable and\nnon-wearable Internet of Medical Things (IoMT) promises to improve the quality\nof diagnosis and facilitate timely treatment for a gamut of medical conditions.\nAt the same time, the proliferation of IoMT devices increases the potential for\nmalicious activities that can lead to catastrophic results including theft of\npersonal information, data breach, and compromised medical devices, putting\nhuman lives at risk. IoMT devices generate tremendous amount of data that\nreflect user behavior patterns including both personal and day-to-day social\nactivities along with daily routine health monitoring. In this context, there\nare possibilities of anomalies generated due to various reasons including\nunexpected user behavior, faulty sensor, or abnormal values from\nmalicious/compromised devices. To address this problem, there is an imminent\nneed to develop a framework for securing the smart health care infrastructure\nto identify and mitigate anomalies. In this paper, we present an anomaly\ndetection model for RPM utilizing IoMT and smart home devices. We propose\nHidden Markov Model (HMM) based anomaly detection that analyzes normal user\nbehavior in the context of RPM comprising both smart home and smart health\ndevices, and identifies anomalous user behavior. We design a testbed with\nmultiple IoMT devices and home sensors to collect data and use the HMM model to\ntrain using network and user behavioral data. Proposed HMM based anomaly\ndetection model achieved over 98% accuracy in identifying the anomalies in the\ncontext of RPM.",
          "link": "http://arxiv.org/abs/2106.11844",
          "publishedOn": "2021-06-23T01:48:41.032Z",
          "wordCount": 676,
          "title": "Detecting Anomalous User Behavior in Remote Patient Monitoring. (arXiv:2106.11844v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1\">Tomas Sipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan Sulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "In many computer vision classification tasks, class priors at test time often\ndiffer from priors on the training set. In the case of such prior shift,\nclassifiers must be adapted correspondingly to maintain close to optimal\nperformance. This paper analyzes methods for adaptation of probabilistic\nclassifiers to new priors and for estimating new priors on an unlabeled test\nset. We propose a novel method to address a known issue of prior estimation\nmethods based on confusion matrices, where inconsistent estimates of decision\nprobabilities and confusion matrices lead to negative values in the estimated\npriors. Experiments on fine-grained image classification datasets provide\ninsight into the best practice of prior shift estimation and classifier\nadaptation and show that the proposed method achieves state-of-the-art results\nin prior adaptation. Applying the best practice to two tasks with naturally\nimbalanced priors, learning from web-crawled images and plant species\nclassification, increased the recognition accuracy by 1.1% and 3.4%\nrespectively.",
          "link": "http://arxiv.org/abs/2106.11695",
          "publishedOn": "2021-06-23T01:48:41.023Z",
          "wordCount": 592,
          "title": "The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "Modern pre-trained language models are mostly built upon backbones stacking\nself-attention and feed-forward layers in an interleaved order. In this paper,\nbeyond this stereotyped layer pattern, we aim to improve pre-trained models by\nexploiting layer variety from two aspects: the layer type set and the layer\norder. Specifically, besides the original self-attention and feed-forward\nlayers, we introduce convolution into the layer type set, which is\nexperimentally found beneficial to pre-trained models. Furthermore, beyond the\noriginal interleaved order, we explore more layer orders to discover more\npowerful architectures. However, the introduced layer variety leads to a large\narchitecture space of more than billions of candidates, while training a single\ncandidate model from scratch already requires huge computation cost, making it\nnot affordable to search such a space by directly training large amounts of\ncandidate models. To solve this problem, we first pre-train a supernet from\nwhich the weights of all candidate models can be inherited, and then adopt an\nevolutionary algorithm guided by pre-training accuracy to find the optimal\narchitecture. Extensive experiments show that LV-BERT model obtained by our\nmethod outperforms BERT and its variants on various downstream tasks. For\nexample, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than\nthe strong baseline ELECTRA-small.",
          "link": "http://arxiv.org/abs/2106.11740",
          "publishedOn": "2021-06-23T01:48:41.016Z",
          "wordCount": 661,
          "title": "LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jihan Zhang</a>",
          "description": "Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.",
          "link": "http://arxiv.org/abs/2106.11769",
          "publishedOn": "2021-06-23T01:48:40.998Z",
          "wordCount": 633,
          "title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1\">Andrew P. Creagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipsmeier_F/0/1/0/all/0/1\">Florian Lipsmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemann_M/0/1/0/all/0/1\">Michael Lindemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1\">Maarten De Vos</a>",
          "description": "The emergence of digital technologies such as smartphones in healthcare\napplications have demonstrated the possibility of developing rich, continuous,\nand objective measures of multiple sclerosis (MS) disability that can be\nadministered remotely and out-of-clinic. In this work, deep convolutional\nneural networks (DCNN) applied to smartphone inertial sensor data were shown to\nbetter distinguish healthy from MS participant ambulation, compared to standard\nSupport Vector Machine (SVM) feature-based methodologies. To overcome the\ntypical limitations associated with remotely generated health data, such as low\nsubject numbers, sparsity, and heterogeneous data, a transfer learning (TL)\nmodel from similar large open-source datasets was proposed. Our TL framework\nutilised the ambulatory information learned on Human Activity Recognition (HAR)\ntasks collected from similar smartphone-based sensor data. A lack of\ntransparency of \"black-box\" deep networks remains one of the largest stumbling\nblocks to the wider acceptance of deep learning for clinical applications.\nEnsuing work therefore aimed to visualise DCNN decisions attributed by\nrelevance heatmaps using Layer-Wise Relevance Propagation (LRP). Through the\nLRP framework, the patterns captured from smartphone-based inertial sensor data\nthat were reflective of those who are healthy versus persons with MS (PwMS)\ncould begin to be established and understood. Interpretations suggested that\ncadence-based measures, gait speed, and ambulation-related signal perturbations\nwere distinct characteristics that distinguished MS disability from healthy\nparticipants. Robust and interpretable outcomes, generated from high-frequency\nout-of-clinic assessments, could greatly augment the current in-clinic\nassessment picture for PwMS, to inform better disease management techniques,\nand enable the development of better therapeutic interventions.",
          "link": "http://arxiv.org/abs/2103.09171",
          "publishedOn": "2021-06-23T01:48:40.991Z",
          "wordCount": 729,
          "title": "Interpretable Deep Learning for the Remote Characterisation of Ambulation in Multiple Sclerosis using Smartphones. (arXiv:2103.09171v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yifei Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dongruo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>",
          "description": "We study the off-policy evaluation (OPE) problem in reinforcement learning\nwith linear function approximation, which aims to estimate the value function\nof a target policy based on the offline data collected by a behavior policy. We\npropose to incorporate the variance information of the value function to\nimprove the sample efficiency of OPE. More specifically, for time-inhomogeneous\nepisodic linear Markov decision processes (MDPs), we propose an algorithm,\nVA-OPE, which uses the estimated variance of the value function to reweight the\nBellman residual in Fitted Q-Iteration. We show that our algorithm achieves a\ntighter error bound than the best-known result. We also provide a fine-grained\ncharacterization of the distribution shift between the behavior policy and the\ntarget policy. Extensive numerical experiments corroborate our theory.",
          "link": "http://arxiv.org/abs/2106.11960",
          "publishedOn": "2021-06-23T01:48:40.958Z",
          "wordCount": 567,
          "title": "Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11731",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1\">Taro Langner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1\">Andr&#xe9;s Mart&#xed;nez Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1\">Robin Strand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1\">H&#xe5;kan Ahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1\">Joel Kullberg</a>",
          "description": "UK Biobank (UKB) is conducting a large-scale study of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging furthermore targets\n100,000 subjects, with 70,000 follow-up sessions, enabling measurements of\norgans, muscle, and body composition. With up to 170,000 mounting MR images,\nvarious methodologies are accordingly engaged in large-scale image analysis.\nThis work presents an experimental inference engine that can automatically\npredict a comprehensive profile of subject metadata from UKB neck-to-knee body\nMRI. In cross-validation, it accurately inferred baseline characteristics such\nas age, height, weight, and sex, but also emulated measurements of body\ncomposition by DXA, organ volumes, and abstract properties like grip strength,\npulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can\nautomatically analyze thousands of subjects within hours and provide individual\nconfidence intervals. The underlying methodology is based on convolutional\nneural networks for image-based mean-variance regression on two-dimensional\nrepresentations of the MRI data. This work aims to make the proposed system\navailable for free to researchers, who can use it to obtain fast and\nfully-automated estimates of 72 different measurements immediately upon release\nof new UK Biobank image data.",
          "link": "http://arxiv.org/abs/2106.11731",
          "publishedOn": "2021-06-23T01:48:40.951Z",
          "wordCount": 650,
          "title": "MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1\">Srivas Chennu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Jamie Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyanagama_P/0/1/0/all/0/1\">Puli Liyanagama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohr_P/0/1/0/all/0/1\">Phil Mohr</a>",
          "description": "Stochastic delays in feedback lead to unstable sequential learning using\nmulti-armed bandits. Recently, empirical Bayesian shrinkage has been shown to\nimprove reward estimation in bandit learning. Here, we propose a novel\nadaptation to shrinkage that estimates smoothed reward estimates from windowed\ncumulative inputs, to deal with incomplete knowledge from delayed feedback and\nnon-stationary rewards. Using numerical simulations, we show that this\nadaptation retains the benefits of shrinkage, and improves the stability of\nreward estimation by more than 50%. Our proposal reduces variability in\ntreatment allocations to the best arm by up to 3.8x, and improves statistical\naccuracy - with up to 8% improvement in true positive rates and 37% reduction\nin false positive rates. Together, these advantages enable control of the\ntrade-off between speed and stability of adaptation, and facilitate\nhuman-in-the-loop sequential optimisation.",
          "link": "http://arxiv.org/abs/2106.11294",
          "publishedOn": "2021-06-23T01:48:40.944Z",
          "wordCount": 599,
          "title": "Smooth Sequential Optimisation with Delayed Feedback. (arXiv:2106.11294v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06671",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1\">Thanh Nguyen-Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1\">Hung Tran-The</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>",
          "description": "We study the statistical theory of offline reinforcement learning (RL) with\ndeep ReLU network function approximation. We analyze a variant of fitted-Q\niteration (FQI) algorithm under a new dynamic condition that we call Besov\ndynamic closure, which encompasses the conditions from prior analyses for deep\nneural network function approximation. Under Besov dynamic closure, we prove\nthat the FQI-type algorithm enjoys the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 -\n2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the\ndimensionality of the state-action space, $\\alpha$ is the (possibly fractional)\nsmoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified\nprecision. This is an improvement over the sample complexity of\n$\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 -\nd/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an\nalgorithmic iteration number which is arbitrarily large in practice.\nImportantly, our sample complexity is obtained under the new general dynamic\ncondition and a data-dependent structure where the latter is either ignored in\nprior algorithms or improperly handled by prior analyses. This is the first\ncomprehensive analysis for offline RL with deep ReLU network function\napproximation under a general setting.",
          "link": "http://arxiv.org/abs/2103.06671",
          "publishedOn": "2021-06-23T01:48:40.933Z",
          "wordCount": 654,
          "title": "Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>",
          "description": "In this work, we train the first monolingual Lithuanian transformer model on\na relatively large corpus of Lithuanian news articles and compare various\noutput decoding algorithms for abstractive news summarization. We achieve an\naverage ROUGE-2 score 0.163, generated summaries are coherent and look\nimpressive at first glance. However, some of them contain misleading\ninformation that is not so easy to spot. We describe all the technical details\nand share our trained model and accompanying code in an online open-source\nrepository, as well as some characteristic samples of the generated summaries.",
          "link": "http://arxiv.org/abs/2105.03279",
          "publishedOn": "2021-06-23T01:48:40.910Z",
          "wordCount": 569,
          "title": "Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Networks have been widely used to represent the relations between objects\nsuch as academic networks and social networks, and learning embedding for\nnetworks has thus garnered plenty of research attention. Self-supervised\nnetwork representation learning aims at extracting node embedding without\nexternal supervision. Recently, maximizing the mutual information between the\nlocal node embedding and the global summary (e.g. Deep Graph Infomax, or DGI\nfor short) has shown promising results on many downstream tasks such as node\nclassification. However, there are two major limitations of DGI. Firstly, DGI\nmerely considers the extrinsic supervision signal (i.e., the mutual information\nbetween node embedding and global summary) while ignores the intrinsic signal\n(i.e., the mutual dependence between node embedding and node attributes).\nSecondly, nodes in a real-world network are usually connected by multiple edges\nwith different relations, while DGI does not fully explore the various\nrelations among nodes. To address the above-mentioned problems, we propose a\nnovel framework, called High-order Deep Multiplex Infomax (HDMI), for learning\nnode embedding on multiplex networks in a self-supervised way. To be more\nspecific, we first design a joint supervision signal containing both extrinsic\nand intrinsic mutual information by high-order mutual information, and we\npropose a High-order Deep Infomax (HDI) to optimize the proposed supervision\nsignal. Then we propose an attention based fusion module to combine node\nembedding from different layers of the multiplex network. Finally, we evaluate\nthe proposed HDMI on various downstream tasks such as unsupervised clustering\nand supervised classification. The experimental results show that HDMI achieves\nstate-of-the-art performance on these tasks.",
          "link": "http://arxiv.org/abs/2102.07810",
          "publishedOn": "2021-06-23T01:48:40.903Z",
          "wordCount": 742,
          "title": "HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1\">Arun Jambulapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1\">Tselil Schramm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We study fast algorithms for statistical regression problems under the strong\ncontamination model, where the goal is to approximately optimize a generalized\nlinear model (GLM) given adversarially corrupted samples. Prior works in this\nline of research were based on the robust gradient descent framework of Prasad\net. al., a first-order method using biased gradient queries, or the Sever\nframework of Diakonikolas et. al., an iterative outlier-removal method calling\na stationary point finder.\n\nWe present nearly-linear time algorithms for robust regression problems with\nimproved runtime or estimation guarantees compared to the state-of-the-art. For\nthe general case of smooth GLMs (e.g. logistic regression), we show that the\nrobust gradient descent framework of Prasad et. al. can be accelerated, and\nshow our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs\n(e.g. support vector machines), answering several open questions in the\nliterature.\n\nFor the well-studied case of robust linear regression, we present an\nalternative approach obtaining improved estimation rates over prior\nnearly-linear time algorithms. Interestingly, our method starts with an\nidentifiability proof introduced in the context of the sum-of-squares algorithm\nof Bakshi and Prasad, which achieved optimal error rates while requiring large\npolynomial runtime and sample complexity. We reinterpret their proof within the\nSever framework and obtain a dramatically faster and more sample-efficient\nalgorithm under fewer distributional assumptions.",
          "link": "http://arxiv.org/abs/2106.11938",
          "publishedOn": "2021-06-23T01:48:40.883Z",
          "wordCount": 668,
          "title": "Robust Regression Revisited: Acceleration and Improved Estimation Rates. (arXiv:2106.11938v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2005.08140",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1\">Sebastian W. Ober</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>",
          "description": "We consider the optimal approximate posterior over the top-layer weights in a\nBayesian neural network for regression, and show that it exhibits strong\ndependencies on the lower-layer weights. We adapt this result to develop a\ncorrelated approximate posterior over the weights at all layers in a Bayesian\nneural network. We extend this approach to deep Gaussian processes, unifying\ninference in the two model classes. Our approximate posterior uses learned\n\"global\" inducing points, which are defined only at the input layer and\npropagated through the network to obtain inducing inputs at subsequent layers.\nBy contrast, standard, \"local\", inducing point methods from the deep Gaussian\nprocess literature optimise a separate set of inducing inputs at every layer,\nand thus do not model correlations across layers. Our method gives\nstate-of-the-art performance for a variational Bayesian method, without data\naugmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC\nwithout tempering but with data augmentation (88% in Wenzel et al. 2020).",
          "link": "http://arxiv.org/abs/2005.08140",
          "publishedOn": "2021-06-23T01:48:40.877Z",
          "wordCount": 662,
          "title": "Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes. (arXiv:2005.08140v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1\">Li Guanlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1\">Guo Shangwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1\">Wang Run</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1\">Xu Guowen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1\">Zhang Tianwei</a>",
          "description": "This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.",
          "link": "http://arxiv.org/abs/2106.11760",
          "publishedOn": "2021-06-23T01:48:40.870Z",
          "wordCount": 571,
          "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1\">Zahra Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1\">Brandon Reagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>",
          "description": "The emergence of deep learning has been accompanied by privacy concerns\nsurrounding users' data and service providers' models. We focus on private\ninference (PI), where the goal is to perform inference on a user's data sample\nusing a service provider's model. Existing PI methods for deep networks enable\ncryptographically secure inference with little drop in functionality; however,\nthey incur severe latency costs, primarily caused by non-linear network\noperations (such as ReLUs). This paper presents Sphynx, a ReLU-efficient\nnetwork design method based on micro-search strategies for convolutional cell\ndesign. Sphynx achieves Pareto dominance over all existing private inference\nmethods on CIFAR-100. We also design large-scale networks that support\ncryptographically private inference on Tiny-ImageNet and ImageNet.",
          "link": "http://arxiv.org/abs/2106.11755",
          "publishedOn": "2021-06-23T01:48:40.863Z",
          "wordCount": 549,
          "title": "Sphynx: ReLU-Efficient Network Design for Private Inference. (arXiv:2106.11755v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+David_M/0/1/0/all/0/1\">Marco David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehats_F/0/1/0/all/0/1\">Florian M&#xe9;hats</a>",
          "description": "Machine learning methods are widely used in the natural sciences to model and\npredict physical systems from observation data. Yet, they are often used as\npoorly understood \"black boxes,\" disregarding existing mathematical structure\nand invariants of the problem. Recently, the proposal of Hamiltonian Neural\nNetworks (HNNs) took a first step towards a unified \"gray box\" approach, using\nphysical insight to improve performance for Hamiltonian systems. In this paper,\nwe explore a significantly improved training method for HNNs, exploiting the\nsymplectic structure of Hamiltonian systems with a different loss function.\nThis frees the loss from an artificial lower bound. We mathematically guarantee\nthe existence of an exact Hamiltonian function which the HNN can learn. This\nallows us to prove and numerically analyze the errors made by HNNs which, in\nturn, renders them fully explainable. Finally, we present a novel post-training\ncorrection to obtain the true Hamiltonian only from discretized observation\ndata, up to an arbitrary order.",
          "link": "http://arxiv.org/abs/2106.11753",
          "publishedOn": "2021-06-23T01:48:40.856Z",
          "wordCount": 598,
          "title": "Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1\">Matthias Fey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakata_M/0/1/0/all/0/1\">Maho Nakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>",
          "description": "Enabling effective and efficient machine learning (ML) over large-scale graph\ndata (e.g., graphs with billions of edges) can have a huge impact on both\nindustrial and scientific applications. However, community efforts to advance\nlarge-scale graph ML have been severely limited by the lack of a suitable\npublic benchmark. For KDD Cup 2021, we present OGB Large-Scale Challenge\n(OGB-LSC), a collection of three real-world datasets for advancing the\nstate-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that\nare orders of magnitude larger than existing ones and covers three core graph\nlearning tasks -- link prediction, graph regression, and node classification.\nFurthermore, OGB-LSC provides dedicated baseline experiments, scaling up\nexpressive graph ML models to the massive datasets. We show that the expressive\nmodels significantly outperform simple scalable baselines, indicating an\nopportunity for dedicated efforts to further improve graph ML at scale. Our\ndatasets and baseline code are released and maintained as part of our OGB\ninitiative (Hu et al., 2020). We hope OGB-LSC at KDD Cup 2021 can empower the\ncommunity to discover innovative solutions for large-scale graph ML.",
          "link": "http://arxiv.org/abs/2103.09430",
          "publishedOn": "2021-06-23T01:48:40.849Z",
          "wordCount": 650,
          "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. (arXiv:2103.09430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11926",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mouradi_R/0/1/0/all/0/1\">Rem-Sophia Mouradi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goeury_C/0/1/0/all/0/1\">C&#xe9;dric Goeury</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thual_O/0/1/0/all/0/1\">Olivier Thual</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zaoui_F/0/1/0/all/0/1\">Fabrice Zaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tassi_P/0/1/0/all/0/1\">Pablo Tassi</a>",
          "description": "Data assimilation (DA) is widely used to combine physical knowledge and\nobservations. It is nowadays commonly used in geosciences to perform parametric\ncalibration. In a context of climate change, old calibrations can not\nnecessarily be used for new scenarios. This raises the question of DA\ncomputational cost, as costly physics-based numerical models need to be\nreanalyzed. Reduction and metamodelling represent therefore interesting\nperspectives, for example proposed in recent contributions as hybridization\nbetween ensemble and variational methods, to combine their advantages\n(efficiency, non-linear framework). They are however often based on Monte Carlo\n(MC) type sampling, which often requires considerable increase of the ensemble\nsize for better efficiency, therefore representing a computational burden in\nensemble-based methods as well. To address these issues, two methods to replace\nthe complex model by a surrogate are proposed and confronted : (i) PODEn3DVAR\ndirectly inspired from PODEn4DVAR, relies on an ensemble-based joint\nparameter-state Proper Orthogonal Decomposition (POD), which provides a linear\nmetamodel ; (ii) POD-PCE-3DVAR, where the model states are POD reduced then\nlearned using Polynomial Chaos Expansion (PCE), resulting in a non-linear\nmetamodel. Both metamodels allow to write an approximate cost function whose\nminimum can be analytically computed, or deduced by a gradient descent at\nnegligible cost. Furthermore, adapted metamodelling error covariance matrix is\ngiven for POD-PCE-3DVAR, allowing to substantially improve the metamodel-based\nDA analysis. Proposed methods are confronted on a twin experiment, and compared\nto classical 3DVAR on a measurement-based problem. Results are promising, in\nparticular superior with POD-PCE-3DVAR, showing good convergence to classical\n3DVAR and robustness to noise.",
          "link": "http://arxiv.org/abs/2106.11926",
          "publishedOn": "2021-06-23T01:48:40.842Z",
          "wordCount": 700,
          "title": "Surrogate-based variational data assimilation for tidal modelling. (arXiv:2106.11926v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no syntax\nerrors). Existing works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.",
          "link": "http://arxiv.org/abs/2106.06600",
          "publishedOn": "2021-06-23T01:48:40.835Z",
          "wordCount": 693,
          "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baeta_F/0/1/0/all/0/1\">Francisco Baeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1\">Jo&#xe3;o Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_T/0/1/0/all/0/1\">Tiago Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Penousal Machado</a>",
          "description": "Genetic Programming (GP) is known to suffer from the burden of being\ncomputationally expensive by design. While, over the years, many techniques\nhave been developed to mitigate this issue, data vectorization, in particular,\nis arguably still the most attractive strategy due to the parallel nature of\nGP. In this work, we employ a series of benchmarks meant to compare both the\nperformance and evolution capabilities of different vectorized and iterative\nimplementation approaches across several existing frameworks. Namely, TensorGP,\na novel open-source engine written in Python, is shown to greatly benefit from\nthe TensorFlow library to accelerate the domain evaluation phase in GP. The\npresented performance benchmarks demonstrate that the TensorGP engine manages\nto pull ahead, with relative speedups above two orders of magnitude for\nproblems with a higher number of fitness cases. Additionally, as a consequence\nof being able to compute larger domains, we argue that TensorGP performance\ngains aid the discovery of more accurate candidate solutions.",
          "link": "http://arxiv.org/abs/2106.11919",
          "publishedOn": "2021-06-23T01:48:40.815Z",
          "wordCount": 596,
          "title": "Speed Benchmarking of Genetic Programming Frameworks. (arXiv:2106.11919v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>",
          "description": "Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.",
          "link": "http://arxiv.org/abs/2106.07346",
          "publishedOn": "2021-06-23T01:48:40.805Z",
          "wordCount": 611,
          "title": "A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1\">Donglin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuaiwen Leon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>",
          "description": "The quest for determinism in machine learning has disproportionately focused\non characterizing the impact of noise introduced by algorithmic design choices.\nIn this work, we address a less well understood and studied question: how does\nour choice of tooling introduce randomness to deep neural network training. We\nconduct large scale experiments across different types of hardware,\naccelerators, state of art networks, and open-source datasets, to characterize\nhow tooling choices contribute to the level of non-determinism in a system, the\nimpact of said non-determinism, and the cost of eliminating different sources\nof noise.\n\nOur findings are surprising, and suggest that the impact of non-determinism\nin nuanced. While top-line metrics such as top-1 accuracy are not noticeably\nimpacted, model performance on certain parts of the data distribution is far\nmore sensitive to the introduction of randomness. Our results suggest that\ndeterministic tooling is critical for AI safety. However, we also find that the\ncost of ensuring determinism varies dramatically between neural network\narchitectures and hardware types, e.g., with overhead up to $746\\%$, $241\\%$,\nand $196\\%$ on a spectrum of widely used GPU accelerator architectures,\nrelative to non-deterministic training. The source code used in this paper is\navailable at https://github.com/usyd-fsalab/NeuralNetworkRandomness.",
          "link": "http://arxiv.org/abs/2106.11872",
          "publishedOn": "2021-06-23T01:48:40.795Z",
          "wordCount": 644,
          "title": "Randomness In Neural Network Training: Characterizing The Impact of Tooling. (arXiv:2106.11872v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mutschler_M/0/1/0/all/0/1\">Maximus Mutschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>",
          "description": "Optimization in Deep Learning is mainly guided by vague intuitions and strong\nassumptions, with a limited understanding how and why these work in practice.\nTo shed more light on this, our work provides some deeper understandings of how\nSGD behaves by empirically analyzing the trajectory taken by SGD from a line\nsearch perspective. Specifically, a costly quantitative analysis of the\nfull-batch loss along SGD trajectories from common used models trained on a\nsubset of CIFAR-10 is performed. Our core results include that the full-batch\nloss along lines in update step direction is highly parabolically. Further on,\nwe show that there exists a learning rate with which SGD always performs almost\nexact line searches on the full-batch loss. Finally, we provide a different\nperspective why increasing the batch size has almost the same effect as\ndecreasing the learning rate by the same factor.",
          "link": "http://arxiv.org/abs/2103.17132",
          "publishedOn": "2021-06-23T01:48:40.786Z",
          "wordCount": 618,
          "title": "Empirically explaining SGD from a line search perspective. (arXiv:2103.17132v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1\">Fu-Shun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shang-Ran Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chien-Wen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan-Ren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1\">Jack Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chung-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Feipei Lai</a>",
          "description": "A continuous real-time respiratory sound automated analysis system is needed\nin clinical practice. Previously, we established an open access lung sound\ndatabase, HF_Lung_V1, and automated lung sound analysis algorithms capable of\ndetecting inhalation, exhalation, continuous adventitious sounds (CASs) and\ndiscontinuous adventitious sounds (DASs). In this study, HF-Lung-V1 has been\nfurther expanded to HF-Lung-V2 with 1.45 times of increase in audio files. The\nconvolutional neural network (CNN)-bidirectional gated recurrent unit (BiGRU)\nmodel was separately trained with training datasets of HF_Lung_V1 (V1_Train)\nand HF_Lung_V2 (V2_Train), and then were used for the performance comparisons\nof segment detection and event detection on both test datasets of HF_Lung_V1\n(V1_Test) and HF_Lung_V2 (V2_Test). The performance of segment detection was\nmeasured by accuracy, predictive positive value (PPV), sensitivity,\nspecificity, F1 score, receiver operating characteristic (ROC) curve and area\nunder the curve (AUC), whereas that of event detection was evaluated with PPV,\nsensitivity, and F1 score. Results indicate that the model performance trained\nby V2_Train showed improvement on both V1_Test and V2_Test in inhalation, CASs\nand DASs, particularly in CASs, as well as on V1_Test in exhalation.",
          "link": "http://arxiv.org/abs/2102.04062",
          "publishedOn": "2021-06-23T01:48:40.774Z",
          "wordCount": 670,
          "title": "An Update of a Progressively Expanded Database for Automated Lung Sound Analysis. (arXiv:2102.04062v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Nandan Kumar Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1\">Zahra Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1\">Brandon Reagen</a>",
          "description": "The recent rise of privacy concerns has led researchers to devise methods for\nprivate neural inference -- where inferences are made directly on encrypted\ndata, never seeing inputs. The primary challenge facing private inference is\nthat computing on encrypted data levies an impractically-high latency penalty,\nstemming mostly from non-linear operators like ReLU. Enabling practical and\nprivate inference requires new optimization methods that minimize network ReLU\ncounts while preserving accuracy. This paper proposes DeepReDuce: a set of\noptimizations for the judicious removal of ReLUs to reduce private inference\nlatency. The key insight is that not all ReLUs contribute equally to accuracy.\nWe leverage this insight to drop, or remove, ReLUs from classic networks to\nsignificantly reduce inference latency and maintain high accuracy. Given a\ntarget network, DeepReDuce outputs a Pareto frontier of networks that tradeoff\nthe number of ReLUs and accuracy. Compared to the state-of-the-art for private\ninference DeepReDuce improves accuracy and reduces ReLU count by up to 3.5%\n(iso-ReLU count) and 3.5$\\times$ (iso-accuracy), respectively.",
          "link": "http://arxiv.org/abs/2103.01396",
          "publishedOn": "2021-06-23T01:48:40.746Z",
          "wordCount": 633,
          "title": "DeepReDuce: ReLU Reduction for Fast Private Inference. (arXiv:2103.01396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04279",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "The classical Langevin Monte Carlo method looks for i.i.d. samples from a\ntarget distribution by descending along the gradient of the target\ndistribution. It is popular partially due to its fast convergence rate.\nHowever, the numerical cost is sometimes high because the gradient can be hard\nto obtain. One approach to eliminate the gradient computation is to employ the\nconcept of \"ensemble\", where a large number of particles are evolved together\nso that the neighboring particles provide gradient information to each other.\nIn this article, we discuss two algorithms that integrate the ensemble feature\ninto LMC, and the associated properties. There are two sides of our discovery:\n\n1. By directly surrogating the gradient using the ensemble approximation, we\ndevelop Ensemble Langevin Monte Carlo. We show that this method is unstable due\nto a potentially small denominator that induces high variance. We provide a\ncounterexample to explicitly show this instability.\n\n2. We then change the strategy and enact the ensemble approximation to the\ngradient only in a constrained manner, to eliminate the unstable points. The\nalgorithm is termed Constrained Ensemble Langevin Monte Carlo. We show that,\nwith a proper tuning, the surrogation takes place often enough to bring the\nreasonable numerical saving, while the induced error is still low enough for us\nto maintain the fast convergence rate, up to a controllable discretization and\nensemble error.\n\nSuch combination of ensemble method and LMC shed light on inventing\ngradient-free algorithms that produce i.i.d. samples almost exponentially fast.",
          "link": "http://arxiv.org/abs/2102.04279",
          "publishedOn": "2021-06-23T01:48:40.728Z",
          "wordCount": 692,
          "title": "Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08902",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Okati_N/0/1/0/all/0/1\">Nastaran Okati</a>, <a href=\"http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Multiple lines of evidence suggest that predictive models may benefit from\nalgorithmic triage. Under algorithmic triage, a predictive model does not\npredict all instances but instead defers some of them to human experts.\nHowever, the interplay between the prediction accuracy of the model and the\nhuman experts under algorithmic triage is not well understood. In this work, we\nstart by formally characterizing under which circumstances a predictive model\nmay benefit from algorithmic triage. In doing so, we also demonstrate that\nmodels trained for full automation may be suboptimal under triage. Then, given\nany model and desired level of triage, we show that the optimal triage policy\nis a deterministic threshold rule in which triage decisions are derived\ndeterministically by thresholding the difference between the model and human\nerrors on a per-instance level. Building upon these results, we introduce a\npractical gradient-based algorithm that is guaranteed to find a sequence of\ntriage policies and predictive models of increasing performance. Experiments on\na wide variety of supervised learning tasks using synthetic and real data from\ntwo important applications -- content moderation and scientific discovery --\nillustrate our theoretical results and show that the models and triage policies\nprovided by our gradient-based algorithm outperform those provided by several\ncompetitive baselines.",
          "link": "http://arxiv.org/abs/2103.08902",
          "publishedOn": "2021-06-23T01:48:40.708Z",
          "wordCount": 648,
          "title": "Differentiable Learning Under Triage. (arXiv:2103.08902v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.02062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chengli Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangshe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junmin Liu</a>",
          "description": "\\textit{Stochastic gradient descent} (SGD) is of fundamental importance in\ndeep learning. Despite its simplicity, elucidating its efficacy remains\nchallenging. Conventionally, the success of SGD is attributed to the\n\\textit{stochastic gradient noise} (SGN) incurred in the training process.\nBased on this general consensus, SGD is frequently treated and analyzed as the\nEuler-Maruyama discretization of a \\textit{stochastic differential equation}\n(SDE) driven by either Brownian or L\\'evy stable motion. In this study, we\nargue that SGN is neither Gaussian nor stable. Instead, inspired by the\nlong-time correlation emerging in SGN series, we propose that SGD can be viewed\nas a discretization of an SDE driven by \\textit{fractional Brownian motion}\n(FBM). Accordingly, the different convergence behavior of SGD dynamics is well\ngrounded. Moreover, the first passage time of an SDE driven by FBM is\napproximately derived. This indicates a lower escaping rate for a larger Hurst\nparameter, and thus SGD stays longer in flat minima. This happens to coincide\nwith the well-known phenomenon that SGD favors flat minima that generalize\nwell. Four groups of experiments are conducted to validate our conjecture, and\nit is demonstrated that long-range memory effects persist across various model\narchitectures, datasets, and training strategies. Our study opens up a new\nperspective and may contribute to a better understanding of SGD.",
          "link": "http://arxiv.org/abs/2105.02062",
          "publishedOn": "2021-06-23T01:48:40.701Z",
          "wordCount": 708,
          "title": "Understanding Long Range Memory Effects in Deep Neural Networks. (arXiv:2105.02062v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>",
          "description": "Semi-supervised learning has been gaining attention as it allows for\nperforming image analysis tasks such as classification with limited labeled\ndata. Some popular algorithms using Generative Adversarial Networks (GANs) for\nsemi-supervised classification share a single architecture for classification\nand discrimination. However, this may require a model to converge to a separate\ndata distribution for each task, which may reduce overall performance. While\nprogress in semi-supervised learning has been made, less addressed are\nsmall-scale, fully-supervised tasks where even unlabeled data is unavailable\nand unattainable. We therefore, propose a novel GAN model namely External\nClassifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to\nimprove classification in fully-supervised regimes. Our method leverages a GAN\nto generate artificial data used to supplement supervised classification. More\nspecifically, we attach an external classifier, hence the name EC-GAN, to the\nGAN's generator, as opposed to sharing an architecture with the discriminator.\nOur experiments demonstrate that EC-GAN's performance is comparable to the\nshared architecture method, far superior to the standard data augmentation and\nregularization-based approach, and effective on a small, realistic dataset.",
          "link": "http://arxiv.org/abs/2012.15864",
          "publishedOn": "2021-06-23T01:48:40.682Z",
          "wordCount": 647,
          "title": "EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.15851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosasco_A/0/1/0/all/0/1\">Andrea Rosasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>",
          "description": "Replay strategies are Continual Learning techniques which mitigate\ncatastrophic forgetting by keeping a buffer of patterns from previous\nexperiences, which are interleaved with new data during training. The amount of\npatterns stored in the buffer is a critical parameter which largely influences\nthe final performance and the memory footprint of the approach. This work\nintroduces Distilled Replay, a novel replay strategy for Continual Learning\nwhich is able to mitigate forgetting by keeping a very small buffer (1 pattern\nper class) of highly informative samples. Distilled Replay builds the buffer\nthrough a distillation process which compresses a large dataset into a tiny set\nof informative examples. We show the effectiveness of our Distilled Replay\nagainst popular replay-based strategies on four Continual Learning benchmarks.",
          "link": "http://arxiv.org/abs/2103.15851",
          "publishedOn": "2021-06-23T01:48:40.675Z",
          "wordCount": 583,
          "title": "Distilled Replay: Overcoming Forgetting through Synthetic Samples. (arXiv:2103.15851v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1\">Quin Thames</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1\">Arjun Karpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1\">Wade Norris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangting Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1\">Liviu Panait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>",
          "description": "Understanding the nutritional content of food from visual data is a\nchallenging computer vision problem, with the potential to have a positive and\nwidespread impact on public health. Studies in this area are limited to\nexisting datasets in the field that lack sufficient diversity or labels\nrequired for training models with nutritional understanding capability. We\nintroduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes\nwith corresponding video streams, depth images, component weights, and high\naccuracy nutritional content annotation. We demonstrate the potential of this\ndataset by training a computer vision algorithm capable of predicting the\ncaloric and macronutrient values of a complex, real world dish at an accuracy\nthat outperforms professional nutritionists. Further we present a baseline for\nincorporating depth sensor data to improve nutrition predictions. We will\npublicly release Nutrition5k in the hope that it will accelerate innovation in\nthe space of nutritional understanding.",
          "link": "http://arxiv.org/abs/2103.03375",
          "publishedOn": "2021-06-23T01:48:40.666Z",
          "wordCount": 630,
          "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1\">Rachana Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1\">Samuel Sharpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1\">Brian Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittenbach_J/0/1/0/all/0/1\">Jason Wittenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1\">C. Bayan Bruss</a>",
          "description": "In the environment of fair lending laws and the General Data Protection\nRegulation (GDPR), the ability to explain a model's prediction is of paramount\nimportance. High quality explanations are the first step in assessing fairness.\nCounterfactuals are valuable tools for explainability. They provide actionable,\ncomprehensible explanations for the individual who is subject to decisions made\nfrom the prediction. It is important to find a baseline for producing them. We\npropose a simple method for generating counterfactuals by using gradient\ndescent to search in the latent space of an autoencoder and benchmark our\nmethod against approaches that search for counterfactuals in feature space.\nAdditionally, we implement metrics to concretely evaluate the quality of the\ncounterfactuals. We show that latent space counterfactual generation strikes a\nbalance between the speed of basic feature gradient descent methods and the\nsparseness and authenticity of counterfactuals generated by more complex\nfeature space oriented techniques.",
          "link": "http://arxiv.org/abs/2012.09301",
          "publishedOn": "2021-06-23T01:48:40.659Z",
          "wordCount": 608,
          "title": "Latent-CF: A Simple Baseline for Reverse Counterfactual Explanations. (arXiv:2012.09301v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shubhada Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1\">Wouter M. Koolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_S/0/1/0/all/0/1\">Sandeep Juneja</a>",
          "description": "Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular\ntail-risk measures in finance and insurance industries as well as in highly\nreliable, safety-critical uncertain environments where often the underlying\nprobability distributions are heavy-tailed. We use the multi-armed bandit\nbest-arm identification framework and consider the problem of identifying the\narm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum\nof CVaR and mean. The latter captures the risk-return trade-off common in\nfinance. Our main contribution is an optimal $\\delta$-correct algorithm that\nacts on general arms, including heavy-tailed distributions, and matches the\nlower bound on the expected number of samples needed, asymptotically (as\n$\\delta$ approaches $0$). The algorithm requires solving a non-convex\noptimization problem in the space of probability measures, that requires\ndelicate analysis. En-route, we develop new non-asymptotic empirical\nlikelihood-based concentration inequalities for tail-risk measures which are\ntighter than those for popular truncation-based empirical estimators.",
          "link": "http://arxiv.org/abs/2008.07606",
          "publishedOn": "2021-06-23T01:48:40.648Z",
          "wordCount": 621,
          "title": "Optimal Best-Arm Identification Methods for Tail-Risk Measures. (arXiv:2008.07606v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.01250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects in\npoint cloud data. We compare the results with different backbone architectures\nincluding the standard ones like VGG, ResNet, Inception with our backbone. Also\nwe present the optimization and ablation studies including designing an\nefficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking\nand validating our results. Our work surpasses the state of the art in this\ndomain both in terms of average precision and speed running at > 30 FPS. This\nmakes it a feasible option to be deployed in real time applications including\nself driving cars.",
          "link": "http://arxiv.org/abs/2006.01250",
          "publishedOn": "2021-06-23T01:48:40.634Z",
          "wordCount": 684,
          "title": "RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1\">Anshuman Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_V/0/1/0/all/0/1\">Vidushi Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1\">Prasant Mohapatra</a>",
          "description": "Hierarchical Agglomerative Clustering (HAC) algorithms are extensively\nutilized in modern data science, and seek to partition the dataset into\nclusters while generating a hierarchical relationship between the data samples.\nHAC algorithms are employed in many applications, such as biology, natural\nlanguage processing, and recommender systems. Thus, it is imperative to ensure\nthat these algorithms are fair -- even if the dataset contains biases against\ncertain protected groups, the cluster outputs generated should not discriminate\nagainst samples from any of these groups. However, recent work in clustering\nfairness has mostly focused on center-based clustering algorithms, such as\nk-median and k-means clustering. In this paper, we propose fair algorithms for\nperforming HAC that enforce fairness constraints 1) irrespective of the\ndistance linkage criteria used, 2) generalize to any natural measures of\nclustering fairness for HAC, 3) work for multiple protected groups, and 4) have\ncompetitive running times to vanilla HAC. Through extensive experiments on\nmultiple real-world UCI datasets, we show that our proposed algorithm finds\nfairer clusterings compared to vanilla HAC as well as other state-of-the-art\nfair clustering approaches.",
          "link": "http://arxiv.org/abs/2005.03197",
          "publishedOn": "2021-06-23T01:48:40.615Z",
          "wordCount": 642,
          "title": "Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1\">Sayyed Ahmad Naghavi Nozad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1\">Maryam Amir Haeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1\">Gianluigi Folino</a>",
          "description": "This paper presents a batch-wise density-based clustering approach for local\noutlier detection in massive-scale datasets. Unlike the well-known traditional\nalgorithms, which assume that all the data is memory-resident, our proposed\nmethod is scalable and processes the input data chunk-by-chunk within the\nconfines of a limited memory buffer. A temporary clustering model is built at\nthe first phase; then, it is gradually updated by analyzing consecutive memory\nloads of points. Subsequently, at the end of scalable clustering, the\napproximate structure of the original clusters is obtained. Finally, by another\nscan of the entire dataset and using a suitable criterion, an outlying score is\nassigned to each object called SDCOR (Scalable Density-based Clustering\nOutlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate\nthat the proposed method has a low linear time complexity and is more effective\nand efficient compared to best-known conventional density-based methods, which\nneed to load all data into the memory; and also, to some fast distance-based\nmethods, which can perform on data resident in the disk.",
          "link": "http://arxiv.org/abs/2006.07616",
          "publishedOn": "2021-06-23T01:48:40.608Z",
          "wordCount": 740,
          "title": "SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v10 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1\">Naeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We propose a novel and effective input transformation based adversarial\ndefense method against gray- and black-box attack, which is computationally\nefficient and does not require any adversarial training or retraining of a\nclassification model. We first show that a very simple iterative Gaussian\nsmoothing can effectively wash out adversarial noise and achieve substantially\nhigh robust accuracy. Based on the observation, we propose Self-Supervised\nIterative Contextual Smoothing (SSICS), which aims to reconstruct the original\ndiscriminative features from the Gaussian-smoothed image in context-adaptive\nmanner, while still smoothing out the adversarial noise. From the experiments\non ImageNet, we show that our SSICS achieves both high standard accuracy and\nvery competitive robust accuracy for the gray- and black-box attacks; e.g.,\ntransfer-based PGD-attack and score-based attack. A note-worthy point to stress\nis that our defense is free of computationally expensive adversarial training,\nyet, can approach its robust accuracy via input transformation.",
          "link": "http://arxiv.org/abs/2106.11644",
          "publishedOn": "2021-06-23T01:48:40.601Z",
          "wordCount": 599,
          "title": "Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Bayesian neural networks perform variational inference over the weights\nhowever calculation of the posterior distribution remains a challenge. Our work\nbuilds on variational inference techniques for bayesian neural networks using\nthe original Evidence Lower Bound. In this paper, we present a stochastic\nbayesian neural network in which we maximize Evidence Lower Bound using a new\nobjective function which we name as Stochastic Evidence Lower Bound. We\nevaluate our network on 5 publicly available UCI datasets using test RMSE and\nlog likelihood as the evaluation metrics. We demonstrate that our work not only\nbeats the previous state of the art algorithms but is also scalable to larger\ndatasets.",
          "link": "http://arxiv.org/abs/2008.07587",
          "publishedOn": "2021-06-23T01:48:40.594Z",
          "wordCount": 580,
          "title": "Stochastic Bayesian Neural Networks. (arXiv:2008.07587v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "The goal of bayesian approach used in variational inference is to minimize\nthe KL divergence between variational distribution and unknown posterior\ndistribution. This is done by maximizing the Evidence Lower Bound (ELBO). A\nneural network is used to parametrize these distributions using Stochastic\nGradient Descent. This work extends the work done by others by deriving the\nvariational inference models. We show how SGD can be applied on bayesian neural\nnetworks by gradient estimation techniques. For validation, we have tested our\nmodel on 5 UCI datasets and the metrics chosen for evaluation are Root Mean\nSquare Error (RMSE) error and negative log likelihood. Our work considerably\nbeats the previous state of the art approaches for regression using bayesian\nneural networks.",
          "link": "http://arxiv.org/abs/2006.08453",
          "publishedOn": "2021-06-23T01:48:40.575Z",
          "wordCount": 615,
          "title": "Bayesian Neural Network via Stochastic Gradient Descent. (arXiv:2006.08453v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11723",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mital_N/0/1/0/all/0/1\">Nitish Mital</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozyilkan_E/0/1/0/all/0/1\">Ezgi Ozyilkan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garjani_A/0/1/0/all/0/1\">Ali Garjani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "We present a novel deep neural network (DNN) architecture for compressing an\nimage when a correlated image is available as side information only at the\ndecoder. This problem is known as distributed source coding (DSC) in\ninformation theory. In particular, we consider a pair of stereo images, which\ngenerally have high correlation with each other due to overlapping fields of\nview, and assume that one image of the pair is to be compressed and\ntransmitted, while the other image is available only at the decoder. In the\nproposed architecture, the encoder maps the input image to a latent space,\nquantizes the latent representation, and compresses it using entropy coding.\nThe decoder is trained to extract the Wyner's common information between the\ninput image and the correlated image from the latter. The received latent\nrepresentation and the locally generated common information are passed through\na decoder network to obtain an enhanced reconstruction of the input image. The\ncommon information provides a succinct representation of the relevant\ninformation at the receiver. We train and demonstrate the effectiveness of the\nproposed approach on the KITTI dataset of stereo image pairs. Our results show\nthat the proposed architecture is capable of exploiting the decoder-only side\ninformation, and outperforms previous work on stereo image compression with\ndecoder side information.",
          "link": "http://arxiv.org/abs/2106.11723",
          "publishedOn": "2021-06-23T01:48:40.568Z",
          "wordCount": 675,
          "title": "Deep Stereo Image Compression with Decoder Side Information using Wyner Common Information. (arXiv:2106.11723v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hsiao-Dong Chiang</a>",
          "description": "Recent progress on deep learning relies heavily on the quality and efficiency\nof training algorithms. In this paper, we develop a fast training method\nmotivated by the nonlinear Conjugate Gradient (CG) framework. We propose the\nConjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a\nquadratic line-search determines the step size according to current loss\nlandscape. On the other hand, the momentum factor is dynamically updated in\ncomputing the conjugate gradient parameter (like Polak-Ribiere). Theoretical\nresults to ensure the convergence of our method in strong convex settings is\ndeveloped. And experiments in image classification datasets show that our\nmethod yields faster convergence than other local solvers and has better\ngeneralization capability (test set accuracy). One major advantage of the paper\nmethod is that tedious hand tuning of hyperparameters like the learning rate\nand momentum is avoided.",
          "link": "http://arxiv.org/abs/2106.11548",
          "publishedOn": "2021-06-23T01:48:40.560Z",
          "wordCount": 571,
          "title": "Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02227",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ji_S/0/1/0/all/0/1\">Shaolin Ji</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peng_S/0/1/0/all/0/1\">Shige Peng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peng_Y/0/1/0/all/0/1\">Ying Peng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1\">Xichuan Zhang</a>",
          "description": "In this paper, we aim to solve the high dimensional stochastic optimal\ncontrol problem from the view of the stochastic maximum principle via deep\nlearning. By introducing the extended Hamiltonian system which is essentially\nan FBSDE with a maximum condition, we reformulate the original control problem\nas a new one. Three algorithms are proposed to solve the new control problem.\nNumerical results for different examples demonstrate the effectiveness of our\nproposed algorithms, especially in high dimensional cases. And an important\napplication of this method is to calculate the sub-linear expectations, which\ncorrespond to a kind of fully nonlinear PDEs.",
          "link": "http://arxiv.org/abs/2007.02227",
          "publishedOn": "2021-06-23T01:48:40.553Z",
          "wordCount": 589,
          "title": "Solving stochastic optimal control problem via stochastic maximum principle with deep learning method. (arXiv:2007.02227v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aker_A/0/1/0/all/0/1\">Ahmet Aker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>",
          "description": "The spreading COVID-19 misinformation over social media already draws the\nattention of many researchers. According to Google Scholar, about 26000\nCOVID-19 related misinformation studies have been published to date. Most of\nthese studies focusing on 1) detect and/or 2) analysing the characteristics of\nCOVID-19 related misinformation. However, the study of the social behaviours\nrelated to misinformation is often neglected. In this paper, we introduce a\nfine-grained annotated misinformation tweets dataset including social\nbehaviours annotation (e.g. comment or question to the misinformation). The\ndataset not only allows social behaviours analysis but also suitable for both\nevidence-based or non-evidence-based misinformation classification task. In\naddition, we introduce leave claim out validation in our experiments and\ndemonstrate the misinformation classification performance could be\nsignificantly different when applying to real-world unseen misinformation.",
          "link": "http://arxiv.org/abs/2106.11702",
          "publishedOn": "2021-06-23T01:48:40.547Z",
          "wordCount": 621,
          "title": "Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of COVID-19 Infodemic. (arXiv:2106.11702v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anand_M/0/1/0/all/0/1\">Mrinal Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayal_P/0/1/0/all/0/1\">Pratik Kayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Automatic code synthesis from natural language descriptions is a challenging\ntask. We witness massive progress in developing code generation systems for\ndomain-specific languages (DSLs) employing sequence-to-sequence deep learning\ntechniques in the recent past. In this paper, we specifically experiment with\n\\textsc{AlgoLisp} DSL-based generative models and showcase the existence of\nsignificant dataset bias through different classes of adversarial examples. We\nalso experiment with two variants of Transformer-based models that outperform\nall existing \\textsc{AlgoLisp} DSL-based code generation baselines. Consistent\nwith the current state-of-the-art systems, our proposed models, too, achieve\npoor performance under adversarial settings. Therefore, we propose several\ndataset augmentation techniques to reduce bias and showcase their efficacy\nusing robust experimentation.",
          "link": "http://arxiv.org/abs/2106.11629",
          "publishedOn": "2021-06-23T01:48:40.539Z",
          "wordCount": 538,
          "title": "On Adversarial Robustness of Synthetic Code Generation. (arXiv:2106.11629v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1\">Francesco D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>",
          "description": "Deep ensembles have recently gained popularity in the deep learning community\nfor their conceptual simplicity and efficiency. However, maintaining functional\ndiversity between ensemble members that are independently trained with gradient\ndescent is challenging. This can lead to pathologies when adding more ensemble\nmembers, such as a saturation of the ensemble performance, which converges to\nthe performance of a single model. Moreover, this does not only affect the\nquality of its predictions, but even more so the uncertainty estimates of the\nensemble, and thus its performance on out-of-distribution data. We hypothesize\nthat this limitation can be overcome by discouraging different ensemble members\nfrom collapsing to the same function. To this end, we introduce a kernelized\nrepulsive term in the update rule of the deep ensembles. We show that this\nsimple modification not only enforces and maintains diversity among the members\nbut, even more importantly, transforms the maximum a posteriori inference into\nproper Bayesian inference. Namely, we show that the training dynamics of our\nproposed repulsive ensembles follow a Wasserstein gradient flow of the KL\ndivergence with the true posterior. We study repulsive terms in weight and\nfunction space and empirically compare their performance to standard ensembles\nand Bayesian baselines on synthetic and real-world prediction tasks.",
          "link": "http://arxiv.org/abs/2106.11642",
          "publishedOn": "2021-06-23T01:48:40.521Z",
          "wordCount": 629,
          "title": "Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xiang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Changhua Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqiang Wang</a>",
          "description": "Recently, Graph Neural Network (GNN) has achieved remarkable success in\nvarious real-world problems on graph data. However in most industries, data\nexists in the form of isolated islands and the data privacy and security is\nalso an important issue. In this paper, we propose FedVGCN, a federated GCN\nlearning paradigm for privacy-preserving node classification task under data\nvertically partitioned setting, which can be generalized to existing GCN\nmodels. Specifically, we split the computation graph data into two parts. For\neach iteration of the training process, the two parties transfer intermediate\nresults to each other under homomorphic encryption. We conduct experiments on\nbenchmark data and the results demonstrate the effectiveness of FedVGCN in the\ncase of GraphSage.",
          "link": "http://arxiv.org/abs/2106.11593",
          "publishedOn": "2021-06-23T01:48:40.504Z",
          "wordCount": 553,
          "title": "A Vertical Federated Learning Framework for Graph Convolutional Network. (arXiv:2106.11593v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabideau_C/0/1/0/all/0/1\">Clayton M. Rabideau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1\">Atsushi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1\">Hajime Asama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinkyoo Park</a>",
          "description": "We introduce the framework of continuous-depth graph neural networks (GNNs).\nNeural graph differential equations (Neural GDEs) are formalized as the\ncounterpart to GNNs where the input-output relationship is determined by a\ncontinuum of GNN layers, blending discrete topological structures and\ndifferential equations. The proposed framework is shown to be compatible with\nstatic GNN models and is extended to dynamic and stochastic settings through\nhybrid dynamical system theory. Here, Neural GDEs improve performance by\nexploiting the underlying dynamics geometry, further introducing the ability to\naccommodate irregularly sampled data. Results prove the effectiveness of the\nproposed models across applications, such as traffic forecasting or prediction\nin genetic regulatory networks.",
          "link": "http://arxiv.org/abs/2106.11581",
          "publishedOn": "2021-06-23T01:48:40.497Z",
          "wordCount": 571,
          "title": "Continuous-Depth Neural Models for Dynamic Graph Prediction. (arXiv:2106.11581v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-Young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>",
          "description": "Federated learning is an emerging machine learning paradigm that enables\nmultiple devices to train models locally and formulate a global model, without\nsharing the clients' local data. A federated learning system can be viewed as a\nlarge-scale distributed system, involving different components and stakeholders\nwith diverse requirements and constraints. Hence, developing a federated\nlearning system requires both software system design thinking and machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning perspectives, our previous systematic literature\nreview on the area shows that there is a distinct lack of considerations for\nsoftware architecture design for federated learning. In this paper, we propose\nFLRA, a reference architecture for federated learning systems, which provides a\ntemplate design for federated learning-based solutions. The proposed FLRA\nreference architecture is based on an extensive review of existing patterns of\nfederated learning systems found in the literature and existing industrial\nimplementation. The FLRA reference architecture consists of a pool of\narchitectural patterns that could address the frequently recurring design\nproblems in federated learning architectures. The FLRA reference architecture\ncan serve as a design guideline to assist architects and developers with\npractical solutions for their problems, which can be further customised.",
          "link": "http://arxiv.org/abs/2106.11570",
          "publishedOn": "2021-06-23T01:48:40.489Z",
          "wordCount": 648,
          "title": "FLRA: A Reference Architecture for Federated Learning Systems. (arXiv:2106.11570v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunchang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Han Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcelon_E/0/1/0/all/0/1\">Evrard Garcelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1\">Matteo Pirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1\">Alessandro Lazaric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study bandits and reinforcement learning (RL) subject to a conservative\nconstraint where the agent is asked to perform at least as well as a given\nbaseline policy. This setting is particular relevant in real-world domains\nincluding digital marketing, healthcare, production, finance, etc. For\nmulti-armed bandits, linear bandits and tabular RL, specialized algorithms and\ntheoretical analyses were proposed in previous work. In this paper, we present\na unified framework for conservative bandits and RL, in which our core\ntechnique is to calculate the necessary and sufficient budget obtained from\nrunning the baseline policy. For lower bounds, our framework gives a black-box\nreduction that turns a certain lower bound in the nonconservative setting into\na new lower bound in the conservative setting. We strengthen the existing lower\nbound for conservative multi-armed bandits and obtain new lower bounds for\nconservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our\nframework turns a certain nonconservative upper-confidence-bound (UCB)\nalgorithm into a conservative algorithm with a simple analysis. For multi-armed\nbandits, linear bandits and tabular RL, our new upper bounds tighten or match\nexisting ones with significantly simpler analyses. We also obtain a new upper\nbound for conservative low-rank MDP.",
          "link": "http://arxiv.org/abs/2106.11692",
          "publishedOn": "2021-06-23T01:48:40.480Z",
          "wordCount": 637,
          "title": "A Unified Framework for Conservative Exploration. (arXiv:2106.11692v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hanxuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Qingchao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>",
          "description": "Graph representation learning is a fundamental problem for modeling\nrelational data and benefits a number of downstream applications. Traditional\nBayesian-based graph models and recent deep learning based GNN either suffer\nfrom impracticability or lack interpretability, thus combined models for\nundirected graphs have been proposed to overcome the weaknesses. As a large\nportion of real-world graphs are directed graphs (of which undirected graphs\nare special cases), in this paper, we propose a Deep Latent Space Model (DLSM)\nfor directed graphs to incorporate the traditional latent variable based\ngenerative model into deep learning frameworks. Our proposed model consists of\na graph convolutional network (GCN) encoder and a stochastic decoder, which are\nlayer-wise connected by a hierarchical variational auto-encoder architecture.\nBy specifically modeling the degree heterogeneity using node random factors,\nour model possesses better interpretability in both community structure and\ndegree heterogeneity. For fast inference, the stochastic gradient variational\nBayes (SGVB) is adopted using a non-iterative recognition model, which is much\nmore scalable than traditional MCMC-based methods. The experiments on\nreal-world datasets show that the proposed model achieves the state-of-the-art\nperformances on both link prediction and community detection tasks while\nlearning interpretable node embeddings. The source code is available at\nhttps://github.com/upperr/DLSM.",
          "link": "http://arxiv.org/abs/2106.11721",
          "publishedOn": "2021-06-23T01:48:40.460Z",
          "wordCount": 635,
          "title": "A Deep Latent Space Model for Graph Representation Learning. (arXiv:2106.11721v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1\">Mark Bun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Shay Moran</a>",
          "description": "We prove that every concept class with finite Littlestone dimension can be\nlearned by an (approximate) differentially-private algorithm. This answers an\nopen question of Alon et al. (STOC 2019) who proved the converse statement\n(this question was also asked by Neel et al.~(FOCS 2019)). Together these two\nresults yield an equivalence between online learnability and private PAC\nlearnability.\n\nWe introduce a new notion of algorithmic stability called \"global stability\"\nwhich is essential to our proof and may be of independent interest. We also\ndiscuss an application of our results to boosting the privacy and accuracy\nparameters of differentially-private learners.",
          "link": "http://arxiv.org/abs/2003.00563",
          "publishedOn": "2021-06-23T01:48:40.452Z",
          "wordCount": 615,
          "title": "An Equivalence Between Private Classification and Online Prediction. (arXiv:2003.00563v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dapeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunpeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Guoliang Fan</a>",
          "description": "In the real world, many tasks require multiple agents to cooperate with each\nother under the condition of local observations. To solve such problems, many\nmulti-agent reinforcement learning methods based on Centralized Training with\nDecentralized Execution have been proposed. One representative class of work is\nvalue decomposition, which decomposes the global joint Q-value $Q_\\text{jt}$\ninto individual Q-values $Q_a$ to guide individuals' behaviors, e.g. VDN\n(Value-Decomposition Networks) and QMIX. However, these baselines often ignore\nthe randomness in the situation. We propose MMD-MIX, a method that combines\ndistributional reinforcement learning and value decomposition to alleviate the\nabove weaknesses. Besides, to improve data sampling efficiency, we were\ninspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to\nexplicitly introduce randomness into the MMD-MIX. The experiments demonstrate\nthat MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge\n(SMAC) environment.",
          "link": "http://arxiv.org/abs/2106.11652",
          "publishedOn": "2021-06-23T01:48:40.445Z",
          "wordCount": 597,
          "title": "MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2106.11652v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Abhin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>",
          "description": "Treatment effect estimation from observational data is a fundamental problem\nin causal inference. There are two very different schools of thought that have\ntackled this problem. On the one hand, the Pearlian framework commonly assumes\nstructural knowledge (provided by an expert) in the form of Directed Acyclic\nGraphs (DAGs) and provides graphical criteria such as the back-door criterion\nto identify the valid adjustment sets. On the other hand, the potential\noutcomes (PO) framework commonly assumes that all the observed features satisfy\nignorability (i.e., no hidden confounding), which in general is untestable. In\nthis work, we take steps to bridge these two frameworks. We show that even if\nwe know only one parent of the treatment variable (provided by an expert), then\nquite remarkably it suffices to test a broad class of (but not all) back-door\ncriteria. Importantly, we also cover the non-trivial case where the entire set\nof observed features is not ignorable (generalizing the PO framework) without\nrequiring all the parents of the treatment variable to be observed. Our key\ntechnical idea involves a more general result -- Given a synthetic sub-sampling\n(or environment) variable that is a function of the parent variable, we show\nthat an invariance test involving this sub-sampling variable is equivalent to\ntesting a broad class of back-door criteria. We demonstrate our approach on\nsynthetic data as well as real causal effect estimation benchmarks.",
          "link": "http://arxiv.org/abs/2106.11560",
          "publishedOn": "2021-06-23T01:48:40.438Z",
          "wordCount": 659,
          "title": "Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. (arXiv:2106.11560v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1\">Michael Rapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mencia_E/0/1/0/all/0/1\">Eneldo Loza Menc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>",
          "description": "In multi-label classification, where a single example may be associated with\nseveral class labels at the same time, the ability to model dependencies\nbetween labels is considered crucial to effectively optimize non-decomposable\nevaluation measures, such as the Subset 0/1 loss. The gradient boosting\nframework provides a well-studied foundation for learning models that are\nspecifically tailored to such a loss function and recent research attests the\nability to achieve high predictive accuracy in the multi-label setting. The\nutilization of second-order derivatives, as used by many recent boosting\napproaches, helps to guide the minimization of non-decomposable losses, due to\nthe information about pairs of labels it incorporates into the optimization\nprocess. On the downside, this comes with high computational costs, even if the\nnumber of labels is small. In this work, we address the computational\nbottleneck of such approach -- the need to solve a system of linear equations\n-- by integrating a novel approximation technique into the boosting procedure.\nBased on the derivatives computed during training, we dynamically group the\nlabels into a predefined number of bins to impose an upper bound on the\ndimensionality of the linear system. Our experiments, using an existing\nrule-based algorithm, suggest that this may boost the speed of training,\nwithout any significant loss in predictive performance.",
          "link": "http://arxiv.org/abs/2106.11690",
          "publishedOn": "2021-06-23T01:48:40.431Z",
          "wordCount": 638,
          "title": "Gradient-based Label Binning in Multi-label Classification. (arXiv:2106.11690v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11514",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yue Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>",
          "description": "Adaptive gradient methods, such as \\textsc{Adam}, have achieved tremendous\nsuccess in machine learning. Scaling gradients by square roots of the running\naverages of squared past gradients, such methods are able to attain rapid\ntraining of modern deep neural networks. Nevertheless, they are observed to\ngeneralize worse than stochastic gradient descent (\\textsc{SGD}) and tend to be\ntrapped in local minima at an early stage during training. Intriguingly, we\ndiscover that substituting the gradient in the preconditioner term with the\nmomentumized version in \\textsc{Adam} can well solve the issues. The intuition\nis that gradient with momentum contains more accurate directional information\nand therefore its second moment estimation is a better choice for scaling than\nraw gradient's. Thereby we propose \\textsc{AdaMomentum} as a new optimizer\nreaching the goal of training faster while generalizing better. We further\ndevelop a theory to back up the improvement in optimization and generalization\nand provide convergence guarantee under both convex and nonconvex settings.\nExtensive experiments on various models and tasks demonstrate that\n\\textsc{AdaMomentum} exhibits comparable performance to \\textsc{SGD} on vision\ntasks, and achieves state-of-the-art results consistently on other tasks\nincluding language processing.",
          "link": "http://arxiv.org/abs/2106.11514",
          "publishedOn": "2021-06-23T01:48:40.412Z",
          "wordCount": 630,
          "title": "Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization. (arXiv:2106.11514v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansal_R/0/1/0/all/0/1\">Raghav Kansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orzari_B/0/1/0/all/0/1\">Breno Orzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomei_T/0/1/0/all/0/1\">Thiago Tomei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touranakou_M/0/1/0/all/0/1\">Mary Touranakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlimant_J/0/1/0/all/0/1\">Jean-Roch Vlimant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunopulos_D/0/1/0/all/0/1\">Dimitrios Gunopulos</a>",
          "description": "In high energy physics (HEP), jets are collections of correlated particles\nproduced ubiquitously in particle collisions such as those at the CERN Large\nHadron Collider (LHC). Machine-learning-based generative models, such as\ngenerative adversarial networks (GANs), have the potential to significantly\naccelerate LHC jet simulations. However, despite jets having a natural\nrepresentation as a set of particles in momentum-space, a.k.a. a particle\ncloud, to our knowledge there exist no generative models applied to such a\ndataset. We introduce a new particle cloud dataset (JetNet), and, due to\nsimilarities between particle and point clouds, apply to it existing point\ncloud GANs. Results are evaluated using (1) the 1-Wasserstein distance between\nhigh- and low-level feature distributions, (2) a newly developed Fr\\'{e}chet\nParticleNet Distance, and (3) the coverage and (4) minimum matching distance\nmetrics. Existing GANs are found to be inadequate for physics applications,\nhence we develop a new message passing GAN (MPGAN), which outperforms existing\npoint cloud GANs on virtually every metric and shows promise for use in HEP. We\npropose JetNet as a novel point-cloud-style dataset for the machine learning\ncommunity to experiment with, and set MPGAN as a benchmark to improve upon for\nfuture generative models.",
          "link": "http://arxiv.org/abs/2106.11535",
          "publishedOn": "2021-06-23T01:48:40.405Z",
          "wordCount": 657,
          "title": "Particle Cloud Generation with Message Passing Generative Adversarial Networks. (arXiv:2106.11535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11633",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Boas_B/0/1/0/all/0/1\">Brenda Vilas Boas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zirwas_W/0/1/0/all/0/1\">Wolfgang Zirwas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haardt_M/0/1/0/all/0/1\">Martin Haardt</a>",
          "description": "A variety of wireless channel estimation methods, e.g., MUSIC and ESPRIT,\nrely on prior knowledge of the model order. Therefore, it is important to\ncorrectly estimate the number of multipath components (MPCs) which compose such\nchannels. However, environments with many scatterers may generate MPCs which\nare closely spaced. This clustering of MPCs in addition to noise makes the\nmodel order selection task difficult in practice to currently known algorithms.\nIn this paper, we exploit the multidimensional characteristics of MIMO\northogonal frequency division multiplexing (OFDM) systems and propose a machine\nlearning (ML) method capable of determining the number of MPCs with a higher\naccuracy than state of the art methods in almost coherent scenarios. Moreover,\nour results show that our proposed ML method has an enhanced reliability.",
          "link": "http://arxiv.org/abs/2106.11633",
          "publishedOn": "2021-06-23T01:48:40.398Z",
          "wordCount": 572,
          "title": "Machine Learning for Model Order Selection in MIMO OFDM Systems. (arXiv:2106.11633v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11603",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1\">Grzegorz Ciesielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Dzikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1\">Mateusz Opala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1\">Piotr Pusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>",
          "description": "We present a number of low-resource approaches to the tasks of the Zero\nResource Speech Challenge 2021. We build on the unsupervised representations of\nspeech proposed by the organizers as a baseline, derived from CPC and clustered\nwith the k-means algorithm. We demonstrate that simple methods of refining\nthose representations can narrow the gap, or even improve upon the solutions\nwhich use a high computational budget. The results lead to the conclusion that\nthe CPC-derived representations are still too noisy for training language\nmodels, but stable enough for simpler forms of pattern matching and retrieval.",
          "link": "http://arxiv.org/abs/2106.11603",
          "publishedOn": "2021-06-23T01:48:40.391Z",
          "wordCount": 557,
          "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw. (arXiv:2106.11603v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghzadeh_A/0/1/0/all/0/1\">Amir Mahdi Sadeghzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_F/0/1/0/all/0/1\">Faezeh Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobhanian_A/0/1/0/all/0/1\">Amir Mohammad Sobhanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalili_R/0/1/0/all/0/1\">Rasool Jalili</a>",
          "description": "Several recent studies have shown that Deep Neural Network (DNN)-based\nclassifiers are vulnerable against model extraction attacks. In model\nextraction attacks, an adversary exploits the target classifier to create a\nsurrogate classifier imitating the target classifier with respect to some\ncriteria. In this paper, we investigate the hardness degree of samples and\ndemonstrate that the hardness degree histogram of model extraction attacks\nsamples is distinguishable from the hardness degree histogram of normal\nsamples. Normal samples come from the target classifier's training data\ndistribution. As the training process of DNN-based classifiers is done in\nseveral epochs, we can consider this process as a sequence of subclassifiers so\nthat each subclassifier is created at the end of an epoch. We use the sequence\nof subclassifiers to calculate the hardness degree of samples. We investigate\nthe relation between hardness degree of samples and the trust in the classifier\noutputs. We propose Hardness-Oriented Detection Approach (HODA) to detect the\nsample sequences of model extraction attacks. The results demonstrate that HODA\ncan detect the sample sequences of model extraction attacks with a high success\nrate by only watching 100 attack samples. We also investigate the hardness\ndegree of adversarial examples and indicate that the hardness degree histogram\nof adversarial examples is distinct from the hardness degree histogram of\nnormal samples.",
          "link": "http://arxiv.org/abs/2106.11424",
          "publishedOn": "2021-06-23T01:48:40.383Z",
          "wordCount": 673,
          "title": "Hardness of Samples Is All You Need: Protecting Deep Learning Models Using Hardness of Samples. (arXiv:2106.11424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1\">Philippe Mary</a> (IETR), <a href=\"http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1\">Visa Koivunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1\">Christophe Moy</a> (IETR)",
          "description": "In this chapter, we will give comprehensive examples of applying RL in\noptimizing the physical layer of wireless communications by defining different\nclass of problems and the possible solutions to handle them. In Section 9.2, we\npresent all the basic theory needed to address a RL problem, i.e. Markov\ndecision process (MDP), Partially observable Markov decision process (POMDP),\nbut also two very important and widely used algorithms for RL, i.e. the\nQ-learning and SARSA algorithms. We also introduce the deep reinforcement\nlearning (DRL) paradigm and the section ends with an introduction to the\nmulti-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples\nto illustrate how the basic concepts of RL are employed in communication\nsystems. We present applications extracted from literature with simplified\nsystem models using similar notation as in Section 9.2 of this Chapter. In\nSection 9.3, we also focus on modeling RL problems, i.e. how action and state\nspaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a\nprospective thought on RL trends and it ends with a review of a broader state\nof the art in Section 9.5.",
          "link": "http://arxiv.org/abs/2106.11595",
          "publishedOn": "2021-06-23T01:48:40.362Z",
          "wordCount": 639,
          "title": "Reinforcement learning for PHY layer communications. (arXiv:2106.11595v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1\">Amir Hosein Afandizadeh Zargari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1\">Seyed Amir Hossein Aqajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodabandeh_H/0/1/0/all/0/1\">Hadi Khodabandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurdahi_F/0/1/0/all/0/1\">Fadi Kurdahi</a>",
          "description": "A photoplethysmography (PPG) is an uncomplicated and inexpensive optical\ntechnique widely used in the healthcare domain to extract valuable\nhealth-related information, e.g., heart rate variability, blood pressure, and\nrespiration rate. PPG signals can easily be collected continuously and remotely\nusing portable wearable devices. However, these measuring devices are\nvulnerable to motion artifacts caused by daily life activities. The most common\nways to eliminate motion artifacts use extra accelerometer sensors, which\nsuffer from two limitations: i) high power consumption and ii) the need to\nintegrate an accelerometer sensor in a wearable device (which is not required\nin certain wearables). This paper proposes a low-power non-accelerometer-based\nPPG motion artifacts removal method outperforming the accuracy of the existing\nmethods. We use Cycle Generative Adversarial Network to reconstruct clean PPG\nsignals from noisy PPG signals. Our novel machine-learning-based technique\nachieves 9.5 times improvement in motion artifact removal compared to the\nstate-of-the-art without using extra sensors such as an accelerometer.",
          "link": "http://arxiv.org/abs/2106.11512",
          "publishedOn": "2021-06-23T01:48:40.355Z",
          "wordCount": 601,
          "title": "An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN. (arXiv:2106.11512v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungmin Cha. Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-06-23T01:48:40.347Z",
          "wordCount": 631,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-06-23T01:48:40.340Z",
          "wordCount": 692,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trautner_M/0/1/0/all/0/1\">Margaret Trautner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1\">Sai Ravela</a>",
          "description": "The optimal design of neural networks is a critical problem in many\napplications. Here, we investigate how dynamical systems with polynomial\nnonlinearities can inform the design of neural systems that seek to emulate\nthem. We propose a Learnability metric and its associated features to quantify\nthe near-equilibrium behavior of learning dynamics. Equating the Learnability\nof neural systems with equivalent parameter estimation metric of the reference\nsystem establishes bounds on network structure. In this way, norms from theory\nprovide a good first guess for neural structure, which may then further adapt\nwith data. The proposed approach neither requires training nor training data.\nIt reveals exact sizing for a class of neural networks with multiplicative\nnodes that mimic continuous- or discrete-time polynomial dynamics. It also\nprovides relatively tight lower size bounds for classical feed-forward networks\nthat is consistent with simulated assessments.",
          "link": "http://arxiv.org/abs/2106.11409",
          "publishedOn": "2021-06-23T01:48:40.332Z",
          "wordCount": 592,
          "title": "Learn Like The Pro: Norms from Theory to Size Neural Computation. (arXiv:2106.11409v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>",
          "description": "Logical relations widely exist in human activities. Human use them for making\njudgement and decision according to various conditions, which are embodied in\nthe form of \\emph{if-then} rules. As an important kind of cognitive\nintelligence, it is prerequisite of representing and storing logical relations\nrightly into computer systems so as to make automatic judgement and decision,\nespecially for high-risk domains like medical diagnosis. However, current\nnumeric ANN (Artificial Neural Network) models are good at perceptual\nintelligence such as image recognition while they are not good at cognitive\nintelligence such as logical representation, blocking the further application\nof ANN. To solve it, researchers have tried to design logical ANN models to\nrepresent and store logical relations. Although there are some advances in this\nresearch area, recent works still have disadvantages because the structures of\nthese logical ANN models still don't map more directly with logical relations\nwhich will cause the corresponding logical relations cannot be read out from\ntheir network structures. Therefore, in order to represent logical relations\nmore clearly by the neural network structure and to read out logical relations\nfrom it, this paper proposes a novel logical ANN model by designing the new\nlogical neurons and links in demand of logical representation. Compared with\nthe recent works on logical ANN models, this logical ANN model has more clear\ncorresponding with logical relations using the more direct mapping method\nherein, thus logical relations can be read out following the connection\npatterns of the network structure. Additionally, less neurons are used.",
          "link": "http://arxiv.org/abs/2106.11463",
          "publishedOn": "2021-06-23T01:48:40.312Z",
          "wordCount": 694,
          "title": "A Logical Neural Network Structure With More Direct Mapping From Logical Relations. (arXiv:2106.11463v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debapriya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lygerakis_F/0/1/0/all/0/1\">Fotios Lygerakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1\">Fillia Makedon</a>",
          "description": "Multi-modal sentiment analysis plays an important role for providing better\ninteractive experiences to users. Each modality in multi-modal data can provide\ndifferent viewpoints or reveal unique aspects of a user's emotional state. In\nthis work, we use text, audio and visual modalities from MOSI dataset and we\npropose a novel fusion technique using a multi-head attention LSTM network.\nFinally, we perform a classification task and evaluate its performance.",
          "link": "http://arxiv.org/abs/2106.11473",
          "publishedOn": "2021-06-23T01:48:40.305Z",
          "wordCount": 502,
          "title": "Sequential Late Fusion Technique for Multi-modal Sentiment Analysis. (arXiv:2106.11473v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafez_A/0/1/0/all/0/1\">Ahmad Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praphul_A/0/1/0/all/0/1\">Atulya Praphul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaradt_Y/0/1/0/all/0/1\">Yousef Jaradt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godwin_E/0/1/0/all/0/1\">Ezani Godwin</a>",
          "description": "Learning node representations on temporal graphs is a fundamental step to\nlearn real-word dynamic graphs efficiently. Real-world graphs have the nature\nof continuously evolving over time, such as changing edges weights, removing\nand adding nodes and appearing and disappearing of edges, while previous graph\nrepresentation learning methods focused generally on static graphs. We present\nConvDySAT as an enhancement of DySAT, one of the state-of-the-art dynamic\nmethods, by augmenting convolution neural networks with the self-attention\nmechanism, the employed method in DySAT to express the structural and temporal\nevolution. We conducted single-step link prediction on a communication network\nand rating network, Experimental results show significant performance gains for\nConvDySAT over various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11430",
          "publishedOn": "2021-06-23T01:48:40.299Z",
          "wordCount": 553,
          "title": "ConvDySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention and Convolutional Neural Networks. (arXiv:2106.11430v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>",
          "description": "Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.",
          "link": "http://arxiv.org/abs/2106.11481",
          "publishedOn": "2021-06-23T01:48:40.291Z",
          "wordCount": 722,
          "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "We characterize the measurement complexity of compressed sensing of signals\ndrawn from a known prior distribution, even when the support of the prior is\nthe entire space (rather than, say, sparse vectors). We show for Gaussian\nmeasurements and \\emph{any} prior distribution on the signal, that the\nposterior sampling estimator achieves near-optimal recovery guarantees.\nMoreover, this result is robust to model mismatch, as long as the distribution\nestimate (e.g., from an invertible generative model) is close to the true\ndistribution in Wasserstein distance. We implement the posterior sampling\nestimator for deep generative priors using Langevin dynamics, and empirically\nfind that it produces accurate estimates with more diversity than MAP.",
          "link": "http://arxiv.org/abs/2106.11438",
          "publishedOn": "2021-06-23T01:48:40.285Z",
          "wordCount": 545,
          "title": "Instance-Optimal Compressed Sensing via Posterior Sampling. (arXiv:2106.11438v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Treven_L/0/1/0/all/0/1\">Lenart Treven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenk_P/0/1/0/all/0/1\">Philippe Wenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfler_F/0/1/0/all/0/1\">Florian D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Differential equations in general and neural ODEs in particular are an\nessential technique in continuous-time system identification. While many\ndeterministic learning algorithms have been designed based on numerical\nintegration via the adjoint method, many downstream tasks such as active\nlearning, exploration in reinforcement learning, robust control, or filtering\nrequire accurate estimates of predictive uncertainties. In this work, we\npropose a novel approach towards estimating epistemically uncertain neural\nODEs, avoiding the numerical integration bottleneck. Instead of modeling\nuncertainty in the ODE parameters, we directly model uncertainties in the state\nspace. Our algorithm - distributional gradient matching (DGM) - jointly trains\na smoother and a dynamics model and matches their gradients via minimizing a\nWasserstein loss. Our experiments show that, compared to traditional\napproximate inference methods based on numerical integration, our approach is\nfaster to train, faster at predicting previously unseen trajectories, and in\nthe context of neural ODEs, significantly more accurate.",
          "link": "http://arxiv.org/abs/2106.11609",
          "publishedOn": "2021-06-23T01:48:40.263Z",
          "wordCount": 592,
          "title": "Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models. (arXiv:2106.11609v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1\">Tung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1\">Atsuhiro Takasu</a>",
          "description": "Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.",
          "link": "http://arxiv.org/abs/2106.11541",
          "publishedOn": "2021-06-23T01:48:40.253Z",
          "wordCount": 686,
          "title": "Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11451",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+He_Q/0/1/0/all/0/1\">QiZhi He</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1\">Panos Stinis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre Tartakovsky</a>",
          "description": "In this paper, we present a physics-constrained deep neural network (PCDNN)\nmethod for parameter estimation in the zero-dimensional (0D) model of the\nvanadium redox flow battery (VRFB). In this approach, we use deep neural\nnetworks (DNNs) to approximate the model parameters as functions of the\noperating conditions. This method allows the integration of the VRFB\ncomputational models as the physical constraints in the parameter learning\nprocess, leading to enhanced accuracy of parameter estimation and cell voltage\nprediction. Using an experimental dataset, we demonstrate that the PCDNN method\ncan estimate model parameters for a range of operating conditions and improve\nthe 0D model prediction of voltage compared to the 0D model prediction with\nconstant operation-condition-independent parameters estimated with traditional\ninverse methods. We also demonstrate that the PCDNN approach has an improved\ngeneralization ability for estimating parameter values for operating conditions\nnot used in the DNN training.",
          "link": "http://arxiv.org/abs/2106.11451",
          "publishedOn": "2021-06-23T01:48:40.247Z",
          "wordCount": 587,
          "title": "Physics-constrained deep neural network method for estimating parameters in a redox flow battery. (arXiv:2106.11451v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Steven Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "With leveraging the weight-sharing and continuous relaxation to enable\ngradient-descent to alternately optimize the supernet weights and the\narchitecture parameters through a bi-level optimization paradigm,\n\\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream\nmethod in Neural Architecture Search (NAS) due to its simplicity and\nefficiency. However, more recent works found that the performance of the\nsearched architecture barely increases with the optimization proceeding in\nDARTS. In addition, several concurrent works show that the NAS could find more\ncompetitive architectures without labels. The above observations reveal that\nthe supervision signal in DARTS may be a poor indicator for architecture\noptimization, inspiring a foundational question: instead of using the\nsupervision signal to perform bi-level optimization, \\textit{can we find\nhigh-quality architectures \\textbf{without any training nor labels}}? We\nprovide an affirmative answer by customizing the NAS as a network pruning at\ninitialization problem. By leveraging recent techniques on the network pruning\nat initialization, we designed a FreeFlow proxy to score the importance of\ncandidate operations in NAS without any training nor labels, and proposed a\nnovel framework called \\textit{training and label free neural architecture\nsearch} (\\textbf{FreeNAS}) accordingly. We show that, without any training nor\nlabels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS\nbaselines. More importantly, our framework is extremely efficient, which\ncompletes the architecture search within only \\textbf{3.6s} and \\textbf{79s} on\na single GPU for the NAS-Bench-201 and DARTS search space, respectively. We\nhope our work inspires more attempts in solving NAS from the perspective of\npruning at initialization.",
          "link": "http://arxiv.org/abs/2106.11542",
          "publishedOn": "2021-06-23T01:48:40.240Z",
          "wordCount": 693,
          "title": "Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongho Kim</a>",
          "description": "The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.",
          "link": "http://arxiv.org/abs/2106.11528",
          "publishedOn": "2021-06-23T01:48:40.232Z",
          "wordCount": 582,
          "title": "Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11447",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Louren&#xe7;o Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1\">Miguel Nobre Menezes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1\">Tiago Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1\">Beatriz Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1\">Fausto J. Pinto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>",
          "description": "Coronary X-ray angiography is a crucial clinical procedure for the diagnosis\nand treatment of coronary artery disease, which accounts for roughly 16% of\nglobal deaths every year. However, the images acquired in these procedures have\nlow resolution and poor contrast, making lesion detection and assessment\nchallenging. Accurate coronary artery segmentation not only helps mitigate\nthese problems, but also allows the extraction of relevant anatomical features\nfor further analysis by quantitative methods. Although automated segmentation\nof coronary arteries has been proposed before, previous approaches have used\nnon-optimal segmentation criteria, leading to less useful results. Most methods\neither segment only the major vessel, discarding important information from the\nremaining ones, or segment the whole coronary tree based mostly on contrast\ninformation, producing a noisy output that includes vessels that are not\nrelevant for diagnosis. We adopt a better-suited clinical criterion and segment\nvessels according to their clinical relevance. Additionally, we simultaneously\nperform catheter segmentation, which may be useful for diagnosis due to the\nscale factor provided by the catheter's known diameter, and is a task that has\nnot yet been performed with good results. To derive the optimal approach, we\nconducted an extensive comparative study of encoder-decoder architectures\ntrained on a combination of focal loss and a variant of generalized dice loss.\nBased on the EfficientNet and the UNet++ architectures, we propose a line of\nefficient and high-performance segmentation models using a new decoder\narchitecture, the EfficientUNet++, whose best-performing version achieved\naverage dice scores of 0.8904 and 0.7526 for the artery and catheter classes,\nrespectively, and an average generalized dice score of 0.9234.",
          "link": "http://arxiv.org/abs/2106.11447",
          "publishedOn": "2021-06-23T01:48:40.214Z",
          "wordCount": 719,
          "title": "Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Duo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1\">Faramarz Fekri</a>",
          "description": "Recently deep reinforcement learning has achieved tremendous success in wide\nranges of applications. However, it notoriously lacks data-efficiency and\ninterpretability. Data-efficiency is important as interacting with the\nenvironment is expensive. Further, interpretability can increase the\ntransparency of the black-box-style deep RL models and hence gain trust from\nthe users. In this work, we propose a new hierarchical framework via symbolic\nRL, leveraging a symbolic transition model to improve the data-efficiency and\nintroduce the interpretability for learned policy. This framework consists of a\nhigh-level agent, a subtask solver and a symbolic transition model. Without\nassuming any prior knowledge on the state transition, we adopt inductive logic\nprogramming (ILP) to learn the rules of symbolic state transitions, introducing\ninterpretability and making the learned behavior understandable to users. In\nempirical experiments, we confirmed that the proposed framework offers\napproximately between 30\\% to 40\\% more data efficiency over previous methods.",
          "link": "http://arxiv.org/abs/2106.11417",
          "publishedOn": "2021-06-23T01:48:40.204Z",
          "wordCount": 573,
          "title": "Interpretable Model-based Hierarchical Reinforcement Learning using Inductive Logic Programming. (arXiv:2106.11417v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1\">Freddie Bickford Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1\">Brett D Roads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaoliang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C Love</a>",
          "description": "Top-down attention allows neural networks, both artificial and biological, to\nfocus on the information most relevant for a given task. This is known to\nenhance performance in visual perception. But it remains unclear how attention\nbrings about its perceptual boost, especially when it comes to naturalistic\nsettings like recognising an object in an everyday scene. What aspects of a\nvisual task does attention help to deal with? We aim to answer this with a\ncomputational experiment based on a general framework called task-oriented\nablation design. First we define a broad range of visual tasks and identify six\nfactors that underlie task variability. Then on each task we compare the\nperformance of two neural networks, one with top-down attention and one\nwithout. These comparisons reveal the task-dependence of attention's perceptual\nboost, giving a clearer idea of the role attention plays. Whereas many existing\ncognitive accounts link attention to stimulus-level variables, such as visual\nclutter and object scale, we find greater explanatory power in system-level\nvariables that capture the interaction between the model, the distribution of\ntraining data and the task format. This finding suggests a shift in how\nattention is studied could be fruitful. We make publicly available our code and\nresults, along with statistics relevant to ImageNet-based experiments beyond\nthis one. Our contribution serves to support the development of more human-like\nvision models and the design of more informative machine-learning experiments.",
          "link": "http://arxiv.org/abs/2106.11339",
          "publishedOn": "2021-06-23T01:48:40.114Z",
          "wordCount": 672,
          "title": "Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Ye Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chun Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yiping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Shaunak Chatterjee</a>",
          "description": "Social media platforms bring together content creators and content consumers\nthrough recommender systems like newsfeed. The focus of such recommender\nsystems has thus far been primarily on modeling the content consumer\npreferences and optimizing for their experience. However, it is equally\ncritical to nurture content creation by prioritizing the creators' interests,\nas quality content forms the seed for sustainable engagement and conversations,\nbringing in new consumers while retaining existing ones. In this work, we\npropose a modeling approach to predict how feedback from content consumers\nincentivizes creators. We then leverage this model to optimize the newsfeed\nexperience for content creators by reshaping the feedback distribution, leading\nto a more active content ecosystem. Practically, we discuss how we balance the\nuser experience for both consumers and creators, and how we carry out online\nA/B tests with strong network effects. We present a deployed use case on the\nLinkedIn newsfeed, where we used this approach to improve content creation\nsignificantly without compromising the consumers' experience.",
          "link": "http://arxiv.org/abs/2106.11312",
          "publishedOn": "2021-06-23T01:48:40.105Z",
          "wordCount": 626,
          "title": "Feedback Shaping: A Modeling Approach to Nurture Content Creation. (arXiv:2106.11312v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmed El-Sallab</a>",
          "description": "Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.",
          "link": "http://arxiv.org/abs/2106.11401",
          "publishedOn": "2021-06-23T01:48:40.099Z",
          "wordCount": 599,
          "title": "Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11374",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bhogi_K/0/1/0/all/0/1\">Keerthana Bhogi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saha_C/0/1/0/all/0/1\">Chiranjib Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhillon_H/0/1/0/all/0/1\">Harpreet S. Dhillon</a>",
          "description": "This paper develops an efficient procedure for designing low-complexity\ncodebooks for precoding in a full-dimension (FD) multiple-input multiple-output\n(MIMO) system with a uniform planar array (UPA) antenna at the transmitter (Tx)\nusing tensor learning. In particular, instead of using statistical channel\nmodels, we utilize a model-free data-driven approach with foundations in\nmachine learning to generate codebooks that adapt to the surrounding\npropagation conditions. We use a tensor representation of the FD-MIMO channel\nand exploit its properties to design quantized version of the channel\nprecoders. We find the best representation of the optimal precoder as a\nfunction of Kronecker Product (KP) of two low-dimensional precoders,\nrespectively corresponding to the horizontal and vertical dimensions of the\nUPA, obtained from the tensor decomposition of the channel. We then quantize\nthis precoder to design product codebooks such that an average loss in mutual\ninformation due to quantization of channel state information (CSI) is\nminimized. The key technical contribution lies in exploiting the constraints on\nthe precoders to reduce the product codebook design problem to an unsupervised\nclustering problem on a Cartesian Product Grassmann manifold (CPM), where the\ncluster centroids form a finite-sized precoder codebook. This codebook can be\nfound efficiently by running a $K$-means clustering on the CPM. With a suitable\ninduced distance metric on the CPM, we show that the construction of product\ncodebooks is equivalent to finding the optimal set of centroids on the factor\nmanifolds corresponding to the horizontal and vertical dimensions. Simulation\nresults are presented to demonstrate the capability of the proposed design\ncriterion in learning the codebooks and the attractive performance of the\ndesigned codebooks.",
          "link": "http://arxiv.org/abs/2106.11374",
          "publishedOn": "2021-06-23T01:48:40.082Z",
          "wordCount": 712,
          "title": "Tensor Learning-based Precoder Codebooks for FD-MIMO Systems. (arXiv:2106.11374v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Eslam Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1\">Ahmad El-Sallab</a>",
          "description": "Moving Object Detection (MOD) is a crucial task for the Autonomous Driving\npipeline. MOD is usually handled via 2-stream convolutional architectures that\nincorporates both appearance and motion cues, without considering the\ninter-relations between the spatial or motion features. In this paper, we\ntackle this problem through multi-head attention mechanisms, both across the\nspatial and motion streams. We propose MODETR; a Moving Object DEtection\nTRansformer network, comprised of multi-stream transformer encoders for both\nspatial and motion modalities, and an object transformer decoder that produces\nthe moving objects bounding boxes using set predictions. The whole architecture\nis trained end-to-end using bi-partite loss. Several methods of incorporating\nmotion cues with the Transformer model are explored, including two-stream RGB\nand Optical Flow (OF) methods, and multi-stream architectures that take\nadvantage of sequence information. To incorporate the temporal information, we\npropose a new Temporal Positional Encoding (TPE) approach to extend the Spatial\nPositional Encoding(SPE) in DETR. We explore two architectural choices for\nthat, balancing between speed and time. To evaluate the our network, we perform\nthe MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of\nthe Transformer network for MOD over the state-of-the art methods. Moreover,\nthe proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",
          "link": "http://arxiv.org/abs/2106.11422",
          "publishedOn": "2021-06-23T01:48:40.074Z",
          "wordCount": 662,
          "title": "MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1\">Melissa Chase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1\">Esha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1\">Marcello Hasegawa</a>",
          "description": "In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n\nIn this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.",
          "link": "http://arxiv.org/abs/2106.11384",
          "publishedOn": "2021-06-23T01:48:40.067Z",
          "wordCount": 640,
          "title": "Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1\">Benjamin Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Large machine learning models achieve unprecedented performance on various\ntasks and have evolved as the go-to technique. However, deploying these compute\nand memory hungry models on resource constraint environments poses new\nchallenges. In this work, we propose mathematically provable Representer\nSketch, a concise set of count arrays that can approximate the inference\nprocedure with simple hashing computations and aggregations. Representer Sketch\nbuilds upon the popular Representer Theorem from kernel literature, hence the\nname, providing a generic fundamental alternative to the problem of efficient\ninference that goes beyond the popular approach such as quantization, iterative\npruning and knowledge distillation. A neural network function is transformed to\nits weighted kernel density representation, which can be very efficiently\nestimated with our sketching algorithm. Empirically, we show that Representer\nSketch achieves up to 114x reduction in storage requirement and 59x reduction\nin computation complexity without any drop in accuracy.",
          "link": "http://arxiv.org/abs/2106.11426",
          "publishedOn": "2021-06-23T01:48:40.061Z",
          "wordCount": 578,
          "title": "Efficient Inference via Universal LSH Kernel. (arXiv:2106.11426v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11428",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Celentano_M/0/1/0/all/0/1\">Michael Celentano</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_Z/0/1/0/all/0/1\">Zhou Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>",
          "description": "We study mean-field variational Bayesian inference using the TAP approach,\nfor Z2-synchronization as a prototypical example of a high-dimensional Bayesian\nmodel. We show that for any signal strength $\\lambda > 1$ (the weak-recovery\nthreshold), there exists a unique local minimizer of the TAP free energy\nfunctional near the mean of the Bayes posterior law. Furthermore, the TAP free\nenergy in a local neighborhood of this minimizer is strongly convex.\nConsequently, a natural-gradient/mirror-descent algorithm achieves linear\nconvergence to this minimizer from a local initialization, which may be\nobtained by a finite number of iterates of Approximate Message Passing (AMP).\nThis provides a rigorous foundation for variational inference in high\ndimensions via minimization of the TAP free energy.\n\nWe also analyze the finite-sample convergence of AMP, showing that AMP is\nasymptotically stable at the TAP minimizer for any $\\lambda > 1$, and is\nlinearly convergent to this minimizer from a spectral initialization for\nsufficiently large $\\lambda$. Such a guarantee is stronger than results\nobtainable by state evolution analyses, which only describe a fixed number of\nAMP iterations in the infinite-sample limit.\n\nOur proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian\ncomparison inequality to analyze the complexity of critical points that satisfy\nstrong convexity and stability conditions within their local neighborhoods.",
          "link": "http://arxiv.org/abs/2106.11428",
          "publishedOn": "2021-06-23T01:48:40.054Z",
          "wordCount": 656,
          "title": "Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. (arXiv:2106.11428v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.",
          "link": "http://arxiv.org/abs/2106.11342",
          "publishedOn": "2021-06-23T01:48:40.025Z",
          "wordCount": 565,
          "title": "Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1\">Alexander Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>",
          "description": "The study of provable adversarial robustness for deep neural network (DNN)\nmodels has mainly focused on static supervised learning tasks such as image\nclassification. However, DNNs have been used extensively in real-world adaptive\ntasks such as reinforcement learning (RL), making RL systems vulnerable to\nadversarial attacks. The key challenge in adversarial RL is that the attacker\ncan adapt itself to the defense strategy used by the agent in previous\ntime-steps to strengthen its attack in future steps. In this work, we study the\nprovable robustness of RL against norm-bounded adversarial perturbations of the\ninputs. We focus on smoothing-based provable defenses and propose policy\nsmoothing where the agent adds a Gaussian noise to its observation at each\ntime-step before applying the policy network to make itself less sensitive to\nadversarial perturbations of its inputs. Our main theoretical contribution is\nto prove an adaptive version of the Neyman-Pearson Lemma where the adversarial\nperturbation at a particular time can be a stochastic function of current and\nprevious observations and states as well as previously observed actions. Using\nthis lemma, we adapt the robustness certificates produced by randomized\nsmoothing in the static setting of image classification to the dynamic setting\nof RL. We generate certificates that guarantee that the total reward obtained\nby the smoothed policy will not fall below a certain threshold under a\nnorm-bounded adversarial perturbation of the input. We show that our\ncertificates are tight by constructing a worst-case setting that achieves the\nbounds derived in our analysis. In our experiments, we show that this method\ncan yield meaningful certificates in complex environments demonstrating its\neffectiveness against adversarial attacks.",
          "link": "http://arxiv.org/abs/2106.11420",
          "publishedOn": "2021-06-23T01:48:40.010Z",
          "wordCount": 693,
          "title": "Policy Smoothing for Provably Robust Reinforcement Learning. (arXiv:2106.11420v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1\">Priyam Tejaswin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1\">Dhruv Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>",
          "description": "State-of-the-art summarization systems are trained and evaluated on massive\ndatasets scraped from the web. Despite their prevalence, we know very little\nabout the underlying characteristics (data noise, summarization complexity,\netc.) of these datasets, and how these affect system performance and the\nreliability of automatic metrics like ROUGE. In this study, we manually analyze\n600 samples from three popular summarization datasets. Our study is driven by a\nsix-class typology which captures different noise types (missing facts,\nentities) and degrees of summarization difficulty (extractive, abstractive). We\nfollow with a thorough analysis of 27 state-of-the-art summarization models and\n5 popular metrics, and report our key insights: (1) Datasets have distinct data\nquality and complexity distributions, which can be traced back to their\ncollection process. (2) The performance of models and reliability of metrics is\ndependent on sample complexity. (3) Faithful summaries often receive low scores\nbecause of the poor diversity of references. We release the code, annotated\ndata and model outputs.",
          "link": "http://arxiv.org/abs/2106.11388",
          "publishedOn": "2021-06-23T01:48:40.003Z",
          "wordCount": 598,
          "title": "How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>",
          "description": "Unsupervised domain adaptation is used in many machine learning applications\nwhere, during training, a model has access to unlabeled data in the target\ndomain, and a related labeled dataset. In this paper, we introduce a novel and\ngeneral domain-adversarial framework. Specifically, we derive a novel\ngeneralization bound for domain adaptation that exploits a new measure of\ndiscrepancy between distributions based on a variational characterization of\nf-divergences. It recovers the theoretical results from Ben-David et al.\n(2010a) as a special case and supports divergences used in practice. Based on\nthis bound, we derive a new algorithmic framework that introduces a key\ncorrection in the original adversarial training method of Ganin et al. (2016).\nWe show that many regularizers and ad-hoc objectives introduced over the last\nyears in this framework are then not required to achieve performance comparable\nto (if not better than) state-of-the-art domain-adversarial methods.\nExperimental analysis conducted on real-world natural language and computer\nvision datasets show that our framework outperforms existing baselines, and\nobtains the best results for f-divergences that were not considered previously\nin domain-adversarial learning.",
          "link": "http://arxiv.org/abs/2106.11344",
          "publishedOn": "2021-06-23T01:48:39.968Z",
          "wordCount": 616,
          "title": "f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}